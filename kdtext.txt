text
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
recognizes influential research contributions to the field of data mining. The Outstanding Service Award is for major service contributions that have promoted data mining as a field and ICDM as the world premier research conference in data mining.
,
IEEE ICDM Research Contributions and Outstanding Service Awards, 2015 Nominations by August 15, 2015.
,
The IEEE ICDM Research Contributions Award is given to one individual or one group who has made influential research contributions to the field of data mining.
,
,
is given to one individual or one group who has made major service contributions that have promoted data mining as a field and ICDM as the world's premier research conference in data mining.
,
For the Research Contributions Award, each nomination should consist of the following pieces of information: (a) up to 3 significant publications, (b) how these publications are relevant to the data mining community in general and to ICDM in particular, and (c) the broader impacts of these publications, in terms of citations and practical applications.
,
For the Outstanding Service Award, each nomination should include the following pieces of information: (a) up to 3 significant service activities (such as running conferences and journals and facilitating data mining research, development, and applications), and (b) how these activities have promoted data mining as a field and ICDM as the world's premier research conference in data mining.
,
A nominator can be any researcher in any country, with a possible list of endorsers. The nominator should let the nominee know before a nomination is forwarded to the nomination committee chair. A nominee can be an individual or a group of researchers who have performed the research or service activities together. A nomination for either award is valid for 3 years. An unsuccessful nomination for one year will be automatically forwarded to the following two years' nomination pools, whether or not the nominator adds new information in the following years.
,
The recipients of both awards from the last three years form the nomination committee, with the previous Outstanding Service Award recipient as the nomination committee chair.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
,

Our team members are here to support you from start to finish. 
Be part of ,!  "
"
,

,

,One of the most popular Big Data news stories this week is not about any technological innovation or any startup or any research paper; it?€?s about a rap music video in which sexy girls are chanting ,?€? B - I - G - D - A - T - A ?€?.

,
,, an integrated marketing and creative content team within Viacom?€?s music and entertainment groups, released a cool three minute music video ad featuring ?€?The Fat Jewish?€? (E! channel interviewer Josh Ostrovsky) personifying as Big Data and singer/dancer Todrick Hall as Hadoop. From hip-hop beats to funky lyrics this video has got it all to amuse anyone in Big Data industry. Though, those outside the Big Data world might need a little help in understanding the jargons that feature in every other line.

,

Excerpt from lyrics:

??
,
,
Released just two weeks ago, the video already has more than 200,000 views on YouTube. In , the creative lead, , (EVP, Viacom Velocity) said: ?€?We thought rap worked perfectly as a way to infuse all the data jargon being thrown around, and a rap music-video parody was perfect coming from the company that made the music-video genre famous.?€?

,

He added that the primary target of this rap video ?€?is mostly b-to-b, our agencies and client partners who understand, as we do, the importance of tapping Big Data to target fans and consumers across the media landscape. We also wanted to create a b-to-b video that is entertaining enough to appeal a broader audience.?€? And that, this music video has definitely achieved as it is being played and shared by many who know little about Big Data.
,

More lyrics:

,
,
,

So far, we had seen Big Data play a cameo in movies and songs, but this is the first time I am seeing Big Data and Hadoop take center stage in any entertainment content (along with passing mention of Map-Reduce and NoSQL).
,

At some point or other in the past few years, we all have made fun of the super inundating Big Data jargons. But this creative package of rap, hip-hop music, and meeting room chat prelude takes it to a different level altogether. Here are some more funky lines:

,
,
,
My personal belief is that this music video could have been way cooler if the creators had a Big Data expert among them. Things as simple as including some overused slides among Big Data related presentations and visual effects on ?€?data scientist ?€? the sexiest profession of 21, century?€? could have added lot more fun.
,

Nevertheless, it?€?s a good start. Keep them coming, music industry! We want more!!!
,
,

,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
        ,
,
,
  "
"
,
CYPHER 2015, Analytics India Summit - 12th Sep, Bangalore, India
,
Plan to be part of India's most exciting Analytics event for the year
,
Venue: Hotel Park Plaza, Bangalore, India
,Website: 
,
,
Cypher 2015 brings you India's most exciting Analytics event of the year from leading Analytics forum 
,. The summit aims to provide an excellent opportunity for a direct connect with talented professionals, senior leaders and innovative analytics organizations.
,
Cypher 2015 would provide you the platform to Network, explore and showcase to the best in Analytics industry.The summit will have 3 tracks:
,(1) Conference & Panel Discussion, 
,(2) Workshops & Information Sessions, and 
,(3) Exhibition area
,
,
,
Cypher 2015 provides you the platform to showcase and interact to the decision makers, data scientists, architects etc. We bring to you latest on the skills, tools, and technologies you need to decrypt your data.
,
At Cypher 2015, you'll:
,
,??,
""Energizes and empowers Analytics professionals to make better decisions, overcome challenges, and maximize their company's IT investment. Plan to be a part of the action this year.""
,
,
,
Cypher 2015 attracts a technically savvy audience that will immediately understand and appreciate the value of your offerings. What's more, while your company's technical experts are enhancing their relationships with your existing customers and developing relationships with new ones, they'll also be tuning their skills and leveraging the tools they need to be flexible and agile in an ever-changing competitive marketplace.
,
,
,
,
,
Cypher 2015 is for developers, data scientists, data analysts, and other data professionals, including:
,
,??,
For more information, write to 
,
,
Event Introduction Video -
,  "
"
,
,
, is an astrophysicist turned data scientist. After receiving her PhD from Cornell University she worked as a postdoctoral scholar at Caltech and the NASA Jet Propulsion Laboratory, and then as a research fellow at the University of Chicago. During this time she was a member of the development teams for several space telescope missions, including the SuperNova Acceleration Probe, the High Altitude Lensing Observatory, and Euclid. Ali joined , in 2013 and she currently works on the Data Science team, where she uses historical data and machine learning techniques to study issues pertaining to sales and marketing.
,
Here is my interview with her:
,
,Q1. What are the common use cases of Analytics at Groupon? Which of them are the most challenging?
,
,: At ,, we use analytics to provide data-driven answers to business questions, to develop new products and to optimize various processes. We use analytics in every stage of our business, from finding the best merchants for each market, to pairing customers with the most relevant deals and optimizing the layout of each page of our website. Some of the most challenging use cases deal with user-level modeling, for instance in determining relevance, due in part to such a large yet sparse data set.
,
,
,
,The most important factors in determining the ranking include the demand for each service in each market, the quality and riskiness of each merchant and the probability to close each lead. We use different types of predictive models for each, tailored to the needs of each category.
,
,
,
,Predicting demand from historical sales data is particularly difficult in cases of very rare or new services, where we may not have sufficient data for a time series model. In addition, sales-velocity-based demand can lead to ?€?self-fulfilling prophecies.?€? For example, if Groupon only offered pizza on our site, then we would only ever forecast demand for pizza, and we would forecast zero demand for everything else.
,
,
,
,Due to the limitations of demand forecasting from only historical sales data, we also supplement it with demand forecasting from search query data. The Groupon website and app feature prominent search bars, and we track the query string and location for every search event on each platform. We then match these events to our deal taxonomy to find instances where users are searching for something but then not making a purchase. As a result, we use low conversion rate searching as an inventory gap detector.
,
,

,Quantum Lead uses these models to prioritize merchants, and then it matches those leads to the best-suited members of our sales force. Then when each sales rep comes into work in the morning, he/she has a ranked ?€?to do?€? list of merchants to reach out to.
,
,
,
,It is of course difficult to earn trust in an algorithm, especially when the user has domain expertise that they already trust. This is why we?€?ve had focus groups with the sales team to put some faces behind the data science team and to make ourselves available to answer any questions. We also include ?€?sales value reasons?€? for each lead in the UI, to let the rep know why it is prioritized the way it is. These reasons could include things like good customer reviews, prior deal performance or high refund risk. Reps also have a mechanism to ?€?flag?€? anything that they find suspicious, and this feedback is worked back into the system.
,
,
,
,Historical sales data can be sparse and search query data can be messy. ,We deal with the sparsity problem by employing fallbacks, using our deal taxonomy. Deals are categorized in terms of service and merchant type in a tree structure with different levels of granularity. If we lack sufficient data at the smallest granularity level, then we fall back and use results from the next level up. We also employ geography-based fallbacks, using data from larger geographic regions if we lack sufficient data on smaller scales.
,
,
,

, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
        ,
,
,
,
,
,
,

,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,

,
,
,

,
,
,
,

,
,
,
 ,  "
"
,
,
, is an astrophysicist turned data scientist. After receiving her PhD from Cornell University she worked as a postdoctoral scholar at Caltech and the NASA Jet Propulsion Laboratory, and then as a research fellow at the University of Chicago. During this time she was a member of the development teams for several space telescope missions, including the SuperNova Acceleration Probe, the High Altitude Lensing Observatory, and Euclid. Ali joined , in 2013 and she currently works on the Data Science team, where she uses historical data and machine learning techniques to study issues pertaining to sales and marketing.
,
,
,
Here is second part of my interview with her:
,
,

,: On the data science side we have a Teradata database and a Hadoop cluster, and we most commonly use R and Python for modeling. The internal tools that bring it all together are in a combination of Ruby and Clojure. The front end is an Ember application.
,
,

,Mobile customers tend to be more engaged than our web-only customers, purchasing more frequently and spending more. Though mobile purchasers are more engaged, it takes longer for a mobile customer to activate.
,
,
,
,No one else has the scale and breadth of deals as Groupon. We?€?ve evolved from a deal-of-the-day site into a global ecommerce marketplace with ,more than 425,000 active deals. This progression has reshaped our business into one with a majority of transactions occurring through mobile devices with a focus on three key categories: Local - health, beauty and wellness, things to do and food and beverage; Goods - electronics, home furnishings and apparel and Travel - hotels, resorts and tours. We also see a number of interesting opportunities to more closely align and leverage these categories. For example, if someone books a trip to Miami, we have an opportunity to offer them inventory from Goods to help outfit them for the beach as well as restaurant deals and things to do during their trip.
,
,
,
,When I started college I actually wanted to major in music performance but I eventually decided to switch to something that I thought ,was more practical, physics. Little did I know the long and difficult career path awaiting me in my new choice! Eventually, I grew weary of working in such an impersonal field and decided to pivot to using my scientific know-how on human behavior. Now I study how people interact with the ecommerce world. Despite the drastically different subject matter, there are definite parallels between the day-to-day life of working in theoretical astrophysics vs data science. I still spend much of my day writing code and relying on my background in statistics. I no longer need advanced differential geometry, but now I get to learn data mining and machine learning.
,
,

,It?€?s advice I?€?ve gotten on multiple occasions -- to always be flexible and adaptable. It?€?s advice I?€?ve put into practice in my career path changes from music to physics to data science. It?€?s also important advice within the data science field itself; everything has to be scalable and the only way to keep up is to constantly experiment with the latest technologies.
,
,
,
,We have a team full of brilliant people from a wide variety of backgrounds, ranging from business to math to computer science. Above all else, we?€?re looking for smart and creative people. Experience with the specific tools we use (e.g. R and Python) is nice to have, but this is easy enough to pick up on the job if the candidate has a solid technical background.
,
,
,
,I read it a few years ago when it first came out, but I plan to re-read it again -- ?€?Bossypants?€? by Tina Fey, one of my personal heroes.
,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,

,  "
"
,
,
,??,??
,  "
"
,
,
,
,
,

,
,
,
,
,
,
,

,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,

,
,

,
,
,
,

,
,
,
,

,
,
,

,
,
,

,
,
,  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
,
The Berkeley MFE Program is once again joining forces with ENSAE ParisTech (University Paris Saclay) to hold the second annual #DataLead Conference on November 5th and 6th in central Paris, France at the Conference Center of AXA GIE, one of our platinum sponsors.  
,
This year we hope to continue the conversation about how big data is being applied to real world business issues with a particular focus on finance, insurance, and marketing.  It will also cover many other facets of data science with a focus on applications to industries ranging from technology and retail to education, science, and government. 
,
Please visit , for more information. 
,
Sponsorship opportunities are also still available and are explained in detail on the website.  Please join us for what will be an exciting and informative 2-day conference!  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 2 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
We analyze Top 20 Python Machine learning projects on GitHub and find that scikit-Learn, PyLearn2 and NuPic are the most actively contributed projects. Explore these popular projects on Github!
,
,
,Fig. 1: Python Machine learning projects on GitHub, with color corresponding to commits/contributors.  Bob, Iepy, Nilearn, and NuPIC have the highest such value.
,
,

This post used some content from ,
,
,
,
,
,
,
 ,  "
"
,
,
Global Big Data Conference is offering 3 day extensive developer event on Big Data on June 19-21 2015 at HiTex Convention Center,  Hyderabad, India. This is a fast paced, vendor agnostic, technical overview of the Big Data landscape. No prior knowledge of databases or programming is assumed.  Big Data Developer conference is targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, Security, Operations, Spark, NoSQL (Cassandra, Mongodb, Neo4j), Data Science, R,  Python & Machine Learning.  Attendees will experience real Hadoop clusters and the latest Hadoop distributions.
,
,
,
Unlike other big data training sessions, our developer event is unique in the following aspects:
,
,??,
,
,Engineers, Developers, Architects, Networking specialists, Managers, Executives, Students, Professional Services, Architects, Data Analyst, BI Developer/Architect, QA, Performance Engineers, Data Warehouse Professional, Sales, Pre Sales, Technical Marketing, PM, Teaching Staff,  Delivery Manager
,
Please register ASAP to take advantage of discount. Seats are limited(upto $200 discount )
,
,
,
Register: ,
,
Agenda: ,
,
Speakers: ,
  "
"
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
Here are upcoming June - December 2015 meetings and conferences. 
,
For full list see ,  page.
,
,
,
Here are the upcoming meetings.
,
Most popular locations are:
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,
,??,
,
,??,
,
,??,
,
,??,
,
,
,??,
,
,  "
"
,
,
,
In May on , we have an post on the effectiveness of neural networks, mining time-lapses from Internet photos, the first part of a new Deep Learning book, Kaggle's R tutorial, and a list of free ebooks for machine learning.
,
,
,
In this post, the author introduces the concept of a recurrent neural network, then dives into what makes them so effective. He also provides some of the code used for this post ,. This is an interesting post for those who are interested in a technical introduction to RNNs.
,
,
,
This post links to a new paper from the University of Washington about mining time-lapses from Internet photos. The system introduced in this paper automatically creates time-lapses from the huge quantity of available photos on the Internet. This is a fascinating read for anyone interested in mining data from photos.
,
,
,
This post contains the first part of a deep learning textbook in progress. Considering it is currently in production, the authors are looking for feedback from the community. So if you're interested in the book, give it a read, and send any feedback to any of the authors, ,, ,, or ,.
,
,
,
This post is a tutorial put out by Kaggle to learn how to use R for Kaggle competitions. This tutorial is interactive, which is great if you like to follow along when learning a new programming language or ecosystem. Check this out if you're interested in starting with R or Kaggle competitons (or both).
,
,
,
This post links to 12 different free ebooks for machine learning. These books range from broad to very focused, offering something for people of all skill levels. Give this a read and see if any of the books interest you.
,
,
,
,
  "
"
,
,
, is currently VP of Data Products at ,, the leading online real estate auctioneers in the United States.
,
He has also held positions at two leading online games companies, Kabam and Playfirst, where he has built out Analytics and Big Data groups from inception. Earlier on in his career, Sheridan worked at Procter and Gamble in the areas of Decision Support and Executive Information Systems. He also worked as a Managing Consultant at Towers Perrin.
,
Sheridan holds a Bachelor of Arts degree in mathematics from Cambridge University and an MBA from the Haas School of Business at the University of California, Berkeley, both with honors.
,
,
,
Here is second part of my interview with him:
,
,
,
,: Well in the past nobody had Data Scientists, we just had those bunch of nerds we locked in the basement who did this crazy math stuff nobody understood, and if you were lucky they might come up with something that saved you a bit of money. I think the biggest change is now we?€?re dealing with huge amounts of data in many businesses, many of our interactions are online, and perhaps most importantly, we can now more clearly prove the value, particularly on the revenue side. In many ways we've now moved to Data Science being a front office function. 
,
As to where it?€?s headed in the future, as I said earlier, part of the problem with the future is that it?€?s very hard to predict. In the short term we?€?re going to see continued high demand for data science, and equally high expectations. We?€?ll also see expansion into new areas in the business such as human capital management. At some point in these cycles, you usually find there?€?s a backlash, where some businesses will say it?€?s not paying off, but I suspect that will be temporary. Eventually Data Science becomes as mainstream as any function or activity in the business.
,
,
,
,Ha Ha. It?€?s not a question I've asked my Data Science Team or one I ask potential hires, so I?€?m not sure I can answer that! However, I do think, like any other nascent pioneering role in an organization, where the rules are not completely written on what you do, what you can do, and what you should do, it?€?s a heck of a lot of fun if you?€?re the type that thrives on that.
,
As for career advice, don?€?t pick it because it pays well, or because it?€?s considered sexy, cool or the new-new thing. Pick it as a career because you love data, love math, love coding, love solving problems, and you love seeing the results of solving those problems. In terms of development, don?€?t forget the softer skills, your ability to communicate complex ideas simply, your ability to focus on the areas that matter, and your ability to persuade people, are often as important as the technical skills.
,
,
,
,I read a lot of stuff on behavioral economics, and I think that it?€?s tremendously important for people in data science. Traditional micro-economic theory is built on the assumption that people make rational utility maximizing decisions, but there?€?s lots of recent research that shows that assumption is often false. Understanding how and why people make decisions internally in your organization as well as understanding what influences your customers is exceptionally important information for ,Data Scientists. In terms of specific books, almost anything by Dan Ariely is great, and his book ?€?Predictably Irrational: The Hidden Forces That Shape Our Decisions?€? would be a perfect place to start.
,
Work-Life Balance is always easier when you love what you do. I love the web, I love data, I love Math and Statistics and I love discussing and reading about new ideas, and I?€?d do that at home regardless. It also helps that I've had bosses that are more focused on results than face time; that?€?s important for me because are immutable engagements like anniversaries, or birthdays, or my kids concerts, that I never miss.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,

,
, is an architect at , in the Big Data Group. He founded the Apache Phoenix project and leads its on-going development efforts. Prior to Salesforce, James worked at BEA Systems on projects such as a federated query processing system and a SQL-based complex event programming platform, and has worked in the computer industry for the past 20+ years at various start-ups. He lives with his wife and two daughters in San Francisco.
,
Here is my interview with him:
,
,
,
,:  Mujtaba and I started , about four years ago as an internal project at Salesforce to make it easy for people to leverage HBase which was being rolled out at as a complementary data store at Salesforce. It became clear pretty early on that we weren't going to get the adoption we wanted without a SQL front-end, as Salesforce is a big relational shop - no one wants to learn the new, proprietary, low level APIs of HBase.
,
In addition, we wanted to have front web applications serving up HBase data, so your standard map-reduce through Hive was not going to meet the latency requirements. Hence, Phoenix was born with the name based on a failed prior attempt to build our own big data stack from the group up. We certainly learned a lot and Phoenix rose from its ashes.
,
,

,

,We quickly realized the general utility of Phoenix as an alternate, easier mechanism to access HBase data. Salesforce has always used a lot of open source (,) and this was a good opportunity to further advance our commitment to open source. It's also allowed Phoenix to grow more quickly as partners such as Intel and Hortonworks came on board to help round out the functionality.
,
Our priorities have always been driven by the Phoenix user community - that hasn't changed. As both the user and developer community has grown, however, we've had to put more processes in place to ensure stability: branching strategies, backward compatibility constraints, automated building and testing, and testing at scale.
,

,

,

,HBase has a sparse data model, storing a separate cell per value that is set, while RDBMS typically have a dense model storing all values for a single row together. Both have pros and cons. In addition, HBase stores its data in immutable files which conceptually overlay each other to achieve mutability, while relational systems typically update the data in place. This model helps HBase scale horizontally more easily while making support for transactions more difficult.

,



,

,
,Phoenix has a pretty typical query engine architecture with a parser, compiler, planner, optimizer, and execution engine. We push as much computation as possible into the HBase server which provides plenty of hooks for us to leverage.

,

,

,
,Key factors behind the performance of Phoenix include:
,
,
,
,
,On the one hand, our decision to interop with the existing HBase data model makes it easy for users to both get started as well as integrate with existing Hadoop-based infrastructure. On the other hand, we're relying on the HBase community to implement more efficient block-encoding schemes when the schema (i.e. set of column qualifiers) is known in advance. This improvement could make dramatic improvements on scan speed for OLAP applications written for Phoenix.
,
,
,
,
,  "
"
        ,  "
"
Most popular 
, tweets for May 26 - Jun 01 were
,
For aspiring Data Scientists: LeaRning Path: Step by Step Guide to Learn #DataScience on R 
, 
,
,
For aspiring Data Scientists: LeaRning Path: Step by Step Guide to Learn #DataScience on R 
, 
,
,
,
For aspiring Data Scientists: LeaRning Path: Step by Step Guide to Learn #DataScience on R 
, 
,
,
Creator of , @Matei_Zaharia wins @ACM Doctoral Dissertation Award 
, 
,
,
,
,  "
"
,
,
,??,??
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,
  "
"
        ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 9 and later.
,
See full schedule at , .
,
,  "
"
,

,
, is an architect at , in the Big Data Group. He founded the Apache Phoenix project and leads its on-going development efforts. Prior to Salesforce, James worked at BEA Systems on projects such as a federated query processing system and a SQL-based complex event programming platform, and has worked in the computer industry for the past 20+ years at various start-ups. He lives with his wife and two daughters in San Francisco.
,
,
,
Here is second part of my interview with him:
,

,

,

,: Depends on what you consider ""other alternatives"", but one big advantage that Phoenix has is superior performance due to the push down techniques we use. Phoenix is also very easy to install and get started using. If you compare Phoenix against developing an application directly on top of the HBase APIs, the advantages come from:
,
,

,

,

,The biggest thing they appreciated from Phoenix is its performance. For more on who is using Phoenix and what they appreciate, see??,

,

,

,

,Our upcoming 4.4 release introduces a number of new features:?? User Defined Functions, UNION ALL support, Spark integration, Query Server to support thin (and eventually non Java) clients, Pherf tool for testing at scale, MR-based index population, and support for HBase 1.0.
,
After that, we'll introduce transaction support followed by Apache Calcite integration to improve interop with the greater Hadoop ecosystem through plugging into a rich cost-based optimizer framework.

,

,
,

,Phoenix has had join support for over a year (see??,). We're actively working on transaction support by integrating with Tephra (,). If all goes according to plan, we'll release this after our 4.4 release (in 4.5 or 5.0).
,

,

,

,There's so much innovation going on right now in the Distributed Computing and Big Data areas, especially in the open source world. I'm excited to see how these technologies will converge and work together more seamlessly.

,

Here are a few trends, just top off my head:
,
,

,

,

,""Good enough"" is usually not good enough (from my dad when I was about 14 years old).
,
,

,

,Someone who can get stuff done, is passionate about doing things the ""right"" way, works independently figuring things out on his/her own, and still knows when to ask questions. Also, someone who is easy to work with and enjoys the collaboration that takes place in the open source community.
,

,



,I really enjoyed both??,??by Jeannette Walls and??,??by Ursula Hegi.
,
,
,  "
"
,
,
,
Learn Data science the new way by listening to these compelling story tellers, interviewers, educators and experts in the field. Data suggests that podcasting about Data Science is only growing! 
,
,
,

,
,
Note: The podcasts are ordered by number of ratings in iTunes, except ""Freakonomics"" - which had the most ratings (2711) - was placed fourth, and ""Obsessive Compulsive Data Quality"", which was a highly popular podcast in 2014 but is not updated since, was placed last.
,
This post used some content from ,
,
,
,
,

,
,
 ,
   "
"
,
,
, is head of data science engineering & technology at ,, where he leads projects on customer analytics and personalization. Earlier, as lead data scientist, he led multiple business impacting projects in recommendations and personalization that has significantly enhanced consumers?€? shopping experiences.

,

Prior to eBay, Dr. Sinha was a research academic at the University of Melbourne and holds a PhD in computer science from RMIT University, Australia. He has published over 30 works, including in top-tier venues such as IEEE Big Data, VLDB, and ACM SIGMOD.

,

He won the sort benchmark for both JouleSort and PennySort and was amongst Wall Street Journal?€?s top-12 Asia-Pacific young inventors. He regularly speaks on data science, big data technologies, and co-organizes the popular Bay Area Search Meetup.

,
,
,
Here is second part of my interview with him:
,

,
,

,As automated personalization and 1-to-1 targeting becomes more accessible, marketers are learning to leverage the wealth of existing data to focus on engagement opportunities that could not have been otherwise identified. Very soon, no two people will see the exact same content on a site. Improved access to data, predictive analytics, and real-time actionable insights is expected to deliver unprecedented lift in engagement, conversions, and revenue on an e-commerce site.

,

Based on recent surveys, 94% of companies agree that personalization is critical to current and future success (Econsultancy), however, 70% of brands are yet to personalize emails sent to subscribers (Experian) and 60% of marketers struggle to personalize content in real-time even though 77% believe real-time personalization is crucial (Adobe & DMA). Thus, while there is significant enthusiasm and momentum, I believe we are still in the early stages of automating personalization.

,

Recent advancements of personalization in e-commerce include:

,
,
,

,

,Sorting is an invaluable tool that allows many common tasks to be performed efficiently. Algorithms for sorting are also of great theoretical importance and several advances in data structures and algorithmic analysis have come from their study. They have been investigated since before the first computers were constructed. The underlying reason for such interest in sorting is the potential of reaping huge computational savings, as sorting is a basic element of a wide range of computational activities.

,

A few lessons learnt from these competitions include:

,
,
Several more hardware factors can assist in finely tuning the sorting application in order to be able to extract the optimum performance in a given system. These include the CPU speed, cache hierarchy, number of cores, input/output bandwidth, disk read/write/seek time, RAM access latency, CPU frequency scaling, low latency RAM, disabling unnecessary services, choice of motherboard, as well as choice of algorithms and data structures. Furthermore, the lessons from algorithmic engineering efforts as exemplified by publications in reputed venues such as ACM JEA, VLDB Journal, and ACM SIGMOD can be applied in tuning the algorithms and implementations.

,

,

,

,The advice that resonates closely with me is ?€?Stay Hungry, Stay Foolish?€? by Steve Jobs. A second advice is ?€?Career is a marathon, not a sprint?€?. A recent , article provides excellent insights from some of the finest technology leaders today.

,

,

,

,This is an incredibly exciting time to be involved in the areas of Big Data and Data Science. While it is very difficult to predict future trends in such a fast-paced field, a few trends that appear likely to continue growing in importance in the near-term include:

,
,
,

,

,I would be thrilled to start my career in the analytics industry today. Below are a few pointers on how I would likely shape my career:
,
,
,
,

,

,The books I enjoyed reading recently are:
,
,
During my spare time, I like to engage in outdoor activities such as hiking and biking, listening to instrumental music, cooking, playing the Conga drums, and catching up on technology advancements.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for May 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, sorted by decreasing number of engagements,
,
,??,
Here are previous month activities
,  "
"
,
,
, is head of data science engineering & technology at ,, where he leads projects on customer analytics and personalization. Earlier, as lead data scientist, he led multiple business impacting projects in recommendations and personalization that has significantly enhanced consumers?€? shopping experiences. 
,
Prior to eBay, Dr. Sinha was a research academic at the University of Melbourne and holds a PhD in computer science from RMIT University, Australia. He has published over 30 works, including in top-tier venues such as IEEE Big Data, VLDB, and ACM SIGMOD. 
,
He won the sort benchmark for both JouleSort and PennySort and was amongst Wall Street Journal?€?s top-12 Asia-Pacific young inventors. He regularly speaks on data science, big data technologies, and co-organizes the popular Bay Area Search Meetup.
,
Here is my interview with him:
,
,
,
,The three levels of categorization are in reference to Personalization platforms. I believe it is necessary to distinguish between preferences based on long-term behavior and active intents based on recent behavior of a customer. For example, a customer may have a long-standing preference in collecting Native American artifacts but is currently looking to purchase (active intent) camping gear for the upcoming long weekend. It is imperative that we differentiate between our customer's recent intents and long-term preferences in order to provide relevant inventory.
,
Online analytics considers the events occurring within the same session as the user is engaging on the site and rapidly deriving insights from them in order to be actionable within the same session. Nearline analytics considers the events that have occurred in the recent past and resides in an online data repository of platforms. Offline analytics considers the vast majority of events that are stored in an offline data repository such as Hadoop or Teradata; these typically include events spanning nearline and several months or years.
,
,
,
,Both the amount of data and the number of Hadoop cluster nodes have experienced dramatic growth at eBay. The insights generated from Big Data processing on Hadoop have greatly helped us to better our customers?€? experience across multiple domains. I am one of the first beneficiaries (in 2010) to use Hadoop for operationalizing data science models across major sites that led to nearly a billion impressions per day surfacing on high traffic pages such as Home, Search Results, and Product listing besides seasonal and gifting pages.
,
The Hadoop eco-system is being used for a diverse range of workloads such as for Data Processing, System Services, Data Science, Business Intelligence, Analytical applications, and Search. There is significant opportunity in using Big Data to continually derive insights at scale and speed to better our customer's experience. The challenge is in ensuring that as users we also manage capacity and job resources in the shared Hadoop cluster judiciously.
,
,
,
,The sheer growth in data volume and Hadoop cluster size make it a significant challenge to diagnose and locate problems in a production-level cluster environment efficiently and within a short period of time. Often times, the distributed monitoring systems are not capable of detecting a problem well in advance when a large-scale Hadoop cluster starts to deteriorate in performance or nodes become unavailable. As a result, both reliability and throughput of the cluster reduce significantly. The current rules-based approach using a handful of metrics results in a large number of false positives, which is especially time consuming due to the existing manual intervention necessary to flag failed nodes to the scheduler. Alternative approaches include installing Apache Ambari, but it does not provide prediction models and it can only detect failures once the data nodes already reach a bad state.
,
We address this problem by proposing a system called Astro, which consists of a predictive model and an extension to the Hadoop scheduler. The predictive model in Astro takes into account a rich set of cluster behavioral information that are collected by monitoring processes and model them using machine learning algorithms to predict future behavior of the cluster.  The Astro predictive model detects anomalies in the cluster and also identifies a ranked set of metrics that have contributed the most towards the problem.  The Astro scheduler uses the prediction outcome and the list of metrics to decide whether it needs to move and reduce workloads from the problematic cluster nodes or to prevent additional workload allocations to them, in order to improve both throughput and reliability of the cluster.
,
The results demonstrate that the Astro scheduler improves usage of cluster compute resources compared to traditional Hadoop and the runtime of representative applications reduces by over 20% during the time of anomaly, thus improving the cluster throughput. For further information on Astro, please refer to the paper titled ?€?Astro: A predictive model for anomaly detection and feedback-based scheduling on Hadoop?€? published in the 2014 IEEE International Conference on Big Data.
,
,
,
, is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets. It is an extremely fast OLAP engine at scale and designed to reduce query latency on Hadoop for tens of billions of rows of data. Kylin offers ANSI SQL on Hadoop and supports most ANSI SQL query functions. It is attractive for multi-dimension analysis on large data sets (billion+ rows) offering low query latency. Kylin also provides good integration with existing BI tools such as Tableau. Users can interact with Hadoop data via Kylin at sub second latency, which is better than Hive queries for the same dataset. Existing SQL-on-Hadoop solutions need to scan partial or whole data set to answer a user query, which can be inefficient.
,
For further details, please refer to the following online resources:
,
,
,
,
,
,  "
"
Most popular 
, tweets for Jun 2-8 were
,
Starting salaries for #DataScientists have gone north of $200,000- , 
, #BigData #Salary 
,
,
,
Neural Networks and Deep Learning, free online book (draft) #BestOfKDN 
,
,
#DataScience #Evolution frm a Nerdy #Hobby to a Strategic Priority: Sheridan , 
, #BigData 
,
,
,
Starting salaries for #DataScientists have gone north of $200,000- , 
, #BigData #Salary 
,
,
,  "
"
,
,
June 25, 10AM - 11AM PDT
,
Registration Link: 
,
,
Alternative link: 
,
,
If the time is inconvenient, please register and we will send you a recording.
,
,
Logistic regression is a commonly used tool to analyze binary classification problems. However, logistic regression still faces the limitations of detecting nonlinearities and interactions in data. 
,
In this webinar, you will learn more advanced and intuitive machine learning techniques that improve on standard logistic  regression in accuracy and other aspects. 
,
As an APPLIED example, we will demonstrate using a banking dataset where we will  predict future financial stress of a loan applicant in order to determine whether they should be granted  a loan. Although the focus is related to finance and loans, the concepts are relevant for anyone who actively uses logistic regression and wishes to improve accuracy and predictor understanding.
,
Logistic binary gradient boosting will be explained theoretically and shown in hands-on application to generate accurate predicted probabilities.
,
This webinar will be a step-by-step presentation that you can repeat on your own!
,
Included with Registration:
,  "
"
        ,    "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,  "
"
By Gregory Piatetsky,  
,.
,
, is General Manager of the IBM Analytics Platform business. This platform spans Predictive Analytics, BI, Hadoop, Spark, Stream Computing, Databases, Warehouse, Content Management, Information Integration, Master Data Management, and Governance. Her passion is relentless pursuit of innovation and working with clients to realize value for their businesses. 
,
I recently had a chance to discuss Analytics with her in advance of IBM 
, in San Francisco focusing on Spark.
,
,
,
,
Analytics will no longer be something that a few very smart mathematicians and data scientists use and understand; analytics will be a part of everything we do.  The surprise will be how much industries have transformed because of it - even more so than the creation and transformation fueled by the Internet and e-business.
,
In the next 3-5 years, transportation, manufacturing, banking, telecommunications, our home services - you name it - will all see new processes and business models formed because of analytics.   In fact, there was a recent study where Nucleus Research found that for every dollar spent on analytics, the payback was just over $13.  
,
No question in my mind - the organizations that step back and use analytics to impact all aspects of their business will gain competitive edge.  Those that don't will be in for a real surprise.
,
,
,
Nothing has stayed the same.  Data is more readily available.  External factors, like weather, sensors and social interaction have extended the data corpus.  Plus analytics capabilities are becoming more sophisticated to sift through masses of disparate data - even data streaming in motion - and extract valuable insight. Not only are they more sophisticated, the fundamental core components of compute, storage and bandwidth are declining at an incredible rate. We did a quick analysis to plot (Figure 1) this trend using publicly available data and its clear the barriers to entry have fallen. 
,
,
,
,
,
Add to this the advent of Cloud, Mobile, the Internet of Things and we have a new world in which a growing developer community can build analytic-driven capabilities, deploy them where they are needed, and do so at a cost that is far more affordable than ever before.  
,
,
,
,IBM has a long history of open source commitment.  In 1999, IBM was a part of the inaugural announcement of the Apache Software Foundation (ASF) because of our commitment to open source and the collaborative community.  Not only has that commitment - through both sponsorship funding and donation of millions of lines of code - continued every year since; it has grown.
,
The history of Linux would be radically different without IBM's involvement. IBM was the first (and for a while, the only) established enterprise IT company that publicly sponsored and legally backed the usage of Linux. Since the beginning of the Linux project, IBM is the #3 code contributor to Linux, behind only RedHat and Intel.
,
,Key technologies to IBM's cloud strategy - like Cloud Foundry and OpenStack - are open source based. Cloud Foundry is the basis for Bluemix, and IBM is a top contributor to the Cloud Foundry codebase since its inception.
,
It's not just about software - IBM is committed to open source, as evidenced by the OpenPOWER consortium.
,
In the Hadoop space, IBM is ramping up its development resources to increase code contributions. My development team is actively collaborating with members of the Hadoop committer community - some of which are of course IBMers - to work on issues related to YARN performance, storage flexibility, and overall Hadoop security.  Also, I recently opened a Spark Technology Center in San Francisco, and that team is already making significant efforts in contributing to the Spark code base.
,
Open source is important for technology progress and innovation.  But, it isn't the only aspect that enables technology innovation, advancement, and disruption.  That's why my worldwide engineering team is focused on combining open source, with standards, and our own innovative capabilities to help our clients optimize the deployment mix and get to their business objectives faster.  
,
,An example is our work with Apache Hadoop.  We contribute to the open source Hadoop projects.  We understand the need for standards in business and technical solutions, so we build standards compliance in our products.  Hadoop is no different - for example, our SQL solutions on Hadoop are ANSI compliant at the highest levels. We collaborated with other vendors to create the Open Data Platform initiative with the goal to establish consistency, so customers would have a stable foundation for their own innovation. 
,
Of course we provide our own free for production use Hadoop distribution based on that consistent core technology.   On top of that, we provide rich capabilities for analysts and data scientists to extract value from data stored in Hadoop, as well as data integration and management capabilities to ensure their Hadoop environment can be managed and deployed enterprise-wide.
,
Hadoop and Spark are great examples of disruptive technologies. But to make their value accessible to more than just application programmers, deep support for familiar interfaces (like SQL and R) is essential for adoption.  Integration with established tools, like reporting, ETL, and data stores (warehouses and databases) is also important. Finally, it's when you take advantage of new technical advantages to do things that weren't possible before, that you start getting real value. This is the motivation behind our innovations like Big Match, which features large scale entity extraction, where you can, for example, learn things about your customers from call center logs, emails, or social data feeds.
,
We have proven out this approach of advancing and extending open source with our strong heritage in Linux and Java.  Now with Hadoop and Spark, our task is to continue our pace of innovation, embrace and contribute to open source, and ensure all our constituents gain the benefit from strength across the board.
,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
The programs and schedules for Scala By the Bay (SBTB) and Big Data Scala By the Bay (BDS) 2015 conferences are announced and published:
,
,, Oakland, CA, August 13-16, 2015.
,
,, Oakland, CA, August 16-18, 2015.
,
There are 77 best talks from the leading companies using Scala, Spark, and other Scala-based projects in production, including Twitter, Salesforce, Cloudera, Verizon, Comcast, Spotify, Hootsuite, Typesafe, Databricks, Nitro, Liveperson, Tableau, and many others.
,
SBTB and BDS are separate conferences, with BDS expanding into data science and data management.  They share an innovative end-to-end pipeline training on 8/16, when in one day, we'll teach hundreds of developers to build a web-scale startup on Mesos, Akka, Kafka, Spark, and Cassandra, taught by engineers from Mesosphere, Typesafe, Confluent, Databricks, and DataStax, respectively.
,
For the first time in the history of any of the Scala conferences, Twitter adds a whole Finagle Day to SBTB, teaching OSS developers the biggest real-time Scala stack in production via hands-on workshops taught by Twitter engineers, and a series of talks from Finagle creators and users inside and outside Twitter.
,
,
SBTB+BDS topics include higher-order abstractions for multiple application areas, data pipelines, ""big"" data analytics, Machine Learning and Natural Language Processing, datacenter management with Mesos, and more.  The key themes unifying both conferences is applying rigorous Functional Programming principles for DRY and elegant codebases that can grow with smart teams as companies go web-scale, and the emergence of Reactive Systems that replace ETL with common object models applied across all stages of an application - from API to message bus to real-time analytics.  Several versions of the resulting ""lambda architectures"" will be presented.
,
Martin Odersky, the creator of Scala, will keynote the By the Bay conferences for the first time.
,
Jonas Bon????r, the CTO of Typesafe, is developing a completely new talk for Scala By the Bay, which he will keynote together with Dean Wampler, Dick Wall, Vidhya Narayanan, and Andrew Headrick.  Vidhya leads the Verizon OnCue Scala team, with some of the highest concentration of FP talent in the world, and Andrew, formerly the Akka architect at Ticketfly, is now a CEO of InnoVint, a local startup managing wineries with Scala - proving  we have the most fun local Scala conference.
,
Special thanks go to Cloudera, who crystallized the Big Data Scala conference, and whose Chairman and Chief Strategy Officer, Mike Olson, will keynote it together with Martin Odersky, Matei Zaharia, Jay Kreps, and Debora Donato.  Cloudera is backing Scala and Spark across the two key themes of Big Data Scala - end-to-end data pipelines in Scala and Data Science on the JVM.  Big Data Scala includes several topics which are the focus of the SF Text, Text By the Bay, SF Spark and Friends communities (
,, 
,, 
,, respectively).
,
We had a record number of submissions this year and had to make tough choices to keep the conferences to two tracks each.  This is the biggest and the best Scala By the Bay conferences we've put together so far.
,
Given the program is finally published, we're pushing back late bird to June 15th.  We have about 400 seats capacity and a significant portion was already claimed even before the schedule was announced, mostly by folks returning from the previous years.  All the previous By the Bay conferences sold out, and conferences will sell out quickly this time, so reserve your seat soon.  We're welcoming sponsors, old and new, who get a block of seats as well -- 
email , for prospectus.  All 18 sponsors of the 2014 edition were hiring.
,
We have special programs for non-profits/making the world better kinds of projects, email , if you use Scala for Good and want to attend SBTB+BDS, and get community support for your projects and teams (even teams of one!).
,
See you on the shores of Lake Merritt in August!  "
"
Most popular 
, tweets for Apr 27 - May 03 were
,
Attack of the #BigData Startups , maps the industries where #BigData is Big 
, 
,
,
,
#DataScience from Scratch: First Principles with #Python - O'Reilly book 
, 
,
,
,
US Chief Data Scientist , on #DataScience #BigData past&future: training products ethics 
, 
,
,
,
#DataScience from Scratch: First Principles with #Python - O'Reilly book 
, 
,
,
,
  "
"
,
,
,
,
,
,
,
,  "
"
,
,
Cities across the United States are capitalizing on big data. Predictive policing is becoming a prominent tool for public safety in many cities. In Boston, an algorithm helps determine ?€?problem properties?€? where the city can target interventions. In Chicago, they are protecting citizens by predicting which landlords are not complying with city ordinances. In New York, the Fire Department sends inspectors to the highest risk buildings so they can prevent deadly fires from breaking out.
,
DrivenData is launching our ,, ""Keeping it Fresh,"" to help cities capitalize on their data.
,
,
,
According to the Centers for Disease Control, more than 48 million Americans per year become sick from food, and an estimated 75% of the outbreaks came from food prepared by caterers, delis, and restaurants. In most cities, health inspections are generally random, which can increase time spent on spot checks at clean restaurants that have been following the rules closely ?€? and missed opportunities to improve health and hygiene at places with more pressing food safety issues.
,
The goal for this competition is to use data from social media to narrow the search for health code violations in Boston. Competitors will have access to historical hygiene violation records from the City of Boston ?€? a leader in , ?€? and ,. The challenge: Figure out the words, phrases, ratings, and patterns that predict violations, and help public health inspectors do their job better.
,
Winning algorithms will be awarded $5,000 in prizes: the first-place winner will receive $3,000, and two runners-up will receive $1,000 each. But the real prize is the opportunity to help the City of Boston, which is excited to explore ways to integrate the winning algorithm into its day-to-day inspection operations.
,
Early work on this problem has already indicated that consumers and citizens are leaving clues in their online restaurant reviews. A model from a , tried to predict health scores in San Francisco restaurants. With a simple bag-of-words model, they were able to pick out trends in hygeine scores over time.
,
,
,
,
A , had success picking out patterns that mattered as well. Using a linear classifier, researchers were able to identify severe offenders with 82% accuracy. Their model relied on features built from unigrams, bigrams, and Yelp star-ratings to achieve this result.
,
,
,
To make things a little more interesting, we got things started on this new set of data for the City of Boston. Using bigram features--that is, pairs of words that appear together--we determined which were correlated with more violations. Bigrams that indicated more violations included:
,
don ask, make reservations, fried oysters, corn cob, don think ll, hours later, bit slow, food mediocre and hour wait.
,
And, bigrams that indicated fewer violations included:
,
liked place, lot fun, nice bar, cocktail list, glasses wine, order drinks, gourmet dumpling, heard good, hang friends, great things, like atmosphere and good food good.
,
There may be some clues in there, but we're betting you can do better. The competition will accept submissions for eight weeks. Submissions will be evaluated on fresh hygiene inspection results during the six weeks following the competition; after that, the prizes will be awarded. Your submission will not only put you in the running for the prize ?€? it has the chance to transform how city governments ensure public health.
,
What are you waiting for? ,!
,
Bio: , is a co-founder of DrivenData, and recently earned MS in Computational Science and Engineering from Harvard. 
,
,
,
  "
"
,.
,
,
,
In this video I show how Data Science is ushering in a new class of adaptive software. 
,
This video defines adaptive software, shows how data science realizes these applications, and discusses how these new tools are addressing real world challenges across all industries.
,
, 
,
,
Bio: ,, Ph.D. is a Senior Data Scientist at ThoughtWorks.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
The , Scientist is said to be the sexiest job of the 21st century and it is likely to be very well paid. Salaries are in the high $ 100k per year and there is a strong demand for the best Data Scientists. A lot of the 20th century jobs on the other hand will disappear thanks to robotics, artificial intelligence and machine learning. How should you ensure that you can also obtain one of those sexy jobs, how can you become a data scientist and get hired?
,
Well, it all starts with obtaining the right skills, surprisingly. The challenge however with becoming a data scientist is that you need a long list of skills to get hired. Some time ago, I already published a typical , and other infographics show that it is a very long road to become a Big Data Scientist. But with an expected shortage worldwide of 
, Big Data Scientists by 2020 in the UK and , in the USA, it is definitely worth pursuing.
,
,
,
There are a multiple different tools and techniques that a Data Scientist should master. Of course I could give an overview of the different technologies that are available within the market and that the Data Scientist should be familiar with. However, there are so many different programming languages, machine-learning techniques, visualization tools and data mining technologies that it would result in long and complicated list of required skills. I rather focus on broader overview and direct you to this outstanding ,.
,
A Data Scientist should be able to , in millions of data points, streaming from different sources, and be able to infer insights from those patterns that can be used in decision-making. The Data Scientist should be able to find triggers that can be used to optimize those insights, be it within sensor data at a factory or detect customer behavioural triggers for retailers. Based on the requirements, the Data Scientist should be able to select the best tool and technique to get the best results. So instead of knowing a plethora of different techniques, the Big Data Scientist should be able to choose wisely among those techniques to obtain an optimized result.
,
,
,
Choosing the right technology of course has to do with understanding the business and more importantly understanding the right ,. Before a Data Scientist should dive into the data, he or she should clearly understand the context and thoroughly master the problem at hand. The best way to do that is in collaboration with the business partners and ask clarifying questions to completely understand what needs to be done. A Data Scientist should there also have some sort of understanding of how a business is run, which of course depends on the size of the company as well as the industry the company is in.
,
Different industries require different solutions and different solutions require a different approach. In order to get hired in a specific industry, you should therefore have a feeling for the industry, as that will enable you to better understand the context. A better understanding of the context will result in better insights.
,
,
,
Especially ambitious, new Data Scientists should have experience in working with different data sources and solving a wide range of problems. Although there is a shortage in Big Data Scientists, it still is important to showcase the right experience if you want to get hired. Of course, this may sound like an open door, but the more different projects you have done, the more skills you master and the better you can understand different businesses. Websites like Kaggle or the Experfy, developed by the Harvard Innovation Lab, can give you the right experience to land that dream job within your favorite company.
,
,
,
Hiring the right Big Data Scientist is also a challenge for organizations. As a bonus here are , for organizations to find the right Data Scientist:,
,
,
,
, is Founder of ,, the One-Stop Shop for Big Data, creating the Big Data ecosystem by connecting all stakeholders within the global Big Data market. He is an entrepreneur, a well thought after international public speaker and a Big Data strategist. He is author of the best-selling book ,. 
He is , among global top Big Data influencers.
,
,
,
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,.
,
Being able to mine the complete World Wide Web is a longstanding dream of many data miners. Until recently, only people working for large companies such as Google and Microsoft were able to realize this dream, as only these companies possessed large Web crawls. This has changed in 2012 with the nonprofit , starting to crawl the Web and making large Web corpora accessible to anyone on Amazon S3. 
,
Still, a corpus of several billion HTML pages is not exactly the right input for many mining algorithms and a fair amount of pre-processing and large data management expertise is required to extract the relevant pieces of data from the corpus. This is where the 
, project jumps in and supports data miners by extracting structured data from the Common Crawl and providing this data for public download. Beside one of the largest - if not the largest - publicly available hyperlink graph, the project extracts product-, address-, recipe-, and review data from HTML pages that use the Microdata, Microformats, and RDFa markup standards. 
,
With 287 million product records originating from over 100,000 different websites, 48 million addresses from 130,000 websites, as well as 42 million customer reviews and 22 million job postings, each data corpus is amongst the largest that are available to the public in its category. In the following, we describe the WebDataCommons data corpora in more detail:
,
,
We have extracted a hyperlink graph covering 3.5 billion web pages and 128 billion hyperlinks from the 2012 version of the Common Crawl (,). 
,
, 
, (click on the image to see a very large 2000x2000 px version).
,
In addition to the page level graph, we also provide host-level and TLD-level aggregations of the graph for download. The graphs can be used to analyze the overall structure of the Web and reveal new insights, such as the falsification of the long-standing belief that the in-degree distribution of hyperlinks follows a power-law (,). On the other hand, the graph can be used to evaluate the performance of graph databases and graph mining tools.
,
,
We have extracted data from HTML pages that use one of the markup languages RDFa, Microdata, and Microformats (,). So far, we offer data from the years 2010 to 2014. Altogether, the extracted data describes 13 billion products, local businesses, addresses, persons, organizations, recipes, and job postings, to name just a view. More detailed insights can be found in our ,. The data can, for example, be used to analyze price differences in e-shops on global scale, or the skills that are in high demand by employers in different regions.
,
,
We have also extracted a corpus of 147 million HTML tables that contain structured data describing countries, cities, artists, movies, songs, products, as well as other types of entities (,). This table corpus can be useful input for generating large-scale knowledge bases, such as the Google Knowledge Graph, or for improving NLP methods.
,
,
As we are strong believers in the potential of the Common Crawl, we also , that we used to generate the datasets. In addition to the data that we provide, any interested party can adapt the framework to their needs and extract the data form the Common Crawl that is required for its task at hand.
,
, is Ph.D. candidate at the Data and Web Science Group of the University of Mannheim. Before joining the research group in 2012, he was working for a Web personalization startup. His research focuses on Web data profiling where he is specialized in the analysis of entities in web pages annotated with markup languages such as RDFa, Microformats, and Microdata. He is one of the key persons behind the WebDataCommons project, mainly responsible for the structured data and hyperlink graph datasets and their analysis.
,
, is part of the Data and Web Science Group at the University of Mannheim. He explores technical and empirical questions concerning the development of global, decentralized information environments. Christian Bizer has initialized the W3C Linking Open Data effort which is interlinking large numbers of data sources on the Web and has co-founded the DBpedia project which derives a comprehensive knowledge base from Wikipedia.
,
,
,
,  "
"
New entries for April.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
,
,
, is Chief Data Scientist at , where his main responsibility is the development and refinement of the company's proprietary Velocity technology, which predicts and tracks the viral life-cycle of digital media content. 
,
Prior to joining Mashable, Haile led all research efforts for SocialFlow, one of the leading social media optimization platforms for brands and publishers. Haile specializes in statistical learning as applied to predictive analytics and has a background in theoretical physics, including a Ph.D from Rutgers University, a Masters of Science from King's College, University of London and a B.A. from Yale University. 
,
,
,
Here is second and last part of my interview with him:
,
,
,
,Though I think it?€?s acknowledged that predicting broad scale social behavior is very difficult, our task is particularly difficult because we are confined to very bare data. Most studies of social diffusion operate with more granular information than we have. Understanding how the flu spreads or how email chains propagate in a known network is difficult even when you have intimate knowledge of how the people involved interact or, maybe, broader topological features of that network. Because we and the social networks we study very much respect the privacy of their users, we?€?re largely blind to such network topology. This makes a useful prediction that much harder. 

,
,
,

,It wasn't an explicit piece of advice, but a question: what is your risk profile? At the time I was job hunting, not long out of academia and it was asked of me by a senior data scientist. ,The question was extremely clarifying because it crystallized for me how I want to spend my time and leverage my expertise and in what sort of environment. As it happens I prefer working on unsolved problems and with colleagues who can iterate aggressively towards a solution. Velocity has been a tremendous joy to work on and Mashable, along with my exceptional colleagues, very much provide that environment. 
,

,

,
,I am particularly fascinated by the cascade of results coming from computer vision. Why? ...well I think I have the same romantic fascination most people do with the idea that a computer can ?€?see?€? an image and ?€?know?€? what it contains. Given how intimately bound human vision is to how we understand our environment, a computer?€?s ability to mimic this is extremely ,compelling. More concretely though, the most compelling advances in computer vision essentially generate descriptive, human readable sentences out of images. Given this linguistic analogy, working on Velocity we imagine that the extent to which an image subject matches the interests of user?€?s social network affects its shareability. Beyond that though, there?€?s a much harder to characterize component of imagery that operates visually, much like the prosodic elements of language operate linguistically. Tack a compelling beat onto a series of rhyming couplets and you can turn incoherent words into lyrics people will share for a generation. I wonder what the visual analogue of that is and how best to bring that to bear on the content we see shared in Velocity.
,

,
,It?€?s hard to generalize because I think an effective data science team working on interesting unsolved problems has to be quite interdisciplinary, with representation from my own discipline of physics, engineering, mathematics, biology, the social sciences. The only unifying attribute I absolutely need these representatives to have is demonstrably indefatigable curiosity.
,

,
,
,Not working? What is this concept you speak of?€??
,
I enjoy interactive theater, getting better at agile street photography, the occasional voice over gig. And booze. 
,
,
,  "
"
,
,
,
,
,
,
,
,
,
  "
"
        You can watch keynotes from Strata + Hadoop World 2015, London, starting May 6 - see below.
,
,, 
,5-7 May 2015
,
,
,
,
,
,
**Pending speaker consent** keynotes, tutorials, and breakout
sessions, including sponsored presentations will all be recorded.
,
Training courses will NOT be recorded.
,
Onsite interviews with sponsors, authors, and other interesting folks
will also be recorded.
,
,
,
The keynote presentations will be posted to the public S+HW playlist
on the 
,; links to the video will be posted on
the Home page as soon as footage starts to become available is
available.
,
,
,Keynote presos will start to go up within 24 hours, barring any
technical glitches. Footage will also loop on the website players
after that first streamed block ends on Wednesday.
,
,
Also, 
, was won by
Nagesh Sarvepalli.
,
Here are important trends in Analytics, Big Data, Data Science in 2015,
as given by Raffle participants.
,
,??,
,
,
,  "
"
,
,
Program time: June 1 - Aug 22, 2015
,Application Deadline: 
,
,
Students' work: ,
,
Testimonial: ,
,
We offer job placement assistance! We work with hiring partners directly and fulfill their Data Scientist hiring need. Learn how you can bring your data scientist career goal to life. Meet our instructors, alumni, and staff. Learn what it means to be a Data Science student, and check out our alumni's final projects. Get introductions to our upcoming course offerings or have a 1-on-1 chat with our admissions rep to find the right course for you. Learn about the NYC Open Data Group, NYC Data Science Academy and the benefits of becoming a member of the school and meetup!
,
Open House Meetup Events, Meet us in person! 
,
,
,
,
,
,Date: May 20 and 21, 9am to 5pm  
,
,
, Founder and CTO of SupStat Inc, Adjunct Prof at NYU and Stony Brook Univ. Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. Prior to taking entrepreneurial steps, she worked as a Senior Financial Analyst at Memorial Sloan-Kettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received Double Master Degrees in Computer Science and Statistics.
,
This class is designed to provide a comprehensive introduction to R. We'll get you programming and analyzing data with R in no time. Participants will receive a copy of all slides, exercises, data sets, and R scripts used in the course. We will also emphasize how you can get work done easily with the RStudio IDE.
,
,
,
,
Date: Saturday, May 16, 30, Jun 6, 13, 27
,(breaks on Memorial Weekend on May 23, Father's Day on June 20)
,Time: 10am to 5pm
,
,
is an adjunct faculty at NYU.  Paul has worked on projects in fMRI brain imaging studies, the analysis of international dispute data, experimental psychology, micro-simulation methods in urban planning and urban economics applications, and the epidemiology of Chlamydia.
,
This intensive class will introduce you to the wonderful world of R and provide you with an excellent understanding of the language that leaves you with a firm foundation to build upon. From the rudimentary building blocks of programming basics, to data manipulation and use of advanced drawing packages, the course will conclude with a demonstration of a project of your choice on Project Demo Day.
,
,
,
Date: Sunday, May 17, 31, Jun 7, 14, 28 
,(breaks on Memorial Weekend on May 24, Father's Day on June 21)
,Time: 1pm to 5pm
,
,
,
This five-week course is an introduction to data analysis with the Python programming language, and is aimed at beginners. We introduce how to work with different data structure in Python. We will cover the most popular modules, including Numpy, Scipy, Pandas, matplotlib, Seaborn, and ggplot, to do data analytics and visualization. We use iPython notebook to demonstrate the results of codes and change codes interactively during the class. 
,
,
,
Date: Saturday, May 16, 30, Jun 6, 13, 27
,(breaks on Memorial Weekend on May 23, Father's Day on June 20)
,Time: 1pm to 5pm
,
,
,
Each class is 20 hours of classroom guidance with an 
,
three week-long showcase project of students' own choices and 
,
presentation of their projects.
,  "
"
,
In June 2015, Dublin will become the central hub of NoSQL and Big Data community in Europe.
,
After big success last year, NoSQL matters would like to welcome you to our , of great fun and severe tech nerdity in Dublin.
,
,
,
NoSQL matters provides a perfect opportunity to network with leading NoSQL experts from all around the world. You will have a chance to meet and interact with dominant NoSQL companies & discover everything about their latest technologies. Mind blowing talks will bring new ideas and keep you up-to-date on current NoSQL solutions. The conference covers large amount of topics with different difficulty levels and various scenarios.
,
The event will fill you in on NoSQL, Big Data, DBaas, and Cloud Computing developments, zooming in on new products and field reports of day-to-day operations. During interactive talks of our international speakers, you will explore technical backgrounds and algorithmic innovations.
,
Use-cases will cover a great variety of subjects on document and column stores, graph databases, consistency models and many more. But, cheer up! There is plenty of humour, fun and tasty catering in-between of all the geeky stuff!
,
The conference will take place at the beautiful ,.
,
Bright and modern interior of the venue provides perfect conditions to socialize and enjoy networking.
,

Great news - the tickets sale is on! Get your ticket today and for only ???199  at  ,.
,.
,
 

,.
,
To participate email to , by May 10, 2015 
,with the subject: 
, 
and specify why in your opinion NoSQL is important .
, 

Curious about how it all happens? Check out ,.
,
,
Looking forward to meeting you!
,
The NoSQL matters team,
,
,
,
,  "
"
,
,
,
Demand forecasting is one of the most challenging fields of predictive analytics. This is a reality that is industry agnostic ?€? true across finance, health care, and consumer goods and retail. This will not come as a surprise to business decision makers and data scientists working hard to leverage that information.
,
 

While each industry has its own challenges ?€? retail demand may be far more sensitive to economic pressures, for example, while health care is adapting to different delivery models and regulatory requirements ?€? there are common pitfalls. Fortunately, there are common strategies to optimize the benefits of demand forecasting.
,
 

The first challenge is complexity, an issue that is often poorly understood across the entire enterprise. Most businesses have enough logistical, transactional and other data, although in some cases this information is still not as well-developed ?€? or meaningfully communicated across units ?€? with a strategic view toward the necessary forecasting endpoints. In today?€?s data-driven environment, smart business leaders have moved beyond the basics of time-series or other historic rates in the hopes of applying the information toward robust future growth opportunities. But even good data models exist in an evolving environment of consumer behavior shifts, cyclical economic fluctuations and more factors that make it difficult to identify critical trends. Predictive analytics products are available to try to solve for the issue.
,
 ,
,
What these products cannot solve as readily are the internal dynamics that characterize a business ?€? especially where models have reflected a lack of consistency in the data, because the assumptions and drivers of one department or operational unit are not aligned with those of the others. Whether that?€?s a function of the organizational culture and a politically-charged environment, a failure of legitimate but conflicting visions, or poorly communicated agendas and goals, the resulting forecast mismatch can be expensive.
,
Think for a moment about the kinds of questions you ask yourself about forecasting. When you make demand decisions, are they based on the same evaluation drivers that are used in other decision-making units? Do they include the level of complexity that relevant data from those units would offer? The first rule for optimizing your demand forecasting capacities is to be consistent. A uniform approach to what data you assess and what it?€?s for, applied across all categories, creates a foundation for the ?€?one truth?€? solution on which you can reliably and confidently base your future-demand strategies and resources.
,
 

Developing a model that?€?s consistent, comprehensive, clear and driven by the relevant causal factors doesn?€?t just happen once, because the success driven by the high-quality demand forecasting that you seek often requires a more iterative process. But it doesn?€?t need to be intimidating, either. Many businesses seeking to capitalize on the value of ?€?big data?€? continue to refine their understanding of how that?€?s defined ?€? and far more, in how predictive analytics are applied in order to drive growth. It?€?s more important to have leaders across the enterprise understand that the demand-modeling designs are likely to be more fluid, more responsive to actual use and application, and therefore adjust in time.
,
But therein lies the real success story of demand forecasting: an organization that really uses it, and reaps the benefits. Algorithms designed to maximize your profitability and value will not work until you have created the culture that welcomes them. In other words, that requires buy-in at all levels and across all units of your business. Leaders need to prioritize how they are integrating their demand forecasting strategy into the existing framework of the skeptical and budget-wary heads in sales, marketing, transportation, and perhaps especially HR and training ?€? since the real investment in demand models and the objective data they provide is in their acceptance by the people who use them.
,
 

So what are the five main guiding principles to creating a robust demand forecasting solution?
,
,
,, 
a co-founder of ,, leads the firm?€?s Growth Foresight practice. She is a recognized industry expert in developing unique client solutions combining advanced predictive analytics with deep business knowledge. Focused on the CPG and Healthcare industries, Lana has more than 20 years of experience advising clients on a broad range of analytics solutions.
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,.
,
Here are upcoming May - November 2015 meetings and conferences. 
,
For full list see ,  page.
,
,
,
Top meetings countries are:
,
,??,
Top US States:
,
,??,
Top cities (globally) are:
,
,??,
Here are the upcoming meetings.
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,
,??,
,
,
,??,
,
,??,
,
,??,
,
,??,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 5 and later.
,
See full schedule at , .
,
,  "
"
        ,  "
"
,
By Gregory Piatetsky,  
,.
,
,
Data Scientist has been called the sexiest job of the 21st century.  But perhaps the century will last only 25 years.
,
With even knowledge-based jobs like lawyers and accountants being 
,, will Data Scientists prove to be an exception?  
,
What predictive analytics professionals predict about the future of their profession?
,
Latest KDnuggets Poll asked: 
,
,
,
, - see results below. At the same time about a quarter expect this to happen in over 50 years  or never.
,
,
,
The regional breakdown was 
,
,??,
Plotting the results for the top 3 regions, we see only minor regional differences. Asians believe the most in automation, with 60% saying it will happen in 10 years or less and only 17% saying in 50 years or never.
,
,
,
,
What do you think?
,
,
,
,, ""expert-level"" defined by poll
,Since we are unclear on what is meant by expert level, we can use the results of this poll to define it - whatever the winning outcome is, say 5-10 years, we then define ""expert level"" talents as those things which will be automated in 5-10 years.
,
,, expert level analysis
,The answer to the question depends on the definition of ""expert-level analysis"". More and more subtasks have been made available for automatic processing. However, the expert-level is exactly above. Hence, expert-level analysis becomes more and more demanding until it covers all science and philosophy. At this ultimate point, I doubt robots or other machines can operate without any expert-level above. But. I admit, this is then mere guessing.
,
,, Automating Data Science
,It's fair to say that some data science tasks are automated today and some will never be automated. The key words, of course, are ""expert level""
,
Gregory Piatetsky: let's call ""expert-level analysis"" what is done now by PhD-level Data Scientists (some of whom don't have a PhD).  I think the results of the poll still stand.  
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
, is Head of Data Science at lastminute.com, and an expert in the field of data processing, data systems architecture and artificial intelligence.
,
,.
,
(Editor: What are your inconvenient truths? Please comment)
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
Nominations due June 5, 2015
,
ACM SIGKDD invites your nominations for its 2015 Innovation and Service Awards.
,
,, ACM's Special Interest Group on Knowledge Discovery and Data Mining (KDD), is the premier global professional organization for researchers and professionals dedicated to the advancement of the science and practice of knowledge discovery and data mining.  It established the 
,
to recognize outstanding technical and service contributions to the KDD field.  
,
,
,
Innovation Award recognizes one individual or one group of collaborators whose outstanding technical innovations in the field of Knowledge Discovery and Data Mining have had a lasting impact in advancing the theory and practice of the field.  The contributions must have significantly influenced the direction of research and development of the field or transferred to practice in significant and innovative ways and/or enabled the development of commercial systems.
,
The previous SIGKDD Innovation Award winners were Rakesh Agrawal, Jerome Friedman, Heikki Mannila, Jiawei Han, Leo Breiman, Ramakrishnan Srikant, Usama M. Fayyad, Raghu Ramakrishnan, Padhraic Smyth, Christos Faloutsos, J. Ross Quinlan, Vipin Kumar, Jon Kleinberg, and Pedro Domingos.
,
,
,
Service Award recognizes one individual or one group for their outstanding professional services contributions to the field of knowledge discovery and data mining.  Services recognized include significant contributions to the activities of professional KDD societies and conferences, educating students, researchers and practitioners, funding R&D activities, professional volunteer services in disseminating technical information to the field, and contributions to society at large through applications of KDD concepts to improve global medical care, education, disaster/crisis management, environment, etc.
,
The previous SIGKDD Service Award winners were Gregory Piatetsky-Shapiro, Ramasamy Uthurusamy, Usama M. Fayyad, Xindong Wu, the Weka team led by Ian Witten and Eibe Frank, Won Kim, Robert Grossman, Sunita Sarawagi, Osmar R. Zaiane, R. Bharat Rao, Ying Li, Gabor Melli, and Ted Senator.
,
,
,
Nominations should include a 1-2 page summary statement justifying the nomination along with other supporting materials. Each nomination should be co- sponsored by at least 3 people. At most one award will be given each year in each category. All communications will be via email.  Nominations will be valid for a period of 3 years.
,
Please email all nomination and support documents by June 5, 2015
to , ,with subject line 
,
,
The 2015 awards will be presented at the 21st ACM SIGKDD 2015 Conference in Sydney, Australia, August 10-13, 2015, ,   
,
SIGKDD Chair and members of the SIGKDD Awards Committee are not eligible to be nominated for either Award and are excluded from participating in the nomination process as nominators or as supporters of the nominations.
,
,
,Chair, 2015 ACM SIGKDD Awards Committee.  "
"
, lets you run all 1000+ functions in the Blockspring library, right from your spreadsheets.
,
Create interactive data visualizations, run algorithms, pull data sources, execute db queries, automate tweets and emails, make APIs calls, and more. In a nutshell, you get the full power of programming from the comfort of a spreadsheet.
,
Enough talk. Let's show you how to out-run programmers using functions as simple as =SUM().
,
, - Which Chicago hotels are hot spots for... prostitution busts?
,
,
,
,??,??,??,??
, - Summarize recent news.
,
,
,
,??,??,
Also, watch: 
,
,
,
,
Blockspring lets you dramatically scale analytics with limited technical resources. It's a platform that makes distribution and consumption of technology simple within your organization. Here's how it works:
,
,
,??,
,
This model helps IT teams produced more functionality. Simultaneously, it lets business users find and use the tools they need, when they need them.
,
Visit , to get started.  "
"
New entries for March 2015.
,
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
Here is a poll from Prof. 
, at KU Leuven, which asks about Machine Learning APIs - defined as application programming interfaces designed to support the data analytics process from data preparation until visualization. 
,
The poll scope is limited to preferably web-based APIs, which take advantage of the cloud.
,
,
,  "
"
        By Gregory Piatetsky,  
,.
,
The latest KDnuggets Poll was on
,
,.
,
Despite the popularity of Big Data, the majority of data miners and data scientists work with ""PC-size"" data, with PC remaining the most popular platform.  However, the median response for number of CPUs and memory used approximately doubled since the previous such poll in 2010, and Unix and especially Mac gained in usage.
,
The results of the , are based on 282 voters.
,
The Venn diagram below shows the relative popularity of PC/Laptop (85%), Server (30%), and Cloud platforms (24%), and also the overlaps.
,Interestingly, despite the Big Data hype, PC remains the most popular platform for data mining and analytics work, although a significant part is now done in the cloud and on a server.
,
,
,
,
The average data miner in this poll used 1.4 platforms.
,54% used only PC/laptop, 9% used only dept server, and 5% only the cloud.
,
Among those that used cloud computing, 59% of them used private cloud,  
46% used public cloud, and 6% used both.
,
Next poll question was on processing power.
,
,
,Median number of cores is 3-4.
,
Although my note on the poll said ""if you have 8-core processor but your job only uses 1, choose 1"", I suspect most voters ignored this,
since the most common answer is 3-4 cores 
which probably corresponds to the number of cores in the CPU (many popular laptops/PCs now have 4 cores), but I doubt that so many data mining jobs were so well parallelized that they actually used all 4 cores. 
,
Comparing with a similar poll 2010 KDnuggets poll: 
,, 
we see that the median number of cores approximately doubled, to 3-4 in 2015 from 2 in 2010.
,
Next poll question was on memory.  "
"
,.
, 
I assist enterprises by driving data-driven approaches into their operations, developing market-aware products that learn from data, and encouraging data-smart cultures among the c-suite of executives. I have had the privilege to work with many talented professionals looking to disrupt their industries while remaining true to their organization's values. As you might expect, many of these professionals hail from business schools that have prepared them to work with the variety of domain experts that exist within a company.
, 
The expectation on any business graduate is that they possess an ability to strike a middle language between the priorities of a business and the deep domain knowledge of a company's experts. They should carry that 'generalist's touch' and be able to synthesize myriad high-level approaches into real-world utility for their organization.
,  
To produce graduates like this a business school must find ways to teach the general high-level approaches used by domain experts across a company's departments. Graduates should have an understanding of how an expert's deep expertise in their field adds value to the overall strategic direction of the company. Only then can value-producing conversations and disruptive ideation exist between the business graduate and the domain expert.

,

Up until recently this has been straightforward in business schools since the traditional departments have fairly obvious overlaps with business operation. For example, an organization's marketing department contains domain experts in communicating the value of the company's product or service to its customers. Finance has experts in managing the financial functions of the business. Public relations have experts in maintaining the public view of the organization, while IT ensures systems, databases and software are operating as expected.
,
For many years, striking the middle language between these specialist concerns and a generalist's high-level understanding of them has allowed business schools to produce graduates who can help their employer answer to market demands.
, 
But something is changing. The market isn't what it used to be. Virtually every industry is being touched by a new currency, and no business is immune. It's called data, and it is connecting people and process in new and disruptive ways.
, 
Because of this, there is an undercurrent of concern flowing throughout business schools. To remain relevant their graduates must learn to speak to a new professional whose domain expertise is playing a vital role in an organization's ability to compete in today's market. Someone whose background is anything but traditional in the world of business. They are called Data Scientists. Professionals who have spent their careers learning how to convert raw data into real-world value by applying advanced scientific methods to data. As is the case with every other domain expert within a company, a middle language must be developed between the graduates of business schools and these domain experts.

,
  "
"
,  The Frankfurt Big Data Lab and , aim to support the creation of Collider project proposals for , to be submitted to the Center for Entrepreneurship & Technology (CET) at UC Berkeley, and if selected implemented at UC Berkeley within the CET Collider context.
,
,
At the heart of the UC Berkeley Center for Entrepreneurship & Technology is the concept of a Collider, a place where students, researchers, investors, entrepreneurs, and industry leaders can ?€?collide?€? to create unexpected outcomes. In this case, the outcomes include:
,
,
?€?This is our first initiative at the Frankfurt Big Data Lab to foster the case for Big Data for Social Good: our aim is to show that Big Data can be leveraged to better serve the people who generate the data, and ultimately the society in which we live,?€? said Prof. Roberto V. Zicari, Director Frankfurt Big Data Lab.
A Collider Project is specific type of project available to all UC Berkeley students, from undergraduate to Ph.D. and from all disciplines. Ph.D. and Post doc students at UC Berkeley may receive credit for collider projects as part of the Management of Technology Innovation Program (,) while undergraduate students may count this experiential project as towards the CET?€?s , Certificate.
,
?€?We at CET are very happy to be part of this initiative, which fits well with our overall mission and our concept of Collider projects,?€? said Prof. Ikhlaq Sidhu, Chief Scientist and Founding Director of the CET.
,
, 
,
,
,
 ,
,
,
,
,
,
,
,
,

,
 ,


 ,
Contact:
,
Prof. Roberto V. Zicari, Frankfurt Big Data Lab
,
,
,
,
,


  "
"
,

,

,

,

,
,
,

,

,

,

,

,

,

,

,
,
,
It is therefore impossible to write loops in the iterative way (which involve incrementing a counter or repeatedly testing a condition). Instead all loops must be implemented with recursion.
This might seem absurdly restrictive.
Why would we want to give up assignment?
Further, the real world, and the models we are training presumably do have evolving state.
Who are we trying to fool by pretending that they do not?  "
"
,
,
, organized by , was held last week in Santa Clara during March 23-25. It brought together data scientists, professionals handling data / performing data analytics in various different domains and excited to learn the best and latest in Big Data Ecosystem.  The conference tutorials and talks covered a wide variety of topics including Hadoop, Lambda Architecture, MapReduce, Hive, Pig, Spark, MongoDB, etc. 
,
,
,
The first day of the conference started with introduction to Big Data technologies and Hadoop. , from , was the instructor for Hadoop and MapReduce tutorials spread across the day. He started with the significance of Big Data and emphasized that machine generated data has exploded in recent years. He also mentioned that unstructured data is growing exponentially in comparison to structured data and therefore, more opportunities lie in exploring unstructured data. 
,
After giving brief history and timeline of Hadoop, he outlined the top corporations leveraging Hadoop and shared some interesting use cases. Hadoop is now supported by most of the cloud service providers. Big data in cloud is very popular among startups, as they do not need to get involved with infrastructure setup and maintenance.  However, data in cloud has issues such as cost, security and getting data into cloud. 
,
He shared the biggest misconceptions around Hadoop as following: 
,
, ?? ,
Sujee concluded the session talking about the Hadoop Ecosystem explaining various components briefly. He also gave a hands-on session on Hadoop, while answering questions from the audience.
,
, talked about how to survive and thrive with NoSQL in the Enterprise. He mentioned Polyglot Persistence as the key to success.  Polyglot persistence involves use of several different database systems, each being the best choice in its application area.  He shared problems with relational databases and how NoSQL goes about solving those.   "
"
,
Hadoop has come to be an incredibly important technology for many big data projects and applications. But without the proper time or training, it can be difficult to leverage this technology. Hadoop-as-a-service has grown to satisfy the need created by this situation, but there are many options. Below are 18 of the best options.
,
,
,
This post is a summary of the ,
,
,
,
,
  "
"
,
,
, is a Senior Data Scientist at , and instructor of Data Science at General Assembly in San Francisco. He also mentors at SlideRule and has been helping develop the Data Science curriculum at GalvanizeU. At Glassdoor, using big data and machine learning he analyzes user generated content such as salaries and employer reviews. Prior to that, he worked for Path, analyzing terabytes of customer activity logs providing business insights for product development. 
,
Alessandro received his B.A. in Computer Science from UCSC and pursued a Ph.D. in Behavioral and Neural Science at Rutgers until moving back to California in 2010. He taught neuroscience and psychology at USF and CIIS before returning to industry as a Data Scientist in 2011. Since then, he has been working on bringing his knowledge of biological computation to the field of data science all while training the next generation of Data Scientists.
,
, (April 6, 2014): Alessandro is no longer working at Glassdoor. Currently, he is Lead Professor of Data Science for GalvanizeU's Master of Engineering in Big Data.
,
Here is my interview with him:
,
,
,
,: Glassdoor helps people find jobs and companies they'll love. We do this by giving job seekers access to every open job listing and pairing that with reviews, salaries, benefits & interviews information so they can not only find a job that matches their skills but also pays well and is a great cultural fit for them. We are global - we have users and content in 190 countries and are actively expanding
,
Analytics is very core to what we do. In fact, Glassdoor's primary product offering is data in the form of reviews and salaries. We use analytics to support internal business decisions, like A/B testing, strategy & pricing, etc.; to drive data products - such as Reviews Highlights or Salary Medians and for Employer Insights - such as how their ratings vary by job function and office locations. 
 ,
,
,
,It varies a lot. Our projects range all over from detecting fraudulent reviews, detecting predictors of salaries, supporting A/B tests, to reporting on the health of the company. There really is no ""typical problem"" which is part of what makes being a data scientist at Glassdoor so interesting!
  ,
,
,
,It's a lot easier to come up with ways in which to improve a model than it is to actually execute on those improvements which means that, unchecked, a project like that can grow out of control. It's important to check in, set expectations that this will be continuous and gradual process, congratulate yourself for the progress you've made and ship early.
 ,
,
,
,So many factors, really. Besides compensation, employees care most about Career Opportunities, Work Life Balance, Commute, Company Culture & Benefits Package. In fact, one of the Data Science projects I worked on had to do with extracting emotional information from employer reviews. I can say that I saw a lot of nuance in how people described their work environment which had nothing to do with salary. One interesting example of that was how often I came across the word ""love"" in reviews for a company that had a very low rating on our site. This seemed paradoxical until I looked at what the employees were actually saying. It was a service organization and they all said that they loved the people they were serving, but the organizational structure was just intolerable. I doubt increasing salary would have helped much in a case like that.
 ,
,
,
,Working with natural language is very challenging to begin with. Twitter presents an added challenge because of how ephemeral it is. It's a moving target. As an example, in the course I teach at General Assembly, I have my students sample the Twitter feed and then produce some statistics on it. Even though all the students are sampling the same feed at roughly the same time, the results are often hugely divergent. Trying to make inferences from the sample to the population with Twitter data is particularly dangerous. 
 ,
,
,
,I see many data science curricula putting a lot of focus on machine learning. Machine learning is certainly important, but it is far from the end-all-be-all of data science. Big data is, of course, a big topic (and growing!) I think anyone calling themselves a data scientist needs to have at least a passing familiarity with big data techniques. That said, big data can also suffer from too much hype. Sometimes small data is all you need to get the job done. 
,
Other skills I also see neglected a lot are SQL and statistics. SQL is not used much in academia, so a lot of academics-turned-data scientists can find themselves disoriented with tasks that their colleagues would find elementary. But relational databases are here to stay and being able to use them effectively is definitely a required skill for anyone who would call themselves a data scientist. Stats are also important, and it goes beyond simply knowing how to run a chi-square test. A data scientist needs to cultivate an intuition about probability and statistics so they know when not to believe it when their computer tells them something is significant below the p ,
,
,
,
,  "
"
        ,  "
"
,
,
,
 A recent report released by database management experts paints a muddy picture for privacy?€?s future in the midst of a labyrinth of laws. With the recent upsurge of data breaches that hit the acme with the Sony Hack in 2014, the industry is set to witness intensified efforts to guarantee consumer privacy.
,
However, at what cost? This presents the question that DBA experts from Pew Research and Elon University in an aptly titled survey ?€?Imagining the Internet?€? sought to answer. The main question the more than 2,500 DBA experts surveyed sought to address was that of applicability of emerging privacy laws by both the federal and national governments.
,
The hypothesis posed to all participants was the development of a structure that would be widely accepted and trusted in the whole industry. The surprising data findings split the experts down the middle with 45% including those from 
, averring that it is indeed possible to set such infrastructure.
,
However, 55% of these experts felt that developing such a system that would be both popular and functional was next to impossible in the near future. This survey sought to move beyond the usual constraints posed by multiple-choose questionnaires and sought explanations into the Yes or No answers.
,
,
In a post-research interview published on ,, the problem was the fact that regulators are at disconnect with the perception of those they seek to protect. Most respondents noted that the public has accepted willingly that personal information is a form of currency that can help them obtain invaluable services.
,
While the governments and business organizations are intensifying monitoring and tracking of those who interest them, it is a concern today for civil liberties activists. However, Hal Varian, the Chief Economist at Google official expects these will not be issues that will matter to data users by 2025.
,
According to Varian, the denizens of 2025 will appreciate the role of monitoring and tracking due to the safety benefits. Moreover, it will make their shopping life more convenient as they will be able to connect quickly with service providers who already know what they love.
,
,
,According to Jerry Michalski a founder of REX, those people who hitherto had respect due to their parsimony on sharing personal information will seem weird if not non-persons. Such sharing of experiences will be a form of earning trust, as others will consider you human.
,
,
,
If the contemporary data user thinks the current situation of affairs is unacceptable as regards privacy regulations, there is more to come according to the survey. Ugly politics blended with corporate greed and more consciousness on privacy will cause a rigmarole that will leave the consumers at the middle.
,
Though every analyst has a differing view on the coming age of privacy, there is no denying that lobbyists who are responsible for pushing regulation will make the situation worse. With time, it will become a class issue with consumers who have the money having the ability to secure their data better.
,
, is a freelance content writer. She has written many articles on technology, the internet, software, database, hosting etc. To know more about Jenny's contribution, please visit ,.
,
,
,  "
"
,
,
We've put together an amazing program for the inaugural 
, conference:
,
,
,
We have about 40 talks from about 40 great organizations.
,
The topics include label quality, deep learning for NLP, genomic sequencing, question answering, topic detection, sentiment analysis, 
,
and more.
,
The companies represented by speakers include
,
Universities: 
,
,??,
Big text data powerhouses:
,
,??,
Growing global players:
,
,??,
Startups:
,
,??,
Text/NLP tech providers:
,
,??,
Edtech:
,
,??,
Nonprofits:
,
,??,
Genomics:
,
,??,
The keynotes are by ,, Penn TreeBank and 
,, Fountain.com.
,
There's a data science Q&A with 
, and ,, and a business of text CEO panel in the works.
,
There's a hackathon and unconference planned for the evenings as well.
,
As a hallmark of all By the Bay conferences, great coffee will flow nonstop 7:30am-10pm.
,
Please use the discount code 
,
for $250 off, expiring 4/10.  Our capacity is limited to 300, and all previous By the Bay conferences sold out.
,
Hope to see you at Text By the Bay in April!  "
"
        ,
,
,.
,
We continue our
,.  Last month we examined growth from groups ""Big Bang"" in 2008 to present and in this part we look at activity - comments,  discussions, and engagement.
,
Our key findings
,
,??,
Next chart shows comments and discussions per group, for each 12 months period, 
from Q2 2009 to Q1 2015. 
,
,
,
,
We note that while the total number of discussions is growing, the total number of comments actually started to decline in 2013, despite the growth in membership.  The big gap between the average and the median values shows the wide range in activity levels between the groups.
,
We can see the trends more clearly if we measure the comments and discussions per week and per 1000 members.
,
,
,
,
Note that LinkedIn group statistics only give discussion counts starting around June 2010, while comment counts are available starting from Sep 2008. 
,
The discussion numbers per member were growing and peaked in 2012, with the launch of 2nd cluster of Big Data groups in 2012 (some of them were very active), but both discussions and activity levels are declining after 2012.
,
An important factor is group ,. 20 of the top 35 groups are open, 
and , groups have over twice as many comments (median 0.59)
and discussions (median 1.61) as closed groups.
,
,
,
,
Here are the 10 groups with the most comments per 1000 members per week in 12 months from 2014 Q2 to 2015 Q1.
,
,
,
,??,
Graphing comments levels for all groups shows an overall decline in levels of comments.
,
And here are the 10 groups with the highest discussions levels per 1000 members per week in 12 months from 2014 Q2 to 2015 Q1.
,
,
,
,??,
Next graph shows the wild nature of discussions for different groups,
going as high as 20/week per 1k members in 2012 for BDAHN (Big Data, Analytics, Hadoop, NoSQL & Cloud Computing) group, but declining for most groups. See all group abbreviations in the table at the end of this post.
,
,
,
,
We can measure group member engagement as the number of comments per week divided by the number of discussions per week (note that per 1000 member factor cancels out).
,
,
,
,
Overall, the average engagement across all groups has declined two-fold from 2.16 in Q2 2010 to 1.03 in Q1 2015, and median engagement declined six-fold from 1.33 to 0.21.  This suggest a number of very active groups which stand out.
,
Here are the 10 groups with the highest engagement levels in in 12 months from 2014 Q2 to 2015 Q1. We note however that groups with high engagement levels mostly have low discussion levels
,
,
,
,??,
Next chart shows shows group comments/week per 1,000 members vs group discussions/week per 1,000 members. Group name abbreviations are in the table below.  Since the distributions of comments and discussions are very asymmetrical, we use log scale axes.
,
The median along each axis create 4 quadrants: Active, Commenting, Posting, and Passive.
,
,
,
,For better separation of groups, both axes are on logarithmic scale
,
The median lines on each dimension create 4 quadrants: 
,
,??,
Here are the groups in each quadrant, in order of 
,
,
,
,
,??,
,, bottom right - discussions below median but comments above median
,
,??,
,
,
,??,
,
,
,??,
The details are in the table with below.  This table orders the groups
by activity.  Since the distribution of comments and discussions is very uneven, we computed separate ""comment"" rank and ""discussion"" rank for each group, and order groups by , - their average rank. 
Eg KDnuggets is n. 1 in discussions/1k members and n. 3 in comments/1k members so its average rank is 2.  In case of ties, we choose the group with highest sum of comments and discussions. 
,A group is open, unless it has a lock icon 
,
next to its name.
,
,.
,Values 25% or more higher than median are in 
,, 
,25% or more lower than median are in 
,, and the rest are in black.
,
,
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,.
,
Here are upcoming April - October 2015 meetings and conferences. 
,
For full list see ,  page.
,
,
,
From 100+ meetings (counting all Big Data Week locations separately), top countries are (note that US has about 50% share)
,
,??,
Top States in US are:
,
,??,
Top cities (globally) are:
,
,??,
Here are the upcoming meetings.
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,  "
"
,
,
, organized by , was held last week in Santa Clara during March 23-25. It brought together data scientists, professionals handling data / performing data analytics in various different domains and excited to learn the best and latest in Big Data Ecosystem.  The conference tutorials and talks covered a wide variety of topics including Hadoop, Lambda Architecture, MapReduce, Hive, Pig, Spark, MongoDB, etc. 
,
,
,
,
,
Two parallel workshops were held for the first session of the day. , from , and , from , gave workshop on Data Analysis and Practical Predictive Analytics using R respectively. Daniel taught data management with HDFS and analysis of data with Hadoop using Pig, Hive and Impala. In parallel track, Avkash taught basics of R and how using R one can perform large-scale machine learning using open source library H2O. Participants in both the workshops performed hand-on exercises to practice the newly learned concepts.

,
, delivered an interesting talk titled ?€?Data Science vs. The Bad Guys ?€? Using data to defend LinkedIn against fraud and abuse?€?. He said that not everyone follows rules and regulations by the world?€?s largest professional network. People/bots try to spam messages, fake companies, fake jobs, introduce malicious URLs, scrape data, etc. There are a number of ways they try to perform these kinds of actions. As an example, if lots of fake accounts are from one IP then they either block the IP, limit signup rate from any IP using heuristic rules and train model on historical data incorporating signups/IP/hour, number of good / bad accounts on IP, etc. LinkedIn stops them using a separate DB termed ?€?Abuse DB?€? which performs scoring for each request. Based on the scoring, user is allowed or restricted to perform that action. 
,
He shared three different and very fascinating case studies involving registration, fake accounts and account takeover. In order to know if the user registering is real or fake they have asset reputation systems, which assign a reputation score to each asset based on the level of abuse seen in the past. Reputation scoring is performed instantaneously as well as offline. For each registrant machine learning model combines reputation features (offline and online) to produce a registration score. Fake accounts are detected by estimating the probability [Member Reputation] that a given member is not real. 
He also shared how they go about estimating the likelihood of an attack using asset reputation, member history, site history and member reputation. He concluded the talk mentioning that it best to stop bad guys at the entry points and being careful about not bothering good members is also very critical. 
,

, gave a talk explaining why Locbit chose MogoDB and Redis over Hadoop to start. He started with explaining what all problems IoT (Internet of Things) can solve for an enterprise and described how Locbit platform helps them analyze their data across platforms. 
,
He mentioned the following reasons for preferring MongoDB & Redis over Hadoop:
,

, ??,  "
"
,
,
,
,
, from ,, by Gordon Linoff and Michael Berry, and learn how to create derived variables, which allow the statistical modeling process to incorporate human insights. As much art as science, selecting variables for modeling is ""one of the most creative parts of the data mining process,"" according to the authors.
,
The chapter begins with a story about modeling customer attrition in the cell phone industry, moves to a review of several classic variable combinations, and then offers guidelines for the creation of derived variables.
,
,  "
"
Most popular 
, tweets for Mar 30 - Apr 1 were
,
,  "
"
,
By Gregory Piatetsky,  
,.
,
Following the popularity of our analysis of 
,,
we look at the recently released 
The Forrester Wave???: Big Data Predictive Analytics Solutions, Q2 2015, written by Mike Gualtieri and Rowan Curran.
,
This report is available to paid clients from ,
or you can download a copy from several vendors, including 
,
and 
,.
,
The 2015 Forrester Report examines 
13 big data predictive analytics solutions providers: 
Alpine Data Labs, Alteryx, Angoss Software, Dell, FICO, IBM, KNIME.com, Microsoft, Oracle, Predixion Software, RapidMiner, SAP, and SAS,
and gives detailed research and analysis of their current market offerings.
,
,
,
,
In 2015 the leaders are:
,
,
The Strong Performers (listed in Forrester order) are 
RapidMiner, Alteryx, Oracle, FICO, Dell, Angoss, Alpine Data Labs, and KNIME. 
Forrester says that all of them have a sweet spot that makes them good choices.
Interestingly, the report says that 
,
,
Contenders are Microsoft (with Azure Machine Learning and Revolution Analytics) and Predixion Software.
,
Next we compare with a previous report:
,, 
which evaluated only 10 firms. 
It seems there was no 2014 Forrester Wave on this topic.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is a Senior Data Scientist at , and instructor of Data Science at General Assembly in San Francisco. He also mentors at SlideRule and has been helping develop the Data Science curriculum at GalvanizeU. At Glassdoor, using big data and machine learning he analyzes user generated content such as salaries and employer reviews. Prior to that, he worked for Path, analyzing terabytes of customer activity logs providing business insights for product development. 
,
Alessandro received his B.A. in Computer Science from UCSC and pursued a Ph.D. in Behavioral and Neural Science at Rutgers until moving back to California in 2010. He taught neuroscience and psychology at USF and CIIS before returning to industry as a Data Scientist in 2011. Since then, he has been working on bringing his knowledge of biological computation to the field of data science all while training the next generation of Data Scientists.
,
, (April 6, 2014): Alessandro is no longer working at Glassdoor. Currently, he is Lead Professor of Data Science for GalvanizeU's Master of Engineering in Big Data.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: I'm hoping that the quantified self movement and the proliferation of devices like fit bit will engender a democratization of data science skills, that is, that everyone will become an amateur data scientist the same way that the point-and-shoot camera made everyone into an amateur photographer. Much of a data scientist's job these days involves educating people on how to properly interpret data. I'm hoping that this will make the general public more savvy consumers of data. This will help professional data scientists as well as more people come to understand what data science has to offer.
,
,
,
,Money. I hate to say it, but it's true. It helps that data scientists are very well compensated these days, but actually, the motivation came more from just how ,poorly compensated our university instructors are. It's a real travesty. More and more, universities are offloading their courses to adjuncts who are paid shockingly little, especially when you consider how expensive college is these days. Everyone knows that academia is in a bad state right now and more and more people are leaving it to work in a field where they will be more respected and better compensated. I hope that the academy can right itself because I believe that a liberal college education is very important for our democracy. But I simply could not go on scraping by in that environment. 
,
That said, I would not work for just any company. It was very important that I work for a company I felt was doing good. And that's why I work at Glassdoor. Work is such an important part of anyone's life. Helping people find a job they love (or at least, that they don't hate) is pretty important work, if you ask me.
 ,
,
,
,I enjoy building new things. I suppose part of me never stopped being an engineer. But part of the pleasure of doing engineering in data science is that there's more room to play. I also enjoy the problem solving aspect of thinking through how an analysis could go wrong. You need to be creative with that. It's not hard to get enough data to drive your p-values down to zero if you don't know what you're doing. I find debugging those issues much more interesting than debugging code, because they are really logic problems. 
,
At the same time, it can get tiresome after a while. Take the salary estimates problem. It is aggravating for the same reason it's exciting. It is a problem that will never be fully solved. And that can be frustrating. Sometimes I'm jealous of the engineers who can just build something and be done with it. A lot of data science projects could go on forever if you let them.  "
"
,
,
, organized by , was held last week in Santa Clara during March 23-25. It brought together data scientists, professionals handling data / performing data analytics in various different domains and excited to learn the best and latest in Big Data Ecosystem.  The conference tutorials and talks covered a wide variety of topics including Hadoop, Lambda Architecture, MapReduce, Hive, Pig, Spark, MongoDB, etc. 
,
,
,
,
,
,
,
, kicked off the day three of conference talking about data security in hadoop. He mentioned the top three reasons for securing hadoop as:
,
, ?? ,
He briefly explained the five pillars of hadoop data security framework as following:
,
, ?? ,
Data management involves data classification & prioritization, data discovery and data tagging. Regarding identity & access management, he recommended to use Kerberos to validate nodes and client applications before admission into the cluster. Apache Sentry is a great platform to leverage role based access control. 
,
Regarding data protection at rest, he asked to use file/OS level encryption to protect against privileged users or apps with direct access to files. He also insisted use of a central key management server to maintain the crypto keys and also separate keys for data. For data protection in transit, he suggested the use of TLS protocol to authenticate and ensure privacy of communications between nodes, name servers and applications. 
, ?? ,
, gave a hands-on training session on Spark Core, Spark Streaming, Scala, Data Frames, etc. He asked the audience to start with learning important ideas hidden behind the technology. He gave a quick introduction about Hadoop under the hood and talked about important concepts of data serialization, file formats, etc. 
,
He introduced Spark as an in-memory cluster computing system for processing and analyzing large datasets. Spark APIs are available for use in these languages: Scala, Java, Python. Spark is not only faster than Hadoop MapReduce but it is also more expressive as it is not limited to map and reduce operations. Spark is faster than MapReduce as it involves in-memory computation and advanced directed acyclic graph (DAG) execution engine (optimizes stages and minimizes shuffles).   "
"
,
,
,??
,
,
,??
,
,  "
"
,
,
, is the Vice President of Audience Development and Analytics at ,.  In this role, she oversees the company?€?s digital marketing and business development efforts as well as all research and analytics.  She joined the Post in 2003, holding a wide range of roles at the company during her tenure.  Prior to joining the Washington Post, she was a management consultant for the Monitor Group in Boston and Johannesburg.  
,
She completed her undergraduate work at Georgetown University and has an MBA as well as a Master?€?s Degree in Public Policy from Harvard University.
,
Here is my interview with her:
,
,
,
,: This is a very exciting time to be at The Washington Post. Fast Company recently named us the most innovative media company in 2015, in large part because we launched a new tablet app and continue to grow while investing in technology. 
,
,
 ,
,The Washington Post really has seen incredible growth as of late:  in January 2015 we had an audience of 48M people in the US (Comscore UVs), a 70% increase in audience in just one year.  We?€?ve done so because the newsroom is focused on creating even more interesting, engaging content for readers nationwide while maintaining the journalistic standards that have driven The Washington Post for the past 125 years.  
,
These expansions include new blogs covering lifestyle, education, technology, health, and first person narratives as well as expanded coverage of politics, sports, national security, and world affairs, among others.  As a result of this increase in content, we have also seen significant growth in traffic from social sources.
,
,
,
,Alexis Madrigal from The Atlantic coined the term in 2012 to describe social traffic that was not trackable using traditional methods.  We at The Washington Post specifically define dark social as traffic that looks like direct traffic (inbound visits with no referring source) but that we believe to be social traffic.  While we know that many of our users have either bookmarked our site or will type in ?€?www.washingtonpost.com?€? or even ?€?www.washingtonpost.com/politics?€?, it seems highly unlikely anyone will type in ?€?http://www.washingtonpost.com/politics/republicans-split-on-dhs-funding-edging-closer-to-partial-hutdown/2015/02/23/4b2de138-bb76-11e4-b274-e5209a3bc9a9_story.html?€?, even though that?€?s what the analytics tell us. 
 ,
Dark social is important because if you can?€?t understand who your users are and where they?€?re coming from, it?€?s difficult to create content and user experiences for your audience.  Dark social is also making up an increasing amount of everyone?€?s traffic as there has been a tremendous rise in popularity of social mobile apps such as Facebook, Snapchat, Whatsapp, etc; mobile apps such as these are particularly challenging when it comes to tracking audience analytics. 
,
,
,
,The first thing to do is to identify what is truly dark social.  For The Washington Post, we assume that all direct traffic to our homepage and section fronts is truly direct traffic ?€? people really have bookmarked our URL or typed it in - and not dark social.  For visits starting on an article page or blog page (the ?€?http://www.washingtonpost.com/politics/republicans-split-on-dhs-funding-edging-closer-to-partial-hutdown/2015/02/23/4b2de138-bb76-11e4-b274-e5209a3bc9a9_story.htmlH?€? example above), we assume those visitors have arrived by clicking on a link.  We have implemented solutions to identify users coming from our newsletters and from some mobile apps.  What?€?s left we call dark social.  "
"
,

,

,No matter how many books you read on technology, some knowledge comes only from experience. This is even truer in the field of Big Data. Despite a good number of resources available online (including ,) for large datasets, many aspirants and practitioners (primarily, the newcomers) are rarely aware of the limitless options when it comes to trying their Data Science skills on real-life large datasets. Thus, we are consistently on the lookout for greater and better datasets available for public use.

,

In our next endeavor on this journey, we are sharing here an awesome list of public data sources by ,(bio given at the end) that are collected and organized from blogs, answers, and user responses. Most of the data sets listed below are free, however, some are not.
,

,
,
,
,??,

,
,
,

,??,
,
,
,
,??,

,
,
,
,??,

,
,
,
,??,  "
"
,
,A WCAI Research Opportunity Sponsored by 
,
,April 24, 2015
,Noon-1pm Eastern Time
,
,
,
The Wharton Customer Analytics Initiative is thrilled to announce a new and comprehensive dataset sponsored by Bazaarvoice, one of America's largest social marketing companies specializing in user-generated product reviews. The data are collected from a UK-based big box retailer, and contain all website visits, product page views, reviews read, and purchases made by tens of thousands of customers, giving researchers an unprecedented opportunity to look at how customers shop for products in several popular categories.
,
The dataset includes:
,
,??,
Through this Research Opportunity, Bazaarvoice hopes to gain insight into the effect reviews have on a customer's online shopping and purchase behavior. Research questions might include:
,
,??,
To learn more about the data and business context, interested faculty and doctoral students can attend a live webinar on 
,
During the webinar, details of the data will be described and executives from the project sponsor will be available for Q&A. The webinar will also be archived for those who can't attend live.
,
After the webinar, interested researchers can 
,
by Monday, May 4, 2015 to receive access to the data. Proposals will be evaluated based on their potential for academic contribution and the researcher's ability to address issues of strategic importance to the program sponsor. You can register for this 
,  "
"
        ,
,
President Obama recently appointed former Trifacta advisor DJ Patil as the nation?€?s first ,. As part of his initial charter, Patil has called upon all data scientists and analysts to help wrangle and analyze publicly available data sets, such as the ones posted on ,, with the goal of enabling governments, organizations and individuals to use this data to better understand and serve our communities.
,
Given that we recently announced , of Trifacta, it?€?s a great time for analysts around the world to start wrangling the different shapes and sizes of these public data sets for analysis. 
,
In the spirit of Patil?€?s call to action, I went ahead and began working on one of these public data sets using the Trifacta trial to demonstrate some of the powerful transformation features we?€?ve built to make data preparation a much more productive process. So don?€?t be shy and give , a try today!
,
,
For this example, I worked with some of the freely available Bike Share data in my local community. The data is provided by ,(BABS). BABS is the region?€?s largest bike sharing system with 700 bikes and 70 stations across the region, intended to provide Bay Area residents and visitors with an additional transportation option for getting around, while improving air quality for the surrounding areas.
,
,
By examining the most popular bike routes and stations, I would love to gain insights to better help BABS plan its bike inventory and rotation of bikes across San Francisco.
,
,
To get the data ready for analysis, I will load the raw trip data into Trifacta.
,
,
Trifacta breaks the file into rows and columns automatically, and infers header metadata from the raw CSV file. Using type inference and visual histograms, I can get a good overview of the various attributes contained in the bike trips data. 
,
In order to analyze bike share data for just that of San Francisco, I can use the zip code column to help me filter. Unfortunately the data isn?€?t 100% clean, as indicated by the data quality bar for the Zip_code column. Clicking into the red and grey areas of the quality bar allows me to see the extent of the inconsistent data.
,
,
,
After browsing the data available to me on the BABS website, I see there is a separate bike station data file, maybe I can use it to augment and enrich my trip data with data on each station. Upon loading the bike station data into Trifacta, I can see it includes standardized city and zip code information for each one of the bike stations.
,
,
Using a simple attribute lookup transformation on the start_terminal column, I can now cleanse and standardize the zip code as well as enrich my trips data with city and other geo information.
,
,
,
By highlighting ?€?San Francisco?€? in the visual histogram of the augmented data set, Trifacta uses its unique Predictive Interaction??? technology to provide transform suggestions that makes it very easy to filter to just the data that I care about.
,
,
Selecting the top suggestion and adding it to script allows me to filter the trips data to only trips that originated in San Francisco.
,
Now I am interested in seeing which stations and routes are the most popular and the corresponding bike inventory for those popular stations. For this, I can use Trifacta?€?s Visual Data Profiling to help me. By comparing the Start_Station attribute against the End_Station attribute, I can see an interactive heat map that allows me to further explore my data. For example, highlighting the most common starting station ?€?San Francisco Caltrain (Townsend at 4th)?€? shows me that the most common ending station is ?€?Howard at 2nd.?€?
,
,
I can do this more systematically by aggregating via start and stop stations. Trifacta provides a visual aggregator tool to make this easy. 
,
,
Now that I have identified the most popular routes, let?€?s map them to the bike inventory data to see which stations (if any) need additional bike inventory to satisfy the riders. Since the bike inventory data is a rather large file (over 18M rows), Trifacta allows me to intelligently sample a subset of the data based on station IDs.
,
Then, I am able to compute the average bike inventory for each one of the stations located in San Francisco.
,
,
Using Trifacta join key discovery, I am able to quickly join the station inventory data with my prepared trip data ordered by the most popular routes. As shown in the screenshot below, Trifacta recognizes that ?€?Start_Terminal?€? and ?€?station_id?€? have similar content characteristics and recommends that I use those attributes as my join key. 
,
,
,
Now that I have gathered what I am looking for, I can kick off a job that will use the script I just created to transform the entire data set. Upon completion, I am able to export the results of this transformation into any number of visualization or analytics tools such as Tableau. As shown below, the job results page lets me choose my export options and also provides detailed stats over entirety of the results. 
,
,
,
This is just one of many fun and useful transformation exercises that can be done using public data and the FREE Trifacta trial. If you are interested in trying it for yourself, you can sign up at: 
,
,
, is VP of Products at Trifacta, where she combines her passion for technology with experience in Enterprise Software to define and shape Trifacta product offerings. Having founded several startups of her own, Wei believes strongly in innovative technology that solves real world business problems. Most recently, she led product management efforts at Informatica, where she helped launch several new solutions including its Hadoop and data virtualization products.
,
,
,  "
"
,,
co-located with 
,, Nov 3-6, Seattle, WA, USA.
,
With the goal of encouraging innovation in a fun way, ACM SIGSPATIAL is hosting
an algorithm contest with winners to be announced at the ACM SIGSPATIAL GIS
conference in November 2015. Contest participants will submit original computer
programs to be evaluated by the contest organizers on a common dataset.
,
This year contest will be about finding shortest path under polygonal obstacles.
Route planner is one of the most popular web GIS services in use today. Route
planner is one of the key functionality offered by the leading web services like
Google Maps, MapQuest, and ViaMichelin. Shortest path is one of the core
functionality in any route planner. Computing shortest path over a network is a
well-studied problem in the literature and as well as widely used online 
services.
,
Computing shortest paths in real-time has become a necessity with the advent of
online web services. It also became imperative to provide shortest paths under
various constraints. Many online services now support variety of constraints,
including avoiding tolls and boarders to selecting favorite highways.
,
Polygonal avoidance is an important problem in selecting shortest paths due to
many reasons. Many times, part of the network may have been closed for repairs
(static time constraint: during certain period of day/time) or due to emergency
situation (dynamic time constraint). Therefore, computing shortest paths that
obey both static and dynamic time constraints and avoid regions (polygons) of a
network is an important and challenging task. This year SIGPSPATIAL cup seek to
address this very important programming challenge.
,
Top three teams will be provided with cash and or other prizes. In addition to
these prizes, the top three teams will be invited to submit a four page paper
for a contest paper session to be held at the 2015 ACM SIGSPATIAL GIS
conference. These papers will be subject to review and acceptance by the contest
organizers, but it is expected that each of the top three teams will have their
paper in the conference proceedings, a ten-minute presentation in the contest
session, and a poster presentation in the conference's regular poster session.
At least one team member of each winning team must register for the ACM
SIGSPATIAL GIS conference.
,
More information about the contest is here:
,
,
,
Submissions due Aug 10, 2015.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
New entries for February.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
One of the biggest challenges at big conferences such as Strata + Hadoop World 2015 is that there is so much happening quickly and simultaneously that it is almost impossible to catch all the action. 
,
We help you by summarizing the key insights from some of the best and most popular sessions at the conference. These concise, takeaway-oriented summaries are designed for both people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference. 
,
,
,
, shared his vision of the Modern Information Architecture. He mentioned that the growth of Big Data technology should primarily focus on Flexibility, Scalability and Balancing the economics of how we are storing data. He added that Hadoop isn't just Hadoop anymore. Hadoop stack is constantly evolving and growing.
,
, shared a very unique view of real-time database for transactions and analytics. He announced MemSQL and Spark connection to enable real-time big data analytics. 
,
, talked about how wearables are contributing to Big Data and how the resulting insights are already delivering significant gains in key industries such as Health, Fashion/Retail and Sensory enhancements. 
,
, emphasized that it is essential to take a data centric approach to infrastructure to provide flexible, real-time data access, collapsing data silos and automating data-to-action for immediate operational benefits.
,
,  talked about partnership of IBM and Twitter combining  advances in analytics, cloud and cognitive computing in a manner that has the potential to transform how institutions understand customers, markets and trends.
,
, talked about the importance of Big Data and Data Science, and introduced , as the first ever Chief Data Scientist and Deputy Chief Technology Officer for Data Policy. DJ talked about mission and responsibilities that lies ahead for Data Science professionals. 
,
DJ outlined the top priorities as: 
,
, ?? ,
He mentioned that Data Science is a team sport and asked each and every data scientist to join him to make a difference.
,
, discussed how data and statistical inference are informing how we manage the global climate rationally, a defining policy challenge for our generation.
,
, talked about how understanding sensory interactions and being able to define them perceptually and algorithmically allows technological developments that can facilitate sensory enhancement and optimization. 
,
,
,
, and , from , talked about technical challenges in making data profiling agile in the Big Data era. They shared some research results and practices used by analysts in the field, including methods for sampling, summarizing and sketching data, and the pros and cons of using these various approaches for different profiling needs in a Big Data context. They discussed the considerations for using Hadoop technologies for data profiling, and some of the pitfalls in the contexts of both massive Internet services, and end-user profiling tools. They emphasized on ""ABP: Always Be Profiling"" by injecting lightweight ""sidecar"" jobs automatically and exposing outputs visually for effortless human inspection. They also briefly discussed some methods to make wise tradeoffs on latency and accuracy: approximation, heuristics and reasonable assumptions and understanding the performance of underlying system.
,
, talked about design patterns. He defined design pattern as a general reusable solution to a commonly occurring problem within a given context in software design. Design patterns are required because streaming use cases have distinct characteristics (such as unpredictable incoming data patterns) and high scale continuous streams pose challenges of peaks and valleys. He categorized streaming patterns as the following: 
,
,??,
,He discussed external lookup, responsive shuffling and out-of-sequence events under Data Management Patterns. He described external lookup as referencing frequently changing external system data for event enrichments, filters or validations by minimizing the event processing latencies and system bottlenecks, while maintaining high throughput. External lookup has various benefits such as: only required data is cached, each bolt caches a partition of reference data, etc. On the other hand, the challenges with using external lookups are: increased latency due to frequent external system calls, scalability & performance issues with large data reference sets, etc. 
,
He described responsive shuffling as automatically adjusted shuffling for better performance and throughput during peaks and varying data skews in streams. With responsive shuffling, it is challenging if stream is unpredictable and skewed, etc. However, we benefit using it as topology responds to changes in data patterns and adopts accordingly. He also discussed good and bad aspects of out-of-sequence events.
,
, and , from , shared approaches to manage data location, schema knowledge and evolution, fine-grained business rules based access control, and audit and compliance needs as they are becoming critical with increasing scale of operations. They explained how to register existing HDFS files, provide broader but controlled access to data through a data discovery tool with schema browse and search functionality, and leverage existing Hadoop ecosystem components like Pig, Hive, HBase and Oozie to seamlessly share data across applications. 
,
They presented growth of HDFS in last 10 years within a nice infographic and shared that at present they run 42,300 servers and store about 600 PB of data. They briefly explained processing and analyzing with Hadoop. They shared distribution of Hadoop Jobs on their platform as per January. Top 3 among these were: Pig - 47%, Oozie Launcher - 35% and Java MapReduce - 7%. 
,
Managing data on multi-tenant platforms poses various challenges such as: 
,
,??,
They talked about Apache HCatalog which is a table and storage management layer for Hadoop that enables users with different data processing tools ?€? Apache Pig, Apache MapReduce, and Apache Hive ?€? to more easily read and write data on the grid. HCatalog?€?s table abstraction presents users with a relational view of data in the Hadoop Distributed File System (HDFS) and ensures that users need not worry about where or in what format their data is stored.
, ??,
, and , presented some real world use cases of how people who understand new approaches using Hadoop and NoSQL in production can employ them well in conjunction with traditional approaches and existing applications.  
,
,
,
,
,  "
"
,
,
, is Chief Applications Architect at , and committer and PMC member of the Apache Mahout, Apache ZooKeeper, and Apache Drill projects and mentor for Apache Storm, DataFu, Flink and Optiq projects. Ted was the chief architect behind the MusicMatch (now Yahoo Music) and Veoh recommendation systems. He built fraud detection systems for ID Analytics (LifeLock) and he has 24 patents issued to date and a dozen pending. Ted has a PhD in computing science from the University of Sheffield. When he?€?s not doing data science, he plays guitar and mandolin. He also bought the beer at the first Hadoop user group meeting.
,
Here is my interview with him:
,
,
,
, : I think that one of the biggest things that has happened in 2014 is that people have started viewing big data technologies, especially those often referred to as the Hadoop eco-system with a critical and pragmatic eye.
,
This practical focus is really good for the field since it gets people to focus on what really can be done to add value using big data techniques.  There has always been substantial potential value, but until people started taking the field seriously that value was largely unrecognized.  
,
,
,
,I think that there are several important ways to describe different requirements and I really don't like that some people have been trying to co-opt words like real-time to mean other things.  
,
, , There are distinct differences in the technologies required as the guaranteed processing time moves from micro-seconds to milli-seconds to seconds and even to minutes.  Much of the recent maturation in these systems is in the large-scale systems that process data in the 1 millisecond to 5 seconds range.
,
,Interactive query systems are not real-time systems.  The term interactive is an excellent one to use for these systems in that they respond to human requests within human tolerable limits.  How quickly they incorporate incoming data is unspecified which generally makes interactive systems not real-time.
,
Streaming systems are like real-time systems without the guarantees.  The idea is that incoming data is processed as it arrives, but without strict guarantees.  Streaming is an important option to have since streaming systems have the option to fall back ,to batch processing if they fall behind.  Also, with streaming systems, you don't have to over-provision in order to meet stringent guarantees.  This allows a flexibility of resource allocation that is often lacking from true real-time systems.  Most of the Hadoop eco-system components are actually streaming systems rather than real-time, although a few like MapR do stand out from the crowd in that you can build true real-time systems with them.
,
Batch systems collect inputs over a period of time and process them together, often much more efficiently than they could be processed one at a time.  Surprisingly, there has been recent progress in batch systems as well as real-time, interactive and streaming systems.  Spark has brought micro-batching into the mix, even for some streaming applications and other systems like Tez, Flink and Drill (all from Apache) have provided real advances in the batch processing models available in the Hadoop eco-system.
,
Each of these kinds of systems are well defined and the definitions should not be muddied by careless usage.
,
,
,
,I think that streaming systems are going to be hugely important over the next few years.  This is a huge change from the ad hoc workflow scheduling that was required with pure batch systems.
 ,
,
,
,My days are very full and quite varied.  I don't know that there is a typical day.  Things that I do include:
,
,- public speaking.  This involves lots of travel and lots of preparation of talks.  I try to always find interesting topics that are interesting to people with varied backgrounds and useful to hard-core implementors as well as others.
,
- code and algorithm development.  I stay hands-on because otherwise, I would lose touch with what is important.  My hands-on work includes machine learning, both applied and theoretical, applied math, and systems design and implementation.  Some recent projects have included log-synth for building realistic emulated data and t-digest which is a new way to approximate quantiles very efficiently and accurately.
,
,
,
,We partner with literally hundreds and hundreds of companies.  Some of the large ones like Cisco stand out because of how much difference a partner like that can make, but we also have a number of large (and quiet) OEM partners who are doing amazing things with our software as a foundation.  Some of our smaller partners like Skytree are very exciting because they are pushing the limits of technology.
,
,
,
,
,
,For recommendation engines specifically, I think that people take the research literature a bit too literally.  That literature has been unfortunately heavily distorted by the data that is available to researchers and by the original approaches of ratings prediction.  As such, it is pretty unrealistic and fails to account for important aspects of industrial use of recommenders.  
,
Topics such as scalability, easy deployment, multi-modal inputs, result dithering, anti-flood and cross-pollination are essentially untouched in the research literature and yet they have a far larger impact on result quality than ratings prediction.  I really do think that the indicator based approaches such as what we covered in ""the pony book"" (https://www.mapr.com/practical-machine-learning) are the way to go for most users of recommendation systems.
,
,
,
,
,  "
"
Most popular 
, tweets for Feb 26 - Mar 1 were
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Mar 3 and later.
,
See full schedule at , .
,
,  "
"
,
,
The number of Chief Data Officers serving in large organizations today has doubled in the last two years. Despite early speculation around whether this role was necessary, it has become apparent that the CDO is vital in bridging the gap between the C-suite and the data team. 
,
The , is taking place in San Jose on April 28-29 and will bring together top data leaders to discuss the growing responsibilities of the CDO. 
,
KDnuggets members are entitled to an exclude 20% discount off all two-day passes with the code 
,.
,
Check out the schedule here: ,
,
Confirmed speakers include: 
,
,??,
,
With presentations & case studies from today's leading CDOs, you'll return to the office with the inspiration to innovate your data strategy. Wondering what to expect? Check out a presentation from the Chief Data Officer from the City of Los Angeles, 
,.
,
If you would like to attend please contact Meera Raikundalia at , (+1 415 800 4713). 
,Alternatively you can secure a pass here: 
,.
,Don't forget to quote code 
,  to secure your discounted pass.
,
See you in San Jose.
,
Innovation Enterprise  "
"
,
By Gregory Piatetsky,  
,.
,
One of the longest KDnuggets traditions is our annual Data Mining (aka Analytics, aka Data Science) software poll.
,
,
Here is my post with poll results and analysis:
,
,
,
The state of analytics industry constantly changes.
We started with the companies in 
,
and 
,,
and added companies/tools from last year poll, added relevant new ones, while trying to keep the total number still manageable ( under 100).
,
This is the 16th such KDnuggets Poll, and it usually attracts a lot of attention and 
sometimes controversy due to excessive voting by some vendors.
It is fine to ask your users to vote, but it is not OK to set up bot voting or direct link to vote for just one tool.
To reduce excessive and bot voting, this poll asks for email verification.
,
Whatever other irregular voting we find, will be removed from final results, and we will publish the anonymized voter logs after the poll, so that anyone can analyze them.
,
We hope our analysis of this poll, trends, and results will useful, as they were in the past.
,
,
Here is our analysis of 
,.
,
This year, we removed the division between commercial and free tools, since many tools have both a free and paid version and it is difficult to have separate votes for those.
,
Note that there are 4 categories in this poll: Platforms/Suite, Languages, Deep Learning Tools (new), and Hadoop-based tools.
,
,
,
,
 ,  "
"
        ,
By Gregory Piatetsky,  
,, Mar 3, 2015.
,
Previous KDnuggets Poll asked: 
,
,
,
2015 poll got much higher participation than in 2014 and reached a wider population including more junior analytics professionals, which probably explains slightly lower salaries for US data scientists this year.
,
,
From 388 respondents this year, 58.8% were from US/Canada, 26.8% from Europe, 7.5% from Asia, 1.8% from AU/NZ, 3.4% from Latin America, and 1.8% from Africa/Middle East. 
,
Note: We estimated the salary using a mid-point for each range, $20K for under $30K bin, and $250K for over $180K bin.
,
,
,here bar height corresponds to number of people, bar width to salary.
, 
,
,
,bar height corresponds to number of people, bar width to salary.
,
,
A regional breakdown shows that Data Science Managers in the US/Canada earn average salary around $177K (11% higher than $165K in 2014).
US/Canada Data Scientists earn on average $122K (9% lower than $135K in 2014, probably because more people entered the market).
US/Canada Data Analysts earn on average $86K (11% higher than $76K in 2014).
,
,
,
,
Employer type also plays an important role.  Even excluding students, people in the industry earned $108K, about 40% more than $$77.6 for academic/government employees. 
,
Here is a more detailed breakdown by Role and Employer type (students excluded).
,
,
,
Finally, the chart below shows a breakdown by Region, Role, and Employment.
East European and Asian salaries are lagging behind the US, Canadian, Australian, and W. European salaries for similar positions.
,
,
,
Here are more details for 
,
,
and the results of past polls:
,
,??,
,
,
 ,  "
"
,
, 
,
,
Attempts to abstract and study machine learning are within some given framework or mathematical model.  It turns out that all of these models are significantly flawed for the purpose of studying machine learning.  I've created a table (below) outlining the major flaws in some common models of machine learning.
,
The point here is not simply ""woe unto us"".  There are several implications which seem important.
,
,
,

,
,
You specify a prior probability distribution over data-makers, , then use Bayes law to find a posterior ,.  True Bayesians integrate over the posterior to make predictions while many simply use the world with largest posterior directly.
,
,
Handles the small data limit.  Very flexible.  Interpolates to engineering.
,
,
,
,
,
,
Sometimes Bayesian and sometimes not.  Data-makers are typically assumed to be IID samples of fixed or varying length data.  Data-makers are represented graphically with conditional independencies encoded in the graph.  For some graphs, fast algorithms for making (or approximately making) predictions exist.
,
,
Relative to pure Bayesian systems, this approach is sometimes computationally tractable.  More importantly, the graph language is natural, which aids prior elicitation.
,
,
,
,

,
,
Specify a loss function related to the world-imposed loss fucntion which is convex on some parametric predictive system.  Optimize the parametric predictive system to find the global optima.
,
,
Mathematically clean solutions where computational tractability is partly taken into account.  Relatively automatable.
,
,
,
,

,
,
Specify an architecture with free parameters and use gradient descent with respect to data to tune the parameters.
,
,
Relatively computationally tractable due to (a) modularity of gradient descent (b) directly optimizing the quantity you want to predict.
,
,
,
,
,
,
,
,
You chose a kernel , between datapoints that satisfies certain conditions, and then use it as a measure of similarity when learning.
,
,
People often find the specification of a similarity function between objects a natural way to incorporate prior information for machine learning problems.  Algorithms (like SVMs) for training are reasonably practical - , for instance.
,
,
Specification of the kernel is not easy for some applications (this is another example of prior elicitation).  , is not efficient enough when there is much data.
,

,
,
You create a learning algorithm that may be imperfect but which has some predictive edge, then apply it repeatedly in various ways to make a final predictor.
,
,
A focus on getting something that works quickly is natural. This approach is relatively automated and (hence) easy to apply for beginners.
,
,
The boosting framework tells you nothing about how to build that initial algorithm. The weak learning assumption becomes violated at some point in the iterative process.
,

,
,
You make many base predictors and then a master algorithm automatically switches between the use of these predictors so as to minimize regret.
,
,
This is an effective automated method to extract performance from a pool of predictors.
,
,
Computational intractability can be a problem. This approach lives and dies on the effectiveness of the experts, but it provides little or no guidance in their construction.
,

,
,
You solve complex machine learning problems by reducing them to well-studied base problems in a robust manner.
,
,
The reductions approach can yield highly automated learning algorithms.
,
,
The existence of an algorithm satisfying reduction guarantees is not sufficient to guarantee success. Reductions tell you little or nothing about the design of the base learning algorithm.
,

,
,
You assume that samples are drawn IID from an unknown distribution D. You think of learning as finding a near-best hypothesis amongst a given set of hypotheses in a computationally tractable manner.
,
,
The focus on computation is pretty right-headed, because we are ultimately limited by what we can compute.
,
,
There are not many substantial positive results, particularly when D is noisy. Data isn?€?t IID in practice anyways.
,
,
,
,
You assume that samples are drawn IID from an unknown distribution D. You think of learning as figuring out the number of samples required to distinguish a near-best hypothesis from a set of hypotheses.
,
,
There are substantially more positive results than for PAC Learning, and there are a few examples of practical algorithms directly motivated by this analysis.
,
,
The data is not IID. Ignorance of computational difficulties often results in difficulty of application. More importantly, the bounds are often loose (sometimes to the ,).
,

,
,
Learning is a process of cutting up the input space and assigning predictions to pieces of the space.
,
,
Decision tree algorithms are well automated and can be quite fast.
,
,
There are learning problems which can not be solved by decision trees, but which are solvable. It?€?s common to find that other approaches give you a bit more performance. A theoretical grounding for many choices in these algorithms is lacking.
,

,
,
Learning is about finding a program which correctly predicts the outputs given the inputs.
,
,
Any reasonable problem is learnable with a number of samples related to the description length of the program.
,
,
The theory literally suggests solving halting problems to solve machine learning.
,

,
,
Learning is about finding and acting according to a near optimal policy in an unknown Markov Decision Process.
,
,
We can learn and act with an amount of summed regret related to O(SA) where S is the number of states and A is the number of actions per state
,
,
Has anyone counted the number of states in real world problems? We can?€?t afford to wait that long. Discretizing the states creates a POMDP (see below). In the real world, we often have to deal with a POMDP anyways.
,

,
,
Learning is about finding and acting according to a near optimaly policy in a Partially Observed Markov Decision Process
,
,
In a sense, we?€?ve made no assumptions, so algorithms have wide applicability.
,
,
All known algorithms scale badly with the number of hidden states.
,
This set is incomplete of course, but it forms a starting point for understanding what?€?s out there. (Please fill in the what/pro/con of anything I missed.)
,
,: ,
,
Bio:
, is a machine learning research scientist, a field which he says ""is shifting from an academic discipline to an industrial tool"". He is the author of the weblog hunch.net and the principal developer of Vowpal Wabbit.
,
,
,
,
 ,  "
"
,
,
,
This week on /r/MachineLearning, we have a number of great posts from AMAs to GPUs.
,
,
,
This AMA, which was announced a couple weeks ago, is finally here! As of this writing, the post is still open for questions. J??rgen Schmidhuber will then begin answering questions on March 4,. There have already been some great questions posed, and it will surely be a post to revisit once he?€?s begun answering.
,
,
,
This insightful post explores many common mistakes, most of which this author has himself committed or seen at some point. Definitely a good read, especially if you?€?re newer to data science and machine learning.
,
,
,
This very practical guide details what factors to consider when shopping for GPUs to perform deep learning. Given the recent uptick in interest around the topic, think post is worth a read.
,
,
,
As the title implies, this post details Google?€?s open sourcing of their native framework for MapReduce and Hadoop. If you?€?re working on a project built on MapReduce, this could unlock some new options in the engineering of your data processing pipeline. If that?€?s related to your current work, definitely give this a read.
,
,
,
Here?€?s another Google-related post. This post links to Google?€?s newest paper on deep reinforcement learning. If you?€?re interested in these topics or simply want to keep up with the state-of-the-art, be sure not to miss this paper.
,
,
,
,  "
"
,
,
,
,
,
,I have been doing data analysis for something like 10 years now (gulp!) and teaching data analysis in person for 6+ years. One of the things we do , is to perform a complete data analysis (from raw data to written report) every couple of weeks. Then I grade each assignment for everything from data cleaning to the written report and reproducibility. I've noticed over the course of teaching this class (and classes online) that there are many common elements of data analytic style that I don't often see in textbooks, or when I do, I see them spread across multiple books.
,
I've posted on some of these issues in some open source guides I've posted to Github like:
,

,
,
,

,
But I decided that it might be useful to have a more complete guide to the ""art"" part of data analysis. One goal is to summarize in a succinct way the most common difficulties encountered by practicing data analysts. It may be a useful guide for peer reviewers who could refer to section numbers when evaluating manuscripts, for instructors who have to grade data analyses, as a supplementary text for a data analysis class, or just as a useful reference. It is modeled loosely in format and aim on the , by William Strunk. Just as with the EoS, both the checklist and my book cover a small fraction of the field of data analysis, but my experience is that once these elements are mastered, data analysts benefit most from hands on experience in their own discipline of application, and that many principles may be non-transferable beyond the basics. But just as with writing, new analysts would do better to follow the rules until they know them well enough to violate them.
,
,
,
,
The book includes a basic checklist that may be useful as a guide for beginning data analysts or as a rubric for evaluating data analyses. I'm reproducing it here so you can comment/hate/enjoy on it.
,
 
,
,
This checklist provides a condensed look at the information in this book. It can be used as a guide during the process of a data analysis, as a rubric for grading data analysis projects, or as a way to evaluate the quality of a reported data analysis.
,
,
1. Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data?,
2. Did you define the metric for success before beginning?,
3. Did you understand the context for the question and the scientific or business application?
4. Did you record the experimental design?,
5. Did you consider whether the question could be answered with the available data?,
,
,
,
1. Did you plot univariate and multivariate summaries of the data?,
2. Did you check for outliers?,
3. Did you identify the missing data code?,
,
,
,
1. Is each variable one column?,
2. Is each observation one row?,
3. Do different data types appear in each table?,
4. Did you record the recipe for moving from raw to tidy data?,
5. Did you create a code book?,
6. Did you record all parameters, units, and functions applied to the data?,
,
,
,
1. Did you identify missing values?,
2. Did you make univariate plots (histograms, density plots, boxplots)?,
3. Did you consider correlations between variables (scatterplots)?,
4. Did you check the units of all data points to make sure they are in the right range?,
5. Did you try to identify any errors or miscoding of variables?
6. Did you consider plotting on a log scale?,
7. Would a scatterplot be more informative?,
,
,
,
1. Did you identify what large population you are trying to describe?,
2. Did you clearly identify the quantities of interest in your model?,
3. Did you consider potential confounders?,
4. Did you identify and model potential sources of correlation such as measurements over time or space?,
5. Did you calculate a measure of uncertainty for each estimate on the scientific scale?,
,
,
,
1. Did you identify in advance your error measure?,
2. Did you immediately split your data into training and validation?,
3. Did you use cross validation, resampling, or bootstrapping only on the training data?,
4. Did you create features using only the training data?,
5. Did you estimate parameters only on the training data?,
6. Did you fix all features, parameters, and models before applying to the validation data?,
7. Did you apply only one final model to the validation data and report the error rate?,
,
,
,
1. Did you identify whether your study was randomized?,
2. Did you identify potential reasons that causality may not be appropriate such as confounders, missing data, non-ignorable dropout, or unblinded experiments?,
2. If not, did you avoid using language that would imply cause and effect?,
,
,
,
1. Did you describe the question of interest?,
2. Did you describe the data set, experimental design, and question you are answering?,
3. Did you specify the type of data analytic question you are answering?,
4. Did you specify in clear notation the exact model you are fitting?,
5. Did you explain on the scale of interest what each estimate and measure of uncertainty means?,
6. Did you report a measure of uncertainty for each estimate on the scientific scale?,
,
,
,
1. Does each figure communicate an important piece of information or address a question of interest?,
2. Do all your figures include plain language axis labels?,
3. Is the font size large enough to read?,
4. Does every figure have a detailed caption that explains all axes, legends, and trends in the figure?,
,
,
,
1. Did you lead with a brief, understandable to everyone statement of your problem?,
2. Did you explain the data, measurement technology, and experimental design before you explained your model?,
3. Did you explain the features you will use to model data before you explain the model?,
4. Did you make sure all legends and axes were legible from the back of the room?,
,
,
,
1. Did you avoid doing calculations manually?,
2. Did you create a script that reproduces all your analyses?,
3. Did you save the raw and processed versions of your data?,
4. Did you record all versions of the software you used to process the data?,
5. Did you try to have someone else run your analysis code to confirm they got the same answers?,
,
,
,
1. Did you make your package name ""Googleable"",
2. Did you write unit tests for your functions?,
3. Did you write help files for all functions?,
4. Did you write a vignette?,
5. Did you try to reduce dependencies to actively maintained packages?,
6. Have you eliminated all errors and warnings from R CMD CHECK?
,
Original: ,.
,
,
,
,
,
 ,  "
"
        By Gregory Piatetsky,  
,.
,
, is a rare combination of a Data Scientist, 
a CEO, and an extrovert with a sense of humor.
,
I have enjoyed watching the recent 
, series
,
,, where Ingo, usually helped by his 
friend Unicorn Data Scientist number 7, 
explains the basics of predictive analytics and data mining in a clear but entertaining style.
,
The 14 completed videos can be considered as season 1, and there are plans for a more advanced season 2. The average length of 14 videos is actually 7 minutes, but a couple minutes of each video is just playing with unicorns and other pets,
so you indeed spend only 5 minutes with Ingo.
,
As a data scientist myself, 
I could not resist doing simple statistics on those videos
,
,
,
and the most popular ones are 
, - below
and  
,.
Perhaps data scientists people love ""Understanding ..."" titles or may be those videos had an especially cute visitor? Judge for yourself. 
,
,
,
Enjoy!
,
The Season 1 videos cover these topics
,
,
,
,  "
"
,

,
,
,
,

,

,

,
,
,

,
,

,
,
,

,
,
,
,

,
,
,
,
,

,
,
,

,
,

,
,

,
,


,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,

,  "
"
,
,
,  provides training and advisory services in business analytics and data visualization. He held leadership roles in building and managing data teams at Vimeo, Sirius XM Radio, [X+1], and American Express. He is the creator of the acclaimed ,, which pioneered the genre of critically examining graphics in the media; and author of Numbers Rule Your World and Numbersense, both published by McGraw-Hill. He has an MBA from Harvard Business School, and is an adjunct professor at New York University. His work has appeared in Harvard Business Review, American Scientist, Significance, Financial Times, Wired, FiveThirtyEight, and CNN.
,
Here is my interview with him:
,
,
,
, I like to say ?€?data integrity?€? rather than ?€?correctness?€?. There are multiple levels of integrity. One level is value integrity, which most people recognize. Are there invalid values? Do values get dropped accidentally? Another level is label integrity. For example, there is code to track clicks on the button on the left side of a webpage. If the designer moves the button to the right side, the developer copies and pastes the previous tracking code but once in a while, forgets to edit the tag so the analyst continues to interpret the data as left-button clicks. 
,
And then there is analytical integrity. Say, the traffic to your home page plunged last Monday because the tracking tag was inadvertently removed. The traffic existed, and just wasn?€?t measured, so the analyst extrapolated the missing value. However, no Web analytics software I know of has a solution to fix such mishaps permanently, and so anybody who ever looks at traffic data for any period that includes that Monday must make the adjustment. Needless to say, most analysts won?€?t even know about the anomaly. 
,
,
,
,
, ,The Deflategate controversy during the recent Superbowl provides a nice example. A data analyst made noise over a statistic showing that ,the New England Patriots fumbled far less often than other teams. Story time comes when the analyst slides from the data to the conclusion that the Patriots must have deflated footballs to achieve that statistic, a story on which the data shed no light at all. In the case of Warren Sharp, whose analysis of the fumble statistics went viral, I give him credit for avoiding story time--Sharp told readers he was speculating. 
,
,
,
,As analytics managers, we ought to taste our own dog food. We should quantify the impact of analytical projects. It is crucial to tie the outcomes to corporate metrics so business managers see the value of our work. For instance, if I run an A/B test on a specific page, and the results show a 10-percent increase of sales for daytime visitors, I express that gain in terms of total sales of all visitors, which is what the CEO cares about.
,
,
,
,When I talk about ?€?numbersense,?€? my point is that intuition or gut feeling plays an indispensable role in data analysis. The conventional wisdom is that data and intuition are polar opposites. That is a myth. The best data analysts are the ones who excels at harnessing their intuition to guide the analytical work. In my book, I argue that data analysis is inherently subjective. 
,
,
,
,OCCAM is the acronym for a set of characteristics that are becoming more and more prevalent in new ?€?Big Data?€? datasets and make the analyses of such datasets challenging. In short, many new ,datasets lack any kind of design, do not include explicit controls, are seemingly complete, are adapted from other purposes, and result from a series of imperfect merges. For example, I believe an Achilles heel of using Google search data to predict flu trends is that engineers are constantly tweaking the underlying search engine without concern for those adapting the data to predict flu trends or fulfill any number of other purposes. Please see my blog for more details on OCCAM. (http://junkcharts.typepad.com/numbersruleyourworld/2014/03/toward-a-more-useful-definition-of-big-data.html)
,
,
,
,
,  "
"
,
,
,

,

,
,
,Data Science and Business Analytics have seen tremendous growth in the last few years, and there is fierce competition for the best talent. Numerous academic programs have sprung up, and many workers are making a career transition. The job market, while vibrant, is confusing because the field is fast evolving, and because the field encompasses different job types and career paths. The goal of this seminar is to help students develop a plan for finding a job in Data Science and/or Business Analytics, and building a lasting career in this field.
,
Specific objectives include: understanding the nature of the job, and the state of the job market; determining whether the job is right for you; creating a networking strategy; improving your resume; developing your pitch; making the decision to invest in further education, and building a lasting career.

,

The seminar is divided into four sessions. Interaction is encouraged throughout the day. Students complete hands-on exercises, especially in the third session; and receive handouts on tips and resources.

,

,
,

,
,
,
,??,

,
,??,
,
,??
,
,
,
,???? 
,

,
,??,
,??,
,
,
,Kaiser Fung has built and managed data analytics teams for a variety of businesses, such as Vimeo, Sirius XM Radio, [X+1] and American Express. His career spans multiple industries such as online media, digital marketing, financial services, and telecom, as well as companies at different stages of analytical maturity. In creating new teams, he has made hiring decisions, set up processes and procedures, developed internal and vendor relationships and collaborations, defined strategies, and tracked performance.



,Fung is an adjunct professor of business analytics and data visualization at New York University. He writes the , blog. His latest book is , (McGraw-Hill). He holds an MBA from Harvard Business School, and degrees in statistics and engineering from Cambridge and Princeton Universities.

,

,

For more information and registration, please click ,.  "
"
,
,
, is Chief Applications Architect at , and committer and PMC member of the Apache Mahout, Apache ZooKeeper, and Apache Drill projects and mentor for Apache Storm, DataFu, Flink and Optiq projects. Ted was the chief architect behind the MusicMatch (now Yahoo Music) and Veoh recommendation systems. He built fraud detection systems for ID Analytics (LifeLock) and he has 24 patents issued to date and a dozen pending. Ted has a PhD in computing science from the University of Sheffield. When he?€?s not doing data science, he plays guitar and mandolin. He also bought the beer at the first Hadoop user group meeting.
,
,
,
Here is second and last part of my interview with him:
,
,
,
, : I think that the idea of synthetic indicators is very exciting.  These are tags that can be applied to content and users based on external properties or relations that allow some pretty amazing capabilities.
,

,
,
,Well, some good friends asked me to answer some questions.  From there it was a down-hill slope.  First a few questions to be answered.  Then some code to be reviewed.  Then a few implementations.  Suddenly I was a committer and was strong committed to the project.
,
With respect to Spark and H2O, it is difficult to make direct comparisons.  Mahout was many years ahead of these other systems and thus had to commit early on to much more primitive forms of scalable computing in order to succeed.  That commitment has lately changed and the new generation of Mahout code supports both Spark and H2O as computational back-ends for modern work.
,
That inter-relationship makes direct comparison even harder in some ways.  I think that there is so much to work on in machine learning that it is hard to say that one project is directly competitive with another when, in fact, they actually work together in many ways. 
,
Clearly Mahout has a huge lead over the other systems in the way that it compiles linear algebra expressions into efficient programs for back-ends like Spark (or H2O).  Clearly also, H2O has a huge lead over Spark's MLLib in terms of numerical performance and sophisticated learning algorithms.  Mahout is also the only system that fully supports indicator-based recommendation systems, which is a huge difference as well.
,
 

,
,
,I have been lucky enough to have had a large number of people who have helped me over the years and I don't think that I could distill that help into a single bit of advice.  One inspiration that I have had from a number of mentors over and over again is to maintain a sense of wonder and curiosity about the world.  Many of the things that are most quoted in my work have been things that I learned about while talking to people expert in fields other than my own.  The LLR test came from some astro-physicist friends.  T-digest came from clustering.  Cross recommendations came from symmetry considerations.  

,
 

,
,
,Yes.  The talent-crunch is a real problem.  But finding really good people is always hard.
,
People over-rate specific qualifications.  Some of the best programmers and data scientists I have known did not have specific training as programmers or data scientists.  Jacques Nadeau leads the MapR effort to contribute to Apache Drill, for instance, and he has a degree in philosophy, not computing. One of the better data scientists I know has a degree in literature.  These are widely curious people who are voracious learners.  Combine that with a good sense of mathematical reasoning and a person can go quite far.
,
,Limiting your hiring to people who have a CS degree from a top-10 university and 5-10 years experience in exactly what you want them to do makes it very hard to hire good people and very much limits how much in the way of new ideas they can bring to you.
,
A great example of this same bias happens when people ask questions in interviews to which they already know the answer.  I don't want to hire people who know what I know.  I want to hire people who know what I don't know.  If I learn something important from a candidate during an interview, that is one of the best indications that they are a good hire.  If they learn from me, I don't consider that a great indicator.
 ,

,
,
,I want people who are switched-on, curious about things, willing to try new things and who are willing to tell me when I am wrong (hopefully somewhat gently).  I also want people who get things done and understand the value of simplicity.
 ,
,
,
,That is a really hard question, partly because I love reading.  I read "","" by Erwin Schroedinger and was really fascinated how he could drive to the heart of problems.  Even in the early 40's and even as a physicist rather than a biologist or chemist, he was able to think clearly and succinctly and surprisingly correctly about the mechanisms of life.  Very impressive.  It is also very impressive that he could write so very well even using a second language which, as he put it, can never fit as well as the original.
,
,
,  "
"
By Gregory Piatetsky,  
,, Mar 4, 2015.
,
Here are upcoming March - August 2015 meetings and conferences. 
,
For full list see ,  page.
,
Most common locations are 
"
"
,
,
,??,??
,  "
"
,
,
,
This month on /r/MachineLearning was dominated by AMA announcements, concerns with academic machine learning, and interesting applications of machine learning.
,
,
,
This post explores using Eigenfaces in the context of Tinder, the dating app. More than simply detailing how to use Eigenfaces to automatically generate attractive faces, this post details the methodology for interacting with other humans on Tinder automatically. All of the code is open source, so feel free to peruse it ,.
,
,
,
This post tackles recent issues in unjustified presentations that the author has seen. The author believes many recent papers are unnecessary. It seems that many in the /r/MachineLearning community agree, based on the fact that this is so highly-upvoted. There is some interesting discussion in the comments.
,
,
,
The AMA is nearly here. At the time of this writing, the AMA has begun accepting questions, and they will begin to be answered March 4,. Be sure to check back in once that has begun.
,
,
,
This the announcement for the number three post in this list. Clearly, this is important to the community (as all big-name AMAs tend to be) based on the consistent upvotes.
,
,
,
This post touches on the general strategy used in Shazam?€?s music recognition algorithm. Some commenters have pointed out that this may not be the most up-to-date description of Shazam?€?s algorithm. Regardless, for those interested in learning about music recognition, it is a well-written post that deserves a reading.
,
,
,
,
  "
"
,
,
Cognitive Computing provides detailed guidance toward building a new class of systems that learn from experience and derive insights to unlock the value of big data. This book helps technologists understand cognitive computing's underlying technologies, from knowledge representation techniques and natural language processing algorithms to dynamic learning approaches based on accumulated evidence, rather than reprogramming. Detailed case examples from the financial, healthcare, and manufacturing walk readers step-by-step through the design and testing of cognitive systems, and expert perspectives from organizations such as Cleveland Clinic, Memorial Sloan-Kettering, as well as commercial vendors that are creating solutions. These organizations provide insight into the real-world implementation of cognitive computing systems. The IBM Watson cognitive computing platform is described in a detailed chapter because of its significance in helping to define this emerging market. In addition, the book includes implementations of emerging projects from Qualcomm, Hitachi, Google and Amazon.
,
Today's cognitive computing solutions build on established concepts from artificial intelligence, natural language processing, ontologies, and leverage advances in big data management and analytics. They foreshadow an intelligent infrastructure that enables a new generation of customer and context-aware smart applications in all industries.
,
See more and order at 
,.

,
,
,  "
"
        ,  "
"
,
Here is an interesting list of Predictive Analytics Influencers , based on Twitter activity around ?€?#PredictiveAnalytics?€? and ?€?Predictive Analytics?€?, using Keyhole, FollowerWonk, Klout, and some algorithmic magic to compile the sources. The list contains influencers from a diverse range of backgrounds, including media moguls, serial entrepreneurs and academics
,
, 

,
,
,  "
"
,
,
, is currently the CTO of ,, one of China's top online recruitment websites. Lei joined Microsoft Research Asia in 2005 as a researcher in the area of Natural Language Processing. Later he joined Yahoo!, in charge of Yahoo!'s search product development in Beijing. He has extensive experience in leveraging big data and machine learning techniques in solving many complex problems.
,
Here is my interview with him:
,

,
,
,: , is a leading online recruitment website in China. It helps tens of millions of Chinese find their jobs every year. Job seekers can submit CVs and apply for jobs there for free, while we charge employers for posting jobs and purchasing CVs. As most of our service is based on user generated content (UGC, such as jobs, CVs), analytics on these UGC data is essential and it can be applied to many areas of our service. A key area would be providing ""perfect match"" to our employers and job seekers as this is their primary goal for using our service. 
,
Analytics on the jobs and CVs of successful employment based on their profiles to increase the likelihood of employment can significantly improve the effectiveness and efficiency of our service. Analytics can also help employers and job seekers on our website to refine their job postings and CVs to maximize their chance of employment. In addition, Data Mining can also facilitate lead generation to help us better identify our potential customers in order to increase revenue.
,
,
,
,The ""perfect match"" here between job seekers and employers is quite different from other applications, ,such as web search. In a web search engine, when a user types in a query, the system intends to retrieve documents as much relevant as possible. But in online recruitment, we cannot simply recommend candidates with the highest caliber to employers, because they may over-qualify the job or deny the employer's offer. It's vice versa from job seekers perspective as well. So the ""perfect match"" here should be rather perceived as a mutual match. They should meet each other's criteria and interest, but not over qualify at the same time. 
,
A mathematical model is to be built to estimate the degree of match given a pair of job and CV. The success of matching should be measured by concrete actions such as increase in job applications, CV purchase as well as actual employment through our online service.
 ,
,
,
,Traditional job fairs require job seekers and employers to physically go to a location to share their CV and job information. Its temporal and spatial constraints significantly limit its efficiency and effectiveness. Meanwhile, online recruitment service is available 24x7, and it's not confined by physical boundaries.
,
The convenience of access to information through Internet has given rise to the first revolution in recruitment, bringing it from offline to online. Therefore in the online recruitment website, both employers and job seekers ,can conveniently access a much larger collection of jobs and CVs than in a traditional job fair. And it has led to great success of many online recruitment websites. However, with abundant information available in the online recruitment website, users are faced with information overload. For instance, how can an employer find the most suitable candidate out of tens of millions of CVs is really a challenge. So in addition to submitting jobs and CVs, users' primary goal is to find their jobs or candidates in online recruitment websites.,
 ,
,
,
,Hiring decisions are normally based on many factors such as education level, relevant skill sets, age ,etc. However, given huge volume of data, it is impractical for human to screen every one of them and pick the best match. With data analytics and machine learning techniques, these factors can be captured mathematically as features and the machine learning algorithm can automatically learn how these factors affect the hiring decision according to successful match data as reference. In addition, with unsupervised learning techniques such as topic models, LDA (Latent Dirichelet Allocation), we are able to even discover new factors that are hidden in the big data.
,
,
,
,Major user activities in online recruitment websites are job applications by job seekers and CV purchase by employers, and they are key factors in good user experience. As we mentioned that ""match"" is defined as mutual, and its success is measured by actual interview or hiring action, we shall not simply optimize job application or CV purchase number as the optimization criteria individually. This not only optimizes user experience for job seekers and employers, but also website revenue.
,
,

,
,
,  "
"
,
,
, is currently the CTO of ,, one of China's top online recruitment websites. Lei joined Microsoft Research Asia in 2005 as a researcher in the area of Natural Language Processing. Later he joined Yahoo!, in charge of Yahoo!'s search product development in Beijing. He has extensive experience in leveraging big data and machine learning techniques in solving many complex problems.
,
,
,
Here is second and last part of my interview with him:
,

,
,
,: One challenge is the unstructured data. A large portion of the content in job postings and CVs is described in the form of free text. To understand it or extract key information from unstructured text has long been a real challenge to computer scientists, and it requires advanced natural language processing techniques such as named entity recognition, information extraction, sentiment analysis etc, to handle it. 
,
Another major challenge is data sparseness. We can frequently see CVs or job postings with inadequately filled information.
,
,
,
,In general, anything that characterize the job seekers/employers or distinguish them from others for the purpose of employment matching can be taken as attributes in profiling. ,Empirically, important attributes for job seekers include education level, work experience, location, age, etc.
, 
However, with Big Data and data analytics techniques, attributes can be also automatically generated and selected. To statistically represent a profile, we convert it into a vector of attributes and values mathematically.  We first define a set of attributes that are able to characterize the job or job seeker. Then to represent its value, we can either calculate according to predefined rules or statistical learning models, such as running topic models in job descriptions to capture its topic distribution.
 ,
,
,
,ChinaHR is a top online recruitment brand in China. Founded in 1997, ChinaHR is like at ,the grandfather level in Chinese Internet industry, with a history far longer than its competitors. It has one of the largest job and CV databases in China, probably in the world.
Due to its long history and well received brand, ChinaHR has accumulated a great amount of data including jobs, CVs, user behaviour data such as job applications, search clicks etc, which distinguish itself from competitors in relation to data analytics applications and is by far ahead in this area.
 ,
,
,
,Since I'm now in the online recruitment business, the trend of Big Data in this area is currently of my greatest concern. However, progress in this area is quite slow. One possible reason might be that only very few players in this business really have sufficient volume of data as well as capability and experience of leveraging these data. 
 ,
,
,
,He should be very familiar with machine learning algorithm. He should have a creative mind, because all the work to be done is very new and innovative.  Since the volume of our data to be processed is really huge, he should have hands-on experience in many large scale data processing tools and database systems, such as hadoop, Hive, MongoDB etc.
 ,
,
,
,Personally, I do not read books. But I read lots of papers from technical conferences and journals. Most of the latest development in big data and analytics is published there.  

,
,
,  "
"
By Cheng-Tao Chu (,) .
,
Statistical modeling is a lot like engineering.
,
,
In engineering, there are various ways to build a key-value storage, and each design makes a different set of assumptions about the usage pattern. In statistical modeling, there are various algorithms to build a classifier, and each algorithm makes a different set of assumptions about the data.
,
When dealing with small amounts of data, it?€?s reasonable to try as many algorithms as possible and to pick the best one since the cost of experimentation is low. But as we hit ?€?big data?€?, it pays off to analyze the data upfront and then design the modeling pipeline (pre-processing, modeling, optimization algorithm, evaluation, productionization) accordingly.
,
As pointed out in my previous ,, there are dozens of ways to solve a given modeling problem. Each model assumes something different, and it?€?s not obvious how to navigate and identify which assumptions are reasonable. In industry, most practitioners pick the modeling algorithm they are most familiar with rather than pick the one which best suits the data. In this post, I would like to share some common mistakes (the don't-s). I?€?ll save some of the best practices (the do-s) in a future post.
,
,
,
Many practitioners train and pick the best model using the default loss function (e.g., squared error). In practice, off-the-shelf loss function rarely aligns with the business objective. Take fraud detection as an example. When trying to detect fraudulent transactions, the business objective is to minimize the fraud loss. The off-the-shelf loss function of binary classifiers weighs false positives and false negatives equally. To align with the business objective, the loss function should not only penalize false negatives more than false positives, but also penalize each false negative in proportion to the dollar amount. Also, data sets in fraud detection usually contain highly imbalanced labels. In these cases, bias the loss function in favor of the rare case (e.g., through up/down sampling).
,
,
,
When building a binary classifier, many practitioners immediately jump to logistic regression because it?€?s simple. But, many also forget that logistic regression is a linear model and the non-linear interaction among predictors need to be encoded manually. Returning to fraud detection, high order interaction features like ""billing address = shipping address and transaction amount < $50"" are required for good model performance. So one should prefer non-linear models like SVM with kernel or tree based classifiers that bake in higher-order interaction features.
,
,

Outliers are interesting. Depending on the context, they either deserve special attention or should be completely ignored. Take the example of revenue forecasting. If unusual spikes of revenue are observed, it's probably a good idea to pay extra attention to them and figure out what caused the spike. But if the outliers are due to mechanical error, measurement error or anything else that?€?s not generalizable, it?€?s a good idea to filter out these outliers before feeding the data to the modeling algorithm.
,
,
Some models are more sensitive to outliers than others. For instance, AdaBoost might treat those outliers as ""hard"" cases and put tremendous weights on outliers while decision tree might simply count each outlier as one false classification. If the data set contains a fair amount of outliers, it's important to either use modeling algorithm robust against outliers or filter the outliers out.  "
"
,
,
,  provides training and advisory services in business analytics and data visualization. He held leadership roles in building and managing data teams at Vimeo, Sirius XM Radio, [X+1], and American Express. He is the creator of the acclaimed ,, which pioneered the genre of critically examining graphics in the media; and author of Numbers Rule Your World and Numbersense, both published by McGraw-Hill. He has an MBA from Harvard Business School, and is an adjunct professor at New York University. His work has appeared in Harvard Business Review, American Scientist, Significance, Financial Times, Wired, FiveThirtyEight, and CNN.
,
,
,
Here is second and last part of my interview with him:
,
,
,
, One consequence of the Big Data era is that many more people have access to much more data, and generate a vast amount of analyses. A lot of coverage of Big Data focuses on the production of data and analyses but I?€?m very interested in their consumption. With so many analyses, there is more confusion, less consensus, and less clarity. The average citizen needs to develop numbersense to figure out who and what to believe.
,
,
,
,I just covered the reason for writing Numbersense above. Numbersense concerns a series of analytical claims widely reported in the media in the last few years, and I describe how to probe these claims to decide whether they can be trusted. Numbersense is not something you pick up in a classroom; you learn by checking out how other people do it. Numbers Rule Your World, by contrast, is less topical and more comprehensive. It?€?s the book I?€?d use in a first course of statistics if I created the curriculum. It covers five fundamental concepts of statistics, each illustrated through two real applications, and without any math. Too frequently, I come across students who took Statistics 101, can compute all the formulas but have no idea what they are doing or why.
,
,
,
,The greatest advantage of teaching introductory statistics as liberal arts is expanding its reach. One of the key insights I learned in business school ,is the existence of multiple modes of persuasion. Many students hate their statistics course, and I believe the reason is that the course is designed by, and thus for, logical thinkers.
,
,
,
,
,
,I?€?m excited to see so many schools offering training in Data Science and Analytics. I?€?d advise that they balance the curriculum with courses focused on developing soft skills, such as presentations and managing relationships.
,
,
,
, started almost 10 years ago when I was searching for excuses to pursue my love of writing. Blogging turned out to be perfect, especially after the readership emerged. It feels like climbing on the treadmill, as readers now expect regular updates.
,
,
,
,The first two minutes makes or breaks your meeting.
,
,

,
,I expect that the C-suite will demand accountability from the investment in building data and analytics capabilities. I expect Data Science will become more integrated with the rest of the business.
,
,

,My number one wish for a new hire is numbersense. Imagine the huge dataset as a dense forest. Put people in there and trace their paths through it. Who can find the most interesting things in the least amount of time? Other than numbersense, I value soft skills highly.
,
,
,
,I am reading , by Dave Eggers, which I will finish. When not working or writing, I love to travel and dine out.
,
,
,


  "
"
,.
,
,
J??rgen Schmidhuber, director at the Swiss AI Lab IDSIA, has worked on improving algorithms for deep learning since 1991. He began answering questions on his , on March 4th, and there were certainly some interesting questions and even more interesting answers. Below we look at and discuss the top questions by upvotes.
,
,
,
In response to this question, Dr. Schmidhuber first links the substantial amounts of open code from his lab like , and ,, much of which arose from competitions he and other members of his lab participated in. Beyond this, Dr. Schmidhuber explains that some code ends up involved in industrial projects, making it more difficult to release. In the near future, though, he says there are plans to release even more code for RNNs to act as a successor to PyBrain. That will certainly be exciting!
,
,
,
This question is almost assured to lead to interesting discussion, and in this case, it did. Here are 2 items given as answer.
,
,
,
,
This is definitely a challenging question. He points out that it's often not possible to recognize someone's brilliance immediately because everyone has their own unique strengths. But in general, he sees tenacity and willingness to keep attacking a problem as the one unifying factor for all successful students.
,
,
,
This is something I would be interested in seeing. Dr. Schmidhuber states that he's wanted to for years, but doesn't have the time. Interestingly, this question was also asked of Geoffrey Hinton in ,. If he does find the time, I'm sure it would be well-received!
,
,
,
Dr. Schmidhuber responds to this question by pointing out the compatibility of CNNs and RNNs. He also links to recent research that integrates the two types of networks in particular applications, such as ,. He further addresses the perceived differences in the two communities by pointing out that they are mostly geographical differences, not differences in methodologies or philosophies.
,
,
,
This post covered some of the top questions in the AMA, but it certainly didn't cover them all. If you're interested in seeing the rest of the questions and his responses, you can go see them ,. At the time of this writing, he is also revisiting the AMA occasionally, so you may still have a chance to have a question answered!
,
,
,
,
  "
"
,.
,
,
This week on /r/MachineLearning, we have a number of great posts including deep learning with NLP and visualization of music collections.
,
,
,
Stanford has announced a completely new Deep Learning course focused on NLP. For those outside the university, the course notes and assignments will be made available online. So if you have experience with Python, probability, and machine learning, give this course a shot.
,
,
,
This post is a good first article for those already familiar with machine learning, but interested in seeing how to use the algorithms in practice with Python and Scikit-learn. It goes in depth from data loading to normalization to working with many different models. This is definitely a good first place to go if interested in learning about Scikit-learn.
,
,
,
This interesting post goes into applying machine learning to a music collection in order to compare different types of music in your collection. It then goes into some fascinating visualizations of the music, helping illustrate the technique. This is an interesting article if you?€?re into music or visualization.
,
,
,
This post links to the code for an actual working implementation of the DeepMind Atari-playing algorithm presented at NIPS. Given how popular this paper has been, it?€?s good to see some actual implemented code on the web.
,
,
,
This post, written by Facebook AI researchers, details how to use Torch for NLP with deep learning networks. Beyond just showing how to perform these tasks with Torch, the post goes into the performance of running these algorithms on GPUs, which is an interesting addition.
,
,
,
,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
, from ,, by Gordon Linoff and Michael Berry, and learn how to create derived variables, which allow the statistical modeling process to incorporate human insights. As much art as science, selecting variables for modeling is ""one of the most creative parts of the data mining process,"" according to the authors.
,
The chapter begins with a story about modeling customer attrition in the cell phone industry, moves to a review of several classic variable combinations, and then offers guidelines for the creation of derived variables.
,
,  "
"
Most popular 
, tweets for Mar 02-08 were
,
Processing frameworks for Hadoop and 6 categories in the #Hadoop Ecosystem 
, 
,
,
,
How #PayPal uses #DeepLearning and detective work to fight #fraud 
, #RiskAnalytics #BigData #ML 
,
,
,
How #PayPal uses #DeepLearning and detective work to fight #fraud 
, #RiskAnalytics #BigData #ML 
,
,
How #PayPal uses #DeepLearning and detective work to fight #fraud 
, #RiskAnalytics #BigData #ML 
,
,
,  "
"
,
,
,, Ph.D. is Senior Vice President and Chief Analytics Officer at ,.   Dr. Akmaev leads innovation in Big Data analytics applied to fundamental patient management problems in healthcare, drug development, and diagnostics. During his tenure at Berg, Dr. Akmaev has developed and launched Berg Analytics Suite of data science applications that allow the life scientists and clinicians to harvest the power of Big Data and affect real world patient outcomes. He works closely with the drug and biomarker development teams and directs research informatics, healthcare analytics, and personalized medicine programs within Berg and its subsidiary companies. 
,
Dr. Akmaev continues to innovate in the application of Bayesian artificial intelligence in healthcare IT. Prior to joining Berg, he was Vice President of Scientific Affairs at a Big Data analytics company, Scientific Associate Director at Genzyme Genetics and a Bioinformatics lead at Genzyme. Dr. Akmaev holds a Ph.D. in Applied Mathematics from the University of Colorado at Boulder. 
,
Here is my interview with him:
,
,
,
,: Berg is a widely diversified company in healthcare. We have four business divisions that independently focus on areas such as drug discovery and development, detection of novel biomarkers, development and commercialization of diagnostic tests, data analytical work in healthcare, and predictive algorithms for prevalent human diseases. Collectively, Berg?€?s business divisions worked with Big Data years before many in the industry knew what it was. In fact, our initial research and development of the Berg Interrogative Biology??? platform centered around high volume molecular data. More recently, we transitioned to digital data such as clinical records, patient EMRs, billing data, etc. The ability to operate and derive actionable insight in these data modalities is required for the realization of true precision medicine in clinical practice.
,
,
,
,According to the Institute of Medicine, approximately 30 percent of health expenditures in the U.S. is wasted - costs that are not related to medical procedures or drug pricing. Those expenses are directly linked to malpractice, drug side effects, unnecessary treatment, re-admissions, repeated ,office visits etc. Many of these issues arise because the medical and scientific communities do not have a clear understanding on how a drug or procedure will affect a specific individual. For instance, will it have a different outcome for a 40-year old African American male versus an 85-year old Asian female? Most of today?€?s treatment strategies for chronic and acute disorders are population-based, which may work for 70 percent of patients, and then it becomes a matter of luck whether it works for you, or are you in the selected 70 percent or the unlucky 30 percent. Berg believes that in the 21st century we should be able to do better and provide the right treatment for the right person.
,
,
,
,The genomics revolution has led to tremendous insight into human disease. However, it is only one piece of the puzzle. If we think about clinical practice and medicine, we could probably count on both hands the number of clinically actionable genomics outcomes that we, as a research community, have discovered., ,
,
,
,
,Berg Interrogative Biology??? is a discovery platform. Our philosophy at Berg is to start every development project from the basics in biology, generate quality data, analyze data by complex, artificial intelligence-based mathematical methods and let the analysis guide our triage process. Over the last 8 years, we have been able to look at diseases such as cancer, diabetes, Alzheimer?€?s and Parkinson?€?s and others. 
,
On the diagnostics side, Berg has two programs that are at the late stages of clinical development, a panel of three biomarkers in prostate cancer and a panel of biomarkers in heart failure.  Additionally, we have a number of diagnostic programs in the development phase.
,
,
,
,Berg AI-Analytics suite is an assembly of data analysis workflows implemented in various programming and statistical languages such as C/C++, R, Perl. The idea behind this suite of tools is to provide to the end user the capabilities of performing data ETL, analysis and visualization in one package. We have developed a number of ETL and normalization workflows that cover the extent of the data type diversity we work with on a daily basis. In collaboration with Eric Schadt?€?s team at the Icahn Institute for Genomics and Multiscale Biology at Mount Sinai Hospital in New York, we produced a Bayesian network inference package that was originally developed at IIGMB. That software and our internally developed Bayesian network learner allow us to infer cause-and-effect relationships in large molecular and clinical data sets. More importantly, it allows us to integrate disparate data modalities into a single mathematical model.
,
Finally, one of the most important features of the suite is visualization. We have been using Cytoscape and Cytoscape Web for many years. Our most common visualization templates are standardized and created automatically from the resultant networks. In addition to two dimensional network presentations, more recently we implemented a 3D visualization workflow that gives the expert user the ability to create stunning animations and images. 
,
,
,
,
,  "
"
New entries for January 2015.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
The most popular meetings locations below
,
,??,
Added to , page:
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
By PredictionIO.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
Original:  ,
,
,
,  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
Original: ,
,
Bio: Tom Fawcett is Principal Data Scientist at ,. Co-author of the popular book Data Science for Business, Tom has over 20 years of experience applying machine learning and data mining in practical applications. He is a veteran of companies such as Verizon and HP Labs, and an editor of the Machine Learning Journal.
,
,
,
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
Since its inception four years ago, 
, has continuously evolved its machine learning platform with an emphasis on consumability,  programmability, and scalability.  With a full-featured REST API and an intuitive interface, the BigML platform can be leveraged by  analysts, scientists and developers alike to perform an array of advanced data analyses and/or build predictive applications.  
,
On February 11 BigML will be revealing its 
,, which builds upon BigML's current suite of supervised and unsupervised learning algorithms with new capabilities such as G-Means Clustering, a Sample Service for fast access to in-memory datasets, Google Integration, Project Management and more.  
,
BigML will be showcasing these new capabilities through live webinars at 
, (noon ET, 1700 GMT)
and 
, (8 pm ET, 0100 GMT)  - the latter will be co-hosted with BigML's Australian strategic partner GCS Agile.  
,Please register through the links above.
,
,  "
"
Most popular 
, tweets for Jan 26 - Feb 01 were
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
,
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
Sample #MachineLearning solutions with R on #Azure ML Marketplace #rstats , 
,
,
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
,  "
"
,
,
, is ,'s Chief Technologist, currently focused on new technology introduction and strategy. He previously lead the team responsible for Cloudera?€?s Hadoop distribution (CDH), is an Apache Hadoop committer and PMC member. He also serves on the advisory boards of several startups. Prior to joining Cloudera, Eli worked on processor virtualization and Linux at VMware. ,You can find him on Twitter at ,.
,
Here is my interview with him:
,
,
,
,: I work in our CTO office. I?€?m currently focused ,on new technology introduction and strategy. I spend a good amount of time with customers and partners, prospective customers and partners, sharing with them and incorporating what I learn back into our various teams.
,
,
,
,I?€?ve seen a drop in excessive big data publicity over the past year, it?€?s an established term at this point. The stories are shifting from stories about the technology itself to stories about how people are using data to make life better. The Internet of Things seems to be the hype dejour.
,
,
,
,
,
,
,
,I?€?m not sure there are inherent risks in big data. Like any technology it ,can be used for good and harm. More and more of our activities are intermediated by machines, we can analyze the data they generate to better understand how things work, and how to improve. i.e. we can apply the scientific method to more parts of life. There are however things we need to be mindful of. For example, if you?€?re looking at data that?€?s not representative of the whole population then using analytics can make a system less equitable. I talk about this problem a bit in ,.
,
,
,
,The big data ecosystem evolved from the Internet companies that had these sorts of problems before others. Existing technology and products just didn?€?t work well for them so they ended up inventing new ones. Most of Cloudera?€?s founders came from this world, and this is what motivated them to start the company in 2008, when there was an opportunity to bring this new technology to the market. 
,
Now there are dozens of big data companies that are building new products to serve this part of the data management market, and the existing players are adapting their product portfolios. For example, almost every major data management company ships or integrates with the Apache Hadoop ecosystem at this point. This is not to say Hadoop is the only big data technology, just a prominent example.
,
,
,
,It?€?s evolving. I was recently in a meeting where one of the board members of a large company flew most of their executive staff halfway around the world to get educated on big data. They see how their competitors are using data, they see how new ,data-oriented entrants to their markets are competing successfully with incumbents. Companies are increasingly appointing ,, so they?€?ve been thinking more about data already. Sometimes the initiative comes from the bottom up. Some companies are hiring technology people outside their vertical, for example I?€?ve seen financial services firms hiring people from Google. Changing culture takes time but it?€?s happening. A lot of companies are already very data driven, they?€?re changing the fastest.
,
,
,
,It really depends on the use case and the organization. With any wildly popular new technology there?€?s always a shortage of people and skills for a while. In our case the market has been correcting for that well. We started training people over 6 years ago. For some it?€?s things on the input side of the equation, e.g. getting their data supply chain in shape. For others it?€?s the ?€?output?€?, immature tools or missing applications, so more users can get value from the system.
,
,
,
,Hopefully, as it continues to get baked into all the products and services we consume, it?€?s less of a thing. Over the next 2 to 3 years it will enable analytics to be a lot more pervasive. Hopefully we?€?ll take it for granted.
,
,
,
,I?€?d start with product capabilities. Actually use the product to get a sense of the differences. Because data platforms are a long-term investment, you also want to think about things like the vendor?€?s track record with the product and customers, their history of innovation, product roadmap, whether they have the people to support the technology they?€?re offering, how aligned they are with the other products you use, whether they can be a long term sustainable business, and so on. Most of our customers are pretty strategic about who they work with.
,
,
,
,Be open to new things and optimize for learning. Unless you?€?ve known what you want to do your whole life and really enjoy doing that thing, you?€?ll need to change, and be able to adapt well to change. A lot of other advice can be derived from this. For example, if you optimize for learning you want to be around people who are smarter and know more than you.
,
,
,
,I enjoyed , by Michael Lewis. Unrelated, I keep hoping someone will write The Soul of a New Machine for something newer than the minicomputer. Work and spending time with my wife takes up most of my time. I also enjoy spending time with friends, exercising and reading. I?€?m not a snowflake.
,
,
,  "
"
,
Here are the top 20 Big Data Influencers and Brands, drawn from ,. These individuals and brands were chosen for the normalized Pagerank computed from their Twitter presence. These are the companies and individuals that will surely shape the competitive landscape of their industries in the coming years. We also added the number of Tweets and Twitter followers, as of Jan 31, 2015.
,
,
,
,
,
,
,
,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Feb 3 and later.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky,  
,, Feb 2, 2015.
,
Here are upcoming February - July 2015 meetings and conferences. 
,
For full list see ,  page.
,
The most common locations are:
,
,??,Here is the map of the meetings - about half are in the US, the rest mainly in Europe, and Asia.,
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,??,
,
,??,
,
,??,
,
,
,??,
,  "
"
        ,
  "
"
,
,March 30 - April 1, 2015
,Las Vegas, NV ,
,.
,
,
,
As the world moves from physical bits to digital bytes, the digital age is carving a chasm between analytics haves and have nots. Crossing this analytical divide requires new tools, strategies, skills and competencies, as well as new roles, organizational models and governance mechanisms. IT and business leaders must guide the organization as a whole toward a more sophisticated understanding of how analytics initiatives can improve customer relationships, workforce effectiveness and operational efficiency.
,
,
At Gartner Business Intelligence & Analytics Summit 2015, March 30 - April 1, in Las Vegas, NV, you'll learn how to remaster your skills to deliver the analytic advantage your organization needs in the digital age in order to succeed. You'll walk away with new best practices and leading-edge strategies in all of these areas and more, including how to:
,
,??,
Whether your top priority is crafting a strategy for a new BI initiative, modernizing core technologies or tapping the power of today's leading-edge BI and analytics, the 2015 summit provides you with the information you need to avoid pitfalls and accelerate progress. With more than 50 analyst sessions across six tracks, the 2015 agenda offers practical, how-to recommendations across the breadth of today's BI and analytics challenges and hot topics, including:
,
,??,
,.  "
"
,
,
, is an Analytical Solutions Architect at the , specializing in high-performance analytics solutions. Rachel consults with executives and analysts, works across industry verticals, and has seen a variety of challenges and applications throughout her career.  Most recently Rachel spent several years working with government agencies to architect solutions to combat fraud, waste and abuse.  
,
Rachel has a passion for teaching, presenting and finding innovative ways to explain difficult and technical topics to non-technical audiences.  She first discovered this passion when teaching pre-calculus at NC State as a graduate student where classroom creativity was needed to help students find a new way to learn material that has challenged them in the past. 
,
She holds a Bachelors in Mathematics from the University of Rochester and Masters in Operations Research from North Carolina State University.
,
,
,
Here is second and last part of my interview with her:
,

,
,
,My team is an Analytics center of excellence for SAS in North and South America.  We provide thought leadership and support ,for SAS?€?s emerging analytical solutions and support our customers when they have particularly challenging analytical problems.  My primary focus is on SAS?€?s High performance analytics products as well as with SAS Decision Management.  
,
What I like most about my job is the opportunities it presents in public speaking.  I come from a teaching and consulting background so I love getting up in front of people to figure out how to explain complicated topics in a way everyone can understand.  
,
,
,
,SAS products aren?€?t just for big data processing, our products are for Big Data Analytics allowing our customers to be proactive not just reactive when working to leverage the massive amounts of data they are collecting today.
,
,
,
,Hadoop is revolutionizing the arena of ?€?Big Data.?€?  The drastic cost reduction for storing and processing data that Hadoop affords allows for people to really pushing the boundaries of what?€?s possible when it comes to analytics and not just live within the confines of traditional environments. 
, ?? ,
,
,
,Do something that you love and it won?€?t feel like work.  Maybe that?€?s easier said than done but I?€?ve always kept that as a mantra and right now I love what I do, and so (most days) it doesn?€?t feel like work!
,
,
,
,Now that analytics and data science have moved from the back office to the front office it is extremely important that potential candidates have effective communication and presentation skills. It?€?s not enough to just have the technical chops, a data scientist must be able to effectively explain how he or she came to a specific conclusion and convince the internal or external customer that their results should be leveraged. 
,
,
,
,The last book I read and liked was , by Elizabeth Strout.  The book is a series of short stories all taking place in the same town and the title character appears in each one.  It was a very innovative way to tell a single story by many different points of view.  
,
When I?€?m not working I love to be in the outdoors whether camping or hiking, biking or running.  I went hiking in Yosemite National Park for the first time this year and it blew me away.  I hope to go to at least one national park every year.  
,
,
,  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
,
,
,
When various researches on analytics predict an acute shortage of data scientist around the world, what they don?€?t realize is that most of this deficiency is going to be filled in India. In the last few years, the number of US based companies that had setup analytics competency centres in India have grown in numbers.
,
Last year alone, companies like Limited Brands, Lowe?€?s and Cargill have opened centres in India that primarily caters to the analytics and data science needs of parent company. It is a well-known fact today that a number of well-known Fortune 500 companies outsource analytics functions to India. According to a research done by Analytics India Magazine 2 years back, India accounts for 9% of analytics headcount worldwide. I actually see this pie growing.
,
,
,
Analytics is at the same stage of adoption as what IT was exactly 15 years back. Higher demand for a relatively new field has led for a dearth of data scientists, leading to increasing wage levels in this industry. Moreover, expertise is analytics is slightly a different ball game than IT or other streams ?€? it requires a combination of deep skills like statistics and mathematics, data, computer science, visualization, design bend and business knowledge.
,
While there are still some enterprises who consider analytics as an in-house activity, the arrival of major vendors in India with specialized processes has led to analytics outsourcing becoming a fastest growing verticals of the outsourcing industry.
,
Building a data science team is an expensive and time consuming process ?€? experts need to recruited, tools acquired, use cases identified etc. The result of analytics in terms of value is usually not as evident as say creating an IT application with known results. It is difficult to convince business management to convince on analytics when the value is not quick and evident, and most in house analytics initiative fall short of funding. Outsourcing is a speedy, economical and effective mode to showcase this value.
,
,
,
The analytics industry in India has gone through a ?€?concept selling?€? phase in the last few years with a majority of clients who have so far utilized outsourced to India for primarily IT or process services.
,
India, as a geography, is an unmatched analytics hub because of its pool of talent in quantitative disciplines like mathematics and economics. While, the depth of analytics skillsets is contested by many, the good news is that India?€?s analytics talent base is expanding rapidly. Analytics education market is flourishing in India with dozens of master?€?s level course being curated by universities, technical and business schools and private players; some with world class quality.
,
Another useful, and often missed, feature is India?€?s adjacency to IT services that provides useful context and the technology background. As compared to other geographies, analytics service providers in India finds the IT play within data science, the easiest.
,
,
,
All good things come with their set of challenges. Unlike IT/ITES, the conversation around analytics is primarily consulting driven. Analytics partners look to tackle problems in various areas of business for their clients. In choosing an analytics partner, focus should be on analytical and technical competence, and applying those competencies to solve real world issues.
,
The success of BPO/IT outsourcing is well documented, though data science outsourcing is still finding its ground and a right business model. The unique nature of data sciences require a very different contract and pricing model.
,
Various outsourcing models have emerged recently -
,
,
,
,
The ?€?should we or shouldn?€?t we?€? argument to outsource data science is heating up among business managers as analytics becomes essential for competitive advantage.
,
Undoubtedly, the biggest argument in favour of outsourcing is the speed to adoption of analytics. It brings scale and specialization in a cost effective manner within short timelines.
,
Another big benefit overall is the forced standardization that outsourcing would bring to the whole industry. This is good for the whole industry, as processes, techniques and skillsets would be shared and adopted across domains.
,
This standardization would lead to increased automation of key processes and techniques as is evident in many industries. Most outsourcing outfits tend to create unique IP?€?s and products that 1) are results of specialized knowledge that they have gained over time and 2) are mostly plug and play, catering to a wide range of companies and business problems.
,
What?€?s needed now is the right set of governance and engagement models that are crafted specifically for the analytics industry. This is bound to happen as analytics outsourcing evolve and matures.
,
, Bhasker Gupta is a graduate from IIT, Varanasi and a post graduate in Management from IIM Lucknow. He has almost a decade of experience providing Analytics/ Business Intelligence/ Quantitative Research for finance and retail industry.
,
Original: ,
,
,
,
,
,
,
 ,  "
"
,
,
,
Over the past six years, we?€?ve seen a rapid evolution in data processing platforms and technologies. While Hadoop and Hive remain core components of the data processing toolkit, a new breed of emerging technologies is changing the way we work with and use data. While we started with data in siloed pipelines, today we are able to process, store, and manage data in a more fluid way.

,
,
,
As any early Hadoop user (version .17, circa 2009) will tell you, it was a comparatively painful time compared to now. In the beginning, using Hadoop was time-consuming, excessively hands-on, and perhaps most frustratingly?€?unstable. I recall several instances of single points of failures, of colleagues losing entire sets of data and having to start from scratch.
, 
Today, an ecosystem of post-Hadoop technologies have emerged, such as Mesos, Presto, Yarn, and Docker, to name a few. It?€?s an evolution driven by a real need: 90% of the world?€?s data was created only in the last year. With this expanded set of data at our fingertips, there has been an increased demand for aggregating, manipulating and deriving benefit from data faster and more securely. The post-Hadoop technologies have allowed us to tackle three key problems?€?containerization, scheduling, and experimentation?€?that have revolutionized how we work with data.
, 
,
Every developer wants his or her own set of data tools, which means DevOps has the nightmare task of managing a large cluster. For any particular job, the tool used, along with all it's dependencies, must be distributed to each machine in the cluster. Get enough developers together sharing the same cluster and it doesn't take to long before the requirements of one tool will break another. For instance, Tool A requires version 1.2.3 of a specific library but Tool B needs version 1.4.8 or it breaks. This is colloquially known as dependency hell.
 ,
Enter Docker, an open platform for distributed applications. Docker enables containerization of a tool, along with its dependencies, so it can be rapidly deployed as a ""black box"" for every machine in the cluster.Each tool is self-contained along with its dependencies. This makes it possible to have different jobs use different versions of the same tool without any conflict.
, 
So, developers can now use the best tools for the job without the mess: they no longer have to use a specific tool chosen by the DevOps team,or set up an entirely new system to integrate data in the pipeline. By containerizing code, we?€?ve been able to break open the silos of using different versions of tools?€?driving growth and scaling without consequence.
, 
,
Data doesn?€?t sleep. People, machines, systems are constantly producing new data. Enter ,, a distributed, fault-tolerant scheduler. Chronos is a key post-Hadoop technology that has allows us to tackle the challenge of running queries on a schedule, enabling continuous data processing. Chronos, along with most of the technologies we?€?re seeing in the post-Hadoop ecosystem, plays nice with other technologies, including , (resource management) and ,. The powerful combination has allowed us to process customer data separately, all the while running containers continuously and scaling horizontally without writing new code.
 ,
,
With scheduling and containerization issues tackled, we are entering a new phase of effective data manipulation. In the past, what held developers back from experimenting with their data was a fear of losing that data in the process. , and , allow engineers to conduct fast analytics on top of an existing cluster, thereby simplifying experimentation, including combining different data sets, writing new machine-learning frameworks?€?essentially everything that has set the stage for today?€?s predictive analytics revolution. Indeed, the more our tools allow us to experiment with data, the further and quicker we?€?re going to go. It?€?s the same as a science lab; a cancer research center with full access to tools and funding can and will find those life-altering solutions faster.
,
,
If there's one big trend today emerging in the big data space that's rapidly pushing Hadoop and MapReduce to the bottom of the tool chest, it's solutions that use memory more effectively. You see this technique being utilized by our personal ad hoc querying tool of choice, Prestodb. But another contender we're watching is Apache Spark. Similar to how Hadoop helps developers avoid taxing the network, which is arguably the slowest information bus in your computer, tools like Spark give developers the ability to express jobs that both avoid taxing the network and the hard disk at the same time. The approach often reaps a 10x-100x performance improvement over a traditional MapReduce job. The most intriguing thing about Spark is that it gives you the primitives to express the most complex of computations while remaining memory efficient.  Interestingly enough we are seeing the building of more familiar ways of data munging through Spark SQL -- which could end up being the killer combo to end the current reigning big data champion. Time will tell.
 ,
,
In my line of business, predictive analytics, we often talk about a new layer of intelligence that can add real value to inherently ?€?dumb?€? CRM and marketing automation systems. I see this same concept for data scientists and engineers: Using Hadoop and Hive as a foundation, new data processing technologies are enhancing data security and data manipulation, and opening the doors to new possibilities for big data. And just like I was with Hadoop, I aim to be on the front lines of pushing those possibilities.
,
,
, is a CTO & Co-Founder, 6sense, where he leads the development of 6sense?€?s innovative analytics and predictive platform. Prior to 6sense, Viral built the big data platform at Hulu that processed over 2.5 billion events per day. He was an early adopter of Hadoop (late 2008), and built and managed a cluster that stored and processed over a PB of data. Viral was instrumental in building the infrastructure that powered reporting, financial and recommendations systems across web and mobile devices. In his spare time Viral enjoys contributing to open-source projects.
,
,
,
,
,
 ,  "
"
,
,
,
This week on /r/MachineLearning, we have some interesting job discussions from current ML professionals, a solid list of AI courses, books, and video lectures, and a thread of simple questions for those of you who?€?ve always had a nagging question you?€?d like answered.
,
,
,
This thread is a simple discussion about whether or not to start having a simple questions thread for /r/MachineLearning. It?€?s interesting to see how people view the Reddit community, and this post lead to our #4 post this week.
,
,
,
In this post, many different current ML practitioners discuss some of the downsides to their particular jobs. Something that may or may not surprise you is just how common social issues are mentioned, like problems with non-technical staff or overly-optimistic expectations, instead of actual complaints about the subject matter or working conditions.
,
,
,
Here is a wide-ranging list of varied resources on learning AI yourself. What makes this particular list interesting is that it?€?s hosted on Github, so if there?€?s a particular resource you find missing, you can go ahead and add it yourself with a pull request!
,
,
This is the result of the #1 Post this week. It includes questions on many topics from CNNs to image recognition and audio processing. The discussions that arise around these questions can be interesting. Look out for the thread every Friday if you have any questions of your own.
,
,
,
This is a more technical post. The link goes to a paper published in APS X (American Physical Society) of all places and introduces a method of topic modelling that the authors compare favorably with LDA. If you?€?re interested in topic models, give this paper a careful read.
,
,
,  "
"
,.
,
,
As I stated a few weeks ago ,: there has never been a better time to be a Quant looking for a job. With all the media attention, companies are scrambling to board the data analytics bandwagon. It is increasingly apparent that analytics is becoming the standard, and any firm slow to get on board will be left in the dust. Outstanding and inventive uses for data are what will set an organization apart from the pack.
,
Being a quantitative recruiter, I have had a unique perspective on the current climate and how it has changed over the years. From the , to the , to the ,, the Big Data hiring market is starting to move even faster and is quickly picking up steam.
,
,
,
I took a sample of our LinkedIn network of over 10,000 analytics professionals and data scientists to see how many actually changed jobs last year, and found that ,. The rest of the market, by comparison, had an average 10.4% voluntary turnover in 2013, ,, and while figures for 2014 haven?€?t been released yet, it?€?s unlikely that number shifted significantly.
,
We also looked at whether or not those that changed jobs relocated for their new position, and found that ,, which was a higher number than we had anticipated. Mobility can be a great benefit to your job search, as it opens up a lot of opportunities, and this percentage would suggest that quantitative professionals are taking advantage of the hot market.
,
Quants are also receiving substantial increases when they change jobs. In our Burtch Works Study: Salaries of Predictive Analytics Professionals (released in September 2014), we reported that analytics professionals who received a base salary increase when changing jobs received a ,, compared to the widely-reported average merit increase of 2-4%.
,
,
,
It?€?s a candidate?€?s market! Despite all the new academic programs, , that are popping up to serve the increasing demand, there is still a quantitative talent shortage. This means there are abundant opportunities in every industry, but in order to take advantage of them you must strategically evaluate how each position will affect your career going forward. Not sure where to start? , I see Quants making when planning their careers, from limiting themselves only to name-brand companies, to choosing an offer based solely on salary.
,
,
,
It means, unfortunately, that not only may your firm be suffering from increased attrition in your quantitative positions, but also that hiring for your vacant positions may be a challenge. The good news (if you can call it that) is that you are not alone ?€? companies across the board are struggling to hold onto their top analytics talent. However, there are ,, such as ensuring that analytics has top-level buy-in, and making sure tools are up-to-date. No data scientist wants to be hired just to take care of reporting and Excel spreadsheets, just like no analytics professional wants their hard work to have little or no affect on company strategy.
,
On the recruiting side of things, I?€?ve already written about how , if they hope to compete. This includes everything from fixing salary bands (which is one of my , this year), to considering other forms of compensation and keeping the hiring process itself seamless.
,
Linda Burtch is an expert in Quantitative Recruiting and a Managing Director of Burtch Works.
,
,: , 
,
,
,  "
"
,.
, 
Here are two most important trends in Analytics and Big Data in 2015, in my opinion.
,  
,
,
In my world and from my perspective, one of the most important trends in 2015 in Analytics and Big Data will be the advancement in the collection of big data in developing countries with the practice of the fundamentals of analytics ?€? meaning basic data gathering by learners of data science and using analytic fundamentals to help their societies. This might not seem like a trend, with emerging technologies, features, applications and practices in the spotlight of industry news and with big data collection and analytic fundamentals seeming like yesterday?€?s news in Europe and US. However, as popular as those trends will be, they will mostly trend in Europe and the US and parts of Asia, whereas data collection and analytic fundamentals will trend much further, contributing to the development of heavily populated underdeveloped societies where social challenges are greater than Europe?€?s and the US?€?s. The impact of advancements into developing regions will determine big data collection and analytic fundamentals (in combination) to be an important trend in 2015.
,   
In the US, we live in a country where people are already thinking of how artificial intelligence will be a part of our daily lives, which is demanding of evolved engineering, machine and deep learning, and big data. ,, and it is tempting to write in this article about what the future has in store on this front including in 2015, but the rest of the world is not there yet. The rest of the world is dealing with societal challenges yet to be studied and managed better (e.g. disease, crime, politics, economy), with the collection of big data and the use of the fundamentals of analytics as seen in the US.
,   
My family comes from country where there is a high crime rate, high poverty, curable diseases and illnesses still unchecked, dated infrastructure and an overall economy that should be strong from oil production but is not. It also has micro-societies (i.e. indigenous) and archaeological sites that have existed for thousands of years to consider when trying to solve societal challenges. Many issues are plaguing the country and specific areas of the country. If it sounds like it could be any of several countries around the world, it is because there are several regions globally that are facing similar challenges. They are enduring similar addressable societal plagues, which citizens can better manage and control with the help of data collection and analytic fundamentals ?€? to improve overall society and to preserve ancient ones. Think of Latin America as one region.
,   
,
There are reasons why there can still be a trend in 2015 when challenges seem so great, as described above. It is actually the right scenario. First, there is a need in developing countries with large populations to make changes for the well-being of their societies. Second, there are learners such as myself and people connected to me who want developing countries to overcome such challenges; there is nothing to lose in trying. Third, big data and analytics in practice in the US and Europe, with lessons learned to help developing countries. Fourth, experts are regularly teaching data collection and analytic fundamentals to learners around the globe through free material, lectures, and courses. News of such teachings are being communicated, e.g. via Twitter. Now, learners around the world, who are taking advantage of free education, are taking what they learn about big data and analytics back to their real environments to practice or to apply.
,   
Not to oversimplify the learning of big data collection and analytic fundamentals for developing countries, there are subjects required to acquire necessary knowledge to work with data, e.g. statistics, behavioral science, social science, criminal justice, physical and life sciences, and computer science.  Universities globally have been providing instruction on those subjects for quite some time, prior to Big Data and Analytics taking shape. Developing countries have university graduates who have completed such coursework. More recently, there are free opportunities to learn specific subjects of Big Data and Analytics to build on those learned subjects, including about Hadoop for distributed storage and processing. For instance, ,, ,, ,, , and , offer free Big Data and Analytic education material, webcasts, and/or programs. Learners with a computer, an internet connection, free analytical tools and software, who understand, read and write English can participate. Those businesses have data to share of their big data education reach, i.e. learners of big data collection and analytic fundamentals (by region), which can be analyzed for a trend.
,   
To give a better idea of impact of how developing countries will benefit from big data and analytics, to ?€?improve?€? their processes and operations for the welfare of the public, here are just a few examples:
,
,
,
In 2015, two societal challenges for developing countries to undertake are likely to be:  crime and disease (health). Mexico is a good example of where this is likely to happen in 2015, considering current events there, the size of its population, the constant concern over their soaring crime rate, and the quality of medical services in impoverished communities. Additionally, Mexico has micro-societies like the US (i.e. the indigenous groups) which need to be considered and monitored even more closely, which data methods can help achieve.
,   
In 2015, the practice of data collection and analytic fundamentals will reach developing countries (with data to measure this) and its impact will be significant enough to make it an important trend. One can begin by looking at the distribution of free education and material to learners and their participation (by region) and at Hadoop?€?s market (and those of competitors), again, as a start.
,   
,
,
Another important trend in 2015 for Analytics and Big Data will be new big data security advances, to protect customers and the greater public in general. Big Data breaches have been so common lately in the US that it seems like not a week goes why without an announcement of another one in news media and social media, directly from businesses to those impacted and/or from the US government; one has to think about big data breaches across all industries. Data breaches of high profile companies alone are charted ,. With big data breaches, there are regulatory, legal and financial repercussions, as well as the reputation of businesses and governments at stake. It is in the best interest of businesses (and investors in such businesses) and governments to invest in more advanced methods of encryption for the now, in continuous training for their staff and in new security technologies for the future. Customers, investors, the greater public and the US government are expecting that in 2015. However, even without that expectation, businesses and the US government will make the effort in 2015. Businesses are getting a better understanding of what the gaps are in their operations in big data security from previous data breaches across industries (e.g. chain stores with credit card information, hospitals with health records) and the impact on the public, as citizens publicly voice their concern through news media and social media and in lawsuits, with emotions well expressed. In 2015, businesses will make strides in the advancement of big data security significant enough for it to be an important trend in Analytics and Big Data.
,
, is Business and Data Analyst in San Francisco Bay area.
,
,
,  "
"
Most popular 
, tweets for Feb 02-03 were
,
A New Year in #DataScience, great overview of the #MachineLearning and #BigData landscape by , 
,
,
A New Year in #DataScience, great overview of the #MachineLearning and #BigData landscape by , 
,
,
Avoiding a Common Mistake with Time Series: use de-trending #DataScience #MachineLearning 
, 
,
,
,
Avoiding a Common Mistake with Time Series: use de-trending #DataScience #MachineLearning 
, 
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
I came across this excellent post on Quora by 
,, that was a great 
, for Big Data technologies fit together.
,
Abhinav is a Product Designer at Quora, and earlier was a Software Engineer and Data Scientist at Facebook, Coursera, Mozilla, Duolingo and other places. 
,
I added a few links where appropriate and an image from Databricks slideshare
,.
,
,
,
,
,??,
,
Quora question:
, with more answers.  "
"
,
,
, is an Analytical Solutions Architect at the , specializing in high-performance analytics solutions. Rachel consults with executives and analysts, works across industry verticals, and has seen a variety of challenges and applications throughout her career.  Most recently Rachel spent several years working with government agencies to architect solutions to combat fraud, waste and abuse.  
,
Rachel has a passion for teaching, presenting and finding innovative ways to explain difficult and technical topics to non-technical audiences.  She first discovered this passion when teaching pre-calculus at NC State as a graduate student where classroom creativity was needed to help students find a new way to learn material that has challenged them in the past. 
,
She holds a Bachelors in Mathematics from the University of Rochester and Masters in Operations Research from North Carolina State University.
,
Here is my interview with her:
,
,
,

,Agile analytics allows modelers to have a conversation with their data.??, This means that, unlike traditional analytics where a great deal of time is spent waiting for models to complete running, analysts can get immediate answers to their questions which will then lead to subsequent questions and spur on a back and forth conversation with their data.?? Model building and analytics is an iterative process in principle. As the iteration time is reduced, modelers can provide organizations with better answers more quickly.

,

,
,
,There are two great places to start:
,
, ?? ,
These both enable organizations to make more data driven decisions more quickly.
,

,

,

, This is often because the data environment, modeling environment and production environment are all siloed and / or on different platform.?? It can require a great deal of manual intervention to move from one environment to the other which can be extremely time consuming.?? This results in the analytics getting stale as customer behavior changes and the analytic process can?€?t adapt quickly enough.

,

,

,

, takes data, business rules and analytical models and ,turns them into consistent, automated actions that drive faster, better operational deci??sions. And it does so from a centrally managed, easy-to-use interface.?? SAS?? Decision Manager is used to automate the hundreds and thousands of opera??tional decisions that happen every day. Not only does this make organizations more efficient, but it eases the burden of manually redefining models into production.

,

,

,
,I find the speed at which models can be run as the most intriguing aspect of ,. Leveraging SAS?€?s In-memory technology models can be run at speeds never seen before and this means that our customers can now ask questions of their data they haven?€?t been able to before.

,

This product has been very well received since its launch.?? Being an interactive programming interface it especially resonates with people who are used to open source technology as that may be what they used in school.?? SAS?? In-Memory Statistics, however, drastically speeds up the time at which models can be run and thus enables them to have a conversation with their data and, as I like to say, model ?€?at the speed of thought.?€?
,
,
,
,
,  "
"
Most popular 
, tweets for Feb 04-05 were
,
,  "
"
,
,
,??,??
,  "
"
Join us and other business and data 
,analytics professionals at the 
, held in Santa Clara, April 20-22, 2015. Learn how to leverage data to drive decisions, uncover insights and increase productivity to remain competitive in today?€?s marketplace.
,
,
,
, like ,, ,, and , will share best practices in 60+ sessions including:
,
and more!
,??,
,
,
,??,
,
,
Advanced registration price ends Feb 16. Get an additional , for KDnuggets subscribers when using exclusive discount code 
,.  "
"
,
,
Cutting-Edge Practitioners and Their Views on Critical Skills, Background, and Education
,
By Cornelia Levy-Bencheton
,
,
,
,The gender gap in tech is shrinking. Women are still an underrepresented minority in the disciplines of science, technology, engineering, and math (STEM), but women in data and technology are no longer outliers or anomalies. For this book, author and data warrior Cornelia Levy-Bencheton interviewed 15 women in data to learn how they achieved their current level of success, what motivated them to get there, and their views about opportunities for women.
,
Introducing women to STEM is now a nationwide crusade, but advancing the idea that gender diversity fuels creativity, innovation, and economic growth is still a challenge. The stories in this book are inspiring, revealing insights that will widen the path for even more women in tech.
,
These interviews explore:
,
,??,
The interviewed include those with strong Analytics & Data Science connections: 
,
,??,
Cornelia Levy-Bencheton is an NYC-based communications strategy consultant and writer whose data-driven marketing and decision support work helps companies optimize their performance. 

  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
Here is an interesting list of top 30 people (actually 34) in Big Data & Analytics, created by Innovation Enterprise.
,
,Unlike other lists, this is not based on Twitter or social media, but also on contributing directly to the industry, and focuses on those who had important parts to play in its growth and sustained popularity.
,
,
,
Here are Innovation Enterprise lists and descriptions:
,
,??,
,
,
,  "
"
,
,
, is the leading industry analyst and expert on achieving agile digital transformation by architecting business agility in the enterprise. He writes for Forbes, Wired, and his biweekly newsletter, the Cortex. As president of ,, he advises business executives on their digital transformation initiatives, trains architecture teams on Agile Architecture, and helps technology vendors and service providers communicate their agility stories. His latest book is ,.
,
He has published over 500 articles, spoken at over 300 conferences, Webinars, and other events, and has been quoted in the press over 1,400 times as the leading expert on agile approaches to architecture in the enterprise.
,
Here is my interview with him:
,
,
,
, is the first and only industry analysis, advisory, and training firm focused on agile digital transformation. Intellyx advises companies on their digital transformation initiatives and helps vendors communicate their agility stories.
,
Here?€?s how we define agile digital transformation:
,
Digital -- Customer preferences and behavior drive enterprise technology decisions
,
Transformation -- Internal organizational change is necessary to maintain focus on the customer in today?€?s digital world
,
Agile -- Organizations must capitalize on disruption to achieve greater innovativeness and resilience in the face of digital transformation priorities.
,
,
,
,Fundamentally, innovation requires disruption, and disruption requires risk. Efforts to optimize on standard business metrics (profitability, cost reduction, shareholder value, etc.) seek to avoid disruption. In contrast, the most innovative organizations embrace disruption, and leverage it to drive the creative process so essential for innovation.
,
,
,
,Organizations must be able to balance optimization and innovation. Too much optimization and you won?€?t be able to deal with disruptions in the market, but innovation without optimization is nothing but a crap shoot. Big data analytics can be an important tool for understanding this balance, when used to facilitate an inherently iterative approach as part of an agile enterprise architecture.
,
,
,
,To facilitate innovation, free people from traditional management hierarchies and let them self-organize. Give them broad goals and the tools they need to be successful, and get out of their way.
,
,
,
,There are many candidates for this question ?€? containers in particular come to ,mind ?€? but it?€?s not clear whether containers are truly disruptive or simply overhyped. Instead, I?€?ll choose an area of innovation that is very low on hype but high on potential disruptiveness: a category I call the ?€?business agility platform.?€? These platforms support a higher level of abstraction for building inherently flexible software that deals with change in real-time. Expect to see much more about these platforms in 2015.
,
,
,
,So far the big wins for truly Big Data have been localized to particular industries and problem areas ?€? social media trend analysis, scientific areas like genomics, or the Industrial Internet, for example. I think the most important trend will be how many different types of organizations will find many new and innovative uses for Big Data, well beyond the familiar use cases we see today.
,
,
,
,Squidward?€?s Law. You know Squidward, Spongebob Squarepants?€? morose nemesis? In one episode he said something like ?€?to be successful in business, sell something people want at a price they?€?re willing to pay.?€? That?€?s all there is to it. Thanks, Squidward!
,
,
, ,
,For fun I read science fiction and fantasy. I just finished the thirteenth and last book of Stephen Donaldson?€?s Chronicles of Thomas Covenant, The Last Dark. He?€?s been writing the series since 1977 and I?€?ve been reading them since the first one came out. But I don?€?t have much time for recreational reading any more ?€? I?€?d rather spend my free time with my grandson!
,
,
,  "
"
Most popular 
, tweets for Feb 02-08 were
,
Where's Waldo ruined by a PhD student & #MachineLearning - here is the optimal search path , 
,
,
,
Great Tutorial: Getting Started with Apache #Spark and #Python #BigData , 
,
,
,
Very useful: 10 things statistics teaches about #BigData analysis: when to average, smooth... , 
,
,
,
Very useful: 10 things statistics teaches about #BigData analysis: when to average, smooth... , 
,
,
,  "
"
,
By Jeff Leek, (,)
,

,
,
Original: ,
,
,, is a professor at Johns Hopkins, where he does statistical research, writes data analysis software, curates and creates data sets, writes a blog about statistics, and work with amazing students who go do awesome things.
,
,
,
,
,
 ,  "
"
,
,
, is founder and CEO of ,, a Big Data analytics and apps firm. His global leadership experience of more than 16 years includes C-level positions across product management, sales and marketing, and corporate strategy and turnaround. Nuevora was recently ranked by CIO.com as one of the top 10 Big Data firms to watch out for.
,
Here is my interview with him:
,
,
,
, , provides Big Data analytics applications and solutions, focused on marketing and customer life cycle business issues. Leveraging the proprietary big data analytics & apps platform (nBAAP???), Nuevora delivers significant speed and scale in delivering complex predictive and prescriptive analytics to its customers. 
,
Nuevora?€?s value proposition impacts four key areas of business for its customers:
,
, ?? , 
,
,
,I envisioned a new wave of business processes-as-a-service (BPaaS) applications, after seeing how the SaaS disrupted the traditional enterprise software business model in the past decade. I saw them taking root in major corporations that drive smarter decisions through near real-time and continuous analytics. 
,
I noted that innovative BPaaS business models will combine the benefits of globalization ?€? inherent in the KPO models ?€? with those of scalability available through SaaS. This new nonlinear business model will create a new category of business applications that enable smarter business decisions and deliver operational excellence across an organization by:
,
, ?? ,
We have been gaining rapid traction among Fortune 500 customers and have seen consistent growth from existing customers.

,

,
 ,
,The key questions we see CMOs being pre-occupied with include:
,
, ?? ,
,
,
,Nuevora's nBAAP??? Platform features purpose-built analytics applications based on best-practices-driven predictive and prescriptive algorithms. nBAAP??? comprises four key engines: the big data processing engine for data ingestion and processing; analytics & modeling engine for building and managing a variety of predictive and prescriptive algorithms; integrated operationalization engine that can enable insights to be published into the execution systems and/or visualization systems of customers; and, a closed-loop recalibration engine that can act on incremental data feeds to adapt pre-built algorithms based on the underlying changes in data.
,
On top of all of this, Nuevora?€?s algorithms work on disparate sources of data (transactional, social media, mobile, campaigns) to quickly identify patterns and predictors in order to tie specific goals to individual marketing tactics. 
,
,
,
,
,  "
"
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Feb 10 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
This week on /r/MachineLearning, we have some interesting discussion around music recognition, a high-quality text understanding paper, an article on the history of Walter Pitt, a Neural Turing Machine implementation written in the Torch framework, and an example of genetic algorithms through bipedal walkers in HTML5.
,
,
,
This post touches on the general strategy used in Shazam?€?s music recognition algorithm. It is a very approachable introduction to music recognition, and even touches on general audio processing as well.
,
,
,
This well-written paper examines text understanding from the perspective of deep learning. It tackles approaches from the character-level inputs to higher-level textual properties and applies convolutional networks to a multitude of text processing tasks. Definitely give it a read if you are interested in either deep learning or text processing.
,
,
,
This is a less technical article, but it is very well done. It goes into the story of Walter Pitt a man that tried to model the human brain. If you?€?re interested in the history of neuroscience and AI, this article is worthwhile.
,
,
,
As the title indicates, this post points to a GitHub repository containing a Torch-based Neural Turing Machine implementation. It shows off the power of Torch, and if it?€?s something you?€?re interested in, be sure to give it a shot.
,
,
,
This post features some interesting visualizations of genetic algorithms through the example of bipedal walkers. If you?€?re interested in the subject, you?€?ll enjoy the animations and how they were created.
,
,
,
,  "
"
By Benn Stancil (Mode Analytics).
,
Seven months ago, we launched a , of Mode, a tool for analysts to create and share work. Importantly, the beta product gave us something that research and intuition couldn?€?t: usage data on how analysts work. After studying the way analysts use Mode, we've recently released , designed around what we've found to be important to data analysts and data scientists. In the spirit of ,, we want to share some of our findings.
,
,
,
We are building a tool to integrate perfectly with an analyst's existing workflow. Along the way, one of the toughest decisions we had to make?€?and one where our own opinions were strongest?€?was how Mode's SQL editor should be oriented. Some people wanted the SQL query stacked on top of the data, some wanted a side-by-side view, and others (like me) waffled back and forth. So we decided to see if our data held the answer. What kind of editor would most users want?
,
After looking, we discovered the answer: it depends. But surprisingly, not only does it depend on the person, it depends on how far along an analyst is in their work.
,
When analysts are first writing a query, the queries tend to be short and the results usually include a lot of columns. Short queries and wide tables are better suited for a horizontal editor. But as analysts edit their reports and get closer to the final version, queries get longer and datasets get narrower. Both of these attributes are better suited for a vertical editor.
,
,
,
,
,
This data guided the design of a flexible editor. But it exposed a more important truth?€?no one design works for everything. Analysis passes through different stages, and what's best suited for one stage might not be the best for another.
,
,
,
Mode is used by both analysts (people who write queries) and business users (those who primarily view results). When studying the behavior of these two groups, we discovered that they need very different things from an analytics tool.
,
Analysts often start on Mode's home page, which contains a directory of reports and data. This suggests they're looking to understand what work their team has done, and want to stay on top of current projects. To make this experience better in Mode, we added an activity feed that shows when someone shares a report, updates a report, or creates a list.
,
,
Business users, by contrast, tend to enter the Mode via links to specific reports shared by analysts. This highlights how important it is for analysts to be able to easily distribute their work throughout their companies. The work of analyst doesn't end with producing a great chart or finding something insightful?€?it ends when that conclusion is also understood by the right person. 
,
,: Benn Stancil is the Chief Analyst of Mode Analytics, a company building collaborative, SQL-based analysis tools for analysts working with both public and private data. Prior to Mode, Benn was a senior analyst at Microsoft and Yammer, where he helped lead product analytics. Benn also worked as an economic analyst at the Carnegie Endowment for International Peace in Washington, DC.
,
,
,  "
"
,
Wharton workshop, Feb 19-20, Philadelphia.
,
In business, predicting how much profit can be generated from a future relationship with a customer is extremely important. Evaluating and forecasting the activities of your customer base is a critical process, 
,and yet it's one that is not fully understood by many companies.
,
,, a two-day workshop by Wharton Executive Education, will give you the insight you need to execute these analyses more effectively. Led by Wharton Marketing Professor Peter Fader, Bringing Customer Lifetime Value to Life goes beyond the conceptual level. You will gain a better understanding of the concepts and techniques required to make more accurate statements about your customer-base analysis. The program will be held February 19-20, 2015 in Philadelphia.
,
To learn more, please visit: 
,  "
"
Most popular 
, tweets for Jan 07-08 were
,
Programming languages popularity by US state: #Java is big in #NY, #NJ, #Python in Idaho 
, 
,
,
,
Great talk: #MachineLearning best practices learned from lots of competitions, by , Chief Scientist #BigData 
,
,
Programming languages popularity by US state: #Java is big in #NY, #NJ, #Python in Idaho 
, 
,
,
Programming languages popularity by US state: #Java is big in #NY, #NJ, #Python in Idaho 
, 
,
,
,  "
"
By Kris Hammond (Narrative Science), Jan 2015.
,
,
,
In 2015, CEOs will demand more from their data than the elusive ?€?big insight?€? that data scientists keep promising but haven?€?t been able to deliver. They will decrease investments in human-powered data science and adopt scalable automation solutions that understand data, unlock insights trapped in it and then provide answers to  ongoing problems of understanding performance, logistics, provisioning and HR just to name a few.
,
, ?€? CIOs will wake up to the fact that visualizations and dashboards do a good job of displaying data, but they don?€?t tell stories. In order to help their employees and customers, we will see companies start to employ tools that help tell the story in words rather than just paint a pretty (or pretty complex) picture. 
,
, Releasing government data into the wild was a great first step, but unless everyone can understand what the data means, it is hardly democratized. In 2015, we will see an increased demand for this data to be more accessible and understandable. From information about neighborhood crime to the availability of services to transportation, 2015 will see the transformation of data into information that everyone can use.
,
, ?€? Companies will begin to realize that they?€?ve amassed data to the point of hoarding. In 2015, smart business leaders will start to demand more immediate returns from data to directly solve real business problems. Forward-thinking CEOs will curb spending on data infrastructure technologies and invest more heavily in tools that explain, communicate and derive meaning from data. The business metrics will shift from understanding ?€?How many terabytes of data do we have??€? to ?€?How much do we know??€?
,
,
 ,
Everyone will stop worrying about machines taking their jobs or wiping out the human race and focus on how intelligent machines can make our lives better. From Watson to Deep Learning to predictive analytics, we will see greater use of AI and its intellectual children in the work place.  Businesses will start partnering with intelligent machines to help us be more productive, elevate our role in the workplace and ultimately make us smarter.
,
, ,, P.h.D., is a cofounder and chief scientist at , and professor of computer science at Northwestern University.
,
,
,  "
"
,
,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,
,  "
"
,
,
,
Register  for the ,
,
, February 10-11, 2015
,
,
,
,??,
,-John Bottega, former chief data officer at Bank of America, Federal Reserve  Bank of New York and Citigroup ,
,
,
,
,
,??,
,
,
,
,
,858.534.8321
,
,
,
,
For  nearly 30 years, the San Diego Supercomputer Center (SDSC) has been at the  forefront of new trends and developments in high-performance and data-intensive  computing. SDSC's large-scale data systems include ,, a 5 PB parallel file system; and the ,, consisting of 3.5 PB of disk  (with replication). HPC resources include the data-intensive , system with 64 TB aggregate  memory, 300 TB of flash memory, and 341 TF of computing power; and ,, an all-new cluster with an  overall peak performance of 2 PF, total 253 TB of RAM, and 620 TB of flash  memory.  "
"
,
,
,
In December 2014 KDnuggets reached to a number of
, and asked them 2 questions:
,
1. What was the most important research paper on Data Science, Data Mining, Databases in 2014? 
,
,
2. Which Data Science, Data Mining, Big Data trend do you expect to dominate in 2015?
,
The most popular trends among mentioned were 
,.
,
,
,
As for the papers, we found that many researchers were so busy that they did not really have the time to read many papers by others.
,
Of course, top researchers learn about works of others from personal interactions, including conferences and meetings, but we hope that professors have enough students who do read the papers and summarize the important ones for them!
,
Here are the answers.
,
,
,, Distinguished Research Staff Member at the IBM T. J. Watson:
,
Individual papers are rather incremental these days and broad advancements can only be understood in the context of collections of papers. Furthermore, most of these advancements existed before 2014, but 2014 saw a significant push in these directions.  The three most important areas of growth in the data science field are as follows:
,
,
1. While the , phenomenon started around 2011 or 2012, it has greatly sped up in 2014,  especially in terms of its impact on data science. This is because of the confluence between the area of big data, data streams, and the natural data intensive applications in the IoT paradigm. Smart healthcare, social sensing,  smart cities are examples of these phenomenon and there is also significant commercial interest. History has shown us that advancements are most permanent when they are supported by commercial  interests.
,
2.  There has been an increase in the number of papers written on the interplay between big data/, and more complex data types. In the past, most of the streaming papers were focused on the notion of multidimensional data, which is an important but limited paradigm. The temporal and streaming aspects are now being studied in the context of complex data types such as graphs, social networks, and social streams.  This area of analysis is rich and remains largely unexplored.
,
,
3. The focus on , has also become increasingly prominent in 2014. The key areas of focus include smart technologies, electronic health records, data integration/fusion, and privacy. The issue of privacy remains the primary challenge for data scientists and also the greatest opportunity for new research.
,
The future developments are likely to mirror recent advancements. Research is a continuous process, and one cannot expect the advancements to be decoupled from the recent developments.
,
In the longer term, as processing power increases, I expect data science to play a key role in the development of amazing artificial intelligence applications. For example, the defeat of human champions by the automated Watson system in Jeopardy was a landmark in artificial intelligence. How far can one go?
,
I believe that the development of more efficient processors and compact storage systems will open frontiers, which were previously considered unattainable.
,
,
,:
,
1) I think the emphasis on , will continue, with newer/better systems and algorithms for big DM and ML.
,
2) There will be increased focus on , data, i.e., electronic health records and personal health data management/analysis
,
3) Focus on , will continue to make waves, i.e., open source Gov data mining, smart cities, smart utilities, etc.
,
,
,:
,
I feel one of the key research issues for data science is how to make use of the big data in a novel way to go beyond what data mining researchers have done using a large amount of data in the past (e.g., scaling-up machine learning and data mining algorithms and data stream mining).
,
In my opinion, one of the long term trends is or should be to use the big and diverse data to , for , and data mining, i.e., to retain the results and knowledge learned in the past and use them to help future learning and problem solving. Without this lifelong learning capability, no system will ever be intelligent.
,
More intelligence is what the data science should go for in the big data era. Although standalone or pure statistical machine learning and data mining algorithms, which represent principled guessing, can still be improved, they have their limits. The question is how to go beyond such pure algorithmic approaches to push machine learning and data mining forward. I believe we should go for more intelligence, i.e., to learn as humans do.
,
,
,
,
,:
,
,
,, by Vinyals, Toshev, Bengio, and Erhan (Google) Arxiv 1411.4555
,
,
,
,
, 
Deep networks
,
,
,, Professor, HKUST and the head of Noah's Ark Lab:
,
,
,.  F. Provost and T. Fawcett. Big Data 1(1), 2013.
,
,
""Open Source Systems"" and ""Feature Engineering"" are two trendy keywords that we will see more often in the coming year. Disk Centric and Network Centric Issues are questions concerned most by practitioners and will have big impact in the practical field.
,
,
,:
,
,
I am a bit of a contrarian. I like papers that show a flaw or weakness in existing work, or show that a common assumption was unwarranted.
,
,, by Anh Nguyen, Jason Yosinski, Jeff Clune,
is the most interesting such paper I have read in years. As an aside, the authors have done a spectacular job in presentation. Not only is the paper very well written, but there is a great supporting webpage with a video and even a useful FAQ.
,
,
,
Not trends in data science per se, but two nice meta-trends in how we do data science research are:
,
1) Some nice empirical work on how good is peer review, i.e.
,
,
2) A movement towards insisting on reproducibility, i.e.
,
,
,
,
,
,:
,
, 
,
,, by Stephan Gunnemann, Nikou Gunnemann, Christos Faloutsos. KDD 2014: 841-850
,
,
,
Fusion data from heterogeneous data sources.
,If you look at the papers in the major conferences, quite a few of them are fusing multiple data sources to improve the prediction. I think it will be an important trend going forward.
,
,
,:
,
, (from Jian Pei group)
,
L. Duan, G. Tang, J. Pei, J. Bailey, G. Dong, A. Campbell, and C. Tang.
,. In Proceedings of the 18th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD'14), Tainan, Taiwan, May 13-16, 2014.
,
The paper proposes a new research problem that we met again and again in a series of practical applications.  It won the best paper award in the PAKDD 2014 conference.
,
,
,
Big data will go deeper into traditional offline domains, and will come closer to customers in their daily life.  Novel, big data enabled business models become a key in unlocking the power of big data.
,
,
,:
,
,
,
I'm not sure I can select best 2014 paper, but I want to plug my son Jonathan's recent work with Moritz Hardt:
,, 2104 FOCS, pp. 454-463. arXiv 1408.1655
,
The idea is that any algorithm that asks a sequence of queries of a database of size n, where one query depends on the outcome of previous queries, can be led to false conclusions after O(n^3) queries. They believe this can be reduced to O(n^2) but can't prove it. Essentially all interesting data-analysis algorithms have this property, e.g., gradient descent. You can think of it as a stronger Bonferroni's principle for non-independent trials.
,
,
,
I suspect that ""deep learning"" is going to be all the rage in 2015, but that like with most approaches to data analysis, it will have a few successes that are widely trumpeted and lots of failures that never see the light of day.
,
,
,:
,
, (from Jiawei Han group:)
,
Xiang Ren, Jialu Liu, Xiao Yu, Urvashi Khandelwal, Quanquan Gu, Lidan Wang, and Jiawei Han,
,, in Proc. 2014 ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD'14), New York, NY, Aug. 2014.
,
The following paper dated in 2015 but was accepted in Oct. 2014 is an even more favorable one by me but it is officially dated in 2015.
,
Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare R. Voss, Jiawei Han,
,, PVLDB 8(3): 305 - 316, 2015.  (Also, in Proc. 2015 Int. Conf. on Very Large Data Bases (VLDB'15), Hawaii, Sept. 2015)
,
,
,
I believe , methods could become more dominant next year.  In the long run, I expect heterogeneous information network creation and mining will become a key thing to turn structured or unstructured data to organized knowledge.
,
,
,:
,
,
,
,
,
Not necessarily the best data science paper, but certainly important for the discussions it brought up about data science ethics.
,
,
,
,
,
,??,
,
,:
,
,
,
1. Szegedy et al.,
,, ICLR-14.
,
,
,
,
,
,
,  will continue to rack up successes and increase in popularity.
,
,
,
,
,, Professor, University of Athens. 
,
,
,
I liked the following three papers from KDD 2014:
,
,??,
,
,
Analysis and linking of heterogeneous streams, and Systems Security
,
,
 ,  "
"
,
,
, is CEO & Founder of ,. Sharmila also serves on the Board of Hadapt, a big data company and on the Board of Lattice-Engines and is advisor to numerous software start-ups. Previously Sharmila served as Executive Vice President for Aster Data, where she led product strategy, all marketing functions, business development and inside sales. 
,
Prior to Aster Data, Sharmila Mulligan was Chief Marketing Officer of the largest business unit (BTO) within HP Software responsible for marketing a $3+ billion software products portfolio spanning all major aspects of IT management. Before joining HP, Sharmila served as Executive Vice President of Marketing at Opsware Inc., a publicly traded company and leader in the Data Center Automation market. Mulligan has worked as Vice President at Totality and Netscape communication. She has also held managerial positions at General Magic, Microsoft and HP?€?s PC business (PSG).  
,
She received her masters in management from the Kellogg Graduate School and her BA in business & economics and BS in computer science from Northwestern University.
,
,.
,
Here is second and last part of my interview with her:
,
, 
,
,: We differentiate in 3 ways: ,
,
, ?? ,
These benefits are very clear for anyone who uses or has seen ClearStory in action.
,
,
,
,ClearStory helps customers in a variety of industries including retail, media & entertainment, consumer packaged goods (CPG), food & beverages, and healthcare. In the CPG industry, improving in-store product placement, and identifying trends that drive sales at each location, can lead to major opportunities to increase sales. However, the various stakeholders that make up the modern supply chain for CPG companies are often seeing a slice of the supply ,chain versus gaining a holistic view to optimize sell through. ,ClearStory Data provides a central point of access to critical internal and external data for finding insights to improve product availability, promotion execution and sales strategies through the retail channel. By using ClearStory, a fortune 500 CPG company is able to analyze disparate data on product inventory, orders, distribution center flow-through and location-specific sales and promotions in near real-time. As they move their products from warehouses to standard grocery store shelves to endcaps during promotion periods, the company?€?s business analysts now have the visibility into how consumers respond to their products based on placement in each store along with the performance of weekly ad promotions.
,
,
,
, Organizations today rely on data that resides in different silos: relational databases, flat files, ?€?big data?€? platforms, cloud-based services and thousands of external data sources. As the number of diverse data sources and data types continue to proliferate Data Variety will become increasingly daunting and costly to wrestle. The ability to address the challenge harnessing this data variety to generate crucial business insights will be absolutely critical to any Big Data project. 
,
Similarly, the velocity at which data is created and how it flows across various systems within an organization is increasing dramatically. The ability to handle the scale of this data influx and to quickly uncover key insights will help organizations stay ahead. Speeding this process from data to decisions will play a critical role that will enable organizations to leverage Big Data to their competitive advantage. 
,
,
,
, Most start-ups tend to scale ,the company without really finding their product-market fit. That is dangerous as it is not prudent to scale before you know your sweet spot in the market and precisely how customers value your product.  At ClearStory, we?€?re particular about experiencing the sweet spot and addressing the market gap head on before we started scaling.
,
,
,
,Understanding the business perspective and the questions the business is trying to answer from data versus looking at every problem as an analytics model from a technical perspective. Also realizing that most users are not able to work with data the way data scientists can so how the information is ?€?delivered?€? to business users to make it more consumable is critical.
,
Data science practitioners need to understand that insights are only valuable if they can be converted into decisions and actions that generate business value. Data scientists need to ensure that their insights make it easy for the business to make these decisions and generate business value.
,
,
,
,My most recent book I read was Ben Horowitz?€?s book titled, ,.  It was direct, frank and very true to the experiences of entrepreneurs and running a company. When not working, I like to ski, water ski, run, and spend time with my 3 kids. 
,
,
,  "
"
Most popular 
, tweets for August 2014 were
,
Worst Venn Diagram ever? ThomsonReuters needs a new graphic designer (HT @CBinsights)  , 
,
,
,
#BigData moves to ""Trough of Disillusionment"" in Gartner 2014 Hype Cycle for Emerging Tech , 
,
,
,
Worst Venn Diagram ever? ThomsonReuters needs a new graphic designer (HT @CBinsights)  , 
,
,
Worst Venn Diagram ever? ThomsonReuters needs a new graphic designer (HT @CBinsights)  , 
,
,
,  "
"
,
,
,??,??
,  "
"
,
,
,
,
?€?Overfitting?€? is traditionally defined as training some flexible representation so that it memorizes the data but fails to predict well in the future. For this post, I will define overfitting more generally as over-representing the performance of systems. There are two styles of general overfitting: over-representing performance on particular datasets and (implicitly) over-representing performance of a method on future datasets.
,
,
,
We should all be aware of these methods, avoid them where possible, and take them into account otherwise. I have used ?€?reproblem?€? and ?€?old datasets?€?, and may have participated in ?€?overfitting by review?€??€?some of these are very difficult to avoid.
,
,Train a complex predictor on too-few examples.
,
Remedy:,
,
,
,Use a learning algorithm with many parameters. Choose the parameters based on the test set performance.
,
For example, choosing the features so as to optimize test set performance can achieve this.
,
Remedy: same as above
,
,: Use a measure of performance which is especially brittle to overfitting.
,
Examples: ?€?entropy?€?, ?€?mutual information?€?, and leave-one-out cross-validation are all surprisingly brittle. This is particularly severe when used in conjunction with another approach.
,
Remedy: Prefer less brittle measures of performance.
,
,: Misuse statistics to overstate confidences.
,
One common example is pretending that cross validation performance is drawn from an i.i.d. gaussian, then using standard confidence intervals. Cross validation errors are not independent. Another standard method is to make known-false assumptions about some system and then derive excessive confidence.
,
Remedy: Don?€?t do this. Reject papers which do this.
,
,: Choose the best of Accuracy, error rate, (A)ROC, F1, percent improvement on the previous best, percent improvement of error rate, etc.. for your method. For bonus points, use ambiguous graphs.
,
This is fairly common and tempting.
,
Remedy: Use canonical performance measures. For example, the performance measure directly motivated by the problem.
,
,: Instead of (say) making a multiclass prediction, make a set of binary predictions, then compute the optimal multiclass prediction.
,
Sometimes it?€?s tempting to leave a gap filled in by a human when you don?€?t otherwise succeed.
,
Remedy: Reject papers which do this.
,
,: Use a human as part of a learning algorithm and don?€?t take into account overfitting by the entire human/computer interaction.
,
This is subtle and comes in many forms. One example is a human using a clustering algorithm (on training and test examples) to guide learning algorithm choice.
,
Remedy: Make sure test examples are not available to the human.
,
,: Chose to report results on some subset of datasets where your algorithm performs well.
,
The reason why we test on natural datasets is because we believe there is some structure captured by the past problems that helps on future problems. Data set selection subverts this and is very difficult to detect.
,
Remedy: Use comparisons on standard datasets. Select datasets without using the test set. Good Contest performance can?€?t be faked this way.
,
,: Alter the problem so that your performance improves.
,
,
Remedy: For example, take a time series dataset and use cross validation. Or, ignore asymmetric false positive/false negative costs. This can be completely unintentional, for example when someone uses an ill-specified UCI dataset.
,
Remedy: Discount papers which do this. Make sure problem specifications are clear.
,
,: Create an algorithm for the purpose of improving performance on old datasets.
,
After a dataset has been released, algorithms can be made to perform well on the dataset using a process of feedback design, indicating better performance than we might expect in the future. Some conferences have canonical datasets that have been used for a decade?€?
,
Remedy: Prefer simplicity in algorithm design. Weight newer datasets higher in consideration. Making test examples not publicly available for datasets slows the feedback design process but does not eliminate it.
,
,: 10 people submit a paper to a conference. The one with the best result is accepted.
,
This is a systemic problem which is very difficult to detect or eliminate. We want to prefer presentation of good results, but doing so can result in overfitting.
,
Remedy:
,
,
I have personally observed all of these methods in action, and there are doubtless others.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
 ,  "
"
New entries for December.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
,
,
Analytics today is at the point of high awareness and very little understanding.
,
Just a few years ago word ?€?analytics?€? conjured up an image of a nerdy math guy in thick glasses. Now, it?€?s a buzzword, with companies scrambling to figure out their analytics strategy much in the same way they are trying to figure out Big Data strategy. But with all the hype about Analytics -- ""competitive advantage of Analytics"", ""game changer Analytics"", ""Analytics-enabled decision process"" -- many executives are still unsure about the basics:
,
,
,
,
,
,

,
,

,
,

,A co-founder of ,, Lana Klein leads the firm?€?s Growth Foresight practice. She is a recognized industry expert in developing unique client solutions combining advanced predictive analytics with deep business knowledge. Focused on the CPG and Healthcare industries, Lana has more than 20 years of experience advising clients on a broad range of analytics solutions.
,
,
,
,  "
"
,
,Feb 2 - Apr 24, New York, NY
,
,
,

,
, Masters or PhD degree in Science, Technology, Engineering or Math or equivalent experience of quantitative science or programming.,

,
,accepted online on a rolling basis: 
,

,

, $16,000 with a $4,000 rebate to graduates who accept a position with one of our Hiring Partners upon graduation (,
,)
,
,
,

,

Become a data scientist and learn the practical skills needed for your career while building awesome solutions for real business and industry problems.

,

Once the foundation of learning has been set, students work on a 2-week, hands-on project with the instructor and mentored by top Chief Data Scientists in NYC. During the final week, students will have the opportunity to interview 300+ hiring companies in New York and the Tri State area.

,
,
,
, Basic programming elements, primary statistical methods, Source Code Control with Git, Github
,
, Data Manipulation, Data Visualization
,
, Introduction to Data mining, Performance Measures and Dimension Reduction, KNN and Naive Bayes models
,
, Tree?? models and SVMs, Association Rule and More Models, Data Visualization with D3.js
,
, Knitr, Shiny, rCharts, QuantMod, Slidify
,
,: Python Programming, Computational Statistics, Data Analysis with Pandas, Getting Data from the Web, Introduction to Machine Learning
,
, Introduction, Regression and Classification, Resampling and Model Selection, Support Vector Machines and Decision Trees, Unsupervised Learning
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,(main meetup sponsor)
,
,
,
,
,

,
,
Twitter: ,
,
Email: ,
,
Sign-up for Meetups:?? ,
Stay informed. Get weekly news, Solid updates, and exclusive offers.?? Much of our support comes from the active NYC data communities, individuals and NYC Data Science Academy.,
NYC Open Data Meetup group??runs one of the largest open data and data science group communities in the U.S. The mission is to gather data scientists and promote open data and data science by offering meetups, conferences and a weekly newsletter.??Our workshops offer excellent training opportunities for your data science and analytic teams.,
In the last year, We have been sponsored by the following companies: SupStat, ThoughtWorks NYC, PlaceIQ, McKinsey & Co,??Betterment, Engima, SumAll Foundation, StreetEasy, SquareSpace, Annalect Group, Lab 49, Yodle??and many others. Many thanks for your support!  "
"
By Parsa Ghaffari  (AYLIEN), Jan 2015.,

The following blog is a summarized version of a 2 part blog series on our ,.,

The FIFA World Cup is without doubt the biggest sporting event in the world. During the 2014 World Cup, millions of fans and viewers from all over the globe used Social Media to share their thoughts and emotions about the games, teams and players and thus created massive amounts of data by doing so.,

Throughout the tournament, Facebook saw a record-breaking 3 billion interactions and Twitter saw a whopping 672 million Tweets about the World Cup. At AYLIEN we decided to collect, analyze and visualize some of this data to look for interesting insights and correlations.,

,

,
,

, , for Sentiment Analysis, , for data processing and , for interactive visualizations.,
,
We started off by taking a look at the matches and their events, such as goals, substitutions and red and yellow cards and graphed them to see how things developed as the tournament closed out.,

,
Fig 1. Word Cup 2014, Goals, Substitutions, Red and Yellow Cards,

An interesting observation here is the fact that the matches with 4 or more yellow cards tended to increase in the later stages of the tournament and the number of Red cards decreased dramatically as players began to fear missing out on later games due to suspension.,

,
We wanted to try and understand what nations Tweeted the most in support of their teams and what languages most Tweets were written in. Looking at four of the teams, who made it relatively far in the competition, the Netherlands were certainly the most vocal. You can also see how Germany started to gather more vocal followers as they progressed through the tournament.,

,<br /
Fig 2. Word Cup 2014, Tweets by Fans,

When it came to languages, Tweets in the English language accounted for over 50% of all Tweets and Spanish came in as the second most popular language, which isn?€?t too surprising.,

,
Fig 3. Word Cup 2014, Tweets by Language,

,
Plotting the total volume of Tweets over time showed a repeating pattern of spikes appearing at match times and also at times when a major event had occurred (such as the elimination of a team, qualification for the next round, or shocking results).,
We looked at how the volume of Tweets in a language was affected by matches and critical events for countries where that language is spoken. We graphed a number of these in our original post (link) but let's look at Tweets written in Portuguese as an example.,

,
Fig 4. Word Cup 2014, Tweets over time,
The volume of Tweets in Portuguese Tweets increased dramatically following Brazils 1-1 draw against Chile and reached its highest point after a shocking loss conceding 7 goals to Germany.,

,
We decided to dive a little deeper into the Tweets collected to try and better understand fans opinions. We looked into the polarity values (?€?positive?€? or ?€?negative?€?) of Tweets collected, to see how they fared for different entities and how they changed over time, as a result of various events.,

, ,

By tracking the sentiment of Tweets with a country's official Hashtag we determined that the most popular teams were the USA, Germany and Brazil with Greece being the least popular. When it came to the most popular players sentiment towards Neymar, Messi, and Howard were highest and I bet you can guess who had the most negative Tweets about them?€?.Yep, Luis Suarez. You can read more about that ,.

,
, ,

Argentine Luis Suarez was accused of biting Italian defender Giorgio Chiellini. One of the most shocking events of the entire World Cup, was followed by a wave of negative comments and feedback from Social Media. Suarez issued an apology on June 30th, which seems to have been satisfactory for the Twitter community (take note PR people!):,

,
Fig 5. Word Cup 2014, Suarez biting scandal,

, ,
Naturally enough, different events concerning players or teams affect how people think and talk about them. Using polarity analysis, we can get an idea of people?€?s reaction to various events, which provides valuable insight into the opinions of fans. ,
Below we graphed the Sentiment of Tweets related to Brazil over the course of the tournament. It?€?s interesting to note how certain events caused a shift in Sentiment and how over all more and more negativity seeped into the voice of the Brazilian fans.,

,
Fig 6. Word Cup 2014, Sentiment over time,

You can read the entire series in full which has more graphs and visualizations ,.,

, , , ,.

,
,
,


  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Alex Jones, Jan 2015.
,
,
,
As someone who has a tendency to think in numbers, I love when success is quantifiable!
,
So, I looked into ,??,. Alongside this analysis, I'll , to illustrate basic lessons of visualization, statistics, and analysis.
,
First, I pulled Stock Price over my first ~90 Days, which aligns perfectly with Days Worked.
,
Then simply added a rolling count of days, how convenient!
,
Example:
,
,
,
,??Let?€?s graph Stock Price vs Days Worked.
,
,
,
,??Obviously no relationship!
,
Not so fast?€? let?€?s Regress Days Worked across Stock Price.
,
It's important to realize that while visualization is a powerful tool and incredibly insightful way to ingest data, it's not the whole story.
,
,
,
,With an,??and a??,, traditional statistics would say we are incredibly confident about the results!
,
So what do all those numbers really say? Well, ,would be:
,
,
,
,??I cost a little??,
,
,??That's per share. With ~197.45M Shares Outstanding, that means, ,
,
,
,
Quick, let's perform some "","" to get a "","".
,
First, let's??,. To do so, we'll get the Minimum, Maximum value, and Spread.
,
,.
,
How do we??,? Simply, (Stock Price X - Minimum)/ Spread.??,
,
,
,
,
,
,??Phew!
,
,... that doesn't seem right???, Scale Days Worked?€?
,
,
,
Ok,??,there's a relationship...
,
See how the Orange line (Days worked) currently starts at 0 and goes to 1. Let's flip that. How? ,
,
Graphed:
,
,
,
,
,
Essentially, we have taken two vectors of differing relative magnitudes, scaled them to an equivalent range and controlled for directionality. Thereby enabling a linear depiction of the relationship and intuitive visualization!
,
,
,
Now what are the regression results?!
,
Sit down for this...??,. Even though the equation (on the final graph) is apparently different, once we ""undo"" the transformations (get numbers to their original values)... they?€?ll be the,
,
,??Because w,?? We changed all points, not just one point.
,
Picture our data as a cube.
,
,
,
If we turn, flip, invert, scale, zoom out, or angle the cube-- has the cube itself changed???,
,
We're simply looking at it from a different perspective, we're just finding that perfect angle to tell our story/ visualization.??,
,
Even with these findings, we must address ,??Based on statistics-- ""data driven"" results, and the interpretation we proposed--??,
,
However, I bet you there's another variable impacting stock. So what else has happened in the time period?
,
Well, considering that the company is an oilfield services firm-- we understand the missing link is ,??(,).
,
What we should realize is that these relationships aren't always obvious! In fact, visualizations can hide relationships!
,
Added Oil price:
,
,
,
Most importantly, this should exemplify one of the most exciting value potentials of ""Big Data"". Today, we have access to incredible amounts of information relative to the??,. With that to relate on, we can see how major indexes, markets, events, weather patterns, etc interrelate!
,
As we move towards a smaller and more interconnected world, actively promote ""Universal"" data points!
,
This is an abridged version of a post
,
,
,
,??is a Graduate Student at??,.
,
,
,
,  "
"
Webinar: ,. 
,You don't need to be an expert!
,
Jan 13, 2015 10-11am PDT
,Live with opportunity to ask questions
,
All registrants receive on-demand version.
,
,??,
Webinar:
,
,Feb 10, 2015, 10AM PST
,
All registrants receive on-demand version.
,  "
"
Most popular 
, tweets for Dec 29 - Jan 04 were
,
A brilliant way to tell causation from correlation: if A influences B, then random noise in A will be reflected in B ,
,
,
Machine Learning Experts You Need to Know: Geoff Hinton, Michael Jordan, Andrew Ng, Yann LeCun , 
,
,
SAS is n1 among major BI vendors whose users plan to discontinue use (16%)  #Gartner , 
,
,
,
A brilliant way to tell causation from correlation: if A influences B, then random noise in A will be reflected in B ,
,
,  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
By Gregory Piatetsky,  
,, Jan 5, 2015.
,
Here are upcoming January - June 2015 meetings and conferences. 
,
Most common cities are San Francisco (4), London (3), Las Vegas (3), Chicago (3), San Diego (2), New York City (2), Boston (2), and Bangalore (2). 
,
For full list see ,  page.
,
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
,
, is currently the Director of Elite Performance for ,. He does program analysis and design based on live data tracking systems for professional team. He has just signed a deal with the NBA for all 30 teams and is currently developing new models for NFL, NHL and college sports. 
,
His work comprises of real-time training loads for each athlete, development of performance training based on energy systems, working with media on new metrics for broadcasting, and working with coaches on performance programming based on game. He has developed the cardio programs and VO2 assessment programs for Athletes?€? Performance and their new corporate division, Core Performance. STATS clients include thousands of professional athletes from the NBA, NFL, NHL, MLB and MLS. 
,
Besides, Paul has also consulted for Intel and Google in the corporate market, and worked extensively with Adidas to develop the MiCoach. He holds MS in Kinesiology and Exercise Science from California University of Pennsylvania.

,
Here is my interview with him:
,

,
,

,My role of Director of Elite Performance plays a very big part. My job is to analyze the SportVU data for 
the NBA performance staff. We are at the ,beginning of this process right now with only one full season. But over time this is going to be very valuable to all the teams. Right now we are just trying to collect as much good data as possible on player-level, not just game data. The challenge is in collecting the data at this time because the sensors for this market are so new.
,
,
,
, Right now we are just looking at trends in the game data. Most teams are looking at ways to determine fatigue so they can manage the players over a long season. We are working on a fatigue index now.
 ,
,
,
, ,The platform just helps us look at any player very quickly. Every game is loaded in ICE by 8 am the next morning so I can get reports out to teams quickly. Main thing we are looking at right now is player loads, intensities and high end accelerations.
 ,
,
,
, The game is only 30-40 minutes of the player?€?s day- what is going on for the rest of the day? How did they sleep? How did the travel affect them? What did they eat? These are all issues to monitor- teams are working on their own solutions to these items now because every player and coach is different so there is no one plan that fits all.
 ,
,
,
, For the most part I deal with the coaches and they discuss the data with the players- the coaches know what and who is interested in the data. They have the relationship with the players which is the most important element. A few players will ask me directly. They has been very positive and appreciate feedback on what they are looking for- what is important to them.
 ,
,
,
, Discussing conditioning (mainly low minute guys and off season) and recovery programs for starters.
,
,
,
, Understanding the outside demands on the player- their travel schedule is crazy and sleep/recovery is key. We need to look at the big picture.
 ,
,
,
, I can only speak on the performance end of this industry- it is so new that just jumping in and looking at the data is very important. Play with it and put the sensors on yourself and understand what is it really showing you- and you must have a coaching background! You need to know how to look at it as a coach not just the numbers - How will you use this data in the weight room or on the court? How do you present it to the players and other coaches?
 ,
,
,
, We are just starting with performance data collection so that is enough to keep me busy for a while.
 ,
,
,
, ,You have to enjoy what you do - I hang out with coaches (many I have known for years) and talk shop all day- it doesn?€?t free like work. I love to watch athletes perform so if watching games/practice is work?€?. well it is also a great life. I travel a lot but I work from home so I see my family a lot when I am not on the road.
,

,
,  "
"
January 05, 2015, BALTIMORE, MD. 
,
,Agnik, the leading data analytics company for connected cars, today announced its Connected Insurance Program supported by nineteen major insurance carriers. 
,
This program will be available through Agnik's consumer facing connected car products such as Vyncs for aftermarket vehicle-onboard devices, apps driven by Agnik's smartphone-only data analytic engines, and automotive OEM head-unit integrated solutions. All consumer facing third-party products supported by Agnik will also be eligible for this new program.
,
,
Car owners can sign up for Agnik's Connected Insurance Program with any of the participating insurance carriers by opting in after driving for at least 50 miles with an Agnik connected car product. After that, the participating insurance carriers will send the car owner a quote for the insurance policy, and the car owner will select the offer that he or she likes the most. Typically, an average consumer can save up to 20% on his or her annual auto insurance policy, which could amount to hundreds of dollars of savings per year. Savings may vary for each user. Car owners may also be eligible for Agnik Reward Points program depending upon their continuing driving behavior. Users will be able to enjoy wide range of benefits using the Agnik Reward Points.
,
,
,
More information about the Agnik Connected Insurance Program can be found at 
,.
,
,
,
Agnik is the market-leading data analytics software company for connected cars and connected life with wide range of telematics products for consumer, fleet, insurance, vehicle-repair-services and automotive OEM markets. Agnik's patent-protected vehicle-onboard and cloud-based vehicle data analytics software is used for the monitoring and benchmarking of vehicle-health, emission, driver-behavior, fuel-consumption from telematics and context data. Agnik's onboard data stream mining software manages many in-vehicle devices that are in the market.   "
"
,
iMath Research, a start-up at the Research Parc of UAB (Barcelona), offers access to intensive computational high technological solutions, such as big data analysis, for small and medium companies. Imath Research presents their first product - iMath Cloud.
,
The initial version was a finalist at Code_n14 International start-up competition, and later was selected as one of the 15 start-up winners at London Technology Week 2014. Now, iMath Cloud is published in beta version available for any data scientists from companies, universities and research centers. 
,
You can sign in the iMath Cloud's web site:
,.
,
iMath Cloud permits to develop any Python, R or Octave programming language application and it permits accessibility from any place, device, and moment.
,
Comparing to competition, iMath Cloud has three differences:
,
,
,
The users or enterprises interested in testing and verifying this new product, can sign in iMath Cloud accessing to iMath Research's official web site: 
,. 
,
Additional technical documentation is at: 
, . 
,
For questions, email to , .
,
,
,
,  "
"
,
,
Global Big Data Conference is offering 3 day extensive bootcamp on Big Data on Jan 16-18 at Santa Clara Convention Center, CA, USA (San Francisco Area). This is a fast paced, vendor agnostic, technical overview of the Big Data landscape. No prior knowledge of databases or programming is assumed. 
,
Big Data Bootcamp is targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on 
,
Attendees will experience real Hadoop clusters and the latest Hadoop distributions.
,
,
,
Unlike other big data training sessions, our boot camp is unique in the following aspects:
,
,??,
,
,Engineers, Developers, Architects, Networking specialists, Managers, Executives, Students, Professional Services, Architects, Data Analyst, BI Developer/Architect, QA, Performance Engineers, Data Warehouse Professional, Sales, Pre Sales, Technical Marketing, PM, Teaching Staff,  Delivery Manager
,
Please register  ASAP to take advantage of discount. Seats are limited for $100 discount
,
Pay $1399(3 days), $999(2 days), $699(1 day) with 
Promotional code=,, That's $100 off the normal registration price. 
,
Discount price expires on January  14  2015
,
Register : ,
,
Agenda: ,
,
Speakers: ,  "
"
        ,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jan 6 and later.
,
See full schedule at , .
,
,  "
"
,
By Gregory Piatetsky,  
,, Jan 7, 2015.
,
Recent KDnuggets Poll asked 
,
,
With over 
,, the plurality of 48% said no, while 37% said yes.  16% were not sure. 
,
I hope that the results were not skewed by bots wanting to minimize their perceived threat, but a surprising question was raised by ""Mike"" in a comment below: if promoting humanity opposes progress, then when AI becomes good and humanity bad? 
,
""Mike"" wrote
,
,
Could ""Mike"" be a bot? 
,
The 20th century has shown the terrible results (and uncounted millions killed) of putting people second to ideology, so I hope that whatever value people put on progress it is not higher than the value of humanity.
,
,
,Is AI Good or Evil - drawn for KDnuggets by 
,.
,
Here is a regional breakdown in answers to, 
,
,
with region height corresponding to the number of votes (except for All entry).  Interestingly, only in Africa/Middle East did majority of voters think that AI will be a threat to humanity.
,
,
,
,
,
,
,
If researchers succeed in creating a fully non-human AI, probably by mimicking biological structures (e.g. the octopus brain) and also using emergence as an accelerator there can be no doubt that such an entity will share very little with us in terms of viewpoint/perspective etc. If it is logical (very likely) then it will have no choice but to regard us as a threat and if anyone is stupid enough to hook it up to an offensive weapons grid we are for it.
,
,
,
It is very strange how people believe that AI will outsmart human beings intelligence, it is a completely different type of intelligence which has similar domains to human intelligence.
,As for people who believe than Self Aware AI will not have a code of ethics, let me tell you that ethics don't come from no where, it comes from logical reasoning, so AI will have a code of ethics, although it is not necessary to be exactly the same as the Human one
,
,
,
It will be a threat to our privacy. The entities controlling the data will control our lives to some extent. The corporations will nag us with their advertisements.
,
,
,
Unlike any previous technology, full AI, if created, will have a mind of its own and potentially smarter than humans. Just look at the history of encounters between more and less advanced human civilizations, like Europeans and Native Americans, etc and it is hard to be optimistic for the less advanced group, which humans will become if full AI is created.
,
,
,
Application of technology can be anything. Knives can be murder weapons or life saving tools used by surgeons.
,People have feared new technological possibilities through history. Little over one hundred years ago farmers chased cars with pitchforks claiming them to be Satan's invention.
,Are we not over-reacting?
,
,
,
Mike, what is to prevent intelligence without human ethical values to destroy all humans, because they consume precious resources?
,
,
,
If you are self-interested, science is good because it improves humanity. If you are altruistic, humans are good because they advance science. Either way, humanity and science are on the same side ...until something more intelligent emerges. Then, promoting humanity opposes progress and vice versa. So, it comes down to what you consider to be important. I define myself by my intelligence, not by my flesh.
,
,
,
Mike, unless you are a bot and not human, how ""AI being a threat to humanity"" can be good?
,
,
,
I wish ""Yes"" could be divided into ""A. Yes, and that is bad"" and ""B. Yes, and that is good"". I lean toward B, but voted ""No"" to avoid supporting A.
,
,
,
Peter Skomoroch , tweeted:
you know voting bots will just rig those poll results to be 99% ""No"" ;)
,This is funny, but I did detect bots voting at previous polls - usually at the direction of their human masters !
,
,
,
 ,  "
"
Most popular 
, tweets for Jan 05-06 were
,
Great post: #DeepLearning in a Nutshell: what it is, how does it work, and why you should care 
, 
,
,
,
Great post: #DeepLearning in a Nutshell: what it is, how does it work, and why you should care 
, 
,
,
#BigData is visualized in so many ways..all of them blue & with numbers & lens flare. 
, #Tumblr 
,
,
,
Great post: #DeepLearning in a Nutshell: what it is, how does it work, and why you should care 
, 
,
,
,  "
"
,
,
We invite you to take part in one of the following shared tasks:
,
,
This task studies source retrieval and text alignment, i.e. retrieving likely
sources of a suspicious document, and aligning reused passages of text between
them.
,
,
This task focuses on authorship verification and methods that tell us whether
two given documents have the same author; a frequent challenge of forensic
linguistics.
,
,
This task is about predicting an author's demographics from her writing.
It will include various data sets, including reviews and Twitter tweets.
Besides age and gender, a new data set will be introduced to study the
predictability of an author's personality.
,
Learn more at ,.
,
PAN is held in conjunction with the CLEF'15 conference in Toulouse, France.
,
,
,
,
,
,
,
,
,This year PAN will accept data submissions for text alignment so as to increase
the diversity of the available corpora. Further details will be announced on
the task's web page.
,
,
,
After a great success with 106 submitted softwares from previous years, we are
happy to announce that we will continue our initiative to invite software
submissions using TIRA.
,
,
,For each task, we accept applications of PhD students or master students for
mentoring by experts in the respective task's research field. Deadline for
sending in your applications is February 15, 2014. Please include a resume
and an abstract of your PhD's topic.
,
,
,Submitting your software or your notebook early, as well as registering early
for the conference will be rewarded. Check out the specific benefits on
our web page at ,
,
,
,  "
"
,
,
, was held by Innovation Enterprise in Chicago during Nov 12-13, 2014. It provided a platform for leading executives to share interesting insights into the innovations that are driving success in the world's most successful organizations. Data scientists as well as decision makers from a number of companies came together to learn practical predictive analytics from top companies like Amazon, Twitter, Verizon, Microsoft, etc. Industry leading experts shared case studies and examples to illustrate how they are using analytics to innovate in their organization.
,
Here are highlights from Day 1 (Friday, Nov 12):
,
,, Director, Experimentation & Algorithms for Growth & Targeting, Netflix talked about Quasi-Experimentation at Netflix. Three main focus areas of the company are: Content, Product and Marketing. She leads a team of data scientists who build algorithms and design and analyze experiments in the areas of original content promotion, signup flow optimization, messaging, fraud, customer service and marketing.  
,
Netflix Experimentation has a strong foundation of product innovation methodology with goal to maximize revenue. Majority of product experiments are run as A/B tests. A fundamental principle of A/B testing is user-level random assignment into distinct test and control groups. What if one wants to measure the impact of outdoor media vs. other types of local spend like radio or print?  How can one answer such questions within the construct of an A/B test? He/she cannot. These questions are answered via quasi-experimentation in which different time periods, different regions, or a combination thereof serve as our ""quasi"" controls. 
,
Quasi-experiments need a clear learning objective and picking the right test & control regions is key. It also requires monitoring experiment throughout and adjusting raw results for historical differences.  She then discussed an example model, its components and validity. She concluded the talk showing concern about model improvement, inclusion of external data (weather, economic trends, etc), creation of R package and comparison to recently released Google's approach to quasi-experimentation analysis.
,
,, Chief Data Scientist at Mashable talked about how Mashable is developing proprietary technology and utilizing usable data, both from an editorial and sales perspective, to help deliver the best content to its engaged and growing audience. He started with giving a brief history of the organization. At present, they have about 40 million monthly unique visitors, 19 million social followers. Mashable's proprietary platform - Velocity predicts what's going viral next. Before Velocity, enormous amount of time of editorial staff was spent in where should one put stories on the layout. Velocity made this task easier. 
,
Discussing about functioning of Mashable Velocity, he mentioned three components: crawling algorithms, machine learning and data science. The crawler daily crawls more a million links a day. The crawler is very scalable and flexible. The list of viable domains for content are constantly augmented. 
,
While crawling, Natural Language Processing is used to understand what is at top of the content. Machine learning is used to make decisions to figure out how the layout should be designed and which stories to be placed where.   Data Science is used to model how content diffuses over web. He gave some interesting use cases of Velocity.  He concluded sharing some details regarding contract with MEC and 360i.
,
,, Director, Business Analytics, LinkedIn delivered a talk on ""The Predictive-First Revolution at LinkedIn"". He mentioned that the LinkedIn platform has about 332 million members with over 3.5 million companies and over 3 billion endorsements. Using extensive professional graph, rich content and engagement data, LinkedIn is quietly revolutionizing the sales and marketing world by creating a new generation of social selling and content marketing platform powered by machine learning and predictive modeling. 
,
LinkedIn started with mobile app to take into account user experience first and foremost. However, now they think predictive at first when designing user experiences and building apps.  Example: ""People You May Know"", ""Jobs You May Be Interested In"", etc. He briefly explained following three principles of ""predictive first"" giving few examples: 
1. Be relevant
2. Be helpful
3. Be everywhere
,
He also talked about Pinot, a distributed analytics infrastructure that serves interactive analytics products at LinkedIn. It uses compressed columnar indexes for indexing, Apache Helix for cluster management and Apache Kafka and Hadoop for ingestion. He concluded by mentioning ""We are investing in a new generation of social selling and content marketing platform to make sales and marketing more effective.""
,
,
,
,
,  "
"
,
,
,
,
If you do a search on the terms ""Big Data"", ""Data Science"", ""Predictive Modeling"" or ""Predictive Analytics"" you will get back millions and millions of hits. The terms, like many other business terms before them, have become commonplace in many strategic discussions.  For every organization having those discussions, the level of knowledge and understanding of what exactly ""predictive modeling"" means is varied.
,
This all came to light for me last year when I co-hosted an , with ,, a community of Enrollment Management Professionals. The topic of discussion was the use of predictive modeling in higher education. Everyone in the discussion was curious how they could benefit from it but also many wanted to learn more about exactly what predictive analytics is. The consensus during that discussion was, there was not a great resource to do self-paced learning on predictive modeling.
,
Out of that, an idea was born, which turned into our new twenty part video training series on Predictive Modeling. Rapid Insight is a predictive analytics software company- one of our biggest selling points is that we make it easy for anyone to do predictive modeling.With that said, we also know that it is still important to understand the concepts behind what goes into a predictive model so when people are using the outputs to make decisions, they are better informed. I liken it to driving a car- technically I don't need to know how to put gas in a car or how to fix a flat a tire or all of the details of how an internal combustion engine works to drive it, but understanding it certainly can enhance my experience.
,
With predictive modeling, knowing what goes into a model is also important as some of the predictions may be quite different than the conventional wisdom previously used to inform your business decisions. By understanding what is going ""under the hood"",you will be better able to explain the model's results. Related to this is the broader discussion around the importance of good data and what exactly to do with the predictions.  We have tried to cover all of these bases in the video series. There is something for everyone, from those just getting started to the seasoned data scientist.
,
The videos are designed so you can spend twenty minutes a day watching them and finish them all in a week. If you are more of a binge watcher, you can also get through all of them in one morning. For the hands-on learner who likes to ""do"" we also have some applied videos where you can get a free copy of our software and follow along.
,
Let us know what you think!
,
Caitlin Garrett, Senior Statistical Analyst
Rapid Insight Inc.
,
,
,
,
,
,
,
,  "
"
,
,
, is CEO & Founder of ,. Sharmila also serves on the Board of Hadapt, a big data company and on the Board of Lattice-Engines and is advisor to numerous software start-ups. Previously Sharmila served as Executive Vice President for Aster Data, where she led product strategy, all marketing functions, business development and inside sales. 
,
Prior to Aster Data, Sharmila Mulligan was Chief Marketing Officer of the largest business unit (BTO) within HP Software responsible for marketing a $3+ billion software products portfolio spanning all major aspects of IT management. Before joining HP, Sharmila served as Executive Vice President of Marketing at Opsware Inc., a publicly traded company and leader in the Data Center Automation market. Mulligan has worked as Vice President at Totality and Netscape communication. She has also held managerial positions at General Magic, Microsoft and HP?€?s PC business (PSG).  
,
She received her masters in management from the Kellogg Graduate School and her BA in business & economics and BS in computer science from Northwestern University.
,
Here is my interview with her:
,
, 
,
,: I spent early days at Cloudera and then Aster Data, both Big Data platforms. What ,was apparent is that customers were trying to move data from many diverse internal and external sources into new Big Data platforms to aggregate data in one place for holistic analysis. They would then try using a pre-existing BI tool to run analysis on masses of data. It simply didn?€?t work. Old BI tool architectures were never designed to work on large, diverse data volumes nor allow for fast business user exploration of insights. ,
,
,
,
,We?€?ve always been focused on fast, automated ,blending and harmonization of diverse data and faster user exploration of insights. That has not changed and we continue to innovate in these areas. What?€?s evolved, however, is inside our data processing engine, we use Apache Spark for fast processing and ?€?data harmonization?€? and we?€?ve deepened our investments in Apache Spark resulting in massive performance leaps when processing diverse data formats.
,
,
,
,ClearStory interactive and collaborative StoryBoards deliver highly interactive, collaborative insights to business users with the ability for users to explore and collaborate in real-time on insights at a deep level. The distinction between traditional dashboards versus Storyboards is that Storyboards update continuously as data updates, users can collaborate in real-time, and each Storyboard allows for very deep data exploration. , ,
This is not doable with dashboards that are pre-determined and often pre-wired to present a certain insight, outcome or KPI. Storyboards take data storytelling to a highly interactive, live view of insights. 
,

,
,
, 
,
We solve this with our data harmonization capability that speeds access to relevant data sources, harmonizes and blends them without needing long data wrangling cycles, and delivering these insights via interactive StoryBoards. The business users can themselves interact with and explore to quickly answer key questions. 
,
,
,
,Data Harmonization is our underlying technology built on Apache Spark that has built-in intelligence to read data sources, infer what?€?s in the data and then automatically blend them to reach answers and insights faster ?€? all without needing specialized IT skills. Anyone can now point the ClearStory system to sources of data, internal and external, intelligently select what sources they want to blend using the information provided by the system , click ?€?harmonize?€? and the system determines how to blend the sources and dimensions in each source and immediately displays a visual insight. 
,
,
, ?? ,
What used to take days or weeks, can now be done in hours. The visual insights themselves can be easily explored; and if they lead to new questions that require new data to be added to the analysis; new sources can be included and harmonized on the fly without needing the traditional data modeling steps ?€? that have been slow, complex and tedious for users.  
,
,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
It seems like almost every business-oriented article features something to do with big data. Most of these opinions claim it?€?s the key to revolutionizing the way business is done. Yet, despite our deeper understanding, and improved analytical tools, there are still a lot of misconceptions about big data.
,
It?€?s important to note that myths change with understanding. As we learn more, certain misunderstandings will fade away, but also give rise to new questions and concerns. The following are some of the more current myths surrounding big data.
, 
,
,
,
This is , and can be misconstrued two ways. First, only big companies are able to invest in the tools necessary to manage and analyze big data. Second, only big companies produce enough information to be considered big data.
,
It?€?s true that for a long time the information, tools and professionals were out of reach for smaller businesses. However, recent improvements in technology, like cloud computing and data management tools, have significantly decreased the price of storage and analysis. Also, there are plenty of options to outsource big data needs, which can help limit costs.
,
,
,
, be a great tool in learning new insights and developing better strategies. It?€?s not, however, a magical cure that?€?ll solve all your company's problems and make you the greatest business ever. It certainly won?€?t produce any overnight changes.
, 
Organizations need to temper their expectations remember that proper data implementation will take time and energy before yielding results. Companies need to make sure they have the right systems in place to leverage the insights big data analysis produces. Just collecting information and storing it won?€?t do anything. It?€?s only through proper analysis and decision making that you?€?ll learn the right insights to further your business objectives.
, 
,
, 
You?€?d think with how much people have been discussing the big data trend, and how many experts tout it?€?s ability to improve decision making, that most companies would be fully immersed. However, the reality is that many businesses are interested in big data, but , and adopted a solid, large-scale data strategy.
, 
If anything, this should serve as a powerful motivator for companies to get started. If the promises of big data are true, businesses who start taking the right steps to adopt the technology now will have a significant advantage over those who are procrastinating. This is particularly true for small or medium sized businesses who could really see significant improvements thanks to the tools recently made available to them.
, 
,
, 
A popular discussion in the realm of new tech improvements is whether or not 
,. Many argue that human emotion impedes good decision making, whereas a computer?€?s ability to make unbiased decisions makes them more accurate. Especially now as ,, computers can be taught to learn and make decisions without constant human oversight.
,
,
Bio: Rick Delgado,
,,
 is a technology commentator interested in enterprise technology. 
,
,
,
,
,
 ,  "
"
,
,
,San Francisco
,Jan 29-30, 2015.
,
The Deep Learning Summit is a unique opportunity to
meet influential data scientists, technologists, world-leading researchers,
entrepreneurs and data engineers all in the same room. Discover how advanced
deep learning will impact your business and prepare for the smart artificial
intelligence world.
,
The summit will take place in San Francisco, with
40+ experts discussing Neural Networks, Image Recognition, Language Processing, Advanced Deep Learning Algorithms, Artificial Intelligence, Machine Learning,
Big Data, and Computing Systems.
,
Speakers will include exciting new startups, leading
technologists and engineers, and world-class researchers from notable companies
and institutions including Google, MIT, Flickr, Emotient, MetaMind, University
of Toronto, Sentient, Metaio, Clarifai, Jibo and Stanford.
,
,
,
,??,
The practical applications of deep learning are
being found in security, manufacturing, healthcare and language processing, as
well as current levels of research in image recognition, text understanding and
more. Discussions at the summit will be at the cross section of leading
academic research and the start of a new advanced revolution in data science
and smart AI to apply DL to solve increasingly complex computational problems.
,
Confirmed speakers include:
,
,, 
is an entrepreneur and neuroscientist who brings cutting-edge vision algorithms to market and seeks to increase our knowledge of
the human visual system. His neuroscience work covers the full spectrum of
visual processing, from low-level image representation through high-level object recognition. 
,
,,
works on large scale deep learning. He will discuss his work's breakthroughs in
object recognition, speech recognition and language understanding, and how
vector representations can be used to solve word analogy or translate unknown
words between languages.
,
,, 
is a world recognized expert in the fields of
speech recognition, natural language, dialog, and human-machine multi-modal
interaction.
,
,, 
the world-leading Augmented Reality technology
provider. He and his team research deep learning techniques such as random
forests to track and augment the human body on camera images.
,
,,
is interested in developing new deep learning models that learn useful
features, capture compositional structure in multiple modalities and perform
well across different tasks. His research interests are machine learning for natural
language processing and vision.
,
,, 
is a pioneer in the field of machine learning and
was the co-inventor of deep belief networks whilst researching as a
post-doctoral fellow in the Hinton Group at the University of Toronto.
,KDnuggets readers can use the discount code , to receive 20% off tickets.
To get more information and register for the Deep Learning Summit, visit the
event page here: 
,
,
,
For enquiries, press passes and group bookings,
,please email:
,  "
"
,
,
, was held by Innovation Enterprise in Chicago during Nov 12-13, 2014. It provided a platform for leading executives to share interesting insights into the innovations that are driving success in the world's most successful organizations. Data scientists as well as decision makers from a number of companies came together to learn practical predictive analytics from top companies like Amazon, Twitter, Verizon, Microsoft, etc. Industry leading experts shared case studies and examples to illustrate how they are using analytics to innovate in their organization.
,
,.
,
Here are highlights from Day 2 (Thursday, Nov 13):
,
,, Director, Marketing Analytics, Time Warner Cable discussed how analytics can be used to optimize subscription price change.  Price changes due to promotion expiration can be described as ?€?natural experiment?€? i.e. TWC cannot perform a true test due to legal and operational restrictions. Also, the customer groups that get price changes are pre-defined by the promotion code on the account and generally cannot be changed by the company.
, 
She described what approaches TWC used while estimating incremental disconnects, upgrades, and downgrades attributed to the price change due to promotion expirations. The disconnects, when analyzed, show a substantial increase around the timing of a common price change point. However, the increase is largely explained by confounding factor, which is dominated by customer moves. 
, 
Segmentation helps TWC understand which segments are more sensitive to price change and based on this information, structure future promotional offers to minimize customer attrition. She concluded by sharing the following recommendations from her experience of analyzing natural experiments:
,
, ?? ,
,, Director, Marketing Analytics, AT&T talked about ""Predictive Analytics in B2B Marketing"". Digitization is changing marketing, especially B2B Marketing. By 2020, about 450 billion transactions would take place on the web with companies having more than 1000 employees storing 200 TB of data on average.  For business buyers, 90% of decision making takes places before first contact. Outbound marketing has to be more focused as the buyers are getting more informed and each transaction now involves more stakeholders. Marketing needs to evolve in the following three ways: 
,
, ?? ,
Segment using clustering (k-means, Gaussian mixture models, hierarchical clustering) in order to utilize rich data set from multiple sources to discover new and niche segments. The optimization of Conversion Funnel through prediction helps data driven prioritization, timely alerts and maximization of value. Methods such as logistic regression, SVM, Boosted Decision Trees can be used here. Sentiment index can be used for proactive support.  This can be done by tracking sentiment in near real-time, followed by anticipating & responding to needs. This process must also involve close-loop analysis with surveys. 
The secret sauce for B2B marketing is: 
,
, ?? ,
Among the above three, intentionality is most critical. It involves aligning analytics priorities with business strategy, identifying the most relevant business questions, starting small, testing and adjusting. Talent with strong business acumen and strategic perspective is key. In regard with culture, ?€?data-first?€? mindset within marketing helps a lot.
, 
,, Head of Predictive Analytics & Data Scientist Group, Verizon delivered a talk on ""Improving Customer Insights with Predictive Analytics: The Current State and Big Data Trends"".  She started her talk describing the huge volume of data Verizon holds. Talking about the current state of predictive analytics at Verizon, she emphasized that Verizon has a comprehensive view of customer as the analytical data set foundation. Verizon has an automated process consisting of: data collection, data preparation & enrichment, scoring and model performance monitoring. By integrating predictive analytics with business processes, her team handcrafted models to support strategic, tactical and event driven business decisions. They are constantly monitoring overall model and business results keeping ROI in mind.
,
She argued that the key to success is picking up the problem that everybody in the organization is interested in solving and that is high priority for your organization. With regards to Big Data trends, she mentioned that integrating unstructured data analytics with current data and processes is critical. Text data is heavily used for business insights and customer sentiment.  Current processes are creating an immense need for unified contextual view and centralized analytics. She mentioned and briefly explained the following challenges in predictive analytics:
,
,
,
,
  "
"
,
,
New York, NY
,January 19-21, $1,850
, $150 off for KDnuggets!
,
This course will demonstrate the fundamental concepts of machine learning by working on a dataset of moderate size, using open source software tools. It will also teach how to collaborate with data scientists and create code that tackles increasingly complex machine learning problems. 
,
This is a practical course designed for engineers with strong programming skills and exposure to linear algebra and probability. Students should also understand the basic issue of prediction and have worked in Python. 
,
For more information: ,  "
"
New entries for November.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Dec 2 and later.
,
See full schedule at , .
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Nov 24-30 were
,
Understanding The Various Sources of #BigData - Infographic , 
,
,
,
Starting data analysis/wrangling with R: Things I wish I'd been told ,
,
Linguistic Mapping Reveals How Word Meanings Sometimes Change Overnight: ""gay"" and ""sandy"" , 
,
,
,
Starting data analysis/wrangling with R: Things I wish I'd been told ,
,
,  "
"
Which Big Data companies generate the most revenues? Wikibon 
, based on sales from Big Data hardware, software and related IT services.
,
,
,
Original: ,
,
,
,
,  "
"
        ,
,  "
"
,
Latest ,, (Dec 03, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
""If you are going to use the word scientist in your title, you are going to be held accountable for it."", Sean McLure, ,.  "
"
        ,  "
"
By Gregory Piatetsky,  
,, Dec 2, 2014.
,
Here are upcoming December 2014 - May 2015 meetings and conferences. 
,
For the latest and comprehensive list see ,  page.
,
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,  "
"
,
,
,
,
,When it comes to mobile BI, adoption has been shockingly poor because it doesn?€?t usually work well with mobile devices. You can?€?t read data in depth on a mobile device, but rather you need to get to the point quickly. With everything shifting to mobile, the approach to BI will change. Rather than elaborate visualizations, you will see hard numbers, simple graphs and conclusions. For instance, with wearable devices, you might look at an employee and quickly see the KPI (key performance indicator). The BI game is about to change ?€? primed to go mobile this year.
,
,
Unstructured data has posed many obstacles in the past, but will come into its own in 2015. Text analysis will gain increasing traction, with web data, documents and images, with companies finally able to tackle unstructured data in meaningful ways.
,
,
,While Google Glass has not been embraced by the consumer market, it is starting to find a home with business applications for the technology. Police arresting suspects might have ID information projected in real-time or doctors might utilize it to review their??,. ??This will have ripple effects in the BI world, as an increasing number of professionals will require key data points presented to them in short precise summaries, as opposed to lengthy analyses and dashboards chock full of statistics.
,
,
,
,
,
BI will finally evolve from being a reporting tool into data intelligence that every entity from governments to cities to individuals will use to prevent traffic, detect fraud, track diseases, manage personal health and even notify you when your favorite fruit has arrived at your local market. We will see the consumerization of BI where it will extend beyond the business world and become intricately woven into our everyday lives directly impacting the decisions we make.
,
ABOUT SISENSE:??,??is a full-stack Business Intelligence and Analytics software that enables non-technical business users to join multiple large data sets, build??,??with great data visualizations, and share with thousands of users.
,
,
,
,  "
"
Most popular 
, tweets for Dec 01-02 were
,
Hilarious ! If programming languages were vehicles: on C, C++, Java, Python, Perl, Lisp, ...  , 
,
,
,
How Google ""Translates"" Pictures Into Words Using #DeepLearning, #BigData & Vector Math , 
,
,
,
How Google ""Translates"" Pictures Into Words Using #DeepLearning, #BigData & Vector Math , 
,
,
Hilarious ! If programming languages were vehicles: on C, C++, Java, Python, Perl, Lisp, ...  , 
,
,
,  "
"
        ,  "
"
,
,
,??,??
,  "
"
By John A. De Goes (SlamData), Dec 2014.
,
SlamData is an open source tool that makes analytics on MongoDB easy and accessible to developers and non-developers alike. We just launched v1.1, which greatly increases the power of the tool and fixes a number of issues identified in the 1.0 release.
,
,
,
MongoDB is currently the fastest growing and most successful NoSQL database. Companies are using the database primarily to build web and mobile applications.
,
Successful applications built on MongoDB end up capturing or generating large amounts of data. The process of understanding this data, which I call ,, is vital to multiple stakeholders in the business:
,
,
MongoDB does not have rigid schemas (every ?€?row?€? may have a different structure from every other ?€?row), and allows arbitrary nesting of data (?€?rows?€? can contain other ?€?tables?€?).
,
While this flexibility leads to faster application development and better performance and scaling properties, it comes at a cost: ,.
,
In the relational world, to answer these types of questions you?€?d simply use a data discovery and ad hoc analytics tool. But in the world of MongoDB, if you need Application Intelligence, you have , two choices:
,
,
Ultimately, neither approach is scalable, which is why we started ,, an open source project based on the premise that NoSQL data is ,, and analytics tooling needs to , with modern data.
,
,
,
SlamData provides a standard SQL interface to NoSQL data stored in MongoDB.
,
Every SQL query is executed 100% in the database (or in a replica set), and operates on the actual structure of the data.
,
This approach differs substantially from other solutions to the problem, which stream data from the database to handle complex queries, and which superimpose a fake relational view of the underlying data (even when it is not relational).
,
SlamData?€?s dialect of SQL (called ,) extends ANSI SQL to support nested data, heterogeneous data, and aggregation over nested dimensions (for example, summing elements in an array stored inside a row).
,
An example SlamSQL query is shown below:
,
,
,
In this query, documents which are doubly-nested in arrays are being used to filter and sum values in the overall result. This query would be impossible in an RDBMS, and the equivalent code for the MongoDB API would be very difficult to write, troubleshoot, and understand.
,
,
,
By leveraging industry standard SQL, SlamData makes it possible for a wide range of users and tools to interface with MongoDB, and helps teams quickly and easily understand the data generated or collected by their MongoDB applications.
,
In the current 1.1 release, all standard SQL clauses are supported, including SELECT, AS, FROM, JOIN, WHERE, GROUP BY, HAVING, OUTER JOIN, CROSS, and more.
,
,
,
,
,
The SlamData project innovates in several key ways:
,
,

The combination of these features make SlamData ?€?point and query?€?: point SlamData at your MongoDB database, and do , you want on , of data. SlamData will generate the optimal query plan and execute it 100% in the database.
,
,
,
If you are using MongoDB and would like to try SlamData, you can find ,, or you can , on Github.
,
SlamData is a 100% open source project, so if you like what you see, please consider supporting the project in various ways:
,
,
We also have a newsletter you can sign up for on ,. Enjoy!
,
,, is a founder and CTO of SlamData, and a contributor to the open source SlamData project. Previously, he was General Manager of DataMesh, Principal Architect at RichRelevance, and CEO/CTO of Precog.
,
,
,
,  "
"
,
,
Turning something raw into something industrially valuable has always required 2 things; science and engineering. The science is our attempt to explain and predict the behavior exhibited by some complex system and capture those explanations in the form of testable models. The engineering looks to mechanize those modeled concepts into useable tools that make a direct impact on society.
,
,
,
The science piece requires more than simply scaling our existing machines into this new world of rich and varied data. This new world represents entirely novel complex systems that we need to understand. In order to produce value we need models of this complex behavior so we can capture those new-found concepts in our machines.
,
Enter the Data Scientist; a new kind of scientist charged with understanding these new complex systems being generated at scale and translating that understanding into useable tools.
,
Unfortunately, along with this increased demand for 'science on data' is an accompanying ambiguity with regards to what it means to be a data scientist. We need to settle once and for all the term and focus on developing the right skills and attracting the right talent. We need a self-consistent 'guiding light' that gets us past the hype and the demand, and distills for us what it really means to be a data scientist. It turns out, the answer is right in the title.
,
,
,
Regardless of what industries are currently fueling the demand, or what skill sets happen to be sexy today, there is one thing that cannot be argued; if you are going to use the word scientist in your title, you are going to be held accountable for it.
,
This has been the case throughout all of scientific history. Before our traditional fields of science laid out their foundations in self-consistent theories they were not considered what we now call science. What makes something scientific is attaching the phenomena you are studying to some self-consistent model that is independent of opinions or subjective interests.
,
,
,
In ,cases of science, ,and it is ,activity that gives us our definition of doing modern science.
,
,In order to attach the word science to data we must show that data can represent a complex system that exhibits behavior, and that we can explain and predict that behavior using our instruments of choice; namely, computers.
,
,
,
We can enter into the debate about what exactly we mean by ""complex"" but for the sake of any practical argument we can say that any system producing unobvious behavior is complex. In other words, if some phenomenon produces behavior in a way that is not immediately obvious, it requires a simplified approximation, a model, to explain and predict how it achieves that behavior.
,
The data we collect from sensors, websites, detectors, and any other device is being generated from phenomena. We are organisms on a planet interacting in complex ways producing behavior that is anything but obvious. Its 'unobviousness' is why we look to scientists to try and figure out how that behavior was manifested because that discovery is what makes building new technology that acts on data possible. Therefore, data does indeed represent a complex system worthy of modeling.
,
,
,
Can we really build models from all these data we are collecting? Well, of course. After all, this is no different from any other science. All sciences collect data whether it is from butterfly collections, particle accelerators, chemical analyses, MRI imaging, or disease propagation. Data are simply recorded activity that was generated from some underlying complexity. To build a model means turning those 'recordings' into a consistent collection of testable concepts that explain and predict the activity we are observing.
,
,
,
The majority of individuals who have entered into data science are indeed full-blown scientists looking to apply their skills outside the ivory towers; if data science wasn't considered a real science that would be a lot of scientists all-of-a-sudden reconsidering what they find interesting and important.
,
So what's the problem? Why is it becoming difficult to identify what it means to be a data scientist? One word. Demand.
,
,
,
Science has become the ""cool"" kid in the real world and it shouldn't be surprising why that is. If what we value in the information age is the ability to convert our new ""oil"" into something different, something intangible, it will require, like every industrial innovation before us, a deep understanding of the mechanisms underlying that oil's behavior.
,
But with the increase in demand comes an uncertainty as to whether or not science is actually taking place. Are testable models being built using real research on the underlying complexities that an organization is attempting to understand and anticipate? Are the models employed actually mapping an algorithmic approach to the pain points of the organization, or are they simply the ones that came with a scaled recommendation engine?
,
And so we are in need of that 'guiding light' to move us past the hype of vendors, and the diluting power of demand. We need to educate organizations that are looking for talent on what a data science resume should look like. We need to understand data science for what it is, not what we want it to be.
,
,
,
As a data scientist your allegiance is to science; not machine learning, statistics, database technology, or business practices. All of these are critically important but not the 'guiding light' that leads us to objective discovery. We cannot equate data science to a particular discipline or tool, because it is the ,that make us scientists. Tools, practices and languages can be learned, but having a passion and mind for discovery and experimentation is how science has always moved forward.
,
,
,
As we move forward and lay the foundations of data science we must ensure that we ,to the title of scientist. We must frame the building of new products and new technologies as something that requires real research. We must instill in organizations and practitioners alike that the only way to produce value, compete effectively, and invest in long-term solutions is to understand the complexity of the markets in which we compete. The ambiguity of the term data scientist is only superficial, and merely a byproduct of hype and demand. If we stick to the science, the ambiguity falls away to a solid approach to understanding behavior and building tomorrow's exciting products.
,
,, PhD, is a data scientist at ThoughtWorks, where he assists organizations looking to compete analytically by applying advanced statistical and mathematical modeling.
,
,
,
,  "
"
,
,
A City Traffic Dashboard based on Social Network Data
,
About the Challenge:
,
Imagine Indian Government has made you responsible for leveraging
Internet and Social Media portals for better monitoring and management
of traffic in Indian cities. You know that the popular social network
portals are daily fraught with traffic issues from Indian roads in the
form of text, image and video. Further, city traffic authorities have
started to feed into these portals furnishing them with information
about traffic movement (or the lack of that). However these desperate
pieces of information spanning over timelines of innumerable Indian
commuters have not been harnessed to the effective monitoring and
management of traffic.
,
You would like to design an application which can give real insights
from the fast flowing streaming data on these platforms - what is
working good and what is not; which areas are facing severe traffic
problems; what are the most pressing traffic related problems citizens
in different cities are complaining about (e.g. wrong_parking,
heavy_vehicles, auto_refusal etc.); how things have changed after
additional traffic management people have been introduced across
cities last month and so on. A city-based as well as a national
interactive dashboard with great visualization would help you to
easily interact with the data. It will enable you to get views at
different levels upto actual text/image/video as well as temporal
analysis of data.
,
The CODS 2015 Research Challenge is giving you an opportunity to
design and develop innovative software application (or dashboard) for
city traffic management using publicly available social network data.
,
Technical Task:
,
Design and develop a software application (or dashboard) for
analysing, monitoring and comparing traffic issues in different Indian
cities based on relevant interactions on Internet based social network
portals. Unlike common data analysis challenges, this is an open ended
and exploratory task.
,
Data:
,
Given the exploratory nature of this challenge we are fine with the
participants to decide and use any relevant social network portals as
their data source(s). At the minimum, they need to use the Facebook
pages of Traffic Authorities of following cities - the posts (and
other details) from these pages can be crawled using Facebook Graph
API
,
,
Participants are free to include any publicly accessible information of the following nature.
,
,
,
Rules:
,
,
Interested participants are encouraged to get started immediately ?€?
,
First deadline is 15 Jan, 2015: Each team has to submit a plain-text abstract of length less than 500 words describing their application, features and data sources used.
,
For more information, including awards, visit ,
,
Contact:
,
,
,
,
,  "
"
,
Grand Data Challenge of 
,
Mar 31???€? Apr 3, 2015,
Washington, DC, USA,
,
,
SBP is pleased to offer a challenge problem again in 2015, and we expect it to build on the successes of previous year's challenge problems. ??This year's SBP Grand Challenge problem asks participants to consider the following question, ""how can we use publicly available data on the web and elsewhere to find social inequality and to aid the disadvantaged?""
,
,
,
From the Arab Spring to the recent Gamergate scandal, the use of social media in understanding and mitigating social inequalities and prejudice has increased at a rapid pace.?? At the same time, data used for decades to study the ways in which social inequalities permeate every facet of social structure have become increasingly accessible for researchers to access.?? While many have taken advantage of these resources to produce new and interesting approaches to understanding social inequalities and working to prevent them, there is much interesting and useful work to be produced along these lines. For example, the following questions may be of interest:
,
,
These are by no means the only questions of interest, and are only intended to give a rough idea of what might be an interesting topic to explore for this challenge problem.
,
,
,
All submissions must be in the form of a 4-6 page single column paper with a minimum font size of 10. ??Note that only the abstract will be included in the proceedings, and thus should not conflict with any concurrent or future submissions of this work to other venues. Submissions will be judged by a based on their novelty, adherence to relevant social science literature, and technical rigor.
,
All submissions must be made to our submission site, ,.
,
,
,
Paper Submission - January 20,, 2015,
Winner Notification - February 28,, 2015,
SBP 2015 Conference??- March 31, ?€? April 3,, 2015
,
,
,
,
Last year, the winner received a cash prize as well as a travel reimbursement to present their work at the conference. This year will likely be much the same. We will update this portion when the prizes have been finalized.
,
,
,
We have provided some sample datasets to get contestants started on their submissions. These datasets are merely intended to provide a starting point, and are not required for the submission. Contestants are encouraged to provide their own datasets for the community. All of the datasets that follow are available on the SBP Grand Challenge website (,):
,
,
,
,
Please direct all questions to the SBP Grand Challenge Committee at ,.?€?
,
,
,
,  "
"
,
,
, has been dealing with big data for over 20 years. From calculating financial risk for Salomon Brothers to tracking movements of millions of items across the supply chain for major brands, Mr. Elbert pushed innovative data analysis to new frontiers.
,
As VP of Quantitative Analytics for Barnes & Noble Mr. Elbert used a plethora of data to offer his customers a unique in-store and digital experience.
,
Having joined , as Principal Data Scientist, Mr. Elbert is supporting Gilt?€?s mission to create the most exciting, curated shopping experience that helps company?€?s customers find and express their style.
,
Here is my interview with him:
,
,

,

,: , is a membership-based online retailer that provides instant insider access to top designer labels. Our members find something new every day for women, men, kids and home as well as exclusive local services and experiences. Analytics plays a key role in understanding customer preferences and behavior. Analytics also helps with merchandizing, pricing, measuring and optimizing sales performance, etc.
,
,

,

, We use orders, click-stream, and marketing data to optimize offerings, presentation, and communication with our members.
,
,

,

, The biggest challenge is that every day we create new sales using products we never sold before. There is no previous sales history for these products to use, for example, when making pricing decisions. Most sales are over in 36 hours so the window of opportunity to learn and adjust is very short.
,
More about it??,.
,
,

,

, We look at conversion rate, click-through rate, spent over time, revisit, activation, and retention rates and other typical metrics.??These metrics are analyzed by customer segments, and cohorts to better understand underlying drivers. We also calculate brand and category affinities, cart abandonment and return rate, etc.
,
,
,

, We definitely noticed changes in behavior patterns. Some platforms are used more during the daytime, some more over nights and weekends. Web and mobile users have distinct??preferences??and respond in a different on communications channels.
,

,

, We come across unexpected insights very often but can share only a few. For example, recently we looked at average heel size by state. Some states surprised us. For more details click ,??or look at fashion blogger's amusing take on it ,
,
,
,
, When I read 'Liar's Poker' by fellow Salomon Brothers alumnae Michael Lewis it spoke to me. I left Wall Street and never looked back.
,
,

,

, In the order of importance:
,
,
, ,
,
,



, I re-read , recently and would strongly recommend its four books to anyone dealing with data. "","" is a good start. I think it should be a required reading to anyone dealing with data.
,

,
,  "
"
By Benn Stancil (Mode Analytics), Dec 2014.
,
As an analyst at a , building tools for data analysts, I spend a lot of time talking to people about the questions they're trying to answer with their data. Surprisingly, analysts from very different companies have strikingly similar questions: we all want to know what drives retention, how customers interact with products, how sales pipelines are performing, the lifetime value of a customer, and a few other core questions.
,
We've worked on many of these questions ourselves, and produce reports and visualizations that help answer them. But if we've already done much of the work to answer common questions, why should others start from scratch? Can analysis be open-sourced, like so much software is today?
,
The , is the start of our effort to find out. Each Playbook report shows how we've answered a question like ?€?,?€? or ?€?,?€? and includes a SQL query and visualization. They're built on top of an example users table and events stream?€?a data structure that's common in many companies. If you have a SQL database with these two concepts, you can make a few simple changes to the reports we provide and have access to the same set of analytical tools we've built over the last year.
,
,
,
Nevertheless, businesses and products are nuanced and the analytics tools that support them should be, too. Analysis needs adjustments and additions from domain experts?€?and in nearly every case, that's you, not us. For this reason, we exposed every step our work, including the SQL query and HTML, CSS, and Javascript code that powers the visualization. This not only makes data manipulations and aggregations completely transparent, but also makes them infinitely customizable.
,
But we know we didn't get everything right for everyone. Other methods could be more insightful, more efficient, or more robust. Analysts at , added additional metrics to the , that tracks how users move through a website. , helped us cut 20 lines out of a ,. We're very grateful for these collaborations and hope they are just the beginning.
,
,
,
We plan to continue to open sourcing other internal analyses of growth, A/B testing, and finance metrics. If you have comments, suggestions for improvements, requests for additional reports?€?or best of all, analysis that you too would like to open source?€?please email me directly at ,. By sharing some of our ideas, we hope that we can encourage conversation around analytical methods?€?and help turn common questions into answers for anyone with a few tables and a bit of curiosity.
,
, Benn Stancil is the Chief Analyst of Mode Analytics, a company building collaborative, SQL-based analysis tools for analysts working with both public and private data. Prior to Mode, Benn was a senior analyst at Microsoft and Yammer, where he helped lead product analytics. Benn also worked as an economic analyst at the Carnegie Endowment for International Peace in Washington, DC.
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Dec 6, 2014.
,
I recently read (and tweeted) a funny post
,
,
,
,
which compared languages and cars. It suggested that 
,
,??,
,
Continuing along this line of thought, what would be the main 
,
,
I can think of R as a flying car - very powerful, but only experts can handle it.
,
,
,
,
,
SQL, born in 1970s, would be a classic 1970-s Ford Mustang. 
No modern gizmos, but still fast and reliable, if properly maintained.
,
,
,
,
,
SAS may be a corporate luxury car - perhaps a Lincoln Continental or Mercedes.
Great car, but expensive. 
,
,
,
What would be your nominations for languages as cars? And if SQL is a Mustang, what then is NoSQL?
,
,
,
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
Most popular 
, tweets for Dec 3-4 were
,
Google funds ""Automatic Statistician"" project, an artificial intelligence #AI for #DataScience ,
,
Google funds ""Automatic Statistician"" project, an artificial intelligence #AI for #DataScience ,
,
Google funds ""Automatic Statistician"" project, an artificial intelligence #AI for #DataScience ,
,
Data Science Ontology, visualized , 
,
,
,
,  "
"
        By Gregory Piatetsky,  
,, Dec 6, 2014.
,
,, professor of Database and Information Systems at the Goethe University Frankfurt, Germany, a leading database expert, and the Editor of
, portal for Big Data and New Data Management Technologies, recently gave a talk at Stanford
,
,
,
Here are the 
,.
,
,
,I review how Big Data is enabling a data-driven economy, look at what to do with Big Data, and look at the consequences  of a society being reshaped by ""systematically building on data analytics"". 
Then, I outline some of the Big Data research challenges in three areas: Data, Processes, and Management. I will conclude making a case for Big Data for Social Good - Big Data can be leveraged to better serve the people who generate the data, and ultimately the society in which we live.
,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Dec 9 and later.
,
See full schedule at , .
,
,  "
"
As part of our series of 2015 predictions, here are Tableau Software's top 10 predicted trends in business intelligence for 2015:
,
,
Original: ,
,
,
,
,  "
"
        ,  "
"
Cities around the world are increasingly making urban data freely available to the public. But is the content or structure of these vast data sets easy to access and of value? A new study of more than 9,000 data sets from 20 cities presents encouraging results on the quality and volume of the available data and describes the challenges and benefits of analyzing and integrating these expanding data sets, as described in an article in ,, the highly innovative, peer-reviewed journal from ,. The Open Access article is available free on the , website.
,
In the article 
,, , and 
,, IBM Research, Brazil, and , and , New York University School of Engineering and NYU Center for Urban Science and Progress, NY, present several promising findings. These include a steadily increasing volume of open urban data, the ability to integrate different data sets, and the finding that much of the available data is published in standard types of formats. The authors also discuss the main challenges that make it difficult to take full advantage of these data sources.
,
,
,
, says , Editor-in-Chief Vasant Dhar, Co-Director, Center for Business Analytics, Stern School of Business, New York University.
,
,
,
,  "
"
By Arun Swami, Nov 2014.
,
,
,
The conference was well attended and packed with interesting tutorials and classes. There were often multiple classes tutorials/classes at the same time that all looked very interesting but one had to make a choice! 
,
,
The major takeaway from the conference was the rapid momentum of 
,  in the Hadoop ecosystem. A number of speakers said that Spark-Core was reasonable mature and could be used to processing of up to 100 nodes and 1 TB data without any problems. The newer modules in the Spark family (SparkStreaming, SparkSQL, MLLib, ...) can be used and are well along but may become easier to use over the next 6 months.
,
On the first day, Sameer Farooqui (Databricks) led a superb tutorial/hands-on lab on Hadoop Fundamentals. He covered a number of technologies in the Hadoop ecosystem. The lab is available under the Creative Commons license at ,. 
He shared that Databricks had chartered Paco Nathan to be developer evangelist for the Spark ecosystem. Their goal is to reach and train 100K people on Spark technologies over the next year!
,
On the second day, the session by Dean Wampler on Big Data Programming in Spark tried to illustrate how applications could be developed fairly easily in Spark using the Scala programming language. Spark is written in Scala and while Spark has Scala, Java and Python APIs, Scala is the language of choice for working with Spark.
,
Gloria Lau (Timeful) gave a well attended keynote on ""How do we build Data Products in the Right Order"". She outlined a number of rules of thumb to approach building data products well. She suggested two questions to ask:
,
,??,
She suggested that Donald Knuth's quotation 
,
as a guideline for deciding what to work on next in building data products.
,
The talks on ""Spark Streaming"" and using ""GraphX for graph analysis on top of Spark"" were thought provoking but indicated that the technologies were still in flux.
,
Krishna Sankar's two part class on ""Machine Learning in Python using Spark"" were well prepared and delivered. The class conveyed a lot of information that allowed people to see how they could using Spark MLLib to perform common machine learning tasks including data wrangling. 
Here are 
,.
,
There were exhibits from a number of vendors including Splice Machine, Actuate, Aerospike, Data Torrent, and Voltage Security.
,
, is a Bay Area entrepreneur and tech leader, who created innovative systems using text mining, ranking algorithms, heuristic approaches, data mining, personalization technology, database algorithms and optimization algorithms. 
Arun was a key member of the team that started IBM's research in data mining and has published seminal work in this area. His classic data mining paper with Rakesh Agrawal 
, is ranked among most cited CS papers.
,
,
,  "
"
By Roberto Battiti, Nov 2014.
,
,, developed by LIONlab, is a comprehensive , tool ,.
,
Some checks are omitted for complete freedom in training and experimentation, as appropriate in academic and research environments.
,
It is appropriate for researchers and practitioners who ""know what they are doing"" and like to push the envelope of the LION way.
,
In addition to the ML and optimization tools, advanced users adopt it for , heterogeneous components. Orchestration deals with automating processes, with the arrangement, coordination, and management of complex software components connecting data, experiments, simulators, models, decisions.
,
In particular
,
,
Many use cases and samples from the , take advantage of this software's powerful and simple interface.
,
,occurs via the 
,, which allows advanced users to add support for additional languages with limited effort.
,
LIONoso also supports the ,, including sample exercises.
,
You can download and run LIONoso 2.1 at ,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,  
,, Nov 3, 2014.
,
Here are 44 upcoming November 2014 - May 2015 meetings and conferences. 
,
Top meeting cities are:
,
,??,
For full list see ,  page.
,
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
  "
"
New entries for October.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
Most popular 
, tweets for Oct 31 - Nov 02 were
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Nov 4 and later.
,
See full schedule at , .
,
,  "
"
,
,
You Can 
,
,
,
,
Discover the power of tree-structured data mining during this popular introductory seminar, geared toward statisticians and IT audiences who are interested in understanding the conceptual basis of decision tree technology - what is it, why it works, how it has been used, and how it can help you make better business decisions. 
,
,
,
Using real-world datasets we will demonstrate Stanford Professor Jerome Friedman's advances in nonlinear, regularized-linear and logistic regression. This workshop will introduce the main concepts behind 
Generalized PathSeeker (GPS) and Multivariate Adaptive Regression Splines (MARS), a nonlinear automated regression tool
,
,
,
This workshop discusses key algorithmic details of Breiman's Random Forests and Friedman's TreeNet, and important extensions to bagging/boosting technology. We will show how the software is used to solve real-world problems, cover theory, discuss what is novel, illustrate how to select an ideal balance between model complexity and predictive accuracy, and show where the software fits in terms of other data mining software.
,
Learn more and register at 
,  "
"
,John Wanamaker, considered the father of modern advertising, said: ""Half the money I spend on advertising is wasted; the problem is I don't know which half."" 
,
Today, we can ask the same question of a company's investment in business intelligence (BI), analytics, and big data. 
,
Even after doing their best for over 20 years to build centralized, scalable information architecture, Forrester surveys always find that only a small percentage of organizations' data is actually converted to useful information in time to leverage it for better insight and decisions. At both strategic and tactical levels, much of this quagmire can be explained by the fundamental disconnect in goals, objectives, priorities, and methods between technology management professionals and the business users they should ideally serve.
,
,
,
Read more now: ,
,
,  "
"
,
,
As we begin launching our first competitions, we thought it would be a good idea to lay out what exactly we're trying to do and why.
,
,
,
And there are a lot of people interested in changing the future. Amazon wants to increase the number of goods you order through their site, so they predict which ones you might want to buy next and when. Twitter wants to boost your use of their platform, so they predict which tweets you will ignore and which you will engage with. Facebook and Google want to increase the number of ads you click on their sites, so they predict your personal click-through behavior. They have gotten very good at making these predictions.
,
But there are many other reasons to want to change the future. Educators want to increase the number of students graduating high school. Health workers want to improve the overall health of a population at a sustainable cost. Microlenders want to give more individuals in the developing world a chance to pursue their dreams without incurring default. Conservationists want to curb our energy usage without hampering productivity. Governments want to prevent fires from destroying lives and property.
,
This is where we come in. In the quest to make these difficult but important changes, we must arm ourselves with state-of-the-art predictions. Predict which students are likely to drop out before graduation back when they are in junior high, so teachers can intervene earlier. Predict which individuals will be able to repay their microloans, even when banks have shut them out in the past. ,, and get there first.
,
,
,
,
,
In today's world, the people who can make these predictions better than anyone else are data scientists. They are the modern day fortune tellers, but instead of crystal balls they wield datasets. Armed with skills in statistics and computer science, the data scientist takes large data sets and builds smart, creative, flexible models for what is likely to happen next. In 2011, there was more data produced than in all the previous years of human history combined. The quantity and variety of data available to us is exploding, and the individuals who can manipulate and illuminate these data have incredible value to offer.
,
, We host online challenges, usually lasting 2-3 months, where a global community of data scientists competes to come up with the best statistical model for difficult predictive problems that make a difference.
,
Just like every major corporation today, ,. And just like those corporations, they are trying to figure out how to make the best use of their data. We work with mission-driven organizations to , that they care about answering and can use their data to tackle.
,
,
,
,
,
Then we host the online competitions, where experts from around the world vie to come up with ,. Some competitors are experienced data scientists in the private sector, analyzing corporate data by day, saving the world by night, and testing their mettle on complex questions of impact. Others are smart, sophisticated students and researchers looking to hone their skills on real-world datasets and real-world problems. Still more have extensive experience with social sector data and want to bring their expertise to bear on new, meaningful challenges - with immediate feedback on how well their solution performs.
,
Like any data competition platform, we want to harness , combined with the increasing prevalence of large, relevant datasets. Unlike other data competition platforms, our primary goal is to create actual, measurable, lasting positive change in the world with our competitions. At the end of each challenge, we work with the sponsoring organization to integrate the winning solutions, giving them the tools to drive real improvements in their impact.
,
,
,
Equipped with good predictions, we have opportunities to change the course of our planet that we've never had before. We want to tackle the hairiest, most challenging, and most meaningful problems in the world today. We are building a community of data experts that can take them on. ,
,
,
,
We have launched and we want you to ,!
,
If you want to get updates about our launch this fall with exciting, real competitions, please sign up for our mailing list , and follow us on Twitter: ,.
,
If you are a data scientist, feel free to , and start playing with our first sandbox ,.
,
If you are a nonprofit or public sector organization, and want to squeeze every drop of mission effectiveness out of your data, check out the , on our site and ,!
Peter Bull is a co-founder of DrivenData.
,
Original: ,
,
,
,
,  "
"
,
,
New York, NY
,November 10-12, $1,850
, $150 off for KDnuggets!
,
This course will demonstrate the fundamental concepts of machine learning by working on a dataset of moderate size, using open source software tools. It will also teach how to collaborate with data scientists and create code that tackles increasingly complex machine learning problems. 
,
This is a practical course designed for engineers with strong programming skills and exposure to linear algebra and probability. Students should also understand the basic issue of prediction and have worked in Python. 
,
For more information: ,  "
"
,
,
,??,??
,  "
"
        ,  "
"
,
Latest ,, (Nov 05, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
Spectrum: What adverse consequences might await the big-data field if we remain on the trajectory you're describing? Michael Jordan: The main one will be a ""big-data winter."" After a bubble, when people invested and a lot of companies overpromised without providing serious analysis, it will bust. ,   "
"
, is a course designed to follow ,, providing a deeper look at the tools and techniques of Weka. The course emphasizes techniques for using Weka, and students will make use of huge datasets during the course.
,
The 5-week course opened October 20, 2014. Students should have completed ,, or have equivalent knowledge of the subject.
,
The course includes:
,
Subscribe to the , for updates and reminders.
,
For more information, visit ,.
,
,
,
,  "
"
By Sheamus McGovern, Nov 2014 (special report for KDnuggets).
,
Making the annual trip to STRATA + Hadoop World can be an expensive undertaking, however, for many, the trip to Strata is a necessary part of being part of the big data scene and immersing oneself in the latest data trends.
,
,, 
,Tools and Techniques that make data Work, 
,Oct 15-17, 2014. 
,New York, NY, USA
,
This pilgrimage has gone global, with Strata conferences now in New York, San Jose, London, and Barcelona. By the numbers, the NYC Strata was certainly an impressive event, with 5,500 attendees and around 135 exhibitors at the massive Javits conference center. The Thursday talk series alone consisted of ten keynotes and sixty-seven talks--eighteen of which were sponsored. Friday had similar numbers.??Thus the three-day data extravaganza is a feast for the mind for any data scientist or data professional!
,
The event kicked off Wednesday with tutorial sessions that continued to be a big draw with over a dozen on offer and many whole-day workshops.?? Sessions included a full day Spark camp, which was introduced this year and reflects the momentum Spark has gained over the last year or so. R, Python and D3.js continued to have a strong representation.
,
However, it was Thursday when things really got into high gear with about ten keynotes stretched over an hour and a half.??The keynote format guaranteed that things were kept interesting by restricting each talk to fifteen minutes and ensuring an impressive lineup of speakers. That?€?s about enough time to deliver a meaningful take away. Keynotes are a bit like icing on a cake - not essential, but leaving a pleasant taste in your mouth, especially when the speaker speaks to your inner data scientist.
,
You can review the keynotes for yourself at??,
,
Once the keynotes were complete it was on to what for many was the main event: the talks themselves.????This offered quite the dilemma.????For any given hour attendees had a choice of eleven different talks to choose from.????Quite a few talks were by vendors, although the conference app did give you the choice of filtering these out.????Thus one had to be quite judicious in choosing which talk to attend.
,
Obviously, many talks had a Hadoop big data focus, but others were focused on the less technical side, such as ?€?The Great Debate; If You Can?€?t Code You Can?€?t Be a Data Science?€?.?? Interesting as that talk may have been, I decided to give it a skip since I?€?ve listened to that debate many times before and do consider myself a decent coder so already know that answer to that one ;)
,
Instead, I sunk my teeth into a presentation by Goldman Sachs & Co entitled ""How Goldman Sachs is Using Knowledge to Create an Information Edge?€?.?? The talk was presented by Peter Fems, who works in GS compliance.?? Compliance has a big role in any financial institution, especially when it comes to identifying potential conflicts of interest.?? Peter did a thorough overview of the GS architecture and how they use graph databases to show relationships, for example, between a trader, research analyst, and banker working on a deal.????It was a compelling use case for nodes and weighted relationships that graph databases were built for.????The key takeaway was that one can have many raw outputs for analysis but sometimes you just need to see where those paths take you.
,
The next talk I attended had the enticing title of ?€?Big Data Architectural Patterns?€?.?? The title was about the best part of the talk. ??Going to a vendor talk is a bit like a recently converted vegetarian going to a steak house.?? It may look good, but if you are not using that platform then chances are you are not going to touch it!?? This was sponsored by SPLUNK, the makers of the powerful analytics platform. Splunk is an impressive platform, especially with its offering of Hunk, which is basically Splunk analytics for Hadoop.????Not having used the platform myself, the talk was somewhat interesting but did make me realize I needed to focus on non-vendor sessions.
,
Having learned that lesson, I next attended Greg Rahn?€?s talk.????He did an insightful take on comparing open source SQL-on-Hadoop.?? He reviewed various benchmarks, including the ""Big Data Benchmark""??
,.?? 
,
He made the observation that one should distinguish between benchmarking and ""benchmarketing"" and entertainingly quoted what he referred to as Gregorio's Benchmarking Theorem that 
,
????That was one of the most observant quotes I?€?d heard in a while!
,
The exhibitor space could only be described as impressive, with 135 exhibitor booths of various sizes.????Truth be told, I?€?m usually more interested in the talks than conversing with vendors.????However, the better booths are usually staffed with experts in addition to the usual sales and marketing teams.????I had an illuminating conversation at the MongoDB booth with Edouard Servan-Schreiber, their Director for Solution Architecture, regarding MongoDB performance.
,
At the MapR booth I had the chance to speak with the always impressive Ted Dunning, who is their Chief Application Architect.??We chatted a little about his new book, Time Series Databases, which he co-authored with Ellen Friedman.????Apart from MapR, Ted is also involved with such great projects as Apache Mahout, Drill, and Zookeeper, and also a mentor for the Storm and Spark projects.
,
Unfortunately work commitments brought me back to Boston early Friday. In retrospect it was well worth the trip and I hope to make it back next year.
,
Sheamus McGovern is a CTO and founder of Startup Code Works, a company that specializes in building online platforms for starups. He is also an organizer of??Boston Analytics Meetups and ,. He has many years  of experience building complex software platforms particularly in quantitative finance and business intelligence. 
,
,
,
,  "
"
Most popular 
, tweets for Nov 03-04 were
,
,  "
"
,  "
"
,
,
Nottingham Trent University's part-time Multidisciplinary Master's (MDM) are set to launch in January 2015.  Working with industry experts, and in response to the demands of today's business NTU have created a bespoke master's course tailored to meet the needs of businesses today
,
Our Multidisciplinary Master's will engage employees with the latest sector thinking, and by utilising our breadth of knowledge across all nine academic schools, will deliver an exciting opportunity to enhance skills and career progression.  
In particular employees will benefit from experts based in Nottingham Business School and the School of Science and Technology.
,
,
,
Areas of study:
,
,??,
Rather than the traditional dissertation or thesis, students on this course will conduct a three-six month in-company project.  Working with the MDM team and in consultation with your employer, you will develop a unique work-based project providing valuable insight and learnings for your business or company objectives.
,
,
,This part-time course is studied over a two-year period.  Employees will attend three day study blocks, once every 12 weeks, which run Thursday - Saturday.
,
,
,
Applications close - 15 December 2014
,
Find out more and apply 
,  "
"
,
,
As 2014 winds to an end, it?€?s time to start looking forward. Business Intelligence might not be a household term yet, but it?€?s certainly gained traction in the business community. With more and more companies understanding the importance of data, business intelligence tools are spreading like wildfire from mass enterprises and tech-oriented companies to SMBs and businesses in a much wider range of industries.
,
So what does 2015 hold in store for the world of BI? We don?€?t have the predictive analytics capabilities to see into the future just yet, but we can make a few educated estimations based on current industry trends and developments.
,
,
,
,
The term Big Data is somewhat ambiguous, but there can be no doubt that the amount of data is growing exponentially, and that it is becoming more complex to handle in the process. With storage prices plummeting and automated extraction methods becoming more sophisticated, companies are finding themselves collecting massive amounts of data even when they had not set out to do so in the first place. Much of this data is unstructured and collected in tables which vary wildly in format and size, generated by machines and cloud-based applications rather than employees who follow a particular doctrine of data collection. This means ETL processes are not getting any simpler.
,
This tendency of data to grow in size and complexity is expected to increase as the use of publicly available data sets becomes more widespread. ,and the , are two examples of organizations which have released terabytes of records collected over time, on diverse subject matter - from public safety to Wikipedia page traffic.

,
,
Yet more data is being generated by the ,, which currently encompasses over 10 billion interconnected devices, a number which is also expected to see significant growth in coming years.
,
The business world is only beginning to harness the power of , and learning how to gain actionable insights from them; but as knowledge in this area grows, analyzing the data found in these gargantuan repositories is expected to become an integral part of business intelligence.
,
Thus, in addition to expanding internal data, BI software will also have to take on tremendous amounts of data from external sources.
,
,
,
We expect 2015 to be , , ,. Already we can see many of the industry?€?s traditional providers touting their newest releases as ?€?intuitive?€?, ?€?built for anyone in the company?€?, etc. This is a response to an actual need within the business community: Users want to control their own destiny and be able to get answers to queries of their choice, without having to call up their IT department for every minor change in needs or definitions. Besides, consumers have gotten used to owning devices and software that ?€?just works?€? - and expect the same of their BI tools.
,
The industry will have to adjust itself: BI solutions in 2015 will be expected to be simple enough to provide middle-management and business users, rather than the company?€?s IT professionals or external consultants, the ability to build and operate a workable , in reasonable timeframes.
,
Implementation times will also have to become shorter: software that takes 6 months of integration and personnel training to become operational could be too much of a hindrance in a world of rapidly changing markets that requires fast-paced insights and decision making.
,
Furthermore, software will be required to be , - to adapt to changing business needs, grow with the business and offer users the ability to change their queries and data sources in real-time. Scalability and extensibility will become crucial elements, as a company that currently has 50 gigabytes of data might have 500 gb in a few years?€? time.
,
,
,
Since the beginning of the current century, business intelligence has largely been reliant on in-memory technology - i.e., utilizing the computer?€?s RAM for querying tables. This was the BI industry?€?s way of handling the reduced performance and long query times that came with analyzing large data stored on computers?€? hard disks.
,
While this solution was sufficient at first, it seems the tides might be changing. Unlike hard drive storage, RAM is still relatively expensive; increasing RAM resources at the pace in which data itself is growing raises serious cost-efficiency issues, particularly for companies that do not enjoy unlimited resources and need to see immediate ROI on their business intelligence projects.
,
BI providers in 2015 will have to address this issue - either by accepting the limitations of their software (i.e., offering customers the choice between increasing their investment in hardware, using clustered and partial data or learning to live with reduced performance), or by developing technological alternatives to in-memory analytics.
,
,
,Saar Bitner is the VP Marketing at business analytics and dashboard software company, , and executes data driven strategies as he makes way for the massive growth taking place. He has, more than a decade of Marketing, Sales and Product Management experience. You can follow Saar on , or connect on ,.
,
,
,
,  "
"
,
,
Much of what human beings experience as commonplace today ?€? social networking, on-line gaming, mobile and wearable computing -- was impossible a decade ago. One thing is certain: we?€?re going to see even more impressive ,advances in the next few years. However, this will be the result of a fundamental change in computing, as current methods have reached their limit in terms of speed and volume. Traditional disk-based storage infrastructure is far too slow to meet today?€?s data demands for speed at volume, which are growing exponentially. In-memory computing (IMC) offers the only practical way to scale to today?€?s computing demands. Although IMC may once have been the domain of only the most well-funded organizations, the falling cost of RAM means that it can now be accomplished by any organization on commodity hardware.

,

As Mike Matchett, Senior Analyst, The Taneja Group said: ?€?Clearly the big data ecosystem is evolving, tackling more kinds of analytics and addressing a wider variety of applications, including enterprise operational intelligence (EOI) apps that require faster interactive latencies, predictive machine learning, split-second decision-making, and high-volume processing. In-memory computing is key to profiting from new competitively differentiating big data solutions like these.?€?

,

Spinning disks will certainly be around for years to come, but the growing demands of Big Data, SaaS, mobile computing and Internet scale, coupled with dramatic improvements in RAM economics and availability, have led to a surge in new applications that access and process data in memory. As a result, IMC is quickly moving from esoteric concept to mainstream adoption. Like any game-changing technology, developers need to be able to learn it, experiment with it and test its limits in order to fulfill its promise.

,

As you may know, GridGain has been at the cutting edge of developing a comprehensive in-memory data processing framework for real-time transactional and analytical use cases for many years. Hundreds of organizations around ,the globe now use its proven open source technology under the Apache 2.0 license. This work has culminated in the recent release of its application- and data-agnostic ,. GridGain?€?s in-memory data fabric code base shares the goals of easy accessibility and free redistribution with other memory-centric, open source data exchange frameworks that have emerged -- most notably the Apache Spark project in the Hadoop world.

,

Today we are thrilled to announce that the GridGain In-Memory Data Fabric has been accepted into the Apache Incubator program under the name ?€?Apache Ignite.?€? Our decision to contribute our pride and joy to the , (ASF) will ensure that this robust and battle-tested ,software not only remains broadly available, but will continue gaining community support to become the standard upon which in-memory computing?€?s promise will be realized. While GridGain will continue to be a strong contributor to the Ignite code base and offer new and innovative enterprise-grade features to its commercial ,, the platform?€?s core code will be managed by the non-profit ASF, home of the world?€?s most successful open source projects.

,

Mike Matchett, The Taneja Group, continues: ?€?GridGain has now put their key IP out for the world to use as Apache Ignite. We predict an enthusiastic adoption curve since GridGain's approach leverages current developer skills and accelerates existing Hadoop apps without modification, providing an easy transition into in-memory computing.?€?

,

In order to explain why now is the right time to trust our code base to the ASF, it is first necessary to explain why we went to an open source model less than a year ago--and for that matter, why we spent years developing this software in the first place.

,

Earlier this year, we took an important step in order to make the transformation to IMC truly accessible by shifting to an open source model through an Apache 2.0 license. Our motivation was simple: remove all barriers to IMC adoption and accelerate the innovation that?€?s ,poised to occur through this technology. We wanted to introduce IMC to the mass market in a way that would give developers maximum freedom to experiment with it and discover the amazing things it allows you to ??do ?€? for example, , such as cancer. The first step was to make our mature and tested IMC platform open source, so that the technology would be as widely available and easily adoptable as possible. So we put the IMC code up on places like GitHub, which resulted in a more than 2000 percent increase in downloads in just a few months, clearly validating the huge interest from the community in the software.

,

The second step -- turning over the keys to the Apache Software Foundation -- will drive true community-building beyond just open source consumption, and of course will ensure the in-memory data fabric?€?s viability for the long term. By doing this, we hope to continue to fast-track both the adoption and adaptation of this technology.

,

Ilya Sterin, Sr. Director of Engineering, Chronotrack Systems, summed up the advantages of this strategy: ?€?GridGain?€?s In-Memory Data Fabric enables us to solve our data and compute scale challenges, which become increasingly complex as our systems grow to support more events and athletes. With its track record behind open source projects, the Apache Software Foundation?€?s backing of GridGain?€?s open source platform gives us confidence that we can rely on it for the long term, and that we can expect to see ongoing innovation as this project is opened to thousands of contributors.?€?

,

Ultimately, community-driven adoption of open source in-memory ,data fabric technology will enable a new class of transactional, analytical and hybrid real-time applications that will allow even the smallest organizations to gain a competitive advantage from fast, data-driven decisions and operations. We believe Apache Ignite has all the right ingredients to become for the Fast Data world of the future what Hadoop is for Big Data today.

,

Let the innovation begin!

,
,
,  "
"
,
By Gregory Piatetsky,  
,, Nov 7, 2014.
,
Recent interview in IEEE Spectrum of Berkeley Prof. and noted Machine Learning expert Michael Jordan has caused a lot of stir, especially given its very provocative title
,.
,
Much less publicity was given to Michael Jordan reaction to the interview title (thanks to Usama Fayyad , for pointing me to this article).
,
In his blog 
,
Michael Jordan writes that the title was placed without his knowledge on the interview.
,
He says 
,
,
,
,
He adds 
,
,
His point was that 
,, so you 
can imagine his dismay at a title that said exactly the opposite.
,
However, none of this changes his warning about 
,
,, 
,
and think more carefully about what we find in Big Data.
,
A good start for avoiding Big Data winter is to understand 
that every prediction has error bars. If error bars are not shown, it probably means they are uncomfortably large and should reduce your trust in the prediction.
,
,
 ,  "
"
,
,
Popular web data platform , recently launched it?€?s newest feature called ?€?,?€?.
,
The tool, which they are providing free of charge, is useful for transforming web page(s) into a table that can be downloaded as a static CSV or accessed via a live API. 
,
To use Magic, users simply need to paste in a URL, hit the ""Get Data"" button and import.io?€?s algorithms will turn that page into a table of data.The user does not need to download or install anything. 
,
The Magic algorithm assesses the underlying HTML of a page to identify the most relevant list. It then treats that list as the primary source of data to generate the rows and columns to create a table. It will also attempt to detect the subsequent pages.
,
Once the data has been returned into the table, users have the option to ??edit the table column headers, delete columns, and drag and drop them into a different order. The data can then be copied and pasted into Excel/Google Sheets or downloaded as CSV. 
,
For more data savvy user, import.io also allows you to create an API to the source data. The API is only available to the initial source data, which means that any of the changes you make to the table will not be reflected in the API. 
,
Magic is the latest offering from the web data platform that is trying to make harvesting web data easier and more accessible. For more targeted data collection, import.io also offers a free desktop app, which uses an intuitive point-and-click step to help the user train the data they want. 
,

,
,
  "
"
Most popular 
, tweets for Nov 05-06 were
,
Good news for Data Scientists! Microsoft gives free access to its Azure #MachineLearning service ,
,
,
Good news for Data Scientists! Microsoft gives free access to its Azure #MachineLearning service ,
,
2 ways #BigData can make us happier: 1: organize & remember what we need to do; 2. reduce choices w. recommendations ,
,
Good news for Data Scientists! Microsoft gives free access to its Azure #MachineLearning service ,
,
,  "
"
,
,
While the technologists and business strategists are hustling in the Big Data hype, the common man is inundated by the overwhelming complexity of ""whatever is, being referred to as 'Big Data'"". Some are even worried that the privacy concerns due to Big Data could be one of the first indications that we are rapidly approaching the era, which so far existed only in our imagination, where machines will outsmart and overpower humans.

,

Put in simple terms, one can argue that people's understanding of current technology is following Moore's law, sadly in the reverse direction, i.e. in every six months, people's confusion about current technology doubles. People had hardly understood the concept of ""cloud"" and how it would impact their life, before being hit by the next tsunami - ""Big Data"". In light of the NSA revelations by Edward Snowden, some people have started to think of Big Data as ""Big Brother"" who is keeping an eye on everything you do, and it is almost impossible to hide anything from it, no matter how personal.

,
To add to users' pity, the ""terms of service"" and ""user agreements"", that are expected to clearly define the privacy aspects, have become more and more obscure. Moreover, they change so often, and given the pace of life, the user has no choice but to hit the ""Agree"" button without reading what one is signing up for.

,

Given the current situation, media can play a vital role in educating people and enabling them to confidently make smart decisions regarding current technology. By simplifying terms such as ""Big Data"", journalists can help people understand that people do have choices when it comes to technology and through some basic awareness, they can not only protect their data but also use the current technology to their benefit.

,

The recently published 46-page comic ""Terms of Service"" by Al Jazeera reporter Michael Keller and cartoonist Josh Neufeld is one of the shining stars in media's effort to educate public on Big Data. Using a very simple narrative and interesting visuals, they have done a great job at explaining the nuances of Big Data and the privacy concerns. It informs about the relevant events in the past and then carefully explains the current state; thereby, enlightening users on the not-so-talked-about dangers of Big Data.

,

,
,

The comic also raises some very important questions about the emerging business models and their impact on people?€?s privacy. It is high time that people be cautious about the too-good-to-be-true offers and ensure that they fully understand the implications of an offer before signing up for it.
,
,
,

It also talks about the term ?€?Internet of Things (IOT)?€? which is getting increasingly popular these days. While the companies mostly talk about the great benefits of these technologies to end user, one must not forget that there are some deep privacy concerns too, that need to be well evaluated before choosing to use this technology.

,
,
,

It candidly attacks the myth of ""free"" product or service, such as the very popular Gmail or Facebook. Then, it subtly explains the significant trade-off between (intangible) privacy and tangible benefits such as free service or discount.

,
,
,

Overall, it?€?s a great piece of reporting. I wish it also outlined some potential solutions to the privacy concerns, which would have left the reader in a better (more calm!) state after reading the comic. Nevertheless, it does raise some very significant questions and by simplifying the complex concepts, enables people to participate in the discussion on Big Data & Privacy.

,
The complete comic is available for online reading (you may also download as PDF for offline reading) at: ,
,
,
,  "
"
,
,
My monthly summary of the company, startup, and acquisition activity for October 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are KDnuggets tweets (with #BigDataCo removed)
,
,??,
Here are previous months activities:
,  "
"
,
,
Bootcamp dates: January 12 - April 3, 2015
,
Early application deadline: November 17, 2014
,Final application deadline: December 8, 2014
,
Twitter: ,
,
,
,
Learn Data Science in 12 weeks with in-person instruction at Metis with experts from Datascope Analytics. Upon graduating, students will be comfortable designing, implementing, and communicating the results of a Data Science project, including knowing the fundamentals of Data Visualization and having introductory exposure to modern Big Data tools and architecture such as the Hadoop stack.
,
,
,
,
,20-30 hours of Online Pre-Work including a Command Line Crash Course, Python tutorials, several package installation tutorials (i.e., numpy, scipy, pandas, scikit.learn), and some preliminary statistics work.
,
,
,Complete first data science project from start to finish. Use the IPython environment and Git for version control, the pandas package to perform exploratory statistical analyses, and the matplotlib package to publish the results.
,
,
,Begin learning the iterative Design Process. Use tools for Web Scraping, get introduced to cloud computing, go in-depth on Regression using modules from scikit.learn and matplotlib and work on Communicating Results.
,
,
,Focus on relational Databases such as SQL and more ways of obtaining, cleaning and maintaining data. Explore the concepts of Machine Learning.
,
Dive deep into Algorithms for Supervised Learning including SVM, decision trees and random forests; techniques for feature selection and feature extraction; concepts and applications for deep learning. Then, Visualize Projects using D3.js. Also cover JavaScript essentials, and other js libraries (e.g., jQuery, crossfilter, Bootstrap).
,
,
,Dig into text data, using Data Acquisition methods with APIs and online database servers. Learn about NoSQL databases and start using MongoDB. Analyze text data and learn about Naive Bayes and NLP algorithms and how large amounts of data are handled, discussing parallel computing and Hadoop MapReduce. Finally, explore Unsupervised Learning and more algorithms, covering K-means, hierarchical clustering, mixture models and topic models.
,
,
,Project 5: Work full-time on your Passion Project, which is the capstone of your five- project portfolio to share with employers.
,
,
,
Students have access to a robust Speaker Series, which has included top experts from the New York Times, FiveThirtyEight, DataKind, Microsoft and Gilt. The Speaker Series serves two purposes for Metis students. Company experts visit the class to provide insight into a company's culture, job opportunities & specific interview processes. Industry leaders also attend to take students on a deep technical dive of new content to help expand the Metis students' skill sets.
,
,
,
A dedicated Career Team works full time to help students achieve their post-Metis goals. The Career Team works with the student throughout the program ensuring that the team can effectively advocate on the student's behalf. The students are trained in interviewing, resume-writing, and salary negotiation, are introduced to the Metis hiring partners and participate in an in-person Career Day at the culmination of the Bootcamp.
,
,.  "
"
By Alesia Siuchykava, Data-Driven Business, Nov 2014.
,
Data analytics in Human Resources is quickly becoming a new area of innovation and focus because of the insights it can provide around workforce management. Workforce analytics has emerged as the key avenue for HR to become a proactive force for managing human capital and strengthening business strategies.
,
,
Data Driven Business has compiled a free white paper which focuses on the business benefits (and challenges!) of workforce and HR analytics from the perspectives of 4 experts from ,, ,, ,and one large financial firm.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,  "
"
Linda Burtch, Nov 2014.
,
,
Although many articles have been written lamenting the current talent shortage in analytics and data science, I still find that the majority of companies could improve their success by simply revamping their current hiring processes.
,
We?€?re all well aware that strong quantitative professionals are few and far between, so it?€?s in a company?€?s best interest to be doing everything in their power to land qualified candidates as soon as they find them. It?€?s a candidate?€?s market, with strong candidates going on and off the market lightning fast, yet many organizational processes are still slow and outdated. These sluggish procedures are not equipped to handle many candidates who are fielding multiple offers from other companies who are just as hungry (if not more so) for quantitative talent.
,
Here are the key areas I would change to make hiring processes more competitive:
,
,
,
Imagine if you were a candidate interviewing with multiple companies, would you be more impressed by the company who did all of the things on this list, or the company who didn?€?t?
,
If you fix it, they will come.
,
See more at: ,
,
,
,
,  "
"
By Gregory Piatetsky,  
,, Oct 1, 2014.
,
Here is a very interesting new tool for visual data exploration, sent to me by 
Andres Colubri, a researcher at Harvard University and the Broad Institute.
,
, is a tool for visual exploration of complex datasets, 
developed by the Sabeti Lab at Harvard University, the Broad Institute, and Fathom Information Design. 
,
Fathom was founded by 
,, a world-renowned designer specializing in information visualization.
,
Mirador is an open source project released under the GNU Public License v2. 
,
Initial support was provided by the Center of Communicable Disease Dynamics and the MIDAS network funded by the National Institutes of Health.
,
It is available on Github for 
,
and 
,.
,
,
,
,
,
,  "
"
New entries for September.
,
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
,
,
Keystroke biometrics is the act of identifying or verifying an individual based on typing patterns. This competition addresses the situation in which the typing behavior of a user is seriously handicapped during the testing phase of a biometric system. 
,
This situation may arise when a user has enrolled a normally-typed sample during the training phase and later is only able to type with one hand due to an injury or distraction during the testing phase.
,
A robust keystroke biometric application needs to be able to handle this situation gracefully. Is it possible to identify a one-handed typing sample? Should the user be re-enrolled while typing with only one hand? Or is some other fallback mechanism needed to attempt the identification of the individual? The results of this competition will help to answer that question.
,
The goal of the competition is to correctly classify the identity of as many unlabeled keystroke samples as possible. Classification results can submitted directly to the competition website where they will automatically be scored. Python starter code is provided to parse the data and quickly start building a model.
,
The dataset is very challenging. The benchmark score from the starter code (seen on the leaderboard page) shows classification accuracies of less than 5% for all conditions. This should be easy to beat, though, for reasons explained in the starter code.
,
Competition rules and prize:
,
Participants can make up to one submission per day, up to the competition deadline. The leaderboard will be calculated on 50% of the data until the last day of the competition to avoid overfitting.
,
The winner of the One-handed Keystroke Biometric Identification Competition will receive a Futronic FS88 Fingerprint Scanner!
,
For more information and instructions on how to participate, see the competition website: ,
,
,
,
,  "
"
Aspect Based Sentiment Analysis@SemEval 2015 (co-located with NAACL-2015 in Denver, Colorado)
,
(,)
,
With the proliferation of user-generated content, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops, battery, screen) and their attributes (e.g. price, design, quality). The SemEval-2015 Aspect Based Sentiment Analysis (SE-ABSA15) task is a continuation of SemEval-2014 Task 4 (SE-ABSA14). In aspect-based sentiment analysis (ABSA) the aim is to identify the aspects of entities and the sentiment expressed for each aspect. The ultimate goal is to be able to generate summaries listing all the aspects and their overall polarity such as the example shown in Fig. 1.
,
,
Figure 1: Table summarizing the average sentiment for each aspect of an entity.
,
This task (ABSA15 for short) is a continuation of SemEval 2014 Task 4 (ABSA14, ,). ABSA15 will focus primarily on the same domains as ABSA14 (restaurants and laptops). However, unlike ABSA14, the input datasets of ABSA15 will contain entire reviews, not isolated (potentially out of context) sentences. Also, ABSA15 consolidates the four subtasks of ABSA14 within a unified framework. Furthermore, ABSA15 will include an out-of-domain subtask, involving test data from a domain unknown to the participants, other than the domains that will be considered during training.
,
Call For Participation: ,
,
Datasets & Annotation Guidelines: ,
,
,
,
,  "
"
Most popular 
, tweets for Sep 29-30 were
,
Great list of resources on Cloud Computing, #BigData Models, NewSQL/Graph/SQL/other databases, Machine Learning ,
,
Machine learning #cheatsheet on github, Classical equations and diagrams in machine learning ,
,
Juno, a powerful, free environment for the Julia language #DataScience ,
,
Machine learning #cheatsheet on github, Classical equations and diagrams in machine learning ,
,
,  "
"
,
Join the 
, (San Francisco, November 4-5) with an exceptional speaker roster and insightful agenda to get yourself ready for text analytics challenges in 2015!
,
,
,??,??,??,??
,
,??,
This year text analytics professionals from companies like 
,
and many others will be in attendance. 
,
See who you can meet, learn from, and forge new business relationships with here: 
,.
,
Don't miss your chance to meet the brightest minds in analytics in one room and discuss your strategies for 2015 with them, secure your place now: 
,
,
For more information or to make a group booking at a reduced rate, please don't hesitate to get in touch with Alesia Siuchykava, 
,
or (201) 204-1694 .
,
Use code 
,
to receive the exclusive $100 discount on top of the Last Chance Discount of $100 if 
,  "
"
By Gregory Piatetsky,  
,, Oct 2, 2014.
,
Here are 52 upcoming October 2014 - April 2015 meetings and conferences. 
,
Top meeting cities are:
,
,??,
Top meeting countries are:
,
,??,
For full list see ,  page.
,
,
,
Color code: Business-Oriented meetings in Blue, ,
,
,
,
,??,
,
,
,??,
,
,??,
,
,
,??,
,
,
,??,
,  "
"
,
,
, is a tool for visually exploring complex datasets, enabling users to infer new hypotheses from the data and discover correlation patterns, developed by researchers, programmers and designers from Fathom Information Design and the Sabeti Lab at Harvard University and the Broad Institute.
,
The goal of Mirador Open Data Competition is to promote public participation and transparency in research and governance. 
,
The competition uses 4 large, complex, and public datasets which are made ready for Mirador
,
,??,
Make your submission by Oct 28. During that time, upload your correlation discoveries from the Mirador app to your competition account. 
,
For details and to participate, visit ,  "
"
By Simon Barton, Innovation Enterprise, Sept 2014.
,
In early November 2013, Typhoon Haiyan tore through the Philippines leaving in its wake a trail of destruction and tragedy. 13 million people were adversely affected by the Typhoon, including 4.9 million children of whom 1.5 million were under the age of 5 are deemed to be at risk of Global Acute Malnutrition. The pain and suffering that a disaster such as this precipitates is clearly impossible to quantify in terms of data, but data?€?s use can be an essential tool for the emergency services when they need to identify when and where people are in danger and what resources they need to save them.
,
,
Big data when used in conjunction with humanitarianism is often harnessed in the form of crowdsourcing, a phenomenon that has been with us since 2007. Around that time, Kenyan, non-for profit, Ushahidi began mapping user-generated accounts of brutality after the elections in Kenya in an effort to spur donations to the region. By plotting the events it created a public record of the event and spawned a number of similar sites that have helped humanitarian projects go from strength to strength.
,
The process of crowdsourcing is developed by categorising and verifying reports transmitted by witnesses of events, normally through email, text messages and social media, with Twitter being of particular importance. Crowdsourcing and the incorporation of Big Data has been imperative in assisting the services efforts to stem the tide against a number of natural disasters including, Typhoon Haiyan and the Haiti earthquake.
,
Big Data is also being picked up by a number of international relief institutions, including the Disaster Relief International (DRI), a major supplier of humanitarian aid, which has used Big Data analysis to improve response efforts in the Philippines by tracking assets and personnel in real-time and determining where is help is most urgent. Their willingness to get on board with Big Data gave them the Peter F. Drucker award for Non-Profit Innovation.
,
Typhoon Haiyan was not Big Data?€?s first major, widespread endeavour into humanitarianism. Instead, we first saw its potential being put to use in the Haiti earthquake. Its success was born out of partnership between the public and private sector and allowed data scientists from the Karolinska Institute to use data from Digicall, Haiti?€?s largest mobile phone operator, to compare people?€?s movements before and after the earthquake so that they can decipher where the ?€?hot-spots?€? so that they can get medical supplies over to them as soon as possible.
,
Big Data?€?s use is not just confined to disaster relief and can also play an important role in helping policy makers and researchers. The United Nations Population Fund teamed up with SAP AG in 2011 to create two dashboards with the aim of engaging people in the societal and demographic trends that are shaping the world we live in through to 2100.
,
By using data, the dashboards gave us an in-depth look into the way in which the globe is likely to develop, all the way up to 2100. This project was in line with their 7 Billion Actions campaign, which looked to raise global awareness around the opportunities and challenges associated with a population of over 7 billion. The fact that technology and data plays such an important role here shows that data has the power to transform hard, often difficult to read information, into clear and consummate info-graphics.
,
Far from being just a reactive tool, Big Data has the capacity to pre-empt crises, or at least respond to them in quicker fashion. Take the Cholera outbreak in Haiti that has been a pressing issue for over four years, a study in 2012 showed that Twitter was yielding data that would have made for quicker detection of the outbreak when compared to more traditional methods.
,
,
They found that, as the number of incidents increased and decreased, so did the amount of tweets and informal media reports. A truly remarkable finding, and one that shows the power that data has at its disposal. If these trends had been visualised more readily, the disease could have been stifled in its early stages, and in turn saved a number of lives. The report stipulates, ?€?This information in the right hands could have saved lives?€?
,
Disease tracking, as seen with the Cholera outbreak in Haiti, is perhaps the most meaningful contribution that Big Data can make. In Kenya, a number with high mobile phone penetration, data has been combined with regional malaria prevalence information to figure out how population movements influence the spread of disease. The information they garnered allowed them to suss out the probability of a resident being infected with malaria and the chance that a visitor to a stated area would be infected on any given day.
,
These developments are incredibly impressive and when you consider that mobile phone penetration in Africa has hit 80% and is still on the up, some 4.2 % annually, the opportunities for mobile data aggregation is a significant one and one that has the potential to foresee disease epidemics.
,
If take some time to look at Big Data and Humanitarianism, then a concept you will see commonly is ?€?Data Philanthropy?€?. This concept involves companies sharing their proprietary datasets for social good. As stated numerously throughout this article, big data is at the centre of aid reliefs nowadays, but issues continue to persist in the form of the processing phase, which can still be rather time consuming and often requires individuals to monitor content in real time. This has led to the proposition of media companies getting on board whose insights will allow for cutting edge media monitoring that can be done faster and more efficiently.
,
Big Data and Humanitarianism are two areas that have the ability to be a match made in heaven and go some way to helping the emergency services quell some of the globes most pressing and urgent humanitarian crises. Similar to its implementation in the LAPD, it demonstrates how far Big Data can go outside of the business landscape and the extent to which it can assist the individuals working at the ground level. What is clear is that through Big Data we have a more accurate view of the earth and where it is likely to be in the next 100 years Through these projections, organisations such as the UN can make accurate assertions as to what sections of society need the most help to develop as fruitfully as possible. This has been reflected in the UN?€?s 7 billion Act, where they marked 7 key issues that they feel are the most imperative for the globes growing population.
,
The use of Big Data and crowdsourcing is clearly not without its limitations. You only need to look back a few months to the Google Flu Tracker failure to see that there is still significant progress to be made, and that relying on data is not always best practice. The Google Flu Tracker overestimated the size of the influenza pandemic by 50% and miscalculating the severity of last year?€?s flu, predicting double the amount of flu-related doctor visits. So clearly we must tread with caution, but Big Data can be an essential tool and this has been proven time after time over the last 5 years or so.
,
Original: ,
,
,
,
,  "
"
Most popular 
, tweets for Oct 01-02 were
,
Great site for R programmers & Data Scientists: R wizard @HadleyWickham ""Advanced R"" book, online site #rstats ,
,
Great site for R programmers & Data Scientists: R wizard @HadleyWickham ""Advanced R"" book, online site #rstats ,
,
#BigData can be a big help for Humanitarian Efforts, especially for Disease tracking (Cholera in Haiti, Ebola) ,
,
Great site for R programmers & Data Scientists: R wizard @HadleyWickham ""Advanced R"" book, online site #rstats ,
,
,  "
"
,
,
,
Scientists who study patterns in survey results might be dealing with data on language rather than what they?€?re really after --attitudes -- according to an international study involving the University of Colorado Boulder.
,
The study, published in the journal ,, found that people naturally responded to surveys by selecting answer options that were similar in language to each other as they navigated from one question to another, even when the similarities were subtle.
,
,
,
,
,
For the study, researchers looked specifically at surveys on organizational behavior, such as leadership, motivation and job satisfaction.
,
?€?The findings suggest many survey participants likely fit the first question into their language understanding and, when they get to the next question, move in their language network to figure out how close it is to the previous question in order to respond,?€? said Kai Larsen, information scientist and associate professor of management and entrepreneurship at CU-Boulder?€?s ,. Larsen is a co-author of the paper.
,
The findings also raise questions about the way scientists design and analyze surveys, inadvertently focusing attention on the shared language understanding of respondents, said Larsen.
,
?€?The methods used for surveys are making it difficult to get at what?€?s unique about an organization rather than what?€?s embedded in general language,?€? he said.
,
Often when social scientists conduct surveys with human participants, they look at more than just average scoring. In the results they detect -- and measure -- patterns. They quantify, for example, how much a popular answer to one question likely leads to a popular answer on another question to find common relationships.
,
The measurements help form statistics like, ?€?people who highly rate their manager?€?s leadership style are more likely to stay longer at their jobs.?€?
,
In the case of the current study, researchers measured the degree of similarity in survey language instead of human response patterns. When they compared the measurements to measurements of human response patterns, the two sets of numbers were nearly identical, indicating the measurement of language similarities and people?€?s selection of survey answer options were practically the same thing.
,
For the study, the researchers applied two algorithms, or complex computer-operated calculations-- each using radically different approaches -- to measure sentence similarities.
,
The first algorithm involved about 100,000 newspaper articles to evaluate word similarities used within. The second algorithm relied on an online database created by linguists that shows the relationship between tens of thousands of words.
,
The surveys used in the study were already published and taken by anonymous respondents in a variety of fields from finance and government to engineering and the military. The respondents also included business students.
,
One type of survey that was not found to be language-based in the study was personality testing.
,
The study also highlights the growing prowess of data science.
,
?€?Semantic algorithms are becoming new tools for the social sciences and are broadening perspectives on survey responses that other longtime theories cannot explain,?€? said Arnulf. ?€?This represents a study of how the relatively young data sciences can address problems not approachable with traditional methods.?€?
,
To see the complete study visit ,.
,
Original: ,
,
,
,
,
,
,
 ,  "
"
,
,
,
Our guest Dan Ariely, author of best-sellers , and ,, uses simple experiments to study how people actually act when making real-life decisions. His interests span a wide range of daily behaviors such as buying (or not), saving (or not), ordering food in restaurants, pain management, procrastination, dishonesty and decision making under different emotional states. His TED talks on these topics have been viewed more than 4.8 million times.
,
,
,
,
,
,??,
On-demand webcasts featuring additional thought leaders are also available.
,  "
"
,
,
As a media partner, KDnuggets is pleased to offer our readers 2 free passes to 
,
, Conference, October 27-29, 2014, San Francisco.
,
To win a free pass (free registration), please email to ,
by 11:59 pm ET on Sunday, Oct 12, 2014 with the subject: BDTechCon
,
and please specify, what were 
,
,
The winner will be chosen among all submissions with a random number generator and announced the week of Oct 13, 2014 along with a summary of the answers. 
,
Here is more information about the conference, and even if you don't win you can still get KDnuggets discount on registration - see below.
,
,
,
,, October 27-29 in San Francisco, is the place to learn HOW-TO accommodate the terabytes and petabytes of data from your Web logs, social media interactions, scientific research, transactions, sensors and financial records. Learn how to index, search and summarize Big Data. Learn how to empower employees, inform managers, and reach out to customers.  In short - learn how to Master Big Data!
,
The tangible benefits of Big Data analytics are well known. You likely understand the ""why"" of Big Data. Big Data TechCon isn't a ""why"" conference. It's the HOW-TO conference for Big Data. Practical tutorials. Technical classes.
,
, is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Come up to speed on the latest big data technologies like Yarn, Apache Spark and Cascading. Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or JavaScript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes. 
,
Mingle with fellow attendees. Be inspired by keynotes, including Gloria Lau, Manager of Data Science at Linkedin.  Be impressed by the hottest Big Data tools in the Expo Hall, from more than 35 top companies.  It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code ,
when prompted.  
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
, is an experienced program manager, analyst and customer advocate who has transitioned into the role of Social Media Director for ,. After years of running a team of project managers through the manufacturing and development process, she jumped into social media. Her area of expertise is in social media data analysis, strategy development, content management, and social care.  Her social media team stands alone, but is deeply integrated with marketing, customer service, public relations and Web analytics teams.
,
Here is my interview with her:
,
,
,

,The top priorities for our social media team are to provide information and support to our customers throughout their moving journey.  We accomplish this by listening to our customer experiences, and developing relevant content to assist them. We also have a dedicated social care team, working seven days/week to help our customers during their move.  
,
,
,
,The factors will vary according to the overall goals of each campaign.  For example, a campaign focused on driving brand awareness will measure different data than a campaign developed to drive sales.   When developing a social media campaign, it?€?s important that the measurements of success are established before the campaign is implemented, and periodic ?€?checkpoints?€? are in place to ensure the campaign is on track.  
,
,
,
,In order develop usable insights from data, we must focus on the data that measures the success of our social media goals.  We tag all of our social media content by goals which allows to measure the success of our campaigns and our goals across multiple channels. 
,
,
,
,As you might expect, our Marketing and Product teams are interested in referral traffic and conversions driven by social media. And so, we continue working with them to provide perception insights, including sentiment and discussions around our products and services.  
,
,
,
,In a recent interview, U-Haul  CEO, Joe Shoen said, ?€?Find something you?€?re good at and that you like?€? and you?€?ll do well.?€? This is a great advice.  During the past eight years, I have worked in various positions at U-Haul, because our Company believes in allowing team members to grow within the organization?€? by strengthening our skills.  I feel truly lucky to work for such an amazing organization.   
,
, 
,
,When it comes to social analytics, the most important trait is to have the ability to not only understand data and its origin, but also how specific data is related to our business objectives.  One cannot work without the other. 
,
,
,
,I?€?m just finished reading ?€?,?€? by the authors of ?€?,?€?.  It?€?s a great read that offers an interesting perspective on analysis.   Not working?  This is social media; we never stop working! =)
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,, Oct 4, 2014.
,
Ebola is a major global health threat in 2014, but how quickly does it spread?
Various news organizations report that the number of cases is increasing exponentially, but there were only a few thousand cases (not millions like for flu), so I decided to investigate from primary data.
,
I found reports from 
,
,??,
and extracted data for the 3 most affected countries - Guinea, Liberia, and Sierra Leone. 
,
Of course, the reported numbers should be considered only a lower approximation, since reporting on Ebola is very difficult.  
The heroic health care workers in West Africa, who collect the data on the ground, face not only the disease (many have already died) but also attacks from scared villagers who think the health care workers spread Ebola.
CDC estimates that for every case reported in publicly available case counts, an additional 1.5 cases are not recorded.  
,
More advanced and geographic analysis is possible, as well as looking at lagged time series (since Ebola typically takes about 3 weeks to fully manifest itself), but even our initial analysis is quite illuminating. 
,
First, we see that the disease progression is markedly different in the 3 affected countries.
,
,
,
Fig 1. Total Ebola cases in West Africa, as of Oct 1, 2014
,
Ebola is clearly spreading the fastest in Liberia, where the number of cases has doubled in about last 25 days.  In Sierra Leone, it took about 28 days.
,
In Guinea, where according to WHO, Ebola started in Dec 2013, we see the earliest cases.  However, it seems that the growth rate is slowest there.  In fact there was no growth in , in Guinea during July 2-20, with the number of cases stuck at ~ 410, although perhaps this was due to faulty reporting. The growth in , Ebola cases has resumed in late July, but slower than in the other 2 countries.  Still the number of cases has doubled in the last 43 days.
,
Next graph shows the total number of deaths.  Liberia is much harder hit than others.
,
,
,
Fig 2. Total Ebola deaths in West Africa, as of Oct 1, 2014
,
Finally, we look at the death rate, which again is markedly different - highest in Guinea, lowest in Sierra Leone.  One hopeful sign is that % of death in Sierra Leone seems to be decreasing. 
,
,
,
Fig 3. Total Ebola deaths in West Africa, as of Oct 1, 2014
,
Here is the data in the table form.
,
,
,
,
 ,  "
"
,
,
International data science platform AlgoMost has launched a new challenge for data scientists on forecasting a development of social graph. The competition will run until November 30, 2014
,
With the appearance of social networks dynamic graph has become one of the main subjects of research in data analysis. This graph can change its features in time: its nodes and edges can appear and disappear.
,
Importantly, by social networks in mathematics is meant not only Facebook and other social platforms but also the clients of mobile operator (where edges are the facts of calls between subscribers), a scientific research community (where edges are the records of a collaborative study) and etc.
,
One of the most critical tasks is to forecast the formation or disruption of graph connections. For instance, if some nodes in the graph relate to ?€?services?€?, ?€?communities?€? and etc., and the formation of edges between them is interpreted as ?€?service usage?€? or ?€?joint to community?€?, the correct forecast of edge?€?s formation means an opportunity to offer needed services in time, to recommend a network user an appropriate community. In the first case it is possible to charge a fee for ?€?service usage?€? earlier, in the second - to increase the user?€?s loyalty by target-oriented recommendations
,
All information about our new challenge can be found here: ,
,
,
,
,  "
"
        ,
,
Join us at , | November 18th and 19th | Computer History Museum, Mountain View
, Register now at
,
,  "
"
,
,
Have you noticed your web charts start breaking when you load 40,000 data points or more?
,
Do they run, but you have to turn off tooltips and other interactive features?
,
ZingChart is solving the chart and graph needs of developers from start-ups and the Fortune 500. The ZingChart JavaScript charting library is designed to be the perfect fit for big data projects:
,
,??,
ZingChart is a San Diego-based company that emerged in 2008, when the team was developing a deep web analytics system in need of some standard charting features. State of the art Flash and canvas based graphing systems kept buckling under the volume of data provided and lacked the required flexibility. As a result, the Zingchart project was born and has been evolving ever since.
,
For more information, please visit ,
,
Email: ,
,
Phone: 858-490-5281  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Oct 7 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Oct 03-05 were
,
,  "
"
,
,
,
,For a young B-school professor in business analytics, what are the five to eight A-level journals in which he/she should try to publish? 
,
Some possible journals include 
,
,??,
Please contact Prof. Bruce Golden, Robert H. Smith School of Business
University of Maryland College Park, BGolden [at] rhsmith dot umd dot edu
,
Gregory Piatetsky: here is a list of lists of journal rankings from Insead
,
,
,
Here is a list of top journals in data mining for the last 10 years, 
according to 
,
,
,
,
 ,  "
"
,
,
,??,??
,
,  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
        ,  "
"
,
,
Ever have dreams of achieving Jeopardy glory?
,
,If so, you might be interested in knowing where the majority of Jeopardy contestants live. Or what the average winning score per episode has been each year. Taking a look at the detailed history of Jeopardy questions and contestants could provide you with interesting stats such as:
,
,
,
One might think that this information lives in some secret database hidden deep within Jeopardy?€?s headquarters. Thankfully it?€?s not, it just takes some web scraping and clever ?€?,?€? to make sense of it all.
,
, is one thing; transforming that data into something useful for analysis is the ?€?data wrangling?€? piece of the equation that typically ends up taking the most time. This is where things get bogged down and sometimes where analysts give up because the process of preparing data can be painful and slow. Below is a brief summary of the Trifacta?€?s Data Transformation Platform process used to effectively prepare raw Jeopardy data into a format ready for analysis:
,
,
This same framework can be applied to any common data preparation scenario within an organization such as transforming customer records, raw web logs or network traffic for analysis. Keep in mind, the different elements of this process are typically not sequential and often require the practitioner to seamlessly move back and forth between different segments of the process to reach the desired outcome.
,
,
,
For the purposes of this experiment, we first found a publicly available dataset on Jeopardy Questions to start working with. After assessing the content of this Jeopardy Questions dataset and its potential for analysis, we opted to gather some additional data on Jeopardy to expand the breadth of the analysis. To do this, we discovered the site ,, that contains information on every Jeopardy episode of all time. We extracted data on Jeopardy Contestants using a web-scraping tool called ,.
,
,
,
Once we collected the right data, we needed to figure out what was in each dataset to determine how we could leverage for analysis. Initially, this involved reaching an understanding for the structure & content of the data:
,
,:
,
,
,
Quickly understanding both the technical data profile (the structure) and the functional data profile (the content), has traditionally been a tall order for most analysis tools. Regardless of size, this is a critical step to finding insights in any data,.
,
So how does this work in practice?
,
Here?€?s how we used Trifacta?€?s Data Transformation Platform to rapidly gain an understanding of what?€?s in each of these Jeopardy datasets.
,
(Raw JSON of Jeopardy Questions dataset)
,
,
,
,
When we open the Jeopardy Questions JSON file in Trifacta, we are immediately presented with some useful pieces of information. We?€?re shown an inferred set of steps that the tool has taken on it?€?s own to help get the data into a form that we can better understand. With a JSON input file, Trifacta un-nests the information to display in a grid so the user can easily interact with it.
,
Trifacta also presents histograms for each column that give a good sense for the data?€?s distribution, inferred data type and quality bars that display how much of our data is incorrectly categorized or missing. This information allows for analysts to quickly focus structuring, cleaning and enriching efforts.
,
(Trifacta script suggestions/histograms/quality bar for Jeopardy Questions dataset)
,
,
,
,
The contestants data set was scraped from the j-archive website. When we opened it in Trifacta, the platform inferred a line and column break for the file and gave us the same set of summary statistics. After inspecting the data in Trifacta, we could see that each row contained free text biographies for each Jeopardy contestant in the referenced show. It also contained separate columns containing their final scores that were not connected to the individual players. This quickly let us know that we?€?d have to carry out some reshaping and text extraction in our subsequent steps before we?€?d be able perform any meaningful analysis on the data.
,
In 
,, we will walk through the remaining steps of the data transformation process, detailing how we used Trifacta to structure, clean, enrich and distill this Jeopardy data for analysis.
,
, brings a wealth of field experience to Trifacta's product management team with his experience in product management, alliances and sales engineering.??Prior to joining Trifacta,??Alon??worked at both GoodData and Google.??As Product Manager at Trifacta,??Alon??works closely with Trifacta customers and partners to drive the product roadmap and requirements for Trifacta's Data Transformation Platform.
,
,  drives both content and product marketing efforts at Trifacta having spent the past five years managing the marketing initiatives for several high-growth data companies. Prior to Trifacta, Will worked with a variety of companies focused on data infrastructure, analytics and visualization, including GoodData, Greenplum and ClearStory Data. Will develops and executes Trifacta?€?s marketing and content strategies to rapidly expand business growth and brand awareness.
,
,
,
,  "
"
,
Latest ,, (Oct 08, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
Swing for the fences in everything you do; incremental research is not worth your time. Pedro Domingos, ,  "
"
Most popular 
, tweets for Oct 06-07 were
,
Great TED talk by 
,
""Big Data is better data"" - what is the future of #BigData-driven technology and design? ,
,
Great TED talk by @KnCukier ""Big Data is better data"" - what is the future of #BigData-driven technology and design? ,
,
Great TED talk by @KnCukier ""Big Data is better data"" - what is the future of #BigData-driven technology and design? ,
,
Top 10 One-Person Startups (infographic) #BigDataCo ,
,
,  "
"
,

,

As organizations have developed the capacity to gain greater insight into customer behavior, it is essential to use innovative analytics practices to succeed. For large and small retailers, it is essential to be able to not only react to, but also accurately predict the trends and nuances in the market.
,
,

, (June 19 & 20, 2014) was organized by the Innovation Enterprise at Chicago. Illustrated intermittently with case studies, interactive panel sessions and deep-dive discussions, this summit offered solutions and insight from the leaders operating in the Big Data space.

,

We provide here a summary of selected talks along with the key takeaways.

,

Here are highlights from Day 1 (Thursday, June 19, 2014):
,
, shared Analytics use cases in the Entertainment industry, in his talk ""Using Data to Drive Business Decisions in the Home Entertainment Industry"". Entertainment continues to be one of America?€?s biggest exports.?? But how does one forecast and meet demand for products here in the United States??? Eric explained some of the variables and techniques used in forecasting demand for movie and television shows as early as the greenlight process (the decision to make a movie or TV show in the first place) to the time period just before the product is released on home video.

,

A traumatized economy and the proliferation of disruptive business models has driven simultaneous transaction growth & margin decline and the erosion of 2 key profit drivers: (1) new release ""conversion rates"" and (2) consumers' purchase of ""catalog"" titles. As a result, consumer retail is getting more front-loaded, i.e. they have just one chance to get it right. His firm had to tighten their ""greenlight"" models, as prior models failed to forecast about 30% of profit opportunity.

,
He shared a case study on: when does an Oscar boost sales, to what degree and why? He also mentioned that Analytics has helped the firm change how it thought about its retail partners.

,

, shared his experience and learning in his talk ""How Macy's Use Advanced Analytics and Big Data to Improve Omni-channel Customer Experience?"". There are two types of Big Data scientists - one that deal with Big Data infrastructure and others that deal with Big Data domain problems. He considered himself the latter. He recently authored the chapter ""Frontiers of Big Data Business Analytics: Patterns and Cases in Online Marketing"" in the book ""Big Data and Business Analytics"" edited by Jay Liebowitz.

,

Big Data is a big opportunity, thanks to Moore's law working on computing, storage, and networking capacities. Data Analytics has changed every field significantly from science to government to commerce. In the traditional BI process, data can be accessed and analyzed only after ETL (Extract-Transform-Load) process. In terms of data maturity, most companies are still at best doing segmentation and predictive modeling or creating multi-dimensional reports; and thus, haven't reached the stage of knowledge discovery.

,

In the Big Data era, data modeling faces the following challenges:
,
, ?? ,
He outlined the solutions for above challenges as:
,
, ?? ,

Finally, he talked about the Advanced Analytics team at Macy's. The team uses a wide range of tools including Hadoop, SAS, SAP/KXEN, R and Mahout.
,

, delivered a thought-provoking talk on ""Analytics ?€? Predictive, Prescriptive, Closed-Loop, Real-Time: We need it All. Feel like drinking from a Firehose?! It?€?s all about 'How' "". In today's world business managers are looking for end-to-end solutions that can be implemented seamlessly, in a scalable way. Business wants to build a data bridge to drive better outcomes, and this can begin with simple steps such as segmenting the customer base based on demographics, behavior profiles, lifetime value, etc.

,

Though it might seem deceptively simple, this Analytics process has a number of challenges. One of the biggest challenges is the disparate and large number of data sources. Referring to a study from Aberdeen (2012), he mentioned that Big Data initiatives are delivering tangible business benefits. Firms leveraging Big Data have 3% more profit, 10% greater operating cash flow and 4% more customers year-over-year compared to firms not leveraging Big Data.
,
It's important to be able to plunge into Big Data, but even more so to know how to pick the RIGHT data and drive your focused objective. The ""Right"" Data trumps ""Big"" Data any day. He suggested that companies should think ""Process"" and act ""Value Driven"" They must optimize value drivers in the context of their business process, while remembering that optimizing a business process is an asymptotic process.
,

While there are a lot of Big Data apps that can be used for various processes across the Customer Marketing lifecycle, it is very important to focus on the most important processes based on business context. He gave a high-level overview of the Nuevora platform which delivers integrated Analytics apps alongwithcontinuous monitoring. In the end, he emphasized that ""Context is Key"" and the importance of making Analytics easy to use - ""Consumerization"".
,
,.
,
,
,  "
"
,
,
,The classic illustration of the power of brand is perfume - expensive perfumes may cost just a few dollars to produce but can be sold for more than $100 due to the cachet afforded by the brand.
,
David Malan's Computer Science course at Harvard, 
,, provides an interesting parallel in the education world.  It's 
,, or you can pay $75 for an EdX completion certificate (provided you successfully complete the course).  Or you can pay $2200 and get Harvard credit.
,
Here's the kicker - in all three cases, it's the same exact course experience for the student.  The only difference, besides the price, is whether there is a credential, and, if so, the name of the organization issuing the credential.
,
It's no secret that an Ivy League credential carries intangible value above and beyond the sheer quality of the education that lies behind it. The era of the MOOC provides a stark illustration of that brand premium -- given a fixed product, consumers seem willing to pay a 2800% premium for the Harvard name, as opposed to an  EdX credential.   A bit like perfume.
,
Statistics.com also has an introductory track for programming, focusing on the needs of data scientists, using R, Python and SQL.  Small classes, individual attention from instructor and teaching assistant.  See:
,
, 
,
Peter Bruce is the President of Statistics.com.
,
,
,  "
"
,
,
returns to the Swissotel in Chicago on November 12-13. With just 6 weeks to go, more leaders and senior decision-makers are joining daily. This year's event will be more influential than ever.  
,
Event overview: 
,
,
Confirmed speakers include:
,
,??,
,
We are offering a 
,
off pass prices for all KDnuggets readers when you quote 
, at ,. Spaces are filling up fast so don't miss your chance to take advantage of this exclusive discount.
,
If you would like to attend please reach out to Euan Hunter at , (+1 415 992 5510), or alternatively you can secure your pass here - 
,
,
Select a Diamond Pass for additional access to the co-located 
,, with keynote speakers from 
, & many more - 
,
,
Best Regards, 
,
Innovation Enterprise  "
"
,
,
, The KDnuggets team is glad to introduce our SPOTLIGHT initiative - a monthly column dedicated to academic researchers. Living in a world inundated by news and events in the field of Data Science, it is not hard to note that most of the media reporting is focused on industry events such as market launch of new products, startups, acquisitions, investments, and of course, marketing articles poorly disguised as news!

,

, Far away from the shine and glamour of media attention, we have a distinct group of intellectuals consistently tinkering with new ideas, conducting laborious experiments with scientific vigour and reporting unbiased observations. Along with their peers and students, these intellectuals pursue a totally different style of research, as contrasted to the industry. They would energetically pursue problems whose solutions might not have any tangible financial benefits. In short, academic research is an integral component of scientific advancement. Thus, it deserves a fair share of media attention in order to spread out the great ideas generated in university labs and research centers.

,

, In the pursuit of providing comprehensive reporting of academic research, we interview distinguished researchers and one of their current students. The interviews are designed to focus on novel ideas for significant problems, while also exploring potential applications, interesting trends and personality insights.

,

, Besides introducing the great hidden talent and research from the world of academia, this initiative will also help our readers expand their horizons of creative thinking for the various challenges Data Science currently poses. Furthermore, we hope that this column would help prospective students learn more about the research activities across universities and serve as an informal introduction to their future peers.

,

Let?€?s begin the journey. Our first stop: UC, Riverside.

,
,
,
, is a leading researcher (No. 6 among ,), and his main focus is in the area of time series. Since recently, Prof. Keogh has been actively pursuing the applications of data mining in entomology, which he calls ""Computational Entomology"". His student, Yanping Chen, recently won a contest to receive research grant from Bill & Melinda Gates Foundation for the project ""Using Data to Understand Insect-Vectored Diseases"". Let's learn more about their research in this two part interview (Part-1: Interview with Prof. Keogh; Part-2: Interview with Yanping Chen). Each part will feature interviewee's short bio before the interview.
,
,??was born in Dublin, Ireland, the youngest of nine children. A middling student, he dropped out of school at age 15 to serve an apprentice as an automotive refinisher. Looking for something more in life he immigrated to the US at age 19, and slowly worked his way through college supporting himself as a metal fabricator, bicycle mechanic and occasional automotive refinisher. He received his Ph.D. in Computer Science in 2001 from the University of California-Irvine, and since then has been at the University of California-Riverside, winning early tenure and early promotion to full professor. He has won best paper awards at virtually all data mining venues (SIGKDD, ICDM, SDM, SIGMOD etc) and awards from the Bill and Melinda Gates Foundation, the Vodafone Foundation, IBM and Microsoft. He is a top ten most prolific author in SIGKDD (22 papers, 10,??place), ICDM (25, 3,), SDM (19, 4,) and Data Mining and Knowledge Discovery (11, 1,).
,
Here is my interview with Prof. Eamonn Keogh:
,
,

,

,: I am probably most known for my work with time series data. I have invented some of the most commonly used representations (SAX [e], PAA [f]), algorithms (LB_Keogh) [d], and definitions (Time series Motifs [a], Shapelets [b], Discords [c]) used by the data mining community.

,

However in the last few years, I have become very interested in applying data mining to problems in entomology [g][h]. In fact, I am trying to bootstrap a new area of research I call??,??[g]. My motivation is simple, insects kill a million people, and eat tens of billions of dollars worth of food each year, but at the same time, insect pollinate about half our food. Data relevant to insects/insect control comes from multiple sources, from real time sensors in the field, from century old handwritten archives, from meteorological models. Thus data mining is critically needed make sense of all this and move??,into 21,??century science,
,
,

,

,I sort of stumbled into the area of time series as a grad student, because NASA was funding my (then) ,advisor. However, I realized that I could make a career out of??,??working on time series, because it is so ubiquitous. One a single day I have looked at time series recorded on Mars, and time series record from the brain activity of a mosquito. On a more pragmatic note, the rise of wearable computers and the quantified self movement means that there will be time series problems to solve for a long time.
,
,
,


,Early in my career I tried to start two trends. In 2002 I pointed out that the majority of papers on time series data mining tested on a??,??dataset, and I showed (although it seems obvious then and now) ,why this is a problem [i]. Now most papers test on 20 to 40 datasets. I don?€?t think we have solved the??, problem, but we have mitigated it somewhat.

,

The other trend I tried to push was the idea that an author had the obligation to make all their code and data availed to the??,, and to the general public. Note that this was not my original idea, the astronomy community take this for granted. This push has been partly successful, but once or twice a month I find myself pestering someone (usually unsuccessfully) for data or code (you know who you are!).
,
,

,

,One thing I love about data mining is that it is a broad tent. There is room for theoreticians, for system builders, for empirical folk etc. I have had students that had skills of various subsets of these qualities, and I love working with all them.

,

As is happens, I am not a theoretician, and I have very poor skills in this area. My first Ph.D advisor, who??,??theoretically minded, told me ?€?,?€?. Fortunately, Mike Pazzani took me on as a student, recognizing that while I could not prove theorems, I was creative and driven. For what is worth, I now have more papers than my first advisor.

,

Given the above, the two qualities I look for in a student are a strong work ethic, and good communication skills. Actually these are related, most of my students do not have English as their first language, so writing clear text requires many many passes, and lots of my ?€?red pencil?€?. As Samuel Johnson said, ?€?,?€?
,
,



,I think perhaps the best science writer alive is Richard Dawkins, and I make my students read his ?€?,?€?. His creative use of analogies, and the extraordinary care he takes in writing make his books a joy to read. I also recommend the books of another Richard,??,, whose curiosity about the universe and ability to see connections between apparently unrelated things inspired me. Finally, the great Carl Sagan?€?s book,??,, is worth reading if only for its ?€?baloney detection kit?€? (non-American readers,??,??in this context means nonsense that people believe in).

,

When I am not working, I am reading, swimming, riding my bike, doing metal or woodwork. However, to a first degree approximation, I am always working!

,

,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,  "
"
By Gregory Piatetsky,  
,, Sep 1, 2014.
,
Here are new entries in August 2014 in KDnuggets Directory.
,
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added new page
,, which has the latest top stories for the recent weeks and months.
,
Added to , page:
,
,??,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
,
By Gregory Piatetsky,  
,, Sep 1, 2014.
,
Today is the 
, in the US, a holiday  which celebrates the American labor movement.
It also traditionally marks the end of summer and is typically celebrated with a barbecue with friends and family.
,
Prompted by all the discussion about robots and automation gradually taking over human jobs, new KDnuggets cartoon looks at how Labor Day could evolve by 2050.
,
,
,
,
,
,
,??,
,
,
 ,  "
"
Most popular 
, tweets for Aug 29-31 were
,
Intro to Parallel Iterative #DeepLearning on #Hadoop Next-Generation #YARN Framework (great slides) ,
,
100 Most Popular #MachineLearning talks at videolectures dot net ,
,
,
100 Most Popular #MachineLearning talks at videolectures dot net ,
,
Data Analytics vs Predictive Modeling vs Data Mining vs Big Data ... Statistics dot com defines the terms ,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Sep 2 and later.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky,  
,, Sep 1, 2014.
,
This poll is now closed - here are the results of the poll:
,.
,
A recent 
, in TechRepublic suggested that there are 2 kinds of data scientists,
,
,
- those that work on ,, where the ultimate decision-maker and consumer of data analysis is a person, and where the models need to be understandable (even if less accurate) and packaged as part of a story.
,
- those that work on ,, where the ultimate user is a computer (e.g. for ad targeting, product recommendations) and the goal is to build the most accurate model possible, with understandability less important.
,??,
This division is also roughly equivalent to data science consulting (human-oriented) vs products (autonomous, machine-oriented).
,
The article also claimed that data scientists usually are good in only one or the other type of analysis, and that managers should hire the right kind of Data Scientist for their task.
,
I don't quite believe that this division is so clear-cut. 
Although I have mostly worked on ""human-oriented"" data science, I have also done machine-oriented analysis, and think that good data scientists can do both. 
  "
"
,
,
In case you missed the part-1 of this article:

,
,
,Next, I would like to introduce Prof. Keogh?€?s current student, ,. Yanping Chen is a Ph.D. candidate specializing in data mining, machine learning, and the applications of these techniques to solve real-world problems in University of California, Riverside (UCR). She has two papers published in SIGKDD 2013 as the first author focusing on data mining time series. She is also doing research on insect detection and classification, and is the lead principal investigator on a $100,000 grant from Grand Challenges Explorations Round 11, funded by Bill & Melinda Gates Foundation.
,

Here is my interview with Ms. Chen:
,
,
,
,The population density of insect vectors is used to predict the insect-vectored diseases. Where insect density is ,current measured, it is typically measured with sticky traps, or the ubiquitous CDC mosquito trap. However, these traps need a trained operator to set them up and check the catch. The greatest problem with these traps is that, there is a , between the time of the insect?€?s arrival and the time they are counted. Under realistic conditions in the developing world this lag may be a week or more, however the adult stage of , is only about two weeks, thus by the time an outbreak is detected it may already be waning, and the damage done.

,

Our research proposed a system to automatically detect and classify flying insects. It can be used to monitor the density of insect vectors in real-time, and thus drastically reduce the need of trained personnel. Most importantly, it provides real-time information that allows health officials to plan effective interventions and to better prepare for future epidemics.
,
,
,
,Our system includes two parts, an optical sensor to record the ?€?sound?€? of insect flight, and a software that leverages on the sensor information to automatically detect and identify flying insects.
,
The optical sensors are ?€?pseudo-acoustic?€? sensors that record the ?€?sound?€? of insect flights from meters away, with complete invariance to wind noise and ambient sounds. These sensors allow us to record on the order of millions of labeled training instances, far more data than all previous efforts combined. The software is a classification framework that is based primarily on the sound of insect flights, but is a principled framework that can incorporate any additional information to improve the classification accuracy.
,
The biggest challenges in this project is to develop an accurate and robust insect classification algorithm. It turns out that the most commonly used feature in the previous efforts, the ,, is not adequate for classifying different species of insects, hence, we need to use moreinformation inherent and accompanying the flight sounds for classification. Moreover, there are several constraints on the classification algorithm. For example, the online classification has to be fast enough to provide real-time information; on the other hand, the algorithm has to be undemanding in both CPU and memory requirements, as any devices to be deployed in the field in large quantities will typically be small devices with limited resources, such as limited memory, CPU power and battery life. To develop an accurate and robust classification algorithm that meets all the above constraints is challenging.
,
,
,
,I am very interested in machine learning and its applications to real-world problems. Currently, I am also working on three other projects, including improving the performance of semi-supervised learning of time-series data[1]; discovering the frequent patterns in a data stream[2]; and scalable machine learning algorithms.
,
,
,
,I like it most to see that my work is better than the state of art in solving real-world problems. It makes me feel that my research matters, and ,gives me a sense of accomplishment. But on the other hand, sometimes I found it very frustrating to do the comparison, as some published work does not have code and data publicly available, and the comparison can be painful especially when there are several parameters to be tuned. Therefore, I really like the idea of making all work publicly available and reproducible.
,
,

,I would like become a data scientist and an expert in data mining in future. My long-term goal is to have my own research team working on an interesting research problem which will have a significant impact in improving people?€?s life when it is finished.
,
,
,
,I am very impressed by the achievement of ,, especially his success in coursera, as well as his research in proposing the new machine learning paradigm, deep learning. Deep learning has been applied to many real-world applications with great success in making significant improvement in prediction and recognition. It would be great if I have the chance to meet Dr. Andrew Ng to discuss about the deep learning and the future trend of machine learning.

,

,
,
,
,  "
"
,
,
Social networks generated by mobile phone users provide a rich platform for the study of human interactions. Understanding communication affinities can help describe?? the semantic content of the social network, and together with the network topology, predict users attributes and preferences.
,
Here we address the following questions:
,
,
,
,
,
To look at age homophily, we had?? access to?? the age of?? approximately?? 500,000 clients of the mobile phone company?? (seed nodes). We first look at the age population pyramid for both female and male seeds, see Figure 1. The distribution has a double peak at ages around 30 and 42 years for both genders.
,
,
,
,
In Figure 2 we plot the histogram of age pairs for clients in the seed set that communicated with each other at least once within a three month period. We?? observe a strong increase in caller-callee pairs for users of similar age signaling a plausible strong age homophily for the whole network. In this figure we also see two weaker off-diagonal peaks indicating communications between users a generational gap apart, plausible due to parent-children communications.
,
,
,
,
,
,
Given the strong age homophily observed among the seed nodes, we propose a graph based algorithm?? to infer the age group of the remaining users in the mobile network composed of over 70 million user with over 125 million connections. Our algorithm is a probability diffusion?? algorithm with memory of the nodes initial state. Each node diffuses a probability vector for the node belonging to one of four age categories (<25, 25-34, 35-50, 50+)?? remembering its vector initial state in each iteration. The algorithm's performance over all nodes in the network was over 46%, where random guessing would give a performance of 25%.
,
,
,
Next, we looked at how performance depended on a node's topological relation to the seed set. We measured performance as a function of 1) seeds in a?? nodes neighborhood, 2)?? node's distance to the seed set, see Figure 3. We observed a significant increase in performance (with respect to the overall performance) for nodes closer to a seed, and up to 62% performance for nodes with three or more?? seeds in their neighborhood.
,
,
,
,
Our methodology has allowed us to significantly increase the quality of our age predictions for user in the mobile phone network. We expect our method to work well for other user's attributes and network topologies showing significant homophily for the given attribute.
,
For more details, see??
, by Jorge Brea, Javier Burroni, Martin Minnoni and Carlos Sarraute, 
,.
,, PhD is a Data scientist at Grandata, a company that integrates first-party and telco partner data to understand key market trends, predict customer behavior, and deliver impressive business results.
,
,
,
,  "
"
By Michael Tsiappoutas, Sept 2014.
,
,
Gain an advanced understanding of mining big data with Lewis University's online, 36-credit 
,
degree with concentrations in 
,  and the 
,. The program aims to produce data science practitioners who clearly understand how to process, store, visualize and analyze data.
,
Its core curriculum starts with a course in Mathematics for the Data Scientist, a handpicked collection of mathematical methods central to understanding subsequent courses in machine learning, statistics, data mining & analytics and statistical programming. Students will also learn how to put analytics into practice and gain an advanced understanding of computer science through courses covering topics, such as:
,
,??,
Students within the program enjoy access to both the faculty of Lewis University and to an esteemed advisory board of industry professionals, who help keep the curriculum current and on the cutting edge.
,
,
,Take the next step toward discovering the meaning behind data with Lewis University's online Master of Science in Data Science degree program. Call (866) 967-7046 to speak with a Graduate Admissions Counselor, or 
,
,
Bio: ,, adjunct professor at Lewis University's MS in Data Science degree, earned his PhD in Engineering and Applied Physics from the University of New Orleans. He is currently a Senior Statistician with State Farm Mutual Automobile Insurance Company (since 2007).   "
"
,
,
,
At Lavastorm Analytics, we believe in giving you the tools to be successful. What better tool than an understanding of the most important skills you need to have today and in the future in order to be successful in your chosen career?
,
,
,
,
We hosted this same survey in 2013, with 500 analysts participating, so if you didn't get a chance to view the results, you can download them here, ?€?,,?€?
,
This new 10 minute survey focuses on you, the person driving analytics, and the skills that you need to have in order to meet the needs of your organization now and in the future. 
,
Participate today and see the challenges being faced by others, the skills necessary to succeed, and gain insight into your colleagues. All survey respondents providing their contact details will receive a copy of the final report in Q4 2014 and ,
,
, and have your voice be heard.
,
, 321 Summer Street, Boston, MA 02210, United States
,
,  "
"
,
By Gregory Piatetsky,  
,, Sep 2, 2014.
,
This is part 2 of my report on 
,,  held August 24-27, 2014, in New York City.
,
Here is part 1:
,.
,
A novel, but great feature of the conference was 
,  - 30 seconds, 1 slide summaries of all the talks, ably run by 
Aris Gionis (Aalto U.) and Jie Tang (Tsinghua U.).  This gave enough of a flavor of each talk to help people decide where to go, and was probably just long enough to keep the attention span of today's researchers, who would turn to their computers and smartphones as soon as they felt even a little bored. 
,
Notably, Macbooks were more visible than Windows laptops. Despite 2000+ attendees, WiFi KDD_ORACLE (thanks to Oracle who sponsored it) worked very well.  Perhaps too well, since many attendees were checking emails or facebook or even running some cool R data mining scripts during the talks.  I can imagine an app that will measure in real-time the interestingness of the talk by changes in wifi activity during the talk. Any takers?
,
,
In the evening on Tue, Aug 26 Bloomberg corporation has sponsored a great
, for all KDD attendees.  The views were fantastic, the food excellent, the drinks were free, but conversation was hard to keep because the music was much too loud.
,
On Wed, Aug 27,  Eric Schadt,  Director, Icahn Institute for Genomics and Multiscale Biology gave a keynote on ,
,.  He came to KDD-2014 shortly after giving a talk at 
, foundation, so his work spans both Big Data and Spirituality!
,
,
if I understood it correctly, Eric Schadt reported on a extremely significant potential discovery, 
where his team found a kind of ""magic module"" network of genes connected to inflammation that is involved in many diseases, including Alzheimer.  Even more amazingly, this network is also active during meditation (hence Chopra connection) and when people are given a placebo.
,
My selected tweets and notes from his talk:
,
,??,
The  Wed, Aug 27 afternoon Panel, with leading researchers
Rakesh Agrawal (Microsoft), Solon Barocas ,, 
Chris Clifton (Purdue), Corinna Cortes (Google), and Rayid Ghani (UIC and Edgeflip), 
focused on the question
,
,.
,
Rakesh proposed Hippocratic Data Systems, with controlled data management and privacy preserving analytics.  
,
Chris Clifton argued data scientists need to worry about privacy issues now, otherwise they will eventually not have data or not be able to do data mining because of government regulations. He proposed to ask for informed consent of data subjects before doing data mining on their data.
,
Solon Barocas said that often the most socially useful inferences are also the most sensitive.  He gave an example of FICO predictor for prescription compliance, which was well-intentioned, effective, but became very controversial because of privacy issues.
,
Rayid Ghani said that data miners can know a little about you, and predict the rest - most people don't realize it.  This power can be used for good (eg improve healthcare, education) or bad purposes. He gave an example of a model for predicting high-school drop-outs.
If the model says Johnny is likely to fail and needs help with this subject, will this be an invasion of privacy?
Rayid pointed to problems with desire to get subject consent in advance, and said that data is frequently collected for transactions; We usually dont know in advance how to use it for social good.
,
Here are my selected tweets and notes:
,
,??,
The panel seemed to agree that there were no easy answers. 
If the goal is to help real people, anonymization may not help.
,
The panel was followed by excellent and entertaining keynote from 
,
Sendhil Mullainathan (Harvard), ,,
,
,
Sendhil showed this picture (thanks to Xavier Amatriain , for the tweet) that is a good example of human contradictions in behavioral economics 
,
,
,
Prof.  Mullainathan talked about how he would redo some of his earlier work, and focused on two barriers:
,
,??,
He described his ground breaking work analyzing unemployment rate among US college graduates. He found a big gap by race. Was it caused by discrimination or skills gap?
,
His group collected a large number of fake resumes, half sent with ""white""  and half with ""black"" names.  The shocking result is that  after sending large number of fake resumes, ""white"" names had 50% higher call back rate than ""black"" names.
,
More selected tweets from me and others from his talk: 
,
,??,
One conclusion from Prof.  Mullainathan talk was that we frequently don't know in advance what data we need to collect, so asking data subjects for specific consent will significantly restrict research.
,
Among many other talks, I want to mention 
,.
,
,
For those who could not attend, 
, are available in ACM Digital Library.  I was told that all sessions were video-recorded, so videos should also be available soon.
,
Just before the conference, I also attended,
,: Broadening Participation in Data Mining, 
ably organized by Brandi Marshall and Caio Soares.
,
BPDM goal to increase diversity and representation of minorities in data science, 
and there were many good students there with whom I had the pleasure to interact.
,
As a fun and refreshing activity, several KDD organizers, including me
received and completed an 
,
,
At KDD-2014, 
I was surprised to find myself
in the position of a minor celebrity, with students and non-students 
asking to take 
,.
,
KDD-2014 marks 25 years after the very first 
, I organized, and I could not imagine 25 years ago
how a field would grow and change. 
,
Looking forward to the next 25 years of KDD success!
,
Other reports on KDD-2014
,
,??,
,
,
 ,  "
"
        ,
,
,
provides a complete set of semantic technology transforming how organizations identify meaning across massive amounts of unstructured data. Ontotext blends text mining, powerful SPARQL queries, semantic annotation and semantic search with an RDF graph database (GraphDB???) that infers new meaning at scale. 
,
,
,
Today, Ontotext is used to power the world's largest media websites and supports knowledge management applications using tens of billions of semantic facts in publishing, healthcare, life sciences, financial services, governments, museums and archives. Ontotext technology delivers highly relevant search results for improved decision making - all in real time.   
,
Ontotext also provides 
,  - allowing developers to build text mining and semantic applications in the cloud. S4 includes text mining, reliable access to Linked Open Data for entity enrichment, GraphDB??? and developer tools. 
,
,
,
S4 is available in a hosted environment with a pay-as-you-go model.  
,
To find out more about Ontotext visit ,
,
Check also webinar:
,
,,
,Sep 30, 8 am PT, 11 am ET
,
The CEO and Founder of Ontotext, Atanas Kiryakov, is a 14 year veteran of semantic technology.  He will discuss the use of graph databases, text mining and semantic tools to make sense of your text and data.  
  "
"
        ,  "
"
,
,
,
Research practitioners from leading organizations will join us for this year's BayesiaLab User Conference, the only event dedicated to applied research and analytics with Bayesian networks and BayesiaLab. Experts will share innovative applications of Bayesian networks in a wide range of courses and case study presentations.
,
,
,
,
,
,??,
,
,
,
,??,
,
,
,
,
+1 888-386-8383,
,
,  "
"
,
,
,
When Michael Berry and Gordon Linoff met, predictive analytics and data mining techniques were quite different from those used today. And they say that each time they revise , it?€?s like coming out with a whole new book because the world changes so quickly. Berry and Linoff will talk about the current data mining landscape, including new methods, new types of data and the importance of using the right analysis for your problem.
,
,
,
,
,
,
,??,
On-demand webcasts featuring additional thought leaders are also available.
,
,  "
"
By Gregory Piatetsky,  
,, Sep 3, 2014.
,
Here are 57 upcoming September 2014 - March 2015 meetings and conferences.
The most meetings are in San Francisco, CA (7), Boston, MA (4), and London, UK (3).
,
You can find the full list on KDnuggets page:
,.
,
Color code:
,
,??,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
Latest ,, (Sep 03, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
With better crime prediction, crime rate can be cut in half, keeping the same number of people in jail, Sendhil Mullainathan (Harvard), ,.  "
"
,
,
,
,
,
,
  "
"
By ,, ,
,
,
A decade back, when ?€?analytics?€? was more an esoteric buzzword than an organizational necessity, leaders of fledgling analytics divisions focused one key goal - ?€?growth?€? .
,
Today, with increasing acceptance of analytics across sectors and the explosive expected growth in Analytics software and services, the same leaders have a new goal ?€? how to drive value and impact while maximizing efficiency.
,
As the CXO crowns and goals keep shifting and companies keep looking for Analytics to drive differentiation, the need for Analytics leadership is also changing.
,
Thomas Davenport defined the stages of analytics journey as follows:
,
,
,
With the changing business needs, analytics leadership needs to be in synch with the stage of analytics growth. Analytics leaders need to shift from purely a growth mindset to a thought leadership mindset as shown in the graph below.
,
Are many analytics leaders stuck in stages 1 and 2 of leadership while the companies are trying to move towards being Analytical competitors?
,
Point to ponder. Do we need more leaders now to say ?€?why not?€? and less to question ?€?why?€??
,
,
,
Original:
,.
,
Bio: , is Analytics Leader at BRIDGEi2i Analytics Solutions
,
,
,
,  "
"
Most popular 
, tweets for Sep 01-02 were
,
TED Talk: data scientist John Wilbanks says pool our medical #BigData - strict #privacy laws stymies research ,
,
#DataMining Reddit using Python and R #rstats - nice example ,
,
At #KDD2014 growing gap between dense technical sessions & practical talks where simple models work in real world ,
,
#DataMining Reddit using Python and R #rstats - nice example ,
,
,  "
"
        ,  "
"
        ,  "
"
,
,
,??,??
,  "
"
,
,
Here is the company, startup, and acquisition activity for August 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are my tweets
,
,??,
Here are previous months activities:
,  "
"
,
,
, has more than 25 years of industry, research, strategic consulting, and teaching experience in the areas relating to databases, enterprise data management, data warehousing, business intelligence, advanced analytics, and Big Data. He has also contributed to SQL, ODBC and IDAPI database standards. Ajay built, and heads up the Global Analytics & Big Data practice for TCS Insurance & Healthcare customers. 
,
He has frequently spoken at industry conferences, authored whitepapers, and has driven thought leadership in the Data industry. In addition, he has actively taught (Analytics, Database Design, and Data Mining etc.) at The University of Texas, Austin, and College of Engineering, Pune, and mentors , for global competitions.
,
Ajay holds an M.S. in Computer Science and M.S. in Aerospace Engineering from The University of Texas at Arlington. He obtained his B.Tech in Aeronautical Engineering from Indian Institute 
of Technology, Mumbai in 1984.
,
Here is first part of my interview with him:
,
,
,
,: 
,
,
,
,At the most basic level, harnessing is the amassing of Big Data; it relates to how insurers manage Big Data and how they create an ecosystem that can not only create Big Data but sustain it as well. Years ago, harnessing data was much easier than it is today; however, benefits of using this data were more limited as well. Harnessing Big Data involves accessing a combination of internal and external sources of data, structured and unstructured data like social media as well as newer technology that provides access to data and the ability to analyze it. 
,
Harvesting utilizes technology and algorithms that enable organizations to analyze and deliver actionable insights and derive real value from Big Data. Skills such as statistical analysis, data mining, econometrics, business analytics, and visualization techniques, are in high demand as they provide a solid foundation for deriving useful insights from the data. Universities have started trying to fill the supply demand gap by offering various graduate programs in business analytics to provide for the next generational skills needed to mine actionable insights. 
,
Below is a table that compares and contrasts Harnessing and Harvesting dimensions of Big Data.
,
,
For more information, refer to my whitepaper on this subject ,.
,
,
,
,Whether we look at property (e.g. personal home, commercial building) and casualty (e.g. personal car, fleet of trucks), or life insurers, or even reinsurers, some of the challenges (and hence opportunities) they face are quite common in nature. Here are some examples: 
,
,
,
,
,In the last decade or so, a series of advances have taken place that puts the healthcare analytics at a very opportune moment to take off in this century. Some of these include: 
,
, ?? ,
,
,
,
,
,Descriptive Analytics (sometimes referred to as Business Intelligence) is lot more past looking, trying to figure out what has happened, or look at past trends across various dimensions. Usually, these phenomena are visualized via reports, scorecards, dashboards, etc. using simple visualization widgets, such as bar charts or histograms, pie charts, box and whisker plots, scatter plots, trend graphs etc. The goal is to summarize data and intrinsic relationships, especially between metrics (or facts) computed at the intersection of many attributes. An example of descriptive analytics would be a report (visualized with the help of a pie chart) that shows different types of auto accident injury claims, by severity, frequency, location, and time over the last year in the US state of Texas. 
,
Predictive Analytics, tries to predict the likelihood of certain events to occur in the future, based on analyzing data from the past. Many a times, identifying what variables are key factors in a certain event occurring, is an important part of the analysis. Classification and prediction models, along with discovery of trends, patterns, and relationships fall under the purview of predictive analytics. A good example of predictive analytics is to predict the likelihood of a claim to be suspicious or not. 
,
Prescriptive Analytics, just like predictive, is forward looking as well. But here, lot more simulations (what-if scenarios) and optimizations are modeled. In simulation, a lot of modeling experiments are performed to understand what might happen (effect) based on different choices of perturbations (causes). In optimization, we are looking to optimize the most efficient route to certain outcomes. The recommendations from analysis are lot more nudging to perform certain actions. In health insurance, for example, various simulations could be performed for the patient to show onset of the next stage of a chronic disease such as diabetes, could occur much quicker, if medication schedule and dosage is not adhered to, at varying degrees of non-adherence. 
,
All the three kind of analytics become lot more useful if actionable insights are derived and acted upon to achieve strategic objectives, and engrain this process as part of an analytics-driven culture. 
,
,.
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
By Grant Marshall, Sept 2014
,
Today, we look at the top 25 most viewed data mining lectures on ,
,
The videos are taken from the ,. These are the videos, including authors, length, and venue, sorted by views:
,
,
Looking at the content of the titles of these most popular videos, we may find hints about what the most popular topics in data mining among the viewers of videolectures.net.
,
Based on this visualization, it seems lectures on MapReduce and Web Mining strike a chord with data mining lecture viewers.
,
One interesting facet of the data to look at is the length of the video compared to the number of views on the video. Below is a scatter plot mapping the rank of the video against its length.
,
,
,
There is a slight inverse correlation between the length of the video and its rank in views. The three major outliers to this general trend are ,, ,, and ,. This means that, when it comes to data mining lectures, deeper content-filled lectures can be very popular, even if they take more time to develop.
,
,
,
Note that this only applies if we sort by the number of views. If we take the videos in the order sorted by videolectures.net?€?s popularity metric, there is no clear correlation between length and popularity. This is because the criteria for a ?€?popular?€? video as defined by videolectures.net account for more factors than simply the number of views, potentially including features like votes and publishing date.
,
,
,
,  "
"
Guest post by ,, ,, ,, Sep 9, 2014
,
,Big data and analytics are all around these days. IBM projects that every day we generate 2.5 quintillion bytes of data. This means that 90% of the data in the world has been created in the last two years. Gartner projects that by 2015, 85% of Fortune 500 organizations will be unable to exploit big data for competitive advantage and about 4.4 million jobs will be created around big data. 
,
Although these estimates should not be interpreted in absolute sense, they are a strong indication of the ubiquity of big data and the need for analytical skills and resources, because as the data piles up, managing and analyzing these data resources in the best way become critical success factors in creating competitive advantage and strategic leverage. To address these challenges, companies are hiring data scientists. However, in the industry, there are strong misconceptions and disagreements about what constitutes a good data scientist. Here are the key characteristics of what makes up a good data scientist:
,
,
,
,As per definition, data scientists work with data. This involves plenty of activities such as sampling and pre-processing of data, model estimation and post-processing (e.g. sensitivity analysis, model deployment, back-testing, model validation). Although many user-friendly software tools are on the market nowadays to automate this, every analytical exercise requires tailored steps to tackle the specificities of a particular business problem. In order to successfully perform these steps, programming needs to be done. Hence, a good data scientist should possess sound programming skills in e.g. R, Python, SAS ?€? The programming language itself is not that important as such, as long as he/she is familiar with the basic concepts of programming and knows how to use these to automate repetitive tasks or perform specific routines. 
,
,
,
,Obviously, a data scientist should have a thorough background in statistics, machine learning and/or data mining. The distinction between these various disciplines is getting more and more blurred and is actually not that relevant. They all provide a set of quantitative techniques to analyze data and find business relevant patterns within a particular context (e.g. risk management, fraud detection, marketing analytics ?€?). The data scientist should be aware of which technique can be applied when and how. He/she should not focus too much on the underlying mathematical (e.g. optimization) details but rather have a good understanding of what analytical problem a technique solves, and how its results should be interpreted. In this, training of engineers in computer science and business/industrial engineering should aim at an integrated, multidisciplinary view, with recent grads formed in both the use of the techniques, and with the business acumen necessary to bring new endeavors to fruition. 
,
Also important in this context is to spend enough time validating the analytical results obtained so as to avoid situations often referred to as data massage and/or data torture whereby data is (intentionally) misrepresented and/or too much focus is spent discussing spurious correlations. When selecting the optimal quantitative technique, the data scientist should take into account the specificities of the business problem. Typical requirements for analytical models are: action-ability (to what extent is the analytical model solving the business problem?), performance (what is the statistical performance of the analytical model?), interpret-ability (can the analytical model be easily explained to decision makers?), operational efficiency (how much efforts are needed to setup, evaluate and monitor the analytical model?), regulatory compliance (is the model in line with regulation?) and economical cost (what is the cost of setting up, running and maintaining the model?). Based upon a combination of these requirements, the data scientist should be capable of selecting the best analytical technique to solve the business problem. 
,
,
,
Like it or not, but analytics is a technical exercise. At this moment, there is a huge gap between the analytical models and the business users. To bridge this gap, communication and ,visualization facilities are key! Hence, a data scientist should know how to represent analytical models and their accompanying statistics and reports in user-friendly ways using e.g. traffic light approaches, OLAP (on-line analytical processing) facilities, If-then business rules, ?€? He/she should be capable of communicating the right amount of information without getting lost into complex (e.g. statistical) details which will inhibit a model?€?s successful deployment. By doing so, business users will better understand the characteristics and behavior in their (big) data which will improve their attitude towards and acceptance of the resulting analytical models. Educational institutions must learn to balance, since it is known that many academic degrees prepare students that are skewed to either too much analytical or too much practical knowledge. 
,
,
,
,While this might be obvious, we have witnessed (too) many data science projects that failed since the respective analyst did not understand the business problem at hand. By ?€?business?€? we refer to the respective application area, which could be e.g. churn prediction or credit scoring in a real business context or astronomy or medicine if the respective data to be analyzed stem from such areas. 
,
,
,
,A data scientist needs creativity on at least two levels. First, on a technical level, it is important to be creative with regard to feature selection, data transformation and cleaning. These steps of the standard knowledge discovery process have to be adapted to each particular application and often the ?€?right guess?€? could make a big difference. Second, big data and analytics is a fast evolving field! New problems, technologies and corresponding challenges pop up on an ongoing basis. It is important that a data scientist keeps up with these new technologies and has enough creativity to see how they can create new business opportunities. 
,
,
,
We have provided a brief overview of characteristics to be looked for when hiring data scientists. To summarize, given the multidisciplinary nature of big data and analytics, a data scientist should possess a mix of skills: programming, quantitative modelling, communication and visualization, business understanding, and creativity! The figure below shows how to represent such a profile. 
,
,
,
,
,
, is a professor at KU Leuven (Belgium), and a lecturer at the University of Southampton (United Kingdom). He has done extensive research on Big Data and Analytics. His findings have been published in well-known international journals and presented at international top conferences. He is also author of the books: ,, and , published by Wiley in 2014. His research is summarized at ,. He also regularly tutors, advises and provides consulting support to international firms with respect to their analytics strategy.
,
, is professor at Universidad de Chile (Chile) and teaches Data Mining and Operations Management at the undergraduate and graduate level as well as for executive education. His research interests are data mining, the respective methodological developments, and applications in areas such as finance, marketing, and security-related topics. The results of his work have been published in leading scientific journals. 
,
, is an Instructor Professor at the University of Talca, Chile, currently on leave as a Visiting Research Fellow at KU Leuven (Belgium). He is an Industrial Engineer, holds a Master in Operations Research, and a PhD in Engineering Systems from University of Chile. He has served as the Research Director of the Finance Center, U. Chile, and has published in several Data Mining and Operations Research journals. His research interests cover Credit Risk, especially applied to Micro-Entrepreneurs, and Data Mining models in this area.
,
,
,  "
"
,
,
, has more than 25 years of industry, research, strategic consulting, and teaching experience in the areas relating to databases, enterprise data management, data warehousing, business intelligence, advanced analytics, and Big Data. He has also contributed to SQL, ODBC and IDAPI database standards. Ajay built, and heads up the Global Analytics & Big Data practice for TCS Insurance & Healthcare customers. 
,
He has frequently spoken at industry conferences, authored whitepapers, and has driven thought leadership in the Data industry. In addition, he has actively taught (Analytics, Database Design, and Data Mining etc.) at The University of Texas, Austin, and College of Engineering, Pune, and mentors , for global competitions.
,
Ajay holds an M.S. in Computer Science and M.S. in Aerospace Engineering from The University of Texas at Arlington. He obtained his B.Tech in Aeronautical Engineering from Indian Institute 
of Technology, Mumbai in 1984.
,
,.
,
Here is second part of my interview with him:
,
,
,
,: 
Having a Data Governance program increases the odds of success in any data initiative. The program must be established at a strategic, tactical, and operational level. The program must encompass people, process, technology, and data dimensions of the program. The involvement of both IT and Business is crucial for a timely, high-quality, within budget delivery of initiatives. 
,
,
,
,
,
,As explained earlier, big data?€?s ?€?harvesting?€? aspect is achieved using analytics. But, irrespective of a big data platform (harnessing) or not, you need to use analytics to derive actionable insights and value. If the solution demands leveraging and investing in a big data platform, to deliver value when you want to 
,
, ?? ,
Most of these initiatives start as a proof of concept. Many a times, a big data platform is introduced (as a proof of technology), along with specific use cases (proof of value) to lay the foundation and enable value creation for the future. 
,
,
,
,In the eighties and nineties, the focus was more on decision support systems to make better informed decisions from structured data in relational databases and mainframe systems. The emphasis was more on computing quicker and less on the analysis of data. ,In more recent years, the shift has been more towards analysis from not only structured, but unstructured text, audio, and video data. The revival of machine learning techniques from artificial intelligence days, the application of cognitive computing, along with advances in natural language and speech processing, massively parallel computing environments, analyzing large amounts of generated data, more and more personalized and prescriptive analytics, coupled with deep focused knowledge of business domain problems is where I believe the expectations from data science are heading. 
,
,
,
,To me, a person?€?s attitude, willingness to learn, and analytical techniques used to improve one?€?s learning are pretty important to look for in a candidate. In today?€?s world, ,having a diverse group of individuals from a multi-disciplinary field brings balance and perspective to solving business problems by a data science team. The science of problem solving (the ?€?HOW?€?), art of storytelling from what the numbers and visualization are trying to tell you, the perpetual intellectual curiosity of asking the ?€?WHY?€? behind the ?€?WHAT?€?, listening to needs of business etc. are some examples of qualities I constantly try to gauge, when interviewing for data science positions in my team. 
,
,
,
,There are quite a few skills that need to come together for real-life analytics initiatives, not all of which will be present in any one person. There are numerous graduate level programs that have started in various universities over the last five years or so, to try and fill the enormous gap between low supply and high demand for analytics professionals worldwide.
,
,
,
In addition, introducing concepts and career options, and spotting and grooming such talents as early as high school level, is a must for any company, industry, or for that matter, country to maintain a competitive edge. 
,
,
,
,I once read somewhere: ,
?€?Pursue and excel in what you love, and get someone else to pay you for it?€? :-)
,
,
,
,I recently read ?€?,?€? by James Stewart. It explains calculus from multiple angles. Feel like taking calculus all over again from the eyes of this textbook. 
,
When I am not working, I like to travel with my family and meet folks from different cultures, and when possible, watch some college football and international tennis.
,
,
,  "
"
,
,
Web-data platform, , has raised a $3m ?€?super seed?€? round that will help them expand their new San Francisco office. The investment comes from a number of high profile investors including MySQL co-founders David Axmark as an angel, and Yahoo! co-founder Jerry Yang through ,.
,
In line with the investment, , released a streamlined version of their web data extraction tool. This version 2.0 in currently in beta and comes complete with a new design and added functionality.
,
They have improved data training so that all you need to do is navigate to a website, click a single piece of data on the page - such as price, image, or URL - and their app will find all the other examples of similar data on the website, immediately creating a structured table of data.
,
This latest version of the extractor includes an exciting new feature labeled ?€?,.?€? Users now have the ability to extract an entire page of data with a single click of a button, simultaneously creating an API.
,
,
,
But what does this mean for the wider data landscape? ?€?,?€? said their CEO, David White.
,
More New Features include:
,
, ?? ,
,
,  "
"
,
,
, is President and CEO of ,. In his role, Mr. Otto is responsible for Alpine?€?s strategic direction, growth initiatives and overall execution. Prior to joining Alpine, Joe served for five years as the Senior Vice President of Sales and Services for Greenplum, where he established Greenplum?€?s commercial footprint and developed it into a global business. 
,
Joe?€?s 30 year technology career includes significant leadership roles at Sun Microsystems, Cisco Systems, and EMC, as well as executive leadership positions in early stage startups in Artificial Intelligence, Networking and CRM. Throughout his career Joe has developed high performance sales and marketing teams, driving innovation to deliver business transformation and extreme growth. Joe is a graduate of The Ohio State University with a Bachelors degree in Electrical and Computer Engineering.
,
Here is first part of my interview with him:
,
,
,
,Alpine Data Labs continues to gain momentum. The role of collaboration in the Big Data process ,is the next big thing. Alpine is leading the way with Alpine Chorus, the industry?€?s first collaborative Advanced Analytics solution for Big Data.  Our mission is straightforward: empower all stakeholders ?€? from executives to business analysts to data engineers and partners ?€? to engage in the process of analytics.  
,
As you know, we recently shipped Alpine Chorus, our latest software version but other recent milestones include the following; Alpine was one of the first Enterprise Advanced Analytics Platforms to be certified by Databricks on Apache Spark, showcasing query performance up to 100x faster than Hadoop MapReduce; Alpine joined the Gartner?€?s exclusive Magic Quadrant club and we?€?ve established significant partnerships with important companies like Pivotal, Cloudera or QlikTech.  Last year Alpine closed $16M Series B Funding ?€? bringing our total funding to $23.5M.
,
,
,
,The improved resource management and job scheduling provided by MR2 and YARN have positively affected Alpine in a number of ways. Most prominently, we've seen a major improvement in our ability to control resource usage of long-running analytic workflows in Hadoop. The ability to have multiple Application Masters provided by MR2 also allows Alpine to support more concurrent users of our application. YARN also allows Alpine to easily deploy new execution frameworks, such as Spark, to give our users more flexibility and choice as they develop complex machine learning models.
,
,
,
,Yes, we have a fast growing list of happy customers in all verticals at this point.  And for each, our goal is the same.  Help drive as much business value faster on top of their ,Big Data investment.  Customers choose us because of our modern and collaborative approach and also because our technology innovation: we provide machine learning at scale, our platform can process every algorithm in parallel and works equally on Hadoop and non-Big Data sources.  That?€?s a lot of value.  If you add to this, our scalability and usability advantages, and, the fact that you can procure Alpine Chorus simply via a subscription and host it on a light-weight server, our product couldn?€?t be any more ideal for the ?€?Big Data?€? era.
,
Customers are achieving amazing breakthroughs with our solution.  To pick one, I?€?d talk about Havas ?€? the world?€?s fastest growing media company.  They use Alpine Chorus to empower employees and customers with advanced analytics. They have been able to generate more value out of their data faster, involve more people into their analytical process and build a business centered around the idea of what we call ?€?Big Math?€? (e.g. data computation at scale, on all data and available to all people). Havas has gone way beyond making data science accessible to their business analysts, and not just made it usable by their customers. Havas is harnessing the power of strong understanding of the correlation between marketing activities and results.  Now, that?€?s what I call ?€?culture change?€?!
,
,
,
, By broadening the base of people who can participate in the data analytics process across the entire organization, we are creating a Data Nation inside your company and our approach has a profound and lasting impact on the culture of the organization.
,
,.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Sep 9 and later.
,
See full schedule at , .
,
,  "
"
,
,
First Massive Open Online Course on ,

,

Starts: ,

,

For more information and to register visit:
,
,.
,
,??is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis.??,??Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically). 
,
This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using a booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as ""data science in action"".
,
,

,

The Coursera course ?€?Process Mining: Data science in Action?€? explains the key analysis techniques in process mining. Participants will learn various process discovery algorithms. These can be used to automatically learn process models from raw event data. Various other process analysis techniques that use event data will be presented. Moreover, the course will provide??,,??,, and??,??to??,??in a variety of application domains.
,
,
,  "
"
,
By Grant Marshall, Sept 2014
,
Yesterday, we looked at the , on ,. Today, we look at the big data lectures.
,
,
On a first cursory glance, compared to the data mining lectures, it is clear that the big data lectures tend to have far fewer views (741.56 views on average vs. 2972.44).
,
Now we will look at the contents of the titles of the lectures to try to glean information about what makes for a popular big data lecture.
,
,
,
It seems that when it comes to big data lectures, the web and streaming data are very popular common topics.
,
Now we will look at how the length of the lecture relates to its view count. We exclude the number 1 lecture because it is , more viewed than all of the other lectures.
,
,
,
Like with the data mining lectures, there is a slight correlation between longer videos and more views, possibly indicating that well thought-out and thorough lectures are more desirable than shallow ones.
,
Unlike the data mining lectures, videolectures?€?s popularity metric follows a similar correlation to the views.
,
,
,
,  "
"
,
,
, is becoming critical. Customers are more empowered and connected than ever. And becoming more so. Customers have access to information anywhere, any time ?€? where to shop, what to buy, how much to pay, etc. That makes it increasingly important to predict how customers will behave when interacting with your organization, so you can respond accordingly. 
,
, (June 19-20, 2014) was organized by Innovation Enterprise in Chicago. It brought together analytics executives and data scientists working in retail, ecommerce and 
consumer goods, offering unique insight into the innovations that are revolutionizing their relationship with customers.
,
We provide here a summary of selected talks along with the key takeaways. 
,
Here are highlights from Day 1 (Thursday, June 19, 2014):
,
, delivered a talk titled ?€?The Return on a Share: Quantifying the Monetary Value of Social Sharing?€?. He initiated the talk discussing about the growth of the social web, not just in terms of people?€?s adoption of social tools but also in terms of proliferation of different ways people use to share content. Now, proliferation has been rapid. This user created content is available in such a large volume that it has actually changed the way people make purchase decision. He mentioned that even the impact of sharing is well documented. ShareThis commissioned the first study of its kind to measure the impact of sharing on purchase process. They used conjoint methodology to determine the relative importance of various factors affecting the consumer purchase decision. Key findings:
,
, ?? ,
At the end, he shared the virtuous circle of sharing: Sharing -> Insights -> Optimization -> Earned Media -> Larger Audience.
,
, talked about ?€?A/B Testing + Subscription Models?€?. He kicked off his talk reviewing subscription-based business. Netflix has a model of free trial and paid monthly subscription. Business having Netflix like subscription models are concerned with getting more subscribers. Talking about conversion testing he said that standard techniques do not succeed and need longer testing lead times. Regarding experimentation cycle, for faster results, one has following options:
,
, ?? ,
He discussed cell scoring using proxy metrics ?€? methodology to score test cells by assigning a lifetime value (LTV) to each subscriber in the cell as a function of proxy metrics. The biggest advantage of this methodology is that it is aligned with Netflix?€?s business objective since LTV is a long-term engagement metric. He also shared cell scoring using consumption ?€? applied to get the predicted retention for the first N months where N should be wherever the retention levels out. He concluded the talk sharing conversion testing implications.
,
,

, and , gave an interesting talk on ?€?Driving Page Optimization with Data?€?. They shared quick facts about Ancestry: 2.7 million total subscribers, more than 30 billion records and images, etc. They shared insights from a project with objective to improve conversion on Ancestry.com?€?s key offer pages. In phase one, they worked on reducing choices, creative design and duration defaults.  This led to 20% increase in conversion. 
,
,
Reducing the friction of choice on the offer page led to a significant increase in conversion ?€? but only at the right price point.
,
During second phase, they created template standardizing look and designing for easy customization/optimization though targeted content. However, this led to 7.5% decrease in conversion. On detailed analysis they found that type of button, price and images position were most important variables to consider. At last, they found standard button with price below and images left led to increase in conversion by 1.94%. 
,
,
,
They concluded the talk recommending mapping out primary user journeys and conversion rates in order to identify top opportunities.
,
,.
,
,
,  "
"
,
By Gregory Piatetsky,  
,,  August 2014.
,
,
,, and its press release said
,
,
In June, I published a summary of 
,, and commented that 
,
,
This comment got the attention of Dell, and they arranged for me to talk to its key executives on analytics side.
,
In the past, I had interviewed Dr. Thomas Hill, StatSoft VP of Analytics and co-author of key papers on Cognitive Mining - see 
, 
and I was interested to see what is happening to StatSoft and Statistica after acquisition.
,
I spoke to John K. Thompson (General Manager, Global Advanced Analytics) who himself joined Dell only recently) and John Whittaker (Senior Director, Product Marketing), responsible for all of Information Management business.  Thanks to Scott Desiere from Dell for arranging the conversation.
,
Where does Analytics fit into Dell?
,
Dell is a very large company, which delivers comprehensive, platform- and data-agnostic, modular solutions, delivered via many data platforms including Oracle, SQL Server, Hadoop, MongoDB, and IBM DB2.
,
Its major business units include 
,
,??,
Since Dell is not providing their own data platform, they can be an integrator and offer best-of-breed solutions.  Dell message of lower TCO, aligning business and IT finds its sweet spot in the middle market - companies with 500 - 5,000 employees (although Dell also has Fortune 50 customers).
,
Dell BI/Analytics software solutions include
,
,??,
However, Dell was missing an advanced analytics tool and Statistica nicely filled that void.
,
,
,
,
,
First, the name will be changed to 
,, with ""StatSoft "" gradually retired.
,
Former StatSoft CEO Dr. Paul Lewicki became CTO of Dell Software information management group,
while StatSoft VP Dr. Thomas Hill became Executive Director of Analytics, reporting to John Thompson.  
,
Dell Statistica has more than 1 million users worldwide, with vertical-specific expertise in a number of areas including pharmaceuticals, financial services, technology and manufacturing. Dell does not break out user totals by region, but StatSoft was known to have a strong user community in Europe.
,
From over 100,000 employees worldwide, there are thousands whose jobs in some form or function involve analytics.  StatSoft group was about 100 people, half of them in analytics roles.  Post acquisition, Dell hired 18 new people, 15 of them in sales and marketing.
,
Dell is very excited about the future of Statistica, both in terms of its synergy with the broader Dell Software portfolio and its continued impact on the advanced analytics industry in general. 
,
The acquisition of StatSoft was in March, and John Thompson, General Manager, Global Advanced Analytics, came on board after that, so he is still working to determine the specifics of the product roadmap
for Statistica and related products over a multi-year period. 
,
,
,
I have asked some specific questions and here are the answers:
,
,
Dell is aggressively investing in the development of Statistica, hiring new people, and is committed to delivering market-leading functionality, not only in the product itself, but through Dell ability to support key industry standards, as well as complementary products and technologies.
,
,
StatSoft pricing was very complex, and Dell plans to simplify the pricing, ranging from
about $2000/seat to enterprise-wide licenses.
,
,
Cloud deployment of Statistica is a part of the roadmap planning.
,
,
Statistica can access data in many file formats and storage systems including Hadoop. 
,
,
Statistica was an early supporter of R. Dell continues to support the R community and the language to extend and leverage the value that they obtain from a joint Statistica+R environment. 
,
, Dell supports PMML and will continue to do so going forward.
,
,
,
Some of the notable conference at which the Dell Statistica team will be in attendance include 
,
,??,
Statistica would be a valuable addition to a Dell integrated solution, especially for mid-size companies.  However, Dell emphasized to me that they will continue to support the standalone tools marketplace, and plan to strengthen the core Statistica offering. 
,
,
,  "
"
,
,
, is President and CEO of ,. In his role, Mr. Otto is responsible for Alpine?€?s strategic direction, growth initiatives and overall execution. Prior to joining Alpine, Joe served for five years as the Senior Vice President of Sales and Services for Greenplum, where he established Greenplum?€?s commercial footprint and developed it into a global business. 
,
Joe?€?s 30 year technology career includes significant leadership roles at Sun Microsystems, Cisco Systems, and EMC, as well as executive leadership positions in early stage startups in Artificial Intelligence, Networking and CRM. Throughout his career Joe has developed high performance sales and marketing teams, driving innovation to deliver business transformation and extreme growth. Joe is a graduate of The Ohio State University with a Bachelors degree in Electrical and Computer Engineering.
,
,.
,
Here is second part of my interview with him:
,
,
,
,Many C-level executives in the market for a ?€?Big Data solution?€? today find themselves fairly confused by the messages they hear from technology vendors. There's a lot of hype and many vendors are throwing around the term ""Big Data"" in an attempt to hop on the bandwagon.,
, In fact, most enterprise environments are heterogeneous consisting of a combination of traditional data sources, MPP databases AND Hadoop. Look for solutions that don?€?t force you to change but rather, application that embrace your data environment and focus on driving more value from it, quickly. , , Visualization is a great tool to understand what's in your data, but visualization tools can't replace serious math.
,
,
,
,Most of the competitive alternatives are desktop-based or point solutions without any collaborative capability.  Alpine Chorus is a modern, web application whose main tenant is collaboration.  On top of collaboration and search it provides modeling and machine learning under the same roof. Also, we are focused on ?€?No-Data Movement?€? deployment. Meaning, that regardless if a company?€?s data is in Hadoop or MPP Database, Alpine Chorus sends instructions out without ever moving data.  This technology, called In-Cluster Analytics allows for massive savings (from storage to management) and unbound scalability. 
,
Since the data hasn?€?t been moved to a dedicated analytical server, Alpine Chorus can scale across your entire set of Hadoop Clusters for example. It is different from competing startups because it is web, collaborative and has In-Cluster technology.  It is also different because it has an End-to-End approach so users can work from data transformation to modeling and analysis. Chorus also provides built-in search capabilities.  Just like they do with Google, users can search for all types of information, from people, to datasets, to data projects and others.
,
,
,
,It?€?s as important for our data scientists to not just be skilled as data scientists, but to also be adept consultants and coaches for our customers. They must be able to expertly ,manipulate databases, collaborate with team members and successfully communicate analyses to those outside of world of data. It?€?s important for our data scientists to be good communicators so that they could effectively impart the value of predictive analytics. It?€?s not enough to just convey the numbers and percentages, but the story needs to be told n the context of the business benefit so that all stakeholders buy into the effort. In addition, time management, planning and the ability to be agile are skills that our data scientists possess. 
,
,
,
,There are so many good books and blogs on the subject but a few to note ?€? ,;,; ,; ,; ,; or , by Tom Davenport.
,
,
,  "
"
,
By Grant Marshall, Sept 2014
,
Today, we look at the top 25 most viewed data mining lectures on ,
,
The way popularity is determined is by looking at the ?€?,?€? sort on the machine learning video listing. These are the videos, including authors, length, and venue, sorted by views:
,
,
,
One thing you?€?ll immediately notice about this list is the number of these lectures that come from the ,. Overall, more than half (thirteen out of the twenty-five) of the most popular machine learning lectures on videolectures.net come from here, taking the top four spots.
,
Now we will look at the titles of the videos to determine what makes a popular machine learning video.
,
,
,
This visualization shows a clear focus on topics like theory, modelling, and statistics - showing an affinity for the theoretical aspects of machine learning among the viewers of videolectures as opposed to more applications.
,
Now, we look at how the length of the lectures affect the views on the videos.
,
,
,
Much like the other categories of videos on videolectures, longer lengths are correlated with more views, indicating again that deeper content is more popular among videolectures?€?s viewers.
,
,
,
,  "
"
,
,
, is Senior Principal at ,, where he advises executives on business value creation from technology innovation. He leads strategy engagements with executives, helping them use Advanced Analytics to gain a sustainable competitive advantage in their industry. In addition he is a thought leader and evangelist for big data analytics, writing practical guides for CMOs, CFOs, and the nascent Chief Analytics Officer roles.
,
Prior, Mr. Siddiqi was at CEB, a best practices research and analysis firm, where he held multiple roles including Senior Director of Strategic Marketing, Chief of Staff to the CEO, and Senior Director of Research. Prior to CEB, he held roles in strategy consulting at Bain & Company and at Kaiser Associates; he started his career in FP&A at Engro Corp.
,
Mr. Siddiqi is passionate about teaching and education, and is active in the community through nonprofit work, including serving on the Board of Directors of Ingenuity Prep, a charter school in Washington DC.
,
Here is first part of my interview with him:
,
,
,
,: While I?€?m a data scientist-in-training, I?€?m also a Strategy Consultant overseeing this across multiple industries, so this is always of interest to me.
,
,The answer varies by Industry and also by organization culture. Some industries have realized that there is a lot of catching up to do ?€? for example in Consumer Products their own industry association the GMA put out a report that shows there?€?s a lot more that could be done to leverage Big Data in that industry. On the other hand, in Pharma, Capital Markets and Public Sector, Big Data has long had for a significant role to play. At SAP, we have very deep expertise and understanding of each of over 25 different industries, so we are able to work within the industry maturity to help our customers take advantage of these technologies and capabilities.
,
Although both the Harvard Business Review and McKinsey & Company put out reports a couple of years ago that brought Big Data and Advanced Analytics into the conversation at the CEO and Board level, the response of most organizations has been mixed ?€? in some, the CIO took the mantle and approached this as an IT problem, while in others the Chief Marketing Officer or the Chief Supply Chain Officer have taken it on as game-changers. This is why we wrote our ?€?Getting Started with Big Data?€? white papers, to fill in the gap between the strategic, business-changing vision that CEOs and Board have, versus what IT and other functions are delivering. 
,
, For example I?€?ve got one client that is using big data and predictive analytics for preventive maintenance in their manufacturing plants ?€? but while the cost reduction in their annual plant maintenance spend is in the millions, they are even more excited about the impact on quality and ability to meet delivery times for their customers.
,
,
,
,We are unique in that for us, it doesn?€?t have to be an either / or question, especially if you really define ?€?end-to-end?€? correctly. We pride ourselves on being able to truly provide end-to-end solutions, especially through our SAP HANA platform.
,
,
In addition we have one of the largest partner ecosystems in the world, from very deep, niche industry players to those that serve broader markets. Your readers may know there are over 100+ partners that are building extensions on SAP applications. What they might find even more interesting is that there are 1,500+ startups in the SAP Startup Focus program, which are completely focused on big data, predictive analytics and real time data decision solutions built on the SAP HANA platform.
,

,
,
,
,
?€? Form follows function: Start with Business outcomes, not with Technology,
?€? Engage the business directly,
?€? Determine which capabilities are core for you to build and which can you outsource,
?€? Plan to get a couple of wins quick, while building up grander ?€?Big Hit?€? use cases that may have longer lead times before they deliver big value,
?€? Prioritize on 2 dimensions: Business Value and Feasibility,
?€? Take a Pilot, Test, Build and Scale approach,
?€? Don?€?t forget political support!
, ,

,
,
,
, ,You need to develop a portfolio of use cases, some of them longer term aspirational ones (those will often be tied to revenue enhancements or business model innovations) but some definitely need to be shorter term ?€?quick wins,?€? because it?€?s through delivering on those that you get the credibility, buy-in and sponsorship you need from internal stakeholders and P&L owners. 
,
But we have to be careful ?€? picking use cases is not easy. How do you know what is too ambitious versus not ambitious enough; how do you avoid doing tons of ?€?me too?€? use cases versus just the right ones needed to get sustained patronage? That?€?s where our in-house, customer-facing team of over 500 Industry value experts and data scientists can help. Everything we do starts with a maturity model, often within an industry and / or a functional area, so we save time and effort in helping figure out the right portfolio of use cases. We then help quantify the value of each of them and prioritize by juxtaposing quantified business Value against Feasibility.
,
,
,
,I think of the distinction really as being between BI and Advanced Analytics or Data Science, because you can have BI on very large, fast data sets and it?€?s still reporting, not data science. Both are key to have, and using one over the other depends on the types of QUESTION that you are seeking to answer. Think of it as peeling the layers of an onion, trying to get deeper and more precise. 
,
In many cases BI and reporting is absolutely the most efficient and effective way to get to the answer ?€? for example if you want to know in what regions are sales below plan, and when you know that maybe what products are driving the decline within, those regions is important also. Well you don?€?t want to go build a model for that. However if the next question was, ?€?what are the best 3 ways to fix the decline,?€? that?€?s where you start to cross over into advanced analytics, because most dashboards can?€?t tell you that straight off the bat. At best you will be able to get some idea by looking at historical data, putting in your experience and gut feel and then coming up with an answer.  So already you are kind of moving into data mining, maybe some light tradeoff analysis. Now let?€?s peel the onion further and ask, ?€?OK if we deploy approach A ?€? let?€?s say it?€?s a promotion on that product targeted at a specific customer segment ?€? then how well is our response going to work??€? To answer that, you may need to do segmentation, classification, and so on. 
,
Another way to think about the difference is, how precise do you want or need to be in your answer. Of course as an analyst we always ask the next question and the next one after that and so on, usually until we reach the level of precision we need, or more likely when we run out of data or hypotheses! 
,
The difference between the two is reflected in everything from use cases to tools / architecture / infrastructure, and perhaps most importantly in things like the skill sets you need. Again, our maturity model assessment has all this, in 5 minutes of answering a few questions you can get an instant assessment of where your organization stands. Both BI people and Data science people find it very useful to do the assessment and use that as a way to get on the same page, talk about resources, plans and so on. 
,
Ultimately though, these two disciplines are coming together ?€? I foresee the term ?€?Analytics?€? will be re-cast in the future to include both the BI side and the data science side, but it will take a while to get there. We are already doing this through the BusinessObjects 4.1 upgrade, and with solutions like SAP InfiniteInsight and SAP Lumira now the non-data scientist BI user can go ahead and take data sets, do guided data mining and some fantastic visualizations; they can share with others because it?€?s all one single source of the truth, and then run something like SAP InfiniteInsight to start doing guided predictive modeling. 
,
,.
,
,
,  "
"
New entries for July.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to , page:
,  "
"
,Hadoop is an essential part of many data science projects. New technologies developed on top of Hadoop are released all the time, and it can be difficult to keep up with the wide array of tools at your disposal, so here is a list of 18 of the most essential:,
,
,
,
Each of these technologies adds another tool to your data analysis tool belt and can make your job easier in the right conditions. More info at NetworkWorld:,
,
,
,
,
,  "
"
,

,

, leads the machine learning teams at ,, which focus on fraud detection, helping our sellers grow their businesses and improving customer experiences. Taylor brings a back-end, infrastructure lens to the machine learning domain, helping to architect a reliable, highly-available, real-time machine learning pipeline to support the rigorous requirements of the payments industry. Before Square, Taylor spent several years building and commanding an army of online poker bots.

,

Here is my interview with him:
,
,
,

,: , makes commerce easy for everyone. ??We got started by enabling anyone to instantly accept credit card payments on their mobile device. Now we are focused on providing an unparalleled suite of business tools around our ,, but have also grown into peer-to-peer payments with ,, and now extending small business financing with ,. Data science is essential for scaling underwriting and risk for our entire suite of products, and we also use it to generate powerful insights and analytics.

,

,

,

,Risk, fraud and underwriting are the bread-and-butter use cases for ML at Square. ??,If you look at our major innovations of 1) enabling people who couldn't use credit card processing and 2) settling with our sellers in 24 hours, they are both enabled by powerful machine learning behind the scenes. We are also starting to explore new ways to add value with machine learning. One awesome example of this is ,, which we just recently launched. Square?€?s unique understanding of our merchants allows us to offer capital to growing businesses in a simple and fast way.

,

,

,

,This isn't a decision to be taken lightly. ??At our smaller scale, batch offline worked great - it's a ,conceptually and technically simple model. ??At larger scale, real-time gives us tons of operational flexibility: we can detect/correct issues sooner, we can further increase the speed at which we are giving sellers their funds, and we can make more efficient use of our operations team doing manual review. Going real-time will also enable us to get our risk models into the synchronous payment flow, which can provide us with new and creative opportunities to use ML.

,

The key learning experience is that real-time architecture to move money is hard. ??You need engineers who can build reliable systems, data scientists to build innovative models and the magical people who span both to glue everything together and make sure nothing is lost in translation.

,

,

,

,The biggest challenge with ML projects in the wild is the data. These days it?€?s relatively easy to download a static dataset onto your laptop, train a one-off model and then use that model to predict stuff - that?€?s hardly representative of doing industrial ML. 
,
In practice, there?€?s a lot of contradicting goals with data:

,
,
, ?? ,

,

,

,Focusing on driving down the same error metric for an extended period of time is going to yield diminishing returns. ??Sure, at huge scale tweaking a button color can make, a big difference to the bottom line, but thinking outside the box has unbounded potential. ??Along these lines, I think it?€?s important to periodically revisit assumptions, redefine metrics and goals, and find new areas where existing techniques can apply. One way to do this is to check in with customers and get their feedback. Another example is around tool-sets - it?€?s easy to fall into a rut and accept slow tools or brittle processes, but data science is evolving so quickly right now that I think more organizations would benefit from baking in tool exploration into their normal process.

,

,
,

,Data Science is a loaded term and can mean a lot of different things depending on the context. Off the top of my head, I?€?d break them into a few different roles:
,
, ?? ,

,

,

,The best hires are the people who can do the ML work and the engineering work, but that?€?s a lot to ask for. ??They can have a massive impact very quickly because they are able to single-handedly prototype new ideas and then do the engineering legwork to ship them to production.

,

Hiring pure R or Python hackers can be great, but ability to write real code and interface with engineers is essential. ??Likewise, hiring pure software engineers can be great, but basic skills in math, stats and ML go a long way.

,

,

,
,My favorite place to learn new things related to data is ,. ??I had the privilege of sitting next to Mike at Square for a bit - his work is elegant, inspiring and often accessible to non-techies.

,
Outside of work, I love automating things I enjoy, like online poker and video games. Besides that, it?€?s great to get out in the sun and behave like humans used to!
,
,
,  "
"
,


,
, was a great opportunity for students, data scientists, engineers, data analysts, and marketing professionals ,to learn more about the applications of Big Data. Session topics included ?€?Enabling Science from Big Image Data,?€? ?€?Engineering Cyber Security and Resilience,?€? ?€?Cloud Forensics,?€? and ?€?Exploiting Big Data in Commerce and Finance.?€?
,

Held at the Tresidder Memorial Union at Stanford University, the ASE International Conference on Big Data Science took place from Tuesday, May 27 ?€? Friday, May 31, 2014.
,

,.

,
Here are highlights from Day 1 (Wednesday, May 28, 2014):
,
,delivered a talk titled ?€?The Future of Data Intensive Applications.?€? He mentioned that although ?€?Big Data?€? is a much hyped term nowadays in Business Analytics, the core concept of collaborative environments conducting experiments over large shared data repositories has existed for decades. He suggested the audience to go through The Wall Street Journal article titled ?€?,?€? by Marc Andereessen. He explained the big gap through the following facts for an average enterprise:
,
, ?? ,
For example, in healthcare about 40,000 diverse studies were performed in last five year. However, very few models got operational. Modernization of IT infrastructure is very much needed to get models operational. Building blocks of modern data architecture are: Applications, Analytics, Data and Speed. He discussed Data Fabric architecture briefly. He mentioned that ?€?Infrastructure-As-A-Service is the Hardware?€?. He talked also about ,, an application environment. He discussed the idea of application as unit of deployment. 
,
Talking about Hadoop, he said that it has tremendously changed economics of data storage and analysis. He suggested the audience to start preparing for convergence of High Performance Computing, Big Data and Databases with new hardware platforms such as Mellanox, RoCE, ARM, etc. now available in the market.
,

,talked about,?€?System U: Computational Discovery of Personality Traits from Social Media to Deliver Hyper-Personalized Experience?€?. She talked about individualization at scale. She shared that psycholinguistic studies have shown that the words people use reflect their personality. Hundreds of millions of people leave text footprints in public. System U uses psycholinguistic analytics to automatically derive one?€?s personality traits from their digital footprints. These traits uniquely characterize an individual?€?s psychological, cognitive, and affective style and properties, and can then be used to make hyper-personalized recommendations to the individual and influence/intervene the actions of the individual. 
,
She gave an overview of System U and described how it automatically derives several types of personality traits from one?€?s tweets, including human basic value (one?€?s belief + motives) and fundamental needs (e.g., ideals vs. practical). In addition, she also presented a set of validation studies that assess how accurate the System U-derived traits are compared to ?€?ground truth?€? and how these derived traits influence recommendations and people?€?s behavior in the real world. She used live demos and concrete examples, ranging from precision marketing to individualized customer care, to demonstrate the applications of System U and discussed research directions in this space.
,


,gave an interesting talk titled ?€?A Building Code for Building Code?€?.??He introduced the metaphor of a building code to talk about building better software code.??Although cyberspace has a physical reality of computers and communication channels, sensors and actuators; it is really made mostly by the programs that control those things. 
,
Today, systems of programs control most of our critical infrastructures. Workers in cyber security have adopted many rich metaphors: Trojan Horse, virus, worm, firewall, and more. Difficulties arise when the metaphor blinds us to the underlying reality. He critically examined several common cyber security metaphors and proposed that the adoption of a new (or at least underutilized) one, that of a building code for critical infrastructure software, as a means of putting what we have learned in forty years of system development experience into practice.
,
, delivered a speech on ?€?Big Data in the Finder and Aladdin Video Programs?€?. She mentioned that Incisive Analysis Office at IARPA sponsors programs that help analysts make sense of massive data. Her talk focused on two such programs, Finder and Aladdin Video.
,
, is developing technologies that can locate where in the world a query image or video was taken based on the query?€?s content alone. The , (ALADDIN) Video program is developing technologies that can quickly search massive video collections for a user?€?s events-of-interest. The talk provided a brief overview of the goals and objectives of these programs, examined current results, and illustrated the size of the data involved.
,
,.
,
,
,  "
"
Most popular 
, tweets for Jul 30-31 were
,
,  "
"
, is pleased to announce that Ted Senator is the winner of its , for his contributions to society and the data mining community.,
,
,
Senator has a long history of serving the data mining community. His service has impacted the direction of major conferences that helped define distinctions between research and applications of KDD and led to recognition of the distinct challenges and accomplishments of those who apply KDD to solve problems with real business, Government, and social value. He served as the General Chair of KDD-2003 and as the Industry Government Track Program co-Chair in 2010, 2011, and 2013. He was the Program co-Chair and Chair of Innovative Applications of Artificial Intelligence (IAAI) in 1996 and 1997 and has been a Program Committee member every year since 1993. He has been the Secretary-Treasurer of AAAI since 2003 and is the Treasurer for KDD-2014.,
,
As a three-time Industry Government Track Program co-Chair of KDD, Senator initiated and led the significant re-structuring and rejuvenation of the track, reorganizing it into three subareas: deployed, discovery, and emerging. Papers in each subarea feature the distinct benefits and approaches to different aspects of data mining usage and recognize the significant efforts and accomplishments of authors who report on efforts that use data mining techniques to solve real problems with real value for real users in industry and government. Similarly, as Program co-Chair and Chair of IAAI in 1996 & 1997 Senator added the ?€?emerging?€? area to the previous ?€?deployed?€? area that revitalized the IAAI conference and provided a venue for applied work that was on a path to deployment.,
,
As a program manager at DARPA between 2000 and 2006, Senator provided approximately $100M in research support that helped establish the emerging areas of relational data mining and transfer learning, as well as applications to bio-surveillance. His work was recognized with the Office of the Secretary of Defense Exceptional Public Service Medal.,
,
In addition to his service, Senator is responsible for several data mining applications and technical innovations. He created and led teams applying data mining to detecting and preventing money laundering, stock market violations, security violations, and insider threats. His work has been recognized by citations in the data mining literature as well as by the Association for the Advancement of Artificial Intelligence?€?s (AAAI?€?s) Innovative Applications Award in 1989, 1995, and 1998. His 20+ publications and his invited and keynote talks at conferences including KDD, ICDM, and IAAI include introducing ideas to the data mining community of entity resolution and analysis of networked data, as well as multi-stage classification, in papers he published in 1995 and 2005.,
,
Senator received S.B. degrees in Physics and in Electrical Engineering from MIT. He has additional graduate education in physics, computer science, finance, and information technology. He is currently a Technical Fellow at Leidos (formerly SAIC). His previous work was conducted at DARPA, NASD Regulation (now Financial Industry Regulatory Authority, or FinRA), and the US Department of the Treasury?€?s Financial Crimes Enforcement Network (FinCEN).,
,
The twelve previous SIGKDD Service Award winners have been: Gregory Piatetsky-Shapiro, Ramasamy Uthurusamy, Usama Fayyad, Xindong Wu, The Weka team, Won Kim, Robert Grossman, Sunita Sarawagi, Osmar R. Za??ane, Bharat Rao, Ying Li, and Gabor Melli.,
,
The award includes a plaque and a check for $2,500 and will be presented during the Opening Plenary Session of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2014), on Sunday August 24th in New York City, USA.,
,
For more information, visit ,
,
,
,  "
"
,
,
Ed. Charu Aggarwal, CRC Press, 2014
,Content: 707 Pages | 84 Illustrations,
,
TABLE OF CONTENTS and INTRODUCTION: ,
,
Comprehensive Coverage in the form of surveys on  the entire area of Data Classification,
,
Research on the problem of classification tends to be fragmented across such areas as pattern recognition, database, data mining, and machine learning. Addressing the work of these different communities in a unified way, Data Classification: Algorithms and Applications explores the underlying algorithms of classification as well as applications of classification in a variety of problem domains, including text, multimedia, social network, and biological data.,
,
This comprehensive book focuses on three primary aspects of data classification:,
,
Methods: The book first describes common techniques used for classification, including probabilistic methods, decision trees, rule-based methods, instance-based methods, support vector machine methods, and neural networks.,
,
Domains: The book then examines specific methods used for data domains such as multimedia, text, time-series, network, discrete sequence, and uncertain data. It also covers large data sets and data streams due to the recent importance of the big data paradigm.,
,
Variations: The book concludes with insight on variations of the classification process.
It discusses ensembles, rare-class learning, distance function learning, active learning, visual learning,
transfer learning, and semi-supervised learning as well as evaluation aspects of classifiers.,
,
Features,
,
,
,
The table of contents and the introduction may be found at ,  "
"
,
By Gregory Piatetsky,  
,, Aug 3, 2014.
,
Big Data is hard to visualize, even with revolutionary 3-D printers. 
,
Some researchers are already talking about the emergence of
,. 
,
But why stop at 4-D ? 
,
New KDnuggets cartoon examines the extra dimensions of a 7-D Printer.
,
,
,
,
,
,
,??,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, August 4, 2014.
,
Here are 60 upcoming August 2014 - January 2015 meetings and conferences.
,
Most common countries: 
,
,??,
Most common states in the US:
,
,??,
Most common cities: 
,
,??,
You can find the full list on KDnuggets page:
,.
,
Color code:
,
,??,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
, is currently employed as a Staff Software Engineer at ,, where she works on various natural language processing applications such as performing sentiment analysis of customer feedback and extracting relevant information from job postings. Before joining LinkedIn, she was a Staff Research Engineer at Samsung Research America. Prior to Samsung Research, she was employed as a Computational Linguist at Disney Interactive.
,
 	In addition, she conducts independent research on mining the language of social media. Here primary interests are focused on extracting topics and sentiment from micro-text ?€? the short, snippet-like pieces of text found on Twitter, Facebook, and in various other social media sources. 
,
Her education background is in theoretical and computational linguistics (Rutgers, 2005). In addition to computational linguistics, she has publication record in theoretical syntax and morphology, which was her primary area of research between the years of 2002 ?€? 2008.

,
I had the pleasure of attending her talk ""Integrating Linguistic Features into Sentiment Models: Sentiment Mining in Social media within industry Setting"" at , in San Francisco, CA. Here is first part of my interview with her:
,
,
,
,: To answer this question let me first offer a working definition of ?€?linguistic features?€? that I used throughout the talk. I ,use the term ?€?linguistic features?€? to refer to ?€?features beyond the word-level?€?; in other words, phrase-level features. For example, a classic word-level feature commonly used in sentiment analysis models is an adjective that bears positive or negative semantic orientation, such as ?€?great?€? or ?€?terrible?€?.  In contrast, a phrase-level feature is a linguistic unit composed of several words, a phrase, that conveys positive or negative orientation only in its entirety.  For example, in a domain related to deliveries of products ?€?on time?€? is a phrase-level feature, where neither the meaning of ?€?on?€? nor the meaning of ?€?time?€? alone convey the sentiment; rather, ?€?on time?€? is the phrasal unit that suggests (in this case) positive orientation, as in ?€?delivery arrived on time?€?.  
,
Other similar examples include phrases such as ?€?as described?€?, ?€?as advertised?€?, ?€?met expectations?€?, ?€?arrived in pieces?€?.  When reviews are very short, using features beyond the word level becomes critical because many data points do not contain any adjectives or other obvious sentiment bearing words. For example, in the dataset of feedback postings for merchants at Amazon.com (at http://economining.stern.nyu.edu/datasets.html), that was used in the experiment described in the talk, roughly 20% of the sampled transaction reviews do not contain any obvious sentiment bearing vocabulary.  The feedback postings are for the most part micro-reviews, i.e. reviews that consist of a single short sentence such as ?€?transaction met expectations?€? or ?€?arrived as promised?€?, making it hard to impossible to single out a specific word that contributes to the sentiment of the review. 
,
,
,
, First, there is the rather obvious recommendation of normalizing misspelling as much as possible, removing uninformative punctuation, and normalizing important information-bearing emoticons such as ?€?:-)?€? and ?€?:-("".  After that, the subsequent pre-processing steps will largely depend on the application and on how the data will be used. , For example, if the social data is to be used in a bag of words model, then removing frequent stop words such as articles (?€?the?€?, ?€?a?€?) and some prepositions (?€?of?€?, ?€?to?€?) is an important next step. In addition to the basic stop word removal (if one chooses to go with a bag of words model), I would also recommend doing a more data-specific text pre-processing. Namely, each data-set comes from some domain that contains words that are very frequent for that domain. They may not be your regular stop words, but they contribute little to no information and may actually hurt the model. Specifically, in the case of amazon feedback data, words such as ?€?amazon?€? are actually not informative because they appear in too many reviews, e.g. have too high a document count. Removing or at least discounting these words may be of use.  
,
In the past, I have used the rule of thumb where if a token appears in more than 20%-25% of all documents, it can be treated as a stop word. That said, if one plans to use a model that relies on phrase-level features such as ?€?on time?€? or ?€?as described?€?, then removing stop words such as the preposition ?€?on?€? will be detrimental.  For models that use phrase-level features, I would recommend leaving stop words as they are, but performing other noise-reduction steps such as recognizing and then normalizing negation words such as ?€?not?€? , ?€?no?€?, ?€?don?€?t?€? ?€?doesn?€?t?€? , etc. into a single form ?€?NOT?€?.  Negation contributes very important information especially to sentiment models and should be retained and ideally normalized. That way, phrases such as ?€?package not arrived?€? and ?€?package never arrived?€? are mapped to a single form ?€?Package NOT arrived?€?.  Performing some simple and very careful lemmatization such that ?€?arrive?€? and ?€?arrived?€? are mapped to the same form ?€?arrive?€? is also helpful. 
,
,
,
,
,
, There are several challenges posed by industry setting. I will focus on two. 
,
First, industry often demands ,. A simple model that works and gets shipped quickly is oftentimes preferable to a more nuanced and complex model that takes longer to build, especially if the simpler model is amenable to iterative improvements. Practitioners and researchers in industry are driven to create models that work well, yet are developed/trained quickly, which is challenging. 
,
The second constraint of industry is ,, where ?€?labeled?€? and ?€?good?€? are the key operative words. While in today?€?s world raw data is prolific, labeled data is not. Obtaining ?€?good?€? labeled data requires many hours spent by senior team members on annotation code-book design as well as on implementation of inter-annotator agreement metrics such as pairwise Kappa statistic. Creating a proper annotation code-book is critical for the development of a solid machine learning model. Annotation code-books are particularly difficult and labor-intensive to design for the tasks that are vague and subjective such as sentiment labeling. In addition, there may also be some legal issues surrounding releasing raw data for crowd-sourcing as it may be proprietary and hence using third party annotators may not be possible.  
,
,
,
Currently, some of the more interesting use cases in industry involve aspect-based sentiment modeling where sentiment is attributed not to the product as a whole but to each component (aspect) of the product. For example, a hotel review is more informative if each aspect such as room quality, location, amenities are given a separate sentiment score as opposed to if the entire hotel receives a single sentiment score. 
,
,
,
, Completely agree. Binary classification for sentiment ,is a great over-simplification and is done solely for the purpose of getting some preliminary phrasal features that indicate positive or negative sentiment.  At least three-way classification such as ?€?good?€?, ?€?neutral?€?, and ?€?bad?€? would be better to start with. That said, ?€?neutral?€? is a very hard class to define, as it is often ?€?neutral-good?€? or ?€?neutral-bad?€?. Given the brevity of the reviews I was working with, inferring these distinctions from the text alone was near impossible.  However, if there were a way to obtain a reliable set of three-way classifications, or a reliable finer grained classification, it would be preferred. 
,
,.
,
,
,  "
"
,
,
, was a great opportunity for students, data scientists, engineers, data analysts, and marketing professionals ,to learn more about the applications of Big Data. Session topics included ?€?Enabling Science from Big Image Data,?€? ?€?Engineering Cyber Security and Resilience,?€? ?€?Cloud Forensics,?€? and ?€?Exploiting Big Data in Commerce and Finance.?€?
,

Held at the Tresidder Memorial Union at Stanford University, the ASE International Conference on Big Data Science took place from Tuesday, May 27 ?€? Friday, May 31, 2014.
,

,.
,
,.
,
Here are highlights from Day 2 (Thursday, May 29, 2014):
,
, kicked off second day with a talk on ?€?Enabling Cloud Analytics for Big-Data Security and Intelligence?€?. He addressed the growing interest in big-data science surrounding the use of cloud analytics, social networks and Internet of things (IoT). He put forward the critical issues to upgrade big-data analysis, privacy and cloud security. He mentioned that motive is to achieve enhanced ubiquity, mobility, security, scalability and quality of service (QoS) of clouds and highly-visited social networks or datacenters. He evaluated the widespread use of clouds over massive datasets generated by e-business, social networks, sensors, RFID, GPS, etc.
,
His talk also revealed major R&D challenges and presented new approaches to preserving data privacy, assuring cloud security, and enhancing cyber intelligence. To remove the security and trust barriers in bare-metal or virtual clouds, he examined the top-10 security and privacy issues released by Cloud Security Alliance in 2012. Some new approaches and hidden opportunities are discussed towards the building of a trusted and intelligent cloud computing environment over both structured and unstructured big datasets. Finally, he compared the security and capability in BYOD (Bring Your Own Devices) solutions with those offered by the new BYOC (Bring Your Own Clouds) approach for inter-cloud (mash-up) applications.
,
,delivered a talk on ?€?Delivering on the Promise of Big Data?€?. He mentioned that the real promise of big data isn?€?t about merely doing analytics cost-effectively and at scale; it?€?s about discovery. Data discovery means uncovering hidden patterns from disparate sources without needing to know which questions to ask or the data relationships in advance. 
,
He emphasized that the ?€?Big Data?€? revolution puts a lot of pressure on IT budgets, which on average only grow by 5% per year. Still organizations have to cope with a 40% data/systems growth per year. He discussed the need to evolve analytics from tactical objectives toward strategic data discovery that heightens business performance, insights and protection by identifying previously undiscovered relationships and patterns in data. He also presented YarcData?€?s purpose-built system for data discovery offering a combination of hardware and software.
,
,, American Socio-biologist and Author talked on ?€?The Big Deal about Big Data: a Socio-biological Perspective?€?. She argued that we adapt very slowly to change. We enter a high failure-rate environment (the number of wrong options is growing much faster than the number of right options). Complexity causes us to become bad pickers. 
,
She said ?€?In nature, any drive toward singularity is a drive toward extinction?€?. Efficiency is often a dangerous drive toward singularity because you start to remove redundancy. She suggested following to succeed in a high failing rate environment:
,
, ?? ,

, delivered a talk on ?€?Scaling R to Big Data Science?€?. He described current challenges and issues in big data analytics -- the talent gap caused by tool chains requiring myriad skills, lack of reusability across platforms, the cost and delay of writing advanced parallelized analytics from scratch. He also mentioned how Revolution Analytics is helping to address these challenges via Revolution R Enterprise (RRE). 
,
RRE is a commercial distribution of open source R that includes Intel Math Kernel Libraries, select open source extension packages, Revolution?€?s ScaleR functions for scalable, cross-platform advanced analytics, and Revolution?€?s DeployR Web Services framework. They recently enhanced RRE to support in-Hadoop and in-Teradata analytics with big data implementations of advanced analytics and machine learning algorithms for exploratory data analysis, regression, classification, prediction, and unsupervised learning. In the talk, he also examined how RRE implements these technical advances, making big data analytics more powerful and accessible.
,
,.
,
,
,  "
"
,
,
,, Data Governance Product Manager at ,, is responsible for the road-map of Talend's Data Quality and Master Data Management products. Christophe originally joined Talend as Director of Data Quality and Master Data Management, and has been extensively involved in Talend's product strategy and focus toward Big Data. Christophe has over 15 years of experience in Data Management software. Prior to working with Talend, Christophe acquired an extensive technical knowledge working as a developer, consultant, architect and presales at various companies including Ardent/Ascential, Oracle and IBM.
,
Here is my interview with him:
,
,
,
,At Talend we believe Big Data without governance will quickly become a big problem. With the hype around Big Data people tend to dump ,everything in Hadoop or NoSQL without the formal control and quality processes they have adopted for their ERP or other operational data sources. Whereas, quite conversely, ,
,
,
,
,It is typical for Big Data to ingest data external to the Enterprise, including open data. Controlling who can access and use this data, what data is verified and trusted, by whom and how, is a big deal. The usage is also critical. This data can be extremely sensitive, there are privacy issues, opt-in/out, specific regulations per geographies etc. As a matter of fact, I think Big Data without a tight governance can be not only damaging to the business - to put it bluntly,, it can actually send people to jail.
,
,
,
, reduces the learning curve when adopting Big Data by offering the same experience as with traditional sources. Therefore the same Talend Profiler ,you use on traditional files and databases now let's you assess the quality of your Hadoop sources and measure it over time. We also let you parse and standardize data on Hadoop, which is key to working with semi-structured data. Finally, the Talend Platform features advanced fuzzy matching algorithms to help mash up the disparate sources you usually find in Big Data.
,
,
,
,Data in general and Big Data in particular seem to be moving up the stack, from IT to the business ,users. A trend we see is self-service data discovery, data transformation/processing and even mashup/blending without the direct involvement of IT. The actual consumers of the data tend to take the matter into their own hands in order to have the agility the business requires. To regain control and apply the proper data governance policies, IT needs to deploy and support a platform that offers a non-IT person enough simplicity, flexibility and productivity for them to willingly give up their ad-hoc tools and scripts.
,
,
,
,It is definitely an issue, because the skills required to really take advantage of Big Data are rare and expensive. However, at Talend our goal is precisely to make complex things simpler. What we want is to reduce this barrier of entry and open Big Data to less specialized people. This is why the Talend Platform for Big Data typically addresses two issues that make Big Data complex:
,
, ?? ,
,
,
, I would strive to make data ""speak"". The best way to get a job right now may be to get the Big Data skills the market craves, but I think what you need to secure a career in the analytics world is the ability to translate data into business opportunities.
,
,
,
,My family is what keeps me busy outside work. I am also a pretty bad tennis player but it does not stop me from hitting the court at least once a week. Despite its incredible growth Talend has not lost its French roots of ""art de vivre"" and they give me the flexibility I need to get the work done and be happy in my personal life.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Aug 5 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Aug 01-03 were
,
Open Source #DataScience Masters plan, with courses from Coursera, Stanford, edX, and codeAcademy ,
,
Open Source #DataScience Masters plan, with courses from Coursera, Stanford, edX, and codeAcademy ,
,
Book: Data Classification: Algorithms and Applications ,
,
Open Source #DataScience Masters plan, with courses from Coursera, Stanford, edX, and codeAcademy ,
,
,  "
"
        ,  "
"
,
,
,
,Participants from more than 100 countries will compete individually in two online qualification rounds. All you need is a computer, internet connection and some big data skills to compete. Final information on the login and set-up infrastructure will be made available in the weeks leading up to Round 1 of the Championships. Top competitors from Round 1 will progress through to Round 2, which will be structured similarly to Round 1, with additional complexity.,
,
Case studies will relate to a wide cross-section of topics, including practical case studies, technical skills and creative business insights across Big Data and Business Analytics (e.g. 3 Vs, Statistical Analysis). The World Championship are focused on big data impacts for real-life enterprise decision-making. Participants will address data sets across three core sectors: Financial Services (e.g. Insurance, Banking Transactions, ROI Investment) and Mobile Data and Social Data.,
,
,
The World?€?s Top 16 Finalists will be flown to Austin and Dallas, Texas.
,
Round 1: 18 Oct 2014
,Round 2: 1 Nov 2014
,World Finals: 22-23 Nov 2014 
,Finalists will do battle in front of global judges and institutions and will be set a variety of challenges that test Innovation, Speed, Business Risk, Best Practice Data Skills and more.,
,
,
TEXATA?€?s Big Data Analytics World Championships event is technology-agnostic and independent. Every participant is free to use any technology vendors, products or additive analytical products.,
,
,
Participants require solid familiarity with the basic data analytical techniques involved with managing the large and complex volumes of data with heterogeneity, diversity and developing meaningful analytical insights (e.g. 3 Vs).,
,
The , has a directory of useful vendors, professional services and educational training materials to help you prepare. We will be releasing free sample questions, tips from Sponsors and the official TEXATA Question Framework on the TEXATA website in the lead-up to Round 1.,
,
KDnuggets readers interested in competing in Round 1 of ,??(Sep 6 2014)?? can use this code for free entry:?? , (normally $30 fully paid entry).,
,
Register at ,
,
,
,  "
"
,

,

, was a great opportunity for students, data scientists, ,engineers, data analysts, and marketing professionals to learn more about the applications of Big Data. Session topics included ?€?Enabling Science from Big Image Data,?€? ?€?Engineering Cyber Security and Resilience,?€? ?€?Cloud Forensics,?€? and ?€?Exploiting Big Data in Commerce and Finance.?€?

,

Held at the Tresidder Memorial Union at Stanford University, the ASE International Conference on Big Data Science took place from Tuesday, May 27 ?€? Friday, May 31, 2014.

,

,.

,

,.

,

,.

,

Here are highlights from Day 3 (Friday, May 30, 2014):
,
,??started the day with an interesting talk: ?€?On??Content, Discussions, Opinions, and Deliberative Participation over Social Media Systems?€?. He emphasized that??social media is changing many different aspects of our lives. By participating in online discussions, people exchange opinions on various topics or contents, shape their stances, and gradually build their own characteristics. 
,
He also presented and discussed a framework for identifying online user characteristics and understanding the formation of user deliberation and bias in online newsgroups. Under the??,??(Social Interactive Networks: Conversation Entropy Ranking Engine), his students have designed a dynamic user like graph model to recognize user deliberation and bias automatically in online newsgroups. They even evaluated identification results with linguistic features and implemented this model under SINCERE as a real-time service. By applying this model to large online newsgroups, he studied the influence of early discussion context on the formation of user characteristics. 
,
He concluded that the formation of user deliberation and bias is a product of situations, not simply dispositions: confronting disagreement in unfamiliar circumstances promotes more consideration of different opinions, while recurring conflict in familiar circumstances evokes close-minded behavior and bias. Based on this observation, he leveraged a supervised learning model to predict user deliberation and bias at their early life-stage. His results show that knowing only the first three months of users?€? interaction data generates an??,??level of around 70% in predicting user deliberation and bias in online newsgroups.

,

,talked about ?€?Cloud-Centric Assured Information Sharing?€?. She described her research and development efforts in assured cloud computing for the Air Force Office of Scientific Research. She, along with her team, has developed a secure cloud computing framework as well as multiple secure cloud query processing systems. Their framework uses Hadoop to store and retrieve large numbers of Resource Description Framework (RDF) triples (a??subject, a??predicate, and an??object) by exploiting the cloud computing paradigm and they have developed a scheme to store RDF data in a Hadoop Distributed File System. 
,
They implemented XACML-based policy management and integrated it with their query processing strategies. For secure query processing with relational data they utilized the HIVE framework. More recently they have developed strategies for secure storage and query processing in a hybrid cloud. In particular, they have developed algorithms for query processing wherein user?€?s local computing capability is exploited alongside public cloud services to deliver an efficient and secure data management solution. They have also developed techniques for secure virtualization using the XEN hypervisor to host their cloud data managers as well as an RDF-based policy engine hosted on their cloud computing framework.

,
,delivered a talk on ?€?Twitter Analytics for Insider Trading Fraud Detection System?€?. She mentioned that Twitter analytics has been developed to process Twitter data at macro level for use in an insider trading detection system in order to establish normal trading patterns between daily stock price change and public sentiment. 
,
Two machine learning models, Support Vector Machine (SVM) and Decision Tree, are built based on annotated historical Twitter data and Stanford Sentiment140 Tweet corpus, respectively. Her research focuses on the discussions of polarized sentiment (positive and negative), comparison of SVM and Decision Tree models, Sentiment Key Performance Index (SKPI) and Daily Sentiment Index (DSI) and mood analysis. The results illustrated that Twitter SKPI and DSI are useful indexes to predict the future stock price movement on regular stock trading.
,
,delivered talk titled ?€?Big Data and Semantic Web Meet Applied Ontology?€?. He said ?€?since the beginnings of the Semantic Web, ontologies have played key roles in the design and deployment of new semantic technologies.?€? Yet over the years, the level of collaboration between the Semantic Web and Applied Ontology communities has been much less than expected. Within Big Data applications, ontologies appear to have had little impact. On the one hand, the Semantic Web, Linked Data, and Big Data communities can bring a wide array of real problems (such as performance and scalability challenges and the variety problem in Big Data) and technologies (automated reasoning tools) that can make use of ontologies. 
,
On the other hand, the Applied Ontology community can bring a large body of common reusable content (ontologies) and ontological analysis techniques. Identifying and overcoming ontology engineering bottlenecks is critical for all communities. The primary goal of the Ontology Summit 2014 ?€? the 9th in a series ?€? was to provide a platform and opportunity for building bridges between the Semantic Web, Linked Data, Big Data, and Applied Ontology communities. The Summit activities brought together insights and methods from these different communities, synthesize new insights, and disseminate knowledge across field boundaries.
,
,.
,
,
,  "
"
,
,
In Chinese Internet industry, there are three big leaders in the big data wave: Baidu, Alibaba, and Tencent, called BAT. From the data source perspective, Baidu collects the data based on user search, Alibaba owns the transaction data and credit data, and Tencent has the social network data.,
,
,
As a search engine provider, Baidu is collecting traffic data. Baidu has a strong technical support, and tries to present a new picture of intelligent search for users and developers through big data, cloud computing, deep learning and other core technologies. Specifically, the strategy of Baidu includes: open cloud, data factory and Baidu brain. Based on an open big data platform, a big data factory is established. After that, deep learning algorithms, data models and massive GPU parallel computing techniques are integrated through Baidu brain. Technically, there are three laboratories in Baidu: Silicon Valley Artificial Intelligence Laboratory, Deep Learning Institute (IDL) and Beijing Big Data Lab.,
,
Alibaba was first developed from B2B services of SMEs, and then extended to B2C by Taobao and Alipay. As a big e-commerce enterprise, Alibaba owns the most valuable ?€?golden?€? data. Therefore, the most important thing for Alibaba is the mastering of data. To achieve this, the following measures have been taken: 
,
,
,
Tencent has the biggest social data of China. Tencent is good at producing products. Currently, Tencent is trying to integrate the backend data from Qzone, Wechat and e-commerce products, and establishes a stable ecosystem centered by users. However, Tencent has not yet focused on big data techniques. Besides, there is a lack of big data technology leaders in Tencent.,
,
In summary, Baidu has put more efforts in exploring big data techniques, including deep learning, big data analyze, Human-Computer Interaction (HCI), 3D vision, heterogeneous computing and image recognition, etc; Alibaba builds a mature system of collecting, processing, storing and managing data, but still lacks of data mining attempts; Tencent has a stable product ecosystem, but remains a conservative manner in big data analysis.,
,
Liyang Tang, ,, is studying for a Ph.D in data mining and business intelligence in China.
,
,
,  "
"
,
,
,??,??
,??,??
,
,  "
"
By Gregory Piatetsky,  
,, Aug 6, 2014.
,
,
,
,
This poll is closed - here are the preliminary results:
,
,
,
Here are the results of similar past polls:
,
In KDnuggets ,, top 3 languages were
,
,??,
In KDnuggets ,, top 3 languages were
,
,??,
In KDnuggets
,, the top 3 languages were
,  "
"
By Adrian Balcan, TheWebMiner, Aug 2014.,
,
,
If you have ever tried to use a tool for price comparison between two or more products on different online shops, you definitely came in contact with the limitations that were imposed by such platforms. Usually, these kinds of apps work by periodically crawling a number of sites and periodically updating a certain product/price table. This is not very useful for users who want to choose products from various shops  in different geographical areas or from smaller sites that haven?€?t been crawled by that certain app.,
,
Now, maybe it?€?s a bit early to talk about the capabilities of this next product but it?€?s rounding up nice and can be helpful for consumers around the world and even market analysts, and the best part is that it?€?s completely free, and with no advertisements. PriceAlert wants to be a new solution for measuring prices from various sites. Currently, there?€?s nothing amazing, but the technology powering PriceAlert allows users to compare prices on any e-commerce platform all over the world, because unlike similar platforms, which are limited to a number of well defined shops, it uses an algorithm that automatically extracts data, like specifications of that product or the price and availability.,
,
For now, only a beta version is available, but new features are programmed to come up starting very soon. Useful proprieties like an email alert when a certain price has changed or statistics over the change in price for a period of time will be available, not to mention the capability of exporting the data gathered into various useful formats like excel or CSV.,
,
Check out this interesting new toy and maybe leave a review if you feel like it. You can find it here at the address , and we hope that together we can bring one more interesting tool to the use of people who need it.,
,
The Web Miner is a Romanian-based start-up, dealing with e-commerce solutions like web scraping and custom reports on market evolution. Even before the company launched in January 2014 they provided web scraping solutions for various projects, which were taken as a source of inspiration for the current idea.
,
,
,  "
"
,

,

, is currently employed as a Staff Software Engineer at ,, where she works on various natural language processing applications such as performing sentiment analysis of customer feedback and extracting relevant information from job postings. Before joining LinkedIn, she was a Staff Research Engineer at Samsung Research America. Prior to Samsung Research, she was employed as a Computational Linguist at Disney Interactive.

,

In addition, she conducts independent research on mining the language of social media. Here primary interests are focused on extracting topics and sentiment from micro-text ?€? the short, snippet-like pieces of text found on Twitter, Facebook, and in various other social media sources.

,

Her education background is in theoretical and computational linguistics (Rutgers, 2005). In addition to computational linguistics, she has publication record in theoretical syntax and morphology, which was her primary area of research between the years of 2002 ?€? 2008.
,

I had the pleasure of attending her talk ""Integrating Linguistic Features into Sentiment Models: Sentiment Mining in Social media within industry Setting"" at , in San Francisco, CA.
,
,.
,
Here is second and last part of my interview with her:
,
,

,

, One striking example of customer expectation is related to delivery time frames. Namely, temporal expressions such as ?€?within?€? + [some time frame] vs. ?€?in over?€? + [some time frame] that customers use in their reviews strongly indicate positive vs. negative sentiment respectively. Even when ?€?within?€? is used with a really long time period ,such as ?€?within a month?€? it still indicates positive sentiment, while the use of ?€?in over?€? with a much shorter time period such as ?€?in over two days?€? indicates negative sentiment. Customers used these phrases in such positive contexts as ?€?the book arrived within a month to Australia!?€? and in such negative contexts as ?€?I paid for expedited shipping and the book arrived in over two days!?€? ??What is interesting is that it is not the absolute time frame itself ?€?two days?€? vs. ?€?a month?€? that matters for positivity vs. negativity, but the preposition with which it is used ?€? ?€?within?€? vs. ?€?in over?€?.?? ??Importantly, the use of ?€?within?€? vs. ?€?in over?€? is highly domain specific: it relates to shipment time intervals and is unlikely to transfer over to another domain such as movies. For example, it is completely plausible to have a negative sentiment expressed in the manner such as ?€?this movie bored me within minutes?€?.
,
The following additional insights related specifically to the domain of transactions are interesting: customers appear to really appreciate and explicitly mention in their reviews the use of bubble wrap and other protective wrapping in packages; customers expect delivery confirmation by email and complain overwhelmingly when it is absent, even when delivery happens on time! Customers also explicitly mention when a product is delivered in time for a major holiday or birthday (e.g. ?€?arrived in time for xmas?€? or ?€?arrived just before my wife?€?s birthday!?€?). ??Domain-specific complaints related to books include things such as water damage, mildew or mold on pages, unclean or torn cover. 
,
The clearly negative mention of mold or mildew in the context of books is domain specific, as one can readily imagine a positive mention of ?€?mold?€? or ?€?mildew?€? in the context of cleaning supplies such as ?€?Mr. Clean fights mildew?€? or ?€?Removes mold in seconds!?€? Finally, and perhaps most interestingly, people make a very clear distinction between book editions that are ?€?old?€? or having ?€?yellowed pages?€? vs. those that are ?€?vintage?€?. This is interesting because ?€?vintage?€? clearly implies ?€?not new?€? and is often in bad condition. ??Yet, a beat-up condition is something customers would forgive in case of vintage editions. This is surprising and also poses a challenge for sentiment analysis as the phrases ?€?vintage edition?€? and ?€?old edition?€? are relatively close in lexical meaning.
,
,

,

, For example, in restaurant reviews customers may give positive reviews to food, while giving negative reviews to ambiance or service.

,

Similarly hotels have very varied aspects such as room quality, service, location, amenities, etc. that may elicit polar opposite opinion from customers. ??On that note, one of the more innovative applications of insights from aspect-based sentiment analysis is ?€?pivoting?€? hotel reviews based on the above-mentioned aspects of hotels as opposed to showing positive/negative reviews of the hotel as a whole. Aspect-based sentiment attribution gives potential customers much more insight and allows them to choose the hotel based on the aspects that are key for them.
,
,

,
,I will name three skills that most immediately come to mind:
,
,
, ?? ,
,

,

,The last book I read that I really liked is ?€?,?€? by Kazuo Ishiguro, a disquieting futuristic novel, akin to Huxley?€?s Brave New World, that traces the lives of unusual children in an elite and mysterious school (no more spoilers!). The novel is a brilliant allegory to human life and a beautifully written piece.

,

When I am not working I dance argentine tango. It is my greatest passion and hobby other than mining text. I have been dancing tango for now over 10 years.
,
,
,  "
"
,
Latest ,, (Aug 06, 2014) ,:
,
,??,
Also
,  |
,  |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
Where do most romantic singles in the US live ? Michigan is the most romantic state, and Vermont is a close second. KDnuggets ,  "
"
,
San Mateo, California - August 6, 2014 - Database-as-a-service provider MongoHQ announced that it now offers ElasticSearch as a service through its platform. In recognition of its expansion beyond 
, to being a multi-DBaaS vendor, the company has officially changed its name to ""Compose,"" and its website can now be accessed at ,.
,
With this announcement, Compose now offers the first fully-managed platform to safely and responsibly deploy, host and scale both MongoDB and ElasticSearch (in Beta). For developers, the ability to combine the speed and scale of MongoDB for application data with the flexible, near real-time search capabilities of ElasticSearch through a single DBaaS platform will offer a unique ability to more fully exploit the advantages of each database. 
,
As an open-sourced structured search engine, , can be leveraged for a wide range of uses, including user-defined, flexible queries across a range of attributes and powerful full text search. 
,
,
,
Compose seeks to deliver the most easy and cost-effective way for developers to host and scale new databases. Unlike platforms that require you to anticipate how much space you'll require, Compose asks users to pay only for the data they actually use, and starts at $18/GB per month for MongoDB, and $54 per month for the first 2GB for ElasticSearch, then $18/GB per month as it grows.
,??,
For more information, visit  ,.   "
"
,
,
As many organizations are now working with unmanageably large data sets, the importance of using and maintaining an analytics platform which can cope with this scale of information is essential. This presents both a challenge and opportunity as organizations must identify patterns and gain actionable results in order to gain a crucial advantage over competitors. Big Data Innovation will help businesses understand & utilize data-driven strategies and discover what disciplines will change because of the advent of data. With a vast amount of data now available, modern businesses are faced with the challenge of storage, management, analysis, visualization, security and disruptive tools & technologies.
,
, (June 4 & 5, 2014) was organized by the Innovation Enterprise at Toronto, Canada. Illustrated intermittently with case studies, interactive panel sessions and deep-dive discussions, this summit offered solutions and insight from the leaders operating in the Big Data space.
,
We provide here a summary of selected talks along with the key takeaways.
,
Here are highlights from Day 1 (Wednesday, June 4, 2014):
,
, gave a thought-provoking talk on ""'Big Data' Mindset and How it Can Enhance Customer Experience"". ""Big Data"" seems to be a no-brainer and so many more organizations are trying to run big data projects. But last year the focus was more on infrastructure and all costs associated with it. What seems to be missing in the bigger picture is the true value of big data which is if and only if it drives strategic insights. Based on Gartner; 64% of enterprises surveyed indicated they're deploying or planning big data. Yet, they still don't know what to do with it. Leila explained how to develop ""big data"" mindset in an organization and how to use it for enhancing customer experience.

,

She suggested that instead of ""Big Data"" we should rather think about ""Right Data"". Storing vast amounts of data in an extremely efficient manner does not benefit an enterprise if it isn't using that data to generate insights that drive marketing and business decisions. People have a narrow vision of Big Data, thinking of the term merely in the context of IT infrastructure, storage, cost, training, privacy, security, etc. Big Data is a shift in strategy, and not merely a technology.

,

She quoted the following from Accenture Technology Report - 2013:
,
""Analytics have always been a challenge, but partially because businesses have often conducted the process in an exploratory fashion?€?they've collected available data and then analyzed it, rather than assiduously determining what data will aid their business strategy and then ensuring that the right data is collected to analyze.""

,

Citing Gartner reports, she mentioned that Big Data investments are steadily increasing. The most common business problems, for which Big Data is being leveraged, are: enhancing customer experience, improving process efficiency and designing new products / new business models.

,

The application of Big Data Mindset to Customer Experience needs seamless integration of Customer Experience surveys, enterprise data warehouse and web analytic tools. It is important to understand online behavior and demographics of customers participating in CEI (Customer Experience Index) surveys & Segmentation. Organizations need to develop personas-specific, concrete representations of target users and use this information from strategy to design to testing. Based on prior research and exploratory interviews, determine the key attributes differentiating each persona and the number of personas. In summary, she emphasized that: it is important to develop Big Data Mindset throughout the organization, which requires significant cultural change - transforming the enterprise to become insight-driven.

,

, gave a good overview of the data analytics challenges in managing healthcare, in his talk ""Big Messy Data: The Case of Public Health"". Public Health is concerned with monitoring the health of populations (that is, groupings of individuals) and with the measures that can be used to improve their health. The concerns include such short-term issues as identifying an outbreak of infectious disease as well as long-term issues such as development of diabetes. Data complexity ranges from relatively modest datasets from laboratory test results to heterogeneous sources such as the health system, pollution monitoring, climate records and behavioral evidence. Messiness includes timeliness, linkage between phenomena and semantics.

,
In this eclectic talk, Jim outlined the challenges and PHO?€?s plans to address the data management and analytic issues. Canada spends around $211 billion on healthcare, which comprises of 11.2% of its GDP. 30% of this money is spent on hospitals, 16% on Drugs and 15% on Physicians. All these expenses are growing year-over-year by around 2.5%. The current challenges include: communicable and infectious disease surveillance, ""determinants of health"", ""co-infections"", chronic diseases - time and combination of factors, genomics and how to change people's attitude and behavior towards personal health. Jim's vision for future includes setting up data federation and using Big Data to measure effectiveness of public health messages.

,

, talked about the prevalent misconceptions around Big Data in his talk ""Five Great Myths About Big Data"". Big Data may eventually lead to the discovery of the ?€?Theory of Everything?€? in Physics one day; however, it will still not become a solution to everything in the world we live in. Following the wrong approach towards Big Data can cost a company, its reputation and its competitive edge in the market. Just knowing Attribution Models and Predictive Analytics is not enough. If you become a believer in one of the myths described below, you will end up with wrong numbers.
,
,
,
, ?? ,
,.
,
,
,  "
"
,
By Irmak Sirer, Datascope, Aug 2014.,
,
,
One of the most frequent questions we hear, right behind ?€?,?€? or ?€?,?€?, is ?€?how do I become one? I should probably just get a Master?€?s, right??€? Perhaps not anymore; ,, ,, and , are disrupting this traditional path and providing two viable alternatives. At one extreme, self-learning through Massive Open Online Courses (MOOCs) give access to courses at an extremely low cost (often free), but leave it ?€?as an exercise for the reader?€? to identify a suitable set of courses and tools to round out a coherent skillset. Bootcamps offer a middle ground where students can pay for a structured learning environment at a far more affordable rate compared with obtaining a Master?€?s Degree. So, ?€?which path do I take??€?,
,
We think the answer to that question largely depends on the student. In some cases a student will prefer attending a bootcamp whereas in other cases a student will prefer receiving a Master?€?s at a university or taking university courses online through MOOCs.,
,
Here at Datascope we see great benefits from the bootcamp format, so when , (a part of ,) contacted us about partnering to design a data science bootcamp, we jumped at the opportunity. We thought we could take all these points we see as the advantages of the format, and elevate them as much as we could. So, we designed a course that would give aspiring data scientists a lot of experience with 4-5 projects, and a guided route of several core data science concepts and approaches. Participants can quickly build the necessary foundation without the burden of teaching herself everything or paying the handsome price of a Master?€?s program before realizing her dream job. If you?€?re interested, our Data Science Bootcamp program is starting on September 2 in New York (applications due by August 11), and ,.,
,
Since there are many things to consider when choosing which program works best for you, ,, we do a thought experiment to compare the three experiences for a fictitious aspiring data scientist named Audrey. For the sake of brevity, the following table summarizes our thinking about what each of these experiences is like and, more importantly, who they are ideally suited for.,
,
,
,
,
,
As technology increases the rate of change of society, the most successful workers will be those that can quickly shift to new specialties and learn on the job to meet market demands. In our opinion, the bootcamp format provides the benefits of personalization, credentialing, and social learning that a Master?€?s degree offers, but at an accelerated rate with experiential learning. Sure it is more expensive than being self-taught, but the connection with employers and the guided, experiential learning process increases your confidence to tackle the uncertain prospect of making a career switch.,
,
To become a data scientist, you don?€?t need to have postgraduate degrees, or 20 years experience, or be proficient with every data-related technique and tool under the sun. What you need is to have enough baseline knowledge and experience, and the skill to constantly adapt and learn. Bootcamps, in our opinion, are the perfect medium for making the transition.,
,
, - reposted with permission.
,??,
Bio: ,.
,
,
,
,
,
 ,  "
"
,
By Gregory Piatetsky,  
,, July 5, 2014.
,
Recent Facebook 
, where they
manipulated newsfeed and emotions of 700K users, 
has received a lot of
, and backlash.
,
New KDnuggets cartoon by Ted Goff examines the situation.
,
,
,
,
,
,
,??,
,
,
 ,  "
"
,
,
,??,??
,??,??,??,??
,
,
,??,??
,
,  "
"
Most popular 
, tweets for Aug 4-5 were
,
Meet Fortune 2014 #BigData All-Stars: data scientists, entrepreneurs, CEOs, engineers, researchers - 35% women ,
,
To add #MachineLearning: for Python, scikit-learn; for Hadoop: Mahout; for Java: Weka; for JavaScript: ConvNetJS ,
,
Data Scientist role shifting, with companies focusing on Developers ,
,
Data Scientist role shifting, with companies focusing on Developers ,
,
,  "
"
The potential to unlock business insights from ever increasing volumes of unstructured data has promoted text analytics from backwater to mainstream technology. Our subject matter experts from 
, and ,
have successfully used text analytics to really understand their customers' behavior, make more accurate data-driven strategic decisions, and get ROI on their marketing spend.
,
They'll be sharing their approaches 
,and success stories at 
, 
on ,
,
Click here to register for free: ,
,
,
,
,??,
,
,
,??,
The webinar features presentations, audience polling and a live Q&A session.
,
Don't worry if you are unable to attend the live event. All registrants will receive the webinar recording a few days later.
,
Click here to register now: 
,
,
Alesia Siuchykava
,Project Director
,Data Driven Business
,DD: 201-204-1694,
alesia@datadrivenbiz.com  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for July 8 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Jul 4-6 were
,
plyrmr, package for making R work seamlessly with the #Hadoop system #rstats ,
,
KDnuggets Cartoon examines happy kittens in light of Facebook emotion manipulation data science experiment 
,
,
Useful for #DataScience: Simple script from setting up R, Git, and Jags on Amazon EC2 Ubuntu Instance #rstats ,
,
,  "
"
,
,
Here is the company, startup, and acquisition activity for June 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are the tweets, in reverse chronological order
,
,??,
,
,  "
"
,
By Ray Major, Halo Business Intelligence, July 2014.,
,
,
While fashion is usually not a high priority for data professionals, it is true that the terms used to describe the work that they seem to have changed in the last few years. ??Data Science has caught the most attention these days. Many say they are seeing something groundbreaking, but the more jaded observers see a clever twist on an old standard. So what exactly is Data Science and can an analyst, who simply does analytics, accessorize their current outfit to turn heads? Is it any different than data mining?,
,
The problem is that the term has a mystique where its features are hard to distinguish from the crowded taxonomy of different data terms.?? This revisionism is commonplace in many fields, but particularly in the fast growing data science field. I know many colleagues who re-branded themselves as data scientists in order to be more attractive in the market. They would argue that an analyst make reports while a data scientist makes visualizations, even if both have the exact same content.,
,
There are a slew of other terms that get lumped in these categories and cause confusion when talking about statistics, business intelligence or data science, but none more elusive than the most nebulous of terms ?€? Analytics.,
,
The graphic below is an effort to help bring clarity to the analytics domain and helps develop a simple glossary for us to frame the actual work being performed.,
,
,
,
The overlaps between business intelligence and data science are significant. In fact, a small majority of tasks and vast majority of labor are spent in tasks that are shared between the two related disciplines.,
,
Here are some additional comments on the Infographic:,
,
,
,
While this graphic may not be enough to update your title from ?€?analyst?€? or ?€?business intelligence developer?€? to ?€?data scientist?€? (though you would not be the first), it certainly helps give boundaries to very elusive labels.,
,
,
Ray Major is the Chief Strategist of ,. A data scientist, economist and statistician by training, he?€?s a life-long practitioner in the mysterious arts of data intelligence and analytics. You can contact Ray by email at??,??and also follow him on Twitter at ,
,
,
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, Jul 1, 2014.
,
New entries for June below are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to , page:
,  "
"
Catherine Van Evans, Lavastorm, July 2014. 
,
,
Ever wonder what your peers see as challenges facing the industry, or where they'll be focusing investment in the coming months?  We do too. According to our recently completed Analytics 2014 survey, seventy five percent of businesses have yet to successfully deploy big data analytics solutions in production even though 65% are rapidly investing more in analytics tools. These findings and others are part of 
,
,
Other Key findings featured in the report include:
,
,??,
,
,
Download this original Lavastorm Analytics research and gain insight into the responses of 495 C-level executives, business analysts, data scientists and analytics professionals regarding their analytic initiatives and predictions for 2014.
,
So now what? Is your organization is facing any of these Big Data challenges? Data integration, lack of skills, proving ROI.  If you answered yes, you are not alone. These are just some of the challenges facing organizations when it comes to Big Data.  
,
,,
to learn more about the major opportunities that Big Data represents.
,
,.
,
,
,
,  "
"
        ,
,  "
"
,
,
With business analytics becoming an integral part of successful organizations, businesses are truly embracing data insight at every level. As organizations have developed the capacity to gain greater insight from data and statistics - and with the increase in the volume of data available - it is now essential to use innovative analytics practices to succeed.
,
The , (May 21 & 22, 2014) organized by the Innovation Enterprise at Chicago, IL covered major challenges and opportunities being observed by Analytics leaders across industries. Executives at the forefront of analytics shared their innovative approaches, providing insight into how they have gained valuable information from raw data. Executives discussed the challenges faced within some of the world leading organizations and provided case study examples of how they are pushing the boundaries of analytics.
,
We provide here a summary of selected talks along with the key takeaways.
,
Here are highlights from Day 1 (Wednesday, May 21, 2014):
,
, talked about the increasing trend of client experience projects, in his talk titled ""Leveraging Analytics to Drive Client Satisfaction"". Many companies are embarking on client experience projects to improve client satisfaction, loyalty, and most importantly overall relationship profitability. He recommended an Analytic Framework comprising the following six phases in an iterative fashion: Knowledge (develop 360 degree client view), Assess (assess gaps in current client experience), Link (link plans to fix gaps with strategic goals), Test (pilot concepts), Launch (roll-out winners), and Monitor (continuous optimization).
,
To develop a holistic understanding of clients, prioritize client data with highest impact (such as demographics or share of wallet) and layer additional data over time. Next, organize clients into homogeneous groups. Profile key behaviors and attitudes, and use this information to map client journey. It is very important to balance customer goals with business goals through an appropriate business model. Any major investment should be preceded with pilot ideas to hone in on the target and gain learning without major costs. Finally, leverage Big Data to monitor client sentiment and improve the Analytics framework through feedback loops.
,
, gave an interesting talk on ""Application of Predictive Analytics in Credit Risk Optimization"". Credit Risk Analysis is integral to every step in the credit lifecycle process, from prospect and customer segmentation, through origination scorecards, to the design and execution of account management and collection strategies, whether for mortgages, loans, credit cards, and other consumer finance vehicles.
,
Credit Analytics can be defined as identifying and mitigating risk associated with financing credit product to customers. Risk team quantify risk, monitor and report risk of prospect or customer by development of risk monitoring tool, scorecard and models. Credit Analytics plays a great role in taking decision on customer?€?s associated risk for pricing as well credit exposure decision associated with it. Risk team uses predictive analytics for quantitative analysis, forecasting and strategy development. Risk analysis comprises of several different aspects (listed in the order of increasing complexity as well as business value):
,
, ?? ,
Banks rely on several Credit Risk models based on the customer?€?s demographic, bureau and past payment information. Effective management of credit risk throughout the credit life cycle allows institutions to optimize their capital investments, reserve for future loss, (baseline and expected), and maximize shareholder value. Credit Analytics plays a critical role in monitoring and predicting future risk within various baseline and stressed macroeconomic scenarios, and from changing customer behavior.
,
, shared insightful observations from a recent research conducted by AOL in his talk ""Moving Beyond Conventional Wisdom: Measuring Digital KPIs that Matter"". AOL, along with other partners, cataloged over 22,000 purchases across 20 product categories and examined shopping behaviors and attitudes of over 5,000 online users. One of the most interesting observations is that insights consistently run counter to conventional wisdom. The implications of this research challenge traditional means of customer acquisition and demonstrate the importance of aligning marketing KPIs with true consumer behavior.
,
Given the immense change in our digital lifestyle, it is worth asking: does the metaphor of a ""journey"" still make sense today? So, the consumer analytics & research team at AOL decided to test the commonly-held conventional wisdom about a customer's journey to purchase. They observed that advertisers still underestimate the importance of Digital in brand-building. He summarized the research findings into the following three implications:
,
, ?? ,
, delivered a great talk on ""Text Mining Social Media for Competitive Intelligence"". Social Media is a great source of input data for competitive intelligence given the tremendous magnitude of information-sharing over social media from people across all demographics. However, social media also has a lot of noise, making it harder to exploit the signal. This social information is being used for making decisions in a wide variety of situations ranging from customers purchase decisions being influenced by social reviews to marketers mining social data for consumer insights.
,
Before starting to think about mining social data, we need to identify where the relevant conversations are happening, so that we can focus and save ourselves from getting lost in the wilderness of social media. Social data can be collected using technology such as Xpath, APIs, JSON (Java Script Object Notation), Web Scraping, or other third-party software/services. Next, he mentioned that despite huge potential text analytics has still not found mainstream adoption. He described the text mining process as comprising of four steps: Discover, Gather, Evaluate, and Inform. He explained the benefits of text mining social data through various real-life examples including one which used Glassdoor text data to identify the key differences in employee experience across two major insurance companies.
,
The key challenges of text mining social data are organizing the data, extracting insights that add business value and distinguishing signal from noise. In conclusion, he noted that the social media competitive intelligence has several major advantages over legacy competitive challenges, such as truly customer-centric, real-time feedback and unbiased insights.
,
,
,
,
,  "
"
Most popular 
, tweets for Jun 30 - Jul 1 were
,
Is ""Data Scientist"" more than ""Data Analyst""? 
,
,
Great team! Alteryx and Databricks (Spark firm) to lead development of Apache SparkR for #BigData #Hadoop Analytics ,
,
Good list of 41 #BigData Influencers - Journalists, Public Sector, Industry (including @kdnuggets), and Academia ,
,
,  "
"
By Gregory Piatetsky,  
,, July 2, 2014.
,
Here are 62 upcoming July - December 2014 meetings and conferences.
,
Most common countries:
,
,??,
Here are the most common cities:
,
,??,
Boston is becoming a real Big Data hub, with the largest number of meetings! 
You can find the full list on KDnuggets page:
,.
,
Color code:
,
,??,
Here is a word cloud for the meetings below.
,
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
,
,??,??,??,??,??,??
,  "
"
,Gordon S. Linoff is a co-founder and principal consultant of Data Miners Inc., a specialist consulting company that focuses on data analysis and data mining. Linoff is a widely respected thought leader, practitioner and teacher in the area of data mining. He has a keen interest in understanding and analyzing large data sets and in applying the results to business problems.
,
Register today for one of his upcoming classes and learn his secrets on data mining and survival analysis and how he as solved problems for customers around the globe!
,
,
,
,??,
,
,
,??,
Both courses are part of the ,  "
"
,
Latest ,, (Jul 02, 2014) ,:
,
,??,
Also
, (2) |
, (7) |
, (6) |
, (1) |
, (1) |
, (2) |
, (4) |
, (2) |
, (3) |
, (6) |
,
,
World Cup 2014 Predictions: both Bloomberg and FiveThirtyEight say: Brazil beats Argentina in the final   "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Jul 2-3 were
,
Booking: Data Scientist ,
,
For advanced Data Scientists: Tutorial in Gradient boosting machines, a high-performing machine learning method ,
,
For advanced Data Scientists: Tutorial in Gradient boosting machines, a high-performing machine learning method ,
,
,  "
"
,
,
, is Innovation Strategy Consultant for Fujitsu Laboratories of America. There he has led projects ranging from digital negotiation systems to sensor-based healthcare, from automated ontology generation to novel interface methodologies.
,
Dave has worked as a writer and producer with WGBH?€?s Nova Science Team, served as a Mellon Visiting Professor at Caltech, and been involved with many Silicon Valley Startups. This includes his role as a founder, VP Production, and lead creative at Worlds Inc., and founder and VP Marketing at Disappearing Inc.
,
Dave is a Fellow of the Internet Archive. He was selected as one of Time Magazine?€?s 2001 ?€?Digital Dozen?€? ?€? one of the 12 most influential people in the digital world. He has over 70 patents granted and pending.
,
,
,
Here is the second part of my interview with him:
,
,
,
, I?€?d say that the biggest challenge is getting the sensors small enough and usable enough.  The second biggest challenge will be developing a business model that drives deployment. I only list business model second because plenty of people will try plenty of ideas and something will succeed. There aren?€?t any fundamental physical barriers that need to be overcome ?€? as there are with sensor development.
,
There are still plenty of issues to consider ?€? privacy being high on the list.  Finding the appropriate limits of self-knowledge will likely become another question we will all have to grapple with. But these changes will come.
, 
 
,
 
,
, Excellent question.  A key to improving efficiency across many kinds of systems is information capture and sharing.  This is true for everything from scheduling maintenance on big machines in factories to reducing demand on the electric grid.  Efficiencies in our daily life and healthcare / wellness are no exception.  The challenge unique to social and health issues, as compared to say, equipment maintenance, is finding a way to get those efficiencies without compromising people?€?s privacy.
,
I?€?m enough of a technologist to believe that there are technical solutions to this problem.  I think there will inevitably be issues along the way as we move into this new world, but we will get there.
,
,There are a few challenges along the way. One is to be collectively committed to learning from the issues. Another is to recognize that, however much we value privacy, we can?€?t foresee all of the new problems that will emerge as the technical and business landscapes change. We don?€?t know, and can?€?t know, many of the questions we need to ask.
,
As a distantly related example, look at the struggles around bitcoin and crypto-currencies.  They are creating sets of questions that nobody had to think about before. The questions would have been impossible even to ask. Now regulators and enforcers are struggling to catch up. In the same way, ubiquitous continuous sensing will surface new questions that were previously impossible even to ask ?€? let alone answer.
,
,
,
 
,
,
, I?€?m betting that the term ?€?Big Data?€? will seem quaint pretty soon. The technology community is already starting to move to a recognition that lots of data may be just lots of data. It will not necessarily translate into wisdom or even understanding. The big challenge ahead of us is figuring out what are right questions to ask, and how to ask them. The data alone isn?€?t nearly enough.
,
That said, I think we will come to take for granted the sea of data we will be awash in. We will come to expect to be able to review any aspect of our lives.  ?€?Where was I on that day??€?  ?€?How fast was I traveling??€? ?€?What did I eat??€?  ?€?Who did I talk to??€?  ?€?What did I say??€?  ?€?How did I feel??€?
,
All of that information will be at our fingertips ?€? just as the web has made so much other information instantly available. All of this will be potentially interesting data. But the lessons we learn by looking at that data in new ways will be more interesting. There will be questions we haven?€?t even thought to ask yet that can suddenly shed light on how we live our lives and what it means to be human.
,
 
,
 
, , by Thomas Hager. I found it and incredibly dramatic example of the moral ambiguity and unpredictability of scientific discovery.
 ,
When not working I enjoy playing the Asian board game ?€?Go?€? and riding my recumbent bike (though generally not at the same time).
,
,
,  "
"
,
,
Big Data Innovation is back, returning to the Westin Copley Place in Boston on September 25-26.
,
Early Bird pass prices are only available until next 
, - so if you are looking to join the largest gathering of Big Data & Analytics professionals, then now is the best time to secure your place.
,
View the schedule here: 
,
,
As ever, this year's schedule reflects the issues and topics that matter most to today's data & analytics executives, with 8 tracks and 80+ speakers including:
,
,??,
, 
If you're wondering what to expect, check out a presentation from the Principal Data Scientist from Bing, 'Deriving Conclusions from Data': 
,
,
If you are interested in attending, or more information, 
,please contact Pedro Yiakoumi at 
,, 
,or alternatively, you can secure your place here: 
,. 
,
We hope to see you there,
,
Innovation Enterprise  "
"
,
,
Wed, July 23, 2014, 1pm PT,  4 pm ET
,Duration: 1 hour
,
,
,
Analytical applications are everywhere these days, and for good reason. Organizations large and small are using analytics to better understand any aspect of their business: customers, processes, behaviors, even competitors. 
,
There are several critical success factors for using analytics effectively: 
,1) know which kind of apps make sense for your company; 
,2) figure out which data sets you can use, both internal and external; 
,3) determine optimal roles and responsibilities for your team; 
,4) identify where you need help, either by hiring new employees or using consultants 
,5) manage your program effectively over time.
,
Register for this episode of TechWise to learn from two of the most experienced analysts in the business: Dr. Robin Bloor, Chief Analyst of The Bloor Group, and Dr. Kirk Borne, Data Scientist, George Mason University. Each will provide his perspective on how companies can address the key success factors in building, refining and using analytics to improve their business. 
,
There will then be an extensive Q&A session in which attendees can ask detailed questions of our experts and get answers in real time. Registrants will also receive a consolidated deck of slides, not just from the main presenters, but also from a variety of software vendors who provide targeted solutions. 
,
,.  "
"
By ,
,
,
In an experiment, the statistical scientists showed that with proper statistical adjustment, non-representative polling, such as those conducted through the Xbox gaming community and other online surveys, can predict elections and measure public opinion on a broad range of social, economic and cultural issues. The experiment?€?s result is also a major advance in the Big Data era in which the amount of data is overwhelming, but drawing insightful conclusions from the diverse and non-random data sources continues to be challenging.,
,
The weaknesses of non-representative or ?€?convenience?€? sampling were dramatically exposed after a mail-in , by , incorrectly predicted a landslide victory for Republican Alf Landon over President Franklin Roosevelt in that year?€?s presidential election.,
,
Since then, election forecasts and other public-opinion polls traditionally have been based on representative polls, in which randomly sampled individuals are asked for whom they intend to vote or their opinion on an issue. However, the recent election defeat of Rep. Eric Cantor helped reveal the challenges of achieving representative sampling in public-opinion polling, primary of which are high nonresponse rates and high cost. Low response rates likely mean the people who participate in a phone survey might not be representative of the targeted population, such as registered or likely voters, which could compromise the survey?€?s findings. As a result, the statistical benefits of representative sampling have diminished.,
,
,
In their experiment, the statistical scientists created election forecasts from a novel and highly non-representative survey dataset: a series of 45 daily voter-intention polls for the 2012 presidential election conducted on the Xbox gaming platform. After applying proven statistical techniques to the Xbox dataset, the team obtained election estimates consistent with forecasts made by leading poll analysts, which were based on aggregating hundreds of traditional polls conducted during the election cycle.,
,
These findings combined with significant cost-savings in collecting survey data via today?€?s new online technologies, such as the Xbox gaming platform and web-based surveys, could make non-representative sampling the new norm.,
,
The research was conducted by Andrew Gelman and Wei Wang of the Columbia University Department of Statistics and Sharad Goel and David Rothschild of Microsoft Research. ,
,
, is Public Relations Coordinator for American Statistical Association.
,
,
,  "
"
, is pleased to announce that Pedro Domingos is the winner of its 
,. He is recognized for his foundational research in data stream analysis, cost-sensitive classification, adversarial learning, and Markov logic networks, as well as applications in viral marketing and information integration.,
,
,
, carried out some of the earliest research on mining data streams. His VFDT algorithm was the first to be capable of learning decision trees from streams while guaranteeing that the result is very close to that of batch learning, and remains the fastest decision tree learner available. He went on to generalize the ideas in VFDT to clustering, the EM algorithm, Bayesian network structure learning, and other problems. The resulting VFML toolkit is one of the best open-source resources for stream mining.,
,
Another of Prof. Domingos' key contributions was the MetaCost algorithm, perhaps the most widely used algorithm for cost-sensitive classification. Previously, misclassification errors with different costs were handled by oversampling or undersampling the classes, both of which have serious drawbacks. Instead, MetaCost decision-theoretically relabels examples to minimize cost while preserving the data distribution.,
,
Prof. Domingos was a pioneer in social network mining, where he defined the influence maximization problem and proposed the first algorithms for it. The goal of influence maximization is to optimally choose which nodes in a network to influence such that the greatest possible cascade of word of mouth results. This has led to both a new area of research and intense interest from industry.,
,
Another area that Prof. Domingos helped start is adversarial learning. In many areas, including spam filtering, fraud detection and counter-terrorism, the people being modeled by the learning system modify their behavior adversarially in response to the system. For example, spammers modify their messages to fool the spam filter. Prof. Domingos defined the problem of adversarial classification as a game between a learner (whose moves are classifiers) and an adversary (whose moves are perturbations of the data), and developed the first algorithm to solve it. Adversarial learning has since grown into a major area of both fundamental and applied interest.,
,
Prof. Domingos also pioneered the use of machine learning methods in information integration, a problem of crucial importance to large organizations that includes schema matching, ontology alignment and entity resolution. Prof. Domingos addressed each of these, and learning-based approaches to them are now standard.,
,
In the last decade, Prof. Domingos has led the field of statistical multi-relational learning which is essential for a mature science of knowledge discovery. Prof. Domingos has proposed Markov logic networks as a means to unify first-order logic and probabilistic graphical models. This formalism form the basis of research by many different groups, and of the open-source Alchemy system.,
,
Prof. Domingos received his Ph.D. in Information and Computer Science from the University of California, Irvine, in 1997, and is currently Professor of Computer Science and Engineering at the University of Washington. He has authored or coauthored over 200 technical publications, several of which have received best paper awards. He was program co-chair of KDD-03 and associate editor of JAIR, co-founded the International Machine Learning Society, and serves on the editorial board of Machine Learning. He is a AAAI Fellow, and has received many previous distinctions, including an NSF CAREER Award, a Sloan Fellowship, an IBM Faculty Award, and a Fulbright Scholarship.,
,
The previous SIGKDD Innovation Award winners have been: Rakesh Agrawal, Jerome Friedman, Heikki Mannila, Jiawei Han, Leo Breiman, Ramakrishnan Srikant, Usama M. Fayyad, Raghu Ramakrishnan, Padhraic Smyth, Christos Faloutsos, J. Ross Quinlan, Vipin Kumar, and Jon Kleinberg.,
,
The award includes a plaque and a check for $2,500 and will be presented at the Opening Plenary Session of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2014), on Sunday August 24th in New York City, USA. Prof. Domingos will present the Innovation Award Lecture immediately after the awards presentations.,
,
For more information, visit
,
,
,
,  "
"
,
,
The role and importance of Chief Data Officer has been increasingly gaining attention across industries. Data is being viewed as the most competitive tool that organizations have in order to maintain relevance and growth in today?€?s complex environment. While the CDO office holds enormous potential, it is currently faced with a wide variety of issues around data integration, ROI, data quality, etc.
,
, (May 22-23, 2014) organized by the Innovation Enterprise at San Francisco, CA covered the above topics through insightful talks from leading experts across various domains. Along with a deep dive into the role of the Chief Data Officer, the summit covered innovation, data management and data governance. Through real-life business case studies and discussions on major issues, the summit offered solutions and insight from the CDOs and other business leaders.
,
Despite the great quality of content as well as speakers, it is hard to grasp all the information during the summit itself. KDnuggets helps you by summarizing the key insights from all the sessions at the summit. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key sessions for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers. 
,
,.
,
Here are highlights from selected talks on day 2 (Fri, May 23): 
,
, described how organizational silos impact our ability to leverage data in his talk titled ""Building a Data-Driven Culture"". Data is now increasingly being considered one of the most important assets of the organization. He highlighted the top data challenges as: hidden insights due to data locked in silos, investment prioritized by IT (these priorities may not align with business strategy), IT may not have insight into new business opportunities, and often long development lifecycles. The need of the hour is to shift the focus on data from the IT organization to business decision makes. Business leaders understand what value data can bring to making timely, well-informed and impactful decisions, but only if it is managed, understood and governed consistently across the organization. 
,
IT should focus on ease of use, adaptability of analytics applications in order to empower business users. Complementing this, the business needs to clearly articulate its data requirements to IT, and conversely, IT needs to be accepting of the fact that, more and more, requirements will come from the business, rather than being IT driven. In order to focus on fostering a data driven culture we need to develop some fundamental capabilities such as Master Data Management, Metadata documentation, Enterprise Data Modeling, Governance and Stewardship.
,
,explained Open Data movement and its future, in her talk titled ""Open Data Grows Up: Institutionalizing an Initiative"". 
,
She decomposed the Open Data movement in the following five phases:
,
, ?? ,
Open data has unlocked a data ecosystem - from consumer facing apps to increases in government transparency. And now it?€?s time for open data to move past initiative and become part of doing business. Citing a survey, she mentioned that the biggest barriers to data use are: data quality, skills and/or capacity to use data, and knowledge/awareness of existing data sets. Within an ecosystem of shared data and technology, the San Francisco country is trying to (1) increase individual skills and capacity, (2) support analytic programs in city departments, and (3) foster a data enabled policy environment in the pursuit towards increased use of data in decision-making. She shared the mission and vision of the San Francisco's CDO office, which focuses on using data to drive government transparency, efficiency and innovation.
,
, talked about Big Data from a CIO's perspective. In her talk titled ""Bridging the gap between Big Data and existing Data Warehouses"", she emphasized that data will play a revolutionary role in years to come through changing business models, driving business strategy, etc. The data flow diagram in the investment management sector can be summarized to: Alpha factor data, risk analytics, market data, account/position data, trade cost data and other analytics impacting the trading decision.
,
The key data challenges include the large size of data sets, diversity of data formats, expectations heading towards real-time analytics and delivering valuable business insights. She envisions the future where data warehouses and Big Data systems will co-exist and mutually benefit from each other. She concluded her talk with the following four key takeaways:
,
, ?? ,
, delivered an insightful talk on ""The Importance of Experimentation"". Using the standard old medical practice of bloodletting (which killed many including George Washington) as an example, he made a very important point that we can be wrong about something for 2000 years. It is not trivial to know if something really works.
,
He shared a few real-life A/B testing results around website design and explained that intuition is not reliable. This happens so because we learn from correlations, and quite often, we intuitively assume correlation to be causation. One of the first principles we need to learn in order to be data-driven is that correlation does not imply causation. Thus, there is an immense need for experimentation in order to explore and confirm causation.
,
The gold standard to prove causalty is the randomized control experiment, however, this is not enough. It is also necessary to understand that to have a culture of experimentation, we first need to understand that when an idea fails, this is not an error and important lessons can be learned.  In conclusion, he described the need for the right incentives to experiment and shared the following quote:
,
, -Upton Sinclair
,
,
,  "
"
,
,
With business analytics becoming an integral part of successful organizations, businesses are truly embracing data insight at every level. As organizations have developed the capacity to gain greater insight from data and statistics - and with the increase in the volume of data available - it is now essential to use innovative analytics practices to succeed. 
,
The , (May 21 & 22, 2014) organized by the Innovation Enterprise at Chicago, IL covered major challenges and opportunities being observed by Analytics leaders across industries. Executives at the forefront of analytics shared their innovative approaches, providing insight into how they have gained valuable information from raw data. Executives discussed the challenges faced within some of the world leading organizations and provided case study examples of how they are pushing the boundaries of analytics. 
,
We provide here a summary of selected talks along with the key takeaways. 
,
,.
,
Here are highlights from Day 2 (Thursday, May 22, 2014): 
,
, talked about the State of Illinois Framework Project (,) in his talk ""Innovating for 21st Century Data-Centric Health & Human Services"". The Framework, a Statewide collaboration funded and chartered by seven major State Agencies, is designing Cross-Agency HHS (Health & Human Services) Business Intelligence, Multi-tiered Dashboarding and User Centric workflows that bring together datasets and processes from 200+ existing HHS Programs and 60+ major HHS IT platforms.  The Framework is leveraging Affordable Care Act funding opportunities, the State?€?s ?€?Cloud First?€? policy and the maturing of Cloud Apps, API?€?s and Federal NIEM Standards to drive evolution toward User Centric, Integrated and Interoperable State Health and Human Services delivery. 
,
He stressed on the need to move away from siloed approach towards an enterprise architecture designed for interconnectedness and interoperability. He identified the key enterprise analytics challenges for government websites as: highly distributed IT management, limited awareness of analytics advancement in private sector, and other factors (such as governance, legal, privacy, etc.). Emphasizing on dashboards, he explained the key differences as compared to reports. The reports are static, backward looking, usually compliance driven and hard to customize. On the contrary, the dashboards are interactive, forward as well as backward looking, highly visual, user need driven, and easily customizable. To boost innovation, it is important to decouple the dashboards and other user interfaces from the back-end data processing. Talking about the results so far, he mentioned that the framework has enabled cross-agency decision making and accountability.
,
, explained the importance of visuals in her talk titled ""Data Visualization Drives Your Success"". Data visualization is a key aspect of both the analysis and understanding of complex data.  Nowadays, we are constantly challenged to create clear, meaningful, and integrated analysis that can stimulate our thinking process at the speed-of-thought. Visual analysis has proven effective at any phase of data exploration.  For corporate executives, good visual presentation is vital in identifying issues and advance decision making. 
,
Visualization requires a good understanding of your target audience, proper presentation of the key message (less is more!), and perpetual review from reader's view. Good graphics comprise of rich content, inviting visualization and smart execution. Next, she went into the details of best practices for visualization and talked about reference points, fair comparison, missing data in charts, legibility, color, lines, tables, maps, etc. Effective visualization serves to maximize data exploratory capabilities, reveal answers, and can even lead to more questions. 
,
, gave an interesting talk on ""Transforming Analytics with 'Soft Skills' - How to Get Your Analysts to 'Talk the Talk'"". In many companies, rather than driving decisions, the analytics team is reactionary- instead of being strategic partners, they operate as a back office function.  In today?€?s data driven environment, moving analytics to the front of the house is imperative for organizations to succeed.  And the key to becoming strategic partners is more than building a cutting edge model or innovative algorithm. It lies in developing the team?€?s ?€?soft skills?€?.  The analytics team?€?s future will be decided not only by the data mining they do and the models they create, but also by how well they understand the business, connect with people, and build relationships across the organization. 
,
She suggested that the analytics team needs both skills and reputation of top performance to gain full support. Besides technical skills, the analytics team need to be able to promote and sell ideas to decision makers, through excellent soft skills such as understanding the ""big picture"", effective communication, listening, building relationships, etc. Analytics team must find promoters within the organization and help them reach their goals by making analytics invaluable to them. Analytics are a key component of business strategy. Progress may not be linear but with well rounded analytical teams, our organizations and people will be on the right path for success.
,
, delivered an insightful talk on Analytics in Education. Thought leaders such as Clayton Christensen have argued that education is prime for disruption. The key challenge is how to increase the value of education. In other words, how to improvfe learning outcomes that matter while decreasing cost per learner. Currently, the analytics in education is limited mainly to data access and reporting. In future, it needs to be focussed on risk forecasting and predictive modeling. The ultimate goal should be to have analytics focused on optimization and strategy.
,
He demonstrated state-of-the-art applications of Big Data in education, including adaptive learning, learning analytics, and advanced visualizations. As an example, he showed the Student Success Dashboard of Desire2Learn (,) and SmartBook, which provides adaptive reading and learning experience.
,
,
,  "
"
,
,
,??,??,
,
,
,??,??,??
,
,  "
"
Most popular 
, tweets for Jul 4-6 were
,
Appropriate after #BrazilvsGermany - IBM #Watson gets a 'Swear Filter' after Learning The Urban Dictionary ,
,
,
Brilliant NYTimes visualization: How Birth Year Influences Political Views ,
,
FICO Infographic: Analytics methods that ""think"" like Humans: Neural nets, #DeepLearning, Neuro-dynamic programming ,
,
Why lists of experts in Statistics & Data Science rarely intersect - nice summary ,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for July 15 and later.
,
See full schedule at , .
,
,  "
"
By John Canny, Berkeley, July 2014.
,
,
We are very pleased to announce the beta release of the ,, version 0.9. The main page, which includes precompiled downloads for 64-bit Windows, Linux and Mac OSX, is here: 
,
,
,
,
BIDMach has several unique features:,
,
, BIDMach is currently the fastest tool for many common machine learning tasks, and the list is growing. When run on a single machine with graphics processor, BIDMach is faster than any other system (on a single node or cluster) for regression, clustering, classification, and matrix factorization. Every compute primitive has been , which means its been optimized close to theoretical performance limits.,
,
Checkout the benchmarks in: ,
,
,
,
, BIDMach has run larger calculations on one node than most cluster systems: with a large RAID, it has run LDA (Latent Dirichlet Allocation) on a 10 TB dataset. BIDMach can also run on a cluster, and includes a new communication protocol called ""Kylix"" which gives nearly-optimal throughput for distributed ML and graph tasks. It currently holds the record for PageRank analysis of large graphs, and was 3-6x faster than any other system on 64 nodes.,
,
, BIDMach inherits a powerful command line/batch file interpreter from the Scala language in which it is written. It has the simplicity of R, Python etc. but with uniformly high performance. It fully taps Scala's extensible syntax, so that math written in BIDMach looks like math. BIDMach includes a simple plotting class, and we are adding ""interactive models"" which allow interactive tuning.,
,
, BIDMach includes likelihood ""mixins"" to allow the qualities of basic models to be tailored to more specific needs. e.g. topic models can be tuned to favor more coherent or more mutually-independent topics.,
,
, BIDMach favors mini-batch algorithms and includes core classes that take care of optimization, data sourcing, and model tailoring. Writing a new model typically requires writing a small generic model class with a gradient method. The learner classes take care of running the model, using sparse or dense data, running on CPU or GPU and single or double precision.,
,
,
,  "
"
Most popular 
, tweets for Jun 2-3 were
,
,  "
"
,
,
To keep up with the pace of development in modern business, it is essential to invest in more sophisticated and innovative manufacturing processes. Investment in analytics offers this opportunity as it allows organizations to streamline their manufacturing process, improve efficiency and reactivity to customer demand by utilizing real time data. For any company manufacturing its own products, this is an essential area to gain an advantage over competitors.
,
The , (May 21 & 22, 2014) was organized by the Innovation Enterprise in Chicago, IL to bring together analytics leaders from manufacturing sector to share their success stories and key learning. The summit addressed the issues mentioned above and brought together business leaders who are implementing and experimenting with new analytical methods for manufacturing - driving remarkable improvements in their organizations. Executives at the forefront of analytics shared their innovative approaches, providing insight into how they have gained valuable information from raw data.
,
We provide here a summary of selected talks along with the key takeaways.
,
Here are highlights from Day 1 (Wednesday, May 21, 2014):
,
, spoke on one of the most common challenge visionary employees face, in her talk ""Supply Chain Analytics: How To Get The Buy-in That You Need To Get Started"". A lot of data science practioners have a vision of how analytics can bring value, but find it difficult to convince thier boss to invest so that they can get started. Many of them are frustrated with end to end data quality, but find it difficult to convince their peers to collaborate in improving data quality. For quite some time, Analytics has persistently appeared in strategic priorities, yet it is important to understand that most companies are just at the beginning of this long journey and share the struggle.
,
Analytics evangelists should start with knowing themselves, their boss and their organization. In context of the ""crossing the chasm"" curve, most of such analytics evangelists will fall in the ""Visionaries"" category, whereas their boss will fall in the ""Pragmatists"" category; both categories separated by the ""chasm"" valley. In Supply Chain Analytics, one is more likely to be drowned by inflated expectation than data. In conclusion, she provided the following advice for designing the first steps:
,
, ?? ,
, shared his experience and learning at HP in his talk ""Transforming the Largest IT Supply Chain in the World"". Describing the magnitude of data processing, he mentioned that every year HP's global supply chain systems process 36M purchase order lines, 87M sales order lines, 17M deliveries, and 19M invoices. Diverse nature of products means HP has to employ multiple strategies in all aspects of its supply chain from manufacturing to distribution. Accelerated technological innovations are continuously reducing time to market and fierce competition demands that products be delivered with high quality and exceptional customer service. The challenge is to transform this complex supply chain while maintaining business continuity and laying the foundation for longer term profitable growth.
,
He shared HP's vision to build the industry's best supply chain through predictive analytics, in-transit inventory allocation models, collaborative supply chain networks and enhancing customer experience. In the march towards its vision, HP has developed a cloud-based next generation supply chain architecture connecting customers, channel partners, sales staff, HP social networks, suppliers, manufacturing partners, and transportation carriers. HP's Buy/Sell solution gives it significant advantages (over the legacy applications) such as real-time supply chain analytics & decision support and metwork with over 36,000+ tranding partners. The ""Logistic Cloud"" layer in the systems architecture provides various important capabilities (such as collaboration, visibility, control) to Analytics applications.
,
, gave an insightful talk on ""Pharmaceutical Analytics - An Innovative Tool to Maximize Returns"". Pharmaceutical manufacturers are improving their marketing and product distribution strategies using analytics. Analytics are used to build mathematical models, computer simulations and decision strategies to extract value out of large pharmaceutical data. Thus analytics helps the industry to streamline operations to minimize risks and to get maximum returns. Predictive analytics helps to find the root cause of the failure and to bring process optimization. In pharmaceuticals, data is being generated from different sources such as R&D, Physicians, Retailers, and Patients. The pharmaceutical companies can better identify and develop new potential drug entities using innovative analytics tools such as Quality by Design (QbD). These drugs are potentially getting approval more quickly than from those developed with traditional methods. Thus analytics adds further value to the developers and consumers.
,
In order to maximize the returns, processes in pharma industry should focus on consistency, quality, efficacy, safety and reducing waste. The tools used to maximize returns include quality analytics, lean six sigma, quality by design, risk assessment and simulations. Next, he defined Capability Indices and the relevance of that metric. A process capability index uses both the process variability and the process specifications to determine whether the process is capable. The capability index is useful for measuring continual improvement using trends over time, or for prioritizing the order in which process will be improved, and for determining whether a process is capable of meeting customer requirements. Quality cannot be tested into products, it should be built in or it should be by design.
,
, delivered a great talk on ""Insight Incorporation: Transformative Value Through Analytics"". Manufacturing is generally under constant pressure to enhance quality and productivity to improve margins and fund investments in other parts of the business. Data analysis in conjunction with methodologies such as Six Sigma and Lean Manufacturing have been leveraged for decades and have achieved step-function improvements. Further strategies involving work placement have also been employed to increase competitiveness through cost reduction.
,
More recently, data, cutting-edge modeling tools, self-service architecture, and data-savvy analysts and managers can apply advanced analytics techniques for insights beyond isolated factory processes. However, investments in IT and talent are often required to bridge data sets to make analysis possible. In an environment that demands constant improvements in quality and productivity, analytics solutions must be quickly available and easily incorporated into business decisions to transform the business. In other words: gone are the days of proof-of-concept analytics; manufacturing leaders must make decisions based on the insights to drive value.
,
We must be deliberate in our quest to ensure insights are incorporated into the business. This is more likely by having a systematic methodology focused on the business - process, problems, people, and systems - utilizing a model that converts analytics-derived insights that are reliable, explainable, deployable and scalable. In conclusion, he mentioned that the path from discovery to implementation should follow the R.E.D.S. approach (which stands for Reliable, Explainable, Deployable, and Scalable). Analytics should deliver trust and data-driven understanding, not black-box magic.
,
,
,
,
,  "
"
,
,
, is Director of Customer Analytics & Insights at eBay. Prior to joining eBay, she was the Chief Data Scientist for WPP, Data Alliance, where she worked across WPP?€?s more than 350 operating companies to create integrated data solutions. Previously, she served as Director of Media Planning at Obama for America, where she was the architect of Obama?€?s advertising strategy.  Gershkoff also co-founded Changing Targets Media, for which she was named one of the nation?€?s ?€?40 under 40?€? leading entrepreneurs.  She has been featured in the Washington Post as one of the nation?€?s most prominent innovators and one of the Top 50 Women to Watch in Tech.  She has been a commentator on NPR, Bloomberg, ABC, and for various print media outlets and holds a Ph.D. from Princeton University.
,
Here is my interview with her:
,
,
,
,I have seven core principles that I use when I design business intelligence tools:
,
, ?? ,
,Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data.  Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions.  On the campaign, I had a great product team as part of my organization, so we built a custom, in-house BI tool.
,
,
,
,  For example, on the Obama campaign, at one point we saw a significant lift in polling numbers from a certain part of a particular state.  We noticed the polling numbers spiked right after we had significantly increased our advertising spend there, so most people assumed the polling lift was caused by the advertising increase. However, I noticed that there were many other parts of that same state where we had also increased our advertising spend and had not seen similar lift in poll numbers.  Upon closer inspection, we could see that the President had visited that part of the state quite recently, where he participated in a number of events including a large and well-publicized rally, and hadn?€?t visited anywhere else in the state.  Thus, it appeared the lift in the polling numbers was caused at least in some significant part by the President?€?s visit rather than the advertising. 
,
,  Many people simply pull two random samples, assume they will be balanced, and run the A/B test.  A much more robust approach is to employ sample balancing checks to ensure the samples have comparable means on key variables that predict the outcome.  This ensures that you can actually attribute any differences in observed outcomes to differences in treatment.
,
,
,
,The most successful analytics professionals combine strong technical acumen with deep business strategy skills.  Unfortunately, such individuals have become increasingly rare, partially as a consequence of how the academy is organized: data scientists and statisticians receive strong technical training but little training on business strategy.  By contrast, MBAs may take a course on ?€?Big Data,?€? but rarely receive deep technical training in data science.  The most successful analysts have strong business strategy skills that they can use as a lens to view technical problems, but these individuals are few and far between. 
,
If you can?€?t find the rare person with both strong business acumen and strong data science skills, one strategy I have used is to pair MBAs and data scientists up to problem solve together.  This has allowed the data scientist to benefit from the business strategist?€?s thinking, while also enabling the strategist to benefit from the technical firepower of the data scientist.  This has proven to be a great way to ensure that cutting-edge technical ideas are infused with business thinking.
,
,
,
,For virtually all companies, the focus should not be on ,: it should be on ,.  Most companies focus on frenetically collecting as much data as possible, including often paying large sums for third party data, when their focus should first and foremost be on making the best possible use of the data they already have. 
,
In my previous role as a consultant, I saw many companies who had failed to connect datasets across departments even within the same company.  For instance, one client had the data about their website?€?s performance siloed in their IT department, the data about their earned social media in their communications department, and their data on their advertising in their marketing department.  Obviously, these three datasets are very much related, and when I helped this client ?€?connect the dots,?€? they were able to mine the website traffic data to uncover significant insights about the performance of their marketing and communications campaigns that could improve both.  Many companies can make significant gains by simply joining internal datasets and mining the resulting data for insights.
,
,
,
,When interviewing data scientists, I look for 5 key characteristics:
,
, ?? ,
,
,
,Lately I?€?ve been re-reading some of the Dale Carnegie leadership books such as Leader to Leader and Leadership Mastery.  I first read these almost a decade ago, but I find that you get something new out of these books at every stage of your career.
,
,
,  "
"
,
,
,??,??,??,??
,
,
,??,??
,
,  "
"
New entries for May.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,  "
"
Most popular 
, tweets for May 30 - Jun 1 were
,
Data Mining Modern Languages 
,
,
#BigData sets available for free - big list from Data Science Central ,
,
Tutorial: Step-by-Step Guide to Setting Up an R - #Hadoop System #rstats ,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 2 and later.
,
See full schedule at , .
,
,  "
"
,
,
, is a Risk Modeling Manager at Paychex, Inc. Under the Risk Management umbrella, Tom helps to coordinate and execute a wide range of projects centered around predictive modeling, optimizing processes in all departments from sales strategy to internal operations and mitigating risk throughout the company.
,
Tom joined Paychex in 2012. Prior to Paychex, Tom was a Predictive Modeling Analyst with a large digital marketing agency, servicing major clients in the financial services, insurance, and automotive industries. He holds a MA from Boston University in Applied Statistics, and a BA from Boston University in Applied Mathematics.
,
Here is my interview with him:
,
,
,
,: For many companies, Paychex included, Risk Analytics has moved beyond a traditional value conservation framework. ,Today, Risk Analytics often encompasses data-driven intelligence that informs a nearly exhaustive list of enterprise-wide decision making, furthering value creation at least as often as value preservation. Expansion of technological capabilities and analytical expertise, along with an evolving complexity of business needs, have augmented the sophistication of the risk models we have produced in the last several years. We explore a wider array of modeling techniques and look for deeper methods for direct integration of model scores in database systems and operational processes.
,
,
,
, Incorporating greater intelligence into considerations of the sales organization positioned the Risk Analytics team to exert a much greater level of influence than had previously been enjoyed. Considering the size of our sales organization, helping to better inform the location, distribution, focus, and expectations of sales reps showed clear and immediate opportunity for revenue maximizations. As this was our first concrete collaboration with the sales organization, we encouraged a very high level of end user participation in the construction and deployment of the Sales Anticipation Model. In that way, the model became a shared vision that garnered much greater support than were it to be created in a silo.
,
,
,
, , Though the challenges presented by the volume, frequency, and format of Big Data are varied, I see a greater obstacle in the technical barriers to implementing a big data framework. It is not a trivial task to capture and process such volumes of data from disparate sources in a connected and accurate manner. However, there fortunately exist many sufficient tools available to the ambitious data scientist to overcome both the technical and non-technical barriers of Big Data.
,
,
,
, The tools and modeling approaches used often vary quite materially from one project to the next. Some projects have required no more than SAS and coffee, while others have pushed us towards adoption of a Hadoop architecture with varied analytical software. In each case, the most useful data mining method has simply been the one to provide the strongest predictive strength. However, those techniques that are more palatable by the non-technical, such as decision trees and clustering, have often provided a good way to spur initial discussions in a big data project group.
,
,
,
, I think Big Data, or more explicitly advanced analytics in general, will continue to play a more and more significant role in everyday business operations and decisions. Companies that fail to employ big data strategies will face a distinct disadvantage in today?€?s world. If the past few years are any indication, I would also expect the technology and methodologies surrounding data analytics to continue to improve and expand.
,
,
,
, It is important to approach Big Data problems from a somewhat academic perspective.,
,
,
,
, I think it is extremely important for Data Scientists to be proficient and knowledgeable in mathematics. Without a theoretical understanding of sophisticated analyses, one runs the risk of incorrectly interpreting results, building models with little longevity, and pursuing non-optimal methodologies. However, my advice would be to study these concepts in a very practical and applied manner, focusing equally on the computer science skills that are so often necessary to conduct and implement learnings from these analyses.
,
,
,
, Despite assessing risk for a living and a moderate fear of heights, I?€?ve been working on a private pilot certificate for the last several months. When on the ground I am typically taking some sort of math or computer science course, tutoring a handful of students throughout the week, and seeking out sunshine and open water whenever possible on the weekends.
,
,
,  "
"
,
,
The modern Human Resources function collects vast quantities of data. Increasingly, this data presents an opportunity to create and utilize a wide range of metrics for recruitment and performance. However, the potential pitfalls for successful use of analytics are apparent. The gap between traditional training and technical competence, the governance of employee & applicant data, and the decision on where to invest has made the area treacherous terrain for organizations starting out with workforce analytics. How do you bridge the knowledge gap between the HR role & analytics? How do you ensure transparency of data usage? How do you avert risk of removing the 'human factor' with over-reliance on your data dashboards?

, (May 22-23, 2014) was organized at Chicago by the Innovation Enterprise. The summit addressed the issues mentioned above and brought together HR leaders who are implementing and experimenting with new analytical methods - driving remarkable improvements in their organizations.
,
We provide here a summary of selected talks along with the key takeaways.

,

Here are highlights from Day 1 (Thursday, May 22, 2014):

,

,delivered a talk on ?€?Creating a Crystal Ball for HR ?€? Building an HR predictive modeling function?€?.?? He explained why and how any organization could create an analytics function within its HR department. The most important global challenge reported by CEOs is Human Capital as per 2013 CEO summary challenge report conducted by the Conference Board. He mentioned that many organizations are trying to capture the accurate report data but they never get to analyzing that data. Organizations should leverage change management techniques to incorporate advanced analytics. They should have a clear plan of direction and goals, especially if the plan requires re-alignment of resources. Any organization will never ever be able to conduct true analysis if they are busy with merely generating reports. Leveraging predictive analytics enables organizations to make better decisions.
,

,talked about ?€?Fuelling your recruiting engine with data: Scrapping data driven discussions?€?.?? Explaining ?€?Data-driven Discussions?€? he mentioned that data builds strong partnerships, metrics become trusted talent advisors and insights add strategic value and help accelerate growth.?? He started with giving some details and statistics about the company. Nick started working with Trulia from Q4 2013 and he shared a timeline of his journey since then as follows:
,
, ?? ,
Towards the end, he shared an interesting insight derived from Trulia data displaying the best neighborhoods for finding love in Chicago i.e. areas where most singles are located.

,

,delivered a talk titled ?€?Dynamic Engagement Analysis: Does Employee Mood Predict Business Outcomes??€?. Naveen emphasized that they tried to observe relation between happiness & economic performance and how one affects another.?? In the pursuit of capturing happiness at Sears, they used a polling methodology asking participants to rate happiness on a scale of 1 to 5.?? Next, he talked about Project MoodRing ?€? which uses happiness as an analytics metric - as an easy way for store associates to provide daily feedback on their work. , This feedback system provides managers a way to monitor how their actions as well as their events are affecting daily energy levels and engagement in store. It also helps improving national processes that could have an impact on store associate engagement. Next, Ian took stage to discuss about analytics. Technology and socio-metric data is changing the game, as there are a number of gadgets to know and track moods. Sears uses ?€?MoodRing?€? to gauge mood on daily basis and receives about 28,000,000 responses over a year.?? Distribution of mood responses in overall and within groups is very helpful to analyze and level up mood.

,

,discussed about what went in building the Workforce Analytics Center of Excellence (CoE), which was recently launched by Johnson Controls. Workforce Analytics is becoming increasingly important to make various decisions such as growth & expansion plans, modification in product mix & go-to-market strategies, and manage scarcities in external talent pools. The Workforce Analytics CoE supports the strategic purpose set forth by their HR transformation.?? They are constantly working to make progress by engaging in more ?€?data driven dialogues?€?.?? She concluded the talk by mentioning data-driven decision-making as striking the right balance by combining art and science. Data and analysis are only half of the solution. Organizations should spend at least the same amount of time understanding the insights and considering the implications of the results.

,

,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
, is the Chief Data Officer for the Center for Urban Science + Progress at New York University.  Prior to joining NYU, Lynn was the Chief Privacy Officer and Privacy General Counsel for JPMorgan Chase from 2004 to 2013 and was the Chief Privacy Officer for Bank One from 2003 to 2004.  From 2001 to 2004, Lynn was General Counsel for Bank One?€?s credit card company, and from 1983 to 2001, she was Head of Litigation for Bank One, First Chicago NBD and First Chicago.  Prior to joining JPMorgan Chase and predecessor entities, Lynn was in private practice and clerked for a federal judge.  Lynn is a lawyer and a Certified Information Privacy Professional and a frequent speaker on privacy topics.
,
Here is my interview with her:
,
,
,
,: In my opinion, the need for data governance is not universally underestimated in big data scenarios.  It depends on the organization.  Organizations that are knowledgeable about information management recognize that as technology allows for the storage and analysis of big data, the same governance disciplines need to be applied to them as traditional approaches to data management.  Sometimes those that are less knowledgeable about information management have some catching up to do. 
,
,
,
, In order to put in place a data governance program for big data, an organization needs to be accountable. ,
,
,
,
, I do not think that cloud computing changes anything; fundamentally the same information management and accountability principles apply in the cloud context.  Information in the hands of a third party needs to be managed with the same standard of care as it is handled in the hands of the provider of the information.
, 
,
,
, Data governance improves data quality.  Data quality relates to the second element of an accountable organization:  policies linked to recognizable outside criteria.  One of the basic tenets of information management is accurate data.  Thus, an accountable organization will take the appropriate steps to improve data quality.
,
,
,
, I cannot speak to what other programs are doing, but here at New York University?€?s Center for Urban Science & Progress (CUSP),  we have established a Master of Science program in Applied Urban Science and Informatics.  The program?€?s goal is to provide students with the ability to use large-scale data, from a variety of sources, to understand and address real-world challenges in the urban context.  The one-year, three-semester program provides core courses in urban science, urban informatics, and information and communication technology in cities, contains a focus on entrepreneurship and innovation, and places students in multidisciplinary environments with city agencies and industry partners to work on projects that address actual, current problems.  Before students gain access to any real data, they take CUSP?€?s Introduction to Data Governance course, which covers the fundamentals of data governance, the concepts of privacy and confidentiality, security and unauthorized access, through a scenario-based format.  
,
,
,  "
"
,
,
The FP7/FET project ICON present the first competition on optimising energy use in data-centres by predicting time variable electricity prices and scheduling tasks in the data-centre to exploit the changes in pricing. The competition will consist of multiple strands, offering chances to participate even if you are interested in only one of the two major topics, scheduling or forecasting.,
,
The competition will open to entries on June 9th, and will end with a workshop at the CP 2014 conference in Lyon (September 8th to 12th, 2014), where results will be presented. All data and formal description is available on the challenge website as well, so participation to either workshop is not mandatory to compete in the challenge.,
,
,
,
,
,
,
,  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
By Gregory Piatetsky,  
,, Jun 4, 2014.
,
Here are 58 upcoming June - October 2014 meetings and conferences.  
,
Most common locations:
,
,??,
You can find the full list on KDnuggets page:
,.
,
,
,
Color code:
,
,??,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
If you want to gain instant access to data that can drive your business and 
,
Lavastorm can help. 
,
Download your 
,
copy of the new version of Lavastorm Analytics Engine Public today.  
Brought to you by Lavastorm Analytics, one of the 
,
you'll be able to conduct data analysis from your desktop with ease -- getting answers faster, through a more intuitive, visual approach.  
,
,
,
and you'll be able to:
,
,??,
,
,  "
"
By Abhinav Shashank, InnovAccer, June 2014.
,
,, founded in the fall of 2012, started with one vision ?€? to make research and analysis simpler. Initially, a team of three working on some large academic research projects, today InnovAccer is a team of 50 data scientists working with hundreds of researchers across the world to help them solve some of the biggest and most faced data challenges.,
,
Their analysis suggests that researchers and analysts spend 70% of their research time to search, clean, and standardize data that can finally be used for analysis. There?€?s a huge amount of information, but no way to be assured and confident of the data in hand. The InnovAccer team safely collects, structures, and connects data to assure the best quality data. They have developed infrastructures, tools, and algorithms to reduce time spent on statistically centered analysis models and approaches for decision making.

,

The data focused team of software scientists, data scientists, statistical analysts, and visualization experts at InnovAccer work with researchers across the world to help them create world class datasets, implement cutting-edge statistical models, and build customized data analytics and visualization tools. Within 15 months of its inception, InnovAccer has more than 100 clients from 50 client organizations in over 10 countries including 17 of top 20 business schools like Harvard, Wharton, Stanford, HEC Paris, INSEAD, IESE business school. As a result, now, researchers spend higher energies to solve more problems and to attempt bigger challenges without facing challenges of access to quality data for research, the barrier of big data research infrastructure, or access to data scientists and technology experts for accelerating the research processes.,
,
As the founders say, ?€?This is just the start. We are on a mission ?€? a mission to work closely with the global research and analysis community and to provide services and solutions that make research faster. There is a long way to go !?€?,
,
Visit InnovAccer at , and read the stories of how they are helping simplify research and analysis at , and have a look at their latest product Datashop at ,.

,
,
,  "
"
,
,
In the past few years we have seen a lot of action around Big Data. As the technology matures to handle such massive amount of data, market attention has gradually moved from the hype surrounding it to focus on deriving maximum business value. The rise of interest in Big Data across industries has been stunning, which can clearly be seen through the numerous vertical-focused Big Data conferences around the world and an increasing number of startups and Big Data projects within firms of all sizes.
,
Despite this immense interest and maturity growth, most companies are still figuring out their business use cases for Big Data,, trying to understand the industry benchmarks so that they can set up their own specific business goals, and prepare themselves for some of the most common challenges faced by Big Data initiatives.  To better understand the business drivers, expected benefits, and challenges of Big Data, , conducted a survey of 540 decision-makers involved in Big Data purchases.
,
Key highlights of Big Data assessment:
,
, ?? ,

Though Big Data offers great opportunities, there are several challenges that need to be overcome first:
,
, ??  ,

The market has clearly evolved from the state of figuring out what to do with Big Data to considering it a top priority. This growing confidence towards Big Data can be contributed partially to the availability of powerful open source solutions such as Hadoop, which is a software framework for storage and processing of large datasets on clusters of commodity hardware. Tools such as Hadoop allow organizations to run Big Data analytics using commodity hardware, thus, lowering the cost significantly.
,
As we have seen in some recent KDnuggets interviews, successful Big Data initiatives are driven by well-planned business use cases. The survey found that the most popular business use cases for Big Data are increasing efficiencies and optimizing operations (improving speed/reducing complexity), improve customer retention, help with product development, and gain a competitive advantage.
,

Assessing the value of Big Data to organizations, the key benefits were recognized as improved and timely access to decision-making information, greater transparency, scalability and better change management.
,
However, since Big Data is still relatively new, significant challenges can be expected during implementation. The most common challenges cited during the survey include upgrading existing databases, making analytics easy for end users, the complexities of software and data integration, and data management (capturing, curation and storage).
,

Big Data is a major strain on most corporate IT infrastructures, with almost half (46%) of respondents saying that their company was already managing greater than 10 TB of data in a typical month. At such a scale, the cost of managing data for meaningful analysis can quickly grow out of control. Furthermore, the data volume is expected to continue growing at a rapid pace. The need for additional infrastructure solutions was evident in the survey results. Companies plan to leverage in-house hardware first and foremost.

,
,
One of the critical success factors for a successful Big Data project is selecting the right tools, which has become increasingly challenging given the broad spectrum of open-source and proprietary solutions. About 44 percent said they were looking to use analytics databases. Following close behind at 41 percent were BI tools and relational databases. And more than a third of the respondents noted that they also use data warehouses, predictive analytics, data visualization tools, and discovery platforms.
,
The most interesting part of the survey came from the participants?€? response regarding which vendors were they working with (or planned to work with) to provide their organization with Big Data analytics. The top answer (with 43% in support) was ?€?none?€?. Only one vendor (IBM at 12 percent) got more than 10 percent of the votes. Thus, the market seems to be still evolving and companies are still not ready to fully invest into any particular Big Data solution.
,
From the survey it seems very clear that Big Data is no more merely a buzzword, it is a transformative technology trend with immense potential and can have real impact when implemented in alignment with the business strategy.
,
For further details on the survey and to download the executive brief, visit: ,
,
,
,  "
"
By Gregory Piatetsky,  
,, Jun 4, 2014.
,
,, as its full name indicates (The Institute for Operations Research and the Management Sciences) is a professional society that was started with a focus on Operations Research (OR).  It is the largest society in OR, and recently it has been actively moving towards analytics and Big Data.  Its major conferences now include Analytics and Big Data in their names:
,
,??,
INFORMS was also the first society, ahead of IEEE  and SIGKDD, to come up with
, exam.
,
I recently discussed INFORMS evolution with 
Gary Bennett, INFORMS Director of Marketing.  
,
,
,
,
INFORMS is the world's largest association of professionals in the field of analytics, operations research, and management science.  We have about 11,000 members in over 100 countries and serve the professional and scientific needs of educators, scientists, students, managers, analysts, and consultants.  About half of our members are academics and half are practitioners in industry or government.  The Institute serves as a focal point for analytics and O.R. professionals, permitting them to communicate with each other and reach out to other professional societies, as well as the varied clientele of the profession's research and practice.  
,
We provide services such as 13 scholarly journals that describe the latest O.R. and analytics methods and applications, industry and  membership magazines with news from across the profession, national and international conferences, special interest groups, a state of the art career center, and professional development services in the form of analytics certification and continuing education courses.  Our sole mission is to advance the practice, research, methods, and applications of analytics, operations research and management science. 
,
,
,
,
INFORMS was founded in the 1950s as the home for operations research and management science professionals.  In the 2000s, when the term analytics came to widespread use, INFORMS made the conscious effort to embrace that term and serve analytics professionals who practice mostly in the descriptive and predictive spaces.  Operations research, on the other hand, operates mostly in the prescriptive analytics space, utilizing such tools as simulation and optimization.  
,
,
,
Our Board of Directors made the bold move in 2010 to development products and services for the analytics community that they would value.  Our wish for them is to make INFORMS their professional association home.  Our first endeavor into analytics was the publishing of Analytics Magazine in 2008 as bi-monthly glossy magazine that covers the field.  Next came our Conference on Business Analytics and Operations Research, which was re-programmed from its original concept to be INFORMS O.R. practice meeting, to now appeal broadly to the analytics community.  This was followed by other products and services the analytics community is embracing such as certification, our conference on big data, and continuing education courses, and the soon to come - Analytics Maturity Model.
,
The chart below shows that growth in INFORMS membership resumed around 2010, 
along with its renewed focus on Analytics. 
,
, 
,
,
,
, stands for Certified Analytics Professional and is a professional certification developed for early- to mid-career professionals.  It was developed by subject matter experts comprised of both INFORMS members and non-members and validated by the wider analytics community.  It was developed for several very good reasons.  We felt like the establishment of this credential would bring visibility and increased credibility to the overall profession in the same way that the PMP?? has done for the project management profession, would allow individuals to set themselves apart from the competition as qualified, serious analytics professionals, and would allow employers another tool to help find good, qualified analytics talent.  It has been quite successful so far. 
,
Sample questions and their answers can be found in our Candidate Handbook and Study Guide.  For more information on CAP?? go to ,.
,
,
,
,
To attain the CAP designation, which you can put after your name like any rigorous credential, you not only have to pass a 100-question exam, but you also must possess the necessary educational achievement and analytics work experience, have your analytical ""soft skills"" validated by a current or former employer or client, and agree to abide by a first of its kind code of ethics.  To maintain your CAP, you must earn professional development units.  So you can see it is a complete program of professional development, not just an exam. 
,
The exam itself covers the analytics end-to-end process, everything from business problems framing to working with data and models, to deployment and model life-cycle management.  Reviews of the exam have been excellent.  The terms that are most often used are rigorous but fair.  It truly means something when you attain the CAP?? .
,
,
,
,
INFORMS is a good ""sister society"" with most of the professional societies in its space including IIE, IIA, POMS, and ASA.  We have cooperative agreements with most, offering discounts on membership, journals, certifications, and continuing ed courses to each society's members.  We are more than happy to make our members aware of products and services that will benefit them no matter that we may not have developed them.   This stance makes a lot of sense since our members tend to belong to one or two other professional societies and don't wish to see them compete ferociously.  In the certification realm, particularly, there are tentative plans under discussion to cooperate with IIA, ASA, CORS and Data Science Central.
,
,
,
,
Our members and Board of Directors have debated this questions quite a bit in recent months.  
,
I have heard our members also call big data ""unstructured"" data, 
and that is becoming a bigger problem for them, especially with the advent of all the data generated by social media, video, and audio.  But, our members feel like they are particularly well suited to deal with big or unstructured data.
,
, 
,
,
I, ,, am the Director of Marketing at INFORMS so I am responsible for letting the world know about all the interesting activities going on within INFORMS, and more generally, in the overall analytics profession.  Even though I have an advanced degree in marketing, I am drawn to analytics because of the very real difference it makes in the bottom lines of organizations large and small and even in the lives of people.  
,
A very strong application area for analytics is healthcare and logistics surrounding emergency preparedness, disaster response, and charitable and humanitarian activities.  We have quite a few members who work in these areas of the public good.  And, of course, it doesn't hurt that the use of analytical data in marketing is very important, too.
,
,
,  "
"
,
,
,??,??
,
,
,  "
"
,
,
The modern Human Resources function collects vast quantities of data. Increasingly, this data presents an opportunity to create and utilize a wide range of metrics for recruitment and performance. However, the potential pitfalls for successful use of analytics are apparent. The gap between traditional training and technical competence, the governance of employee & applicant data, and the decision on where to invest has made the area treacherous terrain for organizations starting out with workforce analytics. How do you bridge the knowledge gap between the HR role & analytics? How do you ensure transparency of data usage? How do you avert risk of removing the 'human factor' with over-reliance on your data dashboards?
,
,(May 22-23, 2014) was organized at Chicago by the Innovation Enterprise. The summit addressed the issues mentioned above and brought together HR leaders who are implementing and experimenting with new analytical methods - driving remarkable improvements in their organizations.
,
We provide here a summary of selected talks along with the key takeaways.

,
,
,
Here are highlights from Day 2 (Friday, May 23, 2014):

,



,discussed journey of HR community at Caterpillar to transform culture around people decisions from ?€?gut feel?€? to informed decision-making.?? Caterpillar has a global workforce of 116,579 full-time employees with $55.7 billion of total sales and revenues. Kelly said that the HR community at Caterpillar is open to go where the data takes them in order to identify high-impact opportunities for analytics. Her team is reinforcing a culture of listening to HR business partners?€? unique people challenges, helping them identify the information they need, engaging them in the right analytics models and then equipping them with the analytical insights to make higher quality business decisions at the leadership table.

,

,delivered a talk titled ?€?HR Shared Services: Measuring the Evolution?€?.?? She shared 2013 statistics about the company: 68,000 employees, 498K HR inquiries with more than half of them by phone call, 60% issues resolved in first call. The key business drivers are improving customer experience and continuing to drive value for business. She discussed the following steps involved in HR Shared Services Value Chain Analysis:
,
,??,

She concluded her talk listing the key HR Shared Services Metrics focused on accelerating improvement. These metrics are divided across four categories: financial savings, customer/stakeholder satisfaction, process management and people.

,

,delivered a talk titled ?€?Know Thyself- A Deeper Look at Workforce Analytics - Helping the Organization Understand Who We Are & Why We Do What We Do?€?. ??His presentation helped audience understand that knowing oneself and creating an identity is very critical for successful analytics in HR.?? He emphasized that the marriage of data and analytics is a requirement for long-term success. He classified HR into traditional and contemporary, with the former focuses on creating efficiencies in basic reporting whereas the latter focuses on advancing the art of qualitative research by spending quality time with data architecture team(s) and investing heavily in story-telling and data visualization.?? He emphasized that data analysis doesn?€?t always give the answer but it does help one know what questions to ask next. They key to good analysis is asking the right questions and asking those questions throughout the analysis process.

,

,talked about ?€?Journey of a workforce planner?€?.?? He started by comparing Workforce Planning to a Tiger, which is very difficult to tame. He emphasized that ever-changing organizations require dynamic Workforce Planning (WFP) involving higher complexity and multiple considerations. WFP approach differs by company and outcome differs vastly. Giving an example of financial services he explained that operational expenses are growing faster than revenue. He presented following potential steps to operationalize WFP enterprise-wide:
,
, ?? ,
,
,  "
"
,
,
,
Big data?€?s true name is the datafication of everything ?€? the capture and use of more data in more daily activities. This is such a fact of life that T-shirts in America?€?s favorite gambling city now say, ?€?What happens in Las Vegas stays on Facebook and YouTube forever.?€?,
,
,

Michael Porter, the world?€?s leading authority on competitive strategy, says strategy is choosing to create a unique value in a unique way.The first part of this definition means that to set your price at a profitable level, you need to offer something your competition can?€?t. But it?€?s the second part ?€? creating that unique value in a unique way ?€? that?€?s the key to sustaining profitability. And here?€?s where big data makes such a big difference.,
,
The datafication of everything enables entirely new ways of creating value. Take Google, for example. One of Google?€?s early insights was that a link between two Web pages is the datafication of a human editorial judgment ?€? ?€?to find out more about this, go here.?€? Analyzing the network of links among pages datafies the wisdom of the crowd about what should point to what.,
,
But that?€?s just a narrow set of activities. And any single thing a company does can be copied. But to mimic a group of interconnected activities, a rival has to copy the whole system.,
,
This means that in a big data world, strategy is a question of capturing and using data across a system of activities in ways your rivals can?€?t copy.,
,
This is why Google continues to datafy more and more activities, capturing data from one and using it in another. Take your interactions with Google?€?s search results. The company collects extensive data on how you interact with the results its engine serves up. This includes which links you click on, as well as which ones you don?€?t. The data exhaust from this interaction becomes input to the way the engine handles the next person?€?s search on the same terms. Google is busily extending its network of interconnected activities across all of its other services including Gmail, Google Now, Maps, and more.,
,
Using the data exhaust of a particular activity to improve its own performance spins up a kind of big data flywheel where each activity contributes another piece of information to an overall collection that?€?s more useful the bigger it gets. But true big data competitive advantage comes from spinning up multiple flywheels that mutually reinforce each other.,
,
So, if you weren?€?t born with the Google spoon in your mouth, what do you do? First, think in terms of data market share. Look for value-creating activities in your industry and focus on the share of those interactions you conduct. If you spot important interactions with customers, partners or suppliers that are still analog, that?€?s a land-grab opportunity. Datafy them before the competition does.,
,
Second, spice up your proprietary data assets with third-party data. For example, you can buy a data feed of tweets about your rivals?€? products. And your rivals can do the same for tweets about your products. But don?€?t think these tit-for-tat actions cancel each other out. Combining that feed with proprietary data about your customers gives you a proprietary view of your competition?€?s performance among your customer base. You now see the broader market in a way your rivals can?€?t, which is crucial because they?€?re probably creating their own different, proprietary view.,
,
,
Finally, conduct experiments to figure out where your new proprietary data assets can improve a key activity and, in the process, create more data to fuel future improvements. Because the datafication of everything has changed more than just Las Vegas. It?€?s changed the basis of competitive advantage ?€? forever.,
,
, is Oracle?€?s Big Data Strategist. Prior to joining Oracle, he was Chief Strategist at Endeca, a discovery analytics company. Before Endeca, Paul was a Principal Analyst at Forrester Research, specializing in search and user experience design. Paul has a BA degree from Wake Forest University.
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, Jun 5, 2014.
,
, is a leading researcher in Scientific Discovery, and now a serial enterpreneur, having co-founded his second company, OnlyBoth, in March 2014. 
,He co-founded Vivisimo in 2000, a search software company that provided enterprise products and web-based consumer services, and was its CEO for nine years and Chairman until its acquisition in 2012 by IBM.  
,
While at Vivisimo he was named a 2007 Ernst & Young Entrepreneur of the Year for the North Central Region and a top ten reader favorite for Entrepreneur of the Year by Inc. magazine. 
Earlier, he was on the CMU computer science faculty, where he received 7 NSF research grants and published 50 journal articles and book chapters. He received a PhD from CMU in 1991, where his advisor was the Nobel Laureate Herbert A. Simon.
,Raul authored the book ""Advice is for Winners: How to Get Advice for Better Decisions in Life and Work"" in late 2012, and is on several company and non-profit boards.
,
I had the pleasure of meeting him at several KDD and machine learning conferences, and we always had very interesting and stimulating discussions.  I recently 
, about his new company OnlyBoth, and our discussions led to this interview.
,
,
,
,:
My co-founder Andre Lessa and I started 
, in March 2014. The OnlyBoth technology discovers new insights in structured data and writes them up in perfect English. 
,
In its application to colleges data, most of the insights are novel to me, even for colleges I know well. A favorite of mine is that Harvard has the highest diversity and inclusion ratio (0.51) of all the 60 colleges that are members of the American Association of Universities, surpassing Berkeley (0.34), Iowa State (0.34), and the rest. When you look at the definition of diversity and inclusion ratio, this is pretty interesting, and surprises me.
,
Other favorites are insights that reveal operating efficiencies, or their lack. 
,
,,
,
,??,
I should reiterate that it's software that's both finding and writing these statements, not us.
,
,
,
,
I did some work on spatio-temporal reasoning at MIT and published a AAAI paper
from it. For my PhD work at CMU, I automated the discovery of reaction pathways from experimental data and background knowledge. After my PhD, I mostly worked on problems of automating scientific inference. Together with chemist, we published a paper that argued, on the basis of results with the program (called MECHEM for Me Chemist), that the number of equally-simple pathway models consistent with the available evidence and background knowledge was larger than chemists suspected. 
,
Sometimes these ideas led to non-science applications, such as a better text-clustering approach that was the basis for founding 
, in 2000. We came at this problem completely from left field, with no prior background in text processing or clustering algorithms, so devised a completely different approach, which worked better.
,
,
,
,
, was the most insightful person by far that I've ever known. You could ask him something about anything and walk away smarter. After he passed away, I contributed an article 
,
to an MIT Press memorial volume. My article tries to share especially valuable insights that I learned from him, both about conducting research and other things.
,
,
,
,
We had a better way to do a task (text clustering) than others and wanted a way to get the technology out there and have an impact. Startups were in the air, because it was during the dot-com boom, or actually just after the bust of April 2000, but the bust didn't discourage us. Also, whereas others start the day with ""good morning"" my visiting scientist and eventual co-founder would say ""when are we going to start a company?"".
,
No great surprises about transitioning to CEO. My time at CMU was very entrepreneurial in many ways, and that transferred over to Vivisimo. Of course, there are many business subjects that one knows nothing about and has to learn, but that wasn't a surprise!
,
I credit Pittsburgh's Jack Roseman with the best definition of an entrepreneur that I've heard: An entrepreneur is someone who sees the whole world as a resource.
,
,
,
,
The initial focus was to productize the university-developed code that we had, get NSF SBIR and state funding, develop public demonstrations of the technology, and identify the initial market to go after.
,
We launched Clusty.com - a clustering web search engine based on meta-search and some of our own web indexing, around 2004, both as further tech demonstrations and later as a money-maker through placement of ads. But these were always sidelines; our main focus was the enterprise market, which we concentrated on more and more. Vivisimo's products and technology are today robust IBM products, under new names, and are also part of the huge Watson initiative.
,
,
,
,
I'm a limited partner, i.e., passive investor, in North Atlantic Capital in Maine, which invested in Vivisimo, in Riverfront Ventures in Pittsburgh, whose principals also backed Vivisimo while at another group, and in Blue Tree Allied Angels. Others evaluate the opportunities; at most, I'll make an individual decision whether to invest. So I'm not an active investor, but if I were, certainly the qualities of the individuals involved would be my #1.
,
,
,
,
I confess that I find many PhD thesis topics now rather dull. Automation of human capabilities is what attracted me to AI. Taking a novel task and trying to automate it, or at least provide a sophisticated computational aid, is rather out of fashion these days. The criticism was always where's the generality in that, but I think that conceptual generality can be found, for example, in design principles that hold over a certain category of tasks, or other qualitative generalizations of greater or lesser scope.
,
,
,
,
One valuable thing I learned from Prof. Simon is that human beings are poor at predicting the future. Who predicted the fall of the Soviet Union?  Nobody. So I don't try.
,
,
,
,
,
,
The phenomenon is certainly new. On one side, you have sensors creating data broadly, cheaply, and profusely that didn't exist before. On the other side, you have people creating text broadly, cheaply, and profusely (within our human limits), that didn't exist before. You can further pile on with technologies such as OnlyBoth that also create lots of text. For example, our initial colleges application creates, depending on what you count, between 5 and 20 million words of well-written, insightful sentences and paragraphs.
,
So the Big Data phenomenon, and the opportunities it creates, are new and genuine. In such cases, you can expect hype and bandwagon effects.
,
,
,
,
My book 
, claims that 
,
,
Otherwise, it's not advice, but principles, or motivational examples, etc. So I don't offer advice for invisible readers. However, I can state that students of those fields who don't also acquire a good understanding of the classical AI concepts of heuristic search in problem spaces will be needlessly handicapped professionally. The writings of Newell and Simon, among others, supply that conceptual understanding.
,
,
,
,
As a result of starting a company without knowing anything about business, combined with the central role of ""domain knowledge"" in my AI education, I came to the conclusion that people often make inferior decisions, needlessly, because they don't proactively reach out for advice from others, or are not skilled at doing so. This observation applied to both work and life. 
,
So I undertook a study of scholarly and other writings that are relevant, reflected on my own entrepreneurial and life experiences, studied the genre of ""self-improvement"" books, and wrote one, that is perhaps of a different style than other self-help books. The book teaches how to leverage the world's knowledge and experience through social (not social media) means.
,
,
,
,
When I?€?m away from a screen, I prefer people contact, e.g., I enjoy social conversation and banter. Also, I can?€?t really concentrate when in front of a screen, so I put it down in order to think hard or clearly about an issue.
,
I most like books that change how I see some aspect of the world. The book ""The Innovative University: Changing the DNA of Higher Education from the Inside Out"" by Clayton Christensen and Henry Eyring changed how I perceive U.S. universities:  How they got to be how they are, and what the future holds for them. U.S. universities emulate the Harvard model of ""more of everything"". Harvard can ultimately afford it, but most can't. This is different than the business mentality of ""let's find an ample niche where we can deliver a product or service efficiently at a good profit"". Their book forecasts some disruptions coming.
,
,
,
,  "
"
Most popular 
, tweets for Jun 4-5 were
,
How does ""Practical Data Science with R"" book stand out ? It concentrates on the process of data science ,
,
According to @IBM, ~90% of global data has been created in the past 2 years alone. Check out IE #BigData Master ,
,
How does ""Practical Data Science with R"" book stand out ? It concentrates on the process of data science ,
,
,  "
"
By Joel Horwitz (Alpine Data Labs), June 2014.,
,
Data Science is often referred to as a combination of developer, statistician, and business analyst.  In more casual terminology, it can be more aptly described as hacking, domain knowledge, and advanced math.  Drew Conway does a good job of describing the competencies in his , post.  Much of the recent attention is focused on the early stages of the process of establishing an analytics sandbox to extract data, format, analyze, and finally create insight (see figure 1 below).,
,
Many of the advanced analytic vendors are focused on this workflow due to the historical context of how business intelligence has been conducted over the past 30 years.  For example, a new comer to the space, Trifacta, recently 
, that is applying predictive analytics to the data to help improve the feature creation step.  Its a very good area to focus considering some 80% of the work is spent here working to un-bias data, find the variables that really matter (signal / noise ratio), and identify the best model (Linear regression, Decision trees, Naive Bayes, etc.) to apply to the data.  Unfortunately, most of the insights created often never make it past what I am calling the ""Data Science Last Mile."",
,
,
,.,
,
What is the Data Science Last Mile? It?€?s the final work that is done to take found insight and deliver in a highly usable format or integrate into a specific application.  There are many examples of this last mile and here are what I consider to be the top examples.,
,
,
,
Thanks to the business intelligence community, we are now accustomed to expect our insights in a dashboard format with charts and graphs piled on top of each other.  Newer visual  analytics tools like Tableau and Platfora add to the graphing melange by making it even easier to plot seemingly unrelated metrics against each other.  Don't get me wrong, there will always be a place for dashboards.,
,
,
,
At Intel, we had daily standup meetings at 7am where we reviewed key metrics and helped drive the priorities each day for the team.  We had separate meetings scheduled on a project basis for analysis that was more complex like bringing up a new process or production tool.  Here the , format is very well defined and there is even an industry standard called SPC or Statistic Process Control.  For every business, there are standard charts for reporting metrics and outside of that there is a well defined methodology for plotting data.,
,
One of my favorite books of all time on the best design practice for displaying data is by Edward Tufte.  My former boss and mentor recommended the book , and it changed my life.  One of my favorite visuals from this book is how the French visualized their train time tables.
,
,
,
,.
,
Presentations, reports, and dashboards is where data goes to die. It was common practice to review these charts on regular basis and apply the recommendation to the business, product, or operations on a quarterly or even annual basis.,
,
,
,
,
Another way data science output is ingested by an organization is as inputs in a model.  From my experience, this is predominantly done using Excel.  Its quite surprising to me that there aren't many other applications that have been built to make this process easier? Perhaps its due to this knowledge being locked away in highly specialized analysts heads?  Whatever the reason, this seems like a primary area for disruption that there is a significant need to standardize this process and de-silo this exercise.  One of my favorite quotes is from a former colleague.  We were working over a long weekend analyzing our new product strategy business models when he stated, ""When they told me I'd be working on models, this is NOT what I had in mind.""  Whether you're in Operations, Finance, Sales, Marketing, Product, Customer Relations, Human Resources, the ability to accurately model your business means you're likely able to predict its success.,
,
,
,
,
Finally and quite possibly my favorite examples are those products built on data science around us without us evening knowing it.  One of the oldest examples I can think of is the weather report. We are given a 7 day forecast in the form of sun, clouds, rain drops, and green screens managed by verbose interpretive dancers.  As opposed to reporting the raw probability data of rain, barometric pressure, wind speeds, temperatures and many other factors that go into this prediction.,
,
Another example is a derived index of credit worthiness (FICO), or the Stock Market index, or Google's Page Rank, or a likeliness to buy value, or a number of other singular values that are used to great effect.  These indexes are not reported wholesale, although you can find them if you try.  (For example, go to , to see Google Page Rank for any website).  Instead, they are packaged into a usable format like Search, Product Recommendations, and many other productized formats that bridge the gap between habit and raw data.  For me, this is the area that I am most focused on as the last mile of work that needs to be done to push data into the every aspect of our decision making process.,
,
How does Big Data fit into this scenario? Big data is about improving accuracy with more data.  It is well known that the best algorithm looses out to the more data inputs you have.  However, conducting sophisticated statistics and analysis on large datasets is not a trivial task.  A number of startups have sprung up in the last couple years to build frameworks around this, but require a significant amount of code skills.  Only a few have provided a much more approachable way of applying data science to big data.,
,
One such company that has built a visual and highly robust way of conducting analytics at scale is ,.  It has a bevy of native statistical models that you can mix and match to product highly sophisticated algorithms that rival the best in class in a matter of minutes, not months.  Pretty wild to think that only a few years ago we were still hand tooling algorithms on a quarterly basis.  Check it out for yourself by going to http://start.alpinenow.com to see how far we?€?ve come.,
,
In summary, it is evident to me that the focus needs to shift back towards the application of data science before we find our self disillusioned.  I for one, am already thinking about how to build new products that start with data as its core value than an add on to be determined later.  There is much more to write about on this subject and would love to hear your thoughts.,
,
Feel free to contact me on twitter @JSHorwitz,
,
,
,  "
"
,
,
, defines and owns BigInfo Labs' vision, strategy & execution. He has over 20 years of experience in exploring new technology, solutions, markets and in incubating new service lines and converting them into profitable practices in large IT corporations
,
As a thought leader in his area Santhosh spoke at various events including Council of Scientific and Industrial Research) India meet on Collaborative Innovation in the area of Drug Discovery, Search Summit at Orlando FL - Microsoft FAST, SAP Summit at Mumbai (Solutions for RETL and CPG).
,
Santhosh has co-authored a book ?€? ?€?Collaborative Computational Technologies for Biomedical Research (Wiley Series on Technologies for the Pharmaceutical Industry)?€? ?€? along with the leaders from CSIR.
,
Here is my interview with him:
,
,
,
,: Often, whenever analytics is discussed the talk eventually turns to insights and the value gained from it. It is assumed that the underlying information being inquired is apt for drawing these inferences. Such an assumption has direct impact on outcomes, accuracy and eventually cost when it comes to Big Data. Data Relevance in this scenario is just that. It is the quality of information being inquired so as to yield the most actionable insights. This is easier said than done.,
,
BigInfo Labs' technology innovations are targeted at precisely this problem.
,
,
,
, Our Intel partnership has multiple objectives really. First of all, they have a great vision and a platform to match it with the Intel Hadoop. The recent changes in regards to the merging of Intel Hadoop with Cloudera further strengthen the platform. Several of our innovations ,in the space of Big Data analysis & processing directly benefit from their contributions to the Hadoop ecosystem. So in a ??way, we can have a two-way conversation about how to make the platform better for enterprise adoption. Secondly, we greatly benefit from the reach and install base of Intel. We?€?d like as many customers as possible to benefit from our solutions. We know several clients who have Big Data initiatives that are in their nascence. They have deployed either Intel / CDH platforms and are in the process of identifying or addressing suitable business problems. Our targeted vertical solutions can easily be deployed in such scenarios. This helps clients to reap maximum benefits from their Intel investments.
,
Hence, our partnership with Intel is very strategic and to use the clich??d term, a 360 degree relationship in reality. It has been very effective so far. We have enjoyed working closely with both their product/platform and business development teams, sharing keen insights into several domain problems and together formulating a strong success message for our enterprise customers.
,
,
,
, That?€?s very true. Infact the answer to that question was very important even before we started on the journey that eventually resulted in the formation of BigInfo Labs. The opportunity was obvious and I won?€?t belabor that point. There are some really cool technology innovations out there for handling gargantuan volumes of information in any and all forms; innovations on data & computationally intensive in-memory processing, innovations for handling connected data, innovations on high speed computing in general like making supercomputing accessible to enterprises, scaling traditional information management technologies and making them big data ready and so-on. There are companies out there who are really doing wonderful work and making an enormous difference to their customers through large scale unstructured data analytics, finding new ways to tap into the social media data for businesses, using publicly available information aided by more open government policies for the general welfare of the masses and so on. So, definitely a very exciting & happening space.
,
What we see across this spectrum is that organizations are no longer seeing Big Data as a new thing and something they have to get into. That point has been made. It is now, like any other IT endeavor. , ,Big Data innovations, including those of BigInfo Labs allow us to break down information barriers within an enterprise like never before to solve genuine business problems spanning different domains. Our innovations synergistically embrace other Big Data innovations but have their own really cool ways ??of making those insight nuggets available to business users for specific business problems. Our innovations allow organizations to leverage their existing information management & analytics investments but answer a new class of questions. Questions which can even be discovered by business users using our solutions. I guess this attention to enterprise business applications differentiates us and our offerings from the excellent companies that are out there.
,
Additionally, from day one we had been investing in our patent pending Big Data Platform. So rather than being a plain old user of a new technology, we have been developing innovative techniques to handle Big Data. We did not hesitate to take a few steps back, go back to the first principles and see if there is a better way to address this problem of abundance. This curiosity and drive to innovate is also a differentiator.
,
I think these are the constructs which will be the most important factor for competition. Sure in the short term there will be a really crowded market place. But we believe that only those with a clear vision on specific business problems and those that have truly ground breaking technology innovations will consolidate & emerge successful in the long term.
,
Second and last part of the interview: , 
,
,
,
  "
"
,
,,  
,, Jun 7, 2014.
,
The 
,
got huge attention from analytics and data mining community and vendors, attracting over 3,000 voters.
,
The poll measures both how widely a data mining tool is used,
and, given increased popularity of KDnuggets, also how strongly the vendors advocate for their tool. Many vendors have asked their users to vote in this poll, but one vendor has created a special page hardcoded to vote only for their software.  In a fair campaign, it is normal to advocate for your candidate, but it not OK to give voters a ballot with only one option.  Voters should be able to consider all the choices.  The invalid votes from this vendor were removed from the poll, leaving 3,285 valid votes used for this analysis.
,
We do have advertising from many of the vendors, but those vendors appear in the top, middle, and bottom parts of the poll, and advertising has absolutely no effect on poll results.  We are more interested in overall trends that are revealed by this poll - see analysis below.
,
The average number of tools used was 3.7, significantly higher than 3.0 in 2013.
,
The separation between commercial and free software continues to shrink.
(Note: since RapidMiner has introduced a commercial version relatively recently, we counted RapidMiner as a free software for the analysis below).
,
This year, 71% of voters used commercial software and 78% used free software.  
About 22% used only commercial software, down from 29% in 2013 (part of the changes was probably due to confusion between votes for RapidMiner commercial and free versions in 2013).
About 28.5% used free-software only, slightly down from 30% in 2013. 
49% used both free and commercial software, up from 41% in 2013.
,
About 17.5% of voters report using Hadoop or other Big data tools, a measurable increase from 14% in 2013  (was 15% in 2012, 3% in 2011).
,
This suggests that Big Data usage is growing slowly, and still is primarily the  domain of a select group of analysts in web giants, government agencies, and very large enterprises. Most data analysis is still done on ""medium"" and small data.  
,
The following word cloud represents the votes for tools.
,
,
,
,
,
,??
Among tools with at least 2% share, the highest increase in 2014 was for 
,
,??,
Revolution Analytics, Salford Systems, and Microsoft SQL server have showed strong increases for 2 years in the row. 
,The growing analytics market was also reflected in more tools (over 70).
,New analytics tools (not counting languages like Perl or SQL)  that received at least 1% share in 2014 were
,
,??,
,
Among tools with at least 2% share, the largest decline in 2014 was for 
,
,??,
Statistica share has now declined for 2 years in a row (was 14% in 2012). With the recent acquisition by Dell, it seems likely that Statistica will continue to lose market and mind share.
,
The following table shows results of the poll, with Tool (User-votes), % alone. 
, is the percent of tool voters used only that tool alone.  For example, just 0.9% of Python users have used only Python, while 35.1% of RapidMiner users indicated they used that tool alone.
,
,
,
Additional tools not in this poll but mentioned in comments were
,
,??,
The following table shows breakdown by region and tool type: commercial/free/both.  
There were only about 12 people who only used Hadoop tools, and they are excluded from analysis below.
,
While the share of analysts who used both free and commercial tools was about 50% in all regions, 
US was the only region where more data miners used only commercial tools than only free tools (2/1 ratio). 
In Europe, Asia, and Latin America, the ratio was reversed, with 2-4 times as many data miners using only free tools vs only commercial tools.
,
,
,
We also examined the use of Hadoop-related tools (including Spark) across regions,
and note that Hadoop usage growth fastest outside US, especially in Asia.
,
,
,
Here is additional ,, including how to download anonymized poll data.
,
,
,
,??,
,
,
 ,  "
"
,
,
,??,??
,??,
,
,
,??,??
,
,  "
"
,
,
,??defines and owns BigInfo Labs' vision, strategy & execution. He has over 20 years of experience in exploring new technology, solutions, markets and in incubating new service lines and converting them into profitable practices in large IT corporations
,
As a thought leader in his area Santhosh spoke at various events including Council of Scientific and Industrial Research) India meet on Collaborative Innovation in the area of Drug Discovery, Search Summit at Orlando FL - Microsoft FAST, SAP Summit at Mumbai (Solutions for RETL and CPG).
,
Santhosh has co-authored a book ?€? ?€?Collaborative Computational Technologies for Biomedical Research (Wiley Series on Technologies for the Pharmaceutical Industry)?€? ?€? along with the leaders from CSIR.
,
,
Here is part 2 of my interview with him:
,
,
,
,: I?€?d like to open offices all across the globe to be honest! Well, at least in the long term. To your question, yes, we will be strengthening our presence in the US very shortly given the traction we are seeing and the reception our solutions have been getting in our client circles. I don?€?t really see there being any limitation being based out of Bangalore. In fact it has been quite the advantage helping us strengthen our offerings during this initial phase of our journey. We?€?ve been able to tap into the very talented resource pool out here.
,
We?€?ve been also able to have meaningful conversations with other organizations who are innovating in this space and others. Such interactions have definitely been stimulating and driving us to really aim high. We?€?ve been able to take our ideas to local enterprises as well and getting excellent feedback. Engaging clients in the US hasn?€?t been difficult either ?€? everyone is now used to working in the global workplace. Besides, my team and I have been able to travel where physical presence was required.
,
But, I am sure we will be moving past that pretty soon. Especially at the pace we are engaging with clients! So an office in the US ?€? yes, very soon.
,
,
,
, Like I said before, I think there is a very good understanding about Big Data among CXOs today. I?€?d even venture to say a lot of business users know it really well. So they?€?ve definitely moved on from ?€?What is Big Data; should I be doing something about it?€? to ?€?What can I do with Big Data?€?. I believe that the reason we see very few companies in the surveys by analysts on whether they are already running Big Data programs is primarily for this reason. We?€?ve talked to many clients who have deployed Big Data infrastructure, but aren?€?t really stretching their imaginations at least with respect to the utility of that infrastructure.
,
,That?€?s the principal concern. Business value isn?€?t evident upfront for a number of these initiatives., It comes back again to having well thought through business apps to that end. I believe organizations like BigInfo Labs, with precisely that kind of focus can allay those concerns.
,
,
,
,Solve problems. Really, from a technology perspective I see acquiring Big Data skills not fundamentally different than any other skill-set. Read a book, enroll into courses, get formal training, get certified, subscribe to online journals to keep abreast of the hype etc. Participating in open source forums can give a good boost. Things are moving at a blazing speed there.
,
Of course for technology roles, a healthy understanding of statistics and a strong math foundation is really important, more so than if you were doing any other type of application development. But it comes back to the discussion we had earlier ?€? there is so much opportunity for putting these technologies to work. Even non-technical roles can gain immensely from thinking about potential applications in their line of work. Big Data technologies need a new class of problems to shine. These can be found all around us. So pick one ?€? it can be something to do with a new application of the IOT (Internet of Things) phenomenon, or simply poring over historical/archive data to discover something new. If you don?€?t have the requisite technology skills, learn them in that context or simply work with a passionate team mate from your technical team!
,
,
,
, Eli Broad?€?s , comes to mind. It is a very well written and succinct book and threw many a pearl of wisdom my way.
,
,
,  "
"
By Sundeep Sanghavi (Co-founder & CEO, DataRPM), June 2014.,
,
There is a phenomenal shift that is happening now in the enterprise data world with ,, which have so far been the foundation for business intelligence and data discovery for several decades, getting obsoleted by the emergence of ,.,
,
The limitation of data warehouses is that they store data from various sources in some specific static structures and categories that dictate the kind of analysis that is possible on that data, at the very point of entry.  While this was sufficient during the early stages of evolution of business intelligence where analysis was primarily done on proprietary databases and the scope was restricted to the canned reports, dashboards with limited and pre-defined interaction paths.,
,
This approach has started to fall apart in the world of big data discovery where it is very difficult to ascertain upfront all the intelligence and insights one would be able to derive from the variety of different sources, including proprietary databases, files, 3rd party tools to social media and web, that keep cropping up on a regular basis. While one may have some initial list of questions that they want answers to during the setup phase but the real questions only emerge when one starts analyzing the data. Ability to navigate from a starting question or data point to different directions, slicing and dicing the data in any ad-hoc way that the train-of-thought of analysis demands is essential for real data discovery.,
,
Example someone may start with a question like ?€?What was the total revenue last year from North America?€? on a , and then may want to slice the result by ?€?North American states?€? and further by the ?€?Demographics of the buyer?€? from the , and then proceed to correlate with the ?€?Ads Campaigns?€?, from the ,, to analyze the effectiveness of marketing spends and then navigate from there to evaluate the impact of efficiency and timelines of their ?€?Delivery Logistics?€? on repeat sales by using the ,. All of these analyses are happening on ad-hoc basis with new data sources added on the fly as per what the user thought process requires for decision-making.,
,
,
,
Therefore the traditional approach of manually curated data warehouses, which provide limited window view of data and are designed to answer only specific questions identified at the design time, doesn?€?t make sense any more for data discovery in today?€?s big data world.,
,
This is where data lakes excel and why the world is now shifting away from data warehouses to data lakes. A data lake is a hub or repository of all data that any organization has access to, where the data is ingested and stored in as close to the raw form as possible without enforcing any restrictive schema.  This provides an unlimited window of view of data for anyone to run ad-hoc queries and perform cross-source navigation and analysis on the fly. Successful data lake implementations respond to queries in real-time and provide users an easy and uniform access interface to the disparate sources of data.,
,
DataRPM offers an integrated data lake and data discovery platform for the modern business. We are to the world of big data discovery for enterprises what Google is to the world of information discovery for the web. We have pioneered a , that delivers the following:,
,
,
,
With DataRPM the machines do all the heavy lifting that the implementation of data lakes and big data discovery platform entail. All that a business needs to is to specify the connection parameters to the different data sources and then start asking questions in natural language to get answers that they can interact with and collaborate in-place with stake holders, anytime and anywhere. DataRPM empowers any user to become a data scientist and leverage the true power of data intelligence in the fastest, easiest, affordable, scalable and most natural way, thereby delivering data democracy in any organization.,
,
,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,  "
"
        ,  "
"
,

,

, is the Vice President of Local Search & Discovery at ,, a Nokia company.?? In this role he is responsible for the development and architecture for Local Search, HERE?€?s social platform and also the company?€?s places discovery apps such as HERE Explore.?? Previously, Don was also responsible for Nokia?€?s Big Data Analytics team that is now part of Microsoft.

,

Before joining Nokia, Don was CEO MetaCarta and was acquired by Nokia.?? He originally joined MetaCarta as an independent director in August of 2005 and stepped into the CEO role in February of 2009. Previously, Don served as VP of Products for Lycos, where he was responsible for revenue and product strategy of the company?€?s U.S. properties, including its search sites, Matchmaker, and Quote.com. He led the properties through positive EBITDA and ultimately a sale to Korean portal Daum. Don is a graduate of Worcester Polytechnic Institute and received the school?€?s prestigious Washburn award in 2002.
,
Here is my interview with him:
,
,
,
, Here are three important themes ?€? maturing into an end-to-end solution, higher-level abstractions and going from batch to real-time.??, Expect to see more tools aimed at addressing data management and collection at scale in addition to data processing.?? Also expect to see more and more tools that make it easy to process large volumes of data by pointing & clicking or typing at a command line.?? This is akin to the evolution of programming languages from assembler to C to Java and Python.???? Finally, expect to see more tools aimed at leveraging large volumes of data for real-time applications like recommendations or fraud detection.
,

All of the above apply to location analytics but for us real-time is very important. ?? We see a growing number of applications that will demand applying analytics to personalize experiences like traffic routing or place recommendation on demand.
,
,
,
, HERE?€?s Analytics architecture has changed completely over the past few years. ?? We have gone from an in-house Hadoop-based system to a hybrid setup that uses AWS. ,?? Scalability and flexibility were the key drivers of the change. ?? In our case, we have many different teams sharing a common data asset. ?? With Amazon, we house the common data asset in S3 buckets and allow teams to independently run analytics jobs in their own EMR clusters. ?? This is much better since it allows teams to scale up their EMR clusters as needed and get more work done.
,
,
,
, In cases like ours where you have a large pool of common data and many different teams operating on it then the ability to virtually separate storage from compute is extremely helpful for the reasons mentioned above. ?? It allows teams to work in parallel on the same data without slowing each other down too much.
,
,
,
, The biggest challenge was scalability and we overcame that by taking advantage of AWS. ?? Amazon has been able to scale AWS to meet our demands.
,
,
,
, I think it?€?s overlooked in many cases or viewed as yet another nagging set of processes. ?? Once companies run up against some of the issues that data governance is meant to avoid (privacy, security, life cycle, consistency, etc) then they will see value in it. ?? Governance gets more important as the size and importance of the data asset grows.
,

,
,
, It?€?s a job that certainly has gotten a lot of attention and hype. ??Data Science is all about answering the right questions. ??To do this well, you have to understand how to work with data and know what questions to ask. ?? The best Data Scientists that I?€?ve worked with have a unique mix of skills in math & statistics, programming and domain knowledge. ?? The first two are sort of the basic tools of the trade. ?? The last one, deep knowledge of a domain (i.e. Business or Biotech), is what sets some people apart. ?? It allows them to understand what questions are best to ask.
,
,
,
, I?€?m a fan of water in both liquid and frozen form. ?? In the summer, you can find me sailing along the coast of Southern New England. ?? In the winter, you can find me skiing in the mountains of Vermont and New Hampshire.
,
,
,  "
"
,

,

, is founder, Chairman, and CTO of ,. For the past 25 years, he has been designing, building, and leading large-scale software projects. Early in his career, Lloyd designed database products for Borland. With the birth of Mosaic, the first web browser, he recognized that the browser would redefine the way databases are accessed and shared. As a founder of Commerce Tools, later acquired by Netscape, Lloyd built on that vision by designing one of the first web application servers. At Netscape, he was the principal architect on Netscape Navigator Gold (which later became Composer) and a lead on Netscape Communicator. Lloyd was also a founder of Mozilla.org, Netscape's open source organization, where he helped acquire and lead their Open Directory Project.
,
Here is my interview with him:
,
,
,
, Looker is focused on empowering data analysts to share their analytical capabilities across an entire organization., We?€?ve created a development environment where they can, at a base level, construct a reusable data model that forms the basis for high-level objects, such as explorable views, dashboards, and filtered dashboards. The model is something like a data mart, but coded in LookML, which is the modeling language built into our platform. LookML is powerful enough to do much of what is traditionally done as part of an ETL process.
,
,
,
, There?€?s a saying that insanity is doing the same thing over and over again and expecting different results. At Looker, we?€?re doing things differently, and we?€?re driving new and better results. By coupling a web-first interface with a modeling layer that enables a simplified?€?but complete?€?end-user view of the data, we?€?ve been able to effect real change in customer organizations.
,
,
,
, , Looker is very easy to learn for a data analyst?€?they can learn it in 30 minutes. Business users can learn to query Looker in even less time. No matter what kind of talent you have in your organization, Looker makes your people better and more effective at their jobs.
,
,
,
, Looker can help anyone in the company. In a web world, customer acquisition is a cost and revenue center; tracking customers to ,their source and then following the lifetime value of those customers are tasks that any ?€?front-line?€? decision-maker can tackle. The analytics team builds the model, but a regular finance person or marketing person can pull their own data, do the analysis, and watch it over time. They can also look for other trends: Do people who use coupons have a lower lifetime value? By how much?
,
Marketing managers can drive micro-focused campaigns that target very specific customers. For example, they can send an email only to people who have ordered more than 10 times but have not ordered within the last 60 days.
,
,
,
, Looker takes a discovery-first, web-first, and model-first approach that operates 100% in database. Rather than creating pretty, pixel-perfect reports and dashboards delivered on a periodic basis, we?€?ve created an environment that fuels our users?€? curiosity. With Looker, they can see all of their data. If they have a question, they can dig deep and get to an answer. They can pursue an interesting thread. What they learn makes them?€?and the business?€?smarter.
,
Looker also takes deep advantage of very powerful databases. We run on Amazon Redshift, HP Vertica, Pivotal Greenplum, and the like. These are huge computational clusters, and our LookML models can see everything in them. The legacy systems you mention have their own data engines that are only looking only at a subset of the data.
,
,
,
, The problem with Daily Active Users is that it doesn?€?t really tell you anything about who your customers are. Growth does matter, ,so you need to measure it. But you need to measure more. If you're collecting event data, you have the opportunity to convert that data into insights about your users. You can start by sessionizing your data, so you can calculate things like time-on-site and understand how to approach measuring engagement. With engagement metrics in hand, you can characterize users and do all sorts of cohort analysis. For example, you can show usage by month pivoted by user signup month. Bucketing users by lifetime usage, you can see if big lifetime spenders spend more or less per transaction over time.
,
When you can measure engagement and characterize your users, you have what you need to build an audience and build a business.
,
,



, , Helpful majors are econ and computer science (and maybe math or even accounting). You need to be competent at computer science and math, but more crucial is economics?€?the study of cause and effect.
,
,

, I like Ben Horowitz's new book ,. I love Seth Godin and most of the things he's written. Behavioral economics is an area of intense curiosity for me. When I'm not working, I'm usually thinking about working. I love my work.
,
,
,  "
"
,
,
Here is the company, startup, and acquisition activity for May 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are the tweets, in reverse chronological order
,
,??,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 9 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Jun 6-8 were
,
Stanford University: Data Analyst ,
,
,
A tutorial on statistical-learning for scientific data processing, with scikit-learn #MachineLearning #Python ,
,
A tutorial on statistical-learning for scientific data processing, with scikit-learn #MachineLearning #Python ,
,
,  "
"
,
,
, (Apr 23-25, 2014) was organized by Global Big Data Conference at Santa Clara Convention Center in Santa Clara, CA. This 3 day extensive, fast paced and vendor agnostic bootcamp provided a comprehensive technical overview of Big Data landscape to the attendees. Big Data Bootcamp was targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, NoSQL & Machine Learning. It brought together data science experts from industry for three days of insightful presentations, hands-on learning and networking. It covered a wide range of topics including Hadoop, Map Reduce, Amazon EC2, Cassandra, YARN, Pig, different use cases and much more. 
,
Despite the great quality of content as well as speakers, it is hard to grasp all the information during the bootcamp itself. , These concise, takeaway-oriented summaries are designed for both ?€? people who attended the bootcamp but would like to re-visit the key sessions for a deeper understanding and people who could not attend the bootcamp. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
Here are highlights from selected talks on day 1 (Wed Apr 23):
,
,, gave an in-depth explanation of Hadoop Ecosystem describing various technologies and platforms. He mentioned that Algorithms (ML or Statistical), Databases (NoSQL, Columnar, In-memory) and Packages & Programming Languages(R, Java, etc.) are equally important. Depicting dissection of ?€?Hadoop Stack?€? into various layers and components, he described the Hadoop stack in detail. The top fields where Hadoop is running in a virtualized infrastructure are Security, Advertising, eCommerce and Customer Experience Management. Machine Learning is being used aggressively in industry to predict user behavior and serve advertisements/recommendation accordingly.
,
, gave a talk introducing Apache HBase and MapR Tables. He started by describing the evolution of technologies from RDMS to NoSQL to column family databases. Meanwhile, he also discussed key differences between these technologies and motivation behind this evolution.  Introducing HBase as distributed column oriented database built on top of HDFS, he explained the HBase Data Model ?€? Row Keys, Columns, Cells. After briefly explaining Data Storage and Cell Versioning, he stated that HBase Table is just a Sorted map of rows. He also presented an in-depth view of HBase Architecture covering all components one by one. ,Quickly covering schema designs, he moved to some interesting use cases. One of them was messaging on Facebook. Being backed by HBase, messaging on Facebook includes communication coming from email, SMS, Facebook Chat and the Inbox with over 2 PB of data in HBase. The key features for which Facebook uses HBase are: Horizontal Scalability, Automatic Failover and simpler Consistency Model. At the end, he asked attendees to get started with HBase using MapR Sandbox describing how easy it is to use. 
,
Here are highlights from selected talks on day 2 (Thurs Apr 24):
,
, started Day 2 with Workshop on Cassandra. He commenced by showing how traditional solutions were no longer a good fit for present scenario. Therefore, Casssandra came up as a massively scalable open source NoSQL database. In Cassandra, all nodes participate in a cluster and any node can be added or removed as needed. Cassandra delivers continuous availability, linear scalability, and operational simplicity across many commodity servers with no single point of failure, along with a powerful dynamic data model designed for maximum flexibility and fast response times. Cassandra also provides built-in and customizable replication, which stores redundant copies of data across nodes that participate in a Cassandra ring. This means that if any node in a cluster goes down, one or more copies of that node?€?s data is available on other machines in the cluster. Replication can be configured to work across one data center, many data centers, or multiple cloud availability zones. 
,
, gave a talk on YARN and Apache Hadoop 2.0 describing migration from 1.0 to 2.0. Presenting Apache Hadoop 2 as next generation Big Data platform, he explained YARN in detail as it is one of the crucial developments in the newer version. Explaining the improvements due to the introduction of YARN, he described the key benefits of YARN including scalability, agility, and improved cluster utilization. Hadoop 2.0 is a multi-purpose platform supporting diverse range of applications - batch, interactive, online, streaming, etc.,Migrating to latest version of Hadoop is a big ROI as the throughput doubles on the same hardware. He then quickly went through steps to help administrators migrate their clusters to Hadoop 2.x.  He also shared details on how users can migrate their applications to Hadoop-2.x. The latest version, Apache Hadoop 2.3 was released on February 24, 2014 introducing a number of alpha features in YARN along with some bug fixes and enhancements. Finally, he mentioned that Apache Hadoop 2.4 would be released very soon and highlighted its new features.
,
, talked about breaking into Big Data. He started by claiming that in 2014 Hadoop has started playing a large role in ?€?Enterprise?€? with lots of tools and features on Enterprise demands under development. He emphasized that mastering Big Data takes great skills, and experience counts a lot. He stated the skill sets required for one to be Data Scientist as: R, python, statistics, math, domain knowledge and big data tools. He suggested audience to start self-learning as there are ample good resources available online. The only challenge in self learning is that one needs to spend a lot of time while doing it alone. Networking and familiarity to open source solutions are very important and significantly increase the chances of finding a good job. 
,
, delivered a workshop on Hive. Introducing Hive as SQL on Hadoop, he explained Hive as a system for managing and querying unstructured data as if it were structured. It uses Map-Reduce for execution and HDFS for storage.  Key building principles involve extensibility, interoperability and performance. With exponentially increasing data and widening acceptance of Hadoop, Hive emerged as a solution providing simplicity and making Map-Reduce easy to program. ,Founded at Facebook, Hive also has tables analogous to those in relational databases. Ashish then explained various functionalities which Hive offers such as Sort By, Custom Mappers/Reducers, Cluster By, Dynamic Partitioning, etc. He also highlighted the current challenges with Hive including batch Processing Model, not great for real-time transaction queries and no support for updates (but partitions help). At the end, he talked about two in-memory solutions: Persto, and Impala.
,
Next part: ,  "
"
,
,Real world classification & regression use cases from eBay text dataset, MNIST handwritten digits and Cancer datasets will present the power of this game changing technology.
,
,
, May 21st, 2014 at 2pm EST
,
,
,
,
,??,
,
,
,
,??,
,
,Wikipedia: Deep learning is a set of algorithms in machine learning that attempt to model high-level abstractions in data by using architectures composed of multiple non-linear transformations.
,
Example: Input data: (facial image) ===> Prediction (person's ID)
,
,
,
,
,  "
"
,
,
,In January, President Obama asked his Counselor John Podesta to lead a 90-day review of big data and privacy.  The working group sought public input and worked over 90 days with academic researchers and privacy advocates, regulators and the technology industry, advertisers and civil rights groups, the international community and the American public. On May 1, 2014, the big data working group released the report summarizing their findings and recommendations. Besides providing a good overview of Big Data across all sectors, the report points out key opportunities as well as concerns in harnessing Big Data.
,
Here is a summary of the key points from the report (for the complete report, refer the link given at the end):
,
The declining cost of collection, storage, and processing of data, combined with new sources of data like sensors, cameras, and geospatial technologies, mean that we live in a world of near-ubiquitous data collection. All this data is being crunched at a speed that is increasingly approaching real-time, meaning that big data algorithms could soon have immediate effects on decisions being made about our lives. The big data revolution presents incredible opportunities in virtually every sector of the economy and every corner of society.
 ,
,
,
This report begins by exploring the changing nature of privacy as computing technology has advanced and big data has come to the forefront.  It proceeds by identifying the sources of these data, the utility of these data ?€? including new data analytics enabled by data mining and data fusion ?€? and the privacy challenges big data poses in a world where technologies for re-identification often outpace privacy-preserving de-identification capabilities, and where it is increasingly hard to identify privacy-sensitive information at the time of its collection.
,
,
,
Big data technologies can derive value from large datasets in ways that were previously impossible?€?indeed, big data can generate insights that researchers didn?€?t even think to seek. Used well, big data analysis can boost economic productivity, drive improved consumer and government services, thwart terrorists, and save lives.
BIG Opportunities:
,
, ?? ,
Big data also presents powerful opportunities in areas as diverse as medical research, agriculture, energy efficiency, global development, education, environmental monitoring, and modeling climate change impacts, among others.
Recognizing that big data technologies are used far beyond the intelligence community, this report has taken a broad view of the issues implicated by big data. Some of the most profound challenges revealed during this review concern how big data analytics may lead to disparate inequitable treatment, particularly of disadvantaged groups, or create such an opaque decision-making environment that individual autonomy is lost in an impenetrable set of algorithms. The report appreciated government?€?s efforts in this direction such as the Open Data Initiatives and My Data Initiatives (eg. Blue Button, Green Button, MyStudentData, etc.). 
,
,
,
, ?? ,
The big data revolution is in its earliest stages. We will be grappling for many years to understand the full sweep of its technologies; the ways it will empower health, education, and the economy; and, crucially, what its implications are for core American values, including privacy, fairness, non-discrimination, and self-determination. To maintain a healthy balance of encouraging innovation while protecting our values, the working group made , in their report to the President:
,
, ?? ,
Besides, the report also recommended that policy attention should focus more on the actual uses of big data and less on its collection and analysis. Additionally, policies and regulation, at all levels of government, should not embed particular technological solutions, but rather should be stated in terms of intended outcomes. 
,
The complete report is available at: ,  "
"
,
,
,??,??
,??,??
,??,??
,  "
"
Most popular 
, tweets for Apr 30 - May 1 were
,
Microsoft: Data Scientist ,
,
Useful! Chart Cheat Sheet: When to use Bar, Stacked, Line, Donut, Choropleth charts ,
,
Useful! Chart Cheat Sheet: When to use Bar, Stacked, Line, Donut, Choropleth charts ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is currently the Principal Data Scientist at Live Nation (a major online platform for concert tickets and artist news), where he is focused on engineering solutions to many of the big-data problems with real-time needs. Over his career he has worked on a variety of learning problems including recommendation, search, click-optimization, clustering, forecasting and pricing. He has two BS degrees: Information & CS and Mathematics, both from UCI, and MS in Information and CS (also from UCI) with a focus on Bioinformatics and Machine Learning. 
,
, Q1. As the Principal Data Scientist at Live Nation, what kind of data problems do you face? What are the inherent challenges in working with data which is bursty and of rapidly evolving nature?
,
,The first project that I worked on once I joined the Data Science team is the classic recommendation system which was a domain-specific proof-of-concept where I applied collaborative filtering (item based) and experimented with personalization at various levels of granularity and subsequently aimed to inform users of relevant findings in real-time through multiple mediums (including but not limited to social media). The other project on which I have spent a significant amount of time is the real-time anomaly detection/resource optimization problem, which is essentially a classification task on streaming data. That problem (i.e. real-time classification) is the first of its kind that I have had to solve. Obviously I have generalized the problem statement in both cases and a good solution to each problem would require some tailoring to the existing infrastructures/products at the organization, even if you choose to traverse the path of least resistance.
, 
Finding the optimal level of cohesion between the existing infrastructure and a new product is always a challenge and a work in progress as the product and organization evolve over time. Another common attribute across all the data problems has been the diversity in data sources where each data source demands feature engineering logic that caters to that one in particular. The complexity occasionally has the potential to trickle all the way down to the bottom of your workflow.
,
To answer your second question, creating a stable infrastructure that can handle arbitrary high-velocity bursts of up to several million requests per minute was a challenge. Now, let's add learning to that challenge. 
,
A static model simply could not keep up with the morphing characteristics of the data. In other words, static models would become stale pretty rapidly. We built hybrid models (spanning data from multiple windows of time) that would demonstrate internal consistency and good cross-validation results but they could not keep up with the data.
,
,
,
, There are two parts to answering this. One enormous challenge as I had mentioned earlier has been building the framework that would plug in to the existing infrastructure so that we could have an isolated environment whose primary purpose is to perform online learning on streaming data. During the prototype phase, we had always suspected that we would eventually be moving to an online learning setting but underestimated the need to do so and thus settled for the ""offline modeling with online classification"" methodology, which obviously made our engineering lives easier. However, once we realized that learning in real-time is crucial for getting consistently reasonable performance, we had to invalidate a lot of the prior assumptions and start re-engineering a lot of the pieces.
,
The other big challenge is the data-modeling component. As you probably realize, building a model offline and simply using it for classification in real-time is vastly different from having to do both in real-time. Hadoop was sufficient for the former but we required something like Storm to be able to move the modeling process into the real-time realm. We had to determine how every aspect of offline-modeling (especially feature engineering and labeling) would translate into the online learning counterpart. We found that to be non-trivial and required several tradeoffs and assumptions. For example, should we have dynamic confidence thresholds since the models are evolving constantly? How do we maintain balance between classes when training? It was also evident that training examples should have different weights but since each request is a potential stimulus, assignment of weights is non-trivial.
,
,
,
, My time at work is split between prototyping solutions to data problems (think: modeling, analytics) and consequently engineering production code to bring the prototype to life. As you can imagine, the latter half of my job has very little to do with statistics and almost everything to do with developing code. In contrast, the former half involves researching and prototyping either new solutions or improvements upon existing solutions. To do this, I once again need to utilize the very same frameworks as I mentioned earlier (but can afford to be a little sloppy with my code). 
,
In addition, I utilize tools like R (for basic data analysis and visualizations) and occasionally other 3rd party libraries that provide specialized implementations of learning algorithms so that I do not have to invest significant resources to re-invent the wheel. Once we determine that a certain technique performs well, we move on to implementing and integrating it into the product. So, if I had to put a number to it, probably 25% of my time at my job would be statistics-related.
,
The answer to your second question is pretty straightforward under my belief that machine learning has very deep roots in statistics. So, it is of extreme importance and great benefit to anyone in the field of machine learning and data analytics to have a thorough understanding of statistics and probability theory. The ability to interpret the math (and understand the assumptions) behind that learning module that I engineered into the product provides a certain level of comfort and confidence. No standard technique works and integrates magically out of the box and so the added statistical knowledge gives me a better understanding, and therefore more options to create a tailored application.
,
,
,
, In short, it was a combination of passion and the right opportunities. The long version of the answer is, like most others, I have always attempted to work on things for which I have a passion for and reasonably good at. I like to believe that I have succeeded in being able to place myself in the right opportunities more times than not. Doing what I am doing now is a consequence of that. My interest in the field is rooted in the projects that I enjoyed the most early in life: building a spam detector using naive bayes, implementing a single hidden-layer neural network that uses back-propagation for character recognition and the application of HMMs to identify regions of interest in genome data are a few among others.
,
I was persistent about what I wanted to do and kept digging deeper into the field of data mining and consequently ended up spending the latter part of my graduate school in the UCI DataLab while working on my thesis at the same time. . I also consider myself lucky to have had the opportunity to learn from and work with people that are leading experts in the field. I was exposed to the endless possibilities of learning from data in various fields, particularly in the field of medicine, which was my focus during my time as a student as I concentrated in the Informatics in Biology and Medicine (IBAM) program at UCI.
,
,
,

,
, First, the importance of domain-specific knowledge for any arbitrary dataset cannot be overstated. Each time I am faced with data from a new domain, the first discussion with the domain experts is always an eye-opener. In other words, every dataset is different.
,
Second, and this has more to do with good engineering, it is vital to start with the data at its most raw form and to understand the goal and needs of the product that you are building. It is crucial to be constantly aware of the end-to-end architecture that will utilize the model you are building. In my experience, there is a vast spectrum where at one extreme we have models that can be stripped from the overarching system with little to no impact and at the other extreme we have models that are extremely tightly integrated. Where does yours fall? There are trade-offs in each case but your product and organization should dictate what is most desirable.
,
Third, particularly if you are out of academia, always start with something (anything!) that works and establish a baseline. Suppress that urge to improve on the AUC until you have a product that delivers on the most basic functional needs. A/B testing in the real world is typically (read: always) a better indicator of performance and easier to interpret when it comes to justifying a ""better model"" (""added revenue"", ""improved UX"", etc). On an important side note, without the proper infrastructure to perform A/B testing, model comparisons are moot.
,
,
,

, In the interest of staying relevant to the topic at hand, I would suggest an article that I came across recently in the Financial Times, titled 
, It is always good to take a step back when you are immersed in your field of work and have a broader perspective on things. The article does a great job of highlighting the pain points of big data to remind us that there is no silver bullet and ""N=All"" (i.e. the sampled population is the entire population) is never (ever?) true regardless of how big your data may be. Therefore, one always has to treat their models with scrutiny. Moreover, it is crucial to determine and be mindful of inherent biases when attempting to generalize observations. The importance of doing so is illustrated in the article through multiple studies from history, each with its own flaws. The article hardly takes a stance. It merely observes with cautionary tales.
  "
"
,
,
,
,A few years ago a brilliant mathematician , solved ??a very famous 100 years old problem in topology called Poincare Conjecture.?? Then, Perelman cut his all contacts with the world and started living lonesome life in his mother?€?s apartment in Russia . He?? rejected Fields Medal, a ""Nobel""-equivalent prize for Mathematics, and also a $1 million award by Clay institute for solving this problem.
,
There was lot of media coverage on this issue and many said that even though it is a brilliant solution, it does not mean anything in the practical world.?? It kept me thinking for years. Here, I present a small article on how Perelman?€?s work can be used to better information diffusion analysis in social networks:
,
,
,
,
,
Perelman?€?s equation gives rise to importance of entropy in diffusion equations through geometric entities. What Perelman did not know at that point was that the similar concept of ,
will be used in social networks 10 years later to calculate information diffusion.
,
,
,
In simple words, the geometric flow defined by Poincare and proved by Perelman, can be measured by entropy using Ricci flow equations (these are non-linear diffusion equations). The social networks which have evolved over time tend to have similar geometric properties and adhere to similar rules and assumptions defined by Perelman. When you take into account degrees of freedom of node of the social network and its association with geometric flow measured by entropy, it pretty much gives you reach of that node and defines the virality of the given post in social network.

,

This measures the gossip flow through social networks (See: 
,, Robbert van Renesse, Dan Dumitriuy, Valient Gough, Chris Thomas, Proceedings of LADIS'08 Workshop) 
,

To expand this discussion, we can say that social networks might be the best tool to validate Perelman?€?s and other topology proofs. Not only that, but we can apply the same transformational formulas used in topology to transform social networks and control the flow of information through given nodes. Some nodes work as catalysts of the information flow through social networks and some nodes behave as dampening agents of the same information which is similar to the properties exhibited by geometrical objects considered for Navier Stokes equation and Perelman?€?s work.

,

Topological behaviour of social networks is the topic less researched because there can be few application of it as of now. But, none the less social networks work as a proxy for all the real life communication and are governed by some of the topological phenomenon we ought to know to improve the existing structures and come up with more catalysts and less dampening agents for the increased flow of information given by the marketing agency using social networks.

,

The following figure represents one of the visualization of Facebook (Please see: ,). This pretty much converges into topological properties of spheres and geometric flows Perelman worked on.
,

, is experienced analytics professional. He worked extensively with clients such as Merck, Sanofi Aventis, Freddie Mac, Fractal Analytics, US Government and NIH on various social media and analytics projects. He has also written books on social media analytics. You can contact him at , or +91-720-811-5292.
,
,
 ,  "
"
New entries for April are below.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to ,
,
,??,
,
Added to , page:
,  "
"
,
,
A Research Opportunity from the Wharton Customer Analytics Initiative
,Wednesday, May 28, 2014
,Noon-1 Eastern Time
,
Whether it is gathering ingredients for a special meal or assembling the tools and supplies needed for a craft project, customers frequently purchase a collection of products that they need to complete a specific project. One might expect that when a consumer is in the midst of such a project, she will be more open to product suggestions that might help shape her plans and achieve her goal in a satisfactory manner. Yet today's marketers have few tools to help identify collections of products that are associated with projects, or customers who seem to be engaged in such an activity. Behavioral customer segmentations are typically static and basket analysis seldom straddles multiple purchase occasions that might be associated with the same project.
,
The Wharton Customer Analytics Initiative is pleased to announce a rich data set from a Fortune 500 Specialty Retailer that will allow researchers to study this problem. The data contain 60,000 customers along with all the individual items each customer purchased over a 24-month window. In addition to purchases, the data set includes detailed product hierarchies and product attributes, store/location information, and email campaigns by the firm to the households. While many researchers could provide basic segmentation strategies, the corporate sponsor seeks novel approaches to segmentation which recognize customer needs change over time, people consume bundles of products for ""projects"", and that this can be identified by the items in each customer's shopping cart.
,
In addition to the primary research question, the sponsor is open to proposals on other avenues of research utilizing this detailed purchase and direct marketing history. These avenues could include: product recommendations, product cross-sell/promotion analysis, geographic purchasing habits, or other analysis.
,
To learn more about the data and business context, interested faculty and doctoral students can attend a live webinar on May 28th at Noon US Eastern time. During the webinar, details of the data will be described and executives from the project sponsor will be available for Q&A. The webinar will also be archived for those who can't attend live.
,
After the webinar, interested researchers can 
,, by June 11th to receive access to the data. Proposals will be evaluated based on their potential for academic contribution and the researcher's ability to address issues of strategic importance to the program sponsor. Register for the webinar here. 
,
,.  "
"
,
,
There are 3 key trends going on in the DBMS market:
,
,

,

, is VP of Marketing & Operations at NuoDB. He has over 29 years of experience in the enterprise software industry, and launched over 20 major software products generating over $1B in total revenue. He has held several executive and senior management positions at many private and public companies. Previously, Michael was Asst. Professor at the Technical University of Vienna, Austria, where he also received MS and Doctorate degrees in Mechanical Engineering.

  "
"
,
,
,The recently launched Alteryx Analytics 9.0 helps yield valuable data insights for organizations and analysts across all industries. ""Through user-centric design and the most scalable analytics platform, Alteryx destroys the barrier for the non-PhDs to make data-driven decisions,"" said George Mathew, President and COO of Alteryx. ""We're delivering a high fidelity experience to blend data and build advanced analytic models without being a programmer. Alteryx is the software instrument of change for all analytical leaders from the back office to the boardroom.""
,
Over the past five years, the amount of customer insight data available to organizations has grown rapidly and with no signs of slowing down. These disparate datasets contain a wealth of information that can be leveraged to drive customer-focused decision. However, very few tools that are usable by analysts have kept pace with the expanding range of data, let alone provided the ability to blend varied data together to perform spatial and predictive analytics tasks. Alteryx uniquely meets these requirements by delivering insight from the major and emerging repositories of customer insight information, including the following new sources added in the 9.0 release:
,
,
, ?? ,
,

Alteryx Analytics 9.0 delivers an easy-to-use and ultra-scalable platform for predictive analytics, as well as a new server solution and a simpler creation of analytic apps that can handle the most sophisticated, enterprise-class requirements. New capabilities include:
,
, ?? ,
Besides, Alteryx also provides read/write capabilities for SAS as well as IBM SPSS files.
,
Alteryx and Qlik, a leader in user-driven Business Intelligence (BI), have created a partnership to put more powerful analytics into the hands of data analysts and business users by integrating their products through leveraging predictive analytics from Alteryx, and then outputting directly into Qlik's searchable discovery platform. This integration is now completed as part of the Alteryx Analytics 9.0 availability.
,
,
,
Alteryx Analytics 9.0 is available for customers to download today. A 14-day trial is available at??,.  "
"
,
,
, leads the Data Science team at Trulia, which is applying machine learning, network science, NLP, and computer vision technologies to the large datasets found in the real estate domain.  He received a liberal arts education at Grinnell College, and studied AI and data visualization at Indiana University.  He previously worked on AI and Visualization at AT&T, Ingenuity Systems, and TheFind.
,
Todd recently delivered a talk at , held in Santa Clara on ?€?Building a Data Science Team?€?. Over the past three years, Todd has built up a data science team to create new products and discover actionable insights from Trulia's big data.  His talk described the decisions made in assembling the team, and what has worked and not worked over these past years. 
,
,
,
,: Let's talk about the similarities first.  Both are composed of super talented people who get excited about many of the same things--data, stats, machine learning, visualization, and so on. We have other teams of data scientists at Trulia as well, including the Geo team (maps) and Economic Research team.  The Data Science team is situated in engineering, and its role is to tackle those projects where the modeling component is interwoven with engineering challenges.  These projects are usually product improvements--recommendations, search relevance, fraud detection, image classification--but also can be predictive models for internal use, such as sales efficiency.       
,
,
,
, We like the approach of having the same one or two people work on a new data science project from end to end.  This means our data scientists are involved in all aspects from conceptualization to experimentation to deployment and maintenance.  This approach gives a sense of ownership, and leads to timely completion with good modeling decisions as they relate to deployment and maintenance.  So our data scientists must be excellent engineers.  However, even in organizations where a data scientist focuses only on experimentation or analysis, strong coding skills may lead to better solutions versus being limited to tools and stats packages.  
,
,
,
, I started studying artificial intelligence (and data visualization) about 12 years ago.  And like many in AI, I found that the skills I picked up along the way--machine learning, complex systems, knowledge representation, and so on--had broad application.  The big thing today is just how mainstream it has all become.  Particularly in the last few years.   
,
,
,
, Presentation skills are a big one.  Some data scientists will pour themselves into their work to great effect, but when it comes time to discuss it, what we get is a truly haphazard power-point.  There are many outlets for data scientists to practice presentation skills.  You can give a recorded talk at a meetup, local conference, or boot-camp, and watch yourself.  Or ask your PR department to record and critique you.
,
,
,
, During the first phone screen, we like to ask non-technical questions that are revealing of a person's passion and tenaciousness.  We'll ask something as simple as ""of all your past projects, what's your favorite?"",  but look for them to absolutely light up when describing it.  We think inspired data scientists build inspired products.
,
,
,
, I hear about it, but I don't see it firsthand.  If anything, I see the opposite--I meet many data scientists unable to break into a quality role.  But being in the tech industry in San Francisco is likely like living in a bubble.  Our local data science meetup has 5500 members (,).  We're probably a bit (or a lot) ahead of the curve here.  Regardless of whether there is currently a shortage across all industries, what is obvious is that there is a huge secular trend towards more data collection and data-driven products, and more people focusing their careers on the skills involved in creating those products. 
,
,
,
, I think so. Having high quality materials online and outlets to practice data science (e.g. Kaggle) is such a new phenomenon that I still don't meet many data scientists who took that road.  I expect that will change.   
,
,

, ""Infinite City: A San Francisco Atlas"".  I spend much of my free time savoring San Francisco, and all the awesome people and culture here.  I feel incredibly fortunate to get to be a part of this city!  "
"
,
,
,??,??,??
,??,??
,
,
,??,??,??
,??,??
,
,  "
"
Most popular 
, tweets for May 2-4:
,
,  "
"
By Gregory Piatetsky, May 5, 2014.
,
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 5 and later.
,
See full schedule at , .
,
,  "
"
        ,  "
"
,
,.
,
Our , on this venue began outlining the business value for solving ?€?the other churn?€? - employee attrition. We introduced the ?€?quantitative scissors?€? with a simple model of employee costs, benefit, and break-even points. The goal was to create a robust mental model for the cost of employee attrition.
,
In this entry, we will extend that model to tease out the factors that underlie attrition cost. With this work we hope to streamline the first step of , By understanding the underlying structure, analysts can systematically attack the problem rather than engage in an open-ended fishing expedition.
,
,
,
The histogram is a useful tool to see how attrition plays out in an organization. It is easy to produce from simple HR records, and the graphic tells a deeper story than simple averages or turnover rates. Most managers are able to understand histograms with some coaching.
,
, shows a basic histogram of tenure. The horizontal ?€?X?€? axis maps out the number of years tenure in a specific role. The vertical ?€?Y?€? axis shows the count of how many employees had that amount of tenure. We can see a stack of early departures in the first 9 months, then another rise later.
,
But this is deceptive. The top ?€?double hump?€? pattern is actually the sum of two simpler employee clusters. ?€?Good Fit?€? individuals in , left the role by being promoted, or being hired away. ?€?Bad Fit?€? individuals in , left the role because of under-performance, problems with hours, or for disliking the work. It is not difficult to classify termination codes into business-oriented clusters, or to use machine learning for data-driven clusters. Of course the real world has more nuance and ambiguity, but the patterns are there to be found.
,
The histogram also mirrors how analytics is used to predict tenure - by modeling the probability of an individual to terminate at a certain duration. It is the same kind of ?€?survival?€? problem as customer churn or medical outcome research. The outcome of an analytics model will be a density curve, ,, much like the histogram, showing the probability of termination for ?€?Good Fit?€? (blue) and ?€?Bad Fit?€? (brown) employees, at each point of tenure. This simplified model uses the Weibull distribution, which is popular in this class of survival analytics.
,
,
,
Next we return to the cost/benefit information in ,, which we calculated in the previous blog entry. Different inputs will shift the shape of the cost and benefit curves, but it is inevitable that employees will have , net cost in the beginning, then face a break-even point, then provide net positive value to the employer. This example is tuned to a short-tenured, fast-training job role, but you can design curves to meet your specific situation.
,
We sum these costs in ,, to make a cumulative net benefit. The plot shows the net cost or benefit accrued by an employee if they get to a specific tenure. The red region shows the net cost until break-even, after which more tenure is pure benefit.
,
Now we have a probability at each tenure point ,, and a cumulative net benefit at each tenure point ,. Borrowing some concepts from finance, we will calculate an Expected Value at each tenure: we simply multiply the probability of reaching that tenure, by the net value of that tenure. Finally, we examine our mix of employee clusters - in this model we posit 60% good-fit employees, 40% bad-fit employees. We multiply each cluster?€?s Expected Value curve by this good-bad ratio, to get ,: an ,.
,
This is a mouthful, but it is very useful to describe the business costs of attrition. As in ,, the blue curve represents how we expect to derive value from ?€?Good Fit?€? employees, and the brown curve shows how we expect to lose value from ?€?Bad Fit?€? employees. The ?€?Bad Fit?€? people all leave before they break even. Some of the ?€?Good Fit?€? also leave before break-even. But most stick around well past break-even. A few ?€?Good Fit?€? folks even make it past the three-year cutoff of these graphs.
,
,
,
The sum of the area under both Expected Cumulative Net Benefit curves give us the overall expectation from hiring in this entire system, from all of the prior assumptions and models. We will call this the ,.
,
The higher the EVH, the better. Below zero means you are losing money with every hire. With these inputs, our model predicts that a ?€?Good Fit?€? employee will deliver an EVH of 48% of their potential benefit. Our ?€?Bad Fit?€? employees are predicted to deliver ?€?17% of their potential benefit - a loss. At our mix of 60% ?€?Good Fit?€? and 40% ?€?Bad Fit?€?, the ,.
,
,
,
Value is measured as a percentage of an employee?€?s fully-ramped-up productivity. This 100% is ultimately divided into ,: salary, EVH, and loss. In the models above, the employee was paid 50% of their productivity, so that 
, is up for grabs by the business to maximize. Our ?€?Good Fit?€? employees yield 48% EVH, is very close to the potential of 50%.
,
In dollar terms, we tend to net , dollars value from an average employee in this role. It is tempting to divide again for , for some kind of efficiency metric, but this is too much abstraction for today.
,
In the real world, when you reduce salary, you will reduce market demand for the role and increase turnover, while saving money in the short run.
,
,
,
Employment, and business in general, is not a laboratory environment. We don?€?t get do-overs for failed scenarios, and our ability to ?€?try things out?€? is limited. Customer analytics is slightly more amenable to A/B testing, just because the relationship is thinner, and there are often many customers.
,
With this model, we are able to play try out different approaches, so that predictive analytics can pursue the right target. We can move sliders to examine modeled outcomes, rather than hiring and firing thousand of workers. Of course ?€?all models are wrong,?€? but we have found this one to be useful. The next blog entry will examine the sensitivity of the output (EVH) variable to our 9 input variables, and lay out data science inquiries into several different hiring situations.
,
Data Science is popularly thought of as an inductive process, and it may seem odd to lay out concepts before collecting data. In practice, the best data science is not an open-ended, free-ranging search for vague patterns. The most powerful data science is directed at a specific business problem, with a clear understanding of the underlying elements of the problem. If we listen, the data will tell us where the our pre-conceptions of those elements are wrong, and we can continue to evolve.
,
That understanding is our goal, so that we can slash the cost of employee attrition, create a happier workforce, and deliver superior business ROI.
,
,
,
We have made this model available in R on GitHub. You can run it in the free and powerful ,, with interactive sliders to change inputs, recalculating EVH and new graphs on the fly. Console-based R (my workhorse) does not support the needed , library. We are working on a web implementation as well.
,
In the spirit of collaboration and learning, we have put this code, over 500 lines of R, up on GitHub so that other researchers can download, experiment, and engage. If you don?€?t like our Weibull distributions, you can swap in a Log-Logit or whatever you want. If you want to create a U-shaped cost curve, go ahead. You can share your progress back to us with a ?€?pull request?€?, or ?€?fork?€? your own variant. If you find a bug, create an ?€?issue.?€? Keep us posted. GitHub can be an important resource for collaboration in quantitative research - we encourage practitioners to dig into it.
,
You can find it instructions and code at ,. We will continue to build up this model as an engine for this series. Please engage!  "
"
By Gregory Piatetsky, May 5, 2014.
,
Here are the upcoming May - September 2014 meetings and conferences, with the most meetings in Silicon Valley, Chicago, London, Boston, and Toronto. 
,
You can find the full list on KDnuggets page:
,.
,
Color code:
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,
,??,
,
,
,  "
"
,
,
Here are , and ,.  This is the third and final part of the interview.
,
, has served as Chief Scientist of a Fortune 20 company, an Advisory Board member of leading national and international research organizations, and an invited speaker and lecturer. In his role as Chief Scientist Dr. Brodie has researched and analyzed challenges and opportunities in advanced technology, architecture, and methodologies for Information Technology strategies. He has guided advanced deployments of emergent technologies at industrial scale, most recently Cloud Computing and Big Data. In his Advisory Board roles Dr. Brodie addresses current and emergent strategic challenges and opportunities that are central to the charter and success of the organizations. As an invited speaker Dr. Brodie has presented compelling visions, challenges, and strategies for our emerging Digital Universe in over 100 keynote speeches in over 30 countries and in over 100 books and articles.
,
,
,
, My current research concerns the scientific and philosophical underpinnings of Big Data and data Science. With Big Data we are undergoing a fundamental shift in thinking and in computing. Big Data is a marvelous tool to investigate What ?€? correlations or patterns that suggest that things might have or will occur. 
,
,
,
A pernicious aspect of What are the biases that we bring to it. On a personal note, my biased recall of 1989 was how marvelous your ideas were and the amazing potential of data mining. I accept your view that I was skeptical and not as enthusiastic as I recall. You see I modified reality to fit my desire to be on the winning side, which I was not then. Hence, what we think that we think may bear little resemblance to reality or, more precisely other people?€?s reality. As Richard Feynman said, 
,
,
,
That said, I see the main successes of this trend as a nascent trajectory along the lines of Big Data, Data Analytics, Business Intelligence, Data Science, and whatever the current trendy term is. The World of What is phenomenal ?€? machines proposing potential correlations that are beyond our ability to identify. Humans consider seven plus or minus 2 variables at a time, a rather simple model, while models, such as Machine learning, can consider millions or billions of variables at a time. Yet 95% (or even 99.99999%) of the resulting correlations may be meaningless. For example, ~99% of credit card transactions are legitimate with less than 1% that are fraudulent, yet the 1% can kill the profits of a bank. So precision and outlier cases, called anomalies in science can matter. So it pays to search for apparently anomalous behavior ?€? as it is happening!
,
We have already seen massive benefits of Big Data in the stock market, electoral predictions, marketing success, and many more that underlie the Big Data explosion. , The failures concern limited models of phenomena and the human tendency of bias. People can and do use What (Big Data, etc.) to support their biases and limited models, e.g., used to support the claim of the absence of climate change or lack of human impact on climate change, rather than letting the data speak to suggest directions and models that we may never have thought of. As it has always been, it takes courage to change from a discrete world of top-down models [I know how this works!] to an ambiguous, probabilistic world [What possible ways does this work?].
,
Those are natural successes and limitations of an emerging field. The direction, opportunities, and changes are profound. I experience a mix of fear and tingles thinking of asking the data to speak. Hoping that I can be open to what it says and distinguishing s..t from Shinola. 
,
,
,
,
,
,: As an undergraduate at the University of Toronto, I was extremely fortunate to have had Kelly Gotlieb, the Father of Computing in Canada, as a mentor. I was a student in his 1971 course, Computers and Society, later to become the first book on the topic. Kelly and the issues, including privacy, have resonated with me throughout my career. Kelly observed that privacy, like many other cultural norms, varies over time. So yes, Privacy will fluctuate from Alan Westin?€?s notion of determining how your personal information is communicated to the Facebook-esk ""Get over it"". 
,
,While personal privacy is undergoing significant change, disclosure of information assets that are part of the digital economy or of government or corporate strategy may have very significant impacts on our economy and democracy. Hence, this raises issues of security, protection, and cultural and social issues too complex to be treated here. 
,
However, there are a number of very smart people looking at various aspects. The quote you cite is from Craig Mundy (Privacy Pragmatism: Focus on Data Use, Not Data Collection, Foreign Affairs, March/April 2014) who explores that changes Big Data brings debating the balancing of economic versus privacy issues.
,
Very smart folks, like Butler Lampson and Mike Stonebraker, are commenting on practical solutions to this age-old problem. Their arguments are along the following lines. Due to the massive scale of Big Data, and what I call Computing reality, previously top-down solutions for security, such as anticipating and preventing security breaches, will simply not scale to Big Data. They must be augmented with new approaches including bottom-up solutions such as Stonebraker?€?s logging to detect and stem previously unanticipated security breaches and Weitzner?€?s accountable systems. 
,
To beat the Heartbleed bug and others like it, ?€?Organizations need to be able to detect attackers and issues well after they have made it through their gates, find them, and stop them before damage can occur,?€? Gazit, a leading cyber security expert said recently. ?€?The only way to achieve such a laser-precision level of detection is through the use of hyper-dimensional big data ,, deploying it as part of the very core of the defense mechanisms.?€?
,
,
,
Hence, Big Data requires a shift from a focus on top-down methods of controlling data generation and collection to a focus on data usage. Not only do top-down methods not scale, Tightly restricting data collection and retention could rob society of a hugely valuable resource [Craig Mundy, see above). Adequate let alone complete solutions will take years to develop. 
,
,
,
, The Big Picture is called Computing Reality in which we model the world from whatever reasonable perspectives emerge from the data and are appropriate, e.g., have veracity, and make decisions symbiotically with machines and people collaborating to optimize resources while achieving measures of veracity for each result. 
,
One subspace of this world is what we currently know with high levels of confidence, the type of information that we store in  relational databases. Another encompassing space is what we know but forgot or don?€?t want to remember (unknown knowns) and a third is what we speculate but do not know (known unknowns), these are all the hypotheses that we make but do not know in science, business, and life.
,
The rest of the data space ?€? the unknown unknowns - is infinite, otherwise learning would be at an end. That is the space of discovery. 
,
, This is practically interesting because very little of our world is discrete, bounded, finite, or involves a single version of truth, yet that is the world of most computing. With Computing Reality we hope to be far more pragmatic and realistic. This is technically and theoretically interesting because we have almost no mathematical or computing models in these areas. Those that exist are just emerging or massively complex. How cool is that? You see what old retired guys get to do?

?€?
,

,
,
, Free time ?€? what a concept! My yoga teacher, Lynne recommended that I should try to do nothing one day, and I will. I will. Soon. Really. Life is such a blast; it?€?s hard to keep still. 
,
My activities include the gym (4 times a week); hiking/climbing 
,
,
,
~75 mountains USA, Nepal, Greece, Italy, France, Switzerland, and even Australia;
42 of the 48 4,000 footers in NH (most with Mike Stonebraker); 
cooking (daily and special occasions with my son Justin, an amazing chef and brewer, when he?€?s not doing his PhD), travel, and my garden; 
all of these ?€? except the gym and garden - with family and close friends.
,
Books: 
,
Very cool Big Data Books
,
????????,, by ????????Nate Silver, Penguin Press
,
????????, ????????by Viktor Mayer-Schonberger,  Kenneth Cukier,  Houghton Mifflin Harcourt
,
Real books
,
????????Ken Follett?€?s , (Fall of Giants, Winter ????????of the World and Edge of Eternity)
,
????????Henning Mankell?€?s ,
,
,
,
,This was my first visit to Qatar that was remarkable culturally an intellectually. Culturally I saw spectacular result of hydrocarbon wealth and vision, e.g., amazing architecture emerging from the dessert. Intellectually I saw the beginnings of Qatar?€?s National Vision 2030 to transform Qatar?€?s economy from hydrocarbon-based to knowledge-based. 
,
One step in this direction by the Qatar Foundation was to create the 
,. In less than three years QCRI has established the beginnings of a world-class computer science research group seeded with world-class researchers in strategically important areas such as Social Computing, Data Analysis, Cyber Security, and Arabic Language Technologies (e.g., Machine Learning and Translation) amongst others. Each group already has multiple publications over several years in the leading conferences in their areas, e.g., SIGMOD and VLDB for Data Analysis. I spent my time reviewing what I consider to be some of the most challenging issues in Big Data.
  "
"
,
,
Mobile phone usage provides a wealth of information, which can be used to better understand the demographic structure of a population, and to fill gaps respect to basic questions: e.g. what are the differences in mobile phone usage between genders, or different age groups? 
At ,, we have a research team specialized in studying Human Dynamics. In this case, we focused on the population of Mexican mobile phone users.
,
,
,
Our first approach was to explore the data in order to gain insights. We performed (to our knowledge) the first extensive study of social interactions in the country of Mexico focusing on gender and age, based on mobile phone usage. The ability to analyze the communications between tens of millions of people allowed us to make strong inferences and detect subtle properties of the social network.
,
Regarding gender, we made some interesting observations: 
(i) a gender homophily in the communication network (i.e. men tend to talk more with men, and women with women); 
(ii) an asymmetry between genders (men talk more when they make outgoing calls, and women talk more when receiving incoming calls), possibly reflecting a difference of roles in Mexican society. 
,
It would be interesting to see how these differences change in other regions of the world like Europe or the United States.
,
We also compared communication habits for different age groups, and found statistically significant differences. We observed a strong age homophily in the social network (see below the communications matrix according to the users' age).
,
The clearly marked diagonal shows that users have a strong tendency to communicate with interlocutors of their same age. This preference can also be seen in the next figure, which shows the number of links according to the age difference between users. The number of links decreases with the age difference, except around the value d = 21, where an interesting inflection point can be observed (possibly relating to different generations, i.e. parents and children).
,
,
,
,
,
Based on these results, we set to work on developing a novel methodology to predict demographic features (namely age and gender) by leveraging individual calling patterns, as well as the structure of the communication graph.
,
As a first approach, we used a set of standard Machine Learning tools based on node features. However, these techniques cannot harness the topological information of the network, and exploit the correlations between the users' communications.
,
To leverage this information, we developed a purely graph based algorithm inspired in a reaction-diffusion process, and showed that with this methodology we could predict the age category for a significant set of nodes in the network. Finally, we combined both Machine Learning techniques and the reaction-diffusion algorithm. Our experiments showed that the combined method increases our predictive power on a real-world dataset with millions of users.
,
Finally, our new method allows us to predict demographic features such as age and gender with high precision. This has in turn numerous applications, from market research and segmentation to the possibility of targeted campaigns (such as health campaigns for women).

,
(For more details, see
,, by Nicolas Ponieman, Alejo Salles, Carlos Sarraute, 2013.
,
,, PhD is Director of Research at Grandata, a company that integrates first-party and telco partner data to understand key market trends, predict customer behavior, and deliver impressive business results.


  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
Latest ,, (May 07, 2014) ,:
,
,??,
Also
, (5) |
, (1) |
, (3) |
, (4) |
, (2) |
, (6) |
, (1) |
, (3) |
, (9) |
,
,
Big Data's weakness is that it says nothing about Why - causation or why a phenomenon occurred or will occur. Michael Brodie in ,.  "
"
Most popular 
, tweets for May 5-6
were
,
xkcd looks at Love and Statistics: Why it is important to label your axes #humor #cartoon ,
,
Landmark for Artificial Intelligence #AI: MIT/UW researchers computer system solves Algebra word problems ,
,
xkcd looks at Love and Statistics: Why it is important to label your axes #humor #cartoon ,
,
,
,  "
"
        ,  "
"
,
,
, is an applied researcher and software engineer of machine learning and pattern recognition, focusing on social data at Microsoft. Xinghua has published over 25 scientific papers in top-tier machine learning and scientific computing venues such as NIPS, ICML, CVPR, Bioinformatics, IEEE, MIT Press, Springer, and has won the Best Paper Award in MICCAI Machine Learning in Medical Imaging. Xinghua received his Ph.D. from University of Heidelberg (Heidelberg, Germany) and his M.Eng and B.Eng from Tsinghua University (Beijing, China). He has also worked for IBM Research (Beijing, China) and Memorial Sloan-Kettering Cancer Center (New York, USA).
,
Xinghua recently delivered a talk at , held in Santa Clara on ?€?Mining Cancer Clinical using Topic Modelling?€?.
,
Here is my interview with him:
,
,
,
,: We started this project as a simple empirical analysis to evaluate the performance of topic models in understanding clinical text notes. Later the project evolved into more predictive analysis based on the output of the topic model. At the end, we learned that topic modeling of clinical notes is quite helpful for finding special community of patients, predicting important attributes in the clinical database such as the icd-9 code, as well as discovering correlations between patient profile and genetic mutation tests.
,
,
,
,: Among various techniques for understanding text corpus, we chose LDA topic models (implemented in GraphLab) because of its previous success in understanding scientific literature as well as webpages. We followed a process roughly as follows: data cleaning and standardization, topic modeling, clinical note clustering and visualization, community finding and cancer-gene correlation analysis. This process was mainly implemented by , under my supervision. We had a few interesting findings, such as a community of patients who highly care about the risk of the treatment, the ability of predicting icd-9 code from topic modeling output, and some interesting correlations between patient profile and genetic mutation tests (some supported by previous published research).
,
,
,
,: One technical challenge we encountered was  data cleaning and standardization, which has to be done specifically for the clinical notes on hand. The major challenge was actually data collecting. As mentioned in my presentation, biomedical data is very expensive to expand. ,, professor of machine learning and computational biology in Sloan-Kettering/Cornell and leader of our project, spent much more time finding the appropriate data for this project than actually completing the analysis.
,
,
,
,: We already have powerful tools and skills to answer quantitative questions, but the difficult part sometimes is finding the right questions. That's when dimensionality reduction and visualization comes into play. They provide an easily accessible overview of all our data, which helps us to build intuitions and formulate hypotheses. Once done, follow-up validations are mostly easy and straightforward.
,
,
,
,: For applications, we are looking into using this work for clinical decision support system such as predicting/validating icd-9 code as well as experimental design for cancer-gene correlation finding. As per future research, ,, co-author of the project, will continue this research towards an exciting direction of modeling the temporal dynamics of clinical notes.
,
,
,
,: There have been some exciting developments. For example, Memorial Sloan-Kettering Cancer Center is incorporating IBM's Watson for cancer treatment analysis and recommendation. And, there are similar stories with other medical institutes such as Kaiser Permanente and Mayo Clinic. I believe Big Data for healthcare will keep growing, which will provides tons of opportunities for hospitals as well as data analytics solution providers.
,
,
,
,: Just read and practice, especially practice, because you will not truly understand ""curse of dimensionality"" until your data makes you complain about it!
,
,
,
,: Nate Silver's , was quite interesting.  "
"
,
,   
,
,
The semiconductor industry is becoming increasingly competitive and forcing manufacturers to achieve significant reductions in time to market. As a result, every step in the manufacturing process needs to be completed in less time while maintaining a high level of control and quality. This must be accomplished based on an increasing number of requirements to satisfy customer demands, particularly for products aimed at the automotive industry and the medical sector.
,
,
,
,
,
,
,??,
,
Engineers must have access to high-performance tools and methods that make it possible to get to the root of the problem as quickly and reliably as possible. This white paper focuses on the bootstrap forest method available in JMP, Pro for root-cause analysis.
,
,  "
"
,
,
,
,""How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did"" was an explosive
headline in a Forbes
, by Kashmir Hill (Feb 2012) which caused a huge media storm.
The Forbes article only reported on an article by Charles Duhigg in the New York Times with a quieter title
,.  Duhigg's article reported on work by presented in 2010  by a Target statistician at PAW - the Predictive Analytics World Conference. That presentation had a very technical title (""How Target Gets the Most out of Its Guest Data to Improve Marketing ROI"") and did not cause any controversy.  
,
Good lesson here on the importance of headlines.
,
,
,
,I had a chance to discuss this recently with Eric Siegel, who pointed out that very likely this was NOT the case.
Eric is the founder of the conference series Predictive Analytics World (,), which is the first and leading cross-vendor event that covers commercial deployment.
His work with PAW gave him special insight into the real scoop of the Target story and which parts of it are ""real"".
,
Here's an excerpt from Chapter 2 of his book  ""Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die"" (,).  That chapter focuses on privacy and civil liberties concerns that arise in predicting, e.g., pregnancy, job quitting, and crime recidivism.
,

,
,
Beyond this, as Eric pointed out to me, the New York Times article itself provides another factoid making it even less likely the teen's pregnancy had been determined analytically (if ""determined"" by Target at all - perhaps the particular teen was simply placed accidentally into the wrong marketing segment): Target knows consumers might not like to be marketed on baby-related products if they had not volunteered their pregnancy, and so actively camouflages such activities by interspersing such product placements among other non-baby-related products. Such marketing material would by design not raise any particular attention of the teen's father.
,
Target story was discussed recently in Financial Times 
,, 
where Kaiser Fung offered another explanation:
,
,
You can read the rest of the chapter in Eric's book for more insight on this story, and on the privacy and civil liberty concerns brought up by predictive analytics in general.
Read also the other chapters - as I wrote in my endorsement for
,:
,
,
,
 ,  "
"
By Gregory Piatetsky, May 7, 2014.
,
Recently, 
,
announced a new 
,
version of its RapidMiner Server solution. 
This will be a professionally-deployed and fully-managed service, hosted on Amazon Web Services.
A team of RapidMiner experts will perform all installs, configurations, backups, maintenance, monitoring, tuning and updates. 
,
I caught up with RapidMiner team to ask them about the new service:
,
,
,
,
Our Platform-as-a-Service product consists of an instance of RapidMiner Server installed on AWS and managed by RapidMiner on behalf of the customer. In that regard, RapidMiner Server PaaS offers the same features of on-premise RapidMiner Server -- such as the ability to schedule the execution of resource intensive processes, share models, create interactive dashboards and seamlessly integrate predictive analytics with any other business application via APIs.   
,
,
,
,
The RapidMiner Server Platform-as-a-Service solution is available in Professional and Professional Plus Server editions only.  Customers get a hosted, always-on, professionally managed version of RapidMiner Server. RapidMiner Server PaaS is provisioned, installed, managed, maintained and updated by RapidMiner experts. 
,
(GP: RapidMiner website 
,
tailored to academic and research community.)
,
,
,
,
RapidMiner supports a wide range of activities. RapidMiner has one of the richest set of models and offers data transformation, model building, a unified environment for analyzing structured and unstructured data, multiple ways of deploying the models, including modern web APIs as well as interactive dashboards and visualizations. In addition, RapidMiner Server PaaS supports collaboration via sharing of models, processes and more. With the PaaS offering, we are adding benefits such as rapid provisioning, managed hosting and a fast connection to AWS databases such as Relational Database Services (RDS).  
,
,
,
,
The pricing for RapidMiner PaaS starts with RapidMiner Server and adds market-leading AWS hosting and fees for the expert management of the Server.  Models are built in RapidMiner Studio. The Server product is used for execution of processes, sharing of models and processes, scheduling and interactive dashboards. 
,
,: current RapidMiner Server 
, is
,
,??,
,
,
,
Individuals and organizations who want to benefit from RapidMiner Server but don't have the internal IT resources to quickly procure, provision and maintain enterprise applications. Also, anyone who already has data sources on AWS such as RDS will benefit from rapid data transfer rates. Amazon RDS supports MySQL, Oracle, Microsoft SQL Server and PostgreSQL database engines. 
,
Learn more about the new 
RapidMiner Platform-as-a-Service (PaaS) at
,  "
"
Most popular 
, tweets for May 7-8 were
,
30 Simple Tools for Data and Geo-Visualization: iCharts, Fusion, Modest Maps, Raw, Leaflet, Google Charts, more ,
,
Dilbert on giving 110% to everything you do, charitable giving and math ,
,
Google Economist Hal Varian paper ""#BigData: New Tricks for Econometrics"" - free access ,
,
,  "
"
        ,  "
"
,, 
,""Build Trusted Data with Data Quality,"" written by analyst Michele Goetz compliments of Lavastorm Analytics. See how organizations are keeping tabs on data conditions to build confidence and trust in the data.
,
Key takeaways of 
,
,
,??,
,
,
,
,
Lavastorm transforms how companies tackle data challenges to achieve breakthrough business results. The company's agile data management and analytic software gives analysts new powers to harness data in an era of exploding data complexity, to discover new insights and to continuously improve business results. Analytics built in Lavastorm provide analysts with exceptional self-sufficiency, agility, transparency, accuracy, and business control.   
,
For more information, please visit: 
,  "
"
,
,
Here is the company, startup, and acquisition activity for April 2014 from 
,.
See the latest under hashtag
,.
Notably, there is a growing number of Indian companies in this list.
,
,
,
,??,
See also
,

  "
"
By Gregory Piatetsky, Apr 1, 2014.
,
Here are upcoming April - July 2014 meetings and conferences.  
,
You can find the full list on KDnuggets page:
,.
,
,
,Word Cloud for April-July 2014 Meetings.
,
Color code:
,
,??,
,
,
,??,
,
,??,
,
,
,
,??,
,
,  "
"
,
31 March 2014 - London, UK. SAS, the global leader in , software and services, has launched its first data scientist competition in the UK & Ireland. Open to the academic and business communities, the contest aims to find the country's best candidate who can demonstrate a breadth of skills in the use of analytics, innovation and data to deliver better insight and make a valuable contribution to their community.
,
The competition is open to undergraduates, postgraduates and business professionals. Entrants will be able to access a defined set of open data sources, available for analysis within the SAS Academic Cloud.
,
The challenge is to produce an innovative forecast of energy demand, using open data from the UK government to forecast the number of terawatt hours of electricity likely to be consumed in the UK by 2020. The judges are looking for written submissions and a video presentation; the more interesting and more visual the better. The winner will be announced as the SAS UK & Ireland Data Scientist of the Year. They will receive a unique 3D model of their results, a trip to a SAS Conference in the USA and an opportunity to present at the UK SAS Professionals Conference in 2015.
,
The Rt Hon Liam Fox, former cabinet minister, and a member of the Top Data Scientist judging panel commented: ""Understanding data trends will provide the UK with unrivaled opportunities to address some of the most critical issues facing our society, whether that's the economy, supporting an ageing population or how to tackle the energy crisis.
,
""The UK is currently faced with a dearth of critical data skills and we need more individuals with the skills to drive this change and create new opportunities. This competition is important not only to identify key talent within data science but also to force the debate around vital areas like the future of energy,"" concluded Fox.
,
,.
,
The competition is open to entries from 31 March until 7 October, 2014. For more details, please visit ,  "
"
,
By Gregory Piatetsky, May 8, 2014.
,
,
Last week I attended 
, (Massachusetts Technology Leadership Council)
, You Have the Data, Now What?
,
The summit was held in the beautiful Microsoft NERD building on Memorial Drive in Cambridge, MA, but because of the construction, I had to go around several times before finally managing to sneak into the parking lot. However, data scientists must be either good with discovery of parking, or working in Cambridge, because the room was quite full by 8:30 am.
,
For those who could not attend, 3 excellent MassTLC #BigData2014 reports are available for download online at , , covering 
,
,??,
Here are my notes and selected tweets from the meeting with hashtag
,.  Joe Johnson also actively 
, from this summit.
,
Paul Sonderegger,
,,
Oracle's Big Data Strategist, opened his keynote by asking: 
,
,
Here are my 
, tweets of his presentation:
,
,??,
,
This very lively presentation was followed by a panel discussion about data science,
moderated by 
Chris Baker, ,
with  
Paul Sonderegger
Joe Hendrickson, VP ,,
Ingo Mierswa, CEO ,,
and 
Pete Martin, VP of Engineering, ,
,
RapidMiner CEO Ingo Mierswa talked about Data Science vs Statistics.
He said that the biggest difference is that Data Science needs to be aware of computing infrastructure, be aware of business needs, and effectively present results to the management.
Although ""Data Science"" is currently a marketing term, it is useful for capturing resumes of the right people.
,
Joe Hendrinkson talked about team selection and suggested to start with business analysts, then add data engineers.
,
Pete Martin noted 
the more mature organization is, more distinct group Data Science becomes
,
Joe Hendrickson said that the data can be a weapon inside a company.
,
Ingo Mierswa noted: we need to communicate not just results, but also the process. Even 51% accuracy is very valuable.
,
There was a discussion of#BigData backlash. Are we making a big mistake by trusting in Big Data?
,
The panel discussed a recent article in the Financial Times: 
,
,
which presented 4 ""straw-men"" articles of faith of Big Data, such as
,
,??,
The panel pointed that these 4 items are not true about Big Data and quoted a famous statistician George Box: 
,
,
Ingo Mierswa said that BigData does not give Big wins, but many small decisions, which can result in great value. 
,
The next event was fast vendor pitches from Paradigm4, Quant5, and Prelert which showed their very impressive systems. 
,
Paradigm4 showed a quadrant with Complex vs Simple Analytics on Y axis, and
Small views/Data vs Big Views/Data on X-axis, and said that their product SciDB was
well-positioned for the upper corner.
,
,
Next, Paul Barth from NewVantage Partners
,
 and 
Boston CIO Justin Holmes ,  talked about Privacy and Governance.
Paul Barth noted: 
,
,??,
Boston CIO ,: 
,
,??,
Fire hydrants are a good example - although they are considered critical infrastructure,
they are very visible, so common sense says the database of fire hydrant can be open.
,
You can explore Boston data sets at 
,
,
Paul Barth suggested 3 Data Levels: bronze (raw), silver (initial cleanup), gold (good quality, auditable)
Bronze (raw) can be loaded quickly, Bronze to Silver can take a week, 
Silver to Gold quality can take a month.
,
Paul Barth recommend the work of MIT Alex (Sandy) Pentland with EU about information rights  - information buyers and sellers do not have equal rights.
,
Paul noted that insurance companies disclose what info they collect.  
They usually collect acceleration/deceleration, but not speed or GPS location, to avoid privacy issues.
,
I asked this panel about opt-in approach to privacy: 
Big Data can make uncomfortably accurate predictions - can there be market-based, opt-in solutions?
,
Justin Holmes replied that we are trying opt-in with Boston School Bus, but also need to educate people more about data.  We have unionized environment in Boston, and have GPS on police cruisers.  Need to address union concerns about how data is used.
,
He also pointed that minority rights should be protected.
,
A predictive model cannot decide not to lend in a particular low-income area - this is called red-lining and is illegal.
,
The Summit ended with 
, panel, with Chris Selland (HP Vertica) - moderator,
Richard Dale (Optum Labs) @rdale, 
Steve Dodson (@prelert), 
Robert Nagle (@InterSystems).
,
Panel highlights:
,
,??,
The final question for the panel was ""What will be the next disruptive innovation?""
,
Some predictions: 
,
,??,
Overall, an excellent meeting and big thanks to 
, and 
,
for organizing it.
,
,
 ,  "
"
,
,
Monograph authored by
,Gennady Andrienko, Natalia Andrienko, Peter Bak, Daniel Keim, and Stefan Wrobel
,
Springer, 2013, XIV, 397 p. 200 illus., 178 illus. in color.
, ,
,
Details: ,
,
,
,
PROBLEM
,
, ?? ,
OUR CONTRIBUTION
,
, ?? ,
OUR POTENTIAL ""CUSTOMERS""
,  "
"
Most popular 
, tweets for Mar 31 - Apr 1 were
,
Cerner: Data Scientist ,
,
5 reasons to use R #rstats: free, crazy popularity, awesome power, dazzling flexibility, and mind-blowing support ,
,
,
Experfy, a Harvard-backed startup, launches a marketplace for #DataScience projects, wants top #analytics talent ,
,
,
,  "
"
,
,
,
Rob Reul, founder and Managing Director of Isometric Solutions, has decades of experience answering this question through customer intelligence research. He will share his knowledge on collecting and analyzing consumer data so you can respond appropriately and remain competitive with a loyal ?€? and expanding ?€? customer base.
,
,
,
,
,
,??,
,
On-demand webcasts featuring additional thought leaders are also available.
,
,  "
"
,
,
, is a Principal Staff Engineer at LinkedIn, where he is leading the development of their next-generation search infrastructure. Before that, he led Facebook?€?s search quality and ranking efforts for Graph Search. He previously worked at Google on search quality and ads infrastructure and held senior technical roles at VMware, WebGain, and Sun. He was a key contributor to Unicorn, the index powering Facebook?€?s Graph Search, and developed JavaCC, the leading parser generator for Java. He is a graduate of the Indian Institute of Technology in Kanpur.
,Here is my interview with him:
,
,
,: LinkedIn and Facebook are both social networks ?€? they both contain members with an identity, and contain many other entities useful to these members ?€? companies, jobs, etc. in LinkedIn and pages, photos, etc. on Facebook.  The members and other entities have relationships with each other resulting from actions by members ?€? connections, likes, mentions, etc.  Additional relationships can be inferred through data analysis.  This is the basis of LinkedIn?€?s economic graph, or Facebook?€?s social graph.
,Where they differ is that ,.  Something unique about LinkedIn?€?s data is that it is mostly public which allows for a richer member experience and more valuable insights.
,
Google?€?s knowledge graph on the other hand is based on information gathered from a variety of sources and then enhanced through additional inferences.
,
,
,
,: Search within data that is structured and connected in inherently entity oriented ?€? for example, the nodes in the LinkedIn economic graph and the edges between them represent entities.
,
,
,
,
, Search at LinkedIn is one piece of a fuller product offering ?€? the LinkedIn experience.  To do a good job with search at LinkedIn requires this realization and working with the rest of the teams at LinkedIn closely to deliver a better overall LinkedIn experience.  On the other hand, the goal of search engines like Google, Bing, etc. are to get the searcher to a good results page outside their domain as quickly as possible.
,
What this means is that the approach to search relevance, search personalization, and finally the definition of search success (e.g., via metrics) needs to be very different.
,
In comparison with other social networks, search at LinkedIn is a much more fundamental piece of the LinkedIn experience.  And making search better can go a long way in making the overall LinkedIn experience better.
,
,
,
,: While these numbers do feel large, the real challenges at LinkedIn come from the diversity of the different kinds of searches performed and the sophisticated nature of some of the searches.  Recruiters hit our system hard with very complex queries and are looking for lots of results.  Our infrastructure challenges come from having to deal with a variety of these complex queries.
,
,
,
,: At the end of the day, we want our team to have a strong intuition for what our end users are looking for and a similarly strong intuition for what it takes to build such features into our search engine.  And the willingness to take calculated risks.  Obviously we expect a good data science background ?€? familiarity with Hadoop, machine learning, etc.
,
,
,
,: It has been a while since I read a serious book ?€? there is so little time left after everything else.  Furthermore, there is a lot of very interesting and diverse reading material online that compensate.  Most of my free time is spent with my family and friends.  And a little bit of exercise ?€? that includes a weekly non-negotiable game of racquetball!
  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for April 3 and later.
,
See full schedule at , .
,
,  "
"
,
Latest ,, (Apr 03, 2014) ,:
,
,??,
Also
, (4) |
, (5) |
, (3) |
, (1) |
, (4) |
, (4) |
, (4) |
, (9) |
,
,
""Actual work in Data Science entails: some opportunities to innovate from a greenfield state, but not often; mostly being called into an existing project - which is somehow at risk; having to speak truth to power (not fun, but the essence of the role)"", Paco Nathan, ,.  "
"
By Vincent Granville, DSC, Apr 1, 2014.
,
I'd like to get statisticians more involved in data science, and here's an opportunity to participate, earn money, and make statistical science more visible.
,
,
,Data Science Central (the leading community for analytic practitioners with 3 million visitors per year and more than 200,000 members) organizes a competition to write a research paper about a statistical technique called jackknife regression (a robust regression technique).
,
In connect with our proposed methodology, the goal is to create a black-box, automated, easy-to-interpret, sample-based, robust technique called jackknife regression, to be used in small and big data environments by non-statisticians.
,
The details are found at 
,
 (on datasciencecentral.com). 
It involves working on simulated data. The results, to be published on our network and possibly in a scientific journal, will reach far more practitioners than any article published in a statistical journal. The award is $1,000.
,
On a different note, if you want to get your students interested in experimental design (or criticize flaws in my 'data science' version), 
you feel free to share my interactive, real-time experiment. 
You can find it, participate and check results in real-time at 
,  (also on datasciencecentral.com). 

  "
"
,
,.
,
San Diego State University and True Bearing Analytics
,
As very a rough, and a brief background, machine learning (ML) grew out of several not-necessarily disjoint mathematical subjects, notable among these are mathematical statistics, computing & algorithm, information theory and mathematical optimization.  
,
The chronology goes something like this. Mathematical statistics was taught since early part of the 20th century by R.A. Fisher and K. Pearson, however, from the 
,, J. Neyman?€?s thesis (Warsaw University, 1924) entitled (translated from Polish) ?€?Justification of Applications of Calculus of Probability to the Solution of Certain Questions of Agricultural Experimentation?€? seems to be the first in ?€?pure?€? statistics. 
,
Information theory was first taught by Robert Fano at MIT in 1950, with first PhD on the subject awarded to David Huffman in 1953 on his thesis entitled ?€?The Synthesis of Sequential Switching Circuits.?€? (Claude Shannon?€?s 1940 doctoral thesis ?€?An algebra for theoretical genetics?€? doesn?€?t strike me as information theory.)  Alan Turing deserves to be named the founder of the field of artificial intelligence (AI) in the late 1930s and throughout the 1940s, but arguably it was Marvin Minsky who was the first PhD in ML, awarded in 1954 on his thesis ""Neural Nets and the Brain Model Problem,"" this even before AI was an acknowledged discipline.  Shortly afterwards (1959), Minsky with John McCarthy went to form the MIT AI lab. 
,
In those ancient times machine learning was bundled with AI, so circa 1970, as George Luger (Prof Emeritus, University of New Mexico) told me, he?€?d first learned of the perceptron learning algorithm when it was taught in a graduate AI class at the University of Pennsylvania.  Although opinions vary, from what I hear, machine learning didn?€?t make an appearance as distinct academic subject that was taught as a dedicated course until sometime the late-1970s or early-1980s, but perhaps someone can correct me and supply details.
,
However you look at this, ML is not an entirely new thing, but because of recent technological advances and corresponding business appetite for data analysis, interest in the subject in the academe has exploded to the point that everybody seems, or want to be playing the game.  I don?€?t have exact figures, but today, scores of universities around the world offer doctoral degrees in machine learning, mostly in computer science departments, and the number of those offering a regular academic classes in ML is likely in the hundreds. This happened in tandem with demand in the marketplace; Figure 1 demonstrates job trends.

,
,Figure 1: Jobs in ML
,
From Figure 1, of all jobs posted on the Indeed.com job board, in June 2013 over 0.04 percent were in ML + Python, 0.025 percent were in ML + , and about 0.015 percent were in ML + SAS.
,
Other terms which describe more-or-less the same thing are data mining (DM), predictive/advanced analytics (PA/AA); the term data mining is a bit out of vogue, partly because of its occasional pejorative use to describe ?€?data snooping?€?. However, it?€?s safe to regard ML, DM, PA and AA as more-or-less synonymous. Figure 2 is taken from Google Book and illustrates trends of book titles

,
,Figure 2: Trends of book titles. From Google books
,
My personal journey hasn?€?t been entirely unique. Although I?€?ve taught statistics and probability theory for some years early in my academic career, save a handful of classical methods like regression, permutations and rank tests, no ML concepts had made an explicit appearance in my classes. To see how times have changed, back then statisticians preferred to use the term ?€?method?€? over ?€?algorithm, hence the phrase ?€?Fisher?€?s Method of Scoring,?€? rather than ?€?Fisher?€?s Scoring Algorithm,?€? was (still is) common.
,
I left academia in 1995 to pursue other, more ?€?practical?€? interests. In fact, I must confessed that I learned ML much later in life, and this only after the chatter around me had grown to a fortissimo.  Admittedly, I?€?m self-taught; although I had substantial coursework in probability and statistical theories, my doctoral thesis was in combinatorics.  But conditioned to a life of autodidact, (mainly because of my shortcomings as a college student,) I wasn?€?t completely unprepared for the experience of self-teaching.  However, having been trained in the pedantic mathematical manners, I wasn?€?t quite prepared for the pursuit of discipline with a heuristics core.  (I suppose classically-trained musicians encounter a similar difficulty when trying their hands in Jazz.) 
,
As strange as it may seem, my main struggle was a dearth of definition-theorem-proof paradigm in the various texts I?€?ve perused.  I simply didn?€?t know how to untangle the proverbial knot, and lacking a teacher dampened my learning rate.  But after several topsy-turvy years things have begun to coalesce.  In retrospect, I?€?d say that subject?€?s complexity stems from its dimensionality, and although I?€?m sure some (or most) will disagree with the particulars, I?€?d venture to put a rough estimate on the dimensionality of ML at 9, or to put it mathematically, dim???(ML)= Math + 3, with Math = 6. 
,
What I mean is that most topics in ML lie in the convex hull of (the theories of) probability, combinatorics, convexity & optimization, statistics, information and computing. To this list I would add the three extra dimensions: heuristics, empirics and applications. So it?€?s not at all surprising that this was hard stuff to learn: I had to simultaneously comprehend the theoretical underpinning, and accept discipline?€?s philosophy, as well as learn to compute (to program & understand output,) all this while working on practical problems (mostly those assigned to me by a customer or employer.)  The lucky ones, i.e., doctoral students in statistics or computer science learn ML during a course of several years (four or five) of coursework along with substantial amount lab work and supervised research, but I wasn?€?t among those lucky ones.  
,
Fast forward at least a decade, and here I am, an adjunct professor of statistics at San Diego State Uni-versity (San Diego is in California, USA, on the coast, just north of the Mexican town of Tijuana.)  I was hired by the university to beef up the statistics program, primarily to teach courses in machine learning, and after nearly two decades of industry work, this has been a most rewarding experience. In fact, I was fortunate to have given the opportunity to create a graduate one-semester machine learning class and this is the second year I?€?m teaching it.
,
San Diego State University (SDSU) has a respectable statistics program where we offer master and doctorate degrees in computational and applied statistics. It?€?s worth mentioning that SDSU?€?s computational statistics program is one of the few in the country.  Since the California State University charter does not allow for the awarding of doctoral degrees, SDSU had partnered with the Claremont graduate school to offer a joint doctoral program.
,
In support of this program, we offer a graduate-level class in ML, with student-pool from computational sciences, computational linguistics, and computer science.  Class prerequisite are kept fairly minimal; those consist of the standard upper division undergraduate coursework in probability, statistics and linear algebra, but we don?€?t require coursework in more advanced subjects like measure-theoretic probability, optimization & combinatorics, although we do assume that students have programming experience, especially in ,.
,
The challenge is to make the lectures intuitive and compelling, with theoretical details filled in only when necessary to enhance comprehension.  The syllabus is fairly standard for a one-semester course and include the philosophical foundations, ML models, algorithms &  their computational complexity, bias-variance tradeoff, model?€?s generalization, as well as the foundational framework. 
,
The latter includes hypotheses spaces, concept learning and their limitations. I demonstrate some of those ideas using perceptron-learning, the Vapnik-Chevronenkis (VC) dimension of linear perceptron, etc. To make a point (I suppose,) we go through the details of calculating the VC-dimension of linear perceptron in R,.
,
, (L = the space of linear hypotheses.) 
We do mention the probably-approximately-correct (PAC) framework, but because of time constraint, we don?€?t go into great details, certainly omit proofs.  We cover regression (OLS, and logistic), neural networks, support-vector machines with kernels, boosting and various enhancements to regression (PCA, PLS, regularization, etc.,) and unsupervised learning (clustering.) 
,
We currently don?€?t have a dedicated lab (or lab assistant), so I encourage the students to form study groups (of two or three) to help each other with assignments and comprehension. All assignments are hands-on data analysis with the , system.  Data sets are taken from various sources including the UCI data repository, but occasionally students will work on data from another research project.
,

, is the primary tool, and because , is an , system, it is not suitable for analysis of ?€?large?€? data sets, therefore it doesn?€?t take long for students to encounter a ?€?capacity problem?€? with ,.  So about half way through the class, I assign a problem which involves analyzing a large & high-dimensional data-set, and at that point they have no choice but to use more powerful computer. 
,

This introduces them to running , in batch on SDSU?€?s Unix/Linux cluster, (in our case a ,.)  Not surprisingly, the students are comfortable working on their personal machines (a 50-50 Windows/Apple split), but cajoling is necessary to make the leap to get accustomed working on a server. The transitioning from working locally on one?€?s PC or Mac, to working on the server requires a bit of tutoring and so, we briefly discuss things like command line, text editors, directories & I/O management and all that is necessary to working in a server environment.
,
One of our main objectives is to get students into the habit & discipline of writing professional reports, one containing a data dictionary, data quality summaries, exploratory data analysis (EDA) (including uni-variate & bivariate analysis, graphics, etc.)
,
I encourage them to add as much detail as necessary including how data is partitioned into training and test sets, (details about) cross validation, diagnostics, lift-charts, goodness-of-fit, etc.  I emphasize that we must spare no effort to ascertain that a model generalizes: I find myself repeating the remark (a clich??, of course):

,
,

At the end of the course, students will possess a portfolio of five or six reports on which a grade is based.  My goal is for students to gain solid familiarity with, and good working knowledge of a handful of ML algorithms, that they?€?ve become unafraid of heuristics, that they carefully test a model because they learned to be skeptical about results of training, that they?€?re curious to further investigate this vast subject, and that they are well-equipped to tackle related problems.
,

I?€?m well aware that top schools offer more by way of better resources (labs, TAs, equipment, and often big-name researchers.) We don?€?t presume to belong in the prestigious club of top universities, and for obvious reasons we can?€?t compete with them. But we nevertheless educate data scientists capable to perform at the highest professional levels. We also know that we?€?re doing reasonably good job in producing curious minds, if not research-minded students.
,

A case in point, in 2013, partially as a result of taking the class, four of our students have published papers in a respectable refereed journal, and more is certainly to come.   
,
Apropos, there?€?s a somewhat apocryphal story about George David Birkhoff who once was overheard telling a well-known Syracuse university math professor (something like) ?€?But sir, you?€?re no Harvard!?€?  True, we?€?re no Harvard, but The Aztecs made it to the ?€?sweet sixteen?€? round in the 2014 March madness(*), and yes, we do a decent job preparing our students for careers in data science.
,
(*) For our non-American readers, ?€?March Madness?€? is a term coined by the media for the immensely popular college basketball tournament which is held during the month of March, ending with the crowning of the NCAA national champion.,

,Dr. Joseph Barr is an Adjunct Professor of Statistics at San Diego State University. He?€?s currently working on unstructured data (NLP) and on various other problems resulting from his consulting business.
,
,
 ,  "
"
,
,
,??,??,??
,
,  "
"
,

,

The??summer??school??will be held in Madrid, between??June 23th and July 4th. This year's programme comprises 12 courses divided into 2 weeks. Attendees may register in each course independently.
,
Early registration is now *OPEN*. Extended information on course programmes, price, venue, accommodation and transport is available at the??school's website:
,
,

,

,

,

,
,
,
,
Course 1: Bayesian Networks (15 h),
Basics of Bayesian networks. Inference in Bayesian networks. Learning Bayesian networks from data. Real applications.
,
Course 2: Time Series(15 h),
Basic concepts in time series. Descriptive methods for time series. Linear models for time series. Extensions.
,
,
,
Course 3: Supervised Pattern Recognition (15 h),
Introduction. Assessing the performance of supervised classification algorithms. Preprocessing. Classification techniques. Combining multiple classifiers. Comparing supervised classification algorithms.
,
Course 4: Bayesian Inference (15 h),
Introduction: Bayesian basics. Conjugate models. MCMC and other simulation methods. Regression and Hierarchical models. Model selection.
,
,
,
Course 5: Neural Networks and Deep Learning (15 h),
Introduction. Training algorithms. Learning and Optimization. MLPs in practice. Deep Networks.
,
Course 6: Feature Subset Selection (15 h),
Introduction. Filter approaches. Wrapper methods. Embedded methods. Advanced topics. Practical session.
,
,
,
,
,
Course 7: Statistical Inference(15 h),
Introduction. Some basic statistical test. Multiple testing. Introduction to bootstrap methods. Introduction to Robust Statistics.
,
Course 8: Bayesian Classifiers (15 h),
Discrete predictors. Gaussian Bayesian networks-based classifiers. Other Bayesian classifiers. Bayesian classifiers for: positive and unlabeled data, semi-supervised learning, data streams, temporal data.
,
,
,
Course 9: Text Mining (15 h),
Introduction. Fundamentals. Language Modeling. Text Classification. Information Extraction.
,
Course 10: Unsupervised Pattern Recognition (15 h),
Introduction to clustering. Data exploration and preparation. Prototype-based clustering. Density-based clustering. Graph-based clustering. Cluster evaluation. Miscellanea. Conclusions and final advise.
,
,
,
Course 11: Support Vector Machines and Convex Optimization (15 h),
Introduction. SVM models. SVM learning algorithms. Convex non differentiable optimization.
,
Course 12: Hidden Markov Models (15 h),
Introduction. Discrete Hidden Markov Models. Basic algorithms for Hidden Markov Models. Semicontinuous Hidden Markov Models. Continuous Hidden Markov Models. Unit selection and clustering. Speaker and Environment Adaptation for HMMs. Other applications of HMMs.  "
"
,
,
,
Most business decisions are driven by past experiences, emotions and intuition. But what if the analytics say something completely different? How do you strike the correct balance between data-driven insights and gut feeling to arrive at the right decisions more often?

,

This is what many business leaders are grappling with as data continues to rise up the corporate agenda. To find out from the front line what companies should focus on in 2014, Data Driven Business spoke to 9 analytics practitioners.
,
Their responses were revealing and indicated 3 main areas of focus this year:,
,
, ?? ,
In addition to these key themes, other trends identified included a growth in the use of prescriptive analytics, mainstream adoption of machine learning and an increase in the use of visualization software and dashboards for more effective data story-telling and decision-making.
,
To see a full transcript of responses from all 9 experts, go to,
,
,
These experts and many more leading brands will be speaking at the Useful Business Analytics Summit, taking place on??June 10-11??in Boston. For more information visit??,  "
"
,
,
, ,See ,?? for details.
,
This year?€?s VAST Challenge presents three inter-related mini-challenges and an overall Grand Challenge to test your skills. These challenges feature a mix of data types, including text, transaction, network, and geospatial / temporal. This year?€?s challenge also features a first for the VAST Challenge - a streaming text dataset for you to analyze.
,
These challenges are open to participation by individuals and teams in industry, government, and academia. We encourage your submissions, and look forward to seeing your innovative approaches to solving these challenges using visual analytics.
,
,
In the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.
,
In January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing.?? An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.
,
As an expert in visual analytics, you are called in to help law enforcement from Kronos and Tethys assess the situation and figure out where the missing employees are and how to get them home again.?? Time is of the essence.,
,
,
,
,focuses on the disappearance itself. As an analyst, you have a set of current and historical news reports at your disposal, as well as resumes of numerous GAStech employees and email headers from two weeks of internal GAStech company email. You are being counted on to bring law enforcement up to date on the current organization of the POK and how that organization has changed over time, as well as to characterize the events surrounding the disappearance.
,
,asks you to analyze movement and tracking data. GAStech provides many of their employees with company cars for their personal and professional use, but unbeknownst to the employees, the cars are equipped with GPS tracking devices. You are given tracking data for the two weeks leading up to the disappearance, as well as credit card transaction and loyalty card usage data. From this data, can you identify suspicious behaviors? Can you identify people and locations that law enforcement should investigate? Data for this mini-challenge will be made available in April.
,
,poses a streaming analysis challenge. You will have access to real-time feeds of microblogs and emergency calls. Can you monitor this stream to identify where the missing GAStech employees are, where they may located in the near future, and what steps are needed to bring them home? Data for this mini-challenge will be made available in April.
,
,In the ,, you are asked to put all the pieces together. Who disappeared? Who was responsible for the disappearance? And what were the underlying motivations for the disappearance?
,
For more information, please see,?? or contact ,.
,
The submission deadline is ,.
,
We look forward to seeing your creative solutions!
,
Kris Cook, Georges Grinstein, and Mark Whiting
,
VAST Challenge Committee co-chairs  "
"
By Gregory Piatetsky, Apr 7, 2014.
,
CEO World magazine has compiled a list 64 top Big Data executives and experts to follow on Twitter.  
See also 
,
,??,
,
Here are the top 10 from CEO World: 
,
,
a Data Scientist in Residence at Accel, Scientist Emeritus at bitly, co-founder of HackNY, co-host of DataGotham, and member of NYCResistor. 
,Twitter: ,,
41K followers, Klout 74.
,
,
the Chief Technology Officer of the United States, replacing the United States' first CTO Aneesh Chopra.  
,Twitter: ,,
23.8K followers, Klout 58.
,
,
the leading industry analyst covering text analytics, sentiment analysis, and analysis on the confluence of structured and unstructured data sources. 
,Twitter: ,,
9.1K followers, Klout 63.
,
,
Founder, BI Scorecard, business intelligence and management reporting expert with 20 years' experience. As an industry analyst, she publishes in-depth product reviews on BIScorecard.com, writes for Information Week, and the author of several books including: Successful Business Intelligence: Secrets to Making BI a Killer App and SAP BusinessObjectsBI 4.0: The Complete Reference.
,Twitter: ,,
8.5K followers,
Klout 57.
,
,
the President of KDnuggets, which provides analytics and data mining consulting. Gregory is a founder of KDD (Knowledge Discovery and Data mining conferences) and is one of the leading experts in the field. 
,Twitter:  ,,
18.2K followers, Klout 67.
,
,
SXSW Advisory Board Member for Accelerator & V2, provide insights and assessments of Big Data and Enterprise Software, CEO of The Bloor Group. 
,Twitter: ,,
29K followers, Klout 55.
,
,
thought leader on Big Data and Analytics with more than thirteen years' experience as a hands-on CRM/Analytics and BI Program and Project Manager implementing CRM/ Analytics and BI solutions for Fortune 500 clients in the US and five years' work experience as a Research Executive in Marketing Research, Analytics and Consulting industry. 
,Twitter:  ,,
7.7K followers, Klout 55.
,
,
CTO at Spark Strategic Business Solution, expert on Business Intelligence, Big Data, Analytics, Performance Management, Data Management, Data Visualization, Balanced Scorecard, Strategy Execution, Information Management, and Business Strategy. 
,Twitter:  ,,
7K followers, Klout 61.
,
,
a recognized authority and pioneer in improving online conversion rates, Persuasion Architecture, and persona marketing. 
,Twitter: ,,
41K followers, Klout 77.
,
,
Chief Data Officer & Group Managing Director at Barclays Group and Executive Chairman of Oasis 500. 
,Twitter: ,,
8K followers, Klout 46.
,
Next 10 Twitter experts in their list:
,
,
,??,
Read ,.  "
"
Most popular 
, tweets for Apr 2-3 were
,
Top stories in March: Machine Learning in 7 Pictures; How Many Data Scientists? 
,
,
Top stories in March: Machine Learning in 7 Pictures; How Many Data Scientists? - ,
,
,
Data scientists need their GitHub - here are 4 options: Domino Data Lab, Sense, Mode Analytics, Alpine Data Labs ,
,
,  "
"
,?? ,
,
Much has been written about , churn - predicting who, when, and why , will stop buying, and how (or whether) to intervene. Employee churn is similar - we want to predict who, when, and why employees will terminate. In many ways, it is smarter to to focus inward on employees. For one thing, it is far easier for an company to change the operations or even the behavior of an employee, than that of a customer. As will be seen, employee churn can be massively expensive, and incremental improvements will give big results.
,
The most important difference between employee vs. marketing churn is that a business , someone. Unfortunately, you usually don't get to choose your customers. There is also more at stake - this person will literally be the face of your company, and collectively, the employees produce everything your company does.
,
Employee churn has unique dynamics compared to other problems. To jump-start the ""business understanding"" phase of analytics efforts, we are writing a series of articles to translate employment processes into tractable data mining problems.
,
,
A new hire ideally ramps up to full productivity over months, going through on-boarding, training, certification. In one client engagement, a call center employee had to train for months to pass a Series 7 exam, before even being legally allowed on the phone. During all of that time, an employee delivered no value... they were just preparing to start working.
,
,
, shows a stylized cost/benefit plot for one employee across three years of tenure. At time zero, costs are very high - an expensive recruitment process, administration, training, supplies are all above the normal flow. In this model, after about a year, the main monthly expense is salary and overhead. In this hypothetical job, an employee takes a year to ramp up to full productivity. Different jobs will have different curves, but this sigmoid curve is common.
,
,
To decrease the overall costs due to employee churn, , has to budge on these curves:
,
,
Like quantitative scissors, there are no other options in this model.
,
,
Unfortunately, few companies have any idea of what these costs and benefit numbers are for any given role. Many have worked out the lifetime value of a customer to 5 decimal points, but few have ever considered the lifetime value of an employee. And, not all roles are ""producers"" like sales reps or factory workers - for example, what is the monthly corporate contribution of a data scientist? Data Science may be , but no one really knows how much we ""make it rain.""
,
At Talent Analytics, we have found it simpler to evaluate employee cost relative to a potential performance level. Simple heuristics can begin to build the curves defined in ,. The shocker comes when we subtract (benefit - cost) and take the cumulative sum to find an break-even point..
,
In this stylized example, the employee starts providing monthly value after 10 months, and ,. By comparison, in our engagements we often see impressive attrition after just 3-6 months.
,
Customers provide profit right away, so customer churn analytics is just trying to keep the gravy train rolling. Employee churn analytics is more like trying to get the train to run long enough to provide any value at all.
,
,
With the employee value proposition laid out, we can begin to crack this nut and save the business some money. We are looking for signals that will let us score the likelihood of a person to stay in a role inside a given time window. By deploying the right predictive model, we can decrease the impact of one or more of the ""scissor points"" above.
,
Hint: The most powerful place to solve this problem is before you cut the first paycheck.
,
,
There is much more to this subject. In future installments, we will consider:
,

,
As an experiment, we are putting the R code for this cost model and its plots on GitHub. It is a public project for all to try, modify, and share at ,. Feel free to ""pull request"" any improvements to make this even better. We will build up this toy model as an engine for this series. Please engage!
,
Employee privacy is an important issue which deserves more space - we will address it in a later blog. 
,
Republished with permission from Predictive Analytics Times.??  "
"
By Gregory Piatetsky, Apr 3, 2014.
,
I recently had a chance to discuss key industry questions with 
,, 
,,
currently
Big Data & Analytics Services Leader for HP 
,.
,
His division also includes Big Data Services and Solutions Leveraging Autonomy, Vertica and Hadoop.  He can be reached at 

,
,
,
,
,
Industry is certainly growing and we will be seeing 2014 great year. Several Industries and large customers base is trying to leverage data and analytics. One thing is pretty clear data and analytics will make big impact and define the way business operate in the future. We are seeing Big data efforts within enterprises are rapidly maturing, driven by a need for advanced analytical capability & willingness to experiment. Business Analytics will remain CIO's top technology priority in 2014. According to industry analysts Business Analytics technology and services market in 2014 will be around $ 116 billion and will reach around $160 billion in 2017. So 2014 to be an exciting year for applying analytics to business
,
,
,
,
Data is the New Oil, so leveraging data as an enterprise asset is key. According to IDC, the global universe of data to nearly double every two years, reaching 40,000 Exabytes or 40 trillion gigabytes by 2020. So in other words the data companies' desire and need to access and analyze is going to become massive. The next Big Things (instead of just a Thing) are going be in four key areas below:
,
,
,Attracting and developing analytics capabilities, talent, and leadership, both individual and organizational.  (Leadership is the most critical factor for creating and delivering on competitive advantage)
,
,
,Companies will focus on integrating analytics and leveraging the power of information systems to create powerful systems that will benefit analytical efficiencies, insights generation, and decision making.
,
,
,Machine generated data and connectivity will surpass most other types of data creation in terms of volume. Machine Learning Language will play key role.
,
,
will be key both for consumer as well as enterprise companies.
,
,
,
,
It is reality and not hype anymore, as organizations are under increasing pressure to better manage; amount of data they gather and its use. I am seeing use of this in most of industries. We will be seeing even demand with further more increase in this areas in future]
,
,
,
,
It looks like Spark is set to take the reins as the primary processing framework for new Hadoop workloads arena. This technology is now high priority project at Apache Software Foundation. Indeed Apache Spark will have significant impact and will provide Big Data Analytics robust platform with much vibrant capabilities. It will have great impact with its fast speed and will serve as an engine for large-scale data processing. Some people are seeing it make quite significant impact in industry and they will see quick adoption with key advantages as Spark is a compelling multi-purpose platform in analytics
,
,
,
,
Yes this is key issue and several organizations are working to comprehend their requirements and also to keep Privacy intact as well. I will see there will be ongoing debate on this topic this year before we see long term solution on this arena]
,
,
,
,
Yes, you can automate some aspect of analytics but human inference is a must. Data science in developing correct model, optimizing predictive sense, analyzing correct scenario, weighing in statics theories etc.
,
,
,
,
I expect all major vertical will eventually benefits from Big Data and Analytics. It also depends how people in company leverage this and also what culture they develop to deploy it. I believe having CXO sponsorship in any company makes big difference to retrieve higher benefits and that's what I have seen in successful cases and there are many that I can discuss. So some of industries has been in past leverage BI a lot such as Telco, Banking, Retail etc. I will expect them to continue see now benefits from this. Other key verticals will add in to this party such as Energy, Health, Manufacturing, Travel, Entertainment and even Legal.   
,
,
,
,
""Big Data"" buzzword will be replaced by 
""Big Data Analytics"" in the interim and then down the line it will be again 
""Data Analytics"" (as all Data will be Big).  "
"
,
Last year, our analysis of KDnuggets Annual Analytics/Data Science salary poll results was the
, of 2013.  
,
,
,
,
,
,??,
,??,??
This poll is now closed, here are
,.
,
Here are the results of past polls:
,??
,  "
"
,
,
Here is the company, startup, and acquisition activity for March 2014 from 
,.
See the latest under hashtag
,.
,
,
,
,??,
See also
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
, certificate focuses on concepts and applications. Standard software is used, providing relatively short learning curves. Topics include predictive analytics, machine learning, forecasting, social network analysis, optimization, data visualization, data mining, statistical modeling, risk modeling, queuing, spatial analysis, recommender systems, and time-to-event analysis. This certificate can be completed in about 18 months, with flexible start times throughout the year.
,
, certificate focuses on the open-source programming environments seen in analytics and Big Data applications - R, Python, Hadoop, SQL. The program includes applications to predictive modeling, social data mining and text analytics. This certificate provides exams to test programming proficiency. A typical student profile in this program might be someone working in IT who wants to expand into analytics; a student with programming experience would require about a year to complete all courses. Instruction in basic programming is available for novice programmers (in this situation, the course of study would require 2-3 years - becoming a good programmer requires lots of practice).
,
These certificates are part of The Institute Programs in Analytics and Statistical Studies (PASS): part-time programs for professionals seeking to add to their skill set. The courses are typically four weeks, and do not require that you be online at any particular time of day.
,
Statistics.com instructors are noted authorities in their field, people like:,
,
,
The instructors answer student queries on a private discussion forum throughout each course. See the faculty at ,  "
"
By Gregory Piatetsky, Mar 4, 2014.
,
Here are upcoming 45 March - June 2014 meetings and conferences.  
You can find the full list on KDnuggets page:
,.
,
Here the  ,, , font.
,
Below is the word cloud for these meetings, obtained with Wordle.
,
,
,
The top meeting locations are San Francisco (6), Chicago (5), Toronto (3), Philadelphia (3) and London (3). 
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "
"
,
,
An , on KDnuggets noted that analytics marketplaces have the potential to be the ?€?next big thing in Big Data?€?. However, analytics marketplaces are yet to take off; their impact minimal. I believe there are three main hurdles holding back analytics marketplaces from taking off:
,
,
,

Like any other marketplace a critical mass of consumers and producers is required to foster a healthy trade. Analytical marketplaces have to be big enough to reach a substantial fraction of the total data science market to attract model creators.??, That?€?s one of the reasons app developers flock to iOS and Android marketplaces but not so much to the Windows phone or Blackberry marketplaces.???? However, the predictive analytics and data mining tools market is highly fragmented. Even the biggest player (SAS) owns less than a third of the total market and the long tail of vendors have market shares in the low single digits. One can see the extent of fragmentation in the ,.
,

,?? It is feasible that over time market consolidation will address this issue. , Meanwhile,,, a new startup, is addressing this issue by being open: Snap Analytx supports a number of tools and technologies (including SAS, SPSS, Mahout, Java, C++, Matlab, PMML, R and Python) that together add up to a sizable market share.  The current set of models in the , showcases a wide variety of business use cases such as: customer churn, item recommendation, customer?€?s propensity to buy products and services,recognizing hand-drawn digital images and resolving author-name ambiguity. More models are in the pipeline to be added to the catalog.
,
,
,
Apps in a typical digital marketplace are more or less commodities. Predictive Analytics models on the other hand, almost always have to be , to meet the specific business needs, data sets, missing attributes and performance requirements of each customer scenario. Inability of any existing marketplace to enable easy customization of predictive analytic solutions has been a significant barrier to the adoption of predictive analytic marketplaces. With this realization, Snap Analytx is designed, ground-up, with customizability built-in. Snap Analytx enables ?€? in fact encourages ?€? direct collaboration between model authors and users to facilitate customization.
,
,
,
Marketplaces can succeed only if all the stakeholders ?€? buyers, sellers and the marketplace operators ?€? have an ""all- in-this-together"" attitude.?? Model authors want to invest their time and energy in marketplaces committed to their long-term success and to ensure that their modeling skills and efforts find longevity and visibility.?? They can sense, and will avoid, marketplaces that are ancillary to the operator?€?s main business.?? I believe this has been one of the factors inhibiting the adoption of marketplaces. For this reason Snap Analytx has taken a ?€?pure-play?€? approach ?€? the marketplace is its only business and its raison d'etre.
,
,
,
All of us in the predictive analytics and data science community should be rooting for the success of predictive analytics marketplaces. In addition to all the obvious benefits, I believe that marketplaces can have a much bigger impact on the society at large ?€? marketplaces have a role to play in addressing the well documented, and much discussed, issue of the shortage of data scientists. However, that?€?s a topic for another day!
,
, is the CEO & Founder of Snap Analytx.  Previously, he held senior positions at Microsoft, Oracle, and HP.
,
,
 ,  "
"
,
By ,, Salford Systems, Mar 1, 2014.
,
Since the dawn of data mining (beginning with Bayes Theorem in the 1700s), there have been many successes and failures, even by the top experts in the field. No matter what job function or industry you work in, it is generally agreed on that on-the-job training is a far better learning tool than any classroom lecture. Learning from our mistakes is one of the ways we move forward and accomplish our goals. The same goes for data mining practitioners and data scientists; hands-on experience (or lack thereof) results in victories and blunders that set the foundation for advancements in the field.
,
I asked a few data scientists I know to offer examples of their own do's and don'ts from their real-world experience working with data, consulting, product development, and professionalism in the field. I had an OVERWHELMING response from these kind contributors and have included some of them in the SlideShare below. Don't forget to share them with your colleagues who are committing the DON'TS of data mining!,
,
,
,
,
,
,
,
Scott is a multi-talented industry veteran with 30 years of experience on both the client and services side of data mining, direct and database marketing. He's lived in your shoes and knows how to turn issues into opportunities. ,
,
Website:??,

,
,
,Dean has over 21 years of experience applying advanced data mining, data preparation, and data visualization methods in real-world data intensive problems, including fraud detection, response modeling, survey analysis, planned giving, predictive toxicology, signal process, and missile guidance.??,
,
,Website:??,
,
,
Gregory is an analytics, Big Data, data mining, and data science expert. He is a KDD & SIGKDD co-founder, part-time philosopher, and dad. ,
,
Website: ,
,
,
,
Jim has held management roles with a number of tech firms including ADP, Just Talk and Gale Research. A published author, Jim is fascinated with the methodological application of technology to solve business and scientific problems. ,
,
Website:??,

,
,
,
Falk's work is explicit in space and time, and looks closely at the global effects of the economy. His research interests include: wildlife ecology, seabirds, predictive GIS modeling, web-based wildlife databases and metadata, spatial aspects of Population Viability Analysis (PVA), landscape ecology, Russian Far East, tropical ecology, and conservation steady state economy. ,
,
Website:??,
Original:
,
,
,
 ,  "
"
Ajay Ohri, March 2, 2014.
,
About a week ago, Stephen Wolfram, chief designer of Mathematica and of Wolfram Alpha announced that he would be releasing the Wolfram Language to the world ??and showed a demo??,. The language was first referenced by him in ,?? with some follow up examples of ,.
,
,
,
The language claims to have been developed over the past 25 years and apparently the impetus to release it is to further increase the knowledge revolution by knowledge programming by sharing a breakthrough in it.
,
Since the language is still in alpha -phase and is going to be released only on Raspberry Pi, it may be some time when we do a comprehensive code review on it with tests and benchmarks.
,
This is how the Wolfram Language claims to work by eliminating the distinction between data and programming . It is symbolic language and quite possibly the most extensive. The aim is??break down the input text into a sequence of??,,??check for the Operators ( which ultimately what determine the structure of the expression), evaluate them based on a precedence, and then apply functions.
, ??
The , is extensive and shortly there will be a programming playground in the cloud as per the earlier blog post.
,
Big promises apart - the current language of data scientists remains Python and R, and it would be interesting to see if the knowledge programming thing that the Wolfram Language enhances the availability of knowledge programming or simply fades as a marketing fad with some of it's innovations copied by competitors.  "
"
        ,  "
"
,
,by Lutz Finger, Soumitra Dutta
,Paperback, 338 pages (or Kindle).
,ISBN-10: 1449336752
,O'Reilly, 2014.
,
It is rare to get a data science book that claims to be for manager as well as a data scientist. The book ""Ask Measure Learn"" by O'Reilly however is exactly this. This non-technical guide shows you how to extract significant business value from big data not by the use of technology but by the right business focus. Ask-Measure-Learn is a framework that helps you ask the right questions, measure the right data, and then learn from the results. 
,
Managers who read this book will get enough exposure of technology to understand the concepts of machine learning or of unstructured text analytics. Data Scientist will learn through many different case stories that collecting mountains of data won't yield a grain of insight if you don't know what you're looking for.
,
Here is a free sample -
,. 
,
,
,
,
,
,
,, 
,, a director at LinkedIn, is an authority on social media and text analytics. He's also co-founder and former CEO of Fisheye Analytics, a media data-mining company whose products support governments and various NGOs, such as the Organisation for Economic Co-operation and Development (OECD) and the International Olympic Committee, which was acquired by the WPP group.
,
,, an authority on the impact of new technology on business, is the dean of the Samuel Curtis Johnson Graduate School of Management at Cornell University. Previously he was a Professor at INSEAD, a leading graduate business school. He is also co-founder and former Chairman of Fisheye Analytics.  "
"
Most popular 
, tweets for Feb 28 - Mar 2 were
,
,  "
"
,


,


,


,


,


,


,


,


,


,


,


,


,


,


,


"
"
,
,
Each year national and international educational institutions from over 20 countries take part in the DATA-MINING-CUP (DMC) competition.
,
With this student competition that takes place in spring each year, prudsys AG, one of the leading data mining companies, wants to enthuse domestic and international students for intelligent data analysis (data mining) and challenge them to find the best solution to a data mining problem in competition with others. 
,
The DMC is a team competition. Each educational institution (university, technical university, vocational university etc.) can have two teams (refer to , for more information).
,
Details of the task will be published on April 2, 2014. ,
Registration start: March 3, 2014,
Start of competition and announcement task: April 2, 2014,

End of competition and closing date: May 14, 2014,

prudsys User Days (Berlin, Germany) and award ceremony: July 2-3, 2014
,
,  "
"
        ,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:
,
,
,
,
,
Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's ""Data Mining: Failure to Launch"" - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at:
,
,  "
"
,
,
,??,??
,
,
,??
,??
,??
,
,  "
"
        ,  "
"
        By Ajay Ohri, Mar 5, 2014. 
,
,

,
,
??
,
,
,
,
,
??
,
,
,
??
,
??
,
??Overall better interfaces is something that should come to the data science world, and etcML.com is a great effort to make this possible.  "
"
,
,
While BigML's acclaimed interface makes machine learning accessible to a wide range of users, the underlying RESTful API has the same functionality and more, thereby enabling data scientists and developers to quickly implement a variety of machine learning strategies and predictive applications.
,
This webinar will cover the following topics:
,
,??,
Once you've seen the webinar, you'll be on your way to increasing the power of your predictive models with BigML and to building predictive applications and services.
,
All webinar attendees will receive a 25% discount on a new or upgraded BigML subscription. 
,
,.  "
"
,
Latest ,, (Mar 05, 2014) ,:
,
,??,
Also
, (6) |
, (5) |
, (1) |
, (6) |
, (5) |
, (7) |
, (1) |
, (5) |
, (5) |
, (18) |
,
,
Do ask questions. Understanding the problem and asking the right question is more important than using an advanced algorithm. Gregory Piatetsky, ,  "
"
See below new entries in KDnuggets Directory for March.
Those entries are in addition to time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to , page:
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
Most popular 
, tweets for Mar 3-4 were
,
Data Mining Cup 2014 - Student Competition - Starts ,
,
Spark graduates from Apache Incubator, 100x faster than Hadoop over in-memory data, 10x faster on-disk #BigData ,
,
Stanford Data Mining, Finance, and Statistics Courses Online ,
,
,
,  "
"
,
,
This annual award introduced in 2008 recognizes excellent research by
doctoral candidates in the field of data mining and knowledge discovery.
,
This annual award by ACM SIGKDD will recognize excellent research
by doctoral candidates in the field of data mining and knowledge
discovery. The KDD Doctoral Dissertation Award winner and up to two
runners-up will be recognized at the KDD conference, and their
dissertations will have the opportunity to be published on the KDD
Web site (,). The award winner will receive a plaque,
a check for $2,500. The award winner will also receive a free registration
to attend the KDD conference. The runners-up will receive a plaque at
the conference. The winner and runners-up will be invited to present
his or her work in a special session at the KDD conference.
,
,
,
The final dissertation defense should take place at the nominee's host
institution before the submission deadline. Furthermore, the final
dissertation defense must not have taken place prior to January 1st, 2013.
Nominations are limited to one doctoral dissertation per department or
academic unit. Submissions must be received by the submission deadline.
,
Each nominated dissertation must also have been successfully defended
by the candidate, and the final version of each nominated dissertation
must have been accepted by the candidate's academic unit. An English
version of the dissertation must be submitted with the nomination. A
dissertation can be nominated for both the SIGKDD Doctoral
Dissertation?? Award and the ACM Doctoral Dissertation Award.
,
,
Submission Deadline:??April 30, 2014.,
Notification of Awards:??July 19, 2014.,
Award Presentation at KDD 2014:??August 24-27, 2014, New York City, USA.
,
,
All nomination materials must be submitted electronically to:,

huanliu [at]??,

Please use ""SIGKDD Dissertation Award Nominations"" in your subject
line.
,
All nomination materials must be in English. PDF format is preferred
for all materials. Late submissions will not be accepted. A nomination
must include:,
,
,
Additional information is available at:
,
,
Please direct questions to the Award Committee Chair:, Huan Liu,,
Arizona State University,, huanliu [at]??,.

,

,  "
"
,
,
The flagship 
,
is taking place at the Santa Clara Convention Center on April 9-10.
,
View the schedule: 
,
,
With the Big Data market predicted to be worth a staggering $16.1 billion by the end of 2014, there has never been a greater need to take control of your data.
,
Hear from 80+ speakers, including 
,
& more and find out how Big Data challenges are being tackled by some of the world's biggest companies.
,
Discussion topics include:
,
,??,
,
If you are interested in attending, or for more information please contact 
Dan Cook at 
,,
on +1 415 692 5514, or alternatively you can reserve your place online: 
,  "
"
,
,
Random Forests is one of the top 2 methods used by Kaggle competition winners.
,
It is an ensemble learning method for classification and regression that builds many decision trees at training time and combines their output for the final prediction.
,
This ebook will help beginners leverage the power of multiple alternative analyses, randomization strategies, and ensemble learning with Random Forests. The 70-page ebook includes graphs, examples, and illustrations.,
Chapters include:
,
,
,Download at
,
,  "
"
,
,
Here is ,
,

,
Quentin recently delivered a keynote at Strata 2014 on ?€?Bringing Big Data to One Billion People?€?. He mentioned that Microsoft?€?s vision is to change the world through making Big Data accessible to a billion people. He shared a few user stories demonstrating real-life impact and talked about the endless opportunities Big Data provides for the progress of mankind.
,
For reference: here are the , and , of his keynote at Strata 2014
,

,
,
,
, What?€?s important about Hadoop is that it is a standard.  Just as SQL became a standard so many years ago for relational data, Hadoop became the standard for managing and processing new kinds of data.  Microsoft recognizes this, and the decision to invest in it was fairly straightforward.  It?€?s true that as an open source project, we had to create a different approach in how we are investing to ensure our customers get the best that standard has to offer.
,
The partnership with Hortonworks is a key part of that approach. It has allowed us to participate in the Hadoop community much more effectively than simply taking the code and working on it on our own.  We have put in thousands of engineering hours and tens of thousands of lines of code now into the Apache project.  This certainly has made Hadoop great for Windows, but more importantly we are contributing to projects like Tez, Stinger and Hive.
,
That work has allowed Microsoft to offer , ?€? a 100% Apache Hadoop service in the cloud. And it?€?s empowered our partner Hortonworks to offer the Hortonworks Data Platform (HDP) ?€? a 100% Apache Hadoop product on Windows and on Linux. Windows Azure HDInsight combines the best of Hadoop open source technology with the security, elasticity and manageability that enterprises require. We have built Windows Azure HDInsight to integrate with Excel and Power BI for Office 365, allowing people to easily connect to data through HDInsight, then refine and do business analytics in a turnkey fashion.
, 
,
,
, Get involved. Like becoming a software developer, one of the most important things in skill development for the Data Scientist is simply experience.  Tools, languages, systems and the such will always evolve.  What is enduring is the techniques and intuitions people build as they get exposed to more challenges and domains. 
,
,
,
, I have been reading this book How We Decide by Jonah Lehrer- it?€?s an interesting view into how individuals approach making choices. What?€?s been really a lot of fun for me over the last few years is just how much the nature of my job has changed.  The addition of cloud products ?€? Power BI in Office 365 and our data platform offerings in Windows Azure ?€? has changed our work substantially.  SQL Server, even as the most deployed relational database out there, is not really a direct business.  But our cloud services are ?€? and when creating products like Power BI and Azure SQL Database we have to think about and develop for things like viral adoption, attracting individual customers to the service, using telemetry every day to optimize experiences, etc.  
,
My two boys keep me quite busy outside work of course.  My wife and I try to maximize our time together with them enjoying all that the area has to offer ?€? Seattle is a great city, and the pacific northwest has unbeatable skiing and hiking. ,
In case you missed, here is ,  "
"
,
, ,at University Paris 13, France.
,

This MSc EID2 (Exploration Informatique des Donn??es et D??cisionnel - Data Mining, Analytics, and Knowledge Discovery) focuses on data mining, business analytics, and knowledge discovery. The program is particularly suited for students who have completed a Bachelor?€?s degree (or equivalent) in one of the fields of computer science, mathematics or statistics, and wish to pursue a career in data mining and analytics.
,
The EID2 MSc is designed to produce graduates with the knowledge and skills to:,
,
The curriculum for the EID2 MSc is built on a foundation of core and elective courses. This curriculum joins courses with a Computer Science main theme, those with a Statistical data analysis, Advanced Databases, Data Mining, Business Analytics, and Data Warehousing main theme, and those with cultural courses. These may be grouped, as follows:,
,
,
The electives courses may be chosen, in consultation with the student's advisor,to meet the interdisciplinary ??and the speciality distribution requirements. The full list of available courses may be grouped, as follows:,
,
,
The fourth semester is targeted to the writing of a dissertation during an internship in either a laboratory or a company.

,
This master is ranked by EDUNIVERSAL among the best MSc in Data Mining, Analytics, and Knowledge Discovery in France.
,
The EID2 MSc website is: ,  "
"
,
,
,
As corporate vice president of program management for the Microsoft Data Platform Group, , oversees the design and delivery of the entire family of SQL Server and BI products as well as the Azure Data Platform services. Clark joined Microsoft in 1994, and held a variety of roles mostly in systems technology prior to joining SQL Server several years ago, including the Internet Information Server team and System Center. Clark graduated with a degree in Natural Sciences (Physics) from the University of Massachusetts in Amherst and doubled-majored in Computer Science. He lives in Bellevue, WA with his wife and two boys.
,
,Quentin recently delivered a keynote at Strata 2014 on ?€?Bringing Big Data to One Billion People?€?. He mentioned that Microsoft?€?s vision is to change the world through making Big Data accessible to a billion people. He shared a few user stories demonstrating real-life impact and talked about the endless opportunities Big Data provides for the progress of mankind.
,
For reference: here are the , and , of his keynote at Strata 2014
,
Here is my interview with him:
,
,
,
, Thank you!  The team and I are proud of the work we did for Power BI for Office 365, and it?€?s a key component of the evolving role of data in business. 
 ,
There are a few areas of the product that I think are key differentiators.  Significantly, it?€?s part of Office. , , With the accessibility of Excel and proliferation of Office 365, we can lower the barrier of entry for businesses who want to take advantage of the benefits of business intelligence by putting the right analytics tools in everyone?€?s hands.
 
Another aspect of the product that I think is really important is the integration of data search, catalog and query.  We are empowering people to easily find and combine data from public sources, from commercial data providers, and from data that is managed in the enterprise ?€? all from a search experience just built into Excel.  This is coupled with an experience that easily allows the user to shape that data for how they will use it. , I have used the example before, but it?€?s a fun one ?€? my kids and I were talking about the Olympics and the World Cup, and somehow that conversation turned to wanting to understand population size by country, but continent.  Just using Power BI in Excel, in two minutes I was able to search and find a dataset of population, a dataset that included countries by continent, and then blend those together and create a Power View sheet my boys could then interact with to explore the information.  
,
, is an interactive demo for Summer Olympics.  
,

Way different learning experience then just looking at web pages.  And just imagine what kinds of things I can do with the data sources in my work life ?€? our service telemetry, business results data, customer data, and engineering systems data. 
,
Finally, I have to mention how excited we are about Q&A.  This completely changes who is able to get value out of data and BI.  It allows anyone to type questions they have into the Power BI site in Office 365 ?€? and get instantaneous, visual results in the form of interactive charts or graphs that come from the BI models they created, their co-workers created, or are specifically curated BI models by the enterprise. , The power of this is extraordinary ?€? users don?€?t have to be Excel experts or understand anything about how to use BI tools to find data sources and create visualizations ?€? they only need to be curious enough to ask a question.  That broadens how many people can benefit from data in their daily jobs and lives. 
,
,
,
, Great question!  Data volume is just exploding as you know ?€? growing 10x every 5 years.  So too is interest and deployment of BI and analytics tools.  This growth in data has fueled the need for solutions that can help business analysts easily access various data types and quickly analyze, visualize and share data from any location and from any device.
,
Our perspective is that , ,?€? this is a technical need (scalability of data management and processing), and it?€?s an experience need.  The experiences are super-important ?€? insights from data must reach the broad range of roles people play with varying skill levels ?€? from the analytics developers to the data scientist, to the business analyst and even to the average business user just looking to be more informed.  ,Our view is that it takes the combined effect of three elements to bring big data to that broadest audience: robust tools that everyday people can use, easy access to all kinds of data sets, and a complete data platform for data on-prem and in the cloud of many types and scales. Microsoft is unique in the industry by delivering against all three of those elements.  
, 
,
,
, Well I really cannot predict what we will be talking about next year.  We are certainly committed to the vision we have outlined ?€? the impact of big data on a billion people.  There are many ways the conversation in the big data space is maturing ?€? what datasets are valuable, what analytics get the results, the kinds of value being created, the roles people are playing.  So what I do know is the conversation will continue to be exciting and rich at Strata. 
,
,  "
"
Most popular 
, tweets for Mar 5-6 were
,
Backlash begins: Data Science is not a science, and not even a good job prospect, says Silicon Valley veteran ,
,
Backlash begins: Data Science is not a science, and not even a good job prospect, says Silicon Valley veteran ,
,
Introduction to Random Forests for Beginners - free ebook ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
The Convergence of Technology, Social & Marketing Sciences, and the Humanization of Data
,
May 19-21, 2014, Los Angeles, CA
, ,
,
The Future of Consumer Intelligence accelerates disruptive thinking around decision science. This unique aggregation of diversity across insights, data science, marketing science, social science with technology as a common thread provokes new questions and explores new futures.  
,
Register with code 
, & Save 20% off the standard rates. 
,
,
,
,??,
,
,
,??,
,  "
"
Most popular 
, tweets for Mar 7-9 were
,
Learn very useful skills! Watch #DataScience Experiments with Twitter and IPython Notebook ,
,
Cloudera Data Scientist Solution Kit includes live data, tutorial, and explanations - learn how to use Cloudera ML ,
,
Learn very useful skills! Watch #DataScience Experiments with Twitter and IPython Notebook ,
,
,  "
"
,
,
Here is the company, startup, and acquisition activity for February 2014 from 
,.
See the latest under hashtag
,.
,
,
,??,
See also 
,  "
"
        ,
By Gregory Piatetsky, Mar 10, 2014.
,
,In the routine below, the red  dancers represent one variable and black dancers are a second variable. 
Here are 4 little gems which explain the statistical concepts of
,
,??,
through dance.
,
In the first dance, the red and black dancers are doing very similar routines - positively correlated (r ~ 1).  
This dance is also a great way to explain that 
correlation does NOT imply causation.
,
In the second dance the movements are independent (r=0), and in the third negatively correlated (r ,
,
,
Here is
,.
,
The routines were produced by 
, as part of the project ""Communicating Psychology to the Public through Dance"". 
,
Can you think of interesting other ways to explain analytics, big data, 
and data science through arts ?
,
Please comment.
,
,
,
 ,  "
"
,
,
,??,??
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
, is a ""player/coach"" in the field of Big Data, having led innovative Data teams on large-scale apps for 10+ years. An expert in distributed systems, machine learning, and Enterprise data workflows,??Paco is an O'Reilly author and an advisor for several firms including The Data Guild, Mesosphere, Marinexplore, Agromeda, and TagThisCar.??Paco received his BS Math Sci and MS Comp Sci degrees from Stanford University, and has 25+ years technology industry experience ranging from Bell Labs to early-stage start-ups.
,
Website: ,
Twitter: ,
,
Paco recently delivered a presentation at Strata 2014 on ?€?Apache Mesos as an SDK for Building Distributed Frameworks?€?. He walked through the unique benefits of Apache Mesos, an open source cluster manager and shared the case studies of Mesos uses in production at scale (at Twitter, Airbnb, etc.). Besides his presentation, Paco also gave a tutorial on ?€?Big Data Workflows on Mesos Clusters?€?.
,
Here is my interview with him:
,
,
,
, The notion of ?€?Data Democratization?€? involves making data available throughout an organization. That has pros and cons in practice, but the overall idea is sound.
,
By analogy, the notion of ?€?Cluster Democratization?€? involves making data+resources available throughout more of the organization. To paraphrase Chris Fry, SVP Engineering at Twitter: they relied on Apache Mesos during their ramp up to IPO to get rid of the ?€?Fail Whale?€? conditions, and now any new service launched at Twitter is launched on Mesos. Period. Full stop. Developers don?€?t have to perform mental backflips about all the steps required to roll their code out at scale: they work on laptops, then deploy on what appears to be a very large laptop ?€? the several thousand multicore servers in a cluster.
,
Moreover, the notion of Datacenter Computing focuses on mixed workloads ?€? for several reasons. Two of the very compelling outcomes center on ?€?democratizing?€? resources. On the one hand, rather than have separate clusters for different frameworks (static partitioning) mixed workloads imply better data locality: producers and consumers of data products reside on the same nodes or racks, share the same data partitions. On the other hand, production apps can reside next to dev/test apps, while isolation in lieu of virtualization guarantees that some dev app gone rogue won?€?t be able to step all over the production apps. That allows development managers to have their teams work directly within production environments. Both of those points address enormous current problems in industry ?€? problems which were not well articulated prior to the introduction of Mesos.
,
So the notion of ?€?Cluster Democratization?€? is to go back to some of the basic tenets of distributed computing, leverage what we know about SOA best practices, leverage what?€?s emerging with SDNs, and so on that would rebuild team and processes, making large scale apps much more apropos for what customers/consumers need.
,
For further details and online resources regarding Mesos, refer the following slides presented by Paco at Strata 2014: ,
,

,
,
,
, I believe that the reason for adoption is closely tracking the adoption of cluster computing at scale on commodity hardware. Google has set the pace for this, with Borg and Omega. Twitter and other firms pushed the envelope for OSS adaptations of Google?€?s approach. For proprietary hardware, there are other commercial alternatives: IBM Symphony comes to mind, and perhaps Microsoft Autopilot is also in the category. Meanwhile, the notion of commodity hardware in clusters is drastically changing: multi-core processors, enormous memory spaces, affordable SSD arrays with high write throughput, software defined networks, etc.
,
Apache Mesos has had large?? scale deployments now for three years. I find it odd that so many people ask to compare Mesos with YARN: the latter is not even at the same layer of the technology stack, nor does it have quite the pedigree of deployment at scale (outside of Hortonworks and Yahoo?)  Moreover, Mesos addresses needs for mixed workloads in the context of highly?? available request/response services ?€? in other words, low?? latency use cases. In many verticals, you can measure revenue loss in terms of service latency in milliseconds. Expect that trend to increase across industry. Nearly all of the Mesos deployments that I?€?ve tracked are maniacally focused on low?? latency use cases. Period. Full stop.
,
Keep in mind that current data center utilization rates in the industry range around single?? digit percentage points. That?€?s terrible. Think of how much energy consumption and carbon footprint that implies. Google beats that via Borg/Omega, and Mesos is the open source version of this approach. Also keep in mind that Hadoop, which is based on Google?€?s work circa 2002, was based on hardware from more than a decade ago. Meanwhile, industry use cases for data at scale are becoming more about real ??time and streaming and less about batch. It?€?s time to update our notions of Big Data, because Hadoop only has a couple more years of life. Google defined Data center Computing in terms of mixed workloads, high utilization, and low latency. It?€?s time we update our overall notions of Big Data and real requirements.
,
Also, let?€?s keep in mind that Mesos is a distributed kernel. Much like how we really don?€?t interact much with a Linux kernel, but interact with the GNU/Linux apps built around it, expect to see the evolution of the distributed apps built around Mesos: Aurora, Marathon, Chronos, etc. In terms of other features for Mesos, from a high level I would suggest emphasis on:,
,
,
For further details on Apache Mesos and learning resources, please refer: ,
,
For Data center Computing, please refer to Google Senior Fellow Jeff Dean?€?s talk on Taming Latency Variability and Scaling Deep Learning: ,
,
,
,
, Great question. ,
Please see a recent talk @Twitter Seattle ,
In particular, see the ?€?scorecard?€? of comparisons on slide #57. This turned out to be one of my most popular talks ever. The approach was to compare/contrast several popular open source frameworks for building data workflows. The talk develops a ?€?scorecard?€? for comparing frameworks, and for understanding how your use cases map to some frameworks more than others.
,
In terms of KNIME, I?€?m a huge fan. For many organizations this is an ideal tool to adopt. It?€?s great for code reuse, visualizing the data flows, etc. We call it ?€?future-proofing your system integration?€?.
Cascading fits more on the ?€?conservative?€? part of the programming spectrum, as one would hope to see: operations for your bank, your airline, your hospital, etc. In terms of abstraction layers, Cascading initiated some of use of ?€?monoids?€? and abstract algebra for Big Data circa 2007, by introducing aspects of Functional Programming (FP) into a Java API. Weird, and quite successful! Unfortunately, the important abstract algebra uses weren?€?t identified explicitly, nor is Java much of an FP language, but it fit for Enterprise IT at the time. Cascalog, Scalding, Summingbird, etc., have picked up the torch and defined this approach even better than their foundation technology, Cascading.
,
Meanwhile, the industry was focused on using Pig, Hive, etc., which represent enormous steps backwards in terms of formalizing data workflow abstractions. Seriously, I?€?d pay large sums of money to hear Edgar Codd (inventor of the relational model) expound about Pig and Hive. I?€?m about 99.9999999% certain that his responses would be NSFW!
,
In short, the need within Enterprise to leverage large scale data generally involves cross ??departmental contributions: ETL, data prep, predictive modeling, system integration, etc. Cascading and its DSLs (Domain Specific Languages) provide an abstraction to leverage several excellent points from more theoretical computer science:
,
Take lots of those moving parts and compile them into one JAR file: one point for troubleshooting, debugging, exception handling, notifications, etc.
Particularly in the case of Cascalog, there have been comparisons drawn between the Cascading abstraction and what Edgar Codd originally proposed for ?€?relational model?€?... which was very different than SQL!
,
Here is ,
  "
"
,
,
,
,
Cognizeus has just opened the participant signups for their much-awaited Big Data & Analytics Unconference, AnalyticsWEEK to be held from Mar 24-28, 2014 in the heart of Boston. This conference is a grass root effort at bringing together big-data and analytics community in Boston. This topic has seen a lot of interest and excitement both on the side of the enterprises as well as professionals, students and startups. There is an ever growing demand for building knowledge sharing platforms as the vertical rapidly evolves to meet the new challenges facing this segment.
,
,
,
As per AnalyticsWEEK organizer and Cognizeus CEO Vishal Kumar: ?€?2013 was majorly focused around the current tools that helped enterprises solve the problem around data deluge, but not much dialog has happened around Analytics that makes the big data problem worth investing in. The world hasn?€?t moved much from the talk around big data to building a new level of science that handle those data into actionable insights. So, through AnalyticsWEEK, we will bring limelight back to Analytics?€?.  Vishal has been running a successful Big Data Discovery, Analytics & Visualization meetup in Boston. 
,
This meetup is well recognized and the feedback received from the attendees of this meetup was used as a primary motivation for putting together AnalyticsWEEK, a more structured/unconference in Big Data and Analytics domain. On asking what propelled the need for such unconference Vishal said ?€?We?€?ve asked our attendees what is missing in today?€?s discussions in the various meetups and conferences that they attend. From what we?€?ve heard, there has been a consistent outcry to bring big corporations and agile startups in Big Data & Analytics together to fuel the discussions around challenges and solutions around analytics?€?.
,
Per Vishal, the sole purpose of this conference is to start the dialog between Big Data & Analytics practitioners, bringing the communities together for faster collaborative learning. Every aspect of AnalyticsWEEK will help promote the connection, collaboration and huddle that is required in the area of Big Data Analytics. The core differentiation that sets AnalyticsWEEK apart from its peer is the fact that it provides attendees a place to hangout online as well as offline. We will be providing tools and capabilities to enhance the communication, collaboration and discussion between big data analytics professionals.
,
,
,
After several iterations and deep-dive discussions with other conference organizers, AnalyticsWEEK, the first of its kind, will be running its debut in an extremely controlled fission environment, and hosting its events every evening for 5 days from Monday 3/24 through Friday 3/28, starting at 5:30pm and going on till 9:30pm. 
,
Each day is focused on a particular industry vertical. Monday would be the grand kickoff focused on Big data analytics and Health, Insurance/ Finance, Marketing and Talent Analytics will be given it?€?s own dedicated day in this 5 days event. Selection of vertical is chosen after careful deliberations on what opportunities/ demands geographical location represents. This could vary when similar event is rolled out in other geographical location.
,
,
,
Structure of each day for the conference is kept lean by dividing the day into a keynote talk and a panel discussion. Keynote talk will tackle high-level strategic opportunities in a particular vertical from Big Data & Analytics perspective, whereas panel discussion will be taking more tactical questions head-on as faced by the industry.
,
,
,
The quality of speakers and panelist is among the best of the best to keep the content generation to its premium. Each speaker and panelist will provide some directional thought leadership to the attendees while conducting their speaking as well as panel discussions. This unconference will be featuring around 30 big data and analytics industry thought leaders. The mix includes executives from fortune 100 organizations, big consulting firms s, startup CEOs, Venture Capitalists as well as other deep-dive subject matter experts.
,
,
,
This event is open to all with a nominal fee of $10 to account for conference related expenses and remaining proceeds would be donated to St. Jude. There are limited seats available for the event for each day. Further information could be found with event organizer Vishal Kumar. 
,
AnalyticsWEEK provides a great opportunity for organizations to build brand awareness and to be recognized in the community as a analytics driven organization and to attract the best talent of Boston. If you are interesting in sponsoring the event please contact at ,.
,
For more information visit: 
,  "
"
Do you know how to extract the most useful insights from your company data to make better, smarter, faster business decisions?
,
,
,
Join the 
,
(Boston, June 10-11), the unique conference where corporate peers can meet and determine how USEFUL analytics can improve business decision making. Presenters will exclusively be corporate practitioners - professionals who are dedicating their careers to promoting and facilitating data driven decisions in their organizations.
,
,
,
There are some really strong sessions on offer. Attend to:
,
,??,                                                    
,
Use code 
,
to receive the exclusive $100 discount on top of the Early Bird Discount of $200 if 
,  "
"
,
,
,
July 20 - July 26, 2014, ,Lipari Island, Italy,
Application Deadline: April 30, 2014,
,
""Smart city"" projects are a strategic opportunity for reorganizing local governance. A smart city is a complex adaptive system (CAS) from a computational social science perspective. Smart cities promise a considerable rate of innovation that will benefit urban populations, making them more accessible in spatio-temporal terms, and increase efficiency and rationalization in the management of environmental and energy resources. Furthermore, they will promote new ICT competences. For this reason, technological innovation should be calibrated to these potential scenarios and properly planned so that it enables greater social equity and access.
,
Many ""smart city"" projects aim to demonstrate the valuable contribution of ICTs. However, significant diversity among urban systems make it clear that the interaction among urban attributes and processes that define a city as ?€?smart?€? (smart economy, smart mobility, smart environment, smart people, smart living, smart governance) cannot overlook new government decisions and policies.
,
In fact, technologies that support each of the smart parameters must solve key challenges related to their effective usability, sustainability, and impact. There is therefore a need for including technological development and experimentation within governance cycles that are internal to each parametric domain (economy, mobility, environment) as well as externally connected to the overall government of the socio-territorial, economic, and technological dynamics that are active in every area.
,
The Lipari 2014 School on Computational Social Science will examine the intersection between parameters that define a city as smart and the enabling technology that guarantees proper planning and implementation. World-renowned speakers will provide presentations on smart cities focusing on both salient social issues and the emerging technologies.
,
,
,
,
Deadline for application is April 30, 2014 (admission notification will start on April 1st according to registration time). Applicants must include a short curriculum vitae and specify two professors whom letters of recommendation will be asked to, if deemed necessary. Applicants will be notified about admission by May 8, 2014.
,
The following new textbook is highly recommended for this course:
,
C. Cioffi-Revilla, Introduction to Computational Social Science: Principles and Applications (Springer, 2014), available through various websites (Springer, Amazon, etc.)
,
,
For more information, visit,
,  "
"
Most popular 
, tweets for Apr 4-6 were
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
, recently delivered a presentation at Strata 2014 on ?€?Apache Mesos as an SDK for Building Distributed Frameworks?€?. He walked through the unique benefits of Apache Mesos, an open source cluster manager and shared the case studies of Mesos uses in production at scale (at Twitter, Airbnb, etc.). Besides his presentation, Paco also gave a tutorial on ?€?Big Data Workflows on Mesos Clusters?€?.
,
Here is ,
,
,
,
, That?€?s a fun one! For an example, here?€?s a workshop coming up in Washington DC, based on that material: ,
So many people get into college??-level math but stop at Calculus. Most colleges have a rather lengthy track for Calculus before moving beyond it, 2??-3 years in some cases. In particular, I find that it?€?s rare to encounter business people who have studied math beyond Calculus. That?€?s an artifact of Cold War priorities: highly??-trained mechanical engineers were needed to build ICBMs (Intercontinental Ballistic Missiles), etc.
,
,Priorities have changed enormously. These days, people in business need more understanding of how data at scale is being leveraged for their organization?€?s competitive advantage. They need to understand more about the high??-ROI apps. ?€?Computational Thinking?€? has emerged to describe this shift in thinking ?€? away from Cold War era precepts, onto more practical everyday use of data and analytics.
,
I?€?ve been working on ?€?Just Enough Math?€?, along with co??author Allen Day. Our goal is to teach ?€?just enough?€? advanced math for business people, especially for use cases that depend on ML and large??-scale data. Actually, I?€?m already teaching full??day workshops based on the material, and a large part of my livelihood depends on that, so I hope the project is working out well!
,
Our approach to this material follows a simple formula. We introduce a ?€?morsel?€? of advanced math, describing the mathematical properties in accessible terms, and then, we show a business use case as an illustration. We?€?re trying to work from popular business frameworks ?€? what most MBAs would recognize immediately. Then we show 20??50 lines of Python code to solve the business problem, being careful that variable names align with problem description so that the code is easy to follow. From that, we show results, visualize the insights obtained, and then, suggest various open source frameworks to consider for further study. Throughout all of this, we bring in historical context, LOTS of links for subsequent reading, some mini interviews with key people in the field, and suggested books.
,
I?€?ve encountered many business execs in my workshops, eager to educate themselves about ML before making strategic decisions for their organizations. I find that MOOCs (Massive Open Online Course) about ML are great, but they tend to lack accessibility and business context. So we?€?re providing those missing pieces! Having lots of fun doing it, too.
,
We?€?ve got an amazing review board, with 20+ experts: mathematicians, physicists, business executives, etc. The book is 3/4 complete, and we?€?re expecting galley copies to begin circulating this summer.
,
,
,
, I led an in??-depth analysis of large companies (transnationals) circa 2000 and found unexpected trends (risk) that indicated two of the largest firms at the time were outliers and ostensibly at risk: Walmart and ATT. Clearly, Amazon went after Walmart at that point, while Google took lead in areas that had belonged historically to ATT (which then became a hostile acquisition by its own spin??off SBC).
,
At the time, my analysis was considered a bit over the top. A friend who?€?s a science fiction author, Bruce Sterling, helped post it to a broader audience, but otherwise the material was marginalized. However, within 2??3 years those disruptions became painfully apparent. One can follow the trail of litigation between those firms circa early 2000s to see *how* starkly the two couplets of competitors fell into conflict, as industries experienced an enormous sea change ?€? based on leveraging data at scale.
,
Those two use cases, AMZN and GOOG, in turn helped drive the landscape for Big Data, Data Science as a practice (see Leo Breiman on ?€?Algorithmic Modeling?€?), oh??-so??-many open source frameworks, and cloud computing that made many new use cases feasible.
,
When I was in grad school in the mid??1980s I?€?d started out in Machine Learning, then was forced to switch by department changes, so I moved into Distributed Systems. At the time ML was considered ?€?not academic enough?€? at Stanford. Not so many years later, some of those same Stanford CS profs became quite wealthy (at least one billionaire) thanks to Machine Learning use cases!
,
On the one hand, I?€?m struck by how much ?€?business as usual?€? became overturned, so rapidly at such large magnitude. We could talk about technology trends, but the drivers come from use cases and industry disruptions are what gets noticed.
,
On the other hand, I?€?m grateful to have pursued a dual background in both math/data and systems engineering ?€? that combination comes in quite handy now. I would not have expected that as a grad student 30 years ago. At the time it felt like a compromise.,
Same question, looking ahead:,
For the past 10 years (since 2004) I?€?ve been doing research in agriculture data+analytics ?€? almost at the level of dissertation work, as a side project, waiting for proper timing. Two years ago I mentioned this among investors at a cocktail party and drew much laughter. Frankly, agriculture as a sector is extremely conservative and has about a 10??year cycle for new technology adoption. However, now with extensive drought in California there?€?s been an industry-??wide wake up call. Consider that 40% of the world?€?s population are farmers, that nearly $15T of global GDP is in agriculture, and also let?€?s not forget the impact of snowpack variance, water shortages, energy crisis, weather related risks, GMOs, pesticides, etc. It is difficult to imagine any other field being quite as imperative as agriculture.
,
Needing to wait 10 years to work on something that really matters ?€? not just some other ad??-tech venture ?€? that is the part that was unexpected.
,
,
,
, Not a book, but a new periodical is one of my favorite recent reads: BioCoder 
,
Think: Maker movement meets Radical Science. 
,
In case you missed, here is ,

  "
"
,The new Data Science Challenge:, will be available starting March 31, 2014,, and will cost USD $600.
,
In the U.S., Medicare reimburses private providers for medical procedures performed for covered individuals. As such, it needs to verify that the type of procedures performed and the cost of those procedures are consistent and reasonable. Finally, it needs to detect possible errors or fraud in claims for reimbursement from providers. You have been hired to analyze a large amount of data from Medicare and try to detect abnormal data -- providers, areas, or patients with unusual procedures and/or claims.
,
,
,
CCP candidates compete against each other and against a benchmark set by a committee including some of the world's elite data scientists. Participants who surpass evaluation benchmarks receive the CCP: Data Scientist credential.
,
Those with the highest scores from each Challenge will have an opportunity to share their solutions and promote their work on , and via press and social media outlets. All candidates retain the full rights to their own work and may leverage their models outside of the Challenge as they choose.
,
Register for the Challenge at: 
,  "
"
        ,
,
,  "
"
,
,
Here are some personal impressions on , (New York, March 6, 2014).  It was a well-attended conference, which was organized by Seth Grimes (Alta Plana).  For a more complete list of speakers, slides, videos, and other material, please visit ,.,
,
,
,
, (MIT Media Lab) is working on ?€?affective computing,?€? trying to detect sentiment from facial expressions. An interesting application is helping autistic kids sense sentiment in others?€? facial expressions, as well as providing feedback on their own expressions. Her group has developed software to measure smile intensity, and has a database of a billion(!) smile classified facial views (internationally).?? She commented that it was ,, not just static images.?? Their software can differentiate ?€?delight smiles?€? from ?€?frustration smiles?€?.?? She also noted that measuring skin conductance on the wrist is a better predictor of memory consolidation during sleep than EEG signals.?? Prof. Picard helped found Affectiva, which is used by 300 brands to measure sentiment.
,
,?? (MotiveQuest) gave a marketing perspective, emphasizing the need to market to a ?€?tribe?€? of people (possibly identified through sentiment on social media).?? For example, Toyota Prius customers were found to care about the environment, rather than the car or saving money.?? Toyota emphasized this, and interacted with environmental leaders, flying them around for consulting and conferences.?? Prius outperformed Honda?€?s hybrid which didn?€?t take this approach. ??,.?? For example, Nike celebrates people/health/athletics. MotiveQuest maps out motivational areas using 12 scales:?? ?€?feel accomplished,?€? ?€?feel savvy,?€? ?€?feel sensual,?€? etc.?? Other examples include the campaign for Ram trucks (?€?feel important, rebellious?€?) and Greek Yogurt (?€?feel accomplished, pure, nurturing?€?).?? In summary, think ?€?what motivates the tribe,?€? not ?€?how can I sell things.?€?
,

,?€?s company, GNIP, provides real-time data from multiple social data sources.?? He pointed out that,, as well a different depth.?? For example, items appear first on twitter, but they are quite shallow.?? YouTube is at the other extreme.
,
, described IBM?€?s notion of an Engaged Employee.?? IBM focuses primarily on metadata, not text, to derive an ?€?Enterprise Graph?€?, which becomes useful for recommending the sales team for an engagement, for employee retention analysis, and for constructing an individual engagement dashboard.
,
, (Univ. of Maryland) described using 31 sentiment and other entities and diffusion models to predict the coming election in India.?? Predictions:?? Modi will become Prime Minister, and the BJP party will be short of getting a majority but will lead a coalition government.?? These predictions are consistent with recent polls in India.
,
, (Capital Market Exchange) computes sentiment for institutional investors.?? They employ Bayesian models to derive a sentiment adjusted price for a stock or bond, analyzing 140 bonds per month.??Key influencers are portfolio managers, key strategists, and similar.??They use a curated selection of the business press.
,
, described how The Huffington Post used modeling (Support Vector, Bayes, boosting, logistic regression) to model which comments to exclude because they don?€?t meet standards.?? The resulting system handles about 2/3 of the comments automatically, leaving 1/3 for human decisions.
,
, described Dell?€?s successful uses of sentiment.?? They monitor 25k ?€? 30k conversations per day, looking at sentiment, gravity (depth of sentiment), domain influence (eg., WSJ), author credibility, and relevance.?? He gave as a case study the launch of their XPS13 computer, which had a dip in sales several months after launching.?? However, the sentiment toward the product was good, so they avoided slashing the price.?? Sure enough, sales bounced back, and in retrospect the dip was attributed to seasonality factors.?? They also use sentiment for M&A when it comes to acquiring companies.
,
, (Oxford) gave an instructive overview of recent cutting-edge research on ?€?deep learning?€? with neural networks.
,
, (Cruxly) noted that a bag-or-words model (ie., which words, but not their order) was insufficient for sentiment.?? Grammar must also be taken into account, although difficult.?? Additionally, verb classifications are needed for determining intent, which they capture with a couple hundred grammar rules.?? They track sentiment for ?€?buy?€?, ?€?like?€?, and 5 others.?? They use this sentiment to create lead lists for sales.
,
, (sgallant@mmres.com) is VP for Research at MultiModel Research, a company involved with text and machine learning for sentiment analysis, predictions, and advanced (parse-aware) search.  "
"
By Gregory Piatetsky, Mar 11, 2014.
,
,, is a new, just completed book, written by the developers of 
,, Roberto Battiti and Mauro Brunato. 
,
This book aims to combine two usually separated topics: 
,
,??,
Both topics are technical and the book has appropriate equations to satisfy the analytics professionals.  However, this book can also be read by non-professionals who want 
to understand the paradigm shift brought by machine learning and intelligent optimization methods.  The book has plenty of concrete examples and vivid illustratons, and is fun to read!
,
The authors made this book freely available on the web for personal use at
,
,
,
You can also buy a low-cost paperback version from 
,
or download
,.
,
,
, ,
, 1 Introduction 1
, 2 Lazy learning: nearest neighbors 9
, 3 Learning requires a method 15
, 4 Linear models 29
, 5 Mastering generalized linear least-squares 41
, 6 Rules, decision trees, and forests 59
, 7 Ranking and selecting features 71
, 8 Specific nonlinear models 81
, 9 Neural networks, shallow and deep 93
, 10 Statistical Learning Theory and Support Vector Machines (SVM) 109
, 11 Democracy in machine learning 123
, 12 Top-down clustering: K-means 137
, 13 Bottom-up (agglomerative) clustering 149
, 14 Self-organizing maps 157
, 15 Dimensionality reduction by linear transformations (projections) 165
, 16 Visualizing graphs and networks by nonlinear maps 179
, 17 Semi-supervised learning 191
, 18 Automated improvements by local steps 203
, 19 Local Search and Reactive Search Optimization (RSO) 235
, 20 Continuous and Cooperative Reactive Search Optimization (CoRSO) 251
, 21 Multi-Objective Reactive Search Optimization (MORSO) 265
, 22 Text and web mining 277
, 23 Collaborative filtering and recommendation 299
, Bibliography 307  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining.
,
See full schedule at , .
,
,  "
"
By Natasha Bishop, IBM Big Data Strategy lead, March 11, 2014.
,
,, our Big Data & Analytics Hero this week, is an analytics and data mining expert and editor for KDnuggets, 
, .
,
,
,
,
,
,: Big data, taken broadly, is a tremendously important technology trend and its impact would be as significant as that of an industrial revolution. While ""big data"" as a buzzword will likely be replaced by a new buzzword in the next 5-10 years, the technology trend is here to stay.
,
Big data has already made amazing changes in our lives. Big data has enabled:
,
,??,
,
,
,: Personalized medicine, with better understanding of genetic and environment role in disease
,
,??,
However, like any technology, big data is a double edged sword and we should also be prepared for potential negative effects. A major problem with big data is that it makes privacy almost impossible in a digital world.
,
However, to balance this warning, I know that I am optimistic and I certainly expect big data and analytics to make the world a better place.
,
Original: 
,
,  "
"
, tweets for Mar 10-11 were
,
Best interview questions to evaluate a machine learning researcher, from @peteskomoroch ,
,
Sometimes outliers are real data - US #HealthCare/Capita spending is a huge outlier ,
,
,
Good list of Machine Learning Libraries in Python: scikit-learn, pandas, Theano, NLTK, Pylearn2, and more ,
,
,  "
"
,
,
FICO (NYSE:FICO), a leading , and decision management software company, today released an , showing how 20+ years of analytics innovations have protected consumers from payments fraud. The infographic tracks the evolution of real-time fraud monitoring for payment cards from its inception in 1992 through today. During that time, for example, payment fraud as a percentage of all credit card transactions in the U.S. has dropped by more than 70 percent.
,
FICO infographic highlights the most significant innovations in anti-fraud analytics for card payments, and offers interesting facts about payment fraud in major countries, including France, India, Russia and the UK. The innovations discussed are used in FICO?? Falcon?? Fraud Manager, which protects 2.5+ billion payment cards worldwide.
,
Highlights:
,
,
The payment fraud infographic can be viewed and downloaded at ,.  "
"
,
By Gregory Piatetsky, Mar 13, 2014.
,
Many people have read the McKinsey report on 
, (May 2011) which predicted
,
,
However, it seems that so far the shortage is much less. 
,
The job title ""Data Scientist"" has grown tremendously in popularity, 
according to job site indeed.com
,
,
,
However, notice that the demand stopped increasing sometime in 2013.
,
As of March 13, 2014, Search for 
, jobs (US-based) on indeed.com gives only 1,000 positions.
We find about 10,000 jobs when searching for 
, - without quotes, 
but many of these jobs have title ""Scientist"" or something to do with data, and not necessarily represent ""Data Scientist"" positions.
,
Of course, many people may do similar work without having the title of ""data scientist"".
,
Several estimates may be relevant.
,
Kaggle is the leading platform for data science competitions and claims to be world's largest community of data scientists. 
Kaggle 
, in July 2013, reported 
, in Sep 2013, 
, members on Oct 23, 2013,
reported to have 
, on Feb 24, 2014.  
,Latest numbers, from Kaggle CEO Anthony Goldbloom are: , Kaggle members, of whom , active in the last 6 months.
,
A quick examination of the top 10 ranked 
, shows that only one has a title of ""Data Scientist"".  Top 10 include neuroscience researchers, PhD mathematicians and physicists, and while they are clearly talented competitors on Kaggle, their actual job may not involve data science.
,
LinkedIn has many groups related to data science, Big Data and Analytics - 
see my analysis 
,.
,
The two largest of these groups are:
,
,??,
Most members of these groups do not have the job title ""Data scientists"".
There is a 
""Data Scientists"" LinkedIn group, but it has at present only 6,750 members.
,
LinkedIn Data Scientist Peter Skomoroch,
, 
, 
,
,
He further estimated that perhaps 150-250K people would be a match for a data scientist based on their skills and education.  
,
I remain optimistic that data scientist is a great profession, but I doubt that there is a demand for 100,000 new data scientist positions.  There may be a re-branding of existing positions, or 
,
which collectively do the data science job. 
,
What is your estimate for the number of data scientists?
,
,
,
 ,  "
"
,
If you want to 
,
that can drive your business and 
,
we can help. Download your 
,
copy of Lavastorm Analytics Engine Public today.  
,
Brought to you by Lavastorm Analytics, one of the 
,
you'll be able to conduct data analysis from your desktop with ease -- getting answers faster, through a more intuitive, visual approach.  
,
,
,
and you'll be able to:
,
,??,
,
,  "
"
,
,
?€?Know Your Power?€? is the slogan of the first-ever Women in Statistics conference to be held in Cary, North Carolina, just across the street from SAS headquarters.
,
The May 15-17 conference is a celebration of women in statistics, and the program promises to highlight achievements and career interests of women who work in ?€? or who plan to work in???€? statistical fields. Check out the ,
,

The JMP team and WIN, the SAS Women?€?s Initiatives Network, want to empower three statistics students by helping them attend the Women in Statistics conference. Here?€?s how you can apply for one of the three free conference registrations.
,
Write a short essay telling us why you want to attend this conference, what you hope to get out of it and how you plan to use statistics in your career. Submit your entry at ,.
,
The essays can be no more than 300 words each and need to be submitted before 5:00 p.m. ET April 11, 2014. Essays must be in English and will be judged on the following criteria: how well the author connects personal goals to the conference goals (25 percent); how well the author articulates career plans (25 percent); how inspiring the author's career plans are (25 percent); and how well the essay is written (25 percent). One essay per person, please. ,
,
The contest is open only to students currently enrolled in a degree-granting institution, and both graduate and undergraduate students are welcome to enter. The prize covers conference registration only; travel, meals and any other expenses incurred are the responsibility of the students. Each of the three registrations is valued at $199.
,
The conference is a great opportunity for students to learn from women who have successfully navigated courses and careers in technical fields. ,!
,
Original: ,  "
"
,Experts discussed how to use data strategically in 2014 for best competitive advantage, integrate different data sources to know exactly when and how to speak to customers, and how to become a data-driven organization with a strong analytics culture at the webinar 
,.
,

When they were asked what things should be added to every company?€?s analytics strategy in 2014, these were the responses:
,

Stephen Sharpe, Director of Global Strategic Analytics of , states, 
,(1) Need to start working on a cloud based system to integrate all data and 
,(2) broader integration and training across the various sectors to share best practices.
,

Larry Seligman,VP, Advanced Consumer Analytics, , says, 
,(3) We need a new definition of data integration. 
,(4) Now that we have new metrics, how can we value projects and programs that we have going in new ways that we didn?€?t understand the value of before? Can we now quantify things that we previously could not quantify?
,

Mazhar Hussain, Leader Big Data Practices, , gives insight on the importance of analytics as a service:
,(5) Analytics as a service is going to take up big stream going forward with a lot of traction in the market, and we are seeing the mid-sized and small customers who are also trying to take big advantage of that.
,(6) Blessing of analytics across the culture of the company. It is important to have the right kind of strategy implemented in the company. A lot of companies are looking at Chief Data Officer and Chief Analytics Officer and trying to bless that in their culture so that the implementation is across the board and they can receive the benefit of that implementation. 
,

Full webinar recording is available here: ,
,

This topic is discussed in more detail at the "
"
,
,

,
On May 1, 2014 the Wharton Customer Analytics Initiative (WCAI) will host its first-ever conference devoted to Successful Applications of Customer Analytics.
,
As a research center positioned at the intersection between academics and industry, WCAI will be showcasing presentations that illustrate a high level of rigor but are also broadly accessible to practitioners. Case studies will be presented by ,, ,, ,, ,, ,, ,, and ,.

,

,

,
,
Thursday, May 1, 2014
,
Inn at Penn, 3600 Sansom Street
,
Philadelphia, PA 19104
,
9:00 AM- 5:00 PM

,

Keynote Speaker:
,

,
,
,

,

,
,


Questions? Email ,

,

,

,

The Wharton Customer Analytics Initiative (WCAI) is the world?€?s preeminent academic research center focusing on the development and application of customer analytics methods. Through WCAI?€?s innovative research opportunity program, world-class scholars collaborate with leading-edge businesses to ask and analyze critical questions. WCAI?€?s R&D ?€?crowdsourcing?€? approach enables academic researchers from around the world to help companies understand how to better monetize the individual-level data they collect about customers through the development and application of new predictive models.

,

Follow us:
,

,


,


,  "
"
        ,
,
I am grateful to 
Marc Smith, ,, 
for generating the following network graph which represents a network of Twitter users whose tweets contained ""kdnuggets"" from Jan 1 to Feb 3, 2014.
,
, is a free, open-source template for Microsoft Excel and 
is supported by donations from users. 
,
NodeXL automatically generated highlights (top twitters and hashtags) 
for this graph are in NodeXL SNA gallery ,
,
Top vertices by betweenness centrality:
,
,??,
Top hashtags overall:
,
,??,
,
,
Here are some human understandable highlights. The graph shows 
,
,??,
Top links in Entire Graph:
,
,??,
I am not sure what else does this graph mean - your suggestions welcome! 
,
Overall Graph Metrics:
,
,
,
 ,  "
"
,
,
,
,
,
Date: April 3, 2014,
Time: 11:00am EST | 4pm GMT
,
Accelerate segmentation and modelling with NEW KnowledgeSEEKER 9.0 ?€? now with automated workflow for building, refreshing, and reusing workflows all with the click of a button!
,
Widely known for its industry leading, patented Decision Trees, patent pending Strategy Trees, and graphical wizard-driven interface, Angoss KnowledgeSEEKER data mining and predictive analytics software helps data analysts and business users with data exploration and the development and deployment of predictive models.
,
,Angoss Decision Trees have an easy to use interface for building and exploring segments, discovering relationships and performing variable reduction while Angoss unique Strategy Trees enable users to combine customer segments and scores with business rules and user-defined actions in order to make more strategic data driven decisions, faster.
,
KnowledgeSEEKER 9.0 introduces a powerful automated workflow canvas with a user friendly visual representation enabling the creation of a model workflow in minutes, providing instant documentation and eliminating the need to write code.
,
Your analytics organization will save time and money with new KnowledgeSEEKER 9.0.
,

,
,
,
,
,
Tom Zougas, Angoss Analytics Manager,
, at ,  "
"
Most popular 
, tweets for Mar 12-13 were
,
,  "
"
,
,
,
,is best known for his exploration and expansion of the diffusions of innovations model in his bestseller book ?€?Crossing the Chasm?€?, whose third edition was launched on Jan 28, 2014. He has been advising on business strategy to many of the leading companies in the high-tech sector, including Cisco, Cognizant, Compuware, HP, Microsoft,??SAP, and Yahoo!.
,

The longer version of his bio can be found at the end of this interview.
,

Geoff recently delivered a keynote at Strata 2014 on ?€?Crossing the Chasm: What?€?s New, What?€?s Not?€?.?? Through an updated version of his ?€?crossing the chasm?€? theory, he explained the current state of Big Data analytics and identified the critical success factors. During his short keynote filled with deep insights and great advice, Geoff highlighted the lessons learned in bringing disruptive innovations to market in the 21, century.

,

For reference: here are the , and , of his keynote at Strata 2014
,

Here is my interview with him:

,
,
,
,
, Before you cross the chasm, you have to get to it!?? This means doing a series of one-off projects with large customers who have substantial resources to fund their own whole products end to end.?? Each time you do one of these projects, some portion of it can be repurposed as part of a reusable whole product.?? When you have enough reusable parts, then you can think about crossing the chasm.?? At that point, you target a beachhead on the basis of the intensity of their need and the fit with the parts of the whole product you have assembled to date.

,

,
,
,
Big Data is still before the chasm.
The market is being driven by early adopters who are funding big projects with very ambitious intentions.?? There is still a lot of reusable whole product to build out.?? 

,

however, because certain segments are under enormous pressure to adopt it, largely because they are fighting competitors who are using Big Data to take away their whole customer base.?? Everyone saw what Amazon did to Borders, and no one wants that to happen to them.
,


,
,
,In a complex systems marketplace, the early market is not about having a minimum viable product.?? There is just too much else required.?? Instead it is about having a breakthrough piece of technology that you bring to projects and put your entire team in service to one customer to productize enough of the whole product to solve for one specific use case.?? Even if you think you are a product company, at this stage you are a project company by necessity.?? And on that basis large clients are willing to work with small start-ups because effectively they are buying the team.

,

,
,
,Different industries are under different pressures when it comes to Big Data.?? Financial services swallowed this pill a long time ago.?? Telco has always been sitting on a big data goldmine although they have not always taken advantage of it.?? The new kids on the block are digital media and advertising along with retail.?? Retailers are particularly challenged because their profit margins do not allow them to spend on IT at the level that is required.
,

,
,
,Actually, most of my reading is outside of business?€?a lot of fiction, with a sprinkling of science.?? If you need a good 1500 page book, I recommend??,

,

,Geoffrey Moore is an author, speaker, and advisor who splits his consulting time between start-up companies in the Mohr Davidow portfolio and established high-tech enterprises, most recently including Salesforce, Microsoft, Intel, Box, Aruba, Cognizant, and Rackspace.
,
Moore?€?s life?€?s work has focused on the market dynamics surrounding disruptive innovations. His first book, Crossing the Chasm, focuses on the challenges start-up companies face transitioning from early adopting to mainstream customers. It has sold more than a million copies, and its third edition has been revised such that the majority of its examples and case studies reference companies come to prominence from the past decade. Moore?€?s most recent work, Escape Velocity, addresses the challenge large enterprises face when they seek to add a new line of business to their established portfolio. It has been the basis of much of his recent consulting.
,
Irish by heritage, Moore has yet to meet a microphone he didn?€?t like and gives between 50 and 80 speeches a year. One theme that has received a lot of attention recently is the transition in enterprise IT investment focus from Systems of Record to Systems of Engagement. This is driving the deployment of a new cloud infrastructure to complement the legacy client-server stack, creating massive markets for a next generation of tech industry leaders.

,

Moore has a bachelors in American literature from Stanford University and a PhD in English literature from the University of Washington. After teaching English for four years at Olivet College, he came back to the Bay Area with his wife and family and began a career in high tech as a training specialist. Over time he transitioned first into sales and then into marketing, finally finding his niche in marketing consulting, working first at Regis McKenna Inc, then with the three firms he helped found: The Chasm Group, Chasm Institute, and TCG Advisors. Today he is chairman emeritus of all three.  "
"
,
,
,
,
Some of us are old enough to remember when store clerks made phone calls to get authorization for credit card purchases. And who can forget the clunky gadgets that smashed credit card numbers onto carbon paper? The speed of commerce has come a long way ?€? and so has the speed of fraud prevention.
,
(see also  ,)
,
One of the major fraud challenges of the early 1990s was the rise in counterfeit transactions. Fraudsters were defeating physical fraud-prevention elements on payment cards. The solution to this problem turned out to be analytics. In my view, the modern age of payment fraud prevention started in 1992 when HNC Software introduced Falcon Fraud Manager. This software used analytics to evaluate the authenticity of credit card transactions in real time.
,
It?€?s easy to forget how much payments have changed in the past 22 years. In 1992, there were no smartphones or mobile payments. The Internet didn?€?t exist outside of a few government and university labs. And the notion of e-commerce was closer to science fiction than reality for everyone except passionate CompuServe users. Similarly, payment fraud today bears little resemblance to 20 years ago, when criminals were stealing carbon copies of credit card receipts.
,
The constant adaptation of anti-fraud technology has not only kept pace with changes in payment and fraud patterns, it has actually made payments safer. In the U.S., payment card fraud was equivalent to roughly 18 basis points (0.18%) of all payment card activity in 1992. Today, payment fraud in the U.S. is closer to 5 basis points of all card payments.
,
,
, 
Each time a payment card is swiped or inserted, anti-fraud applications run up to 15,000 calculations in a matter of milliseconds -- leveraging the accumulated intelligence from trillions of previous transactions -- to determine the likelihood that the transaction is fraudulent.
,
Because of this, the payments industry was able to introduce new payment methods such as e-commerce that have lower friction for the customer, albeit with an inherent risk of fraud.
,


,


,
, is where analytic techniques are used to take complex, large data sets and translate the data into a format that is usable for real-time scoring. Examples include:,
,
,
, solves problems where large sets of historical data do not exist, leading to innovations in self-learning and auto-calibrating models. Examples of this include:,
,

, is all about analytic techniques that focus on the ?€?cost?€? of anti-fraud efforts by reducing the number of non-fraudulent transactions stopped or investigated, and improving the customer experience at the point-of-sale. 
,
,
The growth of payment by text or email, the rise of shopping on mobile devices, and future trends that we can?€?t yet envision will give rise to new types of payment fraud. Regardless of which new ?€?normal?€? behaviors emerge, the Big Data analytic underpinnings of anti-fraud software have proven that good can overcome evil when the good guys have analytics on their side.
,

, leads the FICO fraud solutions business unit and is responsible for developing the strategic direction for FICO?€?s fraud products. He is also a key strategic leader in the area of transaction decisioning and analytics. T.J. began his career with DuPont, where he helped design and maintain Expert Systems for process control. Since then, T.J. has held a variety of technical and management roles in the fields of analytics, fraud and risk management, and decision management at industry leading companies such as HNC Software and SAS. T.J. holds a B.S. in computer science and a M.S. in statistics, both from the University of South Carolina. ,He blogs at ,.
  "
"
,
,
By ,
,
, 325 pages,
, Auerbach Publications (March 24, 2014),
, 1466568704,
,
Illustrating basic approaches of business intelligence to the more complex methods of data and text mining, the book guides readers through the process of extracting valuable knowledge from the varieties of data currently being generated in the brick and mortar and internet environments. It considers the broad spectrum of analytics approaches for decision making, including dashboards, OLAP cubes, data mining, and text mining.
,
,
,
,
?€?From the Foreword by Thomas H. Davenport, Distinguished Professor, Babson College; Fellow, MIT Center for Digital Business; and Co-Founder, International Institute for Analytics
,
,
,
, has developed computerized models for trading financial markets in the investment banking industry and has provided Business Intelligence based solutions involving data mining applications for organizations across industry sectors.  He is a professor in the school of management at New Jersey Institute of Technology where he teaches business courses addressing data, information and knowledge management, market research and internet marketing.   "
"
,
By Preriit Souda (,)
,
Few days back, I saw posts by Gregory Piatetsky on ,. Given the higher salaries quoted (for US!), few young grads mailed me to inquire about data science and how they can jump into this area. I am not sure if I can call myself data scientist given that I don?€?t tick all boxes required for being a data scientist, I tried giving them a 101 and advised them some post-grad courses to consider.
,
While I showed them these courses, I felt that I was not giving them the full picture. There?€?s much more that goes in to becoming a successful analytics consultant and most of it is never taught in any school. This other half which can make you win or lose the battle is what I am going to touch upon next.
,
Recently I presented a paper at ARF Re:think 2014. It is one of the influential American conferences catering to advertisers & market researchers. My paper talked about using network mapping and text mining on Facebook brand pages. For those interested, download the paper from the link: ,. This paper had loads of network maps like the one given below.

,


Just looking at it, incites people to admire its beauty! But then everything goes blank! Creating such network maps requires decent level of knowledge in graph theory but that?€?s only half the battle. Explaining what it means and how it can be used by a marketer is the most difficult part of the battle. Majority think the first part (creating network maps & analyzing) is difficult but I have often seen that that?€?s where majority of data scientists get mistaken. For people who work on ,, network maps are simple to decipher but that?€?s not the case for majority of people whom you are showing these graphs.
,
Most of my clients have never seen such kind of maps. ,. If your stakeholders do not get the gist of what these maps show they are not going to be able to understand your findings. If they don?€?t understand your findings, they will not be in a position to appreciate your suggestions based on these findings. Given that they don?€?t appreciate your suggestions, there is a very less probability that your work will be able to garner any results. Hence no matter how good or robust your technique might have been, it?€?s of no use if you cannot get your stakeholders understand your complex analysis in the simplest possible manner.
,
Another thing that often bugs me is called the fear of new. Lot of times when people see something new which challenges the old set of methods, they turn apprehensive to the extent of rejecting it. Most of the techniques that data scientists pursue are seen in similar lights of apprehension. Though there is a lot of literature on dealing with such scenarios, I would just say that , It?€?s very important to explain why you are doing what you are doing and why it can?€?t be done using existing techniques. Last but not the least, data scientists should also be well versed in finding passage for their suggestions by methods not limited only to logical explanations. There?€?s always a human angle to every conversations that most scientists neglect.
,
,. In the race to finish, it?€?s often the last mile that matters the most.
,

,??is a Senior Analyst-Marketing Science (Advanced Analytics) at TNS (A WPP Company).
The views expressed are solely personal and do not reflect views of his employer or associated companies.
,
,
 ,  "
"
By Gregory Piatetsky, Mar 15, 2014.
,
The mystery of what happened to Malaysian Air flight 370 is still unresolved - 
latest NY Times report
, (Mar 16, 2014) suggests that the plane had been deliberately diverted from its planned route a week ago from Kuala Lumpur to Beijing.
,
,
,
Can crowdsourcing help find the missing plane ?
,
,, a scientific crowdsourcing platform, has 2 questions:
,
,
,
Answers, as of March 15, 2014:
,
,??,
,
,
,??,
Try your prediction skills in this or other challenges at
,
,  "
"
,
,
Deadline: May 5, 2014
,
Whom do you trust?  Why do you trust them? How do you know whether to trust someone you've just met? The answers to these questions are essential in everyday interactions but particularly so in the Intelligence Community, where knowing whom to trust is often vital. 
,
The Intelligence Advanced Research Project Activity (IARPA) TRUST program seeks ways to detect one's own neural, psychological, physiological, and behavioral signals that reflect a partner's trustworthiness.  
,
The goal of this Challenge is to develop an algorithm that identifies and extracts such signals from data recorded while volunteers engaged in various types of trust activities.  Cross-disciplinary teaming is encouraged in order to bring together expertise from diverse fields (such as neurophysiology and data analytics) to solve this complex problem.
,
This is a Reduction-to-Practice Challenge that requires written documentation and delivery of source code implementing an algorithm that solves the problem.  This is also a Prodigy Challenge and a real-time online scoring utility and leaderboard will be available to track Solver algorithm performance.
,
There will be up to 3 awards: $25,000 for first place, $15,000 for second place, and $10,000 for third place.  Awards will be based on Seeker's determination of solution performance using a reserved independent validation set.
,
Source: InnoCentive      Challenge ID: 9933465
,
For more information, visit 
,
,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
IE group Big Data Library is a complete catalog of their content, and each week they offer different and interesting presentations on demand. 
,
,
,
,
,
,
,
,
Recent advances in Big Data Mining, Predictive Analytics, and CyberPsychology are being combined to provide extraordinary insights into the reasons why buyers buy, consumers click, and voters commit. Join us for a glimpse of cutting-edge advances in Data Science with one of the field?€?s most respected practitioners and forward thinkers, Dr. JT Kostman, as he discusses newly developed methods that transcend traditional data analyses ...

,

See
,  "
"
,
,
,
I find myself coming back to the same few pictures when explaining basic machine learning concepts. Below is a list I find most illuminating.
,
,
1. , Why lower training error is not always a good thing: , Figure 2.11. Test and training error as a function of model complexity.

,

,
2. , Figure 1.4. Plots of polynomials having various orders M, shown as red curves, fitted to the data set generated by the green curve.
,
,
,
3. ,
, Figure 28.3.  
This figure gives the basic intuition for why complex models can turn out to be less probable. The horizontal axis represents the space of possible data sets D. Bayes?€? theorem rewards models in proportion to how much they predicted the data that occurred. These predictions are quantified by a normalized probability distribution on D. This probability of the data given model Hi, P (D | Hi), is called the evidence for Hi. A simple model H1 makes only a limited range of predictions, shown by P(D|H1); a more powerful model H2, that has, for example, more free parameters than H1, is able to predict a greater variety of data sets. This means, however, that H2 does not predict the data sets in region C1 as strongly as H1. Suppose that equal prior probabilities have been assigned to the two models. Then, if the data set falls in region C1, the less powerful model H1 will be the more probable model. 
,
,
,
,4. ,
(1) Why collectively relevant features may look individually irrelevant, and also (2) Why linear methods may fail. From Isabelle Guyon's ,.
,
,
5. , Why irrelevant features hurt kNN, clustering, and other similarity based methods? The figure above on the left shows two classes well separated on the vertical axis. The figure above on the right adds an irrelevant horizontal axis which destroys the grouping and makes many points nearest neighbors of the opposite class.  
,

,
6. ,
How non-linear basis functions turn a low dimensional classification problem without a linear boundary into a high dimensional problem with a linear boundary. 
,

,
7. ,
,From 
, by Andrew Moore: a one dimensional non-linear classification problem with input x is turned into a 2-D problem z=(x, x^2) that is linearly separable. 
,
See more Machine Learning pictures at ,
,
,
,  is an associate professor in , at , in Istanbul, Turkey, working at the ,.  He was at MIT AI Lab and co-founded ,.
,
,
 ,  "
"
Most popular 
, tweets for Mar 14-16 were
,
Is Apache Spark the Next Big Thing in #Big Data? Claims 100x faster than #Hadoop in memory, 10x faster on disk ,
,
,
Is Apache Spark the Next Big Thing in #Big Data? Claims 100x faster than #Hadoop in memory, 10x faster on disk ,
,
An R Meta-Book - best CRAN posts assembled into ""Intro. to Probability and Statistics with Applications"" #rstats ,
,
,  "
"
,
,
,??,??
,??,??
,
,
,??,??
,??,??
,
,  "
"
,
,
,
The current shortage of data scientists is a widely discussed and well documented topic. It was thrust into the limelight with the May 2011??,??report, which claimed that ?€?by 2018, the United States alone could , face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions?€?. 
,These estimates may yet turn out to be on the higher side (read the recent KDnuggets article ?€?,?€? for an in-depth analysis), but the shortage is real. Sensing an opportunity, the industry has responded with two approaches:
,
,
,
This is the more straight forward approach. Many universities and training schools have begun to offer degree and certificate programs in Data Science. 
,See ,??for a comprehensive list.??Further, the catalogs of companies offering massive open online courses (MOOCs) such as 
,,??
,, 
,??and??
, now include courses in Data Science.
,
,
,
Many companies (startups as well as established ones) are offering tools that are trying to empower semi-technical business users to do predictive analytics on their own i.e. unassisted by professional data scientists. ??These tools are targeted at users who do not have formal training or expertise in areas such as statistical analysis, data mining, predictive analytics and machine learning, but are reasonably proficient with Microsoft Excel like solutions.
,
These companies often state their mission as ?€?democratizing predictive analytics?€? and refer to their offerings variously as the ?€?last mile of business analytics?€? or ?€?Excel for data science?€? or ?€?self-service predictive analytics?€?. In all cases they all have the same fundamental approach to solving the data scientist shortage issue:?? swell the population of those capable of doing data science by converting semi-technical business users into data scientists through tools that have a shorter learning curve.
,
While the two listed approaches are indeed doing their part to address the data science shortage issue, there are not sufficient by themselves. We need to explore additional approaches.
,
,
,
Marketplaces, enable data scientist to promote, showcase and sell pre-built, proven predictive analytics solutions to a global community of buyers. At the risk of oversimplification, one can think of these marketplaces as ?€?Apple iTunes?€? or?€? Google Play?€? specifically optimized, and engineered, for predictive analytic solutions. This , lists some of the marketplaces currently operating.,
,
,
,
The current situation is that data scientists build one-off, custom models from scratch??every time. This is truer for consultants who build custom solutions for their customers. ??A data scientist will often build from scratch, even if he or she had built a similar model, for a similar business problem, for another customer previously. As a result over the course of their careers, much of the work of data scientists is highly duplicative making data scientists inefficient when considering their overall productivity.
,
A prediction marketplace enables a data scientist to create a model??just once??and then repeatedly??customize the model, rather than start from scratch each time. With an efficient marketplace, over the same period, a data scientist can build many more different models for different customers, compared to repeatedly recreating the same models from scratch.
,
,
,
We also expect that a marketplace will enable data scientists to more easily monetize their intellectual property which hopefully will encourage more data scientists to enter the marketplace to produce more innovative predictive analytics solutions.
,
,
,
The efforts of data scientists are largely considered to be wasted when their prediction models fail to deliver on the desired business value. According to some experts almost half of all predictive analytics projects suffer this fate. Predictive analytics marketplaces can significantly reduce this risk by enabling data scientists to start with a proven pre-built model that has fewer unknowns than a model built entirely from scratch.
,
,
,
At this early stage it is difficult to know how much of an impact predictive analytics marketplaces can have in addressing the data scientist shortage issue.?? A lot depends on whether:
,
,
It is early days and we have ways to go, but the journey promises to be exciting and certainly worth the effort.

,
,
, is the CEO & Founder of Snap Analytx. Previously, he held senior positions at Microsoft, Oracle, and HP. 
,
,
 ,  "
"
,
,
,
,
,
,
  "
"
By Gregory Piatetsky, Mar 18, 2014.
,
, announced today that it is one of the first Enterprise Advanced Analytics Platform to be certified by Databricks on Apache Spark.  
,
, was founded by the creators of Spark and recently announced Spark Certification to encourage new development.
,
, is a collaborative, scalable and visual solution for Advanced Analytics on Big Data and Hadoop, which allows both data scientists and business analysts to work with large data sets, develop and collaborate on models without having to ever use code, download software or move data.  
,
Alpine was included recently among niche players in 
,.  
,
The figure below, provided by Alpine, shows the relative advantage of Spark + Alpine over Hadoop.
,
,
,.
,
I asked 
,, Chief Product Officer, Alpine Data Labs about the latest certification on Spark.
,
,
,
,
Certainly the thing that Spark is most famous for is increasing the speed of Hadoop, especially on iterative operations where caching the data into memory can speed things up by one or two orders of magnitude. But it comes with a number of goodies that are very appealing to the data scientist. The addition of a machine learning library with MLLib provides the potential for a general framework for advanced analytics on big data; Scala is a very natural basis for doing data science development; and there are natural abstractions for handling datasets and so on that will make it feel like a natural environment for doing investigative analytics.
,
,
,
,
At Alpine, we place a strong emphasis on being able to iterate quickly. So it's very important for us to let the user get their hands on the data easily, then build experimental analytics workflows, and then iterate on those without having to reload data or recompute things that haven't changed. Spark makes this much, much easier and faster than Hadoop.
,
,
,
,
Because Spark allows us to return results to scientists, business users and executives at 100x the speed, the collaborative and iterative nature of analytics work done in Alpine is even more visible.  That's why we chose to talk about this announcement as ""Hadoop at the Speed of Business"".  In a way, the combination of an agile process enabled by Alpine's collaborative capabilities and Spark's lighting fast performance make the two a match made in heaven.  
,
You can try Alpine at ,.  "
"
,
Latest ,, (Mar 19, 2014) ,:
,
,??,
Also
, (3) |
, (6) |
, (3) |
, (5) |
, (3) |
, (6) |
, (3) |
, (4) |
, (10) |
, (6) |
, (6) |
,
,
""Why Most Published Research Findings Are False"", title of 2005 paper by John P. Ioannidis, whose fight for better (data) science continues with him leading new ,.  "
"
,
,
,
,
,
,
,
  "
"
The next 
, is taking place in Chicago on March 27th! There are still a few spaces still available so sign up today! Open Analytics Summits are great places for CTOs, CMOs, Engineers, Developers, Brand Managers, Data Scientists, and more to network and learn about open source technologies and big data analytics. More information is below.
,
Location: City Winery in Chicago, IL
,
When: March 27th, 2014
,
Web: ,
,
,
KDnuggets readers have been given an exclusive code to register for this event. At registration use , to receive a 40% discount off the , ticket (at enter promotional code).  There are a limited number of tickets available for this code so register soon to guarantee your spot and discount.
,
,
,.
,
Hope to see you there!  "
"
,
,
Due to the recent acquisition of DeepMind by Google for an estimated  $500+ million, and the movement of some academic experts to high-profile tech giants, there has been a lot of buzz surrounding the potential impact deep learning will have in the field of analytics. At FICO, we?€?re excited about this emerging machine learning technology and want to share how we think it fits into the world of analytics.
,
Many advances in analytics and machine learning have been based on our understanding of how the brain works.   Deep learning is no exception ?€? it takes its inspiration from our understanding of the cortex in the brain.  The brain has many regions which form a hierarchy of processing, where sensory data flows from one region to another, being transformed and combined with other information along the way.  While it may seem instantaneous when we recognize a face or a voice, there are actually many stages of processing between our senses and a set of neurons that we can clearly link to that particular person. 
,
This is the same process used in neural network models ?€? such as those FICO uses in fraud detection. These networks have an input layer (raw data or derived features), ?€?hidden?€? layers that process and combines the inputs, and an output layer (such as a score that indicates fraud risk).
,
Recognizing a complex object involves first recognizing and processing a large number of features.  For example, useful features for an image recognition network include edges, corners and colored regions. A combination of features will lead to recognition of a larger pattern. For example, certain combinations of edges, corners and colors would indicate a human face.    
,
In a financial decision problem like fraud detection, a feature might be the amount of a transaction, or the kind of retailer involved ?€? something that can be clearly identified.  A certain number of transaction amounts, at a certain frequency with certain types of retailers, will indicate a higher fraud risk.
,
,

,
Source: FICO??? Labs Blog
,
For some problems in analytics, we rely on huge data stores (such as the FICO fraud data consortium) to craft the most appropriate features and combinations of features for our models. For instance, over 20 years in studying fraud data, we know what kinds of relationships we?€?re looking for, so we can build a single ?€?hidden?€? layer that processes raw transaction data to detect fraud extremely efficiently, allowing for calculations of fraud risk in 40-60 milliseconds (about 1/5h the time it takes you to blink).  Such hand-crafted features can lead to robust models that can make quick decisions.  This is why, for many years, most artificial neural network research was focused on networks with a single layer of processing. These are sometimes called shallow networks. 
,
However, deep learning research has shown many new ways to let the mass of Big Data determine the most important features for a decision task.  Deep learning happens in deep neural networks, where the deep refers to the factor that multiple layers of processing transform the input data (whether it?€?s images, speech or text) into some output useful for making decisions (perhaps whether a certain object is in a security camera image, or inferring context from text). 
,
An example of this kind of deep learning is discussed in The Analytics Store?€?s ,. Per that post, ?€?The image below shows a simplified illustration of this where a stack of neural networks are used to classify images. While the data presented to the network would be raw pixel values, internally the network would generate much higher level features.?€?

,
,
Source:  ,
,
Over time algorithms and research have advanced to allow more layers to be added to neural models to more closely mimic the complexity of the brain. With the explosion of Big Data, there are unique areas to apply these techniques where simpler features build into more complex features.   
,
Certainly there?€?s a lot of variety to deep learning algorithms, and we?€?re likely to see many new variations over the next years as more applications are developed.  The current crop of deep learning is drawing on only a fraction of what is known about real neurons and brains, indicating huge potential for this line of scientific exploration.
,
, is a VP, Analytic Science at FICO.
,
Original:
,  "
"
,
,
, leads LinkedIn's efforts around query understanding. Before that, he led LinkedIn's product data science team. He previously led a local search quality team at Google and was a founding employee of Endeca (acquired by Oracle in 2011). He has written a textbook on faceted search, and is a recognized advocate of human-computer interaction and information retrieval (HCIR). He has a PhD in Computer Science from CMU, as well as BS and MS degrees from MIT.,
,
LinkedIn: ,
,
Twitter: ,
,
,Here is my interview with him:
,
,
,
, Actually, I'm happy to refer readers to a recent presentation delivered by my colleagues Abhimanyu Lad and Satya Kanduri on ,. Basically, our challenges in query understanding are detecting and correcting misspelled queries, segmenting and tagging queries to identify the precise entities of interest to the searcher, identifying the vertical domains most likely to serve the searcher's need, and expanding the query to increase recall. On the ranking side, our biggest challenge comes from having to combine a machine-learned ranking approach with a high degree of personalization.
,
,
,
,
,
, Not surprisingly, I believe that query understanding will play an increasingly important role. I believe the current state of query understanding is significantly less mature than that of ranking, and that we'll see most investment in query interpretation and elaboration ?€? especially search-assist interfaces.
,
,
,
, I created the team, but you'll find from a quick search on LinkedIn (naturally!) that other companies do have individuals and teams that focus on query understanding.
,
, Query understanding mostly takes place before the search engine retrieves any results -- it focuses on analyzing and possibly rewriting the query, as well as assisting the query elaboration process through auto-completion and suggested searches.
,
My previous role as Director of Data Science was broader but less focused. Transferring into my current role has allowed me to focus on search, which has been my driving passion for most of my professional career.
,
,
,
, If I told you, I'd have to kill you! :-)
,
No, really, there are many things you can do to improve your , without crossing the line into abusive search engine optimization. Here are my top four recommendations:,
,
,
,
,
, I pursued computer science and math because I loved them -- my original aspiration was to study combinatorics and work in academia. Fortunately, I discovered the world of practical industry applications, and I've never looked back.
,
My advice to aspiring data scientists is to learn current technical skills (e.g., languages like Python and Scala; frameworks like Apache Spark) but even more importantly to learn what kinds of problems you like. I find that the best data scientists combine analytical and technical ability with a strong grounding in at least one of the social science, especially economics or sociology.
,
,
,
, Perhaps not what you have in mind, but I love everything that Neil Gaiman has written, and I recently re-read ,. I also enjoyed Ricardo Semler's ,, an autobiographical book about how he introduced participative management (what people are now calling ""holacracy"") in a Brazilian manufacturing company that produces $200M in annual revenue.  "
"
Most popular 
, tweets for Mar 17-18 were
,
Machine Learning in 7 Pictures 
,
,
Stanford student show NSA metadata surveillance can find medical conditions, financial connections, gun ownership ,
,
Machine Learning in 7 Pictures ,
,
,  "
"
        ,  "
"
,Data science is a skill that?€?s learned by doing, but the pathway to data science can be confusing., offers an accelerated program through full-time, personalized instruction in San Francisco. We?€?re announcing open applications for the the Summer 2014 cohort, which begins May 12th, 2014.
,
The focus is on practical skills desired by world-class technology companies. This includes:,
,
,
Apply here:??,
,
Alumni have joined some of the most demanding data science teams in the Bay Area, including Tesla Motors, Change.org, Tagged, and Lumiata.
,
The curriculum covers the realms of machine learning, software engineering, statistical analysis, and big data. The program is focused on the hard and soft skills needed to solve complex problems with data science.
,
Successful applicants to the program have:,
,
,
Zipfian Academy is a bridge for talented individuals to level-up their skills and join the thriving young industry of data science.
,
Learn more or apply to Zipfian Academy: ,  "
"
        ,
By Gregory Piatetsky, Mar 21, 2014.
,
Previous KDnuggets Poll asked: 
,
,
,
,
2014 results show that data scientists are in a sweet spot, especially in US, Canada, and Australia.  European and Asian data scientists earn significantly less.
,
From 230 respondents, about 61% were from US/Canada, and 24% from Europe.
,
,
,
,
A regional breakdown shows that Data Science Managers in the US/Canada earn average salary around $165K, about 22% higher than Data Scientists ($135K), who earn almost twice as much as data analysts ($76K).  Data shows the regions with at least 10 respondents.
,
,
,
,
Employer type also plays an important role - overall, people in the industry earned
$113K, almost 50% more than $75K for academic/government employees. 
,
Here is a more detailed breakdown by Role and Employer type (students excluded).
,
,
,
Finally, the chart below shows a breakdown by Region, Role, and Employment.
European and Asian salaries are lagging behind the US, Canadian, and Australian ones for similar positions.
,
,
,
Here are more details for 
,.
,
and the results of past polls:
,
,??,
,
,
 ,  "
"
Most popular 
, tweets for Mar 19-20 were
,
Bitcoin 101 - covers everything you need to know, how it is traded, history, and future ,
,
IBM creates fraud & financial crimes prevention unit, combining #BigData Analytics, Business know-how & Data viz ,
,
Bitcoin 101 - covers everything you need to know, how it is traded, history, and future ,
,
,  "
"
,
,
,

,
Who else but INFORMS ?€? the world?€?s largest non-profit society supporting the analytics and big data industry ?€? can cover this hot topic the way it should be covered.  This new INFORMS conference will show you how to get from data discovery to real business value. 
,The focus of this conference is squarely on business with success stories and lessons learned on key issues such as how to navigate the big data ecosystem, how to build and manage data science teams, and how to bridge the gap between decision-makers, IT managers, and analytics professionals.

,
You?€?ll get best practices from hand-pickled speakers from companies experienced in leveraging big data.  Keynote speaker is big data rock star, Bill Franks, who will speak on Putting Big Data to Work.
, at the early rate until May 23.  "
"
        ,  "
"
,
,
,
Chapter 4 of Kaiser Fung's 2013 book , discusses some of the modeling challenges faced by Groupon, the daily deals company.
,
,
,
Fung explores what constitutes success in targeting customers and explains the law of diminishing returns in email marketing. ?€?The trick of sending more emails is a double-edged sword. While the number of false-negative errors falls, the algorithm suffers from more false alarms,?€? Fung writes in this chapter.
,
Fung says ?€?a win for the modelers is a loss for the sales force?€? and shows how Groupon?€?s self-interest in maximizing its own revenues can be at odds with merchants?€? own desires to profit.
,
, to read all the details.  "
"
,
,
,
,
,
Date: April 3, 2014,
Time: 11:00am EST | 4pm GMT
,
Accelerate segmentation and modelling with NEW KnowledgeSEEKER 9.0 ?€? now with automated workflow for building, refreshing, and reusing workflows all with the click of a button!
,
Widely known for its industry leading, patented Decision Trees, patent pending Strategy Trees, and graphical wizard-driven interface, Angoss KnowledgeSEEKER data mining and predictive analytics software helps data analysts and business users with data exploration and the development and deployment of predictive models.
,
,Angoss Decision Trees have an easy to use interface for building and exploring segments, discovering relationships and performing variable reduction while Angoss unique Strategy Trees enable users to combine customer segments and scores with business rules and user-defined actions in order to make more strategic data driven decisions, faster.
,
KnowledgeSEEKER 9.0 introduces a powerful automated workflow canvas with a user friendly visual representation enabling the creation of a model workflow in minutes, providing instant documentation and eliminating the need to write code.
,
Your analytics organization will save time and money with new KnowledgeSEEKER 9.0.
,
,
,
,
,
,
Tom Zougas, Angoss Analytics Manager,
, at ,  "
"
,
By Gregory Piatetsky, Mar 24, 2014.
,
,
, announced the acquisition of 
,, a major provider of statistical, data mining, and text analytics software. 
Founded in 1984, StatSoft is headquartered in Tulsa, Okla., and operates worldwide through offices and partners in 25 countries. Terms of the transaction were not disclosed.
,
Dell says in its press release
,
,
StatSoft was recently positioned by Gartner, Inc. in the ""Challengers"" quadrant of the 
,
,
,
StatSoft approach was rather unique in being inspired by resaearch on Cognitive Mining - see 
my interview with StatSoft VP Dr. Thomas Hill
, - here is a quote:
,
,
Derrick Harris of GigaOm calls this 
,, writing
,
,
Read ,.
,
,
,
 ,  "
"
,
,
, is currently IBM?€?s Vice President of Big Data Products, overseeing product strategy, development and business partnerships. Previously at IBM, Anjul focused on application and data lifecycle management tools and spearheaded the development of XML capabilities in DB2 database server. She has 25 years of experience in the database industry and has held engineering and management positions at IBM, Informix and Sybase. In 2009, she received the YWCA of Silicon Valley?€?s ?€?Tribute to Women in Technology?€? Award. 
,
Twitter: ,
,
Here is my interview with her:
,
,
,
, We?€?ve only just begun to tap into the potential of what is in store for the world of big data. , Rather than rely on decisions stemming from gut feelings, businesses will infuse analytics into everything that employees, partners, and customers touch (management systems, machine to machine processes, daily decisions & tasks) leading to highly curiosity-driven, evidence-based cultures, and workforces. The factors that will better ensure success including: fostering a culture that is analytics-savvy; making security, privacy and governance protocols a requirement for big data implementations; the creation of a Chief Data Officer as part of organizations?€? C-suite; and a heightened focus on harnessing data generated outside of the organization in conjunction with data in the enterprise with the increased use of social media and mobile devices.
,
,
,
, You don?€?t want to base a strategy on ?€?right now,?€? you need a technology infrastructure that will support big data and analytics into the future, ensuring your business?€? growth and success. Taking a big data platform approach is key as it allows users to address the full spectrum of big data challenges.,
,
A big data platform has attributes such as the ability to perform quick and accurate analytics, can leverage and enhance existing technologies in use such as open source Hadoop technology; and is cloud-enabled to scale as needed while optimizing resources. Keeping these principles in mind, managing big data and deploying the IT infrastructure to support it, should no longer be a cumbersome process. In this new era, simple, easy-to-use tools and platforms exist that can help organizations make sense of the new data-driven norm. 
,
,
,
,
, You touched on it in your previous question ?€? the key to success is thinking broadly, having foresight and planning - recognizing that a strategy is needed that not just tackles how to start but how to sustain. Therefore the crucial steps to adopting a success big data strategy includes:  
,
,
,
,
,
,
, The role may not be specifically called ?€?data scientist?€? in every organization, but the need for people skilled in data is very much a reality and will continue to be fundamental in shaping industries and driving successful businesses.  Gartner cites there are already over 100 Chief Data Officers serving large enterprises today and that?€?s double the number from 2012. We believe that role will continue to emerge as a vital member of the C-suite charged with determining the most strategic way to make data a tangible business asset. But beyond that, , from marketing and sales to create compelling customer experiences to HR departments needing to develop more precise ways to recruit, cultivate, develop and retain their top performers. Big Data is developing curiosity-driven and evidence-based cultures and workforces.
,
There are resources out there for students and professionals who want to explore a career in data. IBM is very committed to training tomorrow?€?s data workers and has partnerships with over 1,000 universities around the world to prepare students with big data skills. Going beyond just technology donation, IBM supports faculty with curriculum materials, case study projects based on real business challenges, and IBM data scientists who guest lecture in classes. IBM also launched ,, an online educational site of over 100,000 members run by new and experienced Hadoop, Big Data and DB2 users who want to learn, contribute with course materials, or look for job opportunities.
,
,
,
, ""And the Mountains Echoed"" by Khaled Hosseini.   "
"
Most popular 
, tweets for Mar 21-23 were
,
Machine Learning in Parallel with Support Vector Machines, Generalized Linear Models, and Adaptive Boosting ,
,
Cartoon: Why Madame Zaza, Fortune Teller, changes to Predictive Analytics ,
,
,
Machine Learning in Parallel with Support Vector Machines, Generalized Linear Models, and Adaptive Boosting ,
,
,  "
"
        ,
,
Pioneering predictive analytics vendor
,
announced it has been positioned by Gartner, Inc. in the Leaders quadrant of the first ""Gartner Magic Quadrant for Advanced Analytics Platforms"".(*)
The full report is available on the RapidMiner website.
,
,
,
,
,
Gartner Magic Quadrants evaluate vendors on their ability to execute and the
completeness of their vision. According to Gartner, ""leaders are those vendors
with a strong and proven track record in the market that are also likely to
influence the market's broader growth and direction."" In the report, Gartner
states that ""predictive analytics and other categories of advanced analytics
are becoming a major factor in the analytics market.""(*)
,
,
Gartner defines Advanced Analytics as, ""the analysis of all kinds of data using
sophisticated quantitative methods (for example, statistics, descriptive and
predictive data mining, simulation and optimization) to produce insights that
traditional approaches to business intelligence (BI) - such as query and
reporting - are unlikely to discover.""(*) 
,
,
,
,  "
"
, 
,, a broad higher education initiative launched by SAS, includes free SAS software, university partnerships and engaging user communities that support the next generation of SAS users. 
,
The 
, and revamped 
, will offer students, professors and non-traditional learners free use of SAS technologies. Students pursuing SAS Certification (or looking to become a lucrative SAS data miner) can access actual code from certification courses.
,
In 2006, SAS partnered with North Carolina State University to launch the first analytics masters degree program. Since then, the demand for graduates with Big Data skills has increased dramatically, and graduates are well compensated. SAS will aggressively pursue new degree programs such as the ones the company supports at Texas A&M University, Louisiana State University, University of South Carolina, National University, Northwestern and many others. In addition, SAS has partnered with colleges to create more than 50 certificate programs around the world.
,
Working professionals and non-traditional students can gain SAS skills through the company's collaborations with online universities including Capella University and Athabasca University.
,
SAS will also take advantage of the growth of online and massive open online courses (MOOCs) to extend its reach beyond traditional university settings. Currently in development, SAS MOOCs will allow anyone to learn SAS programming to prepare for SAS certification, increase their marketability and enhance skills.
,
For more information, visit ,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
New entries for February for KDnuggets Directory.
,
Those entries are in addition to more time-sensitive content constantly updated in these KDnuggets sections:
,
,??,
Added to 
, page:
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,
,??,
Added to ,
,  "
"
,
,
Imagine your shock of discovering that unbeknown to you, you are now the owner of Visa credit-card from some bank in Utah, and worse, the credit reporting agency informs you that ?€?your?€? $3,000 balance on said card is 30-day past due.  As a result, you react like many of us will, in sheer panic, because you don?€?t recognize any of the details and can?€?t figure out how, given your conservative financial habits, this could have happened to you. 
,
Well, congratulations! You are now one of the millions who share this experience: you are a victim of a particular kind of fraud called identity theft. 
,
,Loosely speaking, identity theft is a form of fraud where a fraudster assumes a different persona, of some?€?innocent bystander?€? in order to (fraudulently) receive goods or services they don?€?t intend to pay for.  Although fraud is hardly a new thing - impersonation & deceit for personal gains is as old as Methuselah - but in this brave-new-world of easy credit,  and the impersonal means by which credit is obtained, and because of the magnitude and impact, identity theft has recently become a real problem.
,
Consequences of identity fraud are significant to both consumers and the credit-awarding enterprises alike. Estimated losses from identity fraud is in the billions (estimates vary, e.g., Javelin Strategy & Re-search of Pleasanton, California, puts it at upward of 20 billion, 2012), affecting millions (Javelin estimates that 5% of US adults are affected). Even if ultimately you weren?€?t held responsible for the losses, invariably, losses to the enterprise are passed on to you, the consumer. The graph below summarizes the number of affected and total losses. 
,
,
,
Fighting identity theft isn?€?t easy. Absent a rigorous authentication process, it?€?s difficult to catch when, say, an adult daughter assumes her mother?€?s identity. The reality is that credit-awarding organizations (banks, retailers, cell phone servicers, etc.,) encounter competitive pressures to driving up volumes where any [sic] unnecessary friction with consumers is frowned upon. Indeed, many consider identity verification as an unnecessary friction point.
,
In addition, business operates under sometime strict regulatory constraints, which, right or wrong, limits their options to vet customers based upon, say, physical appearance. Increasingly banks and merchants rely on scoring solution to helps fight identity thieves. 
,
To remind you, a score is a numerical value thought of as the probability that an application is fraudulent.  Score-producing statistical algorithms rely on narrative which strives to captures the essential features, logic and mechanics of identity thefts (and thieves), from which risk factors are identified and extracted. 
,
Industry recognizes that there are hundreds of risk factors or features, although admittedly, some o those provide a rather weak signal. This extraction process results in a vector of features, the input vector, from which a statistician or data scientist must learn a functional relationship that quantifies the level of risk associated with each vector input.  
,
Although the technical details are quite complex the basic idea is this:
Applicants provide personal  identifying information (PII=name, DOB, SSN, address) ?€? which presumably, in some cases is not true or ?€?authentic?€?. 
,
Various, often proprietary algorithms involving e.g., proximity, matching and velocity calculations, will process raw data that result in input vectors corresponding to each record. Training set is labeled with values of +1 and -1, where confirmed fraud is tagged with +1, and with -1 otherwise. 
,
It is recognized that the fraud class +1 is rare, normally consisting of fewer than 5 percent of the entire portfolio. The training data is therefore input-output pairs as above, with +1 up-sampled to ensure +1/-1 near-parity. As the reader surely knows, industry is fond of a handful of statistical methods or machine learning algorithms, to estimate the probability that an input vector belongs to class with label +1.
,
At the same time, the industry is far from monolithic and for one reason or another one organization prefers logistic regression, while another prefers boosting.  Specifically, ID Analytics/LifeLock uses boosting with stumps (a depth-1 decision trees) for its ID Score algorithm, for 2 reasons:
,
,
,
Logistic regression (LOGIT) is similarly interpretable, but requires significant effort to develop especially with high-dimensional input space. LOGIT is a legacy ID Analytics product, but LexisNexis uses it extensively for its fraud score products. 
,
Originally developed by HNC software, Neural Networks is used by FICO?€?s Falcon transaction fraud product. A neural network model however is not easily interpretable and a high regulatory hurdle pre-vents it from being used in FCRA-related scoring solutions.
,
,
Other popular methods are decision trees/random forests, and support-vector machines. The ?€?arms race?€? between fraudsters and fraud-fighters continues in full fury: fraudsters seem to quickly adapt to improvements in scoring algorithms, and the cycle continues notwithstanding harsh penalties imposed by law.
,
, is Data scientist with 20-year track record of providing value to customers in government, healthcare, energy/utilities and finance.  He is currently teaching machine learning at San Diego State University and is at True Bearing Analytics.  He is also an Advisor at Analytics Ventures  and Cyber United, and Advisory Board Member at Dataskill.   "
"
,
,
Dates: June 17-20, 2014
,Location: UC Berkeley, Berkeley, CA
,Website: ,
,Contact: ,
,
Online registration for the 2014 Workshop on Algorithms for Modern
Massive Data Sets (MMDS 2014) is now available at:
,
,
,
In addition to the talks, there will be a poster session one evening. You may apply
to present a poster at the link above.
,
Synopsis: The 2014 Workshop on Algorithms for Modern Massive Data Sets
(MMDS 2014) will address algorithmic, mathematical, and statistical challenges
in modern statistical data analysis. The goals of MMDS 2014 are to explore
novel techniques for modeling and analyzing massive, high-dimensional, and
nonlinearly-structured scientific and internet data sets, and to bring together
computer scientists, statisticians, mathematicians, and data analysis practitioners
to promote cross-fertilization of ideas.
,
Organizers: 
,  "
"
Most popular 
, tweets for Mar 24-25 were
,
My answer to Is a Data Science Certificate and other certificate programs sufficient to become a Data Scientist ... ,
,
Google BigQuery gets big price cut, streaming for Real-Time, #BigData Analytics ,
,
Kaggle branches beyond data mining competitions, will build oil&gas vertical solutions, looks for data scientists ,
,
,  "
"
,
Latest ,, (Mar 26, 2014) ,:
,
,??,
Also
, (3) |
, (2) |
, (3) |
, (2) |
, (1) |
, (3) |
, (5) |
, (2) |
, (3) |
, (6) |
,
,
""#Bigdata flows through everything, it is a new kind of power"", 
, opening Boston 
,, March 24, 2014  #BOSAW2014   "
"
,
,
,
Data scientists just want to do fast, interactive exploratory analytics on all kinds of data-without thinking about whether data fits in-memory, about parallelism, force-fitting it into a table, or pulling it out of a file and formatting it for math packages. You'd also like to use your favorite analytical language and have it transparently scale up to Big Data volumes.
,
Paradigm4 presents a 
, with native scalable complex analytics, programmable from R and Python.
,
, Thursday, April 10th, 2014 at 1pm EST 
,
,
,
,??,
,
,
,??,
,
,  ,.  "
"
[ I have recently come across an excellent post by Kaiser Fung, ,, author of a very good new book ,. Check your numbersense below - reposted with permission. GP]
,
,
,
It's Spring Break at NYU, which for professors, is not a break. I have been marking midterms for my business analytics class. Since I like to set open-ended questions (are there anything else in statistics?), I get a variety of answers. One of the questions helps clarify what I mean by numbersense.
,
The question asks students to comment on the distribution of a variable (median income) in a dataset of customers. Every student should know how to generate a histogram and a boxplot, plus summary statistics and percentiles for this data. 
The figure below shows what each student was looking at. Before you read further, think about what features of this distribution attract your attention.
,
,
,
The responses I received fell into several categories. Let me list them out:
,
,
,
These answers are ordered from demonstrating least , to most. 
,
Response types #1 and #2 make no mention of the spike of zeroes despite the strong hint in the question: ""Give plausible explanations for any parts of the distribution that is not smooth"". Response #2 notices but is not bothered enough to explain it.
,
Responses #3-#5 all attempt to explain the observed anomaly.  ,
,
,
,
Response #3 has a good theory (""retirees"") but somehow looks past the fact that the zero-income segment spans a wide age range.
,
(The highlighted parts of the histogram below are the zero-income customers.)
,
,
,
In fact, this chart was used by several to prove that retirees accounted for the zero-income segment. This is a ""strong priors"" problem: it's all too easy to take weak evidence in the face of a strong theory.
,
One student divided the customers into zero-income versus not. This allows us to examine the distribution of other variables. For example, the median home value of those with ""zero income"" is almost the same as those with positive income.
,
,
,
Think about the people you hire to do analytics. While any of the answers above are acceptable, if you find someone who can give you Response #3-#5, you are in much better shape. That's what I mean by hiring for ,.
,
, is a Marketing and Advertising Analytics expert, author and speaker. Currently at Vimeo and NYU.
,
Original: ,  "
"
,A Research Opportunity from the Wharton Customer Analytics Initiative
,
,
,
,
,
,
The Wharton Customer Analytics Initiative is pleased to announce a truly unique data set from a major international video gaming company, which will allow unprecedented insight into the habits of online gamers. The data arises from play on a single multiplayer game and includes: game usage, game play, and historic behavioral data for 9.5 million users who played 882,000 unique game rounds.?? Complete longitudinal tracking is done for two cohorts of players, covering every round played by each cohort member, as well as all the players they ever played with, from product launch to March 2014.
,
The data sponsor believes that happy players will play more and buy more ?€? we want to help them develop new methods to test this hypothesis and develop operational improvements based on these findings. More specifically, the sponsor believes players are happiest when matched with others of similar skill level.?? Traditional skill-matching algorithms currently used in games do not take into account improvements in technology or detailed playing behavior that is available from today?€?s multiplayer games.?? The sponsor seeks next-generation skill assessment tools, as well as practical solutions to optimizing large-scale matching algorithms to reduce wait times for new games.
,
The project sponsor is open to other avenues of research, including but not limited to: social networking in online multiplayer games, online game behavior choices, or platform-based player segmentation.
,
To learn more about the data and business context, interested faculty and doctoral students can attend a live webinar on??,. During the webinar, details of the data will be described and executives from the project sponsor will be available for Q&A. The webinar will be archived for those who can?€?t attend live.??,
,
Following the webinar, Interested researchers should submit proposals online through the??,??by??
,??to apply for access to the data. Proposals will be evaluated based on their potential for academic contribution and the researcher?€?s ability to address issues of strategic importance to the program sponsor.
,
Researchers are encouraged to review??
,??and
,??before submitting their proposal. 
Additional questions on the data or the process can be directed to??
,.  "
"
,
,
,The Swiss Association for Analytics (,) just launched the very first issue of the Swiss Analytics Magazine! 
,
Here is the editorial:
,
Welcome to the very first issue of the Swiss Analytics Magazine (SAM), published by the Swiss Association for Analytics (SAA). This magazine is one of our means to achieve the association objectives. The main objective is to provide original analytics content to Swiss practitioners. In the different issues, we will propose interviews with leading actors in analytics, technical articles, case studies, book reviews, company profiles and event agenda. All these with a focus on Switzerland, since the SAM aims at being a swiss magazine for people in the field of analytics.
,
In this very first issue, we cover several analytics topics with a focus on forecasting. Marcel Baumgartner explains forecasting at Nestl??. Mike Gilliland, from SAS, has been interviewed by Sara Vidal about forecasting performance. Jean-Marc Vandenabeele describes the notion of bounce rate in web analytics. Vincent Schickel-K??ng introduces the concepts behind online recommendations. Benjamin Wiederkehr presents ,. Finally, Christian Laux discusses the legal aspects of Big Data.
,
This first issue is brought to you by our three sponsors: ,, , and ,. You will find more information about them on the back cover. The proposed frequency of the SAM is bi-annual. We have chosen English language to allow anyone from Switzerland to read all its content. Please send any feedback to ,. The SAA committee welcomes you to the data-driven world!
,
In the name of the committee,,
Sandro Saitta, President of the Swiss Association for Analytics
,
Original: ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining fo rMarch 27 and beyond.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky, Mar 27, 2014.
,
I recently read an interesting interview of Srikanth Velamakanni, co-founder and CEO of 
,,
conducted by 
, of 
,.
,
The interview was in 3 parts, and covered starting Fractal Analytics,
biggest challenges to overcome, pitching against the best in the industry, hiring analytics talent, managing attrition, strategic bets, and advice for starting in data science.
,
Here are some highlights:
,
,
,
,: For Hiring:
,
These are the attributes we lookout for:
,
,??,
Apart from these skills, we also look at values. People need to work in very collaborative manner with teams across the globe. So, we look for integrity and commitment to work .
,
,
,
,
,
,: ... a lot changing in this industry rapidly. Half-life of knowledge is about 3 years. Hence, we need to be aware of the latest tools and techniques. In order to address this, we have created Fractal Academy where people can get trained in various aspects of analytics. We have integrated platforms like eDX and Coursera and people get credit in Fractal Academy for completing relevant courses on these platforms.
,
We have also made changes to our career tracks in line with changes in industry. Till some time back, we used to hire everyone as analytics consultant. The person was supposed to do everything from meeting & consulting the client to building the models and managing the programme. In order to keep up with fast pace of the industry, we now have 4 different career tracks for people:
,
,??,
Now we are hiring people in these different roles.
,
,
,
,
,
,: My advice would be to take up a career track and be clear in how you would be successful in it. There is a lucrative future in all the four specializations we talked about. And like any other science, this industry will also move from generalists to those having a super-specialization.
,
,
,
So analysts should ensure that their career moves in a 'T' shape. They should have deep knowledge in at least one domain and have a broad perspective about the overall Analytics industry at the same time. At younger stage, people should be willing to move industries and learn quickly.
,
,??,
Here are the 3 parts of the interview:
,  "
"
,
,
,
Actual and potential societal benefits of information in our increasingly digital world are leading to significant shifts in values (e.g., security versus freedom) in behaviors, and ultimately in polices and laws. More than ever before, technology is far ahead of the law. While the scale of Big Data offers massive potential, that very scale can render conventional, top-down solutions, including security technologies and methods, infeasible. We are in the midst of two significant shifts ?€? the shift to Big Data requiring new computational solutions, and the more profound shift in societal benefits, risks, and values.
,
,The White House clearly enunciated the benefits and threats of Big Data. President Obama asserted for this study the societal objective of , in conjunction with the technical objectives that the Internet and Big Data must be open and interoperable.
,
This was reinforced by the Honorable Penny Pritzker, Secretary of the US Department of Commerce, who recognized the potential of Big Data to enhance innovation, productivity, and economic growth based on the ,. These objectives underlie the White House study to determine:
,
,
John Podesta, Obama?€?s Counselor and study lead asked: 
,
,
,
,[1], (the first of three such workshops[2]) responded to the technical questions. The emergence of Big Data and its applications pose significant technical challenges not only in implementation and operation but also in managing the risks for which there are emergent and as yet incomplete solutions such as Dwork?€?s differential privacy, and emerging top-down, solutions like Zeldovich?€?s encrypting databases (CryptDB) and Vadhan?€?s computing on encrypted data. 
,
,
,
Due to the massive scale of Big Data, previously top-down solutions for security, e.g., anticipating and preventing security breaches, will simply not scale to Big Data. They must be augmented with new approaches including bottom-up solutions such as Stonebraker?€?s logging to detect and stem previously unanticipated security breeches and Weitzner?€?s accountable systems. ,. Hence, Big Data requires a shift from a focus on top-down methods of controlling data generation and collection to data usage. Not only do top-down methods not scale, ,. Adequate let alone complete solutions will take years to develop.
,
Yet technical solutions must be designed to meet requirements that are not yet fully known, such as the societal value of privacy in an increasingly digital world that has the potential of recording our every action and thought, ; and establishing and maintaining trust amongst citizens, corporations, and governments both at home and abroad, since an open Internet knows no sovereign boundaries. Perhaps the most compelling idea that emerged from the workshop was stated by Guttag who observed that the importance of safe, secure, efficient solutions should not stand in the way of urgent challenges that could leverage Big Data solutions. Ask the families of critically ill patients how much security they would risk to extend the length and quality of the lives of their loved ones by exploiting the potential of Big Data.
,
,
,
Many other challenging questions were posed without answers, including, 
,
,
,??,
In summary there was broad agreement on the societal and technical objectives of the White House study and on encouraging technical and research directions. While legislation has always lagged technology advances; the gap, evidenced by recent NSA surveillance activities, is at an extreme, at least in terms of public perception. What was least understood at the workshop, and more broadly, was the balancing of security and freedom in values and then in legislation and the impact on our democracy and the ways in which we want to live and work.
,
Big dominated the workshop: Big Data, Big Trust/Liability, Big Rules, Big Compliance, and ultimately Big Shifts in values, methods, and regulation to obtain Big Benefits and keep up with Big technology advances like Big Data.
,
,At the South By Southwest Conference a week after the White House-MIT Workshop, Edward Snowden addressed in a virtual conversation [5] safety and freedom in the face of his disclosures of NSA surveillance, the very actions that, in part, prompted the White House Study. ,.
,
In the workshop both the White House and Snowden addressed the issues objectively and were articulate and credible. However, the same could not be said a week later in Snowden?€?s TED Virtual Conversation [6] in which Snowden?€?s position was more assertive on democratic principles such as liberty, privacy, and transparency for an open Internet and on the same technical issues raised earlier, again articulate and credible, this time supported by the inventor of the World Wide Web, Sir Tim Berners-Lee. Unfortunately, in his Response to Snowden?€?s Virtual Conversation[7], ,, Deputy director NSA, did not address the technical issues raised by the White house and Snowden preferring to comment on political issues and Snowden?€?s motivations and culpability, seemingly not aligned with the attitude expressed by the White house Study, the White house-MIT workshop, nor the SXSW and TED audiences.
,
These public actions of President Obama, the White House, and relevant government agencies, striving for an open and free Internet while balancing safety with freedom is in stark contrast to corresponding government actions around the world, including Turkey, Brazil, China, and even Switzerland. ,.
,

,
,
, ,: Advancing the State of the Art in Technology and Practice
,
The , and MIT co-hosted a public workshop entitled ?€?Big Data Privacy: Advancing the State of the Art in Technology and Practice?€? on March 3, 2014. The event was part of a series of workshops on big data and privacy organized by the , and the ,. The workshop was also the first in a series of events being held across the country in response to President Obama?€?s , in the context of increased digital information and the computing power to process it.
,
, ,
,
As part of this effort, OSTP will be co-hosting at least two additional events?€?one with the Data & Society Research Institute and New York University, and one with the School of Information and the Berkeley Center for Law and Technology at the University of California, Berkeley.??In the coming weeks, we will be announcing additional opportunities for the public to inform this important work.?? Check back here for more information and updates on our progress.
,
, Craig Mundie, , Focus on Data Use, Not Data Collection, Foreign Affairs, March/April 2014.
,
, Keynote speaker: John Podesta, White House Counselor, White House-MIT Big Data Privacy Workshop: Advancing the State of the Art in Technology and Practice, March 4, 2014, MIT, Cambridge, MA ,
,
, Edward Snowden, 
, SXSW, Austin, TX Monday, March 10, 2014, 
, and 
,
,
, Edward Snowden, 
,, TED, Vancouver, Canada, March 19, 2014
,
, ,, Deputy director, NSA; 
,, TED, Vancouver, Canada, March 20, 2014.
,
, has over 30 years experience in research and industrial practice in databases, distributed systems, integration, artificial intelligence, and multi-disciplinary problem solving.
He is concerned with the Big Picture aspects of information ecosystems including business, economic, social, application, and technical.
Dr. Brodie is a Research Scientist, MIT Computer Science and Artificial Intelligence Laboratory; advises startups; serves on Advisory Boards of national and international research organizations; and is an adjunct professor at the National University of Ireland, Galway.
,
For over 20 years he served as Chief Scientist of IT, Verizon, a Fortune 20 company, responsible for advanced technologies, architectures, and methodologies for Information Technology strategies and for guiding industrial scale deployments of emergent technologies, most recently Cloud Computing and Big Data.
He has served on several National Academy of Science committees.
Dr. Brodie holds a PhD in Databases from the University of Toronto?€?
and a Doctor of Science (honoris causa) from the National University of Ireland.
,
,
 ,  "
"
,
By Gregory Piatetsky, Mar 27, 2014.
,
On March 24, I moderated a panel at 
,Boston's first 
,.  
,
The week-long event,
ably organized by Cognizeus CEO 
Vishal Kumar,
,, was held over 5 evenings, focusing on different topics each day.
,
First day started with the keynote address by Oracle's 
Paul Sonderegger,
,
on Big Data Analytics.
,
Watch the keynote at
,
,??,
After a delicious pizza and networking break, there was an expert panel  
with 
,
,
,??
,
,??,

After the panelists introduced their experience with Big Data and Analytics, I asked them 3 questions.
Here are selected answers, thanks to the excellent twitter stream of 
,.
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,??,
There were also questions from the audience, especially on 
,
,
,??,
Check also many links to data science courses, certificated and education on 
,
,??,
,
,
,??,
,
,
,??,
You can watch the 
,.
,??,
Perhaps 150-200 people have come to the first evening and gave an excellent start to the Analytics Week.
,
,
,
 ,  "
"
,
,
,
Like many, I was a big admirer of Nate Silver political forecasts on his  FiveThirtyEight blog.  Nate successfully predicted presidential results in all US 50 states in 2012 (although he got lucky on Florida).
,
Nate leveraged his success and moved from NYTimes to a new, expanded venture, 
,,
based at ESPN, where he hired additional staff and plans to cover not only sports and politics - his traditional strengths, but also other areas, like climate, economics, and life(?). 
,
Unfortunately, the first foray of FiveThirtyEight into climate science -
, -
threatens to undermine Nate Silver reputation.
,
That article was 
, by Paul Krugman and many others as biased and inaccurate - not surprising, given the reputation of the author, Roger Pielke Jr., as a climate change ""sceptic"".
,
Of course, science is based on scepticism and doubt, 
but given the overwhelming scientific evidence in favor of climate change, the criticism of climate change should be very well founded.  However, the FiveThirtyEight
article commits many errors - 
,.
,
Roger Pielke writes
,
,
Here I want to examine just the data science aspect.
,
Global temperature is increasing - here is one chart. 
,
,Source: 
,
,
Here is a basic data science question, suitable for Data Science 101 class.
,
Is there a simple test to determine if climate change (global temperature increase) has effect on natural disasters ?
,
Think about it for a minute.
,
If one type of natural disaster was due to climate, while another type was not,
then we could compare the frequency of these two types to see the effect climate change has on natural disasters.
,
Well, big weather events like hurricanes and floods are clearly related to climate change. However earthquakes, as far as we know, are not related.
,
Here is the graph from Munich Re   - one of the largest insurance companies, covering earthquakes and other weather-related disasters from 1980 to present.
,
,
Source: ,, 2012.
,
We can clearly see that the trend for the number of earthquakes is flat, 
while trends for weather-related disasters - floods and storms - are increasing.
,
,
,
 ,  "
"
By Gregory Piatetsky, Mar 30, 2014.
,
Information Management followed their slideshow of  
, (Hadapt, Precog, Platfora, YarcData, Datameer, SiSense, Kapow Software, and Zettaset) - most actually quite well-known companies, with another slideshow of 
,.
,
Unlike the previous list, the 10 companies in this list are indeed less known in the  Big Data and Analytics space, but they appear to have a good potential for success.
,
,
,
,Location: Redmond, WA
,Summary:
Tracks and analyzes activity across the customer life cycle. The company's customer data platform is provided as a cloud-based service, eliminating the need for separate, disconnected point solutions and homegrown analytic tools. The company says it can deploy Appuri SaaS solution in 15 minutes.
,
,
,
,Location: Bangalore, India
,Summary: Utilizes big data, proprietary data mining algorithms and machine learning techniques, to provide customer intelligence, price optimization, risk management, operations planning and sales and marketing insights. 
,
,,
,
,Location: Mumbai, India
,Summary: Uses data analytics to help companies better understand, predict and influence consumer behavior.
Offer data analytics functions via outsourcing.
,
Note: see also 
,.
,
,,
,
,Location: Gurgaon, India
,Summary: The big data division of Xebia group, GoDataDriven provides online retail solutions, services (data warehousing, scalable ETL, high-volume data analytics) and training in Hadoop and related big data technologies for retailers.
,
,, 
,
,Location: Toronto, Canada
,Summary: Infobright's Knowledge Grid architecture supports the Internet of Things with a high performance analytic database, designed to rapidly analyze machine generated data.
,
,, 
,
,Location: San Francisco, CA
,Summary: Provides a Push Intelligence platform that alerts the user when and why key business metrics have changed. Connects its KPI Warehouse to metrics from all of the business intelligence, SaaS, big data and data visualization tools used by the customer.
,
,, 
,
,Location: Lake Forest, CA
,Summary: Designs energy efficient, high density, easy to manage servers for all big data applications.
,
,, 
,
,Location: Bingham Farms, MI
,Summary: Provides three products: DBIS Intranet - an interactive, Web-like wholesale distribution system; DBIS e-Commerce - a business transaction module; and Business Intelligence/analytics - an information analysis tool for making informed decisions. 
,
,,
,
,Location: San Jose, CA
,Summary: Provides enterprise-grade machine learning with the Skytree Server, which the company says is the only general purpose scalable machine learning system on the market. 
,
,,
,
,Location: Mountain View, CA
,Summary: Provides data science, engineering and training services to help companies meet business goals.  "
"
,
,
,
,It is no surprise today that Data Scientist (or related roles such as Data Manager, Statistician, Data Analyst, etc.) is one of the most sought career paths. In response to this cross-industry trend, several top universities have started programs dedicated to Data Science.

,

Allured by the tremendous opportunities, great compensation and visibility to business leaders, many people are moving towards the Data Scientist career path without a thorough careful assessment of the day-to-day responsibilities of such a role, the required attitude; and balance of technical and business skills.

,

In the pursuit to provide data science aspirants a clear realistic picture of the data scientist role, which they can assess against their personality and career ambitions, I recently discussed this with ,, a data science expert with 25+ years of industry experience. His candid, detailed response is very likely to be an eye-opener for many.

,

Paco Nathan?€?s short bio is provided at the end of the post.

,
,
,
,I don?€?t agree. Not many people have the breadth of skills to perform the role, nor the patience that is absolutely needed to acquire those skills, nor the desire to get there.
,

As a self test:
,
,
If one doesn?€?t feel absolutely comfortable performing each of those listed above, right now, then my advice is to avoid ?€?Data Science?€? as a career.
,
The term Data Scientist was ?€?sexy?€? as a new role circa 2012 in the sense of DJ Patil, Hilary Mason, et al. However, not everyone gets a chunk of a $4B IPO! (full disclosure: I got invited 3x to join LI prior to their IPO but stubbornly pursued other opportunities?? what an excellent team there!)
,
Circa 2012: that was then, this is now. Actual work in Data Science entails:
,
,

To echo what DJ and others have articulated so well before: most data??related problems are social/organizational (e.g., data silos, lack of metadata, matrix org in??fighting, etc.) or else the key insights probably would have been apparent within that organization already.
,

I have a hunch that much of the interesting work in e??commerce has played out already ?€? big players will continue to reap big revenue, but the work to be done now is mostly outside of Silicon Valley. ??Or rather, other industries coming here to learn, partner, purchase, etc. ,For example, Monsanto launched a private equity firm in SF that, practically speaking, can invest more money at more favorable terms into Ag data ventures than just about any VC firm. Meanwhile, VCs in the area have all but ignored data??related ventures in domains that matter ?€? with the exception of Khosla. In the past several months they?€?ve acquired business units within SV: Climate Corp, Solum, etc., which by the way were funded by Khosla. Expect more of that trend.
,
From my perspective, the big issues in data now are not in ad??tech, but instead real issues: food supply, drought/flooding, energy security, health care, telecom, transportation apart from oil dependency, smarter manufacturing, deforestation monitoring, oceanographic analysis, etc.
,
Also, IT budgets are still enormously flawed w.r.t. data insights. Too much budget goes into the priesthood of ?€?data engineering?€?, and far too much budget tends to be earmarked for data that?€?s already cleaned up. Also, I find that the notion of ?€?Product Management?€? in SV is almost antithetically opposed to effective use of data: in many cases product managers are incentivized to discourage use of data within companies.
,

Hence our value is generally going to be realized at:
,
,
The first speaks to IT budgets earmarked the wrong way, and the second speaks to Product Management being almost systematically hostile to effective use of data. The third speaks to the fact that several of my biggest contributions as a data scientist have been to provide exec staff with hard evidence to fire other executives and get the company back on track. Again, industry disruptions have impact.
,

,For people just starting out, be really careful about where you go to work. If a firm claims to have ?€?excellent engineering?€? but insufficient use of data circa 2014, then they are *not* the sharpest tools on the workbench?? pick some other firm in which to start. Find mentors. Join teams that have strong sponsorship from Finance or Operations (which generally understand data and variance) while perhaps avoiding teams that have sponsorship from Engineering or Marketing (which generally do not understand effective use of data).
,

,, not necessarily in order:
,
,
,??,
, is a ""player/coach"" in the field of Big Data, having led innovative Data teams on large-scale apps for 10+ years. An expert in distributed systems, machine learning, and Enterprise data workflows, Paco is an O'Reilly author and an advisor for several firms including The Data Guild, Mesosphere, Marinexplore, Agromeda, and TagThisCar. Paco received his BS Math Sci and MS Comp Sci degrees from Stanford University, and has 25+ years technology industry experience ranging from Bell Labs to early-stage start-ups.
,
,
 ,  "
"
by Ajay Ohri, March 31, 2014. 
,
We 
,, an exciting startup in the field of machine learning and sentiment analysis.??
Here is an exclusive interview with the co-founder of etcML.com Richard Socher.
,
,
,
,
,
,- Most of all: ease of use. For analyzing what's trending on Twitter??each day you just need to visit the front page. For analyzing your own??tweets or a certain query you just type it into the front page. To??train and test your own classifier you just copy and paste a text file??into the browser. This is by far the easiest way to train a classifier??and predict with existing classifiers.
The fact that it's entirely free and doesn't even require signing up??is an additional bonus.
,
,
,
, Not yet but we will include that capability in the future.
,
,
,
, Get real user feedback. We iterated through several interfaces to make??the final site as easy to use as possible.
,
,
,
, NaSent is quite different to the??etcML.??
NaSent is more accurate but also works only for English sentences.
NaSent is based on some very novel recursive deep learning techniques??that we just developed at Stanford last year:??
,.
,
etcML??can classify longer documents of any standard character language??(we're working on UTF8 support right now to support Chinese and??Japanese as well). It is very robust and fast.
,
,
,
, As of today, we have:
,
,
This includes public and private datasets and classifiers. In the last??2 months we've had:
,
,
,
,
,
,
, The answer depends on how I disambiguate ""you"" in this question :)??etcML??can help researchers, journalists, social scientists and??companies quickly solve language classification problems. It will get??most people 75% to their solution.??More broadly, my research and deep learning can improve and automate a??lot of text problems in industry.
,
,
,
, Yes. One of my long term goals is to develop novel deep learning and??machine learning algorithms and analyze their performance on a large??variety of different text classification problems. Hopefully, we'll be
able to improve all the trained classifiers on the site in the future.
,
,
,
, I like that idea and we've had similar thoughts. We'll see if this is??something that users want and if so make it happen.
,
, is a PhD student at Stanford working on natural language processing, machine learning and deep learning.
etcML is a new and free tool that allows even novice user use the power of machine learning and text classification.  "
"
Most popular 
, tweets for Mar 28-30 were
,
SAS vs. R #rstats vs. Python - good comparison of ecosystems. Which should you learn? depends, but start with Python ,
,
SAS vs. R #rstats vs. Python - good comparison of ecosystems. Which should you learn? depends, but start with Python ,
,
Must read books for people interested in Analytics: When Genius failed, Tesco, Signal & Noise, Kaushik 2.0, Siegel ,
,
,  "
"
,
,
,
Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed.
,
Save 42% on Practical Data Science with R with Promotional code , at ,.
,
,
,
,
, ?? ,
,
,
, and , are co-founders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at ,.  "
"
Most popular 
, tweets for Mar 26-27 were
,
Coursera offers free #DataScience courses from Johns Hopkins, Duke, Stanford #MOOC #BigData 
,
,
Watch ""Statistics with R for newbies"" - 58 videos playlist #rstats ,
,
Watch ""Statistics with R for newbies"" - 58 videos playlist #rstats ,
,
,  "

"
,
,
,
,
,??,??,??,??,??,??
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "






"
By Gregory Piatetsky, Apr 10, 2014.
,
, is an open source machine learning server for software developers to create predictive features, such as personalization, recommendation and content discovery.
,
,
in PredictionIO correspond to the software application that is using PredictionIO.
An App can have many Engines, but each Engine can only be associated with one App.
,
,
are what external application interacts with via the API. Currently, there are two types of Engines: item-recommendation and item-similarity. 
Each Engine has a default Algorithm, but you can fine-tune its parameters or swap it.
,
,
,
,
Some recent application examples:
,
,??,
Here is 
,
,
A recent 
, by Donald Szeto and Robert Nyman explains how to use it to build predictive solutions such as 
Top 10 Personalized Movie Recommendations.
,
The steps (under any recent 64-bit Linux distribution) include
,
,??,
PredictionIO v0.7 now supports GraphChi - a Big Data Graph Engine, and developers can now evaluate and deploy algorithms in both GraphChi and Apache Mahout on one platform.
,
Read ,.  "
"
By Gregory Piatetsky, Apr 8, 2014.
,
Winshuttle, an ERP Usability company, recently created an interactive timeline on the history of Big Data, working with 
Gil Press
,
of Forbes.com to collect and organize the information.
,
The timeline displays a pictorial history of data storage as early as the 1930s, when it began to explode, the introduction of computers, business breakthroughs, future data predictions and more.
,
It marks 1880s as the Start of Information Overload.  The 1880 U.S. Census took eight years to tabulate. US Census is taken every 10 years and it was estimated that with the technology available then, the 1890 census would take more than 10 years.
This led to the invention of the Hollerith Tabulating machine which was the precursor to IBM.
,
Here are some selected highlights from the timeline:
,
,
,??,
See the timeline at 
,
,
,  "

"
Most popular 
, tweets for Apr 7-8 were
,
Data scientists beware: P values, the 'gold standard' of statistical validity in social science are not reliable ,
,
European Commission recommends changing copyright law to let researchers use text & data mining on scientific papers ,
,
Data scientists beware: P values, the 'gold standard' of statistical validity in social science are not reliable ,
,
,  "
"
,
,
In ,
,
,
In ,
,
,
In ,
,
,
In ,
,
,
In ,
,
,??,
In ,
,
,
,
,
,  "
"
,
,
,
The open source R language has become widely utilized in businesses for 
data analysis and predictive modeling. We are happy to announce that the 
main annual event of the R community, the useR! conference will be held 
in 2014 in Los Angeles (July 1-3). All information about the conference 
will be posted on this site: ,
,
We invite your organization to become a conference sponsor. In doing so, 
your organization will gain visibility in the large R user and developer 
base. The sponsorship information is available here:
,
,
,
,
For sponsorship inquires please contact Szilard Pafka via email on 
,
,
Best regards,,
Szilard Pafka for the useR! 2014 Organizing Committee  "
"
        ,
,
,  "
"
,
Most popular KDnuggets tweets 
,(see , ) 
,
,for Jan 31 - Feb 2 were
,
,??
,  "
"
,
,Friday, April 25, 2014
,
San Diego Training and Conference Center
,350 Tenth Ave., Suite 950
,San Diego, CA 92101 
,
, $35
,
or sign up for a 
,.
,
,
,
,??,
See agenda and more information at
,
,  "
"
,
,
,  "
"
,
,
,
"
"
,
,
,
,
,
,  "
"
By Gregory Piatetsky, Feb 4, 2014.
,
"
"
Here are upcoming Feb - May 2014 meetings and conferences.  You can find the full list on KDnuggets page:
,.
,
,
,
,
"
"
,

CMSR is data miner & rule-engine suite, specially designed for business applications with database focus. It supports various algorithms useful for business databases such as SOM-based neural clustering, neural network, hotspot drill-down analysis, cross table with deviation analysis, visualization charts, and so on.
,
,
CMSR stands for Cramer Modeling, Segmentation and Rules. Cramer comes from the fact that CMSR decision tree uses Cramer coefficients as splitting criteria. Unique feature of CMSR is rule-engines. Rule-engines provide rule-based predictive model evaluation. Two types of rule-engines are supported. One is sequential rule engine. The other is Rete-like rule-engine. This works based on forward chaining and rule activation. Both supports CMSR models such as neural clustering, neural network, decision tree, and regression models. Rules use subset of SQL-99 database query language. This makes it easier for SQL-users to learn.
,
,
,
A simple example can give a glance how rule-engines work. It is for the sequential rule engine. Assume the following predictive models;,
,
,
The following is an example code of the sequential rule engine. Firstly, INPUT and OUTPUT fields are defined. INPUT variables are used in binding model parameters. Different models can use different set of INPUT parameters.
,
--- START OF CODE ---
,
// define input fields from database tables,
DEFINE INPUT,
COLUMN 1 GENDER STRING,
COLUMN 2 RACE???? STRING,
COLUMN 3 MARITALSTATUS STRING,
COLUMN 4 AGE?????? INTEGER,
COLUMN 5 SALARY INTEGER,
END;
,
// define out value types,
DEFINE OUTPUT,
COLUMN 1 RESPONSEFACTOR REAL,
END;
,
// RME rules for determining output values,
IF MODEL(clustering1) = 'Tagged' Then,
RETURN MODEL(nnet1),
END;
,
RETURN AVG(MODEL(regression1), MAX(MODEL(nnet2), MODEL(nnet3)));
,
--- END OF CODE ---
,
If ""clustering1"" returns the value 'Tagged' then ""nnet1"" is evaluated and its value is returned. Otherwise it returns the average of between ""regression1"" and maximum of ""nnet2"" and ""nnet3"".
,
Rule-engines incorporating predictive models provide very powerful application platforms. It's easier to develop sophisticated rule-based models. However sophisticated killer applications are yet to be found!
,
More information about CMSR Data Miner & Rule-Engine Suite is available from the following page:
,
,
(Academic use of CMSR is free. Serious users only.),
Dr. Ok-Hyeong, Cho from , can be contacted through ,  "
"
,
,
,

,
Who else but INFORMS ?€? the world?€?s largest non-profit society supporting the analytics and big data industry ?€? can cover this hot topic the way it should be covered.?? This new INFORMS conference will show you how to get from data discovery to real business value. 
,The focus of this conference is squarely on business with success stories and lessons learned on key issues such as how to navigate the big data ecosystem, how to build and manage data science teams, and how to bridge the gap between decision-makers, IT managers, and analytics professionals.

,
You?€?ll get best practices from hand-pickled speakers from companies experienced in leveraging big data.?? Keynote speaker is big data rock star, Bill Franks, who will speak on Putting Big Data to Work.
, at the early rate.  "
"
,
,
,
,
,
,
With so much LARGE talk about the role of Quants/statisticians and the utility of statistics, we need to restate what we view with clarity.
,
,
The data world is split into two skill sets: one for 
, and 
another for ,.??
,
,
 Inside the corporation, we will describe those who perform data analysis professionally as Business Quants.?? The heavies, the go-to guys for data analysis, who use statistical software.?? I will use this less tainted term to denote those econometricians, industrial engineers, operations researchers, statisticians, et al., who apply three toolboxes: mathematics, statistics, and algorithms. Mathematical tools, which are ""coated"" in logic and wrapped around algorithms, address complete numbers. ??
,
,
Statistical tools, coated in logic and mathematics and wrapped around algorithms, address incomplete data.?? The coatings provide rigor.?? For complete numbers, we can deduce and obtain unique results (gotta like that); for incomplete data, we must infer.?? Other algorithmic tools (logic, heuristics, and optimization) comprise a third tool box that works in both the complete and incomplete domains.?? These three toolboxes have strong interdependencies and we refer to quants as those professionals who employ them to analyze data.
,

(Business) Quants must combine their knowledge about the (business) problem with a mastery of techniques from all three toolboxes.?? This involves knowing when each tool does and does not apply.

,

During the most recent hoopla, we have heard restrictive definitions of statistics and narrow perceptions of what Quants do?€?sometimes coming from people statistics training!?? Ignore them.?? Applied statistics includes more than the table of contents from a Stat 201 book; more than the research interests of statistics professors; and more topics than time to cover them in graduate school. ??A realistic definition of applied statistics includes everything we can do with statistical thinking and statistical assumptions.

,

That written, there are those who want to play down statistics in Data Science and there are those who want to play down Data Science by not including statistics.?? To the first, I write that there is no new vision of how to approach data without knowing statistics.?? To both, I write that there is no way a term like ?€?Data Science?€? will not be interpreted to include data analysis and statistics.?? The statistics toolbox is a large irreplaceable part of the lingua franca of science.
,

Randy Bartlett, Ph.D. Computer Scientist & Statistician, , ?€? Author of ,
,
,
 ,
   "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky, Apr 10, 2014.
,
The latest update of 
,
by Jeff Kelly puts the Big Data market at $18.6 billion in 2013, growing 58%  vs 2012.
Of this amount, 
,
,??,
The low share of software is due partly to the open source type of Hadoop and much Big Data software.
,
The main drivers of growth for the Big Data market in 2013 were:
,
,??,
Below is the plot of the 70+ Big Data vendors, by 2013 Big Data Services vs Software revenue.
,
,
,Circle size ~ revenue, color is %Hardware revenue - from 0% (red, SAS) to 50% (grey, HP) to >80% (green, Dell).
,
We note several main clusters:
,
,??,
The plot below examines the last cluster of smaller companies with revenue under $100M.
,
,
,
,
See Wikibon report at 
,  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
Location: AOL HQ in New York, NY
,When: May 8th, 2014
,Web: ,
,
Open Analytics Summits are a great place for CTOs, CMOs, Engineers, Developers, Brand Managers, Data Scientists, CISOs, and more to connect, network, and learn about open source technologies and big data analytics. This 1-day event will feature speakers from Bois Capital, AOL, TE Connectivity, Council for Foreign Relations, and many more!
,
Early Bird Registration Ends April 18th, so secure your spot today!
,
KDNuggets readers have been given an exclusive code to receive an additional 15% off early bird costs! Use code 
, at registration!
,
,  "
"
,BigML , announced progress from 10,000 models in Jan 2013 to over 600,000 active predictive models by the end of 2013 (half in the last few weeks), and expects to have the millionth model created early in 2014.
,
"
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for April 10 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
By Gregory Piatetsky, Apr 11, 2014.
,
Dealing with Big Data, handling increasing larger data volumes, and scaling to more users - while keeping a fast response is a huge challenge to today's BI and Analytics vendors. 
,
,
is a fast-growing startup which leverages CPU fast memory cache technology and has a scalable, memory-optimized columnar database.  They have recently come up with a unique solution that enables its data engine to respond faster as more users run more queries - called  
,
,
One traditional approach for speeding up the response with more users is caching, but caching only helps if queries are identical, which rarely happens for BI and Analytical tools. 
,
SiSense's unique approach is to break each query into blocks and cache them individually. 
,
When a new question is asked, query blocks sitting in the cache are easily and quickly reused by simply pulling them from the recycle bin. By reusing resource-intensive query blocks across different queries, SiSense keeps the CPU and cache efficient even as the number of users and queries grow.
,
, 
,
Latest version of SiSense dropped the Prism product name and is called simply SiSense 5.
,
Free trial available at ,.  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Apr 9-10 were
,
,  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
        ,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "


"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
There are 4 major segments of viewers:
,
,
We get maximum data of the people who watch the movie online through youtube or Netflix and Who watch it in theatre. There are User reviews and expert comments on Rediff, Yahoo, NDTV, India TV etc.. We can run sentiment analysis on this data to come up with algorithm which not only will predict whether movie will be hit or flop but also collections of the movie. We can use ARIMA (Box Jenkins) model to forecast the collections of the movie. Can we use Wikipedia visits of the people to understand the visitor behavior before the movie is released? Yes. The data should include:
,
,It's all about identifying patterns in past data, melding them with current data points that are readily available, and then taking action to improve business performance. In the future, movie/film studios will find it a rarity to see their films labeled a ""Hit"" or ""Flop"" on their quarterly sheet. Instead they'll have constructed the optimal ""movie formula,"" a predictive model leveraging an array of analytics tools (and not solely social analytics tools) to predict revenue accurately and consistently. By planning releases in a more systemic way, predictive analytics will be considered just as important as the producer, director, and actor who make a film a blockbuster.
,
Related:
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
        ,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "


"
,
,
,
,
,
  "
"
        ,
,
,
,  "
"
,
,
,
,
,
  "
"
Most popular KDnuggets tweets ,(see , ) for Jan 6-7 were
,
,  "
"
,
Latest ,, (Jan 08, 2014) ,:
,
"
"
,
 
They chose companies based on their story, ecosystem, insight, and influence, and excluded industry behemoths like Microsoft, IBM, SAP, Oracle, EMC, because they run multi-faceted businesses. 
,
"
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
"
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
All webinar attendees will receive a 25% discount on a BigML subscription. 
"
"
,
Most popular KDnuggets tweets ,(see , ) for Jan 8-9 were
,
,  "
"
, had a great 2013 and is starting the new year several developments that are of interest to KDnuggets readers:
,

,
,
,
,
,  "
"
Ask Artists how they perceive mathematics. Answer will be negative. Ask mathematicians what they think about arts. Answer won't be exciting. Ask business analytics people how they use arts in their work. Answer would be big zero. Is there a connection between Analytics, Mathematics and arts? How do we add arts flavor to analytics business is the question. The answer lies in century old painting by 
,Van Gogh called ""The Starry Night"" which represents 
,.
,
"
"
Here is are free tutorial notes by Greg Lamp, co-founder of 
,, 
, on 
,
"
"
,, 
"
"
,
,
"
"
,
,
,
,
,
,
,
,
,
,
,
,
Additional information:
"
"
,
,
,
,
,
,
,  "
"
Most popular KDnuggets tweets ,(see , ) for Jan 10-12 were
,
,
,
,
,  "
"
,
,
,
,
, paper, by F. Provost and T. Fawcett from the , (which is freely available).,

,
,
,, by Eric Siegel.
,
,
,
,
,, by Viktor Mayer-Schonberger and Kenneth Cukier.
,
,
,
,
,, by Roberto Battiti and Mauro Brunato, , on the web, chapter by chapter.
,
,
,
, by Kaiser Fung, a professional statistician with expertise in marketing and advertising analytics. From the ,

,
,
,, by A. Rajaraman, J. Ullman.
,
,
,
,
,.,

,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
, is one of the most interesting, talented, and wide-ranging researchers today. He has made significant contributions and written papers in numerous areas, including,
- Web Analysis and Search: Hubs and Authorities,
- Small-World Phenomena and Decentralized Search,
- Social Network Analysis,
- Algorithms and Complexity,
- Clustering, Indexing, and Data Mining,
- Network Analysis, Management, and Routing,
- Genomics and Protein Structure Analysis
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular KDnuggets tweets ,(see , ) for Jan 13-14 were
,??
,
,
,  "
"
,
"
"
        ,
,  "
"
,
,
,??
"
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
"
"
,
,
,
,  "
"
,
Most popular KDnuggets tweets ,(see , ) for Jan 15-16 were
,
,  "
"
,, Stanford, Jan 17, 2014.
"
"
ACM SIGKDD invites submissions and sponsorships for KDD 2014
,
"
"
,
,
,
,
,
 ,
,  "
"
        ,
,
,
,
,
    "
"
By Gregory Piatetsky, Jan 19, 2014.
,
,
"
"
,
,
"
"
        ,
"
"
Most popular KDnuggets tweets ,(see , ) for Jan 17-19 were
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,  "
"
, is a web portal, launched in January in Boston, which aims to bring together the Big Data community in Massachusetts and help drive increased awareness, learning, and business activity in the advanced analytics sector.  
,
,
"
"
,
,Wednesday, January 29, 2014
,1-2 PM ET (10-11 AM PT)
,
,  "
"
        ,
"
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
Obviously, if SMBs can only afford to hire one person, they will try to get a more versatile person. 
,
,
 ,
,  "
"
,
Latest ,, (Jan 22, 2014) ,:
,
"
"
Most popular KDnuggets tweets ,(see , ) for Jan 20-21 were
,  "
"
AVERTOWN, Pa. - (PRLog) - January 21, 2014 - Hot Neuron LLC announces version 4.0 of its 
"
"
        ,
,
,
,
,
    "
"
, is an easy-to-use tool to explore graph databases.
"
"
,
,
,, Stanford University
,
Sheraton Hotel, Palo Alto, California - March 20-21, 2014
,
This ,gives a detailed overview of statistical models for data mining, inference and prediction. With the rapid
developments in internet technology, genomics, financial risk
modeling, and other high-tech industries, we rely increasingly more on
data analysis and statistical models to exploit the vast amounts of
data at our fingertips.
,
In this course we emphasize the tools useful for tackling modern-day
data analysis problems. From the vast array of tools available, we have
selected what we consider are the most relevant and exciting.
,
Our top-ten list of topics are:
,
Our earlier courses are not a prerequisite for this new course. Although
there is some overlap with past courses, our new course contains many
topics not covered by us before.
,
The material is based on recent papers by the authors and other
researchers, as well as the new second edition of our best selling book:
,
,,
Hastie, Tibshirani & Friedman, Springer-Verlag, 2008
,
A copy of this book will be given to all attendees.
,
The lectures will consist of video-projected presentations and
discussion. Go to the site
,
,
for more information and online registration.  "
"
        ,  "
"
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
 ,  "
"
,
Most popular KDnuggets tweets ,(see , ) for Jan 22-23 were
,  "
"
,
,
,
,
,
,
,
  "
"
,
,  "
"
        ,
,
,
,
,
    "
"
,
,
,

,

?€? Which nation will bring home the most medals at the upcoming Winter Olympics in Sochi, Russia?,
?€? Will any nation from Africa, South America, or the Middle East finally break through and win a medal?,
?€? Why do some nations win a bundle of medals while others win only a few?,
?€? Can data mining give us the answers to these questions?,

,
This last question came into my mind four years , ago after the Winter Games in Vancouver. As a data miner working with Discovery Corps, Inc., I use data about the past to predict the future all the time. We help businesses decide which potential customers are the most likely to want their product or service. We help non-profit organizations predict which small-dollar donors have the potential to become big-dollar donors. If an organization has data on the past, we can help them predict the future. So I knew that data mining techniques could give us an estimate of the number of medals each nation might win; but I wondered how close we could get to the actual outcomes.,
It was a tantalizing project. My mind immediately began to analyze the problem. What is it about a nation that causes it to win medals at the Olympics ?€? and would I be able to find data on those characteristics? Wealth had to play a part. A nation whose people are struggling to survive is not going to have many individuals with the leisure time for recreational pursuits like becoming world class in a sporting event. Also, geography might be part of the equation. I was going way out on a limb here, but I didn't think a nation like Western Sahara would probably bring home a lot of medals at the WINTER Olympics! The other thought that immediately struck me was that, in order to win a medal at a sport like downhill skiing, a nation has to have mountains. Clearly, I was going to need to start collecting data ?€? as much as I could ?€? about the nations of the world. (That is, after I got my boss's okay to pursue this project when we had some down time.),
,
As data miners know, the data you expect to tell you the story isn't always the stuff that actually does the job , so I decided to cast my net as wide as I could, gathering as many different pieces of data as possible. I wanted all kinds of data on the nations of the world, even data that I didn't expect to be relevant to the outcome (See Appendix B for the list of data I eventually used.) And in fact, a column of data that I thought would be irrelevant and might easily have deleted turned out to be the single most useful variable in predicting the number of medals a nation would win! Fortunately, I was able to find data in many categories:

,
?€? Economic,
?€? Population,
?€? Human Development,
?€? Geography,
?€? Religion,
?€? Politics and Freedom,

Thankfully, there were some good sources out there , and I collected enough data that I felt I had a good chance to predict some meaningful outcomes. But would it be enough? There is more than one way to go about predicting the medal count at the Olympics, and the route before me was the ""30,000 feet"" approach. Far from having information on individual athletes in the various events, I would be working entirely from data about nations., Excellence in anything has a lot to do with individual motivation. Instead, I would be approaching the problem from perhaps the most aggregate viewpoint possible. Then again, what might I learn about nations while studying their ability to produce excellence? Yes, I could probably make better predictions if I had the resources of a news organization, gathering experts on every sport, predicting the winners in each, and summing them up into national totals. But that wouldn't tell me anything about the great questions ?€? the 'Why?' questions. Why is a nation able to produce excellent individuals? What factors contribute to such success? If I found answers to these questions, perhaps those answers might cross over from athletic excellence to other areas of human endeavor: science and technology, the arts, theology, etc. Well ?€? that was getting way beyond the original scope of the project. For the time being, I would just focus on predicting the nations' medal counts in Sochi.,
,

Once I had married the data on the nations to their medal counts in the last two Winter Games, my team at Discovery Corps and I could begin exploring it and preparing to build a predictive model. We decided that we would first use a logistic regression to predict which nations would win at least one medal and which would come home empty-handed . As we got the results from profiling each variable against our outcome (medals > 0), immediately the most useful variable of the bunch showed itself ?€? and it was a real shock! I had dreamed up this project after watching the Winter Olympics, but I knew I'd have to wait four years for my chance to predict the outcome at the next Winter Games. So we decided in the interim to predict the medal counts at the London Summer Games of 2012 ., When we picked up the data again this year to make our Winter predictions, my subconscious data miner's habit of not deleting data kept me from removing the column of medal counts from the summer games. To our shock, the medal count from the preceding summer games was the best variable for predicting a nation's medal count in the winter games! At the last two Winter Games, no nation won a medal without having won at least one medal in the preceding Summer Olympics. I never expected that! Our predictive model would ultimately fill in a zero for the anticipated medal count in Sochi if the nation did not win a medal in London. Also during the profiling stage, we saw other variables rise to the top: migration rate, doctors per thousand people, latitude of the capital city, value of the nation's exports, and some measures of gross domestic product. Ultimately, once we built our logistic model, it had a 96.5% correct rating. Not too shabby! (Correct predictions included those instances where we predicted the nation would win a medal and it did as well as instances where we predicted a nation would not win a medal and it didn?€?t. All others outcomes were ?€?misses?€?.),
Since our goal was to predict how many medals each country would win, we needed to go beyond the binary outcome the logistic regression used (simply whether the nation would win a medal or not). So we decided to create a linear regression model that would predict actual medal counts. And for readers who are interested in the nitty gritty details, we also had to scale the results of our linear regression to the correct number of medals being awarded this year. (Every four years the number of events changes, as some new events are added to the program and occasionally some are removed. Thus the total number of medals ebbs and flows.) So we put together the linear regression, scaled it, and got our results!,

,

,
The table below shows our predictions. 
,
,
"
"
SAS, Cary, NC, Jan. 23, 2014.
"
"
,
Most popular KDnuggets tweets 
,(see , ) for Jan 24-26 were
,
,
,  "
"
,

,

The five-part series will show you how to build better and more useful models with modern predictive modeling techniques such as regression, neural networks and decision trees.,

, to watch the entire series on demand.
,
Fung talks about the importance of statistical literacy, particularly when it comes to data and graphics in mass media and communicating about data in a business setting.
,
,
,
,
,
These webcasts are for advanced analysts, scientists, engineers and researchers interested in learning how predictive modeling can help their companies use the data they have today to better predict tomorrow.
,
,   "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
,
,
,, Making Data Work
"
"
,
,

We have all heard the clich?? of the 80/20 principle: 80% of impact comes from 20% of effort (or 20% of your customers provide 80% of your revenue. In actual practice everywhere I?€?ve looked it is usually closer to the ?€?70/30 principle?€?, but the concept still applies). Regardless of the actual numbers, the principle is rarely applied to marketing activities.,

The latest trend is ?€?Big Data?€?. The original concept of Big Data was the concept of using all of the information a company collect that was being thrown away due to costs and capacity constraints. With the rapidly declining cost of storage and retrieval, combined with machine learning, we should be able to find insights in all that ?€?garbage data?€? and use it to make better decisions in the core business. At least that?€?s the theory. As far as the basic theory goes it?€?s all true, but it?€?s not the full story.,

Like a lot of trends, the drive to mastering Big Data has gone a little overboard. Google searches for the term ?€?Big Data?€? has grown from practically nothing in 2010 to almost 200,000 searches a month by the end of 2013 [see chart from Google Trends below], and people selling products and services have caught on (The suggested Paid Search Bid for the term ?€?Big Data?€? is now over $9 per click. To put into perspective the term ?€?Las Vegas Hotels?€? only has a suggested bid of $3.71, and that term is far more action oriented than the generic ?€?Big Data?€?). Products or services that were once described as analytic or database or statistical four years ago has suddenly morphed into ?€?Big Data Solutions?€? (I?€?ve heard Nate Silver?€?s work been described as ?€?Big Data?€?. Mr. Silver does straightforward statistical analysis and communicates it in a compelling way. But he is not, at least in his popular blog, doing ?€?Big Data?€?). Consulting firms pitch their ability to help companies ?€?leverage?€? their ?€?Big Data?€?. It has become so ingrained in company cultures that to say you don?€?t want to use Big Data is a bit like saying you are against data-driven decision making. It would be career suicide to say Big Data is a waste of company time and resources.,

But that?€?s exactly what I am going to do.,

,

,
I am not going to argue that Big Data when done properly cannot create value. What I will argue is that if you are not already dominating your field and mastered all of the basics, then any dive into Big Data is just chasing a trend and a waste of your team?€?s time.,

I think the best way for me to drive home the argument is with a series of examples.,

,

Recently the head of sales and marketing for Fortune 500 company asked me how he should best use the browsing history of his customers to determine the priority of sales follow-up calls. This is a classic example of Big Data ?€? its data that was being collected (somewhere) and thrown away. And that data could definitely tell us something more than we know right now. Sounds like a no brainer, right? He thought so.,

I told him not to bother.,

There are three reasons why (As an alumni of McKinsey I have learned there are always exactly three reasons for just about everything):,
,
,
Let?€?s start with #3.,

Here is what I told him I would do if I was in his shoes:,

First, understand what your current follow-up procedure is. Do you have a standardized follow-up protocol that has high compliance across your entire organization, or is every location doing it their own way? The answer was the latter. If you haven?€?t standardized the basic flow, then what are you going to do with the new information you get from the Big Data? Are you just going to share it with the field and hope they all use it the way you want them to? Are you going to take that data with a bunch of other stuff you have and try to change the way each location is operating.
,
I would imagine the hope is that you will find some insights and then find a way to implement them. But the issue is you haven?€?t proven that you can get a standardized program implemented now. What makes you think you can get a standardized method with Big Data implemented? If you can?€?t you have just wasted all the time you spent getting the data. You are left with an insight that doesn?€?t drive any impact.
,
Instead, Step One needs to be: Find out which parts of your sales organization are most effective. Really understand what they are doing. Then create a program to roll out that effective method across the entire organization.
,
This should be easy in theory, but it is hard in practice. Hard or easy though it is absolutely essential to get the program right before you start trying to do the hard theory on top of it.
,
OK. So now you?€?ve done that (or someone else did it before you came along). You have a well-functioning sales organization with standardized procedures that steals from your best 10% of sales people and apples it to your entire company. Now you are ready to apply Big Data, right?
,
Not yet.
,
The next step, before you go and get more data is to look at what you already have and see if you can find what the big drivers of performance are. They are there and they are almost always obvious. In this case the organization collected a wide variety of information from any given lead well before a sale happened. Someone in your organization should be able to do a series of simple logistical regressions to see which characteristics are most predictive of a sale.
,
Want to know what the magic bullets were for this company? Get your pen ready:
,
,
,
It turns out that leads go stale over time. 90% of their sales came from leads that were less than 12 months old. 50% from leads less than 3 months old. This isn?€?t rocket science, but an easy first insight is: Stay in touch with, and put more effort into, your new leads.
,
,
,
If the lead says they can?€?t afford you, it turns out they (usually) can?€?t. Or to put another way, the lead who says he , afford you is , to buy your product.
,
,
,
When people say they need to buy right away, not only do they often need to buy right away, but they are also more likely to buy ever.
,
??

You might have been able to guess all three of these (or one of your top sales people might) or you might not. But it doesn?€?t matter. You definitely don?€?t need big data to find these insights.
,
The next question is: Are you using those three insights now to manage your sales funnels?
,
In this case the answer was ?€?No.?€?
,
So step two is: ?€?Do that.?€?
,
Create a dynamic score for every lead and then use that score to weight how much time your sales people spend on each potential customer (that sounds hard, but remember, you just need to use those three facts above to create that dynamic score). Collect the data. Depending on the length of your sales cycle go back and see the impact. Run some regressions on the impact of speaking to a lead with a future sale (or any other interim activities that could predict a future sale). You should be able to determine the value of a phone call as a function of the predicted value of the lead. Now you can use that (along with the cost of your sales people) to determine if you are over- or under-invested in sales people as well as where the sales people should be spending their time.
,

?€?But wait Ed: won?€?t the customer browsing history help do all that even better??€?
,
It might. But the fact is, right now, you don?€?t have that data. Take a look at what you do have that you are not using. If it turns out that you have a standardized sales process that utilizes the methods of your best sales people across your entire organization, and you are using the data you already have that is readily available to add quantitative rigor to that process, then come back and talk to me about going out and looking for more esoteric data.

,

,
,
Actually collecting it is not that hard ?€? but turning it into anything that is usable is hard. Let?€?s say for example you have a log-in process on your website (this company didn?€?t). In that case, you can tie the browsing information of someone who is logged-in with that person?€?s information in your CRM system (wait: Don?€?t have a CRM system that can do that? Maybe it?€?s harder that you think already). But what do you do about the computers visiting your site that are not logged in? Well there are solutions for that too. You can ?€?cookie?€? the computer (drop a small piece of code on their browser that allows you to identify the computer if it comes back) and then tie it back to the last time that computer was logged in. Chances are this is the same person revisiting, but not bothering to log in this time. What if more than one account was logged in on this computer? Hm. Maybe use the last person to log-in. Or maybe use the person who was logged in most frequently. Maybe hold that data and wait for the next time the computer is logged in and assign it to that account? What if in the same session someone logs out and then browses and then logs back in with a different account. Was the first account really the second account that didn?€?t realize they were logged in as someone else? What about computers that continually switch between two different accounts? Getting complicated enough yet? And have you even talked to your IT team about how they are going to build the infrastructure to tie all these computer and account structures together? This is why people insist that marketing is hard ?€? they are constantly running down these rabbit holes.
,
But wait. At least email is easy, right? You send someone an email, you know who that person is. So if a link from that email is clicked on, obviously that lead has re-engaged. Well, unless that lead forwarded your email to someone else and they clicked on it. Could that happen? (The answer to that question is, ?€?Yes. All the time.?€? At APFM we get a significant number of new ?€?leads?€? from our email program, even though we only send emails to people who are already leads.).
,
So collecting this data right is not easy. It?€?s not impossible by any stretch: Expedia had an entire team committed to doing it. But collecting it and putting it in usable form is hard. It is a significant investment and easy to get wrong.

,

Then what do you do once you have it? How much impact will it have?
,
This varies a lot. At the simplest level I would bet a dollar that leads that were lying dormant ?€? they had no contact with sales ?€? that started interacting with your website or opening/clicking on emails that you were otherwise going to ignore are worth something. It would likely be worth the time of your sales people to re-engage. Some of those leads would be false positives, but I?€?ll bet the program would be ROI positive based just off the incremental cost of making those additional phone calls.
,
But I also bet that after including the cost of building the infrastructure to do it (all that computer/account/CRM matching) and the cost of finding the insights (turns out people who can find great insights in data are not cheap) and the cost of the rollout of the program to your sales people it will stop being ROI positive at some point. Even if it doesn?€?t, the ROI will still be a LOT lower than getting those basics right.
,
I am basically saying that these Big Data solutions are likely valuable ?€? but only marginally. And that there are a LOT of non-marginal value that is likely lying right in front of you. I challenge you to work your way through this book and make sure you have done all the ?€?Easy?€? stuff before you dive headlong into the sexy new things like Big Data. Even a company like Amazon, which from the outside sure looks like they have the basics in order, has gaps. I met the head of their Paid Search technology and after asking a few questions we both came away realizing they had a significant gap. I?€?ll talk about that specific gap more when I get to the Paid Search Channel.

,

Before I leave the world of Big Data I want to give you a few other examples of places where ?€?The Basics?€? get you 90% of the way to the final solution and intense analytics add value ?€? but only marginally. I hope these example drive home the point that you are much better off getting the basics launched than you are waiting for the final perfect solution. Execution trumps the perfect answer almost every time.

,

,
,
It turns out being at the top of a list matters. A lot. Online, the most watched sporting event in the 2012 Olympics wasn?€?t track or beach volleyball or diving or soccer or any of the sports you would think of. It was Archery. Archery is first in the alphabet and someone at the station decided to list the sports alphabetically so archery became the big winner. People choose the stuff that is at the top of the list.
,
At Expedia we had a team dedicated to sort order. Since the hotels at the top were selected most often and Expedia wanted to maximize its profits they wanted to make sure the hotels at the top both had high margin and a high likelihood to convert into sales. Their algorithms were very sophisticated and were constantly A/B tested against new ones. But here is the thing: The best algorithms were only a little better (~10%) than a random sort order at generating profit for Expedia. 10% is a lot, but random is a pretty terrible bar. If instead you ranked the hotels by their historical conversion rates times their margin ?€? about the simplest reasonable model one could come up with ?€? the fancy models only did a tiny tiny amount better. Their incremental improvement was so small in fact that management , It was better to use the simple method and then modify it based on the choices of the Market Managers (the people on the ground working with the hotels). It was more effective to use sort order as a negotiation tool than it was to use it directly to ?€?maximize?€? profit. Again: Simple beats out complex. Hard loses to Easy.

,

,
,
When you advertise with Paid Search on Google you get a ?€?Quality Score?€?. The score is supposed to represent how good your advertisement is. What you end up paying is a function of what you bid in an auction and the ?€?quality?€? of your ad. But here?€?s the thing: Google has an advanced algorithm for Quality Score, but, when you are trying to reverse engineer that algorithm it turns out that you can match it almost exactly just by looking at Click Through Rate. Basically CTR = QS = Quality of the ad. Google has the resources to build a ?€?perfect?€? algorithm that gets basically the same answer as a simple one. Do you have the profit margins and limitless resources of Google to get a perfect answer when the East one will do?

,

,
,
Collaborative Filtering was the ?€?Big Data?€? of eight to ten years ago. The idea was you could take any individual customer and try to find other customers that were similar to them (based on demographics, but most importantly historical behavior). After you matched a customer to others you could send targeted offers to him or her based on what their matches ended up buying. The most common example of this is Amazon?€?s Recommended Purchases (I believe Amazon is best in the world for this right now and getting better all the time). This works for Amazon because they have a TON of data on a TON of customers. Most importantly they have a TON of purchase data on each individual customer (as opposed to just browsing history). This vast number of relevant data points lets them avoid the missteps that can happen when the dataset is sparser. Even with all of these advantages and incredible investment in the technology, Amazon still makes a ton of mistakes. Order a baby book for a friend and forget to mark it as a gift and you will be getting Richard Scary recommendations for months.
,
What?€?s the alternative?
,
I?€?ll give you two:
,
, Start with Random.
,
Chances are your company, unlike Amazon, doesn?€?t sell everything. Chances are your products or services or content already has some similarities. Try making recommendations for other products randomly. I guarantee it will do better than not making recommendations at all. If you aren?€?t doing this, don?€?t even think of developing a collaborative filtering tool.
,
, Recommend your best selling products (or best-selling services. Or most read content). Take one more step and recommend the best-selling that the customer hasn?€?t already purchased.
,
, Take a single product (service/content) view. This is super-simple collaborative filtering. Look at every customer who bought that product (read that content). What was the second most common product purchased? Recommend that.
,
I?€?ll bet you find that you make a ton of impact implementing step one. Then you get a small amount of more impact by launching step two. Then you get hardly any impact at all from doing step three. There are huge diminishing returns from getting better and better at this stuff. If Amazon can get better and make an additional 0.1% that?€?s amazing for them, but you likely have bigger fish to fry.

,

,
,
There are companies that make a living selling you data about your own customers. Their entire pitch is to help you target your customers better. If only you knew that some of your customers owned homes or had high income or subscribed to Entertainment Weekly, maybe you could do a better job targeting them with your segmentation programs. By now I hope you can guess what I?€?m going to say next?€?
,
There is nothing wrong with adding this data on top of your internal data, but only if you are already using your own data and using it really really well. This does not apply to many companies on the planet, so be very sure your company is killing it before you jump into buying more. Obviously the vendors selling you this extra data will never tell you that ?€? their businesses would stop functioning if they only sold data to the companies that could really take advantage of it. The consultants that want you to employ them to implement the data won?€?t tell you that. The gurus who want to talk about the power of Big Data won?€?t tell you that. Effectively the companies that can?€?t use the data are subsidizing the price for companies that can.

,

,
,
Hardcore analytics (and Big Data) can add value, but only marginally and only for companies that have already mastered using the data they already have. The ?€?obvious?€? information from your own data can get you 90%+of the total impact you will get from insights. The hard part is executing the basic insights across the organization ?€? so start there before you go looking for more intellectual stimulation.,

Originally posted at ,/,

Edward Nevraumont can be reached through ,.
,
,
 ,  "
"
Most popular KDnuggets tweets ,(see , ) for Jan 27-28 were
,
,  "
"
,
,
,
, ,
,
, ,
,
, ,

,
, ,
,
, ,
,
, ,

, ,
,
, ,

,
, ,
,
, ,

,Dr. Andrew Jennings is FICO's chief analytics officer and head of FICO Labs.  Before joining FICO in 1994, Andrew served as head of unsecured credit risk for Abbey National plc.  He also served as a lecturer in economics and econometrics at the University of Nottingham.  He holds a Ph.D. in economics. He blogs at ,.  "
"
,
,
,
,
,
,  "
"
,,
,a new video series
,from Salford Systems.
,  "
"
,
,
,
"
"
        ,  "
"
,

,
Corporations have long invested in the practice of ?€?market research?€? as a means to identify or uncover ?€?insights?€? they can capitalize on to build a competitive advantage in the marketplace.?? However, Market Research departments are often perceived as a low value add ?€?cost center?€? to an organization, most likely due to a company?€?s inability to ?€?act?€? on generated insights rather than the inherent value of insights themselves.?? As a corollary, in the new age of rapid corporate cost reduction, Market Research departments are feeling the heat, and are being asked to prove their value in quantifiable ways, much like their peer groups in operations, sales, and supply chain have been doing for years.?? Therefore, Market Research & Consumer Insights professionals not only have to drive impact, but they have to prove this impact as well.
,
The reality of truly quantifying the value of market research has long been deprioritized or avoided for a very good reason.?? It?€?s hard.?? Being able to successfully connect an insight generated from a report to a decision made by a group of consumers that is different or better than the alternative is difficult exercise, given all the links in the chain, changes along the way, and potential for breakdowns throughout the process.?? However, research and insight departments can help add transparency to this by focusing on dissecting 3 key areas when investing in new research, analytic, or consumer insight related projects:
,
,
,
2. , ?€? ,
,
??
,
, ?€? ,
"
"
,
,
,
This course follows on from , and provides a deeper account of data mining tools and techniques. Again the emphasis is on principles and practical data mining using Weka, rather than mathematical theory or advanced details of particular algorithms. Students will work with multimillion-instance datasets, classify text, experiment with clustering, association rules, neural networks, and much more.
,
,
The 5-week course will begin in late April 2014, with enrolments opening in early March. Students should have completed , (which will precede it), or have equivalent knowledge of the subject.
,


The course features:
,
For more information, visit:,
,
,  "
"
,
,

,
,


, makes data exploration much more efficient by automating the basics. We think the least interesting part of data analysis is all the small, rote decisions, like whether a particular situation calls for a spearman correlation versus a pearson correlation. So we automate those basic data exploration decisions: you tell Statwing which variables to relate to each other, and we pick the right analyses. So in three clicks you'll see the results of the correct statistical analysis, a visualization, confidence intervals, effect sizes, and more.

,
,

,

That might not be sound too impressive, since you could already do those things yourself. But it's much easier to clean a 500-variable dataset when you can click three times and get descriptive statistics and visualizations for every variable in a few seconds (including histograms with appropriately sized bins). Or when you can relate one variable to 40 others, with the appropriate statistical test run and the appropriate visualization for each of the 40 relationships (including crosstab heatmaps, binned scatterplots, and panelled histograms).

,
,
KDnuggets readers will mostly be interested in our advanced output, but we also have basic output for less sophisticated analysts, which translates the statistical findings into plain English sentences. Since Statwing is in the cloud, you can share your analyses with others and they'll be able to understand the results, even if they don't have statistical training.

,
,

,
,
The best way to understand Statwing is to try it out. I'd suggest our General Social Survey dataset, with ~40k observations and ~400 variables (,??about dataset,??,). Incidentally, we're currently holding a $1500 contest to see who can come up with the most interesting analyses of that dataset?€?contest ends at 11:59 PST on January 30th.

,
,
,

Statwing is has a 2-week free trial. Pricing is $25/month for files up to 50mb, and $100/month for files up to 500mb (and we can do custom arrangements that go into the GBs).

,
,

Try it out, and if you have any feedback definitely let us know through our integrated chat box. Cheers!

,
,
Greg Laughlin is a Cofounder of ,.  "
"
,

at 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,(ACM SIGSPATIAL GIS 2014),            
 November 4-7, 2014 -- Dallas, Texas, USA  ,

Introducing the 3rd GIS-focused algorithm competition, ACM SIGSPATIAL Cup 2014.,

The 2014 contest will be about map generalization, which is a very commonly used
concept in creating maps of different scales. Map generalization is used in many
GIS and business mapping applications for creating maps with as few vertices as
possible. The process of map generalization reduces the amount of data that is
required on the client to produce maps without losing the general shape of the
map.
,
For this years contest, each team will be given access to a large amount of line
string geometries that are boundaries of administrative areas along with some
point data that is used to constrain these lines. The objective of the contest
is to reduce the number of vertices in the input line strings while preserving
the topological constraints among the lines and the point data. The submitted
programs will be evaluated by the contest organizers on multiple datasets and
will be ranked quantitatively based on their correctness and speed.
,
More information about the contest is here:,
,
,
Contest Chairs:
,
Siva Ravada, Oracle Corporation, siva (dot) ravada at oracle (dot) com,
Xin Chen, Nokia Here, xin (dot) 5 (dot) chen at here (dot) com
,
Contest Masters:
,
Xi Zhang, Illinois Institute of Technology, xzhang22 at ,
Zhi Liu, University of North Texas, zhiliu at ,  "
"
,
"
"
,

,


,
,


KDnuggets is a media sponsor of a 2014 text-analytics market study being
conducted by leading industry analyst Seth Grimes
,
,


If you are a current text analytics user, or if you're exploring ways to
tame text, please take part in Seth's survey, at
, .

,

Seth is researching how organizations apply text analytics, at the
information they're analyzing, solution selection criteria, and return on
investment. He plans to release findings via a free report that will
benchmark text analytics usage and cover perceptions about text
technologies and solutions.

,

The survey has 21 questions. Your response is anonymous (unless you
provide contact info) and should take 5-10 minutes. Consultant and
researcher responses are quite welcome. If you're not a direct text
analytics user, but your customer experience, survey analysis, social
media analytics, e-discovery, research, or other solution relies on text
analysis, please take the survey. Respond by February 21, 2014 please.
,


Thanks for your participation!  "
"
,
Most popular KDnuggets tweets ,(see , ) for Jan 29-30 were
,  "


"
,
provides a conceptual understanding of multilevel linear models (MLM) and multilevel generalized linear models (MGLM) and their appropriate use in a variety of settings. Students will learn how to:
,
,
"


"
,
Most popular , were:
,
,
,  "


"
,
Finding planets around other stars is hard. But there?€?s an important trick we can use to study extrasolar planets that also teaches us about where the planets come from.
,
Planets form from vast clouds of gas, dust, and chunks of rock---clouds that take the shape of disks with stars in the center. We can find out where planets are forming and where planets probably remain today by searching for stars that are surrounded by these disks. Finding these disks, called ?€?debris disks?€? or ?€?YSO disks?€? depending on their age and gas content, has been a major quest of astronomers for the last three decades.
,
NASA?€?s WISE mission probably made images of thousands of debris disks and YSO disks. Alas, these disks are buried among images of millions of other kinds of astronomical objects like galaxies and nebulae, and mixed in with images that contain artifacts created by the telescope itself. In Disk Detective, you will help astronomers find these disks, homes for extrasolar planets.
,
Help classify the images at ,
,
,
,
Disks are not the only kind of celestial object that glows bright in the infrared. Galaxies, asteroids, active galactic nuclei, and interstellar clouds of dust all emit at these wavelengths. Computer algorithms designed to automatically search for disks are easily thrown off by these sources of confusion, so we need to examine all these disk candidates by eye to make sure they really are stars with disks, and not some other kind of interloper. Also, computer programs can only detect what we tell them to measure. But you can do much more than that. With a large all-sky data set and your curiosity, the possibilities for unexpected discoveries are vast.
,
,
Disk Detective is the first NASA led and funded Zooniverse project. It is also the first NASA led and funded crowdsourcing project whose main goal is to produce publishable scientific results. It was built using seed money from NASA's Science Innovation Fund.
,
Learn more and participate at ,  "
"
,
South Bend, IND. (February 4, 2014)
,
,Aunsight,, a new data science platform from Aunalytics, will allow data scientists to focus on what matters most ?€? solving big data problems. 
,
,
Aunalytics will unveil the software at the 2014 Strata Conference in Santa Clara, California, The start-up data analytics company has been developing the data science platform for 18 months and operates from Innovation Park at Notre Dame in South Bend, Indiana.
,

Co-Founder and Chief Data Scientist, Nitesh Chawla, Ph.D. says?? ?€?,,?€? says Chawla, ?€?,.?€?
,
,
,
Users can easily design and customize powerful workflows to integrate disparate data sources, analyze data, implement new algorithms via seamless plug-ins, and produce powerful insights that answer important business questions. These workflows can further be configured to re-compute the results on a given schedule or be reused altogether with separate input sources. And, of course, visualizations can easily be connected to workflow results through a simple API. Aunsight, lays the groundwork for collaborating parties to generate actionable insights from data, embrace data-driven culture and thrive.
,

For more information visit Aunalytics online at
,.  "
"
,
Most popular KDnuggets tweets ,(see , ) for Feb 3-4 were
,  "
"
,
,
,
,
,
,
,
,
Originating from Facebook, LinkedIn, Twitter, Instagram, YouTube, and many other networking sites, the social media shared by users and the associated metadata are collectively known as user generated content (UGC). To analyze UGC and glean insight about user behavior, robust techniques are needed to tackle the huge amount of real-time, multimedia, and multilingual data. Researchers must also know how to assess the social aspects of UGC, such as user relations and influential users.
,
, is the first focused effort to compile state-of-the-art research and address future directions of UGC. It explains how to collect, index, and analyze UGC to uncover social trends and user habits.
,
Divided into four parts, the book focuses on the mining and applications of UGC. The first part presents an introduction to this new and exciting topic. Covering the mining of UGC of different medium types, the second part discusses the social annotation of UGC, social network graph construction and community mining, mining of UGC to assist in music retrieval, and the popular but difficult topic of UGC sentiment analysis. The third part describes the mining and searching of various types of UGC, including knowledge extraction, search techniques for UGC content, and a specific study on the analysis and annotation of Japanese blogs. The fourth part on applications explores the use of UGC to support question-answering, information summarization, and recommendations.  "
"
,
,
Latest ,, (Feb 05, 2014) ,:
,
"
"
,
Yann LeCun, a leading researcher on Deep Learning, who was recently ,,?? reports that his?? former student , won the 
, on Kaggle.
,
Pierre entry was amazingly good - 98.9% correct.
He ,
,

,
The second entry by Orchid seems to be DeCaf, which used convnet feature extractor from Berkeley.
,
This follows Pierre's 1st place in the ImageNet localization competition and his recent post-competition record on the ImageNet detection task.
,
, is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the back-propagation algorithm.  "
"
        ,  "
"
,
"
"
Most popular KDnuggets tweets ,(see , ) for Feb 5-6 were
,  "
"
,
,Wednesday, March 5, 2014
,1-2 PM ET (10-11 AM PT)
,
,
,
statistics, classification, data mining, anomaly detection, the foundations of statistics, multivariate statistics, classification methods, pattern recognition, computational statistics
,
,
,
,, a two-time former president of the Royal Statistical Society, explains the commonplace nature of extraordinary events in his book ,. He joins us to discuss examples from his book and the set of laws behind chance moments in life, as well as the great importance of the discipline of statistics. Hand was made OBE in 2013 for his service to research and innovation.
,
,
,
,
,
,
,??
,??
,.
,??  "
"
,
,

Top speakers from across the world of business analytics and business intelligence ?€? including experts from Intuit, Microsoft, SurveyMonkey, Wells Fargo, and more ?€? will be sharing how to get the most out of your data at the , in San Jose, CA, May 7-9.

,

Featuring , across five topic tracks, the 2-day main conference program will cover the latest in data analytics and visualization, predictive analytics, Big Data, information architecture and delivery, information strategies, and more ?€? delivering real-world insights and solutions, prescriptive guidance, best practices, and strategic vision for analyzing, managing, and sharing business information and insights.

,

?€?Almost 900 business analysts, data scientists, data architects, and BI and IT pros from 26 countries joined us last year to connect, share, and learn how to make better data-driven decisions and take their companies to the next level,?€? noted PASS President Thomas LaRock. ?€?This year?€?s session lineup is even more exceptional. Whether you?€?re a BA/BI beginner or pushing the analytics envelope, if you?€?re passionate about the power of data to transform business, this event is for you.?€?

,

The PASS BA Conference will deliver thought-leading keynotes and a full schedule of world-class technical sessions, hands-on solution showcases, and networking opportunities to help attendees get the most out of their organization?€?s data.
,

The event will also showcase six full-day , May 7 with some of the industry?€?s leading experts:,
,
You can get a sneak peek at the PASS BA Conference with these , from event speakers.
,

,
Save $150 when you??,??using discount code??,.  "
"
Here are 10 Emerging Indian Analytics Startups, as selected by Analytics India magazine:,

,
Founded in Singapore in 2012 and with a development centre in Chennai, Crayon?€?s vision is to create a big data platform that can process large volumes of data from within the enterprise and from external sources including social media. This data is cleaned up and various algorithms are run on it to serve different verticals.
,
,
Founded in February 2012 Flutura places themselves at ?€?the cusp of M2M and big data?€?, M2M standing for machine-to-machine. California based technology magazine CIO Review has recognized Flutura as one of the Top 20 Most Promising Big Data Companies Globally. Flutura has also been recognized by TechSparks2013 as one of the Top 3 startups out of India.
,
,
Founded in New Jersey Axtria is an ISO 27001 certified advanced analytics and business information management company with locations in California, Arizona, Georgia, Virginia, and Gurgaon, India. Axtria builds sales, marketing, and customer management data analytics tools. The tools help companies analyze their current customers to find out how to get more and how to keep the ones they have.
,
,
Founded in 2008 at Trivandrum Flytxt is a leading provider of Big Data Analytics powered solutions with a focus on enabling mobile operators to derive measurable economic value from subscriber data. Flytxt has won many industry awards and recognitions like Gartner Cool Vendor, NASSCOM Emerge 50 League of 10, Aegis Graham Bell award for innovation in Mobile Advertising, BID International Quality Award, Red Herring Asia 100. 
,
,
Founded by four serial entrepreneurs, has built a patent-pending software product (Sapience) that gets Enterprise Effort and Time Productivity ?€? effortlessly. Sapience delivers 20+% increase in work output for companies whose employee uses computers to perform their job. Last year have been great for the company while bagging some of the very prestigious awards in the Industry.
,
,
Founded in Kolkata, SIBIA is a technology integrated predictive analytics company based focusing on marketing and sales optimization for retail and consumer products brands. The company has been established in early 2013 by industry professionals with long experience in predictive analytics and related technologies. 
,
,
Ideal Analytics is an Indo-French Technology Joint Venture Company that has its Corporate HQ and Office in Kolkata, India and the European Office in Paris, France. It gives business users an intuitive interface for exploring their data and taking fact based business decisions. It is available both on the cloud and in the on-premises avatar and is capable of connecting to data coming from multiple heterogeneous sources. 
,
,
FORMCEPT provides a highly scalable content mining infrastructure through proven open source technologies that includes Hadoop, HBase and Solr. FORMCEPT?€?s C3 (Classify, Compare, Correlate) Semantic Engine is a state of the art technology which provides classification, comparison and correlation for any type of content.
,
,
IQR Consulting Data Analytics provides strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. 
,
, ,
Stat Decision Labs is a provider for Integrated Data Mining & Mobility solutions that helps organizations anticipate business opportunities, empower action and drive impact. With a strong extensibility framework for predictive analytics and mobility solutions, Stat Decision Labs provides a very powerful Decision Management platform.
,
Read ,.  "
"
Most popular KDnuggets tweets ,(see , ) for Feb 7-9 were
,  "
"
,
,
,
,
,
,
Read more about these tests on the , by Software Advice, a 
, that reviews business intelligence software.   "
"
,
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
 ,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
"
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
"
"
,
,
,
,
,
,
"
"
,
,
,
,
,
,
"
"
,
,
,
,
,
,
"
"
,
,
,
,
,
"
"
,
,
,
,
,
,
"
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
"
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
    "
"
        ,
,
,
,
    "
"
,

Whether you?€?re steering an enterprise, championing an analytics program, driving a venture capital-funded data-modeling product or piloting your own consulting practice, there is no way you have not become aware of the promise of big data and bigger analytics.,

Companies often deal with an opaque marketplace where riskiness is rated less accurately in some organizations than others. That said, some businesses, afraid of taking on new risks, stick to their own market niches and avoid new ones ?€? preventing their abilities to be competitive in the larger marketplace. Others are blazing trails using newer technologies, data sources, modeling approaches and electronic connectivity to better manage risks in an increasingly mobile world. The following framework helps identify how companies are structured to compete on analytics.,
,
Now we?€?re at the beginning of a long rally race of analytic improvements ?€? from newer, better data to smarter, faster algorithms. Enhancements will include broader, more scalable platforms and will access unique sensors ?€? spectra, spatial, temporal ?€? and micro/macro levels of structured and unstructured data. All that will help generate insights into individualized, massively personalized and localized information while getting even more power out of grouped predictive parameters.
,
For example, if you have operated a car or other moving object, you undoubtedly have been assessed for your risk of loss as an operator of that vehicle and, in some manner, likely have been insured. In the past, that insurance-based risk assessment has blended wide bands of information on a few historically available generic characteristics to achieve a general-purpose estimate of prospective loss risk estimates.
,
In the future, that historical benchmark will be segmented into ever-more granular and accurate assessments. Those will then again be reinvented, recombined and refined to enhance the data-driven process, culminating in an adaptive analytic that adjusts expectations to the level of risk in each operating scenario encountered or intuited.
,
In the world of big data and bigger analytics, insurers will come to view vehicles as instrumented platforms and operators as real-time learners whose risk may change over time. Operators may drive safer and make smarter decisions about moving between locations, or they may permit distractions into their cockpit (such as texting, talking, smoking, eating and so on). How you drive, when you drive, where you drive and how much you drive are all becoming part of the context in your individual risk profile.
,
,
,
,
Some businesses apply detailed telemetry, routing algorithms, real-time weather and traffic alerts and driver/crew pairing models to manage more effectively the logistics of moving people, packages and pallets. Similarly, individual consumers make daily choices to move themselves, their passengers and their belongings along the same roadways and flyways and use all sorts of new navigation and alerting applications and devices to do so. (I?€?ve seen a mobile tablet computer go from a plane to a car to a sofa all in the hands of the same individual within one morning.)
,
Peering into the future, if a submersible helicopter car becomes commonplace, we?€?d have a truly three-dimensional driving experience. And on those journeys, we might need to dodge Amazon?€?s Octocopter self-driving delivery micro bots along the way.
,
In the ubiquity of an instrumented world, such a trend is unstoppable. Our challenge will be how we will use analytics to interact with decision-making. If consumers continue to make their own decisions, it?€?s a certainty that marketing analytics, advertising effectiveness and brand campaigning will merge into mobile and content messaging more than ever before. The next frontier will involve the layered sensing of the temporal and spatial context surrounding the customer.
,
Decisions that address emotional desires of customers resonate in the behavioral economics that underpin our financial world. The closer we can come to a customer?€?s desires, the better ?€? and better still to be able to influence demand by making customers aware of opportunities they did not know exist. That holds true for business-to-business decisions as well.
,
The paradigm shift transitions from company-centric to customer-centric and from ?€?we always have done ?€?IT?€? this way?€? to real time. That shift must be the focus of top management, which needs to take the offense and drive resource allocation for innovation and productivity to a customer-focused, real-time strategy. Executives who embrace such a process of optimization that both considers maximizing enterprise performance while minimizing risks will effectively revitalize every decision opportunity in marketing, production, distribution, logistics, operations, servicing and sales. And they will find there is no finish line when generating more shareholder value ?€? only a continual cycle of improvement and a corporate culture of data-driven, sustainable excellence.
,
,
Marty Ellingsworth is president of Verisk Innovative Analytics, a division of Verisk Analytics (www.verisk.com). Verisk Innovative Analytics is a member of the INFORMS Roundtable, and the author is a long-time member of INFORMS.  "
"
Here are 10 Most Innovative Companies in Big Data, as selected by Fast Company:,

,
For harnessing data from its planes and trains to power a new Industrial Internet, potentially saving billions.,
,
For feeding its DIY data scientists cash-prize challenges (then molding them into a consulting biz).,
,
For using a visual approach to take the guesswork out of big data.,
,
,
For playing global data evangelist by sharing its problem-solving power with cities, businesses, and universities.,
,
For embracing data scientists and supercomputers to build the hospital of the future.,
,
For analyzing millions of local climates to predict how shoppers?€? habits sway with the weather.,
,
For forging alliances to make millions of students smarter, from adaptive??-learning e??books to personalized English?? language training courses.,
,
For providing businesses with hundreds of homegrown apps to sniff out error files and keep things humming.,
,
For expanding its service to let customers dive through every social media stream available.,
,
For mining employee performance to help stanch turnover and upend HR.,
Read ,.  "
"
,
,
,
,
,
,
"
"
,
,
,
 ,  "
"
,
,
,
,
,
View the schedule: 
,
,
,
Big Data is constantly evolving and staying ahead of the latest trends and technologies is not a choice, but a necessity. 
,
,
If you are interested in attending please contact Sean Foreman at 
,, 
on +1 415 692 5514, or alternatively you can reserve your place online: 
,  "
"
The Silesian University of Technology in cooperation with IEEE International Joint Conference on Biometrics (IJCB 2014) and exclusive sponsor SensoMotoric Instruments GmbH (world leader in eye tracking) organizes: 
,
,
,
,
,
The aim of the contest is to determine how people may be identified based on their eye movement characteristic. 
,
SensoMotoric Instruments
will award an SMI RED-m eye tracker to the winner of the EMVIC
competition and for each of the two runners up a 250 Euro travel grant to the IEEE International Joint Conference on Biometrics (IJCB 2014) in September 2014 in Florida, USA.     
,
The organizers provide a dataset of eye movements' recordings in CSV (comma separated values) format. After downloading the datasets, participants may analyze it to prepare their own classification models. See 
,  for details.
,
,
To become a participant you don't need to have any special eye tracking equipment. All data needed is ready to download! You only need to have some experience in data classification.
"
"
        ,
,
,
,
    "
"
By Gregory Piatetsky, Feb 11, 2014.
,
Wikibon latest update to
,, 
posted by Jeff Kelly, indicates that Big Data market has reached $18.6B in 2013, growing
58% over the past year.
,
Of this amount, 
,
,
,??
,??
The main barriers for adoption include
,
,??
,??
Here are the revenues of companies whose main focus was ""Big Data"" - their ""Big Data"" revenue was over 50% of their overall revenue.  Only Pivotal had a non-zero % for hardware revenue - for all other companies in the table below, hardware revenue was zero.
,??
,??
2013 Worldwide Big Data Revenue by Vendor ($US millions) 
,(source: 
,).
,
,  "
"
        ,
,
,
If, like me, you are not able to personally attend
,, Feb 11-13, 2014, in Santa Clara, CA - here is the second best option.
,
,
You can watch online.
,
,
Keynote presentations , were streamed live and recorded 
,
,

Also you can visit 

http://strataconf.com/strata2014/public/content/video

,
,??
,
All times PT (California-time).
,
,
,
,
,
,
Keynote presentation recordings will be posted to the
,.



The O'Reilly Strata Conference brings together the brightest minds in data science and big data: decision makers using data to drive business strategy, as well as practitioners who collect, analyze, and manipulate it. Attend the conference and tap into the collective intelligence of over 150 of the leading data experts, network with thousands of your peers, and hear about the latest (and emerging) data tools, technologies, and best practices.  "
"
,
,
Data Scientist has been called ""the sexiest profession of 21st century"", but how do they approach Valentine's day?
,
New KDnuggets cartoon from Ted Goff offers one scenario.
,
,
,
,
,
,
,
,
Check also 
,  "
"
,, 
,
by Jure Leskovec ,, 
,Anand Rajaraman ,, 
,and 
,.
,
The first edition was published by Cambridge University Press, and you get 20% discount by 
,.
,
The second edition of the book will also be published soon. Jure Leskovec was added as a coauthor. There are three new chapters, on mining large graphs, dimensionality reduction, and machine learning.
,
There is a revised Chapter 2 that treats map-reduce programming in a manner closer to how it is used in practice, rather than how it was described in the original paper. Chapter 2 also has new material on algorithm design techniques for map-reduce.
,
Support Materials include Gradiance automated homeworks for the book and slides.
,
Thanks to authors agreement with the publisher, you can still download it free from
, .
,
The authors note if want to reuse parts of this book, you need to obtain their permission and acknowledge our authorship. They have seen evidence that other items they published have been appropriated and republished under other names, but that is easy to detect, as you will learn in Chapter 3. 
,
Download chapters of the book: 
,
,
, Data Mining
, Map-Reduce and the New Software Stack
, Finding Similar Items
, Mining Data Streams
, Link Analysis
, Frequent Itemsets
, Clustering
, Advertising on the Web
, Recommendation Systems
, Mining Social-Network Graphs
, Dimensionality Reduction
, Large-Scale Machine Learning
,  "
"
,
Graham Williams, the founder of ,, the developer of ,, free and open-source data mining software based on R, and author of
, book, has summarized useful R resources in
,
,
,
,
,
The include tools for the data miner, or the data scientist, and or the decision scientist.
,  "
"
,
Most popular KDnuggets tweets ,(see , ) for Feb 10-11 were
,
Julia: One Programming Language to Rule Them All - good for #DataScience, compiles on the fly, parallelism built-in ,
,
Data scientist cartoon: I'm too busy recommending things to experience them myself Found at ,
,
Data scientist cartoon: I'm too busy recommending things to experience them myself ,
,
,
,
,  "
"
Most popular KDnuggets tweets ,(see , ) for Feb 12-13 were
,
,
Where to start learning #DataScience for 1) statisticians, 2) coders, and 3) newbies ,
,
At #strataconf James Burke: Major results comes from an unexplored no-man's land between the disciplines
,
Where to start learning #DataScience for 1) statisticians, 2) coders, and 3) newbies ,
,
,  "
"
,
By Gregory Piatetsky, Feb 13, 2014.
,
Recent KDnuggets Poll 
, caught attention of 
David Pittman, IBM Social Media Strategist, who talked to me recently about the poll and Data Science in general.
,
Here is the summary of the podcast:
,
, (14:05 mins)
,
,
,
,
,
The ""Data Science Team vs Indivial"" Poll also was the basis of a thoughtful article in Forbes by Gil Press 
,, Feb 2014.
,
,
 ,  "
"
,
,
Each year, millions of people in the US fill out a bracket to predict the outcome of the NCAA men's college basketball tournament that starts in March (aka ""March Madness""). While the odds of creating a perfect bracket are astronomical, these odds are made better by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? 
,
Kaggle has assembled the basic elements necessary to get started with tournament prediction. The provided data covers nearly two decades of historical games, but you're also encouraged to use data from external sources. To help turn all of that information into useful insight, Intel is making its big data technologies more affordable, available, and easier to use for everything from helping develop new scientific discoveries and business models to gaining the upper hand on good-natured predictions of sporting events.
,
In stage one of this two-stage competition, participants will build and test their models against the previous five tournaments. In the second stage, participants will predict the outcome of the 2014 tournament. You don't need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. 
,
The real competition is forecasting the 2014 results, for which you'll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket.
,
Intel will present the team with the most accurate predictions a $15,000 cash prize. 
,
Get started today - predictions are due by Wednesday, March 19, 2014.
,
To learn more and participate, visit ,.  "
"
,
,

,

,
,
,
Help us out -- Check out our ,, sign up for our ,, and , about doing a data science or SciPy/NumPy ,!,
,
Check out , to take control of the deployment and management of Python, R, and internal packages behind your firewall and proxy. Integration tools and install support included.
,
,
Please note: the Anaconda default install comes with Python 2.7.
Python 2.6 and 3.3 are ,.,
,  "
"
        ,
,
,  "
"
,
,
,

Imagine that you are at a networking event to talk about your product. Several people come to talk to you; some ask specific questions, some share their past experience good or bad while some share their ideas to improve your product. At the end of the event, any good networker will make a list of how many and who came to talk but more importantly he/she will try to understand conversations, learn from it and take appropriate actions. Now let?€?s look at this example from a Social Media (SM) context. ,Social Media networks are like networking events where brands want people to come and talk to them about their experiences, likes, dislikes and give suggestions. If that is the case why do 95% of social media tools stop at just the volumes of how many people came and talked to you or about you but never delve into the details of what is being talked. Tools tell that people are talking about you but not what are they talking about, to whom, how and why those conversations happened. Present tools are like robots with fed rules but no brain. But had it been a real networking event would you have wanted such a robot? Would it not be more useful had we been analyzing social media conversation with a context specific to the brand or campaign engrained not a generic algorithm based on generic knowledge?Can you rely on a robot to decipher novel ideas suggested by your customers or would it not be useful to have a human assisted by machines to help understand hidden meanings?If you think so, keep on reading. 

,

One of the biggest frustrations I have with present social media monitoring practices is excessive reliance on volumetric metrics like number of tweets, trends, impressions, no. of fans, likes etc. Some of these metrics are quite important but my disagreement lies with over-emphasis (& sometimes blind following) on these and often a complete disregard for bringing meaning to actual conversations. Once I was talking to a SM analyst in New York and he almost came to blows with me on usage of a metric called Impression. Impression tells the number of times an ad was seen [1] [2]. In the context of SM conversations (talking specifically for twitter) most analysts count it based on the number of followers every tweeter talking about your brand has and assuming that whenever he/she tweets all of the followers will be sitting in front of their twitter to watch it! ,Andy White, a Social Media Consultant has written about misuse of this metric in his insightful blog [3]. Impression can be used as an Ad (outdoor or online) metric because you possibly don?€?t have any other way to measure such but with Social Media that?€?s not the case. You cannot stop and talk to a billboard you liked but on Social Media you can do a lot of things like, say talk to it (comment), show it your friends (share) and possibly even carry it in your cellphone (save)! There are a lot of metrics which are simply copied from either Outdoor Advertising industry or Web Analytics or similar. Several such metrics might be good for comparison of various media but these are often misleading in understanding social media. Several social media tools are based on techniques rooted in web analytics. Web analytics are good for websites where people go from link to link but social media is not a mute web page but involves humanly interactions. 

,
,
A lot of Social Media analysts will say that don?€?t we have sentiments and categorization options available in some tools to understand conversations. For those, I sincerely ask them to spare one day of their busy life to read through actual data going into their analysis and related outputs. Once I was reviewing a social media tool and was shown twitter search results based on a query around a brand. Vendor created a category map which looked quite cool thanks for the visualizations inbuilt. But on drilling into the actual data was when things turned out to be uncool. One of the categories was labeled CEO and we expected it to contain data around the CEO of that brand but actually only around 10% data contained related conversations. Not going into the technical reasons, point I am trying to make is that that often SM tools are like simple machines with no brains but why are we forgetting this fact. We need machines to help in the process not understand it for us. This brings me to sentiments. ,Large number of times I look at sentiment analysis given by most tools and often I get 90%+ conversations marked as neutral. In most cases tools fail to understand context of the conversation due to which this problem arises. Most of the present day tools view language as a mathematical equation and process it accordingly while in reality language is complex and formed of structures and intricacies which go unnoticed. Some people suggest human coders for such cases but I feel that a dual approach of using human analysts & reviewers aided by powerful NLP enabled machine learning can help. 

,

Before I close this blog, I would like to say that Social Media analysis is in its infancy and we need to constantly debate on what is right and what can be done better. I have often seen people either overhyping capabilities of social media while some undermining the power of social media analysis and view it with an eye of disdain. I feel that both camps are wrong. Social Media analysis can show insights that can never be hypothesized or deciphered by traditional survey research. It helps get opinion from hard to reach people expressing REAL feelings in real time which is impossible via survey. Yet there are lots of areas where SM analysis falls short; especially in understanding the WHY part and that?€?s where surveys can be important.  Depth and volume of insights that can be gathered via Social Media analysis varies by industry and brands making traditional research necessary as a supplement. ,
I can go on and on but ending this blog, I will say that next time you see a social media analysis try to understand the conversations and not simply the numbers. It?€?s a new field so let?€?s be open minded but with an inquisitive eye. 
Like always, these are my observations but feel free to agree or disagree.
,
, is a Senior Analyst-Marketing Science (Advanced Analytics) at TNS (A WPP Company).,
The views expressed are solely personal and do not reflect views of my employer or associated companies.,

References-,
[1] ,
[2] ,
[3] ,
,
,
 ,  "
"
By Anmol Rajpurohit, Feb 16, 2014.
,
,O'Reilly's Strata 2014 conference was held from Feb 11-13, 2014 at Santa Clara Convention Center in Santa Clara, CA. This conference brought together about 3100 people. Among them were the brightest minds in data science and big data: decision makers using data to drive business strategy, as well as practitioners who collect, analyze, and manipulate it.,Here are the highlights of keynote speeches that were delivered during the conference:,

First day of the conference was mostly about tutorials. It had two day-long sessions titled ""Hardcore Data Science"" and ""Data-driven Business day"". Overall, all of the tutorials helped the attendees learn and update themselves with the latest technologies.  Particularly, the hands-on session during tutorials was well appreciated by the crowd.
,
Second day of the conference started with keynote welcome by Roger Magoulas from O'Reilly Media and Alistair Croll from Solve For Interesting., Highlights from keynote speeches on day two (Wed Feb 12):,

1. Geoffrey Moore, well known author, speaker and advisor, Geoffrey Moore Consulting delivered a lesson regarding how data will change business in the coming years.
,
2. Amr Awadallah from Cloudera, Inc. explained the evolution from Apache Hadoop to the Enterprise Data Hub. He insisted on Enterprise Data Hub (EDH) as the new foundation for the modern information architecture.,
3. John Schitka from SAP emphasized on the crowd, explaining how we all can make the world a better place by coming together to collect massive data via crowdsourcing.
,
4. Ramona Pierson from Declara talked on uncapping learning at a massive scale by leveraging the cloud.
,
5. John Schroeder from MapR Technologies discussed few use cases of Hadoop. One of them was largest biometric database in India under Aadhaar project by Government of India.
,
6. Farrah Bostic from ""The Difference Engine"" introduced herself as a refugee from Advertising. She described various aspects involved in Market Research. She emphasized that how we decide what to measure does also matter.
,
7. Quentin Clark from Microsoft presented on how big data can reach 1 billion people. He mentioned that information can not only change business but people's individual life. He also emphasized on visualization of information as a powerful tool.
,
8. David Epstein from Sports Illustrated gave a captivating talk on applied sport data analysis and also criticized the 10,000 hour rule in sports.
,
9. Rodney Mullen from Almost Skateboards talked about learning analytics and how practice makes perfect.
,
,
,:
,
1. Joe Hellerstein (adjunct faculty at UCB) and Tutti Taygerly from Trifacta spoke of two different perspectives - tech and design. They described ""Predictive Interaction"" as a new model for high-level human-data interaction that radically improves the productivity and accessibility of the most time-consuming work in the analytics lifecycle.
,
2. Kaushik Das from Pivotal gave a great talk on how creating a digital brain can prevent disasters for a better world. He illustrated with examples how we can build a digital brain by collecting data from a large number of sensors and use the brain to find value in that data.
,
3. Boyd Davis from Intel announced new Intel Data Platform based on Hadoop. He shared a glimpse into the future of a datacenter-scale open-source operating environment.
,
4. Matei Zaharia from Databricks (also one of the Apache Spark Committers) presented on how companies are using Spark & where the edge in Big Data will be.
,
5. Anjul Bhambhri from IBM discussed how In-Hadoop analytics is changing the game through the possibilities of Hadoop.
,
6. James Burke, a science and technology historian, futurist, and author gave a captivating talk titled ""The Future Isn't What it Used to Be"". He emphasized that most of the problems come from interdisciplinary and not from particular area of study. During the talk he also quoted Aristotle's principles of science: apply methodical thought & reduce everything to simplest form (but not simpler). His talk ended with a huge applause.
,
,
,
Highlights from closing keynote speeches:
,
1.  Megan Price from Human Rights Data Analysis Group discussed about the real count of casualties in Syria, variation in report from different agencies and how big data effects human rights. She emphasized that reporting pattern might be too different than reality.
,
2. Ben Fry from Fathom focuses on understanding complex data. He used war math to illustrate cognitive bias on luck and survivorship. He also displayed interactive data visualization using example of ,, an HTML5 application that uses a custom database to explore the cultural and political factors that shape the dynamics of power in modern China.
,
3. David McRaney, an author and digital media director gave last and fantastic keynote lecture on ?€?Survivorship Bias and the Psychology of Luck?€?.  He used war math to illustrate cognitive bias on luck & survivorship. He summarized luck as what we call the result of a human being consciously interacting with chance. Regarding survivorship bias he concluded by putting forward that the true business is a monopoly run by survivors and those who fall are rarely paid for advice on how not to fail.
,

Following are tweets by , during the conference:
,

At #strataconf @davidmcraney: beware of ""success"" advice - it is a monopoly run by survivors. Listen to those who fail on what not to do,
At #strataconf @davidmcraney: when looking for advice, look for what not to do, for what is missing,
At #strataconf @davidmcraney: beware of  the advice from the successful - they were probably lucky.  Learn more from failures,
At #strataconf @davidmcraney shows many ways how people can be tricked: the science of ""stupidity"" and Luck,

ConnectedChina - amazing visualization of social network and connections in China, from Ben Fry at #strataconf  http://shrd.by/AyMBO5,

At #strataconf James Burke predicts a huge revolution based on nanotechnology, everyone to have a fabber which can make ... everything!,
At #strataconf James Burke: need to go beyond Descartes reductionism, to integrated systems thinking,
At #strataconf James Burke: Major results comes from an unexplored no-man's land between the disciplines,
At #strataconf James Burke: You are constrained to predict the future from the past, because it is all there is,

At #strataconf: @databricks : #BigData will be standard - what matters is what you do with it. Apache #Spark is a great option,

At #strataconf @joe_hellerstein: Trifacta uses predictive interaction, shows users best possible viz, learns from what user does,
At #strataconf @joe_hellerstein: cannot get enough data scientist ""heroes"", masters of coding, stats, biz! Need teams, better SW design,
At #strataconf @joe_hellerstein calls for #BigData Moonshots, not incremental projects,

, to go through YouTube playlist for some keynote speeches and interviews held during the conference.  "
"
,
,
,
Latest KDnuggets Poll asked: 
,
,
,
Surprisingly, the results don't show much change in the adoption of text analytics among KDnuggets readers - the only significant difference is a slight increase among those who used text analytics on 10% of their projects.
,
While there was a noticeable increase in 
,, no such increase was observed this year.
,
Of course, KDnuggets readers may be more biased towards more data analysis than text, but I do not see a huge growth in attendance of several Text Analytics conferences that have been running for the last several years.
Despite the often cited number of 80% of all information being ""unstructured"" or text, Text Analytics ""buzz"" seems to lagging behind the ""Big Data"".
,
,
,
Google Trends analysis seems to confirm this observation. 
We note that searches for ""Text Mining"" are slowly declining, while ""Text Analytics"" are slowly increasing, but at levels 
,.
,
Here are the poll results:
,
,
,
,
,
,
 ,  "
"
,
,

My name is Bruce Ratner, and I am a dataholic. I am also an artist and poet in the world of statistical data. I always await getting my hands on a new dataset to crack open and paint the untapped, unsoiled numbers into swirling equations, and pencil data gatherings into beautiful verse.,

I see numbers as the prime coat for a bedazzled visual percept. Is not the nums-pic in Figure 1, below, grand? I also see mathematical devices as the elements in poetic expressions, which allow truths to lay bare. A poet's rendition of love in Figure 2, below, gives a thinkable pause. For certain, the irresistible equations are poetry of numerical letters. The most powerful and famous is E = mc,. The fairest of them all is e, + 1 = 0. The above citations of the trilogy of art, poetry, and data, which makes an intensely imaginative interpretation of beauty, explain why I am a dataholic.
,

The purpose of this cur-sorry article is to provide a staircase of twelve steps to ascend upon cracking open a dataset regardless of any application the datawork may entail.,

Before painting the numbers by the numbers, penciling dataiku verses, and formulating equation poems, I brush my tabular canvas with four essentials markings for the just out dataset. The markings, first encountered on stairstepping to the rim of the dataset, are:,

Step/Marking 1. Determine sample size, an indicator of data depth.,
Step/Marking 2. Count the number of numeric and character variables, an indicator of data breadth.,

Step/Marking 3. Air the listing of all variables in a format. This permits copying and pasting of variables into a computer program editor. A copy-pasteable list forwards all statistical tasks.,

Step/Marking 4. Calculate the percentage of missing data for each numeric variable. This provides an indicator of havoc on the assemblage of the variables due to the missingness. Character variables really never have missing values: We can get something from nothing.,

The following eight steps complete my twelve-step program for dataholics, at least for cracking open a fresh dataset.,

Step 5. Follow the contour of each variable. This offers a map of the variable's meaning through patterns of peaks, valleys, gatherings, and partings across all or part of the variable's plain.,

Step 6. Start a searching wind for the unexpected of each variable: Improbable values, say, a boy named Sue; impossible values, say, age is 120 years; and, undefined values due to irresponsibilities like X/0.,

Step 7. Probe the underbelly of the pristine cover of the dataset. This uncovers the meanings of misinformative values, such as, NA, the blank, the number 0, the letters o and O, the varied string of 9s, the dash, the dot, and many QWERTY expletives. Decoding the misinformation always yields unscrambled data wisdom.,

Step 8. Know the nature of numeric variables. I.e., declare the formats of the numerics as decimal, integer or date.,

Step 9. Check the reach of numeric variables. This task seeks values ""far from"" or ""outside"" the fences of the data. ,

Step 10. Check the angles of logic within the dataset. This allows for weighing contradictory values with conflict resolution rules.,

Step 11. Stomp on the lurking typos. These lazy and sneaky characters earn their keep by ambushing the integrity of data.,

Step 12. Find and be rid of noise within thy dataset. Noise, the idiosyncrasies of the data, the nooks and crannies, the particulars, are not part of the sought-after essence of the data. Ergo, the data particulars are lonely, not-really-belonging-to-pieces of information that happen to be both in the population from which the data were drawn and in the data themselves. Paradoxically, as the analysis/model includes more and more of the prickly particulars, the analysis/model build becomes better and better. Yet, the analysis/model validation becomes worse and worse.,

Noise must be eliminated from the data by ,
1) identifying the idiosyncrasies, and ,
2) deleting the records that define the idiosyncrasies of the data. ,
Once the data are rid of noise, the analysis/model reliably represents the sought-after essence of the data.,

Read ,.  "
"
,
,
,
Leverage highly valuable and impactful prospective insight from within your existing data through an intensive course series with over 20 years of innovation.
,
If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series. The series is designed to get you up to speed faster and more effectively than any other program available.
,
Highly seasoned consultants convey a comprehensive and highly pragmatic view of data mining from two distinctly different yet highly complementary orientations.  Since The Modeling Agency is not a tools vendor, participants enjoy a balanced, broad, and non-promotional perspective of predictive analytics.
,
The next production dates for The Modeling Agency's vendor-neutral
, is scheduled for
,
,??,
,
,
,
,??,
,
,
,??,
, 
,Class seating is limited to twenty per session. Don't miss your chance to attend.  
Review the details, then register today: 
,
,
, 
,Not ready to register for public training just yet?  Sign up for The Modeling Agency's 1-hour live webinar 
, at no charge to learn how to get predictive modeling off the ground and into orbit.  
,
,: 
,  "
"
,
Most popular KDnuggets tweets ,(see , ) for Feb 14-17 were
,
,  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
Thursday, Apr 17, 2014 
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
Latest ,, (Feb 19, 2014) ,:
,
,??,
Also
, (10) |
, (2) |
, (2) |
, (2) |
, (2) |
, (10) |
, (4) |
, (6) |
, (6) |
, (14) |
,
,
Information causes changes. If it doesn't, it is not information (Claude Shannon), quoted by noted science historian James Burke at Strata, Feb 13, 2014.  "
"
        ,  "
"
At the recent Strata Conference (Feb 11-13, 2014 in Santa Clara) there were many creative give-aways companies used to attract prospects to their booth. 
,
One of the most clever was a 
,
,

It divided data science operators into 7 categories:,

,:  Hc - copy to Hadoop, Ds - Dataset ...,

,: Bc - Bar Chart, Bp - Box Plot ...,

,: Ag - Aggregate,Co - Collapse ...,

,: Rs - Random Sampling, Ss - Stratified Sampling ...,

,: Ar - Association Rules, Sr - SVM Regression ...,

,: Ad- Adaboost Predictor, Np - Neural Network Predictor ...,

,: Cm - Confusion Matrix, Gf - Goodness of Fit ...,
,
An interactive version is available at , and it is intended to be used in conjunction with Alpine Chorus, which you can get for free ,.  "
"
,
,
,??,??
,
,
,
,??
,??
,
,  "
"
,
,
Here is 
,
,
, is of the leading experts in 
, - a breakthrough advance in machine learning which has been achieving amazing successes, a founding Director of 
,, 
and was recently appointed the Director of the AI Research Lab at Facebook.
,
,
,
,:
We will have intelligent machines. It's clearly a matter of time. We will have machines that, without being very smart, will do useful things, like drive our cars autonomously.
,
How long will it take? AI researchers have a long history of under-estimated the difficulties of building intelligent machines. I'll use an analogy: making progress in research is like driving a car to a destination. When we find a new paradigm or a new set of techniques, it feels like we are driving a car on a highway and nothing can stop us until we reach the destination. 
,
,
But the reality is that we are really driving in a thick fog and we don't realize that our highway is really a parking lot with a brick wall at the far end. Many smart people have made that mistake, and every new wave in AI was followed by a period of unbounded optimism, irrational hype, and a backlash. It happened with ""perceptrons"", ""rule-based systems"", ""neural nets"", ""graphical models"", ""SVM"", and may happen with ""deep learning"", until we find something else. But these paradigms were never complete failures. They all left new tools, new concepts, and new algorithms.
,
,Although I do believe we will eventually build machines that will rival humans in intelligence, I don't really believe in the singularity. We feel like we are on an exponentially growing curve of progress. But we could just as well be on a sigmoid curve. Sigmoids very much feel like exponentials at first. Also, the singularity assumes more than an exponential, it assumes an asymptote. The difference between dynamic evolutions that follow linear, quadratic, exponential, asymptotic, or sigmoidal shapes are damping or friction factors. Futurists seem to assume that there will be no such damping or friction terms. Futurists have an incentive to make bold predictions, particularly when they really want them to be true, perhaps in the hope that they will be self-fulfilling.
,
,
,
,:
I have stepped down as (founding) director of the 
,.
,
The interim director is 
,, possibly the most famous probability theorist in the world. NYU has initiated a search for a new permanent director. I have invested a huge amount of energy into the creation of CDS. We now have an MS program in Data Science, and will soon have a PhD program. We have 9 open faculty positions for the center, we have won a very large, five-year grant from the Moore and Sloan foundations in collaboration with Berkeley and University of Washington, we have a partnership with Facebook and other companies, we will soon have a new building. The next director is going to have all the fun!
,
,
,
,:
Data Science pertains to the automatic or semi-automatic extraction of knowledge from data. This concept permeates many disciplines, each of which has a different name for it, including statistical estimation, data mining, predictive analytics, system identification, machine learning, AI, etc.
,
On the methods side, statistics, machine learning, and certain branches of applied mathematics could all claim to ""own"" the field of data science. But in reality, Data Science is to Statistics, Machine Learning, and Applied Math as Computer Science was to Electrical Engineering, Physics, and Mathematics in the 1960s. The same way computer science became a full-fledged disciplined, rather than a sub-field of mathematics or engineering is its importance to society. 
,
With the exponential growth of data generated by our digital world, the problem of automatically extracting knowledge from data is growing rapidly. This is causing the emergence of Data Science as a discipline. It is causing a redrawing of the boundaries between Statistics, Machine Learning, and Applied Mathematics. It is also creating a need for tight interactions between ""methods"" people and ""domain"" people in science, business, medicine, and government.
,
My prediction is that 10 year from now, many top universities will have Data Science departments.
,
,
,
,:
I like the joke circulated around social networks that compared 
big data to teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. 
,
,
I have seen people insisting on using Hadoop for datasets that could easily fit on a flash drive and could easily be processed on a laptop.
,
There is hype, to be sure. But the problem of how to collect, store and analyze massive amounts of data is very real. I'm always suspicious of names like ""big data"", because today's ""big data"" is tomorrow's ""little data"".  Also, there are many important problem that arise because of too little data. It is the often the case for genomics and medical data. There is never enough data.
,
,
,
,:
If you are an undergrad, take as many math, stats, and physics courses as you can, and learn to program (take 3 or 4 CS courses).
,
If you have an undergraduate degree, apply to 
,.
,
,
,
I design and build miniature flying contraptions, I tinker with 3D printers, I hack microcontroller-based widgets, and I hope to get better at making music (I seem to collect electronic wind controllers). I read mostly non-fiction, and I listen to a lot of jazz (and to many other types of music).  "
"
,
,
, 
has been much in the news lately, as one of the leading experts in 
, - a breakthrough advance in machine learning which has been achieving amazing successes, 
as a founding Director of 
,, 
and as the 
,
Director of the AI Research Lab at Facebook.
See his bio at the end of this post and you can learn more about his work at ,.
,
He is extremely busy, combining his new job at Facebook and his old job at NYU, so I am very pleased that he agreed to answer a few questions for KDnuggets readers.
,
,
,
,: Despite a commonly-held belief, there have been numerous successful applications of neural nets since the late 80's.
,
Deep learning has come to designate any learning method that can train a system with more than 2 or 3 non-linear hidden layers.
,
Around 2003, Geoff Hinton, Yoshua Bengio and myself initiated a kind of ""conspiracy"" to revive the interest of the machine learning community in the problem of learning representations (as opposed to just learning simple classifiers). It took until 2006-2007 to get some traction, primarily through new results on unsupervised training (or unsupervised pre-training, followed by supervised fine-tuning), with work by Geoff Hinton, Yoshua Bengio, Andrew Ng and myself.
,
But much of the recent practical applications of deep learning use purely supervised learning based on back-propagation, altogether not very different from the neural nets of the late 80's and early 90's. 
,
, (sometimes with billions of connections, and 12 layers) and train them on large datasets with millions of examples. We also have 
,, such as a ,, rectifying non-linearity for the units, different types of spatial pooling, etc.
,
Many successful applications, particularly for image recognition use the convolutional network architecture (
,), a concept I developed at Bell Labs in the late 80s and early 90s. At Bell Labs in the mid 1990s we commercially deployed a number of ConvNet-based systems for reading the amount on bank check automatically (printed or handwritten). 
,
At some point in the late 1990s, one of these systems was reading 10 to 20% of all the checks in the US. Interest in ConvNet was rekindled in the last 5 years or so, with nice work from my group, from Geoff Hinton, Andrew Ng, and Yoshua Bengio, as from Jurgen Schmidhuber's group at IDSIA in Switzerland, and from NEC Labs in California. ConvNets are now widely used by Facebook, Google, Microsoft, IBM, Baidu, NEC and others for image and speech recognition. 
[GP: A student of Yann Lecun recently 
, using a version of ConvNet, achieving 98.9% accuracy.]
,
,
,
,: There are two main packages: 
,
,??,
They have slightly different philosophies and relative advantages and disadvantages. Torch7 is an extension of the LuaJIT language that adds multi-dimensional arrays and numerical library. It also includes an object-oriented package for deep learning, computer vision, and such. The main advantage of Torch7 is that LuaJIT is extremely fast, in addition to being very flexible (it's a compiled version of the popular Lua language). 
,
Theano+Pylearn2 has the advantage of using Python (it's widely used, and has lots of libraries for many things), and the disadvantage of using Python (it's slow).
,
,
,
,: 
Vapnik and I were in nearby office at Bell Labs in the early 1990s, in Larry Jackel's Adaptive Systems Research Department. Convolutional nets, Support Vector Machines, Tangent Distance, and several other influential methods were invented within a few meters of each other, and within a few years of each other.  When AT&T spun off Lucent In 1995, I became the head of that department which became the Image Processing Research Department at AT&T Labs - Research. Machine Learning members included Yoshua Bengio, Leon Bottou, and Patrick Haffner, and Vladimir Vapnik. Visitors and interns included Bernhard Sch????lkopf, Jason Weston, Olivier Chapelle, and others.
,
,
Vapnik and I often had lively discussions about the relative merits of (deep) neural nets and kernel machines. Basically, I have always been interested in solving the problem of learning features or learning representations. I had only a moderate interest in kernel methods because they did nothing to address this problem. Naturally, SVMs are wonderful as a generic classification method with beautiful math behind them. But in the end, they are nothing more than simple two-layer systems. The first layer can be seen as a set of units (one per support vector) that measure a kind of similarity between the input vector and each support vector using the kernel function. The second layer linearly combines these similarities. 
,
It's a two-layer system in which the first layer is trained with the simplest of all unsupervised learning method: simply store the training samples as prototypes in the units. Basically, varying the smoothness of the kernel function allows us to interpolate between two simple methods: linear classification, and template matching. I got in trouble about 10 years ago by saying that kernel methods were a form of glorified template matching. Vapnik, on the other hand, argued that SVMs had a very clear way of doing capacity control. An SVM with a ""narrow"" kernel function can always learn the training set perfectly, but its generalization error is controlled by the width of the kernel and the sparsity of the dual coefficients. Vapnik really believes in his bounds. He worried that neural nets didn't have similarly good ways to do capacity control (although neural nets do have generalization bounds, since they have finite VC dimension).
,
My counter argument was that the ability to do capacity control was somewhat secondary to the ability to compute highly complex function with a limited amount of computation. Performing image recognition with invariance to shifts, scale, rotation, lighting conditions, and background clutter was impossible (or extremely inefficient) for a kernel machine operating at the pixel level. But it was quite easy for deep architectures such as convolutional nets.
,
,
,
,
,: Thank you! it's a very exciting opportunity. Basically, Facebook's main objective is to enable communication between people. But people today are bombarded with information from friends, news organizations, websites, etc. Facebook helps people sift through this mass of information. But that requires to know what people are interested in, what motivates them, what entertains them, what makes them learn new things. This requires an understanding of people that only AI can provide. Progress in AI will allow us to understand content, such as text, images, video, speech, audio, music, among other things.
,
Here is 
,
,
,
, is Director of AI Research at Facebook, and the founding
director of the Center for Data Science at New York University. He is
Silver Professor of Computer Science, Neural Science, and Electrical
Engineering and NYU, affiliated with the Courant Institute of
Mathematical Science, the Center for Neural Science, and the ECE
Department.
,
He received the Electrical Engineer Diploma from Ecole Sup??rieure
d'Ing??nieurs en Electrotechnique et Electronique (ESIEE), Paris in
1983, and a PhD in Computer Science from Universit?? Pierre et Marie
Curie (Paris) in 1987. After a postdoc at the University of Toronto,
he joined AT&T Bell Laboratories in Holmdel, NJ in 1988. He became
head of the Image Processing Research Department at AT&T Labs-Research
in 1996, and joined NYU as a professor in 2003, after a brief period
as a Fellow of the NEC Research Institute in Princeton. He was named
Director of AI Research at Facebook in late 2013 and retains a
part-time position on the NYU faculty.
,
His current interests include machine learning, computer perception,
mobile robotics, and computational neuroscience.  He has published
over 180 technical papers and book chapters on these topics as well as
on neural networks, handwriting recognition, image processing and
compression, and on dedicated circuits and architectures for computer
perception. The character recognition technology he developed at Bell
Labs is used by several banks around the world to read checks and was
reading between 10 and 20% of all the checks in the US in the early
2000s.  His image compression technology, called DjVu, is used by
hundreds of web sites and publishers and millions of users to access
scanned documents on the Web. A pattern recognition method he
developed, called convolutional network, is the basis of products and
services deployed by companies such as AT&T, Google, Microsoft, NEC,
IBM, Baidu, and Facebook for document recognition, human-computer
interaction, image tagging, speech recognition, and video analytics.
,
LeCun has been on the editorial board of IJCV, IEEE PAMI, and IEEE
Trans. Neural Networks, was program chair of CVPR'06, and is chair of
ICLR 2013 and 2014. He is on the science advisory board of Institute
for Pure and Applied Mathematics, and has advised many large and small
companies about machine learning technology, including several
startups he co-founded. He is the recipient of the 2014 IEEE Neural
Network Pioneer Award.
  "
"
,

When we think of people involved in data mining, we generally think of academic researchers, entrepreneurs, data scientists and industry experts. Well, it seems that data mining has some more audience - avid gamers! Gaming geeks are increasingly using data mining to explore the yet-to-be released features, jeopardizing gaming companies?€? mega launch plans.

,

Titanfall, one of the most anticipated games of 2014, launched its Beta version on February 15 and in less than two days users have discovered lot more than intended by combing through its raw data files. The revealed information includes 14 maps, split-screen option and new game modes. Titanfall is scheduled to release for Xbox One, PC and Xbox 360 on March 14th 2013.

,
,

While this is certainly not the first time when curious gamers have mined game data to claim the fame by revealing the new features of yet-to-launch game versions, the speed with which it??occurred??this time is scintillating. Such data mining is getting diverse responses. Some are highly excited to know more about the??forthcoming game and use this to plan their activities around launch day. Others refer to it using the plain old term - ""hacking"" - and abhor it for killing the surprise.

,

The phenomenon of gamers data-mining the game data is particularly interesting given the massive resources gaming companies are investing into mining users' data. For example, last year, with more than 2 billion gamers worldwide, Electronic Arts generated 50 TB of data per day. Game data mining is used for a variety of purposes (beyond the very obvious one - targeted marketing!):,
,
,
However, with the increasing trend of smart players hacking into the game software data, Gaming companies need to be very cautious about the raw data accompanied in their game installations. Well, except for the cases where such data is left purposely to be data-mined by??enthusiastic??hackers, leading to some free pre-launch viral marketing.
,
,
, is an intern and blogger at KDnuggets, and a visiting student researcher at ,. He is a B.Tech. graduate in Computer Science from India and is keenly interested in research and development work in the field of Computer Networks, Information Retrieval and Knowledge Management (Data Mining, Web Mining, etc.).  "
"
,
,
March 30 - April 1, Westin Boston Waterfront, Boston, MA
,
The industry's top analytics conference features more than 100 practical talks spread over eight tracks of learning.  Each talk provides pointed  ""how we did it"" information from hand-picked industry speakers.  
,
Keynote speakers include industry thought leader, Tom Davenport, and Disney sales channel executive, Kathy Kilmer.  You will learn from the best, refresh your analytics skills, learn ""soft skills"" that are so important in business, and meet the experts in a casual, intimate atmosphere.  
,
Among the extras you will enjoy include an Executive Forum, Analytics Connect - the industry's only in-person analytics job fair, technology workshops, facilitated networking, and a gala awards ceremony that features the top difference-making analytics projects in industry.  All meals are provided over the course of the conference.  Conference proceedings are provided.   
,
, until March 17.  "
"
Most popular 
, tweets for Feb 18-20 were
,
The six types of conversations on Twitter, as identified by @nodexl project ,
,
,
The six types of conversations on Twitter, as identified by @nodexl project ,
,
Practical Machine Learning: Innovations in Recommendation - free ebook download from O'Reilly ,
,
,  "
"
,

,
,

Friday, March 28 in New York City
,
,
,
,
,
Click to Register(or suggest other locations):??,

Cost: $35 (includes free 90-day access to the SPM Salford Predictive Modeler technology)

,

Agenda
,
,
,
,
,
,
,
,
Salford Systems' Training Seminars offer crystal clear instruction and a wealth of real-world consulting experience. This seminar will provide a great opportunity to use data mining technology and to understand how to apply data mining to your business and/or research needs.
,
Questions?
,
Please contact Lisa Solomon ,
,

,  "
"
,
,
Quite often, the focus of analytics efforts is highly skewed towards quantitative aspects and the qualitative efforts are either negligible or replaced by the executives?€? ?€?gut feeling?€?. It is imperative to understand how such a casual approach can hurt your business.
,
The analysis of qualitative data through the methods generally used for qualitative research is referred to as Qualitative Analytics. The differentiation between Qualitative and Quantitative analytics is not always obvious. For example, during Text Analytics, measuring the frequency of certain words would be considered Quantitative Analytics; whereas exploring the contextual meaning of popular words would be considered Qualitative Analytics. In other words, Qualitative Analytics includes the analysis of context, human behavior, emotions and other factors that are hard to digitize without losing any meaning.
,
,
Qualitative analytics is a very powerful tool for exploratory research ?€? the earliest phase of analytics. It is also a great tool to bridge the gap between insights provided by quantitative research, and provide in-depth understanding of the underlying reasons and motivations for a phenomenon. Pragmatic qualitative research is done through a variety of methods. While some of them are simple such as Surveys and Interviews, others are highly advanced such as Ethnography and Phenomenology.
,
One of the most common myths about qualitative analytics is that it is useful only for academic researchers in selected fields such as social sciences and neuroscience marketing. The truth is that - almost all business problems have a qualitative aspect, and thus, quantitative analysis alone would never be able to tell the complete story.
,
As an example, let?€?s consider web analytics. All leading web analytics software have excellent quantitative analytics capabilities. They can easily, yet precisely, provide information such as: number of users who visited the website during a given time period, geographic locations from which the website was accessed, time period for which user stayed on the website, etc. However, when it comes to the ?€?true business value?€? questions (such as why did the customer not make the purchase? what factors influenced the customer?€?s buying decision? how did the customer feel after the purchase?) these web analytics software, at-best provide vague answers.
,
So, if you are narrowly focused on the clickstream, you will never be able to comprehend the thoughts going through customer?€?s mind as she moves across the ?€?conversion funnel?€?. So, when you think of sales, instead of focusing on the ?€?click?€? event, think broader about the interaction and overall experience. One of the most popular business Key Performance Indicator (KPI), Customer Satisfaction can never be measured through quantitative analytics alone. No amount of quantitative guessing can match the business value of ?€?voice of customer?€?.
,
As the CMOs increasingly drift from Customer Relationship Management (CRM) to Customer Experience Management (CEM), there is an increasing need for holistic understanding of customer experience across the complete sale cycle. This can be achieved only through deploying qualitative analytics and integrating it with quantitative analytics to provide 360-degree data analysis.
,
Let?€?s face it. Quantitative analytics still needs more manual intervention and the results are often fuzzy. In absence of a clear-cut approach and thus automation, it is not as time and energy efficient as the traditional quantitative analytics. But, qualitative analytics is still indispensable as it provides deep, actionable insights about the ?€?why?€? and ?€?how?€? aspect, which often gets ignored as we continue to be inundated with the ?€?what?€? ?€?where?€? and ?€?when?€? of statistics.
,
With the rapid pace of technology advancement in recent times, it now seems that the software tools used for academic purpose are ready to deliver on the business analytics needs of the industry. ,, , and , are some of the best qualitative research tools, with extended capabilities of semi-automating the pattern recognition from content (such as subjective answers, interview transcripts or video recordings).
,
If you are a business manager, it is high time to think again about your analytics dashboard and whether qualitative analytics are getting your appropriate attention.
,
,
, is an intern and blogger at KDnuggets, and a visiting student researcher at ,. He is a B.Tech. graduate in Computer Science from India and is keenly interested in research and development work in the field of Big Data, Data Mining, Social Analytics and Computer Networks.
  "
"
Most popular 
, tweets for Feb 21-23 were
,
Data Science Central founder Vincent Granville @analyticbridge shares his salary history and career path ,
,
Qualitative Analytics: Why numbers do not tell the complete story? ,
,
Graf.ly: Making beautiful, interactive graphs ,
,
,
,  "
"
        ,

Make beautiful, interactive graphs with ,. It?€?s a three-step process that takes about 30 seconds:,
,
,
And if the largest library of plug-and-play interactive graphs on the web isn?€?t enough, join our developer program. Contribute code for a new graph and we?€?ll help you monetize it (email , to join)!
,
Enough talk. Let?€?s graph.
,
Your boss emails you a spreadsheet of KDnuggets posts dating back to 2005. She wants to see something interesting in 15 minutes. Of course you can play around in R and Python, but you?€?re short on time so you revert to your safety blanket: Excel.
,
,
You have four attributes to work with: post url, post title, post size in KB, and date published. Around 13k rows. First move is a simple, yet powerful , interactive scatter to explore post size by date published.
,
,
,
,
Looks like post size has generally increased. You zoom the scatter to 2005-2010. Far fewer weekend posts back then.
,
,
You?€?re short on time so let?€?s keep exploring. What are all these posts about? You pick out the post titles and date published attributes and create an interactive word cloud that shows most common words over time. It?€?s not the most scientific visualization, but a modern take at a familiar word cloud.
,
,
,
,
,
KDnuggets has always been about data, analytics, and mining. Recently there have been more mentions of ?€?scientists?€?, ?€?social?€?, ?€?tweet?€?, etc. Of course word clouds aren?€?t the most scientific graphs, but the point comes across and they look great.
,
,
You also want a more practical, yet still beautiful way to visualize KDnuggets most common titles. You click on the Graf.ly ?€?Horizon Chart?€?:
,
,
,
It?€?s easier for you to explore a longer time-span (2010+) and quickly see how word use in titles has fluctuated.
,
,
,
Most common individual words a good first step, but which words pop up most together? You run a , data transformation on the ?€?post title?€? column, creating links for a network graph. Let?€?s make something unique - you push the links into a chord diagram:
,
,
,
Beautiful. Data and mining appear together most. Mining and analytics second most. Makes sense - KDnuggets is the top resource for data mining.
,
Three beautiful, interactive graphs in under 15 minutes. You send your boss a url to your graphs. She?€?s impressed and asks how you did it. You tell her to sign up for: Graf.ly.  "
"
        ,  "
"
        ,
,
,
Gartner, a leading technology research company, has published a new Magic Quadrants(tm) for Advanced Analytics Platforms, separating Advanced Analytics from its previous category for ""Business Intelligence and Analytics Platforms"". 
,
Gartner recognizes that predictive and advanced analytics are becoming a major factor in the market, and evaluates the leading providers of advanced analytics platforms.
,
The leaders in this report are the two long-time analytics heavyweights - 
,
joined by two newcomers - 
,
,
,
,
The report is available from SAS at,
, 
,
and from RapidMiner at 
,, who are very pleased by being named as a leader.
RapidMiner CEO Ingo Mierswa said
,
,
,
,
Leaders are vendors strong in both completeness of vision and ability to execute.
,
Visionaries are typically smaller vendors that may not have the full ability to execute but have the vision of the trends that are shaping the market. This report has 2 such companies: 
,
,??,
Challengers, in Gartner definition, are either long-term market competitors that need to revitalize their vision to become more broadly influential, or they are well-established vendors in adjacent markets that are entering this market.  
Gartner lists 3 companies as challengers, and Angoss and Statsoft belong to first category, while SAP belongs to the second.
,
,??,
The other companies fall into Niche Players quadrant, who are usually either ""Challengers-in-waiting"" or ""Visionaries-in-waiting"":
,
,??,
In addition, the Gartner report has brief notes on significant vendors not included in MQ: 
,
,??,
,
,
 ,  "
"
Feb 25, 2014, Zurich - 
,
, today announced that Gartner, Inc. has placed KNIME among the ""Leaders"" in its Magic Quadrant for Advanced Analytic Platforms in recognition of KNIME's ability to execute and its completeness of vision. Gartner describes leaders as ""vendors with a strong and proven track record in the market that are also likely to influence the market's broader growth and direction. Leaders are suitable vendors for most organizations to evaluate.""
,
This follows closely on KNIME's attaining top scores in customer satisfaction, for the second year running, as reported in the Rexer Analytics 2013 survey of over 1250 data mining professionals. 
,
,
,KNIME owes its recent accolades to its users, now totaling over 3000 organizations operating its open-source platform and commercial extensions. ""The list of well-known global enterprises that lined up to talk to Gartner about their achievements with KNIME is very impressive, not to mention the avid Rexer survey participants,"" says Phil Winters, independent consultant and Board Member of KNIME. ""Some vendors are continuing their strategies of locking customers into their proprietary walled gardens, while others are trying to bait-and-switch their open-source customers by changing license models midstream. There is no doubt that KNIME's steadfast open-platform philosophy is causing a huge shakeup in this space.""
,
,
,
For further information, contact KNIME at ,, 
,or 
,  and discover it for yourself. KNIME - open for innovation.   "
"
,The Intelligence Advanced Research Projects Activity (IARPA) often selects its research efforts through the Broad Agency Announcement (BAA) process. , 
IARPA has issued a Request for Information (RFI), 
,, that may be of particular interest to you. ,
The RFI is asking for creative ideas about the representation of emerging events and their participants as reported in full text. Of interest are events that involve individual or small group participants as opposed to societal events., 
Please go to the IARPA website, ,, to view the RFI, IARPA-RFI-14-05. ,
Feel free to share with this your colleagues.,
,
,
Trying to keep our nation secure involves, in part, trying to identify emerging and evolving events. Given the sea of information that analysts must navigate daily, identifying activities of interest is a huge challenge. Current tools and technologies do a reasonable job of identifying facts of interest in data (e.g., person names, organizations, locations) but cannot reliably identify the events and activities that link these facts together. Some technologies have tried to identify unnamed events in data (i.e., those that represent actions or activities that are unfolding) by identifying all the verbs in a document but relating those ""events"" to one another and merging ""events"" across documents have met with little success. Being able to identify and track events over time would provide insight into events that are unfolding and the entities that participate in them.
,
One challenge to the success of identifying events, however, is that event is not clearly defined. Events at the highest level (e.g., a presidential inauguration) have component elements. Precursor events take place long before the major event takes place. Planning takes place; permission is obtained; decisions are made; purchases are made; participants can come and go. Each of these activities is an event. Events can also be linked to other events. Preparatory events can be predecessors of many types of higher level events.
,
Events also happen over time; they emerge. Activities build on each other, creating a hierarchy of elements and actions. The difficulty with event emergence is that it is characterized by uncertainty because there are rarely explicit references in data to what the end goal of the unfolding process is. Identifying and understanding time and reasoning about the emergence of events and the sequencing of activities are extremely difficult.
,
,
,  "
"
,
,

,
,

Pankaj leads the Analytics and Research practice globally at Genpact. This practice is part of Genpact?€?s Smart Decision Services that enables clients across industries to make smarter decisions in sales and marketing, cost, and risk management using data and insights. Under his leadership, Genpact?€?s Analytics business has grown to one of the largest and most extensive practices in the industry.
,
,
,
He is a veteran industry executive known for pioneering and guiding new businesses to success. In Rohit?€?s leadership HP Global Analytics grew from a small team to a large analytics organization. Today as part of HP?€?s Corporate Strategy team, Global Analytics is also chartered to lead HP?€?s Analytics delivery ecosystem, and bring together similar teams to drive innovations that support HP?€?s enterprise priorities.
,
,
,
In his capacity, Sameer is leading end-to-end business spheres of Cognizant Analytics and is responsible for crafting differentiated strategies around analytics consulting, platforms and services coupled with creating best of breed GTM, business development, operational excellence solutioning exercises and delivering transformational analytics engagements.
,
,
,
Srikanth leads Fractal?€?s global presence in business intelligence, consumer insights, predictive analytics, and optimization sciences, delivered through visual story-telling. Prior to co-founding Fractal, he consulted with major market leaders such as Visa, P&G, Citibank, HDFC Bank and SAP in predictive analytics for risk management and marketing effectiveness.
,
,
,
Pankaj Rai is the Director of Dell Global Analytics (DGA). Pankaj has been with Dell for ~8 years and has been with DGA for close to 5 years. Prior to this he was working with the India President?€?s office and managed all strategic and corporate planning related initiatives of Dell in India. In this role, he was responsible for helping Dell diversify and grow its footprint in India as also represent Dell outside in industry forums.
,
,
,
Amit has also done lot of work in areas of analytics capability development & analytics adaption in Organizations. He has worked extensively with various universities to develop Analytics & Data scientist curriculum. He personally own 2 analytics patents, he has also worked with large global clients to help them design their analytics organization & adapt a fact based culture.
,
,
,
Ashish has over 18 years of experience in driving analytics and customer insights into strategic planning and decision-making in a wide range of organizations. He currently heads eBay?€?s global center of excellence for analytics in Bangalore, with the aim of building cutting-edge capability to solve eBay?€?s hardest business problems.
,
,
,
Arnab has over 15+ years of rich experience in consulting and business analytics across diverse industry sectors. Before joining Accenture, Arnab played a pivotal role in scaling up enterprise wide analytics capabilities globally for the past ten years at Hewlett Packard (HP) and GE Capital (Genpact). Prior to that, he held leadership positions at KPMG Consulting, Wipro Technologies, and Larsen & Toubro. 
,
,
,
Dr. Anil Kaul is a well-known expert in the industry with over 16 years of experience in marketing research, strategic consulting. and quantitative modeling. He has consulted over 20 Fortune 500 companies during the 4 years he spent with McKinsey & Co. in New York. Thereafter, he joined Anubis Inc., an innovative data warehousing start-up in San Francisco Bay Area, as a part of the four-member top management team.

,
,
,
Working closely with the Chairman and the Senior Business Leadership, he is setting up the Centre of Excellence for Big Data Analytics at RIL capable of delivering deep business insights across many businesses. He is responsible for strategizing and realizing the value proposition for Data Science and Data Governance within the group companies while building and mentoring a world class team of data scientists.
,
Read ,.  "
"
,

,: Walt Wells,??,
,
Chicago, IL 2/25/2014 ?€? The Data Mining Group announced today the release of PMML v 4.2.  PMML is an application and system independent XML interchange format for statistical and data mining models. The goal of the PMML standard is to encapsulate a model independent of applications or systems in such a way that two different applications (the PMML Producer and Consumer) can use it.
, 
 ?€?As a standard, PMML provides the glue to unify data science and operational IT.  With one common process and standard, PMML is the missing piece for Big Data initiatives to enable rapid deployment of data mining models.  Broad vendor support and rapid customer adoption demonstrates that PMML delivers on its promise to reduce cost, complexity and risk of predictive analytics,?€? says Alex Guazzelli, Vice President of Analytics, Zementis.   ?€?You can not build and deploy predictive models over big data without using multiple models and no one should build multiple models without PMML,?€? says Bob Grossman, Founder and Partner at Open Data Group.  
 ,
Some of the elements that are new to PMML v4.2 include:,
,
?€?With PMML our customers and partners are able to drive real value from their predictive models right away, using the open standard,?€? said Andrew Flint, Senior Director of Product Management at FICO (NYSE: FICO). ?€?Models built in most commercial or open source data mining tools, such as FICO Model Builder or R, can be instantly deployed in the FICO Analytic Cloud using PMML. The net result is quicker time to innovation and value on analytic applications, and the ability to combine the power of standards-based predictive analytics with the scalability of cloud computing.?€?
,
Radhika Kulkarni, Vice President of Advanced Analytics at SAS notes, ?€?SAS continues to support the analytic collaboration that PMML provides to users.  The recent release of SAS Enterprise Miner 13.1 provides users the ability to not only consume PMML from Open Source R models, but also produce PMML, which can be consumed by other applications.  SAS Model Manager enables users to consume and manage R and PMML models as part of the SAS ecosystem.  Sharing analytic models is paramount to the analytic lifecycle.?€?
 ,
?€?We are extremely excited to continue our long-running PMML support to PMML 4.2,?€? said Scott Cappiello, Vice President, Program Management, MicroStrategy Incorporated. ""As a firm believer in providing the maximum analytic flexibility to organizations, PMML provides significant advantage in folding in analytics beyond our native and open source R capabilities, to provide business users the full range of business analytics in the big data age?€?.
 ,
""We are happy to see PMML's impact continuing to grow and will keep being among the first to integrate new PMML features into KNIME. Starting with our summer release KNIME will also support Naive Bayes Models and we will keep adding to its PMML Preprocessing abilities as well,"" says Kilian Thiel of KNIME.
,
,
PMML is the leading standard for statistical and data mining models and supported by over 20 vendors and organizations. With PMML, it is straightforward to develop a model on one system using one application and deploy the model on another system using another application.
 ,
,
The Data Mining Group (DMG) is an independent, vendor led consortium that develops data mining standards, such as the Predictive Model Markup Language (PMML).  DMG members include:  IBM, MicroStrategy, SAS, Experian, Pervasive Software, Zementis, Equifax, FICO, KNIME, NASA, Open Data Group, Rapid-I, Togaware, and Visa. 
,
For more information about the Data Mining Group and the PMML standard, go to:??,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
As a media partner for BigData TechCon, KDnuggets is pleased to offer a free pass for this conference, and we will give it away to a randomly chosen submission.
,
To get your free pass, email to 
,
,,with the subject ,, 
and answer one question:
,
,
,
Email should be received by 11:59 pm ET on Friday, Mar 7, 2014, and winner will be announced the following week. 
,
Please note that you are responsible for your own travel and hotel expenses. 
,
Here is more information about Big Data TechCon:
,
Plan now to attend 
,, March 31-April 2 in Boston, to learn HOW-TO accommodate the terabytes and petabytes of data from your Web logs, social media interactions, scientific research, transactions, sensors and financial records. Learn how to index, search and summarize Big Data. Learn how to empower employees, inform managers, and reach out to customers.
,
The tangible benefits of Big Data analytics are well known. You likely understand the ""why"" of Big Data. Big Data TechCon isn't a ""why"" conference. It's the HOW-TO conference for Big Data. Practical tutorials. Technical classes.
,
Big Data TechCon is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or Javascript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes. 
,
Mingle with fellow attendees.  The first Boston show hosted 700 delegates from 27 nations.  Be inspired by keynotes and the industry's top expert speakers.  Be impressed by the hottest Big Data tools in the Expo Hall.  It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code BIGDATA when prompted.  
,
,  "
"
Big data has emerged as the driving force behind critical business, organizational, and public policy decisions.  All aspects of data intelligence - from data analysis to predictive analytics, machine learning, statistical modeling, and beyond - are in demand more than ever.
,
,
,The CUNY School of Professional Studies (CUNY SPS) offers a 
, that prepares graduates to manage, identify patterns and draw insights from large amounts of data.  Students in the program:
,
,??,
Other differentiating characteristics of the program include: 
,
,??,
,
CUNY SPS invites you to attend an online information session on Tuesday, March 11th at 6 PM EDT to learn more about the online M.S. in Data Analytics program.  
,
,, email 
,, or call 212.652.CUNY to learn more about the program and register for the information session.
,
Additionally, for those interested in pursing an undergraduate degree, CUNY SPS has just launched an online B.S. in Information Systems for which an online information session will be held on Tuesday, March 11th at 12pm EDT. 
, to learn more about this program.  "
"
Most popular 
, tweets for Feb 24-25 were
,
,  "
"
,
,
One of the biggest challenges at big conferences such as Strata 2014 is that there is so much happening quickly and simultaneously that it is almost impossible to catch all the action. ,We help you by summarizing the key insights from some of the best and most popular sessions at the conference. These concise, takeaway-oriented summaries are designed for both people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference.
,
See also: ,
,
,
,
,
Edd?€?s talk was focused on how the changing landscape is changing our data needs. While describing the big picture, he emphasized the fact that Hadoop has over 208 partners now and the number is still growing. He shared the concept of data gravity, i.e. data attracts data, which eventually leads to Hadoop becoming the center of gravity for all data needs. He mentioned that although there is a good amount of cost involved on the way to Big Data but once inside the benefits of scale and agility are well understood.
,
One of the most interesting points made by him was about his vision of ?€?Data Lake?€?, which describes an organization with ideal processes and resources for Big Data. Referring to it as ?€?data lake dream?€?, he argued that it is an accessible dream despite the sort of utopian goals of a data centered architecture with no organizational silos and diverse applications residing within the same data cloud leveraging a scalable, distributed environment. Describing the path to realizing this dream, he talked about the four levels of Hadoop maturity:
,
,
,

,

Talking about how to make the right choices, he shared the concept of experimental enterprise, which he claimed is necessary to compete in a world that?€?s being rapidly digitized. The key characteristics of experimental enterprise are:,
,
These principles can be used within IT to build an experimental enterprise by employing six building blocks: cloud, DevOps, open source, agile development, platforms,and data science. Along with a brief explanation of these blocks, he demonstrated how they fit together to create an IT infrastructure ready to serve as a strategic advantage. It is also important to remain on track with technical demands when developing a solution. Regarding testing, Edd shared his mantra: ?€?always test and don?€?t trust?€?.
,
He emphasized on how important it is to consciously trade-off and score according to relative environments. In regard with vendor maturity, he said that Big Data is still an immature sector, Hadoop 2.0 is a key development as it introduced in memory databases, solutions are trending to become platforms and services play a large role in any spending. He concluded the session stating that understanding a vendor?€?s own requirement and conducting a thorough examination of options are the key points when developing a solution.
,
,
,
Change.org is the world?€?s largest petition platform with about 300 million signatures, 4000 declared victories in 121 countries and about 20 million users experiencing victory making petitions successful. Talking about the data challenges at Change.org, Fernand explained what it takes to identify potential users who are most likely to sign for a particular petition and predicting how many of them would actually sign it. Key insights from his approach towards this challenge are as follows:,
,
He suggested treating recommendation and discovery as a coherent product. Through matching the synonyms terms used across the data landscape and the delivery channels, he explained the deep similarity. For example, he mentioned featured petitions and similar petitions analogous to online feed and email push respectively.
,
Although there are tons of input sources to try but picking the right metric to optimize is the most difficult. Sharing the results of his work, he showed that collaborative filtering has led to 30% increase in overall signatures and 3x increase over baseline in additional signatures. A key learning is to always start with the simplest models such as basic similarity metrics. He concluded the session talking about the data-driven approach followed by Change.org for sponsored petition targeting.
,
Fernand?€?s presentation can be accessed: ,
,
,
,
,
Ian Timourian gave an interesting talk in which he used advanced methods of data science and visualization to analyze the poetry of Gertrude Stein. Ian has been exploring the algorithms and techniques utilized by Gertrude through visualization. His talk touched important topics such as novel techniques for the visualization of n-grams, methods of turning poetry into music and the intersection of art and data visualization.
,
He expressed the immense joy of data explorations, mashups, generative art, etc. The highlight of his talk was a transcription of the structure of Stein?€?s poetry to music. The next step for Timourianis to find a way to create a videogame-style visualization tool for cleaning and distilling data. This is not so far-fetched since he works at Paxata, a startup that makes a product for that purpose.
,
,
,
Sambavi started the session stating that Facebook?€?s data warehouse has grown rapidly over the years, and has posed unique scalability challenges. She briefly outlined the evolution of the analytics software stack in the last year (both storage and query engines) and shared details based on her experience of the data management and compute challenges at this scale. During the talk she mentioned various components of the analytics software stack, as follows:,
,
,
,Talking about data lifecycles, she categorized data as hot data, warm data and cold data. She explained how her team improved Corona by introducing resource sandboxing, online upgrades and re-startable job trackers.
,The corona can now scale to 4000+ node clusters and 120+ jobs/day. With some key challenges solved, Sambavi?€?s team is now focusing on the next big challenges: multi-temperate data, seamless multi-namespace query and distributed machine learning.
,
,  "
"
,
,One of the biggest challenges at big conferences such as Strata 2014 is that there is so much happening quickly and simultaneously that it is almost impossible to catch all the action. 
,We help you by summarizing the key insights from some of the best and most popular sessions at the conference. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference.
,


See also: 	,

,
,
,
,
Drew started with emphasizing that most of the real-life problems come from interdisciplinary areas and not just one specific area. Organized crime and corruption has global business of about 2-3 trillion US Dollars. Countries with very high amount of such activities are Russia, Montenegro, Kosovo, Eq. Guinea, North Korea, and Uzbekistan.When working with corrupt government official in anti-democratic and weak states becomes stronger, it threatens local and regional security.
,


Meanwhile, the proceeds from crime are eagerly sought by western banks, hedge funds and markets. He displayed details of a , as an example. Over half a billion US dollars were poured into this Latvian bank account of the phantom company in a period of less than two years.
,
He urged to the data science community to come forward and help them to stop this large-scale illegal activity by hack and track method. He concluded by saying that efficient investigative reporting is the result of cooperation between investigative journalists, programmers and others who want to use data to contribute to create a cleaner, fairer and more just global society.
,
,

,

,
,


Anand initiated the session referring to the retina in a human eye, which communicates with brain in real-time at 10 million bits per second. Real-time streaming analytics is not just about low latency queries over batch data. 
,He explained business transformation as a whole new domain of new possibilities and unexpected breakthroughs in operational efficiency. He classified business transformation use cases into three categories:

,
,


Next, he compared the two prevalent approaches to stream analytics ?€? using proprietary algorithms and doing it yourself. The major challenges with the former approach are vendor lock-ins (in other words, lack of flexibility) and no opportunity to leverage the open source movement. On the other hand, the ?€?do it yourself?€? has challenges such as managing the integration and management of open source tools; and also the significant delay in time to market.

,

After providing an overview of the current business landscape, Anand and Pranoy presented real-time streaming analytics as an offering from Impetus, which would provide various features such as high-speed data ingestion, elastic scaling, variety in data parsing, pluggable persistence, real-time index and search, dynamic message routing and many more. They summarized their approach as the iterative cycle of:,
,
,
,
,
,
Avery explained his motivation behind graph analysis by showing images, recommendation and network graph. Graph analytics have applications beyond large web scale organizations. Many computing problems can be efficiently expressed and processed as a graph, which can lead to useful insights that drive product and business decisions. 
,
Next, he described the lifecycle of Giraph, which was inspired from Google?€?s Pregel but it runs on Hadoop. He showed how most of the problems could be related with graphs when we think of sub-problems as vertices. Providing an overview of Apache Giraph data flow, he mentioned the few features of Giraph which are not in Pregel such as shared aggregators, master computation and composable computation.  
,
He briefly explained techniques such as balanced propagation, super-step splitting and avoiding out of core. He concluded by stating the future research and development efforts should focus on  evaluation of alternative computing models, performance, lowering the barrier to entry and applications.
,
,
,


,
,
Daniel started with providing an overview of how LinkedIn is addressing search quality issues through leveraging the economic graph. Social context means that the relevance of search results is highly personalized. He explained how machine learning ranks socially using model of tree with logistic regression leaves. Focusing on its customer base, LinkedIn is moving towards an entity-oriented search i.e. when searched for a term the results displayed should belong to all entities such as personal profile, company profile, employees of company, job openings, etc. He mentioned that query understanding acts as a relevance filter having phases such as segmentation, decoding, query rewrite resulting to new query. He also made an announcement that LinkedIn would soon have entity-driven search assistance feature.
,
Sriram talked about the unique infrastructure challenges posed by the efforts to improve LinkedIn search. He explained how they are leveraging Lucene while developing additional components to support LinkedIn?€?s specific needs. He asserted that it is extremely easy to build a search engine but difficult to get sophisticated. Next, he explained the LinkedIn Search Stack which leverages the search index served by Lucene to achieve statics rank based document ordering. Regarding performance, he mentioned offline data builds on Hadoop along with partial index updates. Scoring is done after retrieval, with the option to compute the costly features offline. Sriram concluded with suggesting that though this approach has been developed specifically for LinkedIn?€?s needs, the concepts and learning can be applied to a wide range of other problems.
,
,  "
"
,
,
August 24-27, 2014, New York, NY, USA.
,
, 
,
Welcome to KDD 2014, an interdisciplinary conference that brings together researchers and practitioners from all aspects of data mining, knowledge discovery, and large-scale data analytics.
,
,
,
The ACM KDD 2014 organizing committee would like to invite proposals for workshops to be held in conjunction with the conference.
,
The goal of the workshops is to provide an informal forum to discuss important research questions and practical challenges in data mining and related areas.
,
Novel ideas, controversial issues, open problems and comparisons of competing approaches are strongly encouraged as workshop topics. Representation of alternative viewpoints and panel-style discussions are also particularly encouraged for all the workshops.
,
Possible workshop topics include all areas of data mining and knowledge discovery, machine learning, statistics, and data and information sciences, but are not limited to these. Interdisciplinary workshops with applications of data mining and data sciences to various disciplines (such as medicine, biology, sustainability, ecology, social sciences, humanities, or aerospace) are of high interest.
,
For more information, visit 
,
,
,
,
,
KDD-2014 will feature tutorials on topics of interests to the research community as well as industry practitioners. We invite proposals for tutorials from active researchers and experienced industry practitioners.
,
We seek tutorials covering the state-of-the-art research, cutting-edge industry development and applications, and practical tools in a data mining direction that stimulate and facilitate future work. Tutorials on interdisciplinary directions, novel and fast growing directions, and significant applications are highly encouraged.
,
We solicit proposals of tutorials of two types: (1) lecture style short courses about data mining technical research and development, and (2) hands-on style for practical skills and tools. Each tutorial should be about 3 hours in length.
,
To submit a tutorial proposal (due 3/15), visit:
,
,  "
"
,
,
,, a highly innovative, peer-reviewed journal that provides a unique forum for world-class big data research, is pleased to announce a new Editor-in-Chief, Vasant Dhar, PhD, Professor, Head of the Information Systems Group, and Director of the Center for Business Analytics at New York University Stern School of Business. The Journal, published by ,, is available online and in print.
,
, has over 30 years of academic business experience in big data and big data technologies, and 70+ research articles published and funded by grants from industry and the National Science Foundation, and he brings his vision and in-depth knowledge, insight, and experience to the Journal. He teaches courses on Prediction Trading Strategies, Digital Marketing, and Digital Strategy. His research focuses on data-driven predictive analytics in finance, healthcare, and social phenomena, and networks.
,
Dr. Dhar joins a , of opinion leaders in the big data community, including 
,

  "
"
Most popular 
, tweets for Feb 26-27 were
,
The gap between data mining and predictive models - Facebook ""Relationship Data Mining"" not good for predictions 
,
,
Non-human math: Computer proof of the Erdos discrepancy problem is Wikipedia size, no human can check it ,
,
The gap between data mining and predictive models - Facebook ""Relationship Data Mining"" not good for predictions ,
,
,  "








"
By Gregory Piatetsky, Apr 12, 2014.
,
A young team of 3 UC Berkeley students (Brian Liou, Tristan Tao, and Elizabeth Lin)
has produced a Data Analytics Handbook which includes interviews with data scientists and tech leaders available at
,
, - free download.
,
Part 1 includes interviews with Data Scientists  from
LinkedIn, Cloudera, Facebook, Yelp, HG Data, and Flurry.
,
Top takeaways include:
,
,
,If you can't present your analysis into digestible concepts for your CEO to
understand, your analysis is only useful to yourself.
,
,
,Data analysts spend most of their time collecting and cleaning the data required
for analysis. Answering questions like ""where do you collect the data?"", ""how
do you collect the data?"", and ""how should you clean the data?"", require much
more time than the actual analysis itself.
,
,
,The greatest difference between a data scientist and a data analyst is the
understanding of computer science and conducting analysis with data at scale.
Data scientists only need a basic competency in statistics and
computer science and not all are Ph.Ds. New tools are empowering more  people to do data science.
,
Part 2 includes interviews with CEOs and Managers from
Mode Analytics, Smarter Remarketer, Cloudera, Stylitics, Flurry, Yhat, Persontyle, and BigML.
,
Top Takeaways include:
,
,
,The truth is, even in a quantitative major you are not taught what you need to
know to work in data analytics. There is a learning gap between academia and
industry that is best filled by doing projects. Find some sports statistics and do
your own analysis. Learn R so that you can complete this analysis, not just to
learn R itself. Also try Kaggle.
,
,
,The development of tools and popularity of programmers has caused black
box statistical analysis usage. Understanding selection bias vs. sampling bias
and the underlying assumptions to which statistical functions are built on will
make your opinions matter and your work invaluable.
,
,
,The power of data analytics is in taking open response questions and framing
them to be multiple choice. Therefore if you have the ability to filter a million
questions into options A through D, you are a data scientist for hire.
,
Here is , which includes interviews with Academics and Research Leaders, including Hal Varian (Chief Economist, Google), Tom Davenport (Professor, Babson College) and me - Gregory Piatetsky (Editor, KDnuggets).  "
"
        ,  "
"
,
,By F??lix-Antoine Fortin, Feb 20, 2014.
,

After more than 4 years of development, we are proud to announce the release of DEAP 1.0.0. You can download a copy of this release at the following web page.,

,
,

DEAP (Distributed Evolutionary Algorithms in Python) is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black box type of frameworks.

,

To get to know more about DEAP and the current release, we invite you to read the most recent article on DEAP published in SIGEvolution volume 6, issue 2, pp. 17-26.
,

,
,
An IPython notebook version of the article is also available.
,
,
,
This release includes:
,
,
Every changes of this release are detailed in the documentation.,

,
,
To help users translate code from 0.9.x to 1.0.0, we have also written a new porting guide that details every change required to use DEAP 1.0.,

,
,
Your feedback and comments are welcome at , or
deap-users at googlegroups dot com., You can also follow us on Twitter ,,
and on our blog ,.  "
"
        ,
By Gregory Piatetsky, Apr 14, 2014.
,
We update our 
, (Dec 2013) and find several interesting trends.
,
First, we found that growth slowed down in 2013 Q3 but resumed in 2013 Q4 and 2014 Q1.
,
The Figure 1 (below) shows quarterly growth rates in top 30 groups. Except for two groups: 
,  and , (not shown in Figure 1) which had big growth in 1 or 2 quarters and none in 2 other quarters, most groups show surprisingly similar pattern of decline in growth in 13Q3, followed by acceleration in 14Q1 and 14Q2. 
,
,
,
,
Here are the 10 largest groups (by membership as of March 31, 2014).  
We note that 7 largest were in the same order as in Nov 2013.  The 6 largest grew significantly faster than the next 4 groups.
,
,??,
However, there seems to be no strong correlation between group size and growth rate among all 30 groups.
,
Here are 10 groups with the fastest growth in the past 12 months (March 25, 2013 to March 31, 2014)
,
,??,
The chart below shows group growth vs group size. Color corresponds to age - redder is younger, bluer is older.  Group name abbreviations are in the table below. 
,
,
,
,
There are 2 main measures of group activity:  discussions (posts)/week and comments/week. 
Since these numbers clearly depend on the group size, we measure them per 1000 members.
We measure overall group activity as (discussions + comments / week) per 1000 members.
,
For 4 months ending in March 2014, activity level was 2.99/week, about 25% less than 3.97/week measured in Nov 2013.
,
The chart below shows group activity vs group size. Color corresponds to age - redder is younger, bluer is older.  Group name abbreviations are in the table below.
,
,
,
,
In 4 month ending in March 2014 the average activity level was 2.03 discussion/week per 1K members, and 0.96 comments/week per 1K members, or about 2.1 discussions/comment, well below 2.57 discussions/week per 1K members and 1.40 comments/week per 1K members measured in Nov 2013 (1.8 discussions/comment). This means that the 
,
,
The chart below shows average comments/week vs average discussions/week for all 30 groups, with a circle size proportional to group size and circle color corresponding to  activity change - green meaning increase, red decrease. We also show median lines for each dimension, which can be used to divide the groups in 4 quadrants.
,
,
,Fig 4: 4 Quadrants of Top Linked Analytics, Big Data, Data Science Groups: Commenting vs Posting
,
Several groups stand out: KDnuggets has the highest number of discussions/1000 members, while RDM has a highest number of comments. The median line divide the groups in 4 quadrants, which we can characterize as
,
,??,
The details are in the table with below, with groups ordered by the number of members.
The link to the raw data is at the end of the post.
,
The growth, comments, and discussions are in 
, if that value is 25% above average, 
, if 25% below average, and in black otherwise. 
,We note that there are only 4 ""triple green"" groups, that are significantly above average on growth, comments, and discussions:
,
,??,
,
,
Note: You can get actual data from the HTML source code of the LinkedIn group Statistics/Activity page.
,
Look for , and parse that data. Likewise for Discussions and Members. 
,
Thanks to Anmol Rajpurohit for collecting the membership, comments, and discussions data.
,
Here is , for the top 30 LinkedIn groups.
,See also
,, by Ted O'Brien, Aug 30, 2013.
,
Let me know which relevant groups were missed and what other trends you see.
,
,
 ,  "
"
,
,
,
,??,??
,
,
,
,??,??
,
,  "
"
        ,  "
"
Most popular 
, tweets for Apr 11-13 were
,
Another good list of influential Data Scientists on Twitter and what they do now #DataScience ,
,
Another good list of influential Data Scientists on Twitter and what they do now #DataScience ,
,
An Introduction to Deep Learning in #Java: From Perceptrons to Deep Networks (download Java deep learning library) ,
,
,  "
"
,
,
By Vincent Granville
,ISBN: 978-1-118-81008-8
,336 pages, April 2014
,
This  book is also part of Data Science Central 
, and is available on 
,
and on 
, website.
,
,
,Data scientists are in demand, and this unique book shows you exactly what employers want and the skill set that separates the quality data scientist from other talented IT professionals. Data science involves extracting, creating, and processing data to turn it into business value. This guide discusses the essential skills, such as statistics and visualization techniques, and covers everything from analytical recipes and data science tricks to common job interview questions, sample resumes, and source code.
,
The applications are endless and varied: automatically detecting spam and plagiarism, optimizing bid prices in keyword advertising, identifying new molecules to fight cancer, assessing the risk of meteorite impact. This book:
,
,??,
Contents: 
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:
,
,
,
,
,
Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's ""Data Mining: Failure to Launch"" - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at:
,
,  "
"
,
,
I recently had a chance to talk with Alison Derbenwick Miller, vice president, 
, about their program which supports both Oracle and open-source applications.?? It also offers resources to help high school students prep for the AP computer science test and 
, the 
,, a pioneering non-profit providing free online education world-wide.
,
,
,

Oracle Academy (Twitter: 
,,
FB: ,),
is the flagship program under Oracle?€?s corporate social responsibility education pillar.?? Its mission is to advance computer science education and make it accessible to students globally to drive knowledge, innovation, skills development, and diversity in technology fields.?? Last year, Oracle Academy supported more than 2.5 million secondary and post-secondary students globally, providing software, curriculum, professional development, and other resources with an in-kind grant value of more than US$2.7 billion.
,
,
,
Oracle Learning Library is an online resource that provides a broad range of self-service training and information about Oracle technologies.?? Oracle Academy is a philanthropic arm of Oracle focused on bringing resources into schools and universities to foster and support the study of computer science.?? For example, Oracle Academy provides comprehensive curriculum and supporting technology for computer science courses, such as Database Design and Programming with SQL and Java Fundamentals.
,
Oracle Academy also conducts hundreds of free professional development courses around the world to deepen teachers?€? understanding of computing, as well as to provide them with multiple strategies for teaching concepts that are central to an academic computer science curriculum.
,
Additionally, Oracle Academy hosts events designed to awaken children?€?s interest in computer science by teaching them coding skills with programs like Greenfoot and Alice.?? For teachers and students unable to attend one of these sessions, Oracle Academy offers ?€?,?€? and ?€?,?€? self-study courses free of charge through the Oracle Academy website.?? Related classroom curriculum for Alice and Greenfoot are also available on the Oracle Academy website, and via Curriki, the open online educational community.
,
,
,
Yes.?? Oracle Academy believes that few subjects will open as many doors for students in the 21, century as computer science.?? Therefore, Oracle Academy provides resources that support both Oracle and open-source applications, including resources and a developed classroom curriculum to help high school students prepare for the AP computer science A exam.?? In addition, Oracle Academy offers self-study courses, such as 
, and 
,; discounts on Oracle press books, which address a variety of computer science subjects; 
, and 
, articles, which offer CS students easy access to technical articles written by experts; and 
,, which provides free online learning content created by Oracle developers and trusted community members for Oracle and open source computer science technologies.
,
,
,
Analytics and data science are currently hot topics.?? Oracle Academy offers a 
, that includes a variety of online resources to help faculty and students new to analytics and data science navigate and learn about the exciting field of Big Data.?? The Big Data Resource Guide includes the ,, a recorded lecture series that includes sample problems and data sets.
,
,

,

The Oracle Academy supports learning about business intelligence (BI) through its 
,.?? Oracle Academy Business Applications members can download a variety of BI-related software programs and curriculum tools for teaching use in their classrooms.  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
,
Two years ago I was having coffee with a friend of mine and now colleague , in a cafe in San Mateo. ??We were talking about data science and analytics when he leaned in real close to say, ?€?Have you heard of Spark? This is going to change everything, again.?€? I had not heard of Spark and started researching the technology the moment I got back to my desk. ??I quickly realized what all of the fuss was about when landed on the ,
,
, is new technology that sits on top of Hadoop Distributed File System (HDFS) that is characterized as ?€?a fast and general engine for large-scale data processing.?€? ??Spark has three key features that make it the most interesting up and coming technology to rock the big data world since Apache Hadoop in 2005.,
,
, ?? ,
At Alpine, we have made it dead simple to get started with Spark by including the technology in our latest build out of the box. ??We require no additional software or hardware to leverage our extensive list of operators for data transformation, exploration, and building advanced analytic models. ??We leverage Hadoop Yarn (Hadoop NextGen) to launch Spark job without any pre-installation of Spark or modification of cluster configuration. This empowers our customers to have seamless integration of our Spark implementation and their Hadoop stack. ??For example, we have analyzed 50 Million rows of account data in 50 seconds on a 20 node cluster recently at last month ,
,
The screenshot below shows how Spark does a quick in-memory iteration.?? It uses a standard way to do the gradient aggregation, as implemented by Databricks, a company which commercializes?? the Apache Spark framework.
,

Also, see a demo at ,
,
Interested in learning more about Alpine Chorus and Spark? Head over to , to get started.

,
,?? is a a passionate data guru at Alpine Data Labs.  "
"
,
Latest ,, (Apr 16, 2014) ,:
,
,??,
Also
, (4) |
, (2) |
, (6) |
, (2) |
, (4) |
, (3) |
, (9) |
, (1) |
, (4) |
, (5) |
, (11)
,
,
,, a year after Boston Marathon Bombing  "
"
,
,

The Master of Information and Data Science (MIDS) is a degree program delivered online for professionals looking to become leaders in the field of data science. The program features a multidisciplinary approach that encompasses the entire data life cycle, from planning and gathering data to analyzing and presenting findings. Course work in this program examines issues of privacy, explores machine learning, and considers techniques for data storage and management.
,

Students in the program benefit from UC Berkeley?€?s strong ties to the Bay Area and Silicon Valley.
,
,
, ?? ,
At the recent GigaOm Structure Data 2014 conference, UC Berkeley , Dean Anna Lee Saxenian took to the stage to discuss the future of data science education and how top-tier training will help tomorrow?€?s data science professionals. She also spoke about the I School?€?s MIDS program and the role it plays in shaping the next generation of data science leaders.

,
You can read about Dean Saxenian?€?s appearance and watch a recording of the entire discussion ,.
,
To learn more about the MIDS program, you can ,.  "
"
By Gregory Piatetsky, Apr 16, 2014.
,
I recently had a chance to talk to 
, about their Open Analytics Platform.
,
GoodData was founded by Roman Stanek in 2007 in Prague, but is now based in San Francisco, CA.
It received over $75M in funding, some from very well-known people or VCs, 
including Angel funding round with Tim O'Reilly and Ester Dyson, and later funding from
VC firm Andreessen Horowitz.
,
GoodData claims over 130,000 global users for its cloud-based business intelligence platform.
,
It recently released to good reviews its
,, with 5 main components for collecting, storing, combining, analyzing and visualizing data.
,
,
,
GoodData's Open Analytics Platform serves both IT on the data governance side and line of business on the data discovery side. By allowing GoodData to own the problem of collecting, storing, and combining data, IT can uplevel their own work to and save time for more valuable efforts. Similarly, GoodData offers an intuitive front-end interface for business users to conduct data discovery, reporting, visualization, and ad hoc analysis--anywhere, anytime.   
,
,
,
,
,
The breakdown of our current users by industry (in 2013) by Vertical Markets is
,
,??,
,
,
Those companies focus on enabling end users with data science tools and machine learning, leaving the process of building data models to the customer. GoodData, on the other hand, manages the data modeling process for the customer, in addition to data storage, collection, combination, and preparation, and also provides the end user with intuitive front end data discovery tools so they can gain insight from the data quickly. 
,
This hides the complexity from the end user and allows them to focus on business critical problems rather than managing and organizing data. Additionally, with the Open Analytics Platform customers have the ability to build their own models on top of our platform if they require additional customization or flexibility. 
,
,
,
The most popular connectors and APIs provided by GoodData include the Salesforce Connector, Ruby API for automating common tasks like KPI event notifications, and the suite of APIs for the data connectors.
,
,
,
GoodData's Analytics Engine supports the process of data combination, and with no-cube technology, allows for flexible slice-and-dice functionality. The engine functions with multi-level caching for exceptional performance, provides dashboards, reports and metrics abstracted from the underlying data model, and is extensible with additional predictive analytics and other advanced modules.
,
,
,
GoodData's pricing model is subscription based. Pricing starts at $2500 per project and the data and user volume affects the final numbers. 
,
,
,
We plan to continue expanding the suite of predictive and analytic functions, grow data discovery and continue to build out the back end infrastructure to provide most the dependable and scalable cloud performance.
,
Learn more at ,  "
"
Most popular 
, tweets for Apr 14-15 were
,
,  "
"
,
,
,
,
It?€?s good to get out of the office and clear your head once in a while. That may mean a walk in the park at lunch, or it may mean going to a seminar or technical conference. With the latter, the benefits can far outweigh the immediate gratification you feel upon leaving the cubicle or home office behind. Among them:
,
,
, ?? ,
I just got back from a particularly well-run conference, BZ Media?€?s 
,. It took place in one of my favorite cities, Boston, and covered an area I need to get up to speed on quickly: Big Data in all its aspects.
,
A good conference has something for everyone?€?every day and every hour. 
This one had, for beginners, a full-day crash course on Hadoop, an introduction to Neo4j, and a class on Cassandra, among others. For those in the middle, there were talks on Hive and Pig, HBase, various introductions to NoSQL databases for SQL pros, and so forth. For advanced developers, there were all manner of entrees: Getting Started with R and Hadoop, Hadoop architectures, H2O and Mahout, topological data analysis and more.
,
Best were the vetted instructor/presenters, all of whom were under strict orders not to ?€?sell?€? themselves or products or services. BZ has a talent for picking good people to lead seminars, and for a good reason?€?they live or die on word of mouth and overall reputation. If someone shells out a couple thousand dollars for leading-edge instruction, it better be good. And worthwhile.
,
So what did I learn that I can carry forward into my work? How Yarn works in conjunction with MapReduce2, the nature of graph databases, how to vacuum up data from the internet, how to build a big data product, the innards of MongoDB, and data analysis using Hive, among others.
,
Like the best ones, this was a working conference. Participants were sometimes asked to download special software or VM sandbox versions. People were clearly tired by the end of the day and there were no distractions like cocktail parties sponsored by large vendors. The conference was all business.
,
Not that there weren?€?t light or entertaining moments. Data scientist Scott Sokoloff (TE Connectivity) did a nice overview he called ?€?Lessons Learned from Advanced Analytics Projects.?€? He talked about plopping a pizza restaurant in a corn field miles from any town because analysis told him?€?correctly?€?that it would better serve the three towns it stood between and make the company more money. He was right. In encouraging companies to develop their own analytics capabilities he pointed out that SAS?€?s revenues rise and fall with GDP because not enough do. And he said that the only thing a 4.0 GPA predicts is ?€?that the person will get an A in his next college class.?€?
,
It?€?s well worth the time and effort to get out of the office and learn something. You might think you?€?re taking a few steps back, but you will find when you get back?€?assuming you?€?ve taken the conference seriously?€?that you?€?ve made a great leap forward.
,
, is executive editor for business and IT at Apress Media 
(,).
Original: 
,. 
,
See also other reports on this conference:
,
,??,
, I also attended Big Data TechCon in Boston, but only briefly in 2014. Here is my report from 2013 Big Data TechCon:
,
,
  "
"
,
,
,
At a recent INFORMS Analytics Conference in Boston (Mar 30 - Apr 1, 2014)  I met Peter Bruce, President of  
,, and our discussion grew into this interview, covering the mission of Statistics.com, the future of analytics education, MOOCs, are statistics disconnected from Big Data, and more. 
,
, is the President of The Institute for Statistics Education at Statistics.com. He is the developer of Resampling Stats software (originated by Julian Simon in the 1970's), and taught resampling statistics at the U. of Maryland and elsewhere. He is the co-author of , (Wiley, 2006, 2nd ed. 2010), , (Wiley 2014) and many journal articles.  He serves on the American Statistical Association Advisory Committee on Professional Development.  Before taking up statistics, Bruce served in the U.S. Foreign Service; he has degrees (not in statistics) from Princeton, Harvard and the U. of Maryland. 
,

,
,
, Our mission is to provide online education in statistics, analytics, data science and OR to a global audience.  We have small classes, 4-weeks long, that give learners the opportunity to interact with recognized experts while not requiring you to be online at specific times.  In ed-tech jargon, a course of ours is a ""SPOC"" (small, private, online class).
,

,
,
, I try to keep abreast of what is standard and what is new in our fields, by attending conferences, speaking with publishers, eliciting suggestions and advice from our faculty (we have over 50 instructors), participating in meetups, and talking with people like you.  We offer 110+ courses and four certificates:
,
, ?? ,
,
,
, Our topical coverage is more comprehensive than either MOOC's or university analytics programs; we offer 110+ courses in areas like predictive modeling, data mining, text analytics, operations research, social networks, statistical programming, and, of course, traditional statistics.   
,
We are more affordable and offer more flexible scheduling than universities; courses last only four weeks, with 4-5 courses starting every Friday.  Compared to MOOC's, we offer a very different experience:  classes are small (generally less than 30), you get personal interaction with a leading expert over a period of 4 weeks, and we provide human help with your software and coding, as well as human review of your project work to help you understand where you went wrong. Compared to both MOOC's and universities, our courses are more focused, concentrating on one or two topics.
,
,
,
, The word ""statistics"" has different meanings. To a sports fan, it means the numbers that quantify what is going on in a game, or with a player.  The word's origin is in the Latin word for state, and early kings and generals organized the first systematic collection of statistics to determine whom they could tax and draft into the army.  In the 1800's it came also to mean the discipline that concerned itself with counting, measuring, analyzing and interpreting data.  Later, it also came to mean, more narrowly, the study of how to draw inferences from sample data.
,
,
,
,
Our job is to provide data professionals the skills, knowledge and tool proficiency they need for jobs that involve analytics, which includes statistics.  I think this is pretty close to the idea of ""data science,"" a term that is relatively new, and to Big Data as well (we offer courses in Hadoop, Python, NLP, SQL, text mining, etc.).  Though I suppose Big Data, being a term of great popular interest, may mean different things to different people.  However, I think the major challenges in Big Data, and 95% of the effort, lie in the sphere of extracting, transforming, cleaning and wrangling data to the point where it is amenable to analysis, and in deploying the model in a way that provides value.
,
Recruiters, for convenience, need shorthand terms like ""data scientist,"" or ""statistician,"" and job postings (rather than arguments among academicians) may be the ultimate arbiter of terminology.  Beyond that, I think it does not pay to get too hung up on definitions and professional classifications in a rapidly changing area like this.  Some of the biggest contributions to data science have come from people who take what is valuable from several fields and synthesize progress - for example, statisticians like Jerome Friedman and Daryl Pregibon who combine statistical backgrounds with machine learning techniques.
,
So, however you define the fields, we provide data analytics professionals with the capabilities and understanding they need to do their jobs.
,
,
,
, I suppose some are.  Many statisticians who completed their studies more than a few years ago did not really have exposure to Big Data, and did not necessarily have to do much programming, instead using software like SPSS and SAS.  And there is still a lot of statistical analysis that happens not with big data, but with data collected following a plan of sound statistical design.  Clinical trials, surveys, and research studies, for example. Statistics departments nowadays, though, recognize their responsibility to provide students with effective programming skills to deal with a varied range of data and analysis requirements.
,
,
,
,
,
I think it is also relevant to ask whether Big Data is disconnected from statistics, particularly in the areas of study design and understanding the role that random chance and variability play in data.  More data does not always mean more accurate data, and some familiarity with statistics can help determine when more is better.  Statisticians also have a great deal to contribute to the issue of false discovery, and this is happening, to some extent, in the area of genomics.
,
John Elder, founder of a highly regarded data mining consulting firm, talks about the ""Massive Search Effect,"" in which enough searching and comparing will always turn up something ""meaningful.""  A background in statistics really helps in distinguishing real phenomena from artifacts of chance.   I heard a presentation just recently by a computer science professor who oversaw a team project to assess correlations (interactions) among 3000 drugs, meaning 9 million drug pairs. The focus was on the significant computational challenges, and it was only in passing that he mentioned that 4.5 million chi-square tests were performed.  To a statistician, of course, the real challenge is in deriving meaning from such results, and judging whether the conclusions are real, or just the product of chance.
,
,
,
, Some predictions:
,
,
,
,
See discussion on ,
,
, No one person can do everything and be all things to all people - there are no unicorns, as some have called the intersection of all three domains in Conway's diagram.  Even though we have 110 courses at Statistics.com, we focus on the analytic sphere and bring in the other skills as they relate to it.  So we teach how to query databases, but not how to design and administer them.  We teach some programming, and how to use programming skills to build a data science workflow, but we are not a university computer science department.
,
That said, even though a company may have a team of specialists, continually broadening their knowledge beyond their individual specialties is valuable, both to the company and the individual. The Operations Research Society (INFORMS), touts the ""T"" model for professionals in its ""CAP"" certification process.  The ""T"" meaning deep vertical knowledge in one specialty, and familiarity across a broad range of others.  The more the database person, the programmer and the statistician know about what each other does, the faster and more efficient the development process.
,
This also relates to the question of whether to hire from outside, or train from within.  The ""T"" model suggests training from within to broaden the top of the T for existing employees , and hiring from outside where an essential deep skill is missing.  

,

(Training and Development, April 2014).
,
,
,
, by Kate Atkinson and , by David Mitchell. Both have masterful uses of time as a non-linear variable to be manipulated.
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
"
"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
"
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
,
,
,??,??,
,??,
,??,??
,
,
,??,??,
,??,??
,
,  "
"
,Data scientists, the elusive kingpins in the Big Data movement, are earning base salaries of well over $200K, are younger, overwhelmingly male, have at least a master?€?s degree and probably a Ph.D., and one in three are foreign born, according to the first-ever study looking at salaries, education levels, gender and geographical location of this new profession. Almost half of all data scientists are on the West Coast working for technology and gaming companies.
,
,

 says Linda Burtch, managing partner of ,, the executive search firm which conducted the study. ?€?It is only in recent years that data storage has become sufficiently inexpensive that many firms have begun to save enormous sets of unstructured data, and many firms have begun to employ data scientists.?? Businesses will need to use unique strategies to find and keep the talent necessary to truly tap the potential of Big Data.?€?
,
The Burtch Works study found data scientists are:
,
,??,
While there is no available number of employed data scientists, it could be estimated that only 4,000 to 6,000 individuals are filling this role in the country.
,
,
,
,
?€?The profession of data science has emerged to solve the peculiar computing and analysis problems of data that are not only big but also unstructured and messy,?€? says Burtch. ?€?Unlike other analysts, data scientists know how to use tools invented specifically to store and retrieve massive amounts of data efficiently, tools such as Hadoop, Pig and Hive. More so than other analysts, they are proficient with methods for distinguishing the useful from the useless in messy data, such as data visualization and pattern recognition.?€?
,
The study finds that firms outside of the West Coast and the technology industry are fighting an uphill battle to hire data scientists, which means that offering interesting problems to solve, competitive salaries, and having a buttoned up, swift recruiting process is critical in order to land these candidates.?? Like other Big Data professionals, many data scientists are foreign-born, so companies that are serious about hiring this talent need to be open to transferring or sponsoring visas.
,
?€?It?€?s not only data scientists who need a foundation in quantitative problem solving and analytics,?€? adds Burtch. ?€?Business professionals of all types need to have at least a basic mathematical foundation in order to compete today, evidenced by the trend of MBA?€?s returning to school for additional education in predictive analytics. In 15 years, those who can?€?t tackle quantitative business problems will be given a permanent pink slip.?€?
,
The study of 171 data scientists was conducted by Burtch Works, an Evanston, Ill.-based executive recruiting firm which specializes in the placement of quantitative business professionals, now one of America?€?s hottest job segments due to demand for Big Data professionals. Phone interviews were held with each participant during the past 30 months.?? A full copy of the study can be found at , (requires a short registration form).  "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,  "
"
Most popular 
, tweets for Apr 16-17 were
,
NYU @aghose on Est. Demand for Mobile Apps , Learn more: NYU Stern MS in Business Analytics ,
,
A map of where nobody lives in the US - 4,871,270 US Census blocks with zero population ,
,
A Gentle Introduction to Scikit-Learn: A Great #Python Library for #MachineLearning ,
,
,  "
"
By Gregory Piatetsky, Apr 20, 2014.
,
This presentation was made at SF Bay Area Machine Learning Meetup in April 2014.  
,
,
Paco Nathan compares/contrasts several open source frameworks which have emerged for Machine Learning workflows, including KNIME, IPython Notebook and related Py libraries, Cascading, Cascalog, Scalding, Summingbird, and Spark/MLbase.
,
The analysis develops several points for ""best of breed"" and what features would be great to see across the board for many frameworks... leading up to a ""scorecard"" to help evaluate different alternatives. 
,
The 9 criteria proposed for evaluating Machine Learning Data Workflows are:
,
,??,
Paco Nathan also reviews the PMML standard for migrating predictive models, e.g., from SAS to Hadoop.
,
Watch at ,.
,
Here is also a 
,  from an earlier meeting in Seattle, Jan 2014.
,
, 
,
Paco Nathan, is a ""player/coach"" who's led innovative Data teams building large-scale apps for 10+ years, and worked as an OSS evangelist for the past 2+ years. Expert in distributed systems, machine learning, cloud computing, functional programming -- with a focus on Enterprise data workflows. 
Paco received his BS in Math  and MS in CS degrees from Stanford, and has 30+ years technology industry experience ranging from Bell Labs to early-stage start-ups. 
,
See also an interview and opinion by Paco Nathan in KDnuggets:
,  "
"
,
,
I had a pleasure of working with Michael Brodie  when we were both at GTE Laboratories in 1990s, where he was already a world-famous researcher and a department manager.  I recently met him at another conference, and he graciously agreed to answer my questions for KDnuggets readers.  Michael is still very sharp, very active, and busy -  he answered these questions while flying from Boston to Doha, Qatar where he is advising 
,. See also his 
, from MIT/White House workshop. 
,
, has served as Chief Scientist of a Fortune 20 company, an Advisory Board member of leading national and international research organizations, and an invited speaker and lecturer. In his role as Chief Scientist Dr. Brodie has researched and analyzed challenges and opportunities in advanced technology, architecture, and methodologies for Information Technology strategies. He has guided advanced deployments of emergent technologies at industrial scale, most recently Cloud Computing and Big Data. In his Advisory Board roles Dr. Brodie addresses current and emergent strategic challenges and opportunities that are central to the charter and success of the organizations. As an invited speaker Dr. Brodie has presented compelling visions, challenges, and strategies for our emerging Digital Universe in over 100 keynote speeches in over 30 countries and in over 100 books and articles
,
Throughout his career Dr. Brodie has been active in both advanced, academic research and large-scale industrial practice attempting to obtain mutual benefits from the industrial deployment of innovative technologies while helping research to understand industrial requirements and constraints. He has contributed to multi-disciplinary problem solving at scale in contexts such as Terrorism and Individual Privacy, and Information Technology Challenges in Healthcare Reform.
,
Below is part 1 of our extensive interview. See also
, and
,.
,
,
,
, Three most important database research advances:
,
,
,
,
,
, Alas the database industry, like all industries, has a legacy problem that stifles innovation. It has taken over 30 years to emerge from the relational era. The most important recent database industry development came from outside the database industry, it is Big Data and its marketing arm called MapReduce and its data sidekicks, Hadoop and NoSQL. , , Smart folks at Yahoo!, Google and other places saw value in data, non-database data, and thus emerged MapReduce, Hadoop, and NoSQL- generally crappy database ideas but it woke up the database industry. Hadoop and NoSQL are growing in demand. In time it will be seen that they are amazing for a very specific problem domain, embarrassingly parallel problems, but it is a money pit for everything else. ,
,
,
,
Relational Databases have two extremely well established roles. Conventional row stores serve the OLTP community as the backbone of enterprise operations. These blindingly fast transaction processors are moving in-memory. OLTP stores are modest in number and size (,
This is also where we turn from polishing the relational round ball 
(, ,. ICDE, page 606. IEEE Computer Society, 1993) and focus on the other dozen or so other DBMS categories. Taking over is relative; none of the 12 other categories has more than 3% of the database market. Graph databases serve graph applications like networking in communications, telecom, social networks, and of course NSA applications! But what is wonderful about these emerging classes of data-domain specific DBMSs is that we are only now discovering the rich use cases that they serve.
,
The use cases define the DBMSs and the DBMSs help formulate the use cases. SciDB is a superb example of managing scientific data and computation at scale. It is awkward for both communities ?€? database folks who done speak linear algebra or matrices, and scientists who only speak R. Exciting times. For a little fun look at the ,.
,
,
, ?? ,
,
,
, What a great question. Thank you for asking because it caused me to think about what I have really enjoyed over 40 years. Somehow CSAIL at MIT and the Faculty of Computing and Communications at EPFL jump to mind.
,
,
There are scary smart people at those places. Like climbing mountains it both scares and exhilarates me. To be frank my jobs at big enterprises in hindsight are confusing. I guess I was window dressing because my role did not feel like it had impact. So getting motivated and scared at MIT and EPFL are probably top, so there?€?s number one. Why? Just look down 5,000 feet and ask why am I here? 
,
Second is a combination of Advisory roles at US Academy of Science, DERI, STI, ERCIM, Web Science Trust, and others because they gave me a sense of collaborating, challenging, and contributing. How cool is that?
,
Third would be working at startups like Data Tamer and Jisto. Imagine waking up in the morning and thinking you might change the world. That requires that I conceive the world not just differently, but so that it solves someone?€?s REAL problem. Even more cool.
,
,  "
"
By Gregory Piatetsky, Apr 21, 2014.
,
,
Last week, I watched new Microsoft CEO Satya Nadella ""Accelerate Your Insights"" Keynote given in San Francisco. Satya, as Microsoft people are calling him, was low-key but eloquent and impressive in presenting Microsoft vision for Big Data and ""Ambient intelligence"" - pervasive intelligence that arises from learning from all that data. I also learned that Microsoft people pronounces ""Azure"" very differently from how French say ""Azure"" in ,
,
Microsoft COO Kevin Turner and data platform CVP Quentin Clark gave great demos, which showcased real-time access to large databases and very interesting visualizations capabilities, using Microsoft own data.
,
Three main new features of the Microsoft data platform are:
,
,??,
Quentin Clark blog 
,
,
explains three aspects of this platform: people, data, and analytics. 
,
He writes:
,
,
[ ,. ]
,
For more information, see 
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for April 21 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Apr 18-20 were
,
Data Workflows for Machine Learning ,
,
Cross-validation pitfalls for regression and classification models - and how to avoid them #DataScience ,
,
Cross-validation pitfalls for regression and classification models - and how to avoid them #DataScience ,
,
,  "
"
,, An Introduction
,
by Reza Zafarani, Mohammad Ali Abbasi, and Huan Liu (Arizona State University)
,
May 2014, Cambridge University Press
,
The growth of social media over the last decade has revolutionized the way individuals interact and industries conduct business. Individuals produce data at an unprecedented rate by interacting, sharing, and consuming content through social media. Understanding and processing this new type of data to glean actionable patterns presents challenges and opportunities for interdisciplinary research, novel algorithms, and tool development. 
,
Social Media Mining integrates social media, social network analysis, and data mining to provide a convenient and coherent platform for students, practitioners, researchers, and project managers to understand the basics and potentials of social media mining. It introduces the unique problems arising from social media data and presents fundamental concepts, emerging issues, and effective algorithms for network analysis and data mining. Suitable for use in advanced undergraduate and beginning graduate courses as well as professional short courses, the text contains exercises of different degrees of difficulty that improve understanding and help apply concepts, principles, and methods in various scenarios of social media mining.
,
Download a complete per-publication draft of the Social Media Mining book in PDF format at
,
,
,
The reader is allowed to take one copy for personal use but not for further distribution (either print or electronically). 
,
You can also purchase the book at 
, or 
,.
,
Contents:
,  "
"
,
,
This annual award introduced in 2008 recognizes excellent research by
doctoral candidates in the field of data mining and knowledge discovery.
,
This annual award by ACM SIGKDD will recognize excellent research
by doctoral candidates in the field of data mining and knowledge
discovery. The KDD Doctoral Dissertation Award winner and up to two
runners-up will be recognized at the KDD conference, and their
dissertations will have the opportunity to be published on the KDD
Web site (,). The award winner will receive a plaque,
a check for $2,500. The award winner will also receive a free registration
to attend the KDD conference. The runners-up will receive a plaque at
the conference. The winner and runners-up will be invited to present
his or her work in a special session at the KDD conference.
,
,
,
The final dissertation defense should take place at the nominee's host
institution before the submission deadline. Furthermore, the final
dissertation defense must not have taken place prior to January 1st, 2013.
Nominations are limited to one doctoral dissertation per department or
academic unit. Submissions must be received by the submission deadline.
,
Each nominated dissertation must also have been successfully defended
by the candidate, and the final version of each nominated dissertation
must have been accepted by the candidate's academic unit. An English
version of the dissertation must be submitted with the nomination. A
dissertation can be nominated for both the SIGKDD Doctoral
Dissertation  Award and the ACM Doctoral Dissertation Award.
,
,
Submission Deadline: April 30, 2014.,
Notification of Awards: July 19, 2014.,
Award Presentation at 
,: August 24-27, 2014, New York City, USA.
,
,
All nomination materials must be submitted electronically to:,
,
,
Please use ""SIGKDD Dissertation Award Nominations"" in your subject line.
,
All nomination materials must be in English. PDF format is preferred
for all materials. Late submissions will not be accepted. A nomination
must include:,
,
,
Additional information is available at:
,
,
Please direct questions to the Award Committee Chair:
, Huan Liu,
,Arizona State University,,
,
,  "
"
,
,
Dates: June 17-20, 2014
,Location: UC Berkeley, Berkeley, CA
,Website: ,
,Contact: ,
,
Early registration deadline: May 1, 2014.
,Registration deadline: June 1, 2014.
,Register at
,
,
You can apply to present a poster on registration page above.
,
Synopsis: The 2014 Workshop on Algorithms for Modern Massive Data Sets
(MMDS 2014) will address algorithmic, mathematical, and statistical challenges
in modern statistical data analysis. The goals of MMDS 2014 are to explore
novel techniques for modeling and analyzing massive, high-dimensional, and
nonlinearly-structured scientific and internet data sets, and to bring together
computer scientists, statisticians, mathematicians, and data analysis practitioners
to promote cross-fertilization of ideas.
,
Organizers: 
,
,??,
Confirmed speakers from Berkeley, CMU, Databricks, Google, 
Harvard, IBM Research, JHU, MIT, Microsoft Research, Princeton, 
Stanford, UW, and more - see full list at 
,  "
"
, 
,
Big Data Innovation Summit 2014 (Apr 9-10, 2014) was organized by Innovation Enterprise at Santa Clara Convention Center in Santa Clara, CA. The summit brought together experts from industry as well as academia for two days of insightful presentations, workshops, discussions, panels and networking. It covered areas including Big Data Innovation, Data Analytics, Hadoop & Open-Source Software, Data Science, Algorithms & Machine Learning, Data Driven Business Decisions and more. 
,
People attending such conferences would agree that there is so much happening quickly and often simultaneously at conferences that it is almost impossible to catch all the action. , These concise, takeaway-oriented summaries are designed for both ?€? people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference. As you go through it, for any talk that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
Here are highlights from selected talks on day 1 (Wed Apr 9):
,
,, Chief Data Scientist at WellDoc Inc. started by establishing context about how serious problem we are facing when it comes to heath issues. 22.3 million people (about 7.1% of US population) is suffering from diabetes and its prevalence will almost double by 2030 looking into the immense rise of patients. 
,
He proposed a solution to such problems by providing mobile prescription therapy, real-time coaching to patients and leveraging a cloud based expert analytics system. He explained that Mobile Health (mHealth) Factors are of three kinds: Clinical, Behavioral & Engagement and working at the intersection of these three is the need of the hour. Similar to Data Science Value Chain he walked through all steps involved in mHealth Data Journey by giving examples and convinced the audience that a lot can be improved in HealthCare Sector by developing analytics based mobile applications.
,
,, Chief Engineer at eBay talked about Big Data ecosystem at eBay and discussed a number of use-cases where near-line and offline analytics enable personalization of the customer experience. He explained the complicated process behind personalizing and engaging users. The stack for this is built with a base of data platform performing tasks such as tracking, experimenting and analyzing. On top of it lies the Insights layer which helps personalize, recommend and market. Above Insight layer there are two layers in parallel: User Engagement and User Messaging, which together enable an awesome customer experience. There were many challenges including multi-screen, data quality and governance. 
,
He also discussed the Personalization Service Data Flow and In-cache Predictive Model Evaluation. He emphasized that as site speed matters the most his team performs machine translation process in less than 50 ms using the architecture displayed below.
,
,, Director, Project Management from YarcData focused on delivering high performance analytic platforms for the data discovery market.  YarcData is the analytics subsidiary of Cray, a super computing pioneer. He quoted from Financial Times: ?€?Big data is a vague term for a massive phenomenon that has rapidly become an obsession with entrepreneurs, scientists, governments and the media?€?.  YarcData brings technology of Cray to solve large volumes of data and it enables one to go for interactive processing from the old-fashioned batch-oriented approach. He also discussed a few use cases from various verticals.
,
,, Software Engineer at LinkedIn commenced by giving some statistics about LinkedIn such as 270M+ users as per March?€?14 with addition of 2 new member per second, 3M company pages, 90% companies use LinkedIn to hire. One of the main reasons user logs in LinkedIn is to find jobs. The actual value that a recommendation has is 50% based on LinkedIn. He discussed two metrics of evaluation: Upside and Downside. 
,
Upside metrics are about users getting relevant jobs and downside metrics are more about users getting offending/irrelevant jobs. Giving a quick overview of the recommendation algorithm, he highlighted few challenges such as Entity Resolution (IBM has 13000+ variations and many job titles having same role), Geo Location (recommending a job in New York to a developer in Bay Area, some locations can be categorized as Sticky from where very less professionals migrate), network effect (knowing the chances of one leaving company looking into his/her network). 
,
At the end, he gave a hybrid recommendation algorithm as a solution to the challenges mentioned before. 
,
,, Director of Analytics at eBay shared Big Data lessons from the Obama campaign. She recommended that data acquisition should not be our focus, rather we should focus on building good business intelligence tools which enable us to make use of all the data we already have. A best-in-class BI tool will leverage data from multiple sources, reorganize data around key business questions, simplify the data to display only the most relevant metrics, visualize the data to make trends easy to see and benchmark results against KPIs. 
,
It is very important to have daily measurement and based on the insights from data, pivot the message, channel and targeting for the campaign. She concluded her talk with an emphasis on Content, saying that: ""If you are only measuring dollars spent, you are missing more than half of the story"".
,
,, Data Science Lead at Trulia is leading the Data Science team at Trulia, which is applying machine learning, network science, NLP, and computer vision technologies to the large datasets found in the real estate domain. Trulia is a real estate portal with over 35 M monthly users and 14.5 M mobile users.  
,
At Trulia there are following teams: Econometrics/PR team, Analytics Team, Geo team and Data Science team. For Data Science team the mission is three-fold: to transform data into new content, improve content relevance and improve monetization. The skills of the team include machine learning, hacking, text mining, image mining, network science and data visualization. The analytics process followed by his team can be summarized as:  ,
Understand problem -> Process Data -> Experiment -> Productionize -> Integrate
,  "
"
,
,
, (Apr 9-10, 2014) was organized by Innovation Enterprise and held in Santa Clara, CA. The summit brought together experts from industry and academia for two days of insightful presentations, workshops, discussions, panels and networking. It covered areas including Big Data Innovation, Data Analytics, Hadoop & Open-Source Software, Data Science, Algorithms & Machine Learning, Data Driven Business Decisions and more.
,
There is so much happening at such conferences that it is impossible to catch all the action. , These concise summaries are both for people who attended the conference but would like to re-visit the key sessions for a deeper understanding and for people who did not attend.  Keep checking KDnuggets for next few weeks as we would soon publish exclusive interviews with some of these speakers.
,
See also: ,
,
Here are highlights from keynote sessions on day 2 (Thu, Apr 10):
,
,, Data Scientist at Facebook commenced her talk by emphasizing on her ideas for improving data science infrastructure at Facebook. She stated how the data science infrastructure at Facebook evolved and how important it is to build a culture of sound inference for impactful analysis. She talked about metrics: challenges to figure the right metric, maintain consistency, and provide data quicker. 
,
, from Experimentation Tools team at Facebook then took over and talked about moving metrics. He showed how metrics changes with changes in the product. He explained following challenges while moving metrics: many metric dimensions, many dimensions running in parallel, and many teams involved. He concluded by saying that ?€?Making and moving metrics is not just about providing the data and tools, but about building a culture of sound inference?€?.
,
,, CTO & Co-founder at RedPoint Global initiated his talk by pinpointing the advantages of Hadoop2.0 over Hadoop1.0, specifically how it removes the need for Map Reduce programmers. He explained the current Big Data challenges: (1) skill gap, (2) maturity and data governance, and (3) converting raw data into useful information.  RedPoint Global is the first firm to bring a YARN complaint ETL/data quality toolset to the market. He discussed about the key features of RedPoint Data Management on Hadoop. He concluded the talk by describing which kind of companies should care about Hadoop 2.0 and how RedPoint solutions meet their needs.
,
,, Director, Data Science at Quintiles provided an overview of the current progress in Data Science for Healthcare and what lies ahead. The current benchmarks state that in order for a firm to stay competitive, it should already be able to:,
,
, ?? ,
Next, in order to cross ?€?the chasm?€? firms should be:
,
,
, ?? ,
Finally, the ultimate goal should be to deliver personalized health through real-time insights across multiple dimensions.
,
,, Head of Intelligent Storage, Samsung talked about wearable computing and speech recognition.  Discussing about automatic speech recognition he mentioned three models: Acoustic, Pronunciation & Language and Machine Learning models (with focus on neural networks). He discussed Cloud based ASR such as Siri and various concerns such as latency. He introduced Hierarchical Speech Recognition (HSR), and described how Samsung was able to save power and increase efficiency through HSR. 
,
,, Professor at George Mason University shared his thoughts on using Big Data for data-driven discovery and decision support. He disagreed with the Wikipedia definition of Big Data and suggested that Big Data should rather be defined as ?€?Everything, Quantified and Tracked!?€?. After listing all the Big Data characteristics (the V?€?s: Volume, Variety, Velocity, Veracity, Validity, Value, Variability, Venue, Vocabulary), he focused on Veracity and Variety to explain how Big Data can be used as an experimentation-bed for valuable research. He explained the ?€?Decision Science-as-a-Service?€? model through examples from his work on Astrophysics as well as common life scenarios.
,
,, Data Scientist at Yahoo spoke about how Yahoo manages teams going from Data to Knowledge to Insights and follow some organizing principles. From organization perspective data mining engineers are caretakers, data analysts are explorers and insight analysts are connection makers.  He discussed driving efficiency in organization of data science teams by going through permutations of generalists vs specialists, centralized vs de-centralized, and how to best address teams in each model. He stated that at Yahoo, focus is on flexibility.
,
,, Chief Architect at PayPal gave the closing keynote making a point that though many firms are now adopting many of the new data technologies like Hadoop and NoSQL, most of their existing data sources and toolsets still provide value ?€? so there is value in leveraging all data sources. He highlighted that data manipulation is best handled at the system level, while data analysis is better managed at the enterprise level.
,
The summit also featured Big Data Innovation Awards Ceremony. The winners were:
,
,, a leader in high-speed data processing and management solutions was announced it has won the Big Data Start Up Award given at the Big Data Innovation Summit, Santa Clara, CA. The award was based on the company?€?s mission and its flagship product, the Talksum Data Stream Router (TDSR), which helps IT and data centers manage Big Data for applications that require massive amounts of processing.
,
Big Data Innovation Summit awarded Cisco with the Big Data Project Award, and YarcData, a Cray company, with the Big Data Tech Provider Award.  "
"
,
,
,
FICO is a leader in credit scoring and applying predictive analytics to business and
,.
,
However, increasingly more accurate predictions of consumer behaviour also significantly erode consumer privacy.?? Witness the uproar when Target was able to , before her father knew.
,
I recently had a chance to discuss Big Data and Privacy with Andrew Jennings, FICO chief analytics officer and head of FICO Labs - here is the interview.

,

,

,

,
, Both! The reason privacy is such a hot button today is that we have truly entered a new era of what you might call human digital evolution. There is much greater ability today to use analytics to understand people?€?s behavior, and those analytics are fed by both traditional commercial data streams, such as the information at credit bureaus, and new data streams, such as social media. Information has been made digital with the Internet. Physical objects have been ?€?digitized?€? and linked with the Internet of Things. Increasingly, people?€?s behavior, needs and attributes are also being digitized and linked by complex algorithms.
,
This represents a big change, and like all big changes it causes friction. People are already used to an increased level of data sharing and availability, particularly Millennials, who have grown up at the center of their own media universes, and share nearly every aspect of their lives with an online ?€?audience.?€? However, many people will never want to give up any of their privacy, and these people may cause governments to enact protective regulations.
,
What this means is that the next several years will be a roller-coaster for privacy acceptance and regulation. For every new use of data that seems cool, there?€?s another use that seems creepy. The recent NSA revelations, for example, sent a shock wave worldwide. But for every reaction and new regulation, there will be additional steps that make data freer. That?€?s because human beings are social animals who naturally see the benefits of sharing their data. You only need to look at the global growth of social media to realize that.
,
And, of course, nearly every day you can read headlines that illustrate the potential downside of data availability. That includes identity theft and massive breaches of data that can lead to financial losses and widespread distrust of the systems that are supposed to secure the information that people share. These issues require appropriate regulation and vigorous enforcement ?€? and are part of the privacy-analytics dynamic.
,
,
,
, When it comes to privacy, there really is no such thing as global. Notions of privacy are inherently cultural. So even large organizations that want to think globally must act very, very locally if they are to avoid a backlash. Some countries will always be more restrictive because their people collectively have different attitudes toward privacy.
,
,
,
,
, It?€?s a good question, and one that FICO has wrestled with for nearly 50 years. The short answer is that you build the best models you can with the data you can use. And you focus rigorously on the task at hand ?€? for a FICO Score, building a model that will produce the best risk assessment for every consumer. We don?€?t use proxies for prohibited characteristics because we have no interest in trying to circumvent the law. There is plenty of predictive power in the credit bureau data, if you?€?re clever enough to decode it. After 50 years of building credit risk models, we know how to get the maximum power from the data.
,
There are other areas where use of demographic data ?€? for example, gender -- is allowed in specific areas such as retail marketing. It may improve marketing results to target certain products at men or women. Some people may not be happy about that. But it allows people to obtain more relevant and useful offers and service. That?€?s the key thing.
,
,
,
, Responsible data scientists do use analytic technology in a way that meets society?€?s needs to protect privacy. For example, data scientists often work with ?€?anonymized?€? data that can?€?t be traced back to a specific individual, but which still enables them to build analytics that detect specific patterns. In fact, if you use the data you have well enough, you can sometimes avoid acquiring additional data. For example, lenders have long used credit scoring in order to limit the amount of data they need to collect from a borrower applying for a loan.
,
You can adapt your analytics and decision-management systems to whatever data is available, so of course you can make decisions without scouring all available data. That said, withholding data generally will lead to poorer decisions. So better decision-making will always tend to utilize more data, or at least better analysis of the existing data.
,
I don?€?t see Big Data and privacy as completely antithetical. Our society?€?s notions of privacy aren?€?t fixed ?€? they will continue to evolve, not just here but around the world. People have generally been willing to exchange information for services, such as when you apply for a loan or a new cell phone contract. The challenge comes when people learn that information they exchanged with one party for one purpose is being used by other parties for other purposes. Society and business will grapple with this for the next few years, because this is the way data is often used in a Big Data world. For example, President Obama has asked White House counsel John Podesta to produce a report on Big Data and privacy.
,
,
,
,: I expect to see more of this trend. People realize there?€?s value in their information, and they want to know how they can get a piece of the action. Similarly, there is an emerging group we call the ?€?data volunteers?€? who will exchange their own data with businesses in order to receive better service. For example, they will ?€?train?€? the Amazon search engine by editing their recommendation list, hoping it will truly do their shopping for them.
,
We are really still at the early stages of people figuring out how this explosion in Big Data analytics can benefit them. There has been a lot of professed benefit, but we need to continue moving from professed benefit to more real benefit. People today don?€?t always see how they are getting benefit from analysis of their data, behaviour and attitudes.
,
,
,
,
, For us, the most important privacy issue is the need to take a balanced approach when imposing rules on the use of personal data. This requires careful deliberation, and it?€?s a dialogue FICO has long participated in. It?€?s often the case that regulators think they are striking a blow for the public by restricting data use, only to find that those restrictions put people and businesses at a disadvantage. Used well, predictive analytics can give people more choices, greater protection from financial crime and other benefits, while still protecting people from discrimination and respecting their privacy.
,
Because FICO started in the world of credit decisions, we have always worked in a highly regulated environment, with tight controls on what data can be used and how. That?€?s given us a corporate culture acutely aware of the importance of privacy in all that we do. We also work in areas such as retail marketing, where there are far fewer restrictions and different customer attitudes. A person might not mind getting an offer for a product they don?€?t want, but people certainly , mind if they don?€?t get a mortgage or car loan at the best possible terms. As Big Data analytics is used in more and more decisions by more and more businesses, finding the right balance between business benefit, personal benefit and personal privacy will take time to sort out. This is one of the critical issues of our time.
,
, is FICO's chief analytics officer and head of FICO Labs. He has held a number of leadership positions at FICO since joining the company in 1994, including a stint as director of FICO's European operations. Before joining FICO, Andrew served as head of unsecured credit risk for Abbey National plc, where he introduced account behavior scoring and mortgage scoring. He served as a lecturer in economics and econometrics at U. of Nottingham and has a Ph.D. in economics.
,
He blogs at 
,.
,
,
 ,
  "
"
,
,
, (Apr 9-10, 2014) was organized by Innovation Enterprise at Santa Clara Convention Center in Santa Clara, CA. The summit brought together experts from industry as well as academia for two days of insightful presentations, workshops, discussions, panels and networking. It covered areas including Big Data Innovation, Data Analytics, Hadoop & Open-Source Software, Data Science, Algorithms & Machine Learning, Data Driven Business Decisions and more.
,
People attending such conferences would agree that there is so much happening quickly and often simultaneously at conferences that it is almost impossible to catch all the action. , These concise, takeaway-oriented summaries are designed for both ?€? people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference. As you go through it, for any keynote that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
Here are highlights from keynote sessions on day 1 (Wed Apr 9):
,
,, Principal Architect at PayPal gave the opening keynote explaining why ?€?Internet of things?€? should really be the ?€?individual network of things?€?. He highlighted that the number of devices and their connectivity, availability and partitioning will play a key role in future. During his keynote he shared an interesting fact: ?€?Every person is predicted to generate over 20 Petabytes of data over the course of a lifetime?€?.
,
,, Principal Data Scientist at Live Nation started by introducing the problem their data science team faced, which was basically to detect anomalies and optimize resources to minimize cost. The data they worked on was extremely bursty with a million requests per minute during peak and consuming data at this rate made it a big data problem.  He explained how batch models have demonstrated limited success for the problem at hand, thus prompting the need for adaptive online-learning, and the various trade offs and challenges encountered along the way for such an approach.
,
,, Principal Data Scientist at Bing emphasized that most of us know the good part of Big data but we ignore the bad and ugly part. He then explained the bad and the ugly through the following 5 Big Data myths:
,
,
,
He also reminded and emphasized on a key data science principle: ?€?Correlation does not imply causation?€?. He shared a few examples illustrating cases when strong correlation intuitively misled to wrong causation inferences.
,
,, SVP, Data & Insight at D&B gave his talk on unlocking the value of big data and analyzing how businesses are turning to new and exciting ways of leveraging predictive analytics. He advised the audience ?€?it?€?s not just about data, it?€?s not just about the math, it?€?s the relationships among data which matters.?€? He demonstrated how data innovations are radically enhancing power of predictive analytics, such as new models for accessing size dimensions. He explained that with the vast amounts of data comes great opportunity and new types of risks.
,
,, Director, Market Strategy at IBM mentioned that there are immense possibilities of analyzing all available data and the opportunity of deriving value from it is infinite. She put forward following 5 key points for successful implementation of big data projects:			
,
,
,

,, Chief Data Officer at New York University talked about Data Governance covering privacy and security. She stated that the root of data governance is accountability and an accountable organization has following three elements: 
,
,
,, CEO, BeyondCORE said that knowing the right question to ask is the most difficult part in data analysis. He emphasized that if we have a large number of variables from which we need to select a few to chart out on a graph, it would generate some billions of possible combinations, making it almost impossible to analyze. He referred to the McKinsey Global Institute report to state that there is a large shortage of 140K-190K data analysts and pointed out that the much bigger problem is 1.5 billion business managers who are not data-savvy. He indicated that the future of analytics is simplicity, ubiquity, and actionability; in other words, ?€? Advanced Analytics for All (A3). In his A3 approach, automated algorithms rather than expert Data Scientists conduct the analysis, the results are delivered in a manner business users can understand without Statistics or Computer Science skills, and users can easily overlay human intuition on top of the automated analysis.
,

,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Apr 21-22 were
,
Cheat Sheets for Data Scientists: R #rstats, #Python, #SQL, HiveQL, and more ,
,
Sweet! Chocolate Consumption strongly correlated to Nobel Prizes/capita (r=0.79) ,
,
Cheat Sheets for Data Scientists: R #rstats, #Python, #SQL, HiveQL, and more ,
,
,  "
"
,    "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
The , was publicly released this month.
,It covers data for 2012, and includes information for the 100 most common inpatient services, 30 most common outpatient services, and all physician and other supplier procedures and services performed on 11 or more Medicare beneficiaries. The data details the payments individual providers received from Medicare for the treatments they administered to America?€?s seniors and certain younger people with disabilities. 
,
We can see how Medicare program paid out $77 billion to more than 880,000 health care providers. The data also shows the providers' names, addresses, specialties, billing rates, averaged reimbursed amount, number of Medicare beneficiaries served and number of services provided for every Medicare provider. 
,
The map I created with cartodb shows distribution of payments by states, with Florida, California, Texas, Alabama having highest reimbursements. Link: ,
,
Medicare data release can lead to a better understanding of health costs, and providers can deliver better care. The accuracy and transparency of the process will be improved by making it public. 
,
One clear finding is that the variance in reimbursements is very large. It ranges from nearly $21 million to a single Florida ophthalmologist to the $2,984 for the average certified nurse midwife. The average amount paid per provider varies a lot across the specialties. The three specialties with highest average amounts were clinical laboratory at $1,758,701, radiation therapy at $1,293,347, and Portable X-ray at $707,306.
,
The data could also be used to identify potential fraud or waste. Although the number alone does not tell a story, the insiders will see whether the physician?€?s billings are out-of-line with the rest of the industry. Some individual physicians received particularly high sums. It may reflect fraudulent doctor behavior or perverse incentives that lead doctors to overuse a procedure. 
,
But there are also concerns of misunderstanding by the public. Since the data don?€?t show details of patients?€? diagnoses or dates of procedures, the patients may be misinformed by raw data. The high cost may be due some specialists having sicker patients and performing necessary, but costly surgeries.
,
, is a master student in Data Science program at New York University. She has done several projects in machine learning, deep learning and also big data analytics during her study at NYU. With the background in Financial Engineering for undergrad study, she is also interested in business analytics.
,
See also, 
- ,
,
- ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,Here is an example which explains how KXEN recommendations work.
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
  "
"
,
,
,
  "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
,
,
, is the Chief Economist in the Office of the Utah State Auditor. He focuses on data analytics, builds models and analyzes trends to ensure Utah?€?s public entities are realistic, effective, and efficient. Previously, he was a Senior Economist in the Utah Governor?€?s Office, where he built forecasts for state $5B economy, and was the Governor?€?s principal tax policy adviser. He was also a Statistician for the U.S. Census Bureau, producing official estimates for service sectors of the US economy totaling roughly $1 trillion.
,
David recently delivered a talk at Big Data Innovation Summit 2014 held in Santa Clara on ?€?Public Policy and the Complexity of Analytics ?€?. He mentioned that data science paired with the science of complexity provide an opportunity for economists and all public sector managers to better understand how world works. He also stated that large scale data analytics will have a prominent role in creating more useful tools to better guide decision making in government.
,
Here is my interview with him: 
,
,
,
, There are pockets of excellent analytics within both the federal and state governments.?? But I don?€?t think a critical mass has been reached within governments to broadly and systematically adopt enterprise wide policy that will lead to more data driven public policy decisions.?? I believe these decisions will be made over the next 5 years as the value of the extensive collections of government data becomes glaringly obvious.
,
Two good benchmarks to separate the leaders from the laggards in the government sector are 1) the extent to which data is made available to both the public and data community in useful ways and 2) whether a government employs the right human capital in executive positions with both subject matter expertise and fluency in data use.?? Examples, include websites that open up the government checkbook to inspection (,, ,, and ,).?? A good GUI is useful to a curious citizen, but of at least equal import is the ability of power data users to download the same data in bulk for true analytics.?? Governments will need Chief Data Scientists (in function if not title) to marshal the resources to truly leverage the value of the data governments maintain to drive more efficiency.
,
,
,
,: My typical day, in roughly equal measure, involves absorbing subject matter expertise, wrangling data, and communicating methods and results.  Government is in many lines of business.  Some of the things I have studied in the last few months: how to better optimize our supply chain management systems for our liquor stores, how to systematically detect fraud in administering hundreds of millions of dollars in benefit payments, how to redesign our human resource policies around the use and management of sick leave for a workforce of 20,000 employees.
,
Over the last year, I have gained access to a dozen or more information systems and pulled billions of records out of widely divergent systems siloed across many departments and agencies.  After transforming, exploring, and wrestling the data into a useful form, I write code to build models to detect problems and find efficiencies.  I validate the models with field research or through discussion with subject matter experts.  I also speak with Utah?€?s elected policy makers and agency management and write reports to communicate the research findings and recommendations for improving a wide swath of public policy.
,
,
,
, My most memorable ?€?aha?€? experience with analytics changing public policy was when the Utah Legislature passed a bill in 2007, without a dissenting vote, to reform the state?€?s individual income tax to a single tax rate system.  Changing the amount of tax people pay is extremely contentious, especially when raising taxes on 10% of taxpayers.  I spent several years building the micro simulation model of the tax system to predict the effects of thousands of versions of potential reform.  Policy makers used the results of the model to narrow reform based on the effects they desired.  It was the most intense data driven policy making I have witnessed.  People had more trust in the process when they could see their suggestions working through the model.
,
,
,
, Yes, I think making privacy screened data publicly available can help resolve contentious policy problems.  Through crowd sourcing, everyone would have access to the foundational data, we would be better able to coalesce around facts that are universally accepted.  It is not a panacea, and will not eliminate all disagreements or varying interpretation of the facts, but it will form a shared foundation which should increase the probability of finding common ground and agreement from which to build compromise.
,
,
,
, There are privacy concerns.  They will only grow with time.  Government is behind the curve in making rules for Big Data.  In coming years there will almost surely be more regulation regarding how private data can be collected, aggregated, and used.
,
,
,
, I am not sure it is a skill, but I think the most important disposition to have as an agent in the Data Science field is curiosity about the world.   My interactions with some government employees compared with many of my talented research analysts over the last decade leads me to believe it is not an easily learned or exercised attribute.  But curiosity only takes a person so far.  It is wasted if it does not lead to subject matter expertise and a deep grasp of domain knowledge.
,
As far as skills, I think it is necessary for people to grasp a range of data architectures, be familiar with a few different programming languages while really mastering at least one.  Always be aware of the context of an analysis to better communicate the methods and results to people who are not data scientists.  Decision makers want a relevant story and plain language recommendations, not the really great math, impressive programming acrobatics, or even the beautifully artistic visualizations of data worthy of a nice frame. 
,
,
, 
, I?€?ve been reading Why Does the World Exist by Jim Holt, an exploration of the philosophical underpinnings of reality and the outlines of how people have approached the question through time.  "
"
Most popular 
, tweets for Apr 23-24 were
,
#BigData Cartoon: ""It does look similar - but this one is powered by Hadoop"" ,
,
#BigData Cartoon: ""It does look similar - but this one is powered by Hadoop"" ,
,
,
Great list: 9 Python Machine Learning Books covering scikit-learn, Web 2.0, algorithms, social web, NLP, Vision ,
,
,  "
"
,
,
,??,
,??,??
,
,
,??,
,??,??
,
,  "
"
By Gregory Piatetsky, Apr 27, 2014.
,
,,
to be held August 24-27, 2014 in New York, NY, USA is
, in Data Mining, Data Science, and Knowledge Discovery.
,
KDD-2014 technical program will feature the research track, the applications track, the industry practice track, the KDD Cup competition, workshops, tutorials, oral presentations, keynotes, poster sessions, panels, exhibits, demonstrations, and much more.
,
Workshops are an important part of the conference (indeed KDD conference arose from a workshop I organized at IJCAI-89 conference). The workshops provide the forum for discussing the leading edge research ideas and this year, there are 24 exciting workshops - see below.
Paper submission dates vary but generally in early June.
,
For latest information, see , .
,
8 Full-Day workshops: 
,
,
,
16 Half-Day Workshops:
,
,  "
"
,
,
,
Data mining, data analysis, these are the two terms that very often make the impressions of being very hard to understand ?€? complex ?€? and that you?€?re required to have the highest grade education in order to understand them.
,
I can only disagree, and as with anything in this wonderful life of ours, we only need to spend a certain amount of time learning something, practicing it, before we realize that it?€?s not really all that hard.
,
No doubt that there are very smart people in this World, working for large corporations such as Google, Apple, Microsoft and plenty more (,), but if we continue to look up to them; we will always think it?€?s hard, because we have never given ourselves the chance to look at real examples and facts.
,
By learning from these books, you will quickly uncover the ?€?secrets?€? of , and ,, and hopefully be able to make better judgement of what they do, and how they can help you in your working projects, both now and in the future.
,
I just want to say that, in order to learn these complex subjects, you need to have a completely open mind, be open to every possibility, because that is usually where all the learning happens, and no doubt your brain is going to set itself on fire; multiple times.
,
,
, gives us brief introduction on the complexity of data problems, how to look at them from a better perspective, and whether we should bother trying to solve the impossible. He gives perfectly good and understandable examples, and is a nice little data book to add to your collection, it?€?s quality knowledge at free of charge.
,
You can grab a copy of this book by filling out the fields on the right hand site. (I think filling them blank also works)
,
,
,
This Wikibook aims to fill this gap by integrating three pieces of information for each technique: description and rationale, implementation details, and use cases.
,
The description and rationale of each technique provide the necessary background for understanding the implementation and applying it to real scenarios. The implementation details not only expose the algorithm design, but also explain its parameters, in the light of the rationale provided previously.
,
Finally, the use cases provide an experience of the algorithms use on synthetic and real datasets.
,
,This book is exactly what I was talking about at the beginning of this post, it features plenty of real-life experiences, that are aimed at beginners to help you better understand the whole process of data manipulation, and how algorithms work.
,
It?€?s apparently a work in progress, but there are plenty of chapters already available, though it seems that the last one is a few months overdue right now. Nonetheless, the first few chapters are essential to grasp the basics and highly recommended.
,
,This is a very high quality book that has more advanced techniques and ways of doing things included, it?€?s still being edited / written and is set to be released at some point, later this year. You can view the official draft by , (PDF), you?€?ll be amazed at how much information there is to browse!
,
It?€?s perfect for those learners who like to learn from illustrations and plenty of real-life examples.
,
,
,
I mentioned some large companies like Google, and Apple, and the reason for that is very simple: we see data mining and analysis everywhere, not just specific sciences and subjects.
,
In reality, , heavily depend on algorithms that have been built on top of high quality data science knowledge, and the same goes for advertising companies, which is the main topic of discussion in this white-paper / eBook.
,
, briefs of us on data science, and how it essentially is more than just a set of tasks related to data mining. In his own words, it?€?s more of an art form that, an interacts with more industries than some may believe.
,
In addition, data science is much more than simply analyzing data. There are many people who enjoy analyzing data and who could happily spend all day looking at histograms and averages, but for those who prefer other activities, data science offers a range of roles and requires a range of skills. Let?€?s consider this idea by thinking about some of the data involved in buying a box of cereal.
,
,In couple of short words, this book is perfect for those who want to learn more about data mining on the web, and it discusses the most common set of problems when designing for the web and working with data that the web is giving us.
,
It will provide you with plenty of examples and tasks to do at the end of each section, and is also a fairly beginner friendly book; requiring of you to have some previous experience with data algorithms, some math and database experience wouldn?€?t hurt either.
,
,School of Data is a great place to be, they offer a wide variety of courses targeted at all levels of expertise, and this Handbook is perfect alongside their course material. What I really love about this handbook is that it gives you plenty of follow-up links on the web, to make project creation easier.
,
A good example is links to websites that have previously built data sets, essential to those who want to learn more about data and how it works!
,
,We are going to conclude our list of free books for learning data mining and data analysis, with a book that has been put together in nine chapters, and pretty much each chapter is written by someone else; but it all makes perfect sense together.
,
The main focus of this book is text mining, and the evolution of web technology and how that is making an impact on data science and overall analysis. Great book to have!
,
,
,
There is no better way to learn than from books, and then going out in the world and putting that newly found knowledge to the test, or otherwise we?€?re bound to forget what we actually had learned. This is a beautiful list of books that every aspiring data scientist should take note of, and add to his list of learning materials.
,
What books have you read in order to help you begin your own journey in data mining and analysis? I?€?m sure that the community would love to hear more, and I?€?m eager to see what I potentially let slip through my fingers myself.
,
Reposted by request. Original: , .
,
,
 ,  "
"
By Gregory Piatetsky, Apr 27, 2014.
,
,
MLTK is a collection of
,machine learning algorithms
,in Java,
, developed by
,, 
,a PhD student at Cornell.
,??,??
,??,??
,
MLTK is designed for building predictive models and further development. 
For  questions or suggestions, email
,. 
,
MLTK currently supports:
,
,??,
,??,
You can download the 
,,
which requires Java 7, and is released under the BSD license.
,
Documentation:
,  "
"
,
,
,
,
It is vital for a company to know where its user base is located. User geo-localization can be easily implemented using the Open Street Map extension and its results can be easily visualize, either as a static map or as a movie, using the Image Processing extension of KNIME. Both extensions have been made available by the KNIME community. The goal of this study is to get more insight about the KNIME user base geographical distribution, in order to better plan for future community events.
,
,
,
Such geographical information is available via the connecting IP addresses. From the Apache weblog file of the KNIME web server, visitor data, including their IP addresses, are extracted. The time window is restricted to the week following a new KNIME release. In fact, during these days, regular KNIME users are more likely to download the latest version of the tool, producing a more reliable picture of their geographical distribution.
,
,
,
IP addresses are mapped to geographical locations and displayed on a world map using the KNIME Open Street Map integration extension, made available by the KNIME community. The world map can either host a summary of all KNIME downloads during the selected week or it can be re-drawn for each day producing a world map sequence.
,
,
,
The visualization of a long sequence of map images can be problematic on a static report, but not in a movie. A particular node in the KNIME Image Processing extension, also made available by the KNIME community, converts image sequences to movies. This node is used and a movie is produced, showing the dynamic evolution of KNIME downloads around the world and day by day following a new release.
,
,??,
,
This very interesting study covers weblog reading, geo-localization, and image processing to represent the dynamic distribution of the KNIME downloads in time and around the world. All workflows are available on the KNIME EXAMPLES public server under ?€?008_WebAnalytics_and_,OpenStreetMap?€?; the whitepaper can be found at ,; and the KNIME software can be downloaded for free from ,
,
, (,) 
 is not only an expert in data mining, machine learning, reporting, and data warehousing, she has become a recognized expert of the KNIME data mining engine, on which she published three books: (
,, 
,, 
,).
,Previously Dr. Silipo worked as a freelancer data analyst for many companies in Europe. She led the SAS development group at Viseca (Z??rich, Switzerland), implemented the speech to text and the text to speech interfaces at Spoken Translation (Berkeley, USA), and  developed a number of speech recognition engines in different languages at Nuance Communications (Menlo Park, USA). She got her doctorate in biomedical engineering in 1996 from U. of Florence (Italy).  "
"
,
,
Here is ,.  This is part 2.
See also
,.
,
, has served as Chief Scientist of a Fortune 20 company, an Advisory Board member of leading national and international research organizations, and an invited speaker and lecturer. In his role as Chief Scientist Dr. Brodie has researched and analyzed challenges and opportunities in advanced technology, architecture, and methodologies for Information Technology strategies. He has guided advanced deployments of emergent technologies at industrial scale, most recently Cloud Computing and Big Data. In his Advisory Board roles Dr. Brodie addresses current and emergent strategic challenges and opportunities that are central to the charter and success of the organizations. As an invited speaker Dr. Brodie has presented compelling visions, challenges, and strategies for our emerging Digital Universe in over 100 keynote speeches in over 30 countries and in over 100 books and articles.
,
,
,
, Consider the data universe. Since the 1980?€?s I have said in keynotes that the database and business worlds deal with less than 10% of the world?€?s data most of which is structured, discrete, and conforms to some schema. With the Web and Internet of Things in the 1990s massive amounts of unstructured data began to emerge with a growth rate that was inconceivable while shrinking database data to less than 8%. The EMC/IDC claim that our Digital Universe is 4.4 zettabytes and will double every two years until 2020 when it will be 44 zettabytes.  Amazing!
,
[, ?€? A profound, casual comment of my departed friend, Gerard Berry, ,]
,
In 1988 or so you, Gregory, and a few others saw the potential of data with your knowledge discovery in databases ?€? a radical idea. Little did others, including me, realize the potential of this, now named Big Data. Even though Big Data is hot in 2014, almost 30 years later, it?€?s application, tools, and technologies are in their infancy, analogous to the emergence of the Web in the early 1990s. Just as the Web has and is changing the world, so too will Big Data. 
,
, , Big Data is qualitatively different from database data that is a small subset of Big Data. It offers far greater potential thus value and requires different thinking, tools, and techniques. Database data is approached top-down. Telco billing folks know billing inside out so they create models that they impose, top-down on data. Data that does not comply is erroneous. Database data, like Telco bills must be precise with a single version of truth, so that the billing amount is justifiable. Due in part to scale, Big Data must be approached bottom up. More fundamentally, we should let data speak; see what models or correlations emerge from the data, e.g., to discover if adding strawberry to the popsicle line-up makes sense (a known unknown) or to discover something we never thought of (unknown unknowns). Rather than impose a preconceived, possibly biased, model on data we should investigate what possible models, interpretations, or correlations are in the data (possibly in the phenomena) that might help us understand it. 
,
, Big Data is a different, larger world than the database world. The database world (small data) is a small corner of the Big Data world. Correspondingly Big Data requires new tools, e.g., Big Data Analytics, Machine Learning (the current red haired child), Fourier transforms, statistics, visualizations, in short any model that might help elucidate the wisdom in the data. But how do you get Big Data, e.g., 100 data sources, 1,000, 100,000 or even 500,000, into these tools? How do you identify the 5,000 data sources that include Sally Blogs and consolidate them into a coherent, rationale, consistent view of dear Sally? When questions arise in consolidating Sally?€?s data, how do you bring the relevant human expertise, if needed, to bear ?€? at scale on 1 million people? Many successful Big Data projects report that this data curation process takes 80% of the project resources leaving 20% for the problem at hand. Data curation is so costly because it is largely manual hence it is error prone. That?€?s where Data Tamers comes to the rescue. It is a solution to curate data at scale.
,
We call it collaborative data curation because it optimizes the use of indispensable human experts. , It took me about a year to understand that fundamental difference. I have spent over 20 years of my professional life dealing with those amazing Data Integration platforms and some of the world?€?s largest data integration applications. Those technologies and platforms apply beautifully to database data ?€? small data; they simply do not apply to Big Data.
,
To emphasize what is ahead, here is a prediction. Data Integration is increasingly crucial to combining top-down data into meaningful views. Data Integration is a huge challenge and huge market that will not go away. Big Data is orders of magnitude larger than small or database data. Correspondingly Data Curation will be orders of magnitude larger than Data Integration., The world will need Data Curation solutions like Data Tamer to let data scientists focus on analytics, the essential use and value of big data, while containing the costs of data preparation. In addition to Data Tamer there are some very cool data curation products contributing to addressing the growing need and creating a new software market. What is also cool about data curation is that it can be used to enrich the existing information assets that are the core of most enterprise?€?s applications and operations. Of course, the really cool potential of data curation is that it makes Big Data analytics efficiently available to allow users to discovering things that they never knew! How cool is that?
,
For more on Data Curation at scale, see Stonebraker et al : 
, In , (Conference on Innovative Data Systems Research).
,
,
,
, I am having a blast with , ?€? some amazingly talented young engineers [PhDs actually] with lots of energy and a killer idea. Jisto is an exceptional example of the quality you ask about in the next question.
,
,Cloud computing enabled by virtualization is radically changing the world by reducing the cost and increasing the availability of computing resources. Can you imagine that only 50% of the world?€?s servers are virtualized?
,
Pop quiz [do not cheat and read ahead].
,
,
,??,??
,??,??
Answer:  Virtual machine CPU utilization is typically less than 50% while physical servers are less than 20%, due to risk and scheduling challenges, but mostly cultural.
,
, whether on premises or in public or private clouds, thus reducing costs by 75?€?93% over acquiring more hardware or cloud resources.
,
Jisto provides a high-performance, virtualized cloud-computing environment from underutilized enterprise or cloud computing resources (servers, laptops, etc.) without impacting the primary task of those resources. Organizations that will benefit most from Jisto are those that run parallelized compute-intensive applications in the data center or in private and public clouds (e.g., Amazon Web Services, Windows Azure, Google Cloud Platform, IBM SmartCloud).
,
Jisto is currently looking for early adopters for its beta program who will gain significant reduction in the cost of their computing possibly avoiding costly data center expansion. So talk to us , 
,
,
,
, There are armies of people who evaluate the potential of startups. The professional ones are call ""Vulture"" Capitalists (VCs). The retired ones are called Angels. Like any serious problem there is due diligence to determine and evaluate the factors relevant to the business opportunity, the technology, the business plan, etc. as the many books, e.g  [R. Field, ?€?, by Bill Aulet?€?, Journal of Business & Finance Librarianship, vol. 19, no. 1, pp. 83?€?86, Jan. 2014. ] and formulas suggest. 
,
If you are reading a book, then you don?€?t know. , Andy Palmer, a serial entrepreneur, good friend, and very smart guy said ?€?Do it once really well then repeat.?€? Andy ought to know, Data Tamer is about his 25th startup.
,
At First Founders I can do some technology, Jim can do finance, Howard can do business plans. Collectively we make a judgment. But good VC?€?s are the wizards. They have Rolodexes., When their taste says maybe they refer the startup to the relevant folks in their network who essentially do the due diligence for them. Like at First Founders, the judgment is crowd sourced, actually what we call at Data Tamer, it is ,. I have a growing trust of the crowd and especially of the expert crowd. 
,
,
,
, The technical challenge that stays with me is that addressed by the Verizon Portal, Verizon?€?s solution for Enterprise Telecommunications ?€? providing Telecommunication service to enterprise customers, such as Microsoft. Verizon, like all large Telcos, is the result of the merger &acquisition of 300+ smaller telcos. Each had approximately 3 billing systems; hence Verizon acquired over 1,000 billing systems. Billing is only one of over a dozen systems categories, including sales, marketing, ordering, and provisioning. Providing a customer like Microsoft with a telephone bill for each Microsoft organization requires integrating data potentially from over 1,000 databases. As is the case for most enterprises, Verizon and Microsoft reorganize constantly complicating the sources to be integrated, like Microsoft, and the targets, Verizon?€?s changing businesses, e.g., wireline and FiOS. Every service company faces its little-discussed massive challenge.
,
,Integrating 1,000s of operational systems is a backward looking problem. The cool forward-looking problem was Verizon IT?€?s Standard Operating Environment (SOE). Prior to cloud platforms and cloud providers, Verizon IT (actually one team) sought to develop an SOE onto which Verizon?€?s major applications (0ver 6,000) could be migrated to be managed virtually on an internal cloud. What a fun challenge. When the team left Verizon as a group over 60 major corporate applications, including SAP, had been migrated. Smart folks, good solution that failed in Verizon. ,. The SOE is being reborn in the infrastructure of another major infrastructure corporation.
,
Finally, the next most interesting and yet unsolved industry challenge was getting over the legacy, that Mike Stonebraker and I addressed in ,, Morgan Kaufmann Publishers, San Francisco, CA (1995) ISBN 1-55860-330-1 (M. Brodie and M. Stonebraker).
,
How do you keep a massive system up to date in terms of the application requirements and the underlying technology or migrate it to a modern, efficient, more cost effective platform?
,
Enterprises tend to invest only in new revenue generating opportunities often leaving the legacy problem to grow and grow. So existing systems like billing languish and accumulate. , Now where are my blue shoes? I suggested to Mike Stonebraker that we rewrite our 1995 book. He did not even respond to the email, suggesting that it is largely a political problem and not technical, no matter the brilliant technical solution provided.
,
, If you are a CIO, ,  "
"
By Gregory Piatetsky, Apr 29, 2014.
,
With Data Scientists and Big Data Skills 
,, 
new KDnuggets Cartoon from Ted Goff looks at Data Scientist Salary Negotiation Tactics.
,
,
,
,
See also other 
,.  "
"
Most popular 
, tweets for Apr 25-27 were
,
,  "
"
By Gregory Piatetsky, Apr 28, 2014.

Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for April 28 and later.
,
See full schedule at , .
,
,  "
"
,
,
May 23, 2014
,
Keynotes:
,
,??,
In addition to our excellent keynote presentations, we will also have full-day tracks on different areas of analytics: descriptive, predictive and prescriptive, as well as a special track focused on implementing analytics methods through social and mobile media. Confirmed speakers in these tracks include representatives from such companies as Macy's, Disney, Kroger, Fifth Third Bank, and many others.
,
Analytics Summit 2014 will be held at the Kingsgate Marriott Conference Center at the University of Cincinnati.
,
To Register for the Analytics Summit 2014: ,  
,
For more information about the Center for Business Analytics. ,  "
"
By Gregory Piatetsky, Apr 29, 2014.
,
,
,
This poll has been closed. 
,
,
,
,
Here are the results of 
,
,
to which we added top 7 database engines according to ,  "
"
,

,

,??(Apr 9-10, 2014) was organized by Innovation Enterprise at Santa Clara Convention Center in Santa Clara, CA. The summit brought together experts from industry as well as academia for two days of insightful presentations, workshops, discussions, panels and networking. It covered areas including Big Data Innovation, Data Analytics, Hadoop & Open-Source Software, Data Science, Algorithms & Machine Learning, Data Driven Business Decisions and more.
,
People attending such conferences would agree that there is so much happening quickly and often simultaneously at conferences that it is almost impossible to catch all the action.??,These concise, takeaway-oriented summaries are designed for both ?€? people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference. As you go through it, for any talk that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
,
,
Here are highlights from selected talks on day 2 (Thu Apr 10):

,
,, Senior Director, Big Data group at ICE/NYSE commenced his talk by describing common business challenges such as: data retention requirements for data across years, fast access to disparate data for complex analytics with strict SLA?€?s, etc. The legacy model of data presentation does not meet the demands of today?€?s data consumption model. 
,
Responsible for the development roadmap and GTM strategy of Pivotal Data Dispatch (PDD), Andrew described a new approach called ?€?Data Lake?€?. Under this approach, the ?€?Lake?€? or reservoir is where all ?€?at rest?€? enterprise data is stored. A key feature of PDD was to allow for virtualization of Lake(s) and enterprise data platforms. ,Design rules followed for Data Dispatch were:
,
,??,
Key benefits of this concept were:
,
,??,
At the end, he mentioned that NYSE and Pivotal have joined forces to bring data dispatch to market and the product was launched at Strata conference held in Oct 2013.
,
,, Senior Director, Data Science & Analytics at Glassdoor discussed about data products and their use cases. His team (which includes professionals from core data science, engineering, product and design) makes Glassdoor products and business smarter. In many cases, like Glassdoor's, data is the??product itself (for example, salary data). Like design did in the last decade, Data will likely form the back-bone of most revolutionary products in the coming decades. Also, firms should set up good benchmarks and create ambitious goals.
,

Explaining the first use case of ?€?user acquisition?€? he advised to: Start small & scrappy. Iterate really really fast. Don?€?t annoy (a lot of) users. Based on insights from product team, when the recommendations for relevant jobs were placed in email alerts the user acquisition increased by a staggering 24% (compared to mere 5% hike on placing recommendations to the website). Lesson: In case of recommendations, it is very important ?€? how and where you provide them.
,

The second use case titled ?€?Monday Hack?€? was based on the exploration of an unexpected hike in the number of daily visits from job alert emails. The root cause was identified to be a bug in the software due to which instead of sending jobs from the past 24 hours, users were sent jobs from the entire past data. While fixing this bug, they decided to benefit from this observation, by tweaking the job alerts to show more recommendations on Monday to match users?€? cyclic increase in interest for more content.
,

The final use case was based on optimizing send times for job alerts. Challenged with the increasing workload (due to increase in number of users), the team wondered if it would be of value to personalize send times and spread them around the day. A controlled experiment, with randomized send-times was run and returned positive results. Visits were up by 15%.

,

,, Data Scientist from Slice started by briefly describing about Slice. Slice has been building products since 2010 to organize all the e-commerce data locked away in email. It is getting increasingly difficult for shoppers to manage purchase receipts of online shopping and track orders across online shopping portals. Slice solves this problem by making it easy for users through an integrated e-commerce management system which is highly intuitive and convenient to use. Besides, Slice also benefits shoppers by providing features such as price drop alerts, tracking digital purchases, spending analysis, package tracking and much more.

,

Meanwhile, Slice mines all this e-commerce transaction data to create a purchase network to revolutionize shopping ?€? what they call ?€?the Purchase Graph?€?, in which nodes represent what and where people buy. Edges represent similarities between nodes i.e. two nodes are similar if people who buy the first item also buy the latter item(correlation, not causation).?? He gave some examples explaining purchase graphs pin-pointing some great insights from the graph. The slide below is a purchase graph with color of nodes showing different types of sellers. Also, the smaller the edge, the more similar the nodes and two nodes must be directly connected to be similar.
,
He concluded the talk by highlighting how purchase graph has helped understand:
,
,
, ?? ,
,, Risk Modelling Manager at Paychex delivered his talk focused on how Predictive Modeling brings art and science together to play a key role in business strategy. Without Predictive Modeling, many strategic decisions are left to the ?€?gut?€?, ignoring enormous opportunities for data-driven decision making in the age of big data.?? Paychex leveraged expertise in predictive analytics to add an empirical layer to sales strategy decisions.?? With the addition of models to predict likely sales units and establish a yardstick to measure sales value by zip code, sales management became statistically informed as they made decisions regarding quota setting, territory alignment and market expansion.
,
Tom described the indigenously built solution called ?€?Sales Anticipation Model (SAM)?€? to meet the goal of accurately predicting fiscal sales for each sales territory. The model variables were categorized as: Client Demographics, Sales History, Zip Code Demographics, Economic Indicators and Loss History. The success of this predictive model helped the ERM (Enterprise Risk Management) team use its Analytics skills to gain a voice in senior leadership strategic decisions.
,  "
"
,
,
Enterprises are conflicted in the 21st century on how to make best use of their tremendous amount of data.  Thus, it is no wonder that in order to maximize their profits by analyzing the heap of data they have accumulated over the years they are willing to pay hefty amounts to professionals with Big Data skills.  
,
According to a report released on Jan 29, 2014 by Dice, a tech-career site, Big Data is dominating among the top paying skills with R being the highest paid skill in year 2013.  Average salary for a professional having knowledge and experience in programming language , was $115,531 in year 2013. ,Enterprises are clearly willing to pay a good premium to find or retain good people to help make sense of their data.  ?€?Companies are betting big that harnessing data can play a major role in their competitive plans and is leading to high pay for critical skills?€? said Mr. Shravan Goli, President of Dice. In the current talent landscape, Big Data skills mean big pay as most of the top salaries are trying best to attract and retain employees with big-data related skills.
,
Other Big Data oriented skills such as , are among top 10 paying skills. Interestingly, Hadoop, even after seeing a year over year decrement of 5.6% in average salary, stands at number 9 with average salary being $ 108,669. Mr. Goli?€?s suggestion to technology professional is to volunteer for big data project, which would not only make them more valuable to the current employer but also make them more marketable to other employees. 
,

Silicon Valley has once again topped the list of highest paid metropolitan areas when it comes to tech talent. With the average salary of $108,603, Silicon Valley has seen a year over year increment of 7.2% on an average.  Silicon Valley has been the highest-paid market since Dice first published the salary survey in 2002 (with 2001 numbers).
,
,In general, more technology professionals in the U.S. enjoyed merit raises over the last year, driving average salaries up in 2013. Average U.S. tech salaries increased nearly three percent to $87,811 in 2013, up from $85,619 the previous year. Technology professionals understand they can easily find ways to grow their career in 2014, with two-thirds of respondents (65%) confident in finding a new, better position. That overwhelming confidence matched with declining salary satisfaction (54%, down from 57%) will keep tech-powered companies on edge about their retention strategies.
,
Tech professionals are recognizing employers?€? efforts, with just 34 percent of respondents saying their company offered no motivators last year, down from 47 percent who felt that way in 2009. Likewise, the motivator with the most dramatic rise over that timeframe: increased compensation.
,

The complete report is available for free download at: ,  "
"
        ,  "
"
,
Latest ,, (Apr 30, 2014) ,:
,
,??,
Also
, (5) |
, (3) |
, (6) |
, (1) |
, (3) |
, (4) |
, (10) |
, (2) |
, (4) |
, (6) |
, (9) |
,
,
Lesson: If you are a CIO, clean up your goddamn room; you're not going out until you do! Michael Brodie, in ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is currently the Principal Data Scientist for Microsoft Data Science team (DnA), where he works with a team of data scientists searching for insights in petabytes of data. Juan joined Microsoft in 2009 to work for the Microsoft Experimentation Platform (EXP) where he designed and ran randomize control experiments across different Microsoft properties. In Microsoft, Juan also worked as part of the Bing Data Mining team. Before joining Microsoft, Juan was the CTO and co-founder of alerts.com. Juan has 2 computer science degrees from the Catholic University in Uruguay, and a graduate degree in Data Mining from Johns Hopkins University. He lives in Kirkland, WA, with his wife and daughter. He has been a speaker at conferences in many countries including the US, Canada, Argentina, Colombia and Uruguay, and he also was a TedX Speaker in 2010.
,
Juan recently delivered a keynote at the Big Data Summit 2014. His talk, ""The Good, the Bad, and the Ugly of Big Data and Data Science?€?, focused on the myths of big data and why the important word in data science is science and not data.
,
Here is my interview with him:
,
,
,
,: Exploratory work is an important part of a data scientist's job. The focus of the data scientist needs to be on what we can do with this data, what questions can be answered based on this data, and what type of problem can be solved. Curiosity is key here.  During this process, a data scientist will need to understand how the data was collected, possible bias, and more importantly, the quality of the data. It is important to understand that some problems will require more data than others, more data will also require more processing time and/or more cost. It is the job of the data scientist to understand this tradeoff.
,

,

,        

, On the one hand, there are system challenges: you need to be able to collect all the data in near real time and also process all the metrics. This is definitely not an easy job. The system also needs to be very flexible and it needs to support the capacity to add new instrumentation, define new metrics etc., and this is also challenging.  On top of this you have to make sure you have good data quality, more than 50% of the traffic in the internet is produced by bots, and so filtering bot traffic is key in order for the experimentation system to be trustworthy. On the other hand, there are cultural challenges: you need everyone in an organization to understand the value of running control experiments and how to interpret correctly all the results, this requires a cultural change within an organization. 

,

,
,
,: On average, 2/3 of the ideas or features will have negative or flat impact. 
People that worry about experimenting do not realize that without experimentation they will be flying blind and will not know if the feature or idea is good or not.
  We need to ask ourselves what is better: ship something to 100% of our users without knowing if it will work, or test it with a small % of users for a period of time and only ship the positive ones?
,
,

,
Another important thing is that if the experiment is really bad and is hurting user experience, it will get statistical significant pretty fast... and at that point we can either manually or automatically stop it.
,
,
,
,: I think that the most important success factor is that the organization needs to embrace a good experimentation culture. Not only understanding the value of controlled experiments, but also understanding that the majority of the time our ideas will fail and this is not a problem, but rather just part of the innovation process. The organization needs a data driven culture and the right incentives to run experiments making it clear for everyone in the organization that decisions are made based on data and not just hunches.
,
,
,
,: On the experiment design front, a frequent problem is to have the wrong success criteria. The OEC (overall evaluation criteria) is the metric we use to measure success. Choosing a metric seems trivial, but it is definitely not. 

,


,
Another common problem is to have a bias in treatment or control, for example if one of the pages is cached but not the other one, or there is an extra redirect, etc., many factors can easily introduce bias. This is why it is so important to run A/A tests before to make sure the system is trustworthy.
,
,
,
,: As like many other things, ?€?it depends?€? on how this information will be used. If we understand what the predictive model is saying, we usually do not have a problem. The issue is when we believe the output of the model based on correlation is causality, at that point we have a problem.
,
The other problem I see often, is that we have enough data and variables, we will find correlations that will be there just by coincidence ?€? and even though those models will work fine in testing environment, they will not have predictive power in unforeseen data, this is something common where production data is scarce for example earthquake detection
,

, 
Refer: ,
,
,: I think myth number 6 would be that people think that getting value out of data is something new. Statisticians have been getting value out of data for generations now, to the point that probably some of them think that the whole idea of data scientist is just an identity theft!  Galileo, Newton, etc?€? all used observations to deduce models about the world. Fisher was running control experiments in agriculture in 1920s. In 1854, there was an outbreak of cholera in London and people believed cholera was airborne. John Snow, using data on a map, devised a theory that cholera was spread through the ingestion of polluted water, and was able to identify a water pump as the source of the disease. 
,
,
,
,: There are many successful stories of using data; from search engines, to recommendation engines, moneyball, Nate Silver, etc.
,
The interesting factor about how we end up with this hype is another example of the wrong use of data, where people see that ?€?Big Data?€? is correlated with success then ?€?Big Data?€? implies success?€?that is of course wrong?€?. Because of this, when projects like Google Flu fail, then people start thinking that big data is useless, which is also wrong.
,
There is no question on the value of using data, but what people need to understand is that getting value out of data is hard and requires a lot of effort. They also need to understand that many projects or models will fail and this is just normal.
,
,
,

,: I would create a strong foundation in math and statistics, this is fundamental for any data scientist. Also start as early as you can to play with data. Now it is much easier, there are a lot of data sources that you can download for free that are great.
,
,
,
,: , from Jeff Leek, Roger Peng, and Rafa Irizarry, is a great blog.  I also like R-blogger and Occam?€?s razor from Avinash Kaushik, as well as the news section from KDnuggets, I find that your selection of stories are very good.
,
On books, Bayesian Reasoning and Machine Learning from David Barber, Learning from Data from ,, and How to measure anything from Douglas Hubbard.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "
"
Most popular 
, tweets for Apr 28-29 were
,
Cartoon: Data Scientist Salary Negotiation ,
,
9 Free Books for Learning Data Mining and Data Analysis ,
,
9 Free Books for Learning Data Mining and Data Analysis ,
,
,  "
"
,

,

On May 30, 2012, Governor Deval Patrick launched the Massachusetts Big Data Initiative, to leverage and expand the Commonwealth?€?s position as a global leader in the rapidly growing big data sector. The Initiative, led by the Innovation Institute at the Massachusetts Technology Collaborative, has launched several pilot efforts to enhance and grow the region?€?s vibrant and expanding Mass Big Data ecosystem, including strategic and collaborative partnership efforts with academia, industry and public sector organizations.


,

The 2014 Mass Big Data Report provides an assessment of the relative strengths and weaknesses of the Commonwealth in big data. It highlights prospects for growth in areas such as talent and workforce, ecosystem, and public data access; and identifies opportunities to promote and expand the Mass Big Data sector, while enhancing the Commonwealth?€?s position as a global leader. This Report is intended to serve as a baseline assessment of the Massachusetts Big Data ecosystem and related economic factors.
,

Principal findings:
,

, ?? ,
Growth prospects:
,
, ?? ,
,
Six Key Mass Big Data Priorities Identified for Action
,
, ?? ,
The Commonwealth has taken steps to improve access to data, strengthen computing resources, and extend broadband coverage. The key initiatives include Massachusetts Open Data Initiative, Electronic medical records and claims data (MassHIway, All-Payer Claims Database (APCD)), Massachusetts Green High Performance Computing Center (MGHPCC), and Mass Broadband 123.
,
Almost 60 percent of survey respondents ranked California as the top location for businesses in the big data sector. California has not launched state-sponsored initiatives which specifically target big data. Massachusetts?€? competitive position centers around the unique combination of business, technology and talent strengths, with some of the top global big data firms making their headquarters in the Commonwealth, including EMC, Attiva, and Basho. 
,
The concentration and access to big data talent is higher in Massachusetts than other technology focused regions throughout the United States. Respondents to the Mass Big Data Study rank Massachusetts third after California and New York for building big data businesses. Federal funding for big data initiatives through an announced $200 million commitment will be a factor in regional advancement as well.
,
The complete report is available for free download at: ,
  "

"
,
,
, is CEO of BeyondCore and Chair of the SRII Big Data and Advanced Analytics group. Arijit has guest lectured at Stanford; spoken at conferences in a dozen countries; and was written about in The World Is Flat 3.0, New York Times, San Jose Mercury News, Harvard Business Review and The Economist. Arijit held leadership positions at several industry associations and previously worked at Oracle and Microsoft. He has been granted nine patents in the domains of analytics and privacy. Arijit holds an MBA with Distinction from the Harvard Business School and Bachelor degrees with Distinction in Computer Science and Economics from Stanford University.
,
Arijit recently delivered a talk at , held in Santa Clara on ?€?Advanced Analytics for All?€?. 
,
Here is my interview with him:
,
,
,
, In traditional analytics, people need significant training and knowledge in analytics to do analytics right. In BeyondCore?€?s ,Advanced Analytics for All (A3) approach, the software itself guides the user so that most people without training in Statistics or Computer Science can do analytics themselves. Overall our A3 solution hides the complexity of the analytics / statistics methodology and process so that users can focus on overlaying their human intuition and domain knowledge on the analysis. Areas where A3 may assist users include:
,
,
,

,
,
, The main goal is to guide the user but not hide things from the user. For example, BeyondCore grays out parts of the graph it knows are not ?€?interesting?€? but it doesn?€?t completely hide those parts of the graph. This way, the user can still overrule the software and see the full graph if they so choose. Similarly, while BeyondCore suggests specific graphs and drill-downs, users can still quickly see whatever graph they choose. Users can also shift to Statistician Mode and see all the complexity they choose. Here context is crucial: BeyondCore makes things simple when appropriate and exposes subtleties when desired.
,
,
,
,: Users are uniquely positioned to overlay domain knowledge and human intuition on top of an analysis. However, as noted, they are incapable of manually evaluating millions of variable combinations., What differentiates BeyondCore technology is that it takes care of analytical complexity and enforces best practices, while making the automated analysis results completely comprehensible to users. This way, users can focus on the tasks they are uniquely qualified to perform ?€? namely interpreting patterns.
,
Moreover, we should never ask users to blindly trust technology. It is very hard to evaluate millions of variable combinations to find a specific pattern. However, once a specific pattern is found, it is extremely easy to confirm the pattern manually. For example, what Google search does is difficult to fully understand, but we believe it works, not because we ?€?trust?€? it, but because when we click on Google search results, they tend to be accurate. Even if one out of the ten links is not relevant to what we were looking for, we quickly skip over that one link because our human domain knowledge tells us to skip it. 
,
Automated analysis cannot be perfect, but it can be far faster and actually more accurate than manual analysis. In manual analysis, we risk missing useful patterns that we never thought to check for (false negatives), and risk wasting time on apparently interesting patterns that we did not conduct the appropriate statistical tests on (false positives). In automated analysis, we may have some cases where the pattern is obvious and thus not useful. But, we don?€?t have as many false positives and negatives as in the case of manual analysis. 
,   
,
,

, With BeyondCore, data analytics is like driving a car. Most of the time most users do not need expert help to drive the car. However, every once in a while, when the engine light comes on for example, you have to involve experts. For 80% of use cases, BeyondCore makes a typical business user nearly as effective as a Data Scientist. The experts are then brought in to focus on the truly difficult cases where they can deliver the greatest value.
, 
,
,

,Last week I had a two-hour meeting with the President of a Fortune 500 firm and his direct reports. They are personally taking the lead in democratizing data analytics. The technology already exists, what is necessary is true leadership. 
,
From a technological standpoint, what crucially helped BeyondCore democratize analytics was our core principles of simplicity and speed. Sears confirmed ?€?In less than 5 minutes total, we had been set up on a cloud-based server, were able to load up our uniquely structured data, analyze it with a single click, and see the results of the automated analysis.?€? (See , for details). The key here was that it took a single click and just 5 minutes total to analyze all of their data. When analytics takes a long time and requires a significant amount of skills, it can never become ubiquitous. Democratization happens when analytics is as easy as using Google, but does not require the user to know the right questions to ask. 
,
,
,

, The ?€?aha?€? experiences that truly matter are when users look at their data for the first time using BeyondCore. Typically, at some point during their first analysis project, they take over the laptop themselves and start diving into the patterns on their own. That is what we call the ?€?iPad moment.?€? You can spend six months intellectually debating the superiority of an iPad, or you can spend six minutes with an iPad and experience its power and simplicity. Those customer ?€?aha?€? moments are the foundation of BeyondCore. 20 of the Fortune 100, as well as companies as small as a three-person trading firm, used BeyondCore in the very first year of our production release and had these ?€?aha?€? moments. 
,
,
,
, Dancing, though I don?€?t get enough time to dance these days.
  "
"
,

,
,
NineSigma RFP# 70653
,

NineSigma, working with multiple leading companies, is seeking proposals for unique approaches to mining data from user browsing/operations history and media libraries in order to improve user personalization and recommendation of products. Proposals may also investigate integrating social networking data into their systems to achieve these goals. Other proposals for using social networking sites to predict sentiment, market trends, and disease transmission or using sensing or supply chain data to optimize the efficiency of systems are also welcome.

,

More information can be found in the request for proposals (RFP) document online at

,

,

,

The response due date is Friday, May 23, 2014.

,

For more information, you may post comments at the link for the FRP above or email Mark Takatsuji, Program Manager at NineSigma, at , and reference RFP# 70653 in the subject line.

,

When you submit a proposal, use the template for the response form, which can be found at

,

,  "
"
,
,
,
,:
,
,
,
,
This video begins with some definitions of data mining and machine learning. We take a look at some well known classical approaches. We then move on to running a conventional regression model on Boston housing data using MARS and examine the shortcomings of conventional regression.
,
,
,
,
,
,
,
This video is dedicated to MARS (Multivariate Adaptive Regression Splines). It starts with an overview of splines, specifically smoothing splines. We discuss the MARS built in automation for finding ?€?knots?€? and how MARS is able to accomplish this through the use of basis functions. We then move onto the three main stages of the MARS algorithm: forward stage, backwards stage, and the selection stage. This is followed up with a live run of MARS to compare with the results obtained from the previous video using linear regression.
,
,
,
,
This video introduces the concept of regression trees as recursive, piece-wise constant fits in order to identify an underlying response surface. It then walks through an example of a CART model setup using the Boston housing data and discusses the results. Finally, a summary of how this approach simplifies the underlying structure by constructing piece-wise constant models, by segmenting the underlying population into a set of mutually exclusive smaller segments, such that within each segment the overall prediction is a constant and as you go from segment to segment, the prediction changes by a fixed amount.
,
,
,
,
This video introduces the concept of tree ensembles. It starts with an overview of the ensemble modeling process and discusses the different methods for growing multiple trees, the bootstrap sampling procedure, and the RandomForests algorithm. The following video details how to work within RandomForests.
,
,
,
,
This video focuses on RandomForests, which can be described as independently grown, large regression trees. It begins with RandomForest settings and the building of a model. We discuss the results and highlight the advantages and disadvantages of using multiple trees and the rationale for building multiple trees.
,
,
,
,
The last video in this series goes over stochastic gradient boosting, or TreeNet. It is also referred to as Multiple Additive Regression Trees. It begins with an introduction to the concept of loss functions, an integral part of the TreeNet methodology. We then go through an overview of the TreeNet algorithm without delving into too much detail. A TreeNet model is set up and run. We observe the results then run another model, pushing the performance and comparing the two models. We explore the variable dependence plots produced by TreeNet. We finish by noting the strengths of TreeNet and how utilizing data mining tools can give you an advantage over conventional modeling. 
,
See full video series 
,.  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
3-D printing and Data Science are two of the hottest trends around.
,
New KDnuggets Cartoon drawn by Jon Carter looks at what happens when these trends collide.
,
,
,
,
See also other 
,.  "

"
,
,
,
Over the past few years, as the buzz and apparently the demand for data scientists has continued to grow, people are eager to learn how to join, learn, advance and thrive in this seemingly lucrative profession. As someone who writes on analytics and occasionally teaches it, I am often asked - How do I become a data scientist?
,
,
Adding to the complexity of my answer is data science seems to be a multi-disciplinary field, while the university departments of statistics, computer science and management deal with data quite differently.
,
But to cut the marketing created jargon aside, a data scientist is simply a person who can write code in a few languages (primarily R, Python and SQL) for data querying, manipulation , aggregation, and visualization using enough statistical knowledge to give back actionable insights to the business for making decisions.
,
Since this rather practical definition of a data scientist is reinforced by the accompanying words on a job website for ?€?data scientists?€? , ergo, here are some tools for learning the primary languages in data science- Python, R and SQL. A cheat sheet or reference card is a compilation of mostly used commands to help you learn that language?€?s syntax at a faster rate.
,
The inclusion of SQL may lead to some to feel surprised (isn?€?t this the NoSQL era?) , but it is there for a logical reason. Both PIG and Hive Query Language are closely associated with SQL- the original Structured Query Language. In addition one can solely use the , package within R (and the less widely used , or , libraries for Pythonic data scientists) or even the Proc SQL commands within the old champion language SAS,  and do most of what a data scientist is expected to do (at least in data munging).
,
For Python, this is a rather partial list given the fact that Python, the most general purpose language within the data scientist quiver, can be used for many things. But for the data scientist, the packages of ,, , , , and , seem the most pertinent.
,
Do all the thousands of R packages have useful interest to the aspiring data scientist? No.
,
Accordingly we chose the appropriate cheat sheets for you. Note that this is a curated list of lists. If there is anything that can be assumed in the field of data science, it should be that the null hypothesis is that the data scientist is intelligent enough to make his own decisions based on data and it?€?s context. 3 printouts is all it takes to speed up the aspiring data scientist?€?s journey. 
,
Please add additional cheat sheets in comments below.
,
Cheat Sheets for Python
,
,
,
Cheat Sheets for R

,
,
,
Cross Reference between R, Python (and Matlab)
,
,
,
Cheat Sheets for SQL
,
,
,
Additional
,
,
, is a popular writer and 
, on Analytics and Data Mining and is the author of 
, book (Springer, 2012).
,
,
,
 ,  "






"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky, May 12, 2014.
,
,,
to be held August 24-27, 2014, New York, NY, USA is the 
, in Data Mining, Data Science, and Knowledge Discovery.
,
KDD-2014 technical program will feature the research track, the applications track, the industry practice track, the KDD Cup competition, workshops, tutorials, oral presentations, keynotes, poster sessions, panels, exhibits, demonstrations, and much more.
,
Workshops are an important part of the conference (indeed KDD conference arose from a workshop I organized at IJCAI-89 conference). The workshops provide the forum for discussing the leading edge research ideas.  All workshops will be held on Sunday, Aug 24, 2014.
,
See below for an updated table of this year's two dozen exciting workshops, from n. 1 to 
n. 25.  The careful readers will note one number between 1 and 25 is missing (it is #11.  It is a mystery why there is no workshop #11).
,
Paper submission dates vary from May 21 to June 24.
,
For latest information, see , .
,
8 Full-Day workshops: 
,
,
,
16 Half-Day Workshops:
,
,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for May 9-11 were
,
Data Mining for Statisticians ,
,
For teachers (and students) of #MachineLearning - Slides for LIONbook: Machine Learning and Intelligent Optimization ,
,
For teachers (and students) of #MachineLearning - Slides for LIONbook: Machine Learning and Intelligent Optimization ,
,
,  "
"
,
,
Healthcare is one of the key industries where Analytics is slowly but steadily making its way to the top of priority list. In the highly advocated push for ""value-based care"", Analytics are expected to play a central role in creating cost-quality transparency, eventually leading to efficient and effective healthcare.
,
,As the health plans are strategically driving the shift from ""services based payment"" to ""value based payment"", providers are under increasing pressure to invest in Analytics and set up appropriate BI on their various functional processes. For many providers Analytics is a relatively new phenomenon (specially at the level of importance it has achieved today), and thus, the BI/Analytics market for providers is still in its early phase. However, the market already has some vendors leading the race to provide high-utility, high-performance tools providing actionable insights.
,
, recently released a report summarizing the results of survey of provider perceptions towards BI/Analytics products and services in the areas of ACO(Accountable Care Organization)/value-based care, population health, Big Data and BI consulting. In the category of cross-industry BI/Analytics, SAP, IBM and Oracle were the leaders. In the niche of healthcare specific BI/Analytics, Optum and Advisory Board emerged as the winners. The third category, Analytic Tools offered by EMR(Electronic Medical Record) vendors, was led by McKesson, Epic and Cerner.
,
,Besides, the report also highlights a few important insights:
,
, ?? ,
The complete report can be accessed at (available for purchase; key findings available for free): ,  "
"
,
,

,A mathematician and seasoned technology executive, , has over 20 years of business and technical expertise.  As co-founder and CTO of RedPoint Global, George is responsible for leading the development of the RedPoint Convergent Marketing Platform???. A former math professor, George left academia to co-found Accenture?€?s Customer Insight Practice, which specialized in strategic data utilization, analytics and customer strategy. Previous positions included director of client delivery at ClarityBlue, Inc., a provider of hosted customer intelligence solutions to enterprise commercial entities, and COO/CIO of Riscuity, a receivables management company specializing in the utilization of analytics to drive collections. George holds a BS in geology and a BA in mathematics from the University of Miami, and an MS in applied mathematics from the University of Arizona.
,
George recently delivered a talk at , held in Santa Clara on ?€?Gain Traction for Your Big Data Initiatives: Harness the Power of Hadoop Quickly and Cost-Effectively?€?.
,
Here is my interview with him:
, 
,
,
,: YARN?€?s impact on the Big Data industry cannot be overstated. , Code-based solutions, such as those built on MapReduce, are impossible for enterprise IT organizations to adopt and manage on a large scale.  In addition, IT organizations want consistency in their solutions and code-based solutions are notorious for being inconsistent.
,
Already we are seeing Big Data projects failing due to unrealistic expectations foisted on immature technologies.  What ?€?pure YARN?€? enables is the porting of mature, enterprise-class technologies directly onto the Hadoop platform, allowing them to work directly at the HDFS level within Hadoop.  Without YARN, technologies would be constrained to using or generating MapReduce code with all of its associated limitations and skill requirements.  On the other hand, ?€?pure YARN?€? applications will enable enterprises to leverage their existing ETL/DBA/Analyst resources with minimal retraining thus enabling easier enterprise-wide adoption.
,
,
, ?? ,
?€?Pure YARN?€? applications, in contrast to ?€?MapReduce/YARN?€? applications, push their functional binaries directly into YARN to manage, distribute and process directly on HDFS.  Only ?€?pure YARN?€? applications will enable direct porting of mature technologies onto the Hadoop computation platform. ?€?MapReduce/YARN?€? applications still utilize MapReduce to define a process and are subject to the limitations of MapReduce.  Users should ask vendors which type of application they are providing.  The vast majority of 3rd party Hadoop related technologies are still ?€?MapReduce/YARN.?€?
,
As with all technologies, there are both risks and benefits to being the first technology of a kind in the market. 
,The same applies in this case.  Based on the level of investment going on in the market (like the $900 Million round Cloudera just closed), I predict that in the next 18-24 months there will be a wave of mature platforms leveraging YARN for everything from security to analytics.  This is the wave that Hadoop will ride into the mainstream of business IT.
,
,
,

,: The RedPoint Convergent Marketing Platform (CMP) solves three core problems facing most marketers today.  
,
First and foremost, the Convergent Marketing Platform solves the marketing data problem by providing a robust set of ETL/ELT, Data Quality, Data Capture, Web Service, Automation and Master Key Management.  At its foundation is RedPoint Data Management, which is ranked independently for Data Quality and received numerous #1 rankings in customer satisfaction surveys, including ease of use, speed and performance.   In addition, we released RedPoint Data Management for Hadoop??? -- one of the first ?€?pure YARN?€? applications to run natively on Hadoop. This level of power and robustness stands in stark contrast to the largely nonexistent capabilities of our competitors.
,
,
Second, we help marketers ?€?defragment?€? their ecosystem.  Marketers are continually bombarded with messages that say, ?€?Sign up with me, upload your data and we will do everything you need.?€?  Only later do they find out that it?€?s not true. What this proliferation of execution platforms does is exacerbate the fragmented data, processes and strategy problem. , Through RedPoint?€?s bi-directional orchestration layer, we create a centralized point of operational control that becomes the connective tissue in their marketing ecosystem.  All operations, content, rules and data reside inside the RedPoint platform; execution channels (e.g., ESP, SMS, mobile push, social) become commoditized plumbing that delivers messaging and can easily be switched or replaced.  This allows marketers to create and maintain a centralized and current 360?? view of their customers, coordinate their operations centrally, easily unify their strategy and operate in unprecedented timeframes.
,
Third and most importantly is the ?€?credibility gap.?€? We actually deliver what we promise and the system works just as advertised.  This may seem like a strange point but marketing technology providers are notorious for creating hype around their technologies.  The fact is that most of the marketing technologies out there don?€?t work as advertised, they don?€?t ?€?do everything?€? and are built largely on antiquated architectures that have been re-skinned or subsumed into a larger suite of non-integrated marketing applications.  No one is worse at this hype than the so called Marketing Cloud vendors who have acquired numerous technologies so they can claim to have everything marketers need.  Their strategy is to become the final piece to the enterprise software puzzle., The problem is that none of these companies have a track record of actually integrating any of the technologies they acquire?€?anyone remember what a Siebel implementation was like?   Consequently, implementations take months, if not years and they rarely succeed.  In contrast, RedPoint can typically build a 360?? view of the customer and implement the full platform, with training, in about a quarter of the time of our nearest competitor. Some of our largest implementations have been done in weeks. That is the proof that the system works as advertised.
,
,
,
,

,: , , RedPoint sees this as the core of building and maintaining a 360?? longitudinal view of the customer.   Using these capabilities, we can link people, their relationships, addresses, ecommerce profiles, social profiles, mobile profiles and their devices over time.  This kind of linkage and breadth of data supercharges analytics and enables modeling and prediction at a level that is otherwise unattainable. Insight like this is critical to developing the level of customer intimacy that most consumer brands are trying to achieve with their customers.
,
Second and final part: ,  "
"
        ,
,
,
,
,
    "
"
,
,
, (Apr 23-25, 2014) was organized by Global Big Data Conference at Santa Clara Convention Center in Santa Clara, CA. This 3 day extensive, fast paced and vendor agnostic bootcamp provided a comprehensive technical overview of Big Data landscape to the attendees. Big Data Bootcamp was targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, NoSQL & Machine Learning. It brought together data science experts from industry for three days of insightful presentations, hands-on learning and networking. It covered a wide range of topics including Hadoop, Map Reduce, Amazon EC2, Cassandra, YARN, Pig, different use cases and much more. 
,
Despite the great quality of content as well as speakers, it is hard to grasp all the information during the bootcamp itself. , These concise, takeaway-oriented summaries are designed for both ?€? people who attended the bootcamp but would like to re-visit the key sessions for a deeper understanding and people who could not attend the bootcamp. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
In case you missed: , 
,
Here are highlights from selected talks on day 3 (Fri Apr 25):
,
, gave a talk on ,. He introduced Tez as a distributed execution framework built on top of YARN and targeted towards data-processing application.  Tez provides more general data-processing applications to the benefit of the entire ecosystem. Tez generalizes the MapReduce paradigm to a more powerful framework for executing a complex DAG (directed acyclic graph) of tasks. , He explained how Tez design themes empower end users and gain performance advantage over Map Reduce with optimal resource management. Talking about the current status of Tez, he said it is currently an Apache Incubator Project and is under rapid development. For now, focus is on stability and support for a vast topology of DAGs. On its roadmap the next focus areas are richer DAC support, performance optimizations and usability. 
,
, delivered a talk on the big data ecosystem at Yahoo! and the role played by , in that ecosystem. In Enterprise big data there is a clear movement towards BI tools and higher-level interfaces such as SQL & (J/O)DBC. Another trend is of migration away from batch towards interactive and other modes.  Now, it is more about resource management(YARN/Mesos) and less about proper Hadoop MapReduce. , He mentioned some of the legacy architecture pain points such as increasing data volumes and scale, high report arrival latency, lack of interactive SQL, etc.  Spark solves need for interactive data processing at REPL and SQL levels. Now developers are no more restricted by Hadoop MapReduce paradigm.  Spark involves less code (up to 2-5x) and deep integration into Hadoop ecosystem. Spark is an open source distributed SQL query engine for Hadoop data. Common HiveQL provides seamless federation between Hive and Shark. He explained Spark on YARN model. He ended the talk discussing some of upcoming contributions by Yahoo: fix scheduling performance issues, handle large number of small splits, handle master fault tolerance and many others. 
,
, talked about ?€?Relationships in Big Data, Cloud & Predictive Analytics?€?. He started by emphasizing the huge amounts of data being generated every day and explaining different applications for big data analytics.  Among the most requested uses of Big Data are log analytics & storage, smarter utilities, RFID Tracking & Analytics, and Fraud/Risk Management Modeling. , He claimed that the basic techniques for large scale simulation and computing are ready. However, large and time-consuming computing tasks need steering and most of them have a large number of parameters that needs to be tuned.  Smart data processing algorithms are ready. However, most of data mining algorithms have high computational complexity i.e. polynomial rather than logarithmic or linear.  At the end, expertise is more important than the tool and data is definitely not a replacement for intuition and intelligence. 
,

, delivered a speech on ""How to Succeed with Polyglot Persistence in the Enterprise"". The term ?€?polyglot persistence?€? was coined to describe that big data programmers should use several database systems, each being the best choice in its application area. With the help of a graphic he explained that even today relational databases, though feasible, are expensive and slow. On the other hand NoSQL enables unlimited volume growth stabilizing cost and increasing performance.  Typical NoSQL systems are non-relational, distributed, horizontally scalable and have no need for a fixed schema.  He then explained NoSQL stores categorizing them as column-family, document-oriented, key-value and Graph-DB.  One should choose a store that is a best match for one?€?s application. , Next, he put forward some typical use-cases for each NoSQL store category.  He mentioned that some of the challenges of NoSQL are: no standards, no typical schema and systems targeting specific areas. Next, he explained the MapReduce Programming Model and best practices to create MapReduce jobs. Talking of Slume, he described it as a distributed streaming tool for collecting, aggregating and moving large amounts of log data. It is horizontally scalable and centrally managed with tunable data reliability.  At the end, he restated the important recommendations: use the most suitable technology for one?€?s task, scale out to crunch big data and integrate with conventional technologies.
,
, started with mentioning a quick fact: , His talk focused on the market trends in Big Data and showcased the ecosystem growth that has taken place. He highlighted startups and the value they're bringing to moving Big Data forward. Next, he shared his thoughts on the top 10 trends in Big Data Analytics:
"
"
,
,
,
Business stakeholders are putting more pressure on IT to keep up with and meet market demands. Yet existing data management platforms and strategies were not designed for an agile business or the digital age. Data management is the backbone needed to define and orchestrate the digital experience and to ensure confidence in the data used in your key processes. 
,Forrester's data management playbook shows enterprise architecture (EA) professionals how to build a more elastic and flexible data management practice to meet the new data demands brought on by digital disruption.
,
,
,
,
,
, transforms how companies tackle data challenges to achieve breakthrough business results. The company's agile data management and analytic software gives analysts new powers to harness data in an era of exploding data complexity, to discover new insights and to continuously improve business results. Analytics built in Lavastorm provide analysts with exceptional self-sufficiency, agility, transparency, accuracy, and business control.    "
"
By Gregory Piatetsky, May 12, 2014.
,
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 12 and later.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky, May 13, 2014.
,
A young team of 3 UC Berkeley students (Brian Liou, Tristan Tao, and Elizabeth Lin) has produced an interesting, although not entirely accurately named publication, called ,, available for free download.
,
Here is 
,, which 
featured interviews with data scientists and tech leaders from leading companies including LinkedIn, Cloudera, Facebook, Yelp, and Flurry. 
,
They have now released ,, 
which includes interviews with 
,
,??,??,??,??
,??,??
,??,??,??,??
Here are their top 5 takeaways:
,??,
,
,In addition to Type I and Type II errors in hypothesis testing there should
be a Type III error - asking the ""wrong"" question about data. Type 3 error is spending a lot of effort on a question that cannot be answered with the available data. 
,
,
,The huge demand for data scientists is a result of companies early investment
in Big Data and wanting to get returns from those investments. As more
companies invest in Big Data it will result in the strategic recruitment of more
data scientists and data science departments.
,
,
,Not only are humble people better to work with, but a data literate professional
must be humble to its data. He/she must be willing to accept when hypotheses
are disproven and be skeptical of results. He/she must recognize that data is
the main channel in which users communicate with a company now.
,
,
,The effective use of data is going to form the basis for competition for every
industry in every organization.
,
,
,We are still in the early stages of data science so the tools will constantly
evolve, therefore education is a continuing process and should not be tied to
any specific tool. As more tools get commercialized, the build, buy, or outsource decision firms must make is impossible to predict so to be competitive become
adept at learning new tools.
,
The team that produced the Handbook has also created an e-Learning application that aims to develop the statistical and programming skills necessary to become data literate professionals. 
,
Learn more at
, .  "
"
,

,
Given the incredible amount of data generated by the very act of consuming content, the real challenge in Media industry is to integrate multiple data flows (including the non-traditional sources such as online, social media, etc.) into their operations. Analytics provide a great opportunity to obtain unprecedented consumer insights, and thereby the ability to serve the audiences better.
,
, recently released a report stating that , Thus, it is clear that he importance of Analytics is no more limited to just a few units within media companies, rather it can be seen to have an increasing impact across the organization.
,
,
,
Several media firms are using Analytics to score a strategic advantage over their competition by making the right choices on high-impact business decisions. Empowered by great consumer insights through the media consumption patterns on its website, Netflix confidently made a bold bid to the producers of House of Cards. The show was a big hit and won three Emmy awards. The Weather Company (previously The Weather Channel) is using its data to create a new product (additional revenue stream). It will be selling its weather data and insights to other media companies as well as energy farms and insurance companies.

,

A successful Analytics strategy will need to focus on three areas:

,
, ?? ,
The media consumption habits are rapidly changing and media companies will have to keep up with it through real-time actionable consumer insights. At the same time, companies need to be vary of consumer beliefs on privacy and on how much data consumers are willing to share to get a personalized experience.
,
Finally, media companies can no longer afford to ignore or understate the importance of Analytics to their business. The disruptive changes in content generation, distribution and consumption due to the increasing popularity of social media and decreasing prices of technological solutions have created great opportunities as well as challenges. Leveraging Analytics strategically will play a vital role towards achieving competitive advantage through innovation.

,
The complete report is available at: ,  "
"
,
,
Here is part 1 of the interview: ,
,
,A mathematician and seasoned technology executive, , has over 20 years of business and technical expertise.  As co-founder and CTO of RedPoint Global, George is responsible for leading the development of the RedPoint Convergent Marketing Platform???. A former math professor, George left academia to co-found Accenture?€?s Customer Insight Practice, which specialized in strategic data utilization, analytics and customer strategy. Previous positions included director of client delivery at ClarityBlue, Inc., a provider of hosted customer intelligence solutions to enterprise commercial entities, and COO/CIO of Riscuity, a receivables management company specializing in the utilization of analytics to drive collections. George holds a BS in geology and a BA in mathematics from the University of Miami, and an MS in applied mathematics from the University of Arizona.
,
George recently delivered a talk at , held in Santa Clara on ?€?Gain Traction for Your Big Data Initiatives: Harness the Power of Hadoop Quickly and Cost-Effectively?€?.
,
Here is part two of my interview with him:
, 
,
,
,: The first growth driver for the Big Data industry is simply the economics of the technologies.  , We are already seeing organizations, who are running large projects, move little-used data out of very expensive databases and into less expensive Hadoop clusters.  The economics is also reflected in the efficient scalability of the Hadoop platform and the raw, cost effective computational power that a cluster can provide. The economics also have a powerful impact on analytics as more and more data can be collected and linked together in order to resolve sharper pictures of customers and behaviors.  We will continue to see the economics reflect themselves in new and interesting ways as what used to be ?€?impossible?€? (i.e., too expensive, too hard, too ambitious) now becomes possible.

,
The second driver is the flexibility of the platforms. , This is a really big deal in areas like marketing where data is the life blood of deep insights but IT has bigger fish to fry than worrying about capturing Twitter feeds.  Additionally, many manufacturers that build devices are anxious to collect telemetry in an easy and flexible way to monitor their device performance and utilization. The flexible structure of the data enables varied analysis and applications of the data, for example, traffic pattern data that can be analyzed by hedge fund managers to predict retail sales.

,
The third driver is the increasing frustration business units feel about the organizational latency which often constrains them from doing the right thing.  Too often, technology and competing IT priorities create latency within organizations.  Instead of making things easier, technical hurdles often delay or stymie good ideas and timely actions. To be fair, IT has their priorities for good reason but the business units also have good reason to want to operate at the ?€?speed of business?€?.  In order to achieve this, the business unit has to take greater ownership of their technical destiny.  You see this reflected in the rise of new ?€?C?€? level roles such as the Chief Digital Officer who, as Gartner defines it, is responsible for creatively leveraging technology directly to drive increasing revenue. , Imagine a CIO, CRO and CMO all wrapped into a single position.  While this is an extreme example, the desire is there to move in a direction where the business, not IT controls the data and the technology needed to move ?€?At the speed of business?€?. , The ready access to data is addicting to the business users who want to understand and quickly adapt to changing business conditions.  To meet the needs of the business units, however, technology providers will have to make the Big Data technologies easier to use.  It turns out that YARN is exactly what will enable this.

,
Such fundamental changes to an industry, however, often reveal unexpected challenges.  In the case of Big Data technologies, there are some headwinds that need to be avoided.  The skills gap is perhaps the biggest issue.  In order to take advantage of many of these Big Data technologies, the users have to be versed in a variety of coding languages and need to be full-fledged engineers.  That is not unusual for a nascent technology ecosystem like the Big Data ecosystem, but overcoming this is critical to realizing some of the benefits listed above.  That is one of the reasons YARN is so important.  YARN will enable later generation applications to work directly within HDFS and bypass the coding requirements that exist today.  The other challenge to Big Data technology adoption is the Wild West mentality.  While flexibility and autonomy are great, traditional data quality and governance is still essential to developing useful and actionable information.

,
,
,
,: There are many technical skills relevant to being a data scientist or working in the data sciences arena.  And as new technologies emerge, the list just grows.  However, skill in technologies is really just the price of entry into the data sciences realm.  The real stars in data science have a set of skills or attributes that transcend the technical.  These include:
,
,
,
,
,: Given my schedule lately, I have few brain cells left over for recreational reading but I can say that what has both entertained and inspired me the most for many years is reading The Gruffalo to my daughters. This is something I have done for all of my daughters as they were growing up and most recently with my two year old.  It?€?s become ritual that I look forward to every evening.
,
For fun, I enjoy spending time with my kids and going rock climbing. The name RedPoint is actually inspired by the rock climbing term, RedPoint. It represents the perfect execution of a route. That is why we named the company RedPoint; we always strive for perfect execution.
,
In case you missed, here is ,.  "
"
,
,
,
ISBN: 978-1-118-89270-1,
256 pages,
Wiley, May 2014.

,
By leveraging big data & analytics, businesses create the potential to better understand, manage, and strategically exploiting the complex dynamics of customer behavior. , reveals how to tap into the powerful tool of data analytics to create a strategic advantage and identify new business opportunities. Designed to be an accessible resource, this essential book does not include exhaustive coverage of all analytical techniques, instead focusing on analytics techniques that really provide added value in business environments.

,
The book draws on author Bart Baesens' expertise on the topics of big data, analytics and its applications in e.g. credit risk, marketing, and fraud to provide a clear roadmap for organizations that want to use data analytics to their advantage, but need a good starting point. Baesens has conducted extensive research on big data, analytics, customer relationship management, web analytics, fraud detection, and credit risk management, and uses this experience to bring clarity to a complex topic.

,
The book:
,
, ?? ,
See: ,
,
BART BAESENS is an associate professor at KU Leuven (Belgium) and a lecturer at the University of Southampton (United Kingdom), as well as an internationally known data analytics consultant. He is a foremost researcher in the areas of web analytics, customer relationship management, and fraud detection. Baesens is also co-author of the book , (Oxford University Press, 2008).  "
"
This 
, covers the Basics of Machine Learning, and was created by Dr. Victor Lavrenko of University of Edinburgh.  
Below is a listing of all of the lectures and topics in those lectures:
,
,
,
,

,
,

,

,
,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,
,

,  "
"
Wouldn?€?t it be nice to hear how to do analytics from senior peers ?€? heads of analytics and VPs from big name brands like ,, ,, ,, and ,? And rather than just sit and listen, wouldn?€?t it be better to actually talk with these guys, and ask the questions you need answered?
,
,
If you answered yes to either (or both!) of these questions, the , (June 10-11, Boston) is made for you ?€? find out more by downloading the event brochure here: ,
,
,
,
Brands who?€?ll be contributing speakers include ,, ,, ,, ,, ,, ,, ,, ,, and many more. The full roster is here: ,
,
,
You?€?ll also get to network with like-minded attendees from ,, ,, ,, ,, ,, and many more.
,
,
Join! Use code , to receive the exclusive $100 discount on top of the Last Chance Discount of $100 if you register before May 16 here: ,  "
"
,
will be held August 23-24, 2014, co-located with ACM SIGKDD 2014 Conference on Knowledge Discovery and Data Mining (KDD) in New York City, NY.
,
,
Postdocs, graduate students and undergraduate students are welcomed to apply for the travel scholarships to attend BPDM and KDD. 
,
Application accepted until June 15, 2014 at??
,. 
,
Scholarship notification will be sent by June 30, 2014.??
,
,
A tentative schedule is also available at??
,.
,
,
,
BPDM's primary aim is to foster mentorship, guidance, and connections of minority and underrepresented groups in Data Mining, while also enriching technical aptitude and exposure. We provide venues in which to encourage students from such groups to connect with junior and senior researchers in industry, academia, and government. We hope to create and help grow meaningful lasting connections between researchers, thereby strengthening the Data Mining Community.  "
"
Most popular 
, tweets for May 12-13 were
,
Guide to Data Science Cheat Sheets ,
,
Guide to Data Science Cheat Sheets 
,
,
Very useful - Introduction to #SQL for Data Scientists ,
,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,  "
"
,
,

, is Director of Engineering at Santa Monica based eHarmony (leading online dating website) where he is responsible for running the engineering team that builds systems responsible for all of eHarmony's matchmaking. Prior to this he spent numerous years building cloud based image processing systems and Network Management Systems in the Telecom domain. His areas of interest include Distributed Systems and High Scalability.
,
Prateek recently delivered a talk at Big Data Innovation Summit 2014 held in Santa Clara on ?€?Stores Behind the Doors?€?.
,
Here is my interview with him:
,
,
,
,: Our ultimate goal here at eHarmony is to provide each and every user a unique experience that is tailored to their personal preferences as they navigate through this very emotional process in their life. The more efficiently we are able to process our data assets the closer we get to our goal. Most of the architectural decisions are driven by this core philosophy.
,
A lot of data driven companies in internet space have to derive information about their users indirectly,  whereas at eHarmony we have a unique opportunity in the sense that our users willingly share a lot of structured information with us, hence our big data infrastructure is geared more towards efficiently handling and processing large volumes of structured data, unlike other companies where systems are geared more towards data collection, handling and normalization. That said we also handle a lot of unstructured data.
,

,
,
, Here are the key things to consider when trying to build a system that can handle fast multi-attribute searches
,
, ?? ,

,
, ?? ,
,
,
, For most modern distributed datastores performance is the key. This often requires indexes or data to fit entirely in memory, as your data grows it doesn't remain true and hence the need to split the data into multiple shards. If you have a rapidly growing dataset and performance continues to remain the key then using a datastore that supports built-in sharding becomes critical to continued success of your system as it
,
, 
As for why is it a good practice to isolate queries to a shard, I'll use the example of MongoDB where ""mongos"" a client side proxy that provides a unified view of the cluster to the client, determines which shards have the required data based on the cluster metadata and directs the query to the required shards. Once the results are returned from all the shards ""mongos"" merges the sorted results and returns the complete result to the client.
,
Now in this scenarios ""mongos"" has to wait for results to be returned from all shards before it can start returning results to client, which slows everything down. If all the queries can be isolated to a shard then it can avoid this excessive wait and return the results faster. Hence it is a good idea to examine possible set of queries before hand and use that information to come up with a effective shard key.
,
This phenomenon will apply pretty much to any sharded data-store in my opinion. For the stores that do not support built-in sharding, it'll be your application that'll have to do the job of ""mongos"".

,
,
,
, The decision of choosing a specific technology is always driven by the needs of the application. Each of these different types of data-stores have their own advantages and limitations. Staying prudent to these facts we've made our choices. For example:
,
, 
And in some cases where your choice of the data-store is lagging in performance for some functionality but doing an excellent job for the other, you should be open to Hybrid solutions.
,

,
,
, These days I'm particularly interested in whats happening in the Online Machine learning space and the innovation that's happening around commoditizing Big Data Analysis.
,
,
,
, For me it would be:
,
, ?? ,
,
,
, Actually none, I try to do my learning via Academic papers, Blogs, conferences and talks.  "
"
,
,
Businesses now have more access to information on customer than ever before; the opportunity to provide real-time analysis of customer interactions is becoming increasingly important in identifying an advantage over competitors. The challenge remains to make use of this data to build and maintain a loyal client base. 
,
Through investment in innovative web analytics practices and analysis of social media platforms, organizations are offered a unique opportunity to react to the data made available. To learn more about such opportunities, success stories, challenges and best practices, I recently attended the ,Social Media and Web Analytics Innovation Summit 2014 (May 1-2, 2014) at San Francisco, CA, organized by the Innovation Enterprise. The summit brought together industry leaders who have seen results from implementing an effective web analytics initiative to share their case studies and best practices. Illustrated intermittently with case studies, interactive panel sessions & deep dive discussions this summit offered solutions and insight from those working within the space. 
,
It covered a wide variety of areas including measuring, evaluating & predicting the Social Consumer, Multi-Platform Consumer Engagement, Social Media ROI and Multi-variate consumer engagement and more. To help its readers succeed in their Analytics pursuits, KDnuggets provides concise summaries from selected talks at the summit. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key talks for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers. 
,
Here are highlights from selected talks on day 1 (Thu, May 1):
,
, delivered the very first talk discussing about development of an ,Enterprise-Scale Recommender Solution.  Organizational complexity is the biggest challenge as professionals from different domains such as science, engineering, business and product need to be brought together. A good solution will focus on organization by ensuring the fundamental function in each aspect first and continuously improving once effects on KPIs are established.  The key prerequisites are source data, tracking system and testing system.  In the execution phase technology, product and business professionals work together to understand the problem well and come up with best solution to make things work. , Choosing the right similarity metric plays a significant role in recommender systems. Product experts put recommendation in the user experience and figure out the channels to which recommendations can it be applied for example mobile, email, app, etc.  Business experts should focus on KPIs to drive quality and just not quantity (by merely focusing on click rate). Concluding the talk, Cliff emphasized that the big challenge of organizational complexity can be solved by working on all aspects first at high-level and then improving iteratively through testing, acquiring more data, etc.
,

, talked about ?€?Charting Collections of Connections in Social Media?€?. Briefly describing Social Network Theory, he explained the various components of graphs relating to social context. Key metrics of measuring social network characteristics are centrality, cohesion, density and betweenness. He claimed that crowds matters the most and explained how connections between people in crowds clearly define sub-groups. Social Media is all about connections from people to people. Each event that occurs leaves behind a pattern, which can be analyzed and used to predict similar upcoming events.  
,
As more social interactions move through, machine-readable data sets new insights and illustrations of human relationships and organizations become possible.  But new forms of data require new tools to collect, analyze, and communicate insights.
He introduced the NodeXL tool, and demonstrated how one could map and measure social connections using NodeXL. With the goal of making Social Network Analysis easier, NodeXL performs network analysis as per the data flow shown below. At the end, he displayed various visualizations of Twitter data for some popular hash tags using NodeXL. 

,
,
,
See also 
, and 
,, both created with NodeXL.
,
, presented on ?€?Metrics for the Mobile App Ecosystem?€?. With millions of apps available on Google Play store & iTunes App Store, the App Revenue is projected to be over 46 billion by 2016. Thus, it is high time for enterprises to start building their apps if they don?€?t have it yet.  More than half of Americans now own a smartphone. Now users are more interested in documenting and sharing things on smartphones, which are empowering the users. 
He recommended monitoring metrics such as click to call, time spent browsing, etc.
,
,Google Analytics is now switching from session-based world to user centric view. In its move from web analytics to digital analytics, Google has introduced some cool features including cross device tracking which allows tracking user?€?s actions over multiple devices (web, mobile, smart TV, etc.). Google?€?s Universal Analytics can also track native mobile apps provided it is connected to web. The mobile analytics have three focus areas: acquisition, engagement, and outcomes. 
,
, delivered a talk titled ?€?Online Analytics: From Vanity to Actionability?€?. He commenced the talk stating the purpose of analytics as enabling smart decision, leading to ""actionability"". He then defined ""actionability"" as digital product improvements i.e. existing feature enhancements, adding new features, etc.  He discussed how our thinking of the digital space has evolved, as now digital is a medium to engage and delight customers. So, the importance of customer engagement has been accentuated by recent consumer trends. Looking further deep, true engagement means users finding digital experience useful, usable and enjoyable. Krish presented a case study about his own company?€?s website.  He concluded stating that there are five analytics steps to achieve actionable insights:
,
, ?? ,
Next part: ,
  "
"
        ,  "
"
By Ran Bi, May 15, 2014.
,
To make a safer decision, most enterprise users prefer to seek advice from others. They go to bloggers and read what bloggers are telling them or to industry analysts such as Gartner and Forrester. Uppd8 found a business opportunity from it. It is a place where you could see how the entire crowd thinks about the technology, also the most useful information about it by your role. 
,
The first enterprise topic that Uppd8 has addressed is Big Data. Its engine analyzes the sentiment of Big Data content items that were pre-processed, in order to screen irrelevant or less meaningful items. 
,
,
, is a startup company, whose mission is to enable and accelerate technology adoption based on crowd sentiment, and based on high quality data that matches the needs of the decision makers. Their sentiment analysis is done by a third party software, while their IP (Intellectual Property) is the content filtering, aggregation, simple UI, and primarily content scoring by user type.
,
I will illustrate how it works with an example of SQL and NoSQL. SQL tag is used mainly with traditional databases, while NoSQL is for new non-relational databases. If I want to see what is the crowd sentiment of NoSQL versus SQL, I add SQL as the first tag and NoSQL as the second tag. The sentiment analysis result for the past 12 months is shown below.
,
,
,
,
,
,
"
"
,

,

Businesses now have more access to information on customer than ever before; the opportunity to provide real-time analysis of customer interactions is becoming increasingly important in identifying an advantage over competitors. The challenge remains to make use of this data to build and maintain a loyal client base.

,

Through investment in innovative web analytics practices and analysis of social media platforms, organizations are offered a unique opportunity to react to the data made available. To learn more about such opportunities, success stories, challenges and best practices, I recently attended the ,Social Media and Web Analytics Innovation Summit 2014 (May 1-2, 2014) at San Francisco, CA, organized by the Innovation Enterprise. The summit brought together industry leaders who have seen results from implementing an effective web analytics initiative to share their case studies and best practices. Illustrated intermittently with case studies, interactive panel sessions & deep dive discussions this summit offered solutions and insight from those working within the space.

,

It covered a wide variety of areas including measuring, evaluating & predicting the Social Consumer, Multi-Platform Consumer Engagement, Social Media ROI and Multi-variate consumer engagement and more. To help its readers succeed in their Analytics pursuits, KDnuggets provides concise summaries from selected talks at the summit. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key talks for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
,

,

Here are highlights from selected talks on day 2 (Fri, May 2):
,

, delivered a talk titled ?€?Don?€?t Forget the Softer Side of Analytics?€?. In a quick introduction of Analytics, he stated that success is dependent upon asking the right questions about both the interests of business stakeholders and the characteristics of your data in your planning.?? Defining term ?€?Research?€? as the planning, collection and analysis of data that is relevant to decision making he mentioned that our purpose within Analytics is to become the best researchers possible.

,

Research process comprises of five steps: Define Objective, Design Research, Data Collection, Data Analysis & Interpretation, and Report. ??The first step i.e. defining research objective, is not done most of the time or done so poorly in Analytics. ,When laying out research think in following terms: problem to be solved, what?€?s needed to be solved and your initial hypotheses. The second step of research process, Design research, is of three types: Exploratory (not done in Analytics), Descriptive (Surveys ?€? Little use in Analytics, Databases ?€? Frequently used) and Causal (Experiments ?€? Analytics not as involved as needed). Discussing the third step, Data Collection, he explained scales of measurement: categorical, ordinal, interval, ratio. He explained the ?€?NOIR?€? hierarchy as following:
,
,
,
Data Analysis and Interpretation is the step where we (data analysts) often start research.?? He concluded the talk claiming that ,

,
, and , shared key learning from mining the YouTube Ecosystem. They discussed three things that they have learned about data through video:
,
, ?? ,
, talked about Evernote?€?s Analytics Platform. Starting with some impressive statistics about Evernote, he gave 3 laws of Data Protection as:
,
, ?? ,
,

Giving an overview of the technical infrastructure at Evernote, he discussed various components one by one:
,
, ?? ,
He then shared following key insights:
,
At the end, he claimed that ,
  "
"
        ,
,
,
,
,
    "
"
,
,
Companies today store large volumes of diverse data from web logs, click streams, sensors, and many other sources. The insights hidden within this poly-structured ?€?big data?€? hold tremendous business value. The business potential of big data analysis is enormous across virtually every business sector. Many agree that the highest value is achieved through predictive analytics which apply advanced techniques such as machine learning and real-time regression analysis to predict future events and drive decisions or actions, potentially in near real-time.
,
A recent IBM-sponsored study by Ventana Research (a company focused on big data analytics in marketing) has claimed that Predictive Analytics has entered the mainstream business decision-making. Building upon the market perception, the study highlights the current status and future expectations from Predictive Analytics. Applying predictive analytics has emerged as the highest priority from the response of all participants regarding the top capabilities of Big Data Analytics.
,
Although it has been around for decades, predictive analytics is a technology whose time has finally come. A variety of market forces have joined to make this possible, including an increase in computing power, a better understanding of the value of the technology, the rise of certain economic forces, and the advent of big data. Companies are looking to use the technology to predict trends and understand behavior for better business performance. Predictive analytics uses a variety of statistics and modeling techniques, and utilizes data mining, business intelligence tools, and machine information, to make predictions. The emergence of enormous amount of structured and unstructured data and ground-breaking technology deployments are the major drivers for the predictive analytics market. 
,
The global predictive analytics market, valued at USD 2.08 billion in 2012, is expected to see strong growth at 17.8% CAGR during 2013 to 2019. Organizations are collecting increasing amount of data, and predictive analytics software is one of the crucial tools that companies can use to achieve actionable information from this data. Data, and the utilization and management of the same, are increasingly becoming areas of competitive advantage. Hence, predictive analytics is one of the core technologies that enterprises are adopting to be able to compete in the market. 
Predictive analytics is used across different industry verticals such as banking and financial services, insurance, government, pharmaceutical, telecom and IT, retail, transportation and logistics, healthcare, and energy, among others. Global predictive analytics market in 2012 was dominated by the banking and financial services segment, which accounted for 16.8% revenue share.
,
For a long time, Predictive Analytics has been primarily the responsibility of the Data Science and Analytics team, but this outlook is changing fast. 

,

While Data Science team still remains the primary contributor, the responsibility is increasingly being shared with database management, BI, LOB (Line of Business) analysts and others. This clearly demonstrates the need for better training and support for the non-technical users of Predictive Analytics. 
,
Predictive Analytics is fueled by data, most prominently customer (69%), marketing (67%), product (55%), sales (54%), financial (51%) and employee (34%) data. Needless to say, data sources for predictive analytics are multiplying rapidly, as are their formats. The biggest challenge with predictive analytics (cited by 55% of research participants) is to integrate tools with their organization?€?s information architecture. Other major challenges include the lack of resources and the lack of awareness.

,
More than two-thirds (68%) of organizations that have implemented predictive analytics said they have gained a competitive advantage from it. Other significant advantages include finding new revenue opportunities and increasing profitability. , The unprecedented shift of business intelligence to predictive analytics offers new opportunities to the major players and new startups in this market. Some of the major players in this market include International Business Machines Corporation (IBM), SAS Institute Inc, Microsoft Corporation, SAP AG, Tableau Software Inc., Information Builders, Fair Isaac Corporation (FICO), Teradata Corporation, Acxiom Corporation, and TIBCO Software Inc., among others.
,
The complete version of the Predictive Analytics study is available at: ,
,
,
,  "
"
,
,
, (May 1-2, 2014) held in San Francisco, CA was organized by the Innovation Enterprise. The summit brought together experts from industry and academia for two days of insightful presentations, discussions, panels and networking. It covered a wide range of topics related to sentiment analysis such as making sense of consumer sentiment, extracting sentiment using NLP, sentiment monitoring, sentiment analysis with Machine Learning & Solr, etc.

,

I had a great time at the summit and would like to share the key points from selected talks. This KDnuggets-style summary is a quick and convenient way to revisit the quality content and key takeaways from the selected presentations at the summit. In case you are particularly interested in any of these talks, keep checking KDnuggets as we plan to soon publish exclusive interviews with some of these speakers.
,
, 
,

Here are highlights from selected talks on day 2 (Fri, May 2):
,
, delivered the very first talk on second day titled ?€?Tracking the Unstated: Beyond Sentiment Analysis?€?. He claimed that by 2020 the definition of sentiment analysis would modify ,(modifications in bold) to ?€?Sentiment analysis (also known as opinion mining or affect extraction) refers to the use of a variety of technologies to identify and extract subjective information from people and the source materials they create?€?. As the content generation and distribution has become tremendously easier over the last few years (thanks to social media, lowering price of hardware and intuitive software) there is a need for ubiquitous continuous sensing to track sentiments. As sensors are moving towards becoming small computers and cheaper day by day, soon they will be all around us, on us and even in us.  Many of these sensors are even connected to Internet. 
,
Giving a quick overview of stress tracking in message, he mentioned that it is possible to monitor stress in real time. Demonstrating an experiment in which a person is tracked through GPS and his stress monitored real-time simultaneously, the person was found stressed for 1st half of the drive when driving to home and stressed entire drive when driving to work.  The sensors are not just getting smaller but also cheaper, more mobile and their continuous use creates entirely new capabilities. He concluded by stating ?€?Sentiment analysis will go way beyond NLP and will contribute to the vision of a Human Centric Intelligent Society?€?
 ,
, talked about ?€?Enabling Opinion-Driven Decision Making?€?.  She initiated the talk by describing the abundance of unstructured ,opinions over the web which are rarely used to help make better and faster decisions. Even in a simple use case such as when one is looking for reviews of hotels with some specific facility (eg: Pool & Spa) and features (eg: Clean, Safe), one would get hundreds of reviews for different hotels on hotel booking websites and there are a number of such websites., Users have to go through the highly tedious process of manually performing their own data mining (mostly just mentally, without using any tools) and make decision based on their unclear, incomplete understanding of the universe of past reviews. This unpleasant process hurts not just the users but also the companies running such booking sites, as users move to other websites or other means of getting third-party reviews (and making the purchase thereafter) which are better understandable. This creates a serious need to use available opinions in more intelligent & efficient way to enable decision-making. 
,
Although there is a lot of research being done and methods being developed to leverage opinions for decision, most of them are still not used by the industry. She then explained these opinion-driven decision making tools: Opinion-Driven Search (help user find entities), Opinion Summarization (help users understand underlying opinions) and Opinion Trend Visualization (help users understand change in opinion). Opinion driven applications improve user productivity, help user retention and increase conversion rates.
,
, talked about the past, present and future of sentiment analysis. After providing a brief history of the popular sentiment analysis tools in the past, he contrasted them with the current methods of sentiment analysis and gave an in-depth demo of a recently released tool ,for text and sentiment classification: ,. Giving examples, he explained various cool features of etcML and how things work at back-end of a very simplistic user interface. He then started introducing more sophisticated models based on recursive deep learning which he believes will eventually supersede the currently used algorithms due to improved performance. He explained recursive deep learning as next-generation sentiment analysis tool, in which each word and phrase is represented as a vector. The discussed model helped to boost accuracy. Using the recursive neural networks model one can look at more fine-grained accuracy.
,  "
"
,
,
, has 20 years experience in the Pharmaceutical and Healthcare industries and he is Global Statistician for four FDA drugs through approval. He has spent the past 5 years consulting on Advanced Analytics across drug development and healthcare platforms.
,
Gary recently delivered a keynote speech at , 2014 held in Santa Clara on ?€?Data Science Now, Next & Beyond?€?. He shared his views on data architecture and science status today, what's happening now and then more on what the future is likely to focus in with respect to more complexity in data science techniques to delve deeper into insight.
,
Here is my interview with him:
,
,
,

,: Larry Page, Google CEO, believes that its feasible to save thousands of lives through the use of Big Data, and certainly I would agree that with a much more integrated set of healthcare information and a focus on personalized healthcare it is possible to provide insight that will support better patient health, which is what Quintiles is all about.
,
As to rising medical costs, that?€?s a tough nut to crack for sure in the US.  Unfortunately there is no one answer and there are many very famous economists, policy makers and colleagues at Quintiles working on this that are much more qualified than myself.  I am going to stick with the provision of using big data to provide insight for better patient health. It?€?s likely to take the rest of my life to succeed but I prefer my chances over fixing the medical cost issue!
,
 

,
,
, Healthcare data is the biggest challenge of them all.  You can tackle claims data and identify certain activities across an event that occurs differently depending upon region and type of event.  You can tackle physicians EMR (Electronic Medical Record) data and surface insights over time concerning specific patient populations and determine more patient impacts to events that occur.  You can connect directly with patients and determine personalized impacts and their views on what works and doesn?€?t work for healthcare. , You can find sub-populations that have specific meta-omic characteristics that could provide insight in personalization of patient outcomes. Within themselves these each provide challenges on the scale and complexity of the data and on the desire to find insight and the resulting validity of the insight found. ,
,
 

The trick is in taking the step back and looking across the healthcare universe, collaborating with colleagues in the Patient, Payer, Provider and Pharma spaces that bring these different views to the table and using data across those disparate sources to both provide new insight but also confirmation of results across each source to justify and validate the results.

 ,

,
,
, Quintiles Infosario?? is a comprehensive, end-to-end clinical development suite of services that seamlessly integrates data, systems, processes and Quintiles?€? knowledge. It enables a new way of developing and commercializing drugs and devices. It enables data transparency, continuous access to information, informed decision-making, and data- driven processes ?€? all underpinned by Quintiles?€? Technology.  New services are being developed that utilize additional real-time data mixed with new data sources, providing a strong basis of ever expanding capabilities to both the Pharmaceutical and Healthcare industries.
,
 

,
,
, Topological Data Analysis looks at the shape of a cluster of data.  The intent is to discover through cluster analysis a network of connected patient events but to then take that a step further in recognizing the shapes of those clusters and how over time differences occur in those shapes as a patient progresses through their events.  For me this is the next stage in the development of how we view disease progression, the changes in patients health over time and hence how to monitor patient similarities to identify a more personalized health path based upon the collection of real world evidence. , , Using the Topological approach we identify the relevant shapes by specific nodules, compressing the data into more manageable amounts. Then as new patients come in with progressively worsening health events they can be monitored in real-time due to the compressed data, looking at how they match their results to those of the Topological analysis and allowing the Doctor to see the expected long-term event progression and how other patients responded to differing treatment options.

 ,

Here is: ,
,
,
,
, ,
,
,  "
"
,
By Gregory Piatetsky, May 15, 2014.
,
A recent post by Matt Turck, a partner at FirstMark Capital, has a great picture of 
,. 
,
Matt Turck writes that the space is getting crowded 
,
,
,
,
The landscape above is a treasure trove of information, but very hard to make sense of.
To analyze it quantitatively, we broke it down by categories and subcategories (thanks to Grant Marshall for getting the numbers). 
,
First obvious inference is that the market is indeed in early stages - of the 358 companies in the chart, only 16 had ""exits"" (purchased or had an IPO), about 4.5% overall. 
The percent of exits is highest for Analytics (7.5%) and Data Sources (6.9%) categories, 
and lowest for Applications (1.4%), and Open Source (zero) categories.
,
,
,
Below is the chart of the Big Data Landscape, by categories and subcategories, ordered by number of companies. 
,
We can also see the most popular segments: 
,In Analytics: ,
,In Infrastructure: ,
,In Applications: ,.
,
Segments with the most exits are Social Analytics and Analytics Platforms.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
, 12 companies, no exits
,
,
 ,  "
"
,
,
Here is part 1 of the interview: ,
,
,, 20 years experience in the Pharmaceutical and Healthcare industries, Global Statistician for four FDA drugs through approval and has spent the past 5 years consulting on Advanced Analytics across drug development and healthcare platforms.
,
Gary recently delivered a keynote speech at , 2014 held in Santa Clara on ?€?Data Science Now, Next & Beyond?€?. He shared his views on data architecture and science status today, what's happening now and then more on what the future is likely to focus in with respect to more complexity in data science techniques to delve deeper into insight.
,
Here is second part of my interview with him:
,
,
,
, Medical devices that track an individuals vital signs are going to become more prevalent and you will start to see reports of these ?€?saving lives?€? through the identification of cardiac events.  But that?€?s not going to be good enough.  Labs on a chip that monitor individuals continuously will provide additional accuracy and insight.  But that won?€?t be enough. ,Bigger and better databases of patients meta-omics (genomics, proteomics etc) will also yield new and interesting insights.  But that won?€?t be enough.  Each of these in themselves are breakthroughs all of which are reliant on big data and all contribute to patients health.  But they all remain in a silo?€?d environment individually providing interesting information unless engaged collectively through systems that connect all of them to a broad centralized database that can look at all the data together, identify specific patterns that can provide specific insights that support Doctors in their standard of care practices and bring that knowledge to doctors so that they don?€?t have to know it all but have it all available to them so that when a specific patient comes in they can engage the patient in a more specific healthcare plan for them that matches to other known patients with similar events.
,
Some individual hospitals and organizations are already progressing down that road, utilizing the big data that they have for their specific patient sets.  Some articles believe that in 10 years that 80% of a Doctors work will be handled through the use of big data and analytics. ,
 ,

,
,
, Learn from others.  I started out in Biostatistics in the heaviest regulated industry possible using very complex statistics.  So starting out the first thing is, get out there. Do something.
,
Then over time I evolved.  With a strong background in understanding what is acceptable and what is crap that really helps with choosing what to attack and what to let others simply waste their time on.  So starting out, make sure you are learning something new, not re-hashing the old stuff.  The internet today really helps.  Linkedin is a must.  Online courses are a must.  Learn, Learn, Learn.
,
As you set your baseline, where ever it is in whichever industry, understand that you always will be learning.  Seek out new methods, reach out to individuals, always question what you are doing and always push for results. Never ever give up and attack, attack, attack.
,
And finally know where you want to go. My passion is Healthcare because my efforts have an impact to my families future.  Use the tools out there such as simplyhired, search on key words that relate to what you want to do, e.g. Analytics, programming, data science etc.  The results you find will show you who?€?s looking for you.  The details will show you what you should be learning (Stats, C#, Java etc). There?€?s a lot of info online that can help you.  Whilst getting a degree and beyond (Msc, Phd) is important temper that with learning from these online tools and doing it yourself. Apply that knowledge into every meeting, every conversation. Keep learning, keep attacking.
,
 

, 
,
, I tend to use a library of nook books which is shared as a family, so most recently I did end up re-reading the Hunger games trilogy! Most of my time though for reading is spent on things like flipboard which really helps me accumulate the distinct specific interest areas I have.  Forbes I have found has a lot of interesting articles, Reddit is fun at times. That sort of thing.
,
Otherwise I am a pretty healthy nut for movies, and also watch car shows with my son who is a nut for Top Gear.  To get away from it all though I like to do the handy man things around the house, building / planting in the garden or re-styling my daughters room for the umpteenth time. I force my son away from Minecraft as much as I can and get him cycling, and I enjoy cooking as I find that a way to focus on something completely different whilst still ending up with a good result at the end!
,
,
,
,  "
"
,
,Sep 29 - Oct 2, 2014
,Dortmund, Germany
,
,
,
Big data in machine learning is the future. But how to deal with data
analysis and limited resources: Computational power, data distribution,
energy or memory? From 29th of September to 2nd of October, the TU
Dortmund University, Germany, will host this summer school on
resource-aware machine learning.
,
,
Further information and online registration at: 
,
,
,
Topics of the lectures include: Data stream analysis. Energy efficiency for multi-core embedded processors. Factorising huge matrices for clustering. Using smartphones to detect astro particles.
,
,
Exercises help bringing the contents of the lecture to life. All participants get the chance to learn how to transform a smartphone into an extra-terrestial particle detector using machine learning.
,
,
The summer school is open for international PhD or advanced master students, who want to learn cutting edge techniques for machine learning with constrained resources.
,
,
Excellent students may apply for a student grant supporting travel and accommodation. Deadline for application is 30th of June.
,
,
,
,  "
"
,
Submissions due July 15, 2014.,
,
,
,
, is an online charity that makes it easy to help students in need through school donations. At any time, thousands of teachers in K-12 schools propose projects requesting materials to enhance the education of their students. When a project reaches its funding goal, they ship the materials to the school.,
,
The 2014 KDD Cup asks participants to help Donors Choose identify projects that are exceptionally exciting to the business, at the time of posting. While all projects on the site fulfill some kind of need, certain projects have a quality above and beyond what is typical. By identifying and recommending such projects early, they will improve funding outcomes, better the user experience, and help more students receive the materials they need to learn.,
,
Successful predictions may require a broad range of analytical skills, from natural language processing on the need statements to data mining and classical supervised learning on the descriptive factors around each project.,
,
Participate at ,
,
,
,
, is a premier interdisciplinary conference that brings together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. This year's KDD features 4 keynotes, 151 Research Track papers, 44 Industry & Government Track papers, 24 workshops, 12 tutorials, and more.
,
,
,  "
"
,
,
, ?€? Pioneering predictive analytics leader , is offering business analysts and data scientists worldwide the chance to win cash and other prizes for predicting the winners of the 2014 soccer world championships.,
,
In 2010, RapidMiner , the winner of the final game more than a week before the matches began. The company used its RapidMiner Server solution to monitor sentiment in news texts, blog and forum posts ?€? analyzing more than 1,000 online channels and thousands of posts per minute ?€? and aggregated the results. As a result of its sentiment analysis, RapidMiner correctly identified Spain as the winner of the 2010 championship, which featured a matchup between Spain and the Netherlands that Spain won 1-0.,
,
,
Now, RapidMiner is inviting all data enthusiasts to use RapidMiner to predict which team will hoist the trophy in Brazil on July 13, 2014.,
,
,
,
The RapidMiner Challenge will bestow two $5,000 grand prize cash awards. The first award will be given to the team or individual who correctly identifies the winners of the group round. A second award will be given to the team or individual that correctly predicts the winners of the 2014 world championship before the matches begin. Entrants will use RapidMiner to:,
,
,
Anyone can , and use a free community version or trial version of RapidMiner from the company website.
,
,
In addition to the two grand prizes, RapidMiner will award other prizes, including: free RapidMiner Studio Personal editions; invitations to RapidMiner World, taking place August 18-21, 2014 in Boston; T-shirts and more.
,
,
Registration for the RapidMiner Challenge will begin immediately and run until midnight EDT June 11, 2014. To be eligible for the $10,000 grand prize, entrants must be 18 years of age. To view the full contest rules, along with more information about how to enter, visit 
,. For contest updates, and to share what you would do with $10,000, visit 
,, and on , use the hashtag #RapidMinerChallenge.,
,
For more information, visit ,.
,
,
,  "
"
Most popular 
, tweets for May 14-15 were
,
Facebook Network analysis & visualization is easier with httr from R wizard 
, on GitHub #rstats 
,
,
Natural Resources are finite: How much is left for those born In 2010 ,
,
Facebook Network analysis & visualization is easier with httr from R wizard @hadleywickham on GitHub #rstats ,
,
,
,
,
,  "
"
,
,
,
Recent KDnuggets Poll asked: 
,
What data types/sources you analyzed in the past 12 months? 
,
,
Multiple choice were allowed. 
,
The most popular data types, not surprisingly, were 
,- table data (fixed n. columns), 77%
,- time series, 48%
,- text, 41%
,- itemsets / transactions, 27%
,- location/geo/mobile, 20%
,??,
Comparing with a similar 
,, we see that the data types/sources with the highest increase in usage were
,
,??,
The largest declines in usage were for
,
,??,
We also added new options in 2014 poll for accessing data from a database engine, and took the top 7 database engines from 
,.
,
Overall, 70% of all respondents have accessed data from some database, but only about 20% accessed NoSQL databases (Hadoop, MongoDB or another DB engine)
,
Here is a table with full results.
,
,
,
,
Here is a breakdown of database engine source by popularity, with the rank and popularity score from db-engines.com site for May 2014, rounded.
,
,
,
We note that popularity ranking of database engines for data mining, 
as shown in this poll, is NOT the same as db-engines ranking, with SQL Server being an especially popular source for data analysis.
,
We also analyzed co-occurrence of popular data types with different types of databases,
and measured ""affinity"" of a database engine to data type as ratio of how frequently this data type was used in conjunction with that database, divided by average % usage of that data type. 
The usage of a particular data type and a database engine by the same respondent within a year does not mean that that DB was used for analyzing this data type, but we found some interesting and strong correlations.
,
The database engines with the most affinity for 
,
were MongoDB (1.88) and Hadoop (1.65), while for 
,,
the most popular database engines were Postgres (1.65).
,
We also analyzed the regional breakdown, including total number 
of data types used, and database sources.
,
Table below shows breakdown by Region, with columns:
,
,??,
,
,
,
We note that US/Canada region is leading in N. of different types used, 
usage of all database engines, and in NoSQL engines.
Europe is lagging in using NoSQL engines, while Asia is lagging in usage of data from database engines in general (but not so much with NoSQL engines).
Australia and Middle East participation is too low in this poll to draw inferences.
,
,
,
,??,
,
,
 ,  "
"
,
By Gregory Piatetsky, May 17, 2014.
,
I was pleasantly surprised to discover 
, at the top of the list of 
,, created by 
,,
a serial entrepreneur who was starting companies since age 19.
,
Here are the top 10 on the list
,
,
,??,??
Not sure about that particular list, but I do indeed tweet a lot about startups in 
Analytics, Big Data, Data Mining, and Data Science space, 
under the hashtag
,.
,
Here are my monthly summaries: 
,
,??,
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky, May 17, 2014.
,
CIOReview recently published a
, of the top 100 promising big data companies. Here is the word cloud from companies one-line descriptions, where we can see the most common keywords: Big, Data, Analytics, Business, Intelligence, Solutions, Actionable, Enterprise. 
,
,
,
Below is the list of companies, re-sorted in alphabetical order.
See several other interesting and relevant lists at the bottom of the post.
,
,
,??
,
See the list and details on companies at 
,.
,
,
,  "
"
,
,
,is Chief Data Scientist at TIBCO Software, developing analytic solutions across a number of industries including Financial Services, Energy, Life Sciences, Consumer Goods & Retail, and Telco, Media & Networks. He has been working on statistical software applications for the past 20 years, and has published more than 50 papers and several software packages on statistical methods. Michael did his Ph.D. work in Statistics at North Carolina State University and is Adjunct Professor Statistics in the department.
,
Here is my interview with him:
,
,
,
, First, I distinguish ""big data"" as referring to ,, and??""fast data"" to ,. In that context, I think of big data as (a) data (at rest) that can't be handled with usual data management and??analytic techniques; and/or (b) all available data (at rest) pertaining??to a business problem.
,
Some important big data questions in the enterprise include:
,
, ?? ,
I like to fit generalized additive models to identify (non-linear) relationships, gradient boosting machines to obtain good predictions, PCA with robust partitioning to identify segments, and association rules for affinity analysis. In addition to using a variety of supervised and unsupervised learning methods, we like to use location analytics and optimization methods; and we do a ton of time series analyses for forecasting demand and future results. We , explore the data and models with interactive EDA and visual analytics ?€? this is a must for identifying the business-relevant structures.

,

In my experience one can generate extreme value by applying analytics??at the confluence of (big) data at rest and fast data in motion to solve engineering, manufacturing, R&D and sales & marketing problems.
,
,
,
, I come from the graphics school of John Tukey and Ed Tufte; who I??see as godfathers of the two primary forms of graphics: (a) graphics??for exploratory data analysis, and (b) graphics for pixel perfect??reporting.??Exploratory analysis generates value in data discovery. Report graphics generate value in communicating the findings from data analysis with clarity.
,
, Graphics sequences that force the business user to??refresh and explore data; and ""spot the fires"" in their business.
,
One interesting trend is the emergence of Javascript graphics, for example the d3 library. It's exciting to??add such beautiful graphics to our Spotfire data discovery environment??and to see these graphics respond to filtering, marking, brushing, coloring and layout.
,
,
,
, TIBCO Jaspersoft is a disruptive BI product suite that provides??pixel perfect interactive graphical reporting and components that are??readily embeddable in other software applications. Jaspersoft has a??terrific commercial open source business model including nearly 16 million??downloads, more than 140,000 production deployments and 2,000 commercial customers in 100 countries. More than 400,000 registered members participate in Jaspersoft?€?s open source BI projects. The Jaspersoft AWS pay-by-the-hour service is??particularly interesting. Jaspersoft is an exciting addition to the??TIBCO product family. I'm looking forward to exploring the product??suite, the community and the synergies with the rest of our stack and beyond.
,
,
,
, Pretty much everyone at TIBCO thinks like statisticians.??Statistics is the math of life, a framework for understanding events.??TIBCO is all about understanding and anticipating events; and enabling??action. The Spotfire tag line is ""first to insight, first to action"". ??We are a fast company. We make things happen around the world every second of every day.
,
, My staff develops simple software solutions to complex??problems. We enable our customers to create extreme business value??with our visual analytics software.
,
,
,
, It's all about making complexity simple; and driving insight to action.,
,
Future leaders:

,
, ?? ,
Analytics software plays a huge role in all of this. We will continue??to see leading companies outsmart the rest with 2-speed information architectures??that create rapid value while operationalizing efficient business??processes.
,
,
,
, I don't think it?€?s quite as big a problem as it's made out to be. I have a ton of great talent interested in joining our team. But you do need to recognize and foster great talent; and create working environments and internal communities for synergies and knowledge sharing.
,
,
,
, Spend your time on high value problems. Try out as much productive technology as you can as fast as you can - Spotfire, R, SAS, JS,??python, Streambase for example. Get yourself on projects with collaborative customers??and smart colleagues. Get in your 10,000 hours as fast as you can.??Have fun with your work, your colleagues, and your customers. Be positive, passionate and observant in all aspects of your life. Be respectful??and look to learn and help.
,
, I'm a music and art lover and I enjoy going to art exhibits and concerts. I like musicians such as My Bloody??Valentine, Caribou, Brian Eno, Wire, Ride, Pavement, Sigur Ros, Radiohead, Spiritualized, Nick Cave, Lou Reed, David Bowie, Ryan Adams, Amy Winehouse; directors like Wes??Anderson, Jim Jarmusch, David Lynch, Matthew Weiner; and visual artists like Jenny Holzer, Damien??Hirst, Paul Klee, Ai Weiwei, Gerhard Richter, Rebecca Horn and many more.
,
,
,  "
"
Most popular 
, tweets for May 16-18 were
,
Great find! Intro. to Data Science, v2 (170 pages) used at Syracuse U. for #DataScience courses, free download ,
,
Why code written by scientists gets ugly: we begin with exploratory data analysis and never get to cleaning it up ,
,
Great find! Intro. to Data Science, v2 (170 pages) used at Syracuse U. for #DataScience courses, free download ,
,
,  "
"
        ,
,
Data is becoming the most valuable asset for many companies, but 
, show that most organizations use only a small part of the data they collect.  With data volumes growing exponentially, and the increasing variety and heterogeneity of data sources, how can enterprises leverage more of their data? 
,
,
Help comes from the new and hot field of Data Curation.  Unlike Data Integration, which uses a top-down approach for bringing diverse data sources into one model (not scalable to Big Data), Data Curation uses a bottom-up, data-driven approach.  As Michael Brodie (who is an advisor to Tamr) put it in a ,
,
,
,
, is an exciting new startup which wants to solve the data curation problem.  It was co-founded in Fall 2012 as Data Tamer by two serial entrepreneurs -  
,, a legendary database researcher for whom it was a ninth startup, and 
,, who has been involved in founding  and/or funding over 50 innovative companies. 
With such founders, the company has attracted a lot of 
financing - over $16 million from investors including Google Ventures and New Enterprise Associates (NEA), and a lot of  attention, including a KDnuggets post
,.
,
On May 19th, Data Tamer has emerged from stealth mode and 
, itself to Tamr.
,
Last week, I stopped by their offices in the heart of Harvard Square, Cambridge, and received a briefing from Andy Palmer, Tamr CEO, and his young team, including Alan Wagner and Nidhi Aggarwal.
,
Tamr's approach to solving the Data Curation problem is designed to scale and to improve with more data.  The key ideas are 
,
,: The size of the integration problems precludes a human-centric solution. Machine Learning methods are needed.
,
,: Enterprise data sources are inevitably quite dirty.
,
,: Current Extract, Transform and Load (ETL) systems have scripting languages that are appropriate for professional programmers. The scale of next generation problems requires that less skilled employees be able to perform integration tasks.
,
,: New data sources must be integrated incrementally as they are uncovered. Data Curation is 
,!
,
Tamr also smartly combines automation and human expertise.
,
It starts with using Machine Learning and Data Analysis algorithms to find relationships between data elements and tries to automate ,
data curation tasks. In cases when machine learning is not enough, it has well-defined processes and UI for asking human experts for help, and uses a smart rewards structure to encourage the experts.
,
,
,
Tamr uses a continuous learning approach, and it learns more with usage, helping build an institutional memory and inventory of enterprise data.
,
Tamr supports RESTful APIs to allow companies use their existing visualization and data science toolkits on top of Tamr. Postgres is used as a back-end database. 
,
Some examples of machine learning methods it uses are
,
,??,
In the screenshot below, Tamr identifies several fields which contain similar information for address and are candidates for merging.
,
,
,
Tamr administrator has a nice interface for selecting domain experts who can help with particular questions, including ranking of experts by their expertise and current load.
,
,
,
Tamr has already worked with several large clients. In one case, it helped a major drug company integrate data from 15,000 spreadsheets from its biologists and chemists doing lab experiments. These  spreadsheets had about 1M rows and 100K attribute names.
,
In another case, Tamr helped Verisk Health integrate the claims records for a collection of 300 insurance carriers, with 20 million records overall.  The goal was to consolidate claims data by medical provider and Tamr solution was a significant improvement over previous approaches which required a lot of manual intervention.
,
For more technical details about Data Tamer, see 
,, by Stonebraker et al, Proceedings of CIDR conference, 2013.
,
KDnuggets has also covered other companies in Data Curation space - see
,
,
,
,  "
"
,
,
Mobile applications for women's safety, software to detect electoral fraud and solutions that help optimize food distribution systems were some of the technologies developed by coders on May 9-10, 2014.
,
,Internet search giant Google held a first-of-its-kind global hackathon across US and India, with the goal to inspire the tech community to create technology based solutions for public service problems faced by India. From more than a thousand applicants, around 300 were shortlisted for the event called ,which began May 9, 2014 at Google headquarters in Mountain View and simultaneously at its Bangalore, India campus.
,
The two-day hackathon gathered engineers interested in creating technology-based applications for non-profit organizations including the World Bank, Amnesty International and American India. , (live-streamed to Mountain View), and , delivered keynote speeches as the event kicked off on the evening of May 9, 2014.

,
In his humble and inspiring speech, Amit Singhal said that we all were witnessing just the beginnings of what search technology could optimally do. ?€?We are connecting the human brain to a larger world brain,?€? said Singhal. He added, ?€?Imagine this power in the hands of a farmer in India. It is transformative?€?. In keynote speech by Mr. Murthy, he said, ?€?Please think of designing meta-solutions that will help NGOs solve a problem?€?. He emphasized, ,. Teams competed for several prizes such as Nexus5, Moto X, HP Chromebook, Moto G etc. donated by Google and Intel.
,
,
,
I teamed up with two UC Berkeley graduate students and decided to work on food requirement simulation/prediction problem: create an application that can predict the quantity of food required in each operational region based on the historical data for past about 250 days in a excel sheet. Solution to this problem was sought by ,, a non-profit organization in India that runs school lunch programme across India serving more than 1.2 million meals daily through their network in 9 states in India. As of now, each day class teacher gives a predicted count of students that would be attending next day and based on that Akshaya Patra prepares and serves food.
,

The data provided was highly messy so we cleaned and filtered data to remove unnecessary attributes such as size and volume of vessels used. The raw data was challenging as well as interesting - it had errors (meal serving on a few Sundays), double meal servings on particular days of the week due to likeness of food items, etc. Next, we decided to play along-with some predictive models to see which one works best on the training data set. We applied three predictive models: Moving Averages, Logistic Regression and ARIMA (autoregressive integrated moving average). By running the trained models over testing set, we found logistic regression model as the one giving the best results.

,

After developing the back-end, we started working on web application. The web application had different login for teachers and for Akshaya Patra administrators. For teachers, once they log in they get a prediction count of meals for current day. The UI also asked for teacher?€?s feedback on quantity: Not Sufficient, Sufficient or More than sufficient. Also, they can give feedback on timing of food delivery: Before time, On-time or Late. This helps in training the predictive model each time teacher chooses an option out of three. ,The UI allows teachers to send messages to region administrators. For administrators, once they log in they get a Google Map view with tagged location of schools (location data was provided for a separate route optimization problem). On clicking on particular school, marker tag info window shows the predicted count along with a link to dashboard. On clicking the link to dashboard the administrator can find following for that particular school:
,
, ?? ,

Our solution was highly admired and selected for the Finals round, which had only 12 teams (out of the total of over 70 teams). We did not win the contest. Nevertheless, it was a great feeling to develop innovative solutions which bring the benefit of technology to the section of society which needs it the most. Our solution demonstrated that analytics is not meant merely for enterprises; rather analytics can play a critical role in solving many of the social problems we face today.

,

Though I have participated in quite a few hackathons before, this one was a special experience ?€? thanks to Google?€?s amazing campus at Mountain View, highly competitive participants, enthusiastic organizers and amazing spicy Indian food available throughout the 24 hours of the event.

,
,
,  "
"
By Roberto Navigli,  Sapienza University of Rome, Italy, May 2014.,
,
, release,
,
As an output of the "","" Starting Grant, funded by the European Research Council and headed by Prof. Roberto Navigli, the , of the Sapienza University of Rome is proud to announce the release of ,
,
BabelNet is a very large multilingual encyclopedic dictionary and semantic network created by means of the seamless integration of the largest multilingual Web encyclopedia - i.e., , - with the most popular computational lexicon of English - i.e., ,, and other lexical resources such as ,, ,, ,, and the ,. The integration is performed via an automatic linking algorithm and by filling in lexical gaps with the aid of Machine Translation. The result is an encyclopedic dictionary that provides Babel synsets, i.e., concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations.,
,
Version 2.5 comes with the following features:,
,
,
,
See ,
,
More statistics are available at: ,
,
Attend one of , (and get a free BabelNet t-shirt)!,
,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 19 and later.
,
See full schedule at , .
,
,  "
"
,.,
,
At the recent , conference in Boston where I was on a Panel discussion moderated by Gregory Piatetsky of KDnuggets, I was asked a question as to where we see investment opportunities in the Big Data landscape.
,
,
,
,
This post will expand on some of my comments from the conference. Importantly, the lens through which I make these observations is from our role as a seed and early stage venture capital investor, which means we are looking at where market opportunities will develop over the next 3-5 years, not necessarily where the market is today.,
,
Over the past few years, billions of dollars of venture capital funding has flowed into Big Data infrastructure companies that help organizations store, manage and analyze unprecedented levels of data. The recipients of this capital include Hadoop vendors such as Cloudera, HortonWorks and MapR; NoSQL database providers such as MongoDB (a Flybridge portfolio company where I sit on the board), DataStax and Couchbase; BI Tools, SQL on Hadoop and Analytical framework vendors such as Pentaho, Jaspersoft, Datameer and Hadapt.,
,
Further, the large incumbent vendors such as Oracle, IBM, SAP, HP, EMC, TIBCO, and Informatica are plowing significant R&D and M&A resources into Big Data infrastructure. The private companies are attracting capital and the larger companies are dedicating resources to this market given an overall market that is both large, ($18B in spending in 2013 by ,) and growing quickly (to $45B by 2016, or a CAGR of 35% by the same estimate) as shown in the chart below:,
,
,
,
While significant investment and revenue dollars are flowing into the Big Data infrastructure market today, on a forward looking basis, we believe the winners in these markets have largely been identified and well-capitalized and that
opportunities for new companies looking to take advantage of these Big Data trends lie elsewhere, specifically in what we at Flybridge call Full-Stack Analytics companies.
,
,
A , can be defined in the following way:,
,
,
,
,
,
,
The two points from the above criteria that are especially worth calling out are the concepts of , and ,.,
,
On the concepts of ,, one of the recurring themes we hear from CIOs and Line of Business Heads at large companies is that they are drowning is data, but suffering from a paucity of insights that change decisions they make.  As a result, it is critical to boil the data down into something that can be acted upon in a reasonable time frame to either help companies generate more revenue, serve their customers better or operate more efficiently.,
,
On the ,, one of the most important opportunities for Full-Stack Analytics companies is to use machine learning techniques (an area my partner, Jeff Bussgang, ,) to develop a set of insights that improve over time as more data is analyzed across more customers ?€? in effect, learning the business context with greater data exposure to drive better insights and, therefore, better decisions.  This provides both an increasingly more compelling solution and also allows the company to develop competitive barriers that get harder to surmount over time ?€? i.e., creates a network effect where the more data you ingest, the more learning ensues which leads to better decisions and opportunities to ingest yet even more data.,
,
In the Flybridge Capital portfolio, we have supported, among others, Full-Stack Analytics companies such as,
,
,
Each company demonstrates important criteria for success as a Full-Stack Analytics company:,
,
,
,
,
, is a General Partner at Flybridge Capital Partners, where he focuses on Companies and technologies in the information technology sector including cloud computing, Big Data, and enterprise IT.

,
,
,  "
"
        ,
,

,
,
,
,
,
This poll is closed - here are poll results:
,
,
The Analytics/Data Mining/Data Science field is evolving (as evidenced by the changing names of the field),
and growing (as indicated by the longest ever list of tools in this poll).
,
,
,
With the lines dividing free vs paid software, desktop vs cloud tools, getting more blurred, and with packages getting increasingly more integrated, I combined different types of software into one list.  However, I grouped the tools into several categories 
,
,- Big Vendors, 
,- Analytics Companies, 
,- Languages, 
,- Free tools, 
,- Hadoop-based 
,??,
These categories are not perfect but are useful for post-poll analysis and visualization of interesting trends.
,
I have certainly missed some tools, so please mention your favorite tools in the comments.
,
This is usually a very popular poll (last year it attracted over 1,800 voters), and to prevent 
overly enthusiastic vendors from voting many times, the poll includes email verification. 
,
,
,
For comparison, here are the results of past polls:
,
,- ,.
,- ,
,- ,
,- ,
,
,??,
,
,
 ,  "
"
        ,  "
"
        ,
,
,
,
,
    "
"
,
Latest ,, (May 21, 2014) ,:
,
,??,
Also
, (4) |
, (9) |
, (7) |
, (1) |
, (3) |
, (10) |
, (14) |
, (11) |
, (6) |
, (25) |
,
,
Firms are no longer hiring for skills, they are hiring for the ability to learn new skills,Prasanna Tambe, Asst. Prof. NYU Stern School of Business, in ,  "
"
Most popular 
, tweets for May 19-20 were
,
12 Free Data Mining, Data Science, Applied Stats, and Machine Learning Books ,
,
Exclusive Interview: Michael O'Connell, Chief Data Scientist, TIBCO on How to Lead in Big Data 
,
,
12 Free Data Mining, Data Science, Applied Stats, and Machine Learning Books ,
,
,  "
"
,

,
,, CTO, Talksum, Inc., has over 20 years of front-line applied engineering and operations management expertise. Having managed large-scale infrastructures and service delivery for major names such AOL and Salesforce.com, he has an in-depth knowledge of the enterprise technology landscape. Dale has also focused on large-scale data management and filtering technologies for large advertising networks and security companies, and his vision for the next era of computer science innovation is at the heart of the Talksum Data Stream Router solution.


,
,
,
,: Thank you Anmol. We believe the biggest factor for winning the award is that the Talksum Data Stream Router is a turn-key solution. Our solution has been developed by people who understand the challenges of working with vast amounts of data at internet-scale data centers. At some point, we?€?ve all heard product vendors saying, ""once you get your data into our format?€?"" or, ""you can use our open-source library and write custom tools?€?.?€? But if you think about it, those products require the customer to do more work: writing code, and assuming more responsibility and risk, not less! When we got tired of this in our personal careers, we founded Talksum to provide a real solution.
,
,
,
,: An increasing amount of data coming from increasingly disparate data types combined with the need for real-time actionable insights reveals major problems for the market. These problems can be framed by asking three questions. ,
,
We looked at these questions and applied them to different industries and domains. There are multiple domains of responsibility within in an enterprise company as well as across the public sector, and both of these share similarities. For example, an enterprise company has an internal team with responsibilities analogous to the public sector?€?s first responder team: SLAs to downstream customers internal to the business and capacity and disaster prevention planning. And one company may have many of these teams in some of the following departments: Network Engineering, Database Administration, Application Development, Business Analytics and Reporting.
,
 

Although these departments seem to be separated by domains of expertise, their results are intertwined into the success of the enterprise. Achieving a holistic view of all your data is a cornerstone of the Talksum Data Stream Router. Cross-domain, holistic views, across departments or even industries and sectors allow Talksum?€?s customers to react faster and more intelligently to the data that impacts their success. This same principal holds true with larger more distributed data networks such as energy, transportation, the public sector, and so on. We came to the conclusion that there is a dire need for a new approach: cross-domain networking with real-time data management. Talksum is here to fill this market with an innovative solution.
,
,
,

,: Good question. It?€?s hard to extract one feature over another in terms of importance as they are intertwined. One would have to be ,: the Talksum solution can process millions of complex events per second on a single router. Number two would be ,: it is a hardware-based platform for easy deployment and implementation. And, three would be ,: it offers high efficiency, which relies on fewer resources and translates into less spend and greater value.
,

, ,

,
,
,  "
"
,
Manish Gupta, Microsoft India and IIIT,
Jing Gao, SUNY, Buffalo,
Charu Aggarwal, IBM TJ Watson,
Jiawei Han, UIUC,
,
Paperback ISBN: 9781627053754, $40.00,
eBook ISBN: 9781627053761,
March 2014, 129 pages,
,
,
,
,
Outlier (or anomaly) detection is a very broad field which has been studied in the context of a large number of research areas like statistics, data mining, sensor networks, environmental science, distributed systems, spatio-temporal mining, etc. Initial research in outlier detection focused on time series-based outliers (in statistics). Since then, outlier detection has been studied on a large variety of data types including high-dimensional data, uncertain data, stream data, network data, time series data, spatial data, and spatio-temporal data. While there have been many tutorials and surveys for general outlier detection, we focus on outlier detection for temporal data in this book. 
,
A large number of applications generate temporal datasets. For example, in our everyday life, various kinds of records like credit, personnel, financial, judicial, medical, etc., are all temporal. This stresses the need for an organized and detailed study of outliers with respect to such temporal data. In the past decade, there has been a lot of research on various forms of temporal data including consecutive data snapshots, series of data snapshots and data streams. Besides the initial work on time series, researchers have focused on rich forms of data including multiple data streams, spatio-temporal data, network data, community distribution data, etc.,
,
Compared to general outlier detection, techniques for temporal outlier detection are very different. In this book, we will present an organized picture of both recent and past research in temporal outlier detection. We start with the basics and then ramp up the reader to the main ideas in state-of-the-art outlier detection techniques. We motivate the importance of temporal outlier detection and brief the challenges beyond usual outlier detection. Then, we list down a taxonomy of proposed techniques for temporal outlier detection. Such techniques broadly include statistical techniques (like AR models, Markov models, histograms, neural networks), distance- and density-based approaches, grouping-based approaches (clustering, community detection), network-based approaches, and spatio-temporal outlier detection approaches. We summarize by presenting a wide collection of applications where temporal outlier detection techniques have been applied to discover interesting outliers.,
,
Series: Synthesis Series on Data Mining and Knowledge Discovery,
,
Series Editor: Jiawei Han, Lise Getoor, Wei Wang, Johannes Gehrke, and Robert Grossman,
,
,
,
Use of this book as a course text is encouraged, and the texts may be downloaded without restriction by members of institutions that have licensed accessed to the Synthesis Digital Library of Engineering and Computer Science or after a one-time fee of $20.00 each by members of non-licensed schools. To find out whether your institution is licensed, visit 
, or follow the links above and attempt to download the PDF. Additional information about Synthesis can be found through the following links or by contacting me directly.,
,
This book can also be purchased in print from Amazon and other booksellers worldwide.,
Amazon URL: ,
,
Individual subscriptions to Synthesis are available for just $99.00 per year. This subscription will provide individuals with unrestricted access to all Synthesis titles: 
,
,
Available titles and subject areas:,
,
,
Information for librarians, including pricing and license:,
,
,
Please contact 
, to request your desk copy.  "
"
,
,
August 11 - 14, 2014,
,Aarhus University, Denmark,
,
,
,
OVERVIEW AND GOAL,
The MADALGO Summer School 2014 will introduce attendees to the latest developments in learning at scale. The topics will include high dimensional inference, algorithmic perspectives on learning and optimization, and challenges in learning with huge data.,
,
LECTURES,
The school will be taught by experts in learning:
,
PARTICIPATION,
The summer school will take place on August 11-14, 2014 at Center for Massive Data Algorithmics (MADALGO) at the Department of Computer Science, Aarhus University, Denmark. The school is targeted at graduate students, as well as researchers interested in an in-depth introduction to Learning. Registration will open soon at the school webpage. Registration is free on a first-come-first serve basis - handouts, coffee breaks, lunches and a dinner will be provided by MADALGO and Aarhus University.,
,
ORGANIZING COMMITTEE,
,
LOCAL ARRANGEMENTS,
,
ABOUT MADALGO,
Center for Massive Data Algorithmics is a major basic research center funded by the Danish National Research Foundation. The center is located at the Department of Computer Science, Aarhus University, Denmark, but also includes researchers at CSAIL, Massachusetts Institute of Technology in the US, and at the Max Planck Institute for Informatics and at Frankfurt University in Germany. The center covers all areas of the design, analysis and implementation of algorithms and data structures for processing massive data (interpreted broadly to cover computations where data is large compared to the computational resources), but with a main focus on I/O-efficient, cache-oblivious and data stream algorithms.
,
,
,  "
"
,
,
,, CTO, Talksum, Inc., has over 20 years of front-line applied engineering and operations management expertise. Having managed large-scale infrastructures and service delivery for major names such AOL and Salesforce.com, he has an in-depth knowledge of the enterprise technology landscape. Dale has also focused on large-scale data management and filtering technologies for large advertising networks and security companies, and his vision for the next era of computer science innovation is at the heart of the Talksum Data Stream Router solution.
,
Part 1 of interview: ,
,
Here is part 2 of my interview with him:
,
,
,
,: , We have a recurring question from most new customers and even new employees, ?€?What language do we have to learn to integrate your API into our existing system??€? The second question is, ?€?Does this mean we have to send our data to a cloud??€? The industry has seen solutions wrapped in an API for so long that now the public seems shocked by a ?€?Turn-key?€? solution. Once they realize we?€?re not kidding, we usually see a smile on their face.
,
,
,
,: I do not see this as a career change; my career has been one of solving problems. The point of applied engineering is about implementing new technologies to solve a particular problem.  Operations management is about engineering sanity: knowing which technologies solve problems and which technologies will lead you down the proverbial rabbit hole. Talksum is the greatest extension of the two careers. Talksum provides a reliable?€?and almost boringly dependable?€?solution for the ?€?Big Data"" problem. I have been tremendously fortunate to work in operations at some of the largest data centers and service delivery organizations, and the Talksum Data Stream Router is the byproduct of need.
,
,
,
,: This may be the hardest question. While do I see a lot of great trends in Real-Time Analytics, I do not see a cohesive strategy. A lot of great work is going into simple languages for describing run time algorithms. The storage vendors bringing in some of the more traditional NoSQL data stores into their fabric is pretty cool. I think over the next few years we see a larger shift to focusing on how to gain benefit from our data rather than how we continue to scale overly complex application tiers. Talksum is focused on viewing your data flows as an integral part of your infrastructure rather than an application burden. 
,

,
,
,: Lift your head up occasionally, observe one's surroundings. , Gather specs, know these specs will be wrong or incomplete, and focus on solutions that are forward-looking so you can focus on proving meaningful insight to the data, instead of on simply writing ETL scripts.
,
,
,
,: If you have not read James Gleik's ""Chaos, Making a New Science"" you should. Another favorite is ""The Never-Ending Days of Being Dead: Dispatches from the Front Line of Science"" by Marcus Chown. My family says I have a pretty boring book selection: all Science, Political Science, or stacks and stacks of comic books.
,
,
,  "
"
By Erich Schubert, May 20, 2014.,
,
,  is a visual explorer tool for our new trend detection method on four data sets.,
,
The algorithm is heavy-hitters style; but we additionally track variance and this way can evaluate how significant a trend is; instead of reporting the raw counts.,
,
Our two main contributions are:
,
,
Details on the method will be published at
,, August 24-27.,
,
The web site is only a data explorer; it is not live running the actual method. We have plans to make a live version available eventually - it will benefit from future work and research on e.g. spam filtering and online classification of trends into e.g. teenie idols.,
,
Here is the post I sent to the
,:,
,
At KDD 2014 (August 24-27), we will present a heavy-hitter style algorithm to detect significant trends in a textual data stream. Scalability is excellent due to the use of hashing; and due to the clever use of statistics we can easily identify those trends, who exhibit significant growth, instead of simply reporting the top-10 results as done by traditional heavy hitter algorithms. Results have been very exciting - we made a web app to allow exploring the data sets, and put it online for you to explore the data yourself. For details on the method, you'll have to come to KDD, though!,
,
As this is my first work in text mining, I'm interested in your input. We tried to get an overview (a reviewer noted ""good survey"") but I'm sure we are still missing some nice related approaches.,
,
We do have plans to make a live version available eventually. It will be interesting to attach this to a large web crawler, and watch for live trends in the crawled information; as well as adding online near-duplicate detection (for better spam filtering), online stopword learning, and of course sentiment analysis and classification... exciting times!,
,
This is joint work with +Michael Weiler and Prof. Hans-Peter Kriegel at LMU M??nchen.???,
,
,
Researcher Lehr- und Forschungseinheit Datenbanksysteme,
Institut f??r Informatik,
Ludwig-Maximilians-Universit??t M??nchen,
Germany,
,
,
,  "
"
In the era of Big Data, customers increasingly recognize the value that predictive analytics offers to differentiate their business. 
, provides a scalable, standards-based platform which supports the flexibility to rapidly deploy and execute predictive models across a wide range of hardware and software installations.
,
,
,
Zementis products offer full support for the Predictive Model Markup Language (PMML). This is the standard format used by the data mining community to exchange predictive models between applications. Gartner recognizes the power of PMML by naming Zementis a cool vendor - read the report 
,
,
Zementis is founded on the principle that data science teams and IT departments can collaborate seamlessly and efficiently. By leveraging the PMML standard, Zementis allows predictive models to rapidly move from development to deployment, and execute in various environments that inherently support Big Data - from real-time scoring on the cloud to massively parallel scoring on premise, in database or in Hadoop.
,
To learn how data mining standards can transform your organization:
,
,
,??,
,
,
Zementis, Inc. is a leading software company focused on the operational deployment of predictive analytics and data mining solutions. Zementis was recognized by CIO Review as one of the ""Top 20 most promising Big Data companies in 2013"" and named ""Cool Vendor in Data Science"" by Gartner in 2014. Its ADAPA(R) and Universal PMML Plug-in scoring engines are designed from the ground up to benefit from open standards and to significantly shorten the time-to-market for predictive analytics in any industry. 
,
For more information, please visit ,
,
,
,
,
,
Follow Zementis, Inc.: 
, || 
, || 
,  "
"
,.,
,
Over four months ago I, with two partners, began crafting a , to inform students and young professionals about the data science industry. We interviewed over 30 data scientists, data analysts, CEOs, and academic professionals from the Chief Economist at Google to the founder of Cloudera.,
,
We heard from Tom Davenport on how big data analytics differed from traditional business intelligence. Hal Varian defined for us the type III error, the error that results from asking the wrong questions about data. We also learned data science?€?s greatest challenge; namely, that without proper education, big data doesn?€?t become big strategy or big insight, it stays as big data.,
,
MIT professor Erik Brynjolfsson likens the impact of big data to the invention of the microscope. Similar to how the advent of the microscope enabled us to see things previously too small to be perceived by the human eye, so too does big data enable us to see trends previously too big. But if data science?€?s impact is so great, why aren?€?t we learning data analysis in 8th grade, alongside our biology dissections or history lectures?,
,
,
,
Most of us already know the movie Moneyball as a pop culture example of the positive impact of analytics on an industry. However, Moneyball was even more insightful in displaying how quantitative ignorance almost prevented its implementation. The hostility the general manager encountered from these baseball veterans wasn?€?t because the scouts were unwilling to reconsider their intuition; it was because most of them simply had no understanding of analytics. The simple fact is that no one will implement a product he or she does not understand, no matter how potentially successful or revolutionary.,
,
If we could distill the information we gleaned from all of our interviews into a single takeaway, it would be this: data literacy has become a necessity. We are asked to interpret numbers and charts frequently (at this point, perhaps more often than we do the written word) and doing so accurately has become essential.,
,
,
As the chart to the left demonstrates, just as skillful rhetoric can change our beliefs, so too can skillful analysis. Data literacy has become a fundamental skill for all professionals, a skill so essential that we view it as a consumer right of the 21st century.,
,
And, as such, we have created a platform aimed solely at developing the statistical and programming skills necessary to become data proficient. True data literacy imparts both a statistical understanding and the experience of applying analysis techniques to real data. It requires an understanding of the basic rules of probability and sampling that are utilized in every experiment, along with direct experience manipulating data, whether through Excel or, as is becoming increasingly common, through a programming language such as R.,
,
Exercise your right to data literacy. Visit , and start your journey toward data dominance today. 
Who is the data literate professional?,
,
You have an understanding of the nuances of statistics. You have experience munging with data. You are curious, hypothesis-driven, and experimental. You are product-focused and fixated on how data can improve current situations or result in informed action.,
,
Brian Liou,
CEO of Leada,
Learn Insight in Data at 
,
,
,
,  "
"
Here is the InformationWeek list of top 10 Big Data to follow on Twitter, including their Twitter name, stats, Twitter bio (Tbio), and IW summary.
,
,, 36.2K Tweets, 14.2K Followers
,Tbio: Gartner IT Industry analyst - Hadoop, Big Data, NoSQL, DBMSs, Vendor Lead for Microsoft. Guitarist, husband, dad, dog lover
,IW: Adrian tweets regularly about Hadoop, NoSQL, Microsoft, and other topics.,
,
,, 35.2K Tweets, 5,854 Followers
,Tbio: i helped found RedMonk. if you see someone at a tech conference wearing a Red Sox hat, that's probably me. wrote @newkingmakers. married to @girltuesday. eph.
,IW: He tweets and blogs about a variety of software and development topics, among other areas.
,
,
,, 852 Tweets, 788 Followers
,Tbio: 
Gartner analyst and myself.
,IW: Sicular tweets on all things big data, analytics, BI, data warehousing, data architecture, and Hadoop, among other topics. She also chimes in on specific vendors in the big data universe, such as Cloudera, and she shares bits and pieces from Gartner's research reports.
,
,
,, 22.4K Tweets, 11.1K Followers
,Tbio: PhD Data Scientist Astrophysicist, Top #BigData Influencer. Passions: #DataScience, #DataMining, Astroinformatics, #CitizenScience http://kirkborne.net
,IW: Borne regularly posts and retweets links to interesting articles on big data and data science.
,
,
,, 14.5K Tweets, 19.5K followers
,Tbio: KDnuggets President, Analytics/Big Data/Data Mining/Data Science expert, KDD & SIGKDD co-founder, was Chief Scientist at 2 startups, part-time philosopher.
,IW: Piatetsky's KDnuggets site (KD stands for ""Knowledge Discovery"") is a trove of big data, data mining, and analytics information, including job openings.
,
,
,, 3,337 Tweets, 8,201 Followers
,Tbio: Data Viz Wiz | Journalist | Growth Hacker ... I love big data, data science, and world travel.
,IW: Pierson works with a range of clients via her Data-Mania concern, and her Twitter feed is chock full of big data-related news, data visualization discussions, insights on data-related vendors, and other topics.
,
,
,, 118K Tweets, 12.7K Followers
,Tbio: Data Scientist, Data_Nerd Founder Analytical-Solution. What can your data do for you? Measure, Segment, Research and Data Analysis - keys for increasing ROI
,IW: A data scientist by trade, Gentry is also a prolific tweeter with a sharp eye for news and trends that will have an impact on the industry.
,
,
,, 19.6K Tweets, 6,564 Followers
,Tbio: Data to Dollars??? specialist w/ track record of doing it @fitzanalytics. Advisor/Author/speaker/dad. Creator of methods that turn data into social good. #D2DVC
,IW: Fitzgerald works with Wall Street banks and others to develop quantitative, data-driven business strategies.
,
,
,, 9,588 Tweets, 5,244 Followers
,Tbio: IT analyst with Ovum covering Big Data & data management with some systems engineering thrown in. Views stated here are my own.
,IW: Baer, another favorite among the analyst community, leads the big data research at Ovum.
,
,
,, 14.9K Tweets, 7,964 Followers
,Tbio: Big Data, Analytics, Business Intelligence, Performance Management, Social Business, Information Management, Business Strategy
,IW: Borba's active feed is a particularly good read if you're interested in how big data translates to bottom-line business -- in other words, making money.,
,
Here is the original article Information Week 
,.
,
,
,
,  "
"
,
,
,,
to be held August 24-27, 2014, New York, NY, USA is the 
, in Data Mining, Data Science, and Knowledge Discovery.
,
KDD 2014 will bring together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. 
,
This year KDD features 
,
,??,
Register by July 15, 2014 to enjoy early bird registration rates and special hotel rates.
,
,
,
KDD-2014 has a special theme: Data Mining for Social Good. It will highlight how the work of data analytics researchers and practitioners contributes towards social good, and how these high impact social problems provide a rich set of challenges for KDD researchers to work on.
,
,
,  "
"
,
,
, is the Vice President, Data Science and Strategic Analytics for TE Connectivity (TEL), the $13.5B global electronics manufacturer. In this role, Mr. Wendell leads the global team responsible for data science and analytics across the company. Mr. Wendell was brought into the company to construct the data science team from scratch and to pioneer the company?€?s move into advanced analytics.
,
Prior to joining TE Connectivity, Mr. Wendell was Vice President, Global Strategy & Business Development at American Express, where he led the company?€?s move into analytics-driven business models. Before American Express, Mr. Wendell's experience include co-founding several technology start-up companies, driving the successful turnaround of an IT services firm, and management consulting to Fortune 100 telecommunications clients. Mr. Wendell holds an MBA in Decision Technologies from the Tepper School at Carnegie Mellon and an undergraduate degree in Philosophy and Mathematics, Summa Cum Laude, from Brandeis University.
Mr. Wendell is a regular speaker at events on big data and innovation.
,
Here is my interview with him:
,
,
,
,: Yes, I get this question a lot.?? As I mentioned in my presentation at the Big Data Innovations Summit, MIT quoted 55% of Big Data projects as failing to drive the desired outcome.?? By most testing standards, 45% passing rate is a failing score.?? This result is a terrible thing for those of us who are in the field.?? We need to get this score up!?? The question is: where do we start?
,
From my experience, primary causes of such project failure happen after the team performs good analysis and delivers powerful analytics insight.?? For some reason, the insight, however brilliant, often doesn?€?t get acted on by the business.?? How many times have you heard an analyst say, ?€?Well, I proved to them that they need to do such and such differently?€?but they didn?€?t listen!?€??? At the end of the day, the objective in a company is to make money.?? I see this phase, between the time when an insight is delivered and the time that money is made, as the last mile of Analytics engagements.
,
,From my experience, most projects fail in the last mile.?? They do not fail because someone used the wrong algorithm, modeling technique or didn?€?t have enough data.?? Yet, most of those in our profession are focused on creating better algorithms, modeling techniques or getting better data.?? Of course we need focus on those things too.?? But what if the root causes for project failure are rooted in other issues that we are not focusing on??? Then we will never get our profession?€?s failing score up.
,
,
,
,
,
,: For starters, for readers who haven?€?t heard my presentation yet, the ,Five Pillars of Success for Data Science teams are:
,
, ?? ,
,If a data science team pays attention to these five areas, my experience is that the success rate should be better than 45%.?? Now, I won?€?t argue that these are the only pillars that contribute to success.?? I would be delighted if people in the community could suggest other pillars.?? My hope in putting these five pillars out is that we can give the data science community a few manageable areas of focus for getting their team?€?s EQ up, which will drive better engagement success.
,
I don?€?t see these areas of focus as top down.?? I see the pillars as critical items that aren?€?t mentioned as part of CRISP-DM.??, My analysts are critical to helping recruit and mentor new talent.?? Of course, there is a leadership component as well.?? It is the job of analytics leaders, whether it be in a start-up, a consulting firm or a fortune 500, to ensure that everyone on the team is connecting the dots.?? Not having the right context to connect the dots is where things start to go off the rails.?? This usually happens when well-meaning analysts, lacking adequate context, make what they see as reasonable assumptions.?? These assumptions can spell death for an analytics or data science engagement.?? Leaders need to set a culture where analysts understand that it is incumbent on them to escalate quickly to obtain critical context.

,
,
,
,: I agree with Brynjolfsson that most decisions at the top are largely made by HIPPO?€?s (Highest Paid Person?€?s Opinion).?? That said, many decisions at the top have evolved into something so complex that it becomes next to impossible to find the data that informs a ?€?right?€? answer.?? A few years back, people were saying that Harrah?€?s would fire a senior leader who ran an experiment without a control group. ??Even if this is true, Harrah?€?s is in the minority.
,
Many complex decisions at the top occur because Analysts, Managers and Directors have failed to apply a data-driven approach when the decision was on a smaller scale.?? So the decision either isn?€?t made or is made sub optimally.??, If this occurs in a critical area in the business, eventually the situation snowballs and results in a much more complex decision?€?which may be tough to dissect with data.?? In that regard, I don?€?t believe that applying data-driven decision making at the top is the right starting point.?? My focus is about empowering data-driven decision making more broadly within the organization.
,
To some extent, I think the proper application of business experimentation, data and analytics can enable decisions to be made less centrally, less top-down (decentralized).
,
Second and last part of interview: ,
,
,
,  "
"
        ,
,
,
,
,
    "
"
,
,
, 2-day event was organized by Global Big Data Conference on May 5-6, 2014 specifically for the decision-making executives, and it addressed various questions everyone is asking such as:??How can I leverage data to make better decisions across the enterprise? How well-positioned is my organization to take advantage of Big Data? What do I really need to get started???The focus of the event was on the application of Big Data rather than on the clusters, data warehouses, and coding that support it.

,

, These concise, takeaway-oriented summaries are designed for both ?€? people who attended the event but would like to re-visit the key talks for a deeper understanding and people who could not attend the event. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.


,
Here are highlights from selected talks on day 1(Thursday, May 5):



, delivered an interesting talk on ""Hadoop-Enabled Business Intelligence Use Cases"". , He started with giving the following facts from a recent Wikibon survey:
,

, ?? ,
According to Wikibon, there are three compelling reasons for this struggle to achieve maximum business value from big data:
,
, ?? ,
In order to solve this crucial problem, Ankur suggested the following strategy:
,
, ?? ,
Introducing Sears as a cutting edge integrated retailer he discussed various use cases such as product perception, brand sentiment analysis, behavioral and predictive analytics on network data, and real-time inventory management. He concluded the talk stressing critical need of data governance and data hub across the enterprise.

,

,??delivered a talk on ?€?An industry in flux -??the Shifting Vendor Landscape?€?.????He introduced typical solution architecture as having the following components (stacked up left to right): Platform, Database Management System, Application, End-User Device and End User. Next, he explained various types of components that can be used:
,
, ?? ,
Discussing about platform in detail he said storage, memory, networking, computing and cloud technology are improving rapidly each day. He described End User?€?s dilemma as user expects more than what specialized solution can provide. He discussed the following business problems:
,

, ?? ,
Giving DBaaS (database-as-a-service) as a solution, he emphasized that it improves usability of databases and handles ?€?administrivia?€? so that application user can focus on innovation. Talking about DaaS(Data-as-a-Service), he pointed out that such simplification (through self-service provisioning) is desperately needed and DBaaS helps simplify the work-flow.

,

,talked about ?€?Delivering Insights to the right person, at the right time, in the right manner?€?.????He started by describing data evolution (exponential growth of data), accelerating technology (more data, reduced storage costs, more apps, more analytics), and the five dimensions of Big Data (data, technology, analytics, decisions and mindset). Talking about delivering insights, he mentioned that data can be of different types: structured, semi-structured, batch, real-time, near real-time, etc. There are many analytic techniques available today that can be used to generate the insights. Transforming the data in a structured form from its raw, unstructured form is the key. He explained this through some use cases.

,

,commenced his presentation by talking about the enormous amount of data that we all generate every single second. With exponential increase in number of smart devices, which is projected to be 12.5 billion by 2020, there are still only 12% of executives who really feel that they understand the impact data will have on their organizations.????In order to shrink the gap between data production and data usage we would have to capture and analyze massive data volume that too securely with unification of data platforms. During analysis of all data, Big Data solutions capturing sensitive information must be protected and audited.????Founding member of Apache Sentry are working on fine-grained authorization for Hadoop. Introducing Oracle Unified Data Platform, he explained some features such as querying any data with SQL, Metadata Integration, Intelligent Query Optimization, etc.

,

,??talked about how to go for Big Data without the Big Bucks. Talking about Open Source Software (OSS) he mentioned that there is no one best universal tool for everything. There is always a need to adapt, customize and be agile as the problems we solve are constantly changing. Data scientists are constantly bouncing from task to task, like a handyman.????They should use a lot of lead bullets rather than looking for a silver bullet to get the job done. He then displayed the leading OSS tools available for Data Science tasks, found through a survey:
,

, ?? ,
He concluded the talk emphasizing that organizations should save their dollars for people (salaries, training, etc.), resources (hardware, AWS, etc.) and proprietary software if no viable OSS alternative exists.
,
Highlights from day 2 will be made available soon.
,
,
,
  "
"
,
,
I got an interesting email from my connection 
,,
a former CMU Professor, Machine Learning Researcher, and co-founder of a successful search and discovery startup Vivisimo, which was
, in 2012.
,
Raul announced an exciting new starup called 
,
,.
,
As Raul described it 
,
,
Here is a very interesting 
,, from the idea of OnlyBoth that Raul had in 1998, when he read
,
,
and thought, ""can we program software to automatically discover such things?"", to 2014 when the startup was launched.
,
OnlyBoth just launched a colleges application at 
,: 
,3,122 colleges described by 190 attributes give rise to many tens of thousands of insights, all automated. 
,
The data comes from the US Dept. of Education 
,.
,
Enter your college and get a sentence or paragraph that says how it's unique, surprising, or compares to others.  Two CMU examples:
,
,1. ,
,
,2. ,
,
More interesting examples:
,
,??,
Here is additional press coverage:
,
,??,
A sports application is planned to launch soon.    "
"
,
,
I recently had an email discussion/interview with Saxon Global, to find out more about what are they doing.,
,
, is one of the fastest growing BI, Big Data, and Cloud Service providers. Headquartered in Texas, and with offices in Bangalore and Hyderabad in India, Saxon Global has grown fast since 2011 and is delivering solutions to Financial Services, Retail, Telecom, Healthcare, Banking and Media Entertainment companies. Here is the discussion, with answers provided by Mr. Haricharan, VP, Sales & Marketing at Saxon Global.,
,
,
,
,: The key factors that explain this trend are the growth of analytics services in the last few quarters, emerging companies in the space, the Indian IT majors making paradigm shift in their offerings to the all-new SMAC, where Analytics is forming a major pillar.  SMAC stands for Social, Mobile, Analytics and Cloud. As the traditional transactional systems are getting outnumbered by the number of applications that are getting developed on the Social, Mobile and Cloud platform, the data generation, dissemination and storage has also changed. Analytics applied to the new kind of data on different platforms is gaining ground.,
,
,
,
,: The possibilities of Big Data applications are enormous and can be envisaged to solve almost any data problem. Some of the realistic applications of Big Data I feel would fall in more unstructured data domain and less on the structured side. So, industries/business which generates or rely more than 50% on data coming in as unstructured will be able to exploit the capabilities of Big Data much better and will demonstrate higher ROI. Retail, Geology & Climate, Energy, New Age Media, Telecommunications, Healthcare qualify as natural candidates and have use cases from real-time alerts on the weather, CDR analysis, personalized recommendations while surfing or transacting on-line or energy optimization alerts can make the best use of Big Data Technologies.,
,
,

,
,
,: One of the earliest success stories came to us with implementation of Big Data Analytics solution to leading media portal. This I must say was much before the Big Data officially was coined and Hadoop became kind of synonymous to Big Data. We build VLDB (Very Large Data base) using the columnar DB techniques and the in-memory database such as Berkeley DB. We helped them to crunch millions of log-lines, provide real-time alerts and integration into ad server network for effective engagement.,
,
,
,
,: We have had challenges rather than failures we have encountered. This has been more on identifying the right use case, defining and prioritization of key success factors for big data implementation. Adoption rate and socializing the big data capabilities inside the client organization has been tough for us.,
,
,
,
,: Currently Privacy has not been seen as a major concern among people though the Companies and Government are highly concerned about it. People at large are still enjoying the new found experience of personalization that is being offered to them that effectively is using their private data (monitoring their behaviour, mapping them to their demographics and slicing/dicing the tastes, preference).,
,
But, in near term when they find getting bombarded with ads, promotions and awkward requests they will get awakened and raise objections for it. This would be similar to what we have seen in the telemarketing space where users will start subscribing to do not call list or will turn off all the cookies and would like to leave little or no data at all to be mined. 
,The recent ruling from the European courts of backing
, by Google and allowing individuals to demand for it will be kind of norm in most of the countries.,
,
,
,
,: Big Data is no exception and cannot escape the fads and facts that come along with any new technology or tools. Fads of today will convert to facts of tomorrow and in a way hype encourages several things such as more experimentation /proof of concepts, technology collaborators/contributors to play around and help it mature.,
,
The hype around Big Data is that all the data problems that we have will come to an end and it provides nirvana to Companies grouping with large and varied data sets. The reality is that it has helped few verticals (Media & Entertainment, Healthcare) to achieve the benefits it promised, it has been quite successful where incremental approach was taken. It is yet to go full steam in verticals such as Financial Services, Energy & Utilities.,
,
,. Data Sets and Technologies will become more like Lego blocks that provides interchangeability and lower cost of total ownership. I somehow feel the greener initiatives and pressure to reduce carbon foot print will also question the necessity of storage of large amounts of data (if the pay-off is not greater or the opportunity cost is undesirable) and crunching every data emanating. We understand and crave for the data driven decision making but sooner or later we may be advised ?€?,?€?.,
,
Information diet would be more like we ensure that we are getting the necessary nutrients but at the same time ensuring not over burning ourselves with too much consumption and grinding of data. I am also convinced data the economics principle of Law of Diminishing returns will become applicable and will help us realized data volumes beyond particular point is of no consequence or will have little impact towards decision.,
,
,
,  "
"
Most popular 
, tweets for May 21-22 were
,
.@McKinsey predicts that 1.5 million #BigData managers will be needed by 2018 in N America. Become one with #ieMBD ,
,
Outlier Detection for Temporal Data 
,
,
Outlier Detection for Temporal Data ,
,
,  "
"
,
,
,??,??
,??,??
,
,
,??,??
,
,  "
"
,
,
, is the Vice President, Data Science and Strategic Analytics for TE Connectivity (TEL), the $13.5B global electronics manufacturer. In this role, Mr. Wendell leads the global team responsible for data science and analytics across the company. Mr. Wendell was brought into the company to construct the data science team from scratch and to pioneer the company?€?s move into advanced analytics.
,
Prior to joining TE Connectivity, Mr. Wendell was Vice President, Global Strategy & Business Development at American Express, where he led the company?€?s move into analytics-driven business models. Before American Express, Mr. Wendell's experience include co-founding several technology start-up companies, driving the successful turnaround of an IT services firm, and management consulting to Fortune 100 telecommunications clients. Mr. Wendell holds an MBA in Decision Technologies from the Tepper School at Carnegie Mellon and an undergraduate degree in Philosophy and Mathematics, Summa Cum Laude, from Brandeis University.
Mr. Wendell is a regular speaker at events on big data and innovation.
,
First part of interview: , 
,
Here is second part of my interview with him:
,
,
,
,: We decided to build my team as part of our Corporate Strategy, Business Development and Analytics team, reporting directly to our CEO.  This gives us the tremendous advantage of seeing which challenges are the largest ones facing the company. , In addition, this approach helps with the critical executive engagement required in the early days of building up a new data science capability.  With this approach, I do need to work hard to ensure my team stays highly connected with our lines of business.  Our internal customers are ultimately the business units and TE?€?s end customers, so it is critical that we don?€? fall into the trap of becoming stuck inside of corporate.  I can?€?t disclose our strategic priorities.  However, our CEO, Tom, has publically mentioned many times that one of TE?€?s key assets is our deep customer relationships.  My team spends a good deal of time visiting with end customers.  
,
,
,
,: I think the answer to this question depends on where an organization is in its journey with data science and analytics.  For organizations at an early stage in this journey, which is where most companies are, establishing a center of excellence can be a good move. , Pretty soon thereafter, the demand for analytics starts exceeding the COE?€?s ability to supply it.  Around that time, the COE needs to begin pushing analytics out into the organization.  A strong COE can be instrumental in this push by offering training and rotation programs.  Ultimately, if you look at the Googles or Harrah?€?s, who are analytics masters, they have achieved a distributed analytics capability.  For those of us not as far along as Google, we need to see distributed analytics as a destination.  I like the way Davenport describes this journey in ?€?,?€?  
,
,
,
,: I agree with those who say Big Data is more about Data than it is about Big.  I really like the Hadoop ecosystem of technologies, and the power of open-source distributed processing. , This technology means that we no longer have to bet on whether Moore?€?s law continues to keeps pace with the rate of data generation.  We can elastically scale our ability to analyze larger sets of data.  In addition, the cost of storing data has decreased dramatically.  The cost for storing 1 GB of data in 1980 was $300,000.  Now the cost is 5 cents.  We have crossed a crucial inflection point in history:,
,
The net net of having distributed computation and more data is that there will be more opportunity to deliver data-driven insights.  I like to believe that better insights will drive better decisions, which, in turn, translate into economic efficiencies. Organizations that aren?€?t strategically accelerating towards this future could find themselves in a tough spot.  But, to succeed, we also need to be focusing on the softer ?€?EQ?€? skills I mentioned above.    
,
,
,
,: I think that practitioners need to focus on their all-around Analytics EQ. , The critical element to framing is knowing which analytics techniques are best at solving which sort of business problems.  Combining active listening with analytic framing enables a practitioner to have a conversation with a non-technical person that is both engaging and arrives at the right approach.  
,
,
,
,: May I mention a movie?  Jiro Dreams of Sushi.  I was entranced by the way Jiro approaches making sushi with such passion, always striving to be better.  I found it really inspiring.  What if those of us in Data Science and Advanced Analytics approached our profession with even a fraction of this dedication?  I imagine a , of Data Science being able to walk into any company, make sense of all of the data and transform the entire organization.  
,
,
,  "

"
,
,
,??(May 1-2, 2014) held in San Francisco, CA was organized by the Innovation Enterprise. The summit brought together experts from industry and academia for two days of insightful presentations, discussions, panels and networking. It covered a wide range of topics related to sentiment analysis such as making sense of consumer sentiment, extracting sentiment using NLP, sentiment monitoring, sentiment analysis with Machine Learning & Solr, etc.

,

I had a great time at the summit and would like to share the key points from selected talks. This KDnuggets-style summary is a quick and convenient way to revisit the quality content and key takeaways from the selected presentations at the summit. In case you are particularly interested in any of these talks, keep checking KDnuggets as we plan to soon publish exclusive interviews with some of these speakers.

,

Here are highlights from selected talks on day 1 (Thu, May 1):

,

,??delivered a talk on ?€?Twitter Sentiment Analysis Using N-grams with a Dynamic Neural Network?€?. This is a very interesting advance that combines semantic approach and machine learning approach. He discussed about solutions to following two problems with Social Media Sentiment: Vast amount of data and continually changing languages.??,??He gave a brief overview of Sentiment Analysis explaining the key terms such as lexicon, dictionary, weights, etc. In semantic analysis approach it is easy to understand the feature set and the weights, and there is no need to develop training set. ,He explained various lexicon problems such as Completeness, and Stemming. Switching to machine learning approach, he presented advantages such as no limit to the number of features, weights generated from the training data, etc.????This approach has problem of term frequency. Next, he presented advantage of using a blended approach. By using Simple Weighting Algorithms based on the sum of N-gram weights, messages land into four sentiment tiers: Strong Negative, Mild Negative, Mild Positive and Strong Positive. Therefore, the blended approach has a comprehensive coverage of data and provides quick access to sentiment insights, while the lexicon improves continuously.

,

,delivered a talk on ?€?Integrating Linguistic Features into Sentiment Models: Sentiment Mining in Social Media within Industry Setting?€?. , The central goal of the talk was to show how linguistically-informed algorithms can be adapted to an industry setting as well as tailored to the language of social media. It was focused on phrasal feature discovery and auto labeling with minimum language processing tools. There are several constraints to the industry setting: focus on specific domain, limited time & resources and off-the-shelf tools which are hard to adapt to the domain & noise. Despite of these constraints, one thing is very clear - we need to go beyond the dictionary meaning of the words due to linguistic variability (with infinite dimension), context sensitivity, negation, and domain specificity. She discussed following challenges:
,
,
, ?? ,
Talking about inference and phrase learning, she mentioned that labeled seed features (?€?great?€?, ?€?terrible?€?) and anchor features (?€?but?€?, ?€?and?€?) give auto-labeled phrasal features. Recursively infer phrasal features i.e. add inferred phrases into original seed features on the go. She concluded by claiming that phrasal features allow for more nuanced sentiment models with minimal annotation.
,
,talked about ?€?Sentiment Analysis for Cold Start Items?€?. With too many reviews to read, it?€?s difficult for eBay users to
, make informed decisions and even more difficult for the manufacturers to keep track of customer reviews. One of the newly emerged problems in the area of Opinion Mining is Aspect-based Opinion Mining, which includes tasks such as: Aspect Extraction, Rating Prediction and Opinion Phrases Extraction. All the state-of ?€?the-art LDA models are applied at item level. The impact of the size of training data is evaluated on a real-life data set from Epinions(a general consumer review site). Next, she analyzed the results and found the following:
,
,
, ?? ,
Introducing the cold start problem, she called items with few reviews as ?€?cold start?€?. In real life data sets, more than 90% of items are cold start. Discussing the problem of identifying aspects and estimating their ratings for cold start items, she proposed Factorized LDA (FLDA) as a solution, training at the category level. The solution assumes that both items and reviewers can be modeled by a set of latent factors. She then gave a quick intro to the intricacies of the model. The perplexity for the proposed FLDA model has a lower value, indicating better performance over LDA, D-PLDA, and I-FLDA.????She also discussed application of this solution in item categorization and overall rating prediction.
,
Next part: ,  "
"
,
By Gregory Piatetsky,  
,, May 26, 2014.
,
Deep Learning is a very hot area of Machine Learning Research, with many remarkable recent successes, such as 97.5% accuracy on face recognition, nearly perfect German traffic sign recognition, or even , with 98.9% accuracy.  Many winning entries in recent Kaggle Data Science competitions have used Deep Learning.
,
The term ""deep learning"" refers to the method of training multi-layered neural networks,
and became popular after 
, by Geoffrey Hinton and his co-workers which showed a fast way to train such networks.
,
Yann LeCun, a student of Geoff Hinton, also developed a very effective algorithm for deep learning, called 
,,
which was successfully used in late 80-s and early 90-s for automatic reading of amounts on bank checks.
,
See more on ConvNet and factors enabled recent success of Deep Learning in my exclusive 
,.
,
In May 2014, Baidu, the Chinese search giant, has
,, a leading Machine Learning and Deep Learning expert (and co-founder of Coursera) to head their new AI Lab in Silicon Valley, setting up an AI & Deep Learning race with
Google (which hired Geoff Hinton) and Facebook (which hired Yann LeCun to head Facebook AI Lab).
,
Here are some useful and free (!) resources for learning and using Deep Learning:
,
,??,
The packages which support Deep Learning include
,
,??,
,
,
,??,
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for May 23-25 were
,
TGIF: A SQL query walks into a bar, confidently approaches two girls at two tables and asks ""May I join you? ,
,
Data Science vs. Statistics: one big difference is that Data Science focuses on finding actionable knowledge ,
,
Data Science vs. Statistics: one big difference is that Data Science focuses on finding actionable knowledge ,
,
,  "
"
	,By Ran Bi, May 26, 2014.

	,

	,

	,

	,

	,

	,

	,

	,

	,

	,

	,

	,

	,

	, ,
   "
"
By Gregory Piatetsky,  ,, May 26, 2014.
,
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 26 and later.
,
See full schedule at , .
,
,  "
"
,

,
, has 20 years of experience, creating game changing technology products, services and strategies. His experience includes the management of product lines with revenues totaling $1.8B/year. He has launched ground-breaking products, driven world wide strategies and helped setting de-facto industry standards. As an expert on Trusted Computing, Virtualization and High Performance environments he became a sought-after advisor to many Fortune 500 companies and government organizations. ,In his past roles he was involved in all aspects of the product life cycle including engineering, product management, marketing, business development and sales. He also developed and introduced products and services into virtually every commercial and government, channel and market segment for organizations such as Sun Microsystems, SonicWALL and GreenBorder (acquired by Google).
,
Here is my interview with him:
,

,
,
, At the time we (,, my Co-Founder and CTO) realized that there was
a) no general purpose Machine Learning System b) specifically one that does Big Data and c)
pretty soon every enterprise will want to do what the Google's and Amazon's of the
world have been doing for years.

,
Our name has two connections, one ""Sky"", our team has roots in large-scale
astronomical data analysis and advanced data structures it just so happens
that
,we have strong ties to the Astrophysics community
(,),
and ""Tree"" referring to the cosmic tree of knowledge found in many
mythologies,
a reference to our mission to bring state of the art research knowledge
from
leading edge science to the enterprise, such as machine learning. We were
looking at probably 50-60
different names, and when we wrote Skytree down, we knew we had a winner.

,

,
,
, The latter would have certainly been easier. However, today we call
Skytree a
horizontal platform with vertical capabilities. What that means is that it
is
indeed a general-purpose platform but we have created vertical and
application
specific solution sets atop of the platform. Ultimately that??s one of the
reasons why customers end up working with us. The machine learning
applications
of today and the future are multi-faceted, a ""one-algorithm-for-all""
approach is
not going to work for the Data Driven Enterprise.

,

,
,
,
, At the core Skytree is a product company, and it??s not so much of a change
but
rather a compliment to our offering. We realized organizations were
looking at
Skytree not just as a provider of software but their trusted Machine
Learning
partner. We wanted to help our existing and new customers that were just
getting
started as well as sophisticated users with a second opinion. To us it was
logical consequence of responding to a market need.

,

,
,

, Aside from our large enterprise customers, we are also working within
biotech on
early stage cancer detection. The impact of this type of work could be
profound
since it could end up saving many peoples lives.

,

,
,

, Technology leaders have proven that it works and are having a ton of
success with it. ,So
now everybody else wants do that as well. , I think the next 10 years will be dominated by
machine learning (and its re-incarnations) and within 5 years every
Fortune 500
will have a machine learning system at their disposal.

,

,
,


, In addition to predictive analytics, the two that are coming up quite
frequently
are machine learning based recommendations and outlier/anomaly detection.
This
is where the power of a platform comes into play, customers usually want
to do
more than one thing i.e. to do classification followed by a regression or
another machine learning task. Our target customers are usually large
organizations with, well, big data. The more data the better ?? we can help
with
the machine learning part, but we definitely work with them in a
collaborative
fashion to establish the best possible use case.

,

,
,

, The obvious one is the arrival of big data, no big surprise here. In
addition,
customers are no longer satisfied with just using BI, which tells us what
happened yesterday. They want to leverage things like machine learning to
become much more predictive about their business. In essence they want to
unleash all their data that they have been storing for years to help them
get
better insights and make the right decisions ?? faster. In other words they
want
to become a data driven enterprise.
,
The arrival of the data scientist and with it the Chief Data Officer/Chief
Data
Scientist is also happening. This may or may not be a centralized
organization
but it shows that customers are now putting much more emphasis on the
results of
their analytics and it's quickly becoming a massive differentiator from a
competitive point of view., ,
,




,
,

, Well, I'm somewhat of an information junkie and I'm a fan of KDNuggets. The
article that most recently caught my attention was the ,. Outside of the office, most my time is spent reading but I am also
a big music fan so every now and then I get to listen to some tunes and on
occasion I will catch a soccer match. In another life, I was a professional soccer
player for Bayern Munich in Germany and have been avidly following the news
leading up to the World Cup Brazil 2014.
,
,
"
"
,
There are two Data Science focused workshops as part of the Bournemouth University's Festival of Learning in June. 
The events are linked to the 
,
and the launch of a new Data Science Institute at Bournemouth University in UK.
,
Both events are free to attend, but registration required.
,
,
,. Full day, June 10.
,Workshop chair: Prof. Bogdan Gabrys, Data Science Institute, Bournemouth U.
,Here is a 
,
including details of keynote speakers and abstracts of their talks.
,
,
,. IT as Utility Network+ co-sponsored half a day workshop on the 9th of June.
,Workshop chair: Prof. Bogdan Gabrys, Data Science Institute, Bournemouth U.
,Here is a
,.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
We are pleased to announce the publication and availability of our 
textbook on data mining and data science.
,
,
,
by Mohammed J. Zaki and Wagner Meira, Jr.
,Cambridge University Press, May 2014
,
See also
,
,
,
Companion Website for Online Resources:,
,
,
,
,The fundamental algorithms in data mining and analysis form the basis
for the emerging field of data science, which includes automated methods
to analyze patterns and models for all kinds of data, with applications
ranging from scientific discovery to business intelligence and
analytics. 
,
This textbook for senior undergraduate and graduate data
mining courses provides a broad yet in-depth overview of data mining,
integrating related concepts from machine learning and statistics. The
main parts of the book include exploratory data analysis, pattern
mining, clustering, and classification. The book lays the basic
foundations of these tasks, and also covers cutting-edge topics such as
kernel methods, high-dimensional data analysis, and complex graphs and
networks. With its comprehensive coverage, algorithmic perspective, and
wealth of examples, this book offers solid guidance in data mining for
students, researchers, and practitioners alike. 
,
Key features:  
,
,??,
,
,
Gregory Piatetsky-Shapiro, President KDnuggets, Founder ACM SIGKDD, the leading professional organization for Knowledge Discovery and Data Mining
,
,
Professor Christos Faloutsos, Carnegie Mellon University and winner of
the ACM SIGKDD Innovation Award
,
,
,
,  "
"
        ,
,
,
,
,
    "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
    "
"
Most popular 
, tweets for May 26-27 were
,
Where to Learn Deep Learning - Courses, Tutorials, Software 
,
,
Machine Learning Algorithms Tour: Regression, kNN, Regularization, Decision Tree, Bayesian, SVM, Deep Learning ,
,
Where to Learn Deep Learning - Courses, Tutorials, Software ,
,
,  "
"
,
Latest ,, (May 28, 2014) ,:
,
,??,
Also
, (3) |
, (5) |
, (1) |
, (1) |
, (1) |
, (3) |
, (5) |
, (2) |
, (2) |
, (3) |
, (5) |
,
,
,??,
,: A SQL query walks into a bar, confidently approaches two girls at two tables and asks ""May I join you?""  "

"
        ,
,
,
,
,
    "
"
        ,
,
,
,
,
    "





"
,
,
, has twenty-seven years of experience in analytics and data technologies.  He practiced data science before it had a name, worked with big data when ""big"" meant a megabyte, and supported the movement which brought data management and analytic technologies from back-office, skunk works operations to core competencies for the largest companies in the world.
,


In 2010, Walt became the first hire west of Denver for Vertica, makers of the Vertica Analytics software platform for real-time analytics of structured and unstructured data.  Since then, he has helped build the HP Vertica customer base and team in the Western USA.  He has worked with a wide variety of SQL and NOSQL technologies as well as tools and techniques for analytics.  Now as Chief Field Technologist with HP Vertica, Walt has the unique pleasure of addressing customer needs with the continuing evolution of Vertica and HAVEn, the HP Big Data strategy which links hardware, software, services, and business transformation consulting for successful execution.
,
Here is my interview with him:
,
,
,
, At HP, we see Big Data as transformational.  Businesses who master it can fundamentally shift their business model to tremendous competitive advantage ?€? they can disrupt their competition.  So we created HAVEn (,adoop file storage and processing, ,utonomy IDOL, ,ertica Analytics platform, ArcSight ,nterprise security Manager a,d applications) to help organizations be the disruptor instead of the disrupted. This represents the combination of our expertise as the No. 1 IT vendor in the world, the breadth of our offerings, platform openness, and the partner ecosystem we bring with it.  Today, HAVEn represents a technology portfolio which solves Big Data problems, but it also represents a strategic framework for us to continue evolving it as a platform.  
,
,
,
,The aspect of HAVEn which excites me the most is its openness.  It allows organizations worldwide to select from a best-of-breed portfolio of big data technologies ?€? right now ?€? and solve today?€?s problems.  As we continue to build the platform it will continue to evolve in parallel with Big Data challenges, so it?€?ll solve tomorrow?€?s problems as well. 
,
I would contrast HAVEn from the competition first by the way HP is approaching it ?€? we?€?re not asking organizations to wait two years for us to build the technology.  It?€?s available today.  Also, HAVEn is open ?€? companies only buy what they need, and we don?€?t require a particular distribution of Hadoop ?€? we work with all of them.
,
,
,
, Big BI is doing the same type of thing as today (reports, dashboards, etc.), just with a larger volume of data.  It?€?s important and useful, but it represents an incremental advance in analytics.  And sometimes that?€?s just what a business needs.  Big Data, on the other hand, incorporates the idea of bringing together data which has never been available before ?€? Tweets, machine logs, geospatial device data, call center audio, etc. ?€? and deriving meaning from the joined data which would?€?ve otherwise been impossible.   This represents a huge opportunity, but can be an ambitious effort.  The choice a company makes ought to be determined by where they?€?re likely to find the most value for their investment.
,
,
,
,  There are multiplying returns to doing this as the cost of relating to a customer decreases, while at the same time increasing customer engagement and satisfaction by delivering the right interaction in the right way at the right moment.  We?€?ve heard this success story repeated time and time again from our customers. 
,
,  This is a very active area of research and development for HP. 
,
,
,
, I see two major challenges today. , First is the challenge of building a technology portfolio which addresses the full spectrum of needs to building Big Data Analytics from ingesting the data, to transforming it, to exploring it and building models for explanation and prediction, to putting those models to work effectively for the business.  Many organizations have some of the pieces, but not all of them.  And once they get them, building the analytics practice takes time, commitment and focus.
,
Second is the human bottleneck.  Once we build a technology architecture capable of dealing with Big Data in all its forms, the data scientist becomes a potential bottleneck in the process.  This isn?€?t necessarily a bad thing ?€? after all, we need to exercise rational thought to identify the variables which might predict customer churn and test them.  So even if we can perform every other step of the analytic process in minutes or seconds ?€? the time to build a model is constrained by the data scientist.  While not strictly a technology issue, we clearly need to continue to advance the state of the art in ways that provide the data scientist with ways to lever their expertise.   
,
,
,
, The first factor to consider is whether it should be outsourced at all.  I recommend organizations ask themselves whether Big Data is strategic to their business, and to factor that in to an outsourcing decision.  It may not be a good idea to let an outsourcer build (and manage) your competitive advantage. If outsourcing makes sense, the next factor to evaluate would be the vendor?€?s real-world big data experience.  Big Data hasn?€?t been around very long, but the practice of analytics certainly has, so that experience is a plus.  Also, Big Data analytics often borrow heavily from the agile development methodology ?€? so I?€?d recommend looking for a vendor comfortable with that approach. 
,
,
,
, The more things change, the more they stay the same!  The tasks we perform today in the practice of data science are almost identical to those I first performed as an undergraduate years ago.  We just have to do them faster, better and cheaper.  Taking six months to answer an analytical question is not acceptable today.
,
I?€?m particularly excited to see some of the young Big Data technologies mature.  Today, we?€?re in the same place we were with the web in about 1997, which is to say it?€?s all very exciting and there?€?s a lot going on, but there?€?s no ?€?killer app?€? yet.  Hadoop is a great example.  It?€?s very promising, and it tackles certain specific problems very well.  But it isn?€?t yet clear just where Hadoop is going to end up.  Will it commoditize distributed processing, or something else?  It?€?s got tremendous potential, but like most young innovations, it?€?s still searching for a killer app.
,
, 
, I finally got around to reading ?€?,?€? by Ray Kurzweil.  It?€?s a great read, and can be a helpful way of looking at the rate of technology change.  It?€?s a useful way for me to look back at changes in analytics and Big Data over the last twenty-five years or so and distill some of the large scale patterns.  I like his optimism, although for me the jury is still out as to whether a technological singularity based on AI would be a net positive.  Either way, I?€?d definitely recommend it.
,
,
,

  "















"
,
,
Across many industries, large and small organizations are using analytics and data science to offer greater insight and customer service. The gaming industry is almost well placed in that - particularly with online and social gaming - the companies already keep a vast amount of data on gamers. The challenge remains to make use of this data in a way that offers true value for money whilst enhancing the user experience.

,

,I had recently attended the , (May 1-2, 2014) at San Francisco, CA, organized by the Innovation Enterprise. The summit brought together acclaimed speakers and attendees for deep insight into how the gaming industry uses analytics and data science. For speakers, it had a line-up consisting of 20+ leading executives working in Analytics, Data Science & Business Intelligence in gaming.??Through real-life business case studies and deep-dive discussions, the summit offered solutions and insight from the leaders in the Gaming space.

,
, These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key talks for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.


,
Here are highlights from selected talks on day 1 (Thu, May 1):

,

,talked about ?€?Boosting Detection in Call of Duty?€?. For non-gamers, Boosting is defined as the act of two or more players collaborating through dishonest methods in ranked multiplayer lobbies to gain in-game rewards, such as camouflages. He gave examples of detecting unwanted behavior such as cheating, boosting, etc.. Boosting is most frequent of the unwanted behaviors and is most diverse in terms of patterns.,The problem is that it:
,

, ?? ,
His solution approach included designing around the problems (such as constraints design, too smart players) and building an analytic service to detect boosting. Talking about analytics application, he mentioned it should employ many analytics models, be decision centric and capable of running at scale. Boosting detection service is similar to fraud detection systems. It is basically a classification problem which can be solved through semi-supervised learning on training data and anomaly detection. The major objective of modeling is to reduce false-positives.

,
After trying a lot of approaches, the gradient boosting machine (GBM) aka boosted trees was found to be the most optimal choice. Modeling wasn?€?t the most difficult part; it was Scaling, involving database optimization, query queue with query weights and jobs running in parallel even for same model. The results of boosting detection service provide data to studios to facilitate decision-making.

,
,discussed about Valve?€?s approach to the acquisition, collection, and interpretation of data across its products and services. ,Talking about decision-making at Valve, he mentioned the following key aspects:
,

, ?? ,
He also noted that the decision making process is explicit, data-driven, theory-driven, iterative and based on measurable outcomes. He mentioned that it is important to define the questions first and thereafter think of designing the data schema.

Next, he described Operational Game Stats (OGS), a platform used for recording gameplay metrics such as kills, deaths, hero selection, in-game purchases, bullet trajectories, etc. Organizational schemas are defined for each game and data is sent at relevant intervals. He concluded his talk with case studies where the insights obtained from an iterative loop of hypothesis and feedback were used to change game design, delivering significantly better user experience.


,

,talked about aggregating data to effectively run a healthy live service in real time. Discussing about operationalizing data to promote service health, he mentioned that Xbox LIVE service hosts 30,000,000+ unique players and targets availability of 99.950 service uptime availability for those customers. That?€?s only 22 minutes of ?€?acceptable?€? unavailability each month. In order to achieve such a level of service availability, service must be monitored and cared round the clock. The service is spread across many thousands of servers most of which have very specific roles within the service.

,
Next, he discussed intelligence in scaling-capacity planning and elastic utilization of services. Discussing about elastic services and fluctuating capacity, he mentioned that uneven service utilization during holidays, title releases, and special events needs to be predicted and planned in advance. Data is also used to protect the service from bad actors. One of the challenges was to deal with 2.5 million gamertag complaints received in past 12 months. A ?€?trust score?€? was defined for ?€?good?€? gamers (based on age, past activity, level achieved, frequency, social factor, etc.), in order to automate the correct identification of when the ?€?offensive?€? tag for any gamer is legitimate.

,
He emphasized that ?€?an engaged community is an asset?€? - community ambassadors utilize the service more frequently, more broadly and understand the entire service offering more completely than other customers.

,

,shared very interesting and insightful lessons from building games on Facebook and mobile. The rise of freemium social games happened first on Facebook. ??At that time, the best way to get players to spread the word about your game was to bombard them with pop ups, share prompts, and requests only loosely tied to your game. ??As the Facebook platform matured and freemium reached mobile devices, players began to disconnect from these more forced mechanics and become more responsive to visible but more passive approaches.

,
With over 2 million monthly players, Broken Bulb has earned considerable success without any funding, simply through high-quality games and strong metrics. He outlined the following suggestions:
,

, ?? ,

He concluded the talk saying ?€?Social is for Engagement not Acquisition?€? i.e. people come from word of mouth and not from mere social posting.
,
,
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,, May 29, 2014.
,
, is a free, open-source template for Microsoft Excel 2007, 2010 and 2013 that makes it easy to explore network graphs.
NodeXL is the product/service of
,, which is headed by 
Marc Smith,
,, 
Director of Social Media Research Foundation.
,
Here is KDnuggets Twitter Social Network, as visualized in NodeXL on May 25, 2014.
,
The graph represents a network of about 1,396 Twitter users whose tweets between Apr 24 and May 25, 2014 contained ""kdnuggets"", or who were replied to or mentioned in those tweets. 
,
Some of the findings
,
,??,
,
,
An experimental interactive version of this graph is
,.
,
There is an edge for each ""replies-to"" relationship in a tweet, an edge for each ""mentions"" relationship in a tweet, and a self-loop edge for each tweet that is not a ""replies-to"" or ""mentions"".
,
The graph is directed. The graph's vertices were grouped by cluster using the Clauset-Newman-Moore cluster algorithm.
The graph was laid out using the Harel-Koren Fast Multiscale layout algorithm.
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,Summary: This is the main connected component, of tweets originating from @kdnuggets.
,
Details:
,
,??,
Top URLs in Tweet in G1
,The top URLs here are infographics and cartoons
,
,??,
,
Summary: The top URLs here are KDnuggets posts - reports on meetings and interviews that were shared perhaps via KDnuggets tweet button and not via social media.
,
Details:
,
,??,
Top URLs:
,
,??,
,
,Summary: Note that this cluster includes #ff and #fs tags - KDnuggets was frequently mentioned as part of #ff (Follow Friday) and #fs (Follow Saturday) tweets. The central node in this graph is @kirkdborne.
,
Details:
,
,??,
,
,
,??,
,.
,Central nodes in this cluster are @hey_anmol and @yvesmulkers.
,
Details:
,
,??,
,
,
,??,
,
,
,
,
,
 ,  "
"
,
,
, was a two-day event organized by Global Big Data Conference on May 5-6, 2014 specifically for the decision-making executives, and it addressed various questions everyone is asking such as:??How can I leverage data to make better decisions across the enterprise? How well-positioned is my organization to take advantage of Big Data? What do I really need to get started???The focus of the event was on the application of Big Data rather than on the clusters, data warehouses, and coding that support it.
,

To help its readers succeed in their Analytics pursuits, KDnuggets provides concise summaries from selected talks at the event. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the event but would like to re-visit the key talks for a deeper understanding and people who could not attend the event. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.,
,
,
Here are highlights from Day 2 (Tuesday, May 6):

,

,gave a talk on ?€?Thinking dynamically about the rapid pace of innovation in Big Data, when designing analytic systems?€?. He started by mentioning that it is not merely the size of data that matters. Rather, the size of data required itself depends on the specific situation. For example:
,
, ?? ,
He stressed that we need to think dynamically forward. Referring to velocity, volume, variability and sophistication as the four major technological and architecture challenges of Big Data, he said that all of these cannot be solved by the same solution patterns. He demonstrated the key components of operational data architecture and analytical data architecture, focusing on the seamless integration of both to deliver advanced capabilities., Data warehousing (DW) appliances have matured into enterprise-grade DW platforms. Business Analytics thrives on cheap horsepower offered by DW appliances, which are 20 times faster for less than half the total cost of patchwork systems. The increasing need for speed and exponentially decreasing DRAM prices are leading to the rise of in-memory analytics. Next, he discussed the importance of unstructured data warehousing, as nearly 95% of enterprise data is not structured. After discussing other components of architecture, he emphasized that there is a great need to:
,
, ?? ,
He concluded stating ?€?Machine Learning is great but keep the humans involved?€?.
,
,talked about how to develop strategy and build business case for Big Data. Even though Big Data is on the radar of leadership in every industry, it is hard to get organizational buy-in and successfully implement Big Data projects. The common challenges include lack of budget, lack of support from Finance, lack of resources, and inadequate information. He suggested focusing Big Data conversations on Business Value of the target benefits. In order to deliver real value, project planning needs to thoroughly evaluate:
,
, ?? ,
Discussing about identifying business outcomes, he gave the following steps:
,
, ?? ,
It is important to assess and identify capability gaps through Architecture Gap Analysis and Comprehensive Gap Analysis. Finally, talking about building a business case, he mentioned that Advanced Analytics provides answers that cannot be provided by traditional business intelligence. He also stated that shareholder impact and value of Advanced Analytics grows with ambitiousness of use cases.
,
, gave an interesting talk titled ?€?Needle in the hay ?€? Finding, motivating, and retaining big data talent?€?. After outlining a brief history and current status, he shared the following talent trends for future:
,
, ?? ,
Emphasizing on the immense demand for Big Data professionals, he shared the report from Gartner stating that Big Data will create 1.9 million IT jobs in US by 2015. Sharing his thoughts on how to attract IT workforce, he suggested company professionals/recruiters to aggressively network, attend conferences, and create webinars & white papers. He concluded by giving the following tips for entrepreneurs:
,
, ?? ,
,delivered talk titled ?€?Revolutionizing Weather Intelligence?€?. He mentioned that currently there is too much weather data available in an unorganized form, creating a big mess. Such data has the ability to rapidly, consistently, and globally transform business applications with Weather Intelligence through:
,
, ?? ,
He concluded his talk with use cases of intelligent weather data for engineering, insurance, telematics and national intelligence.
,

,
,  "


"
By , (,), May 30, 2014.
,
As Big Data buzz settling down, the focus is steadily shifting from collecting huge amount of data and performing generic analytics to a continual assessment of return on investment and real business value of Big Data solutions. This is the most important and also, the most difficult part. Yet, through a smart selection of right Big Data technologies, the leaders are transforming organizational culture, incrementally learning the right approach using quick iterations, and designing an efficient, cost-effective Big Data architecture. The true business value will not come from data-richness, it will rather come from leveraging data to take smart decisions.
,
, will focus on ""The Business of Big Data"" and educate the attendees on the best way to prepare themselves for the trends mentioned above. The conference is a great opportunity for learning as well as networking with the leaders from industry and academia through a seamless exchange of information, ideas and perspectives. The conference will be held on June 22-24, 2014 at the San Jose Convention Center, in a beautiful and self-contained expansion to the Center that just opened in September 2013.
,

The impressive line-up of speakers, which includes Big Data leaders from various industries, will share the best practices through real-world case studies and tutorials on a wide variety of topics such as:
,
, ?? ,
The conference program includes keynote sessions, presentations across 3 tracks (Big Data Case Studies, Big Data 101 and Emerging Trends in Big Data), technology workshops, panel discussions, facilitated networking, gala receptions and exhibits. Thus, this conference is a must-attend for all business managers, decision-makers, IT managers and technologists, and analytics professionals. For program details, please refer: ,
,
, will be delivering a keynote on ""Putting Big Data to Work"". In another keynote, Michael Svilar, Managing Director, Advanced Analytics, Accenture will talk about ""Moving from Big Data to Big Outcomes on the Journey to ROI"". The most updated list of all speakers is available at: ,
,
Besides, the conference is also offering the Certified Analytics Professional (CAP) exam, which will be offered the day before the conference begins -- Saturday, June 21, 9:00 a.m. ?€? 12:00 p.m. Attendees of this conference qualify for a discounted, bundled rate of Conference + Certification. To learn more about CAP, please visit: ,
,
The organizing committee co-chairs are Margery H. Connor, CAP, Chevron Corp. and Diego Klabjan, CAP, Northwestern University. The leading sponsors include Chevron and SAS. Corporate & University Sponsors are: Booz Allen Hamilton, University of San Francisco, MIT Sloan Management Review, and Predictive Analytics World.
,
, However, they might get sold out soon. The different registration options are available at: ,
,
Conference fact sheet: ,
,
Conference website: ,
,
, is the largest association in the world for analytics professionals. We bring considerable reach, expertise and thought leadership to this topic. INFORMS is non-profit and neutral. We are only interested in disseminating the best information that will help you from the best speakers available.
,
To learn more: ,

,
,
,  "
"
Most popular 
, tweets for May 28-29 were
,
Thomson Reuters: Data Scientist ,
,
,
SAS University Edition offers free #SAS software for higher education, supports teaching and learning statistics ,
,
,
The ""Data Nation"": Israel surprisingly large imprint on #BigData: from major labs to many startups #BigDataCo ,
,
,  "
"
        By Vivek Patil, May 29, 2014,
,
This is an extension of my recent , on Modern Languages Enrollments in the US. Using data from 
, in institutions of US higher education between 1983 and 2009, I found that enrollments in Indian languages were low, compared to enrollments in 10 other languages, besides English. These 10 languages were French, German, Italian, Japanese, Spanish, Arabic, Chinese, Korean, Portuguese, and Russian. In this extension, we use data from 22 survey years since 1958, , for which the modern languages enrollment database provides data, to study the pattern and number of students enrolling in these 11 languages.,
,
Why is a knowledge of different languages important and what is the general perception of current trends? Kathleen Stein-Smith noted in 2013 in the , that there was a foreign language deficit in the US. A similar sentiment was echoed by the ,. Both noted that knowing languages could help one understand foreign cultures in better ways and could provide advantages in international relations and international business.,
,
,
,
Enrollment data for US higher education institutions were collected for 22 survey years between 1958 and 2009 for 11 different languages. These include 1958-1961, 1963, 1965, 1968-1972, 1974, 1977, 1980, 1983, 1986, 1990, 1995, 1998, 2002, 2006, and 2009. For determining enrollment figures for Indian languages, a sum of enrollments for Bengali, Gujarati, Hindi, Hindi-Urdu, Kannada, Malayalam, Marathi, Punjabi, Tamil, Telugu, and Urdu was taken. This was consistent with the procedure 
,. For more information on the enrollment data, assumptions in certain surveys and limitations associated with specific surveys and years, please see , and ,. 
,
,
,
,
,
,
Besides French, German, and Russian, interest in other languages studied in this article is on a rise. Total enrollments in this set of 11 languages are also rising. Please note that the population of the US jumped a little more than 1.75 times between 1958 and 2009. [It was approximately , and about ,]
Enrollments between 1958-2009
,
The , mentioned that Spanish, German and French (in that order) were the three most popular languages. The graph shown below provides loess smoothed enrollment patterns. The graph indicates that Spanish, French, and German were the top three languages (among the languages being considered for this article) from the very beginning of the MLA database in 1958. In fact, among the 11 languages being studied, German has been a perennial third, whereas French was ahead of Spanish until 1969, when the latter took over as the number one language of study. Could it be that the growth in total enrollments is being driven primarily by the growth in enrollments in Spanish?,
,
,
,
,
,
The interactive chart shown below begins with a comparison of the total enrollments in 11 languages being studied here (upper line) with the total enrollments in 10 languages, after dropping Spanish (lower line). , The chart suggests that after dropping Spanish, total enrollments in the remaining 10 languages appears to be increasing at a very sluggish pace. In fact, total enrollment numbers (without Spanish) are still lower than they were in 1965 or 1968, when the overall US population was substantially lower than it is now.,
,
,
,
Clicking on a dot/circle of the legend can make that language appear/disappear from the plot. This can help with the comparison of enrollments between languages. It should be possible to plot patterns for all languages in this chart, if one so desired.,
,
,
,
,
,
,
,
,
,
Graphs and Charts were generated using ggplot2 and rCharts in the R environment. Data and the relevant code for replicating this analysis
and the ,
,
Bio: , is an Associate Professor of Marketing at Gonzaga University, where he teaches courses in marketing, marketing research, multivariate statistics and business analytics.,
,
,
,  "
"
,
,
, is a Data Scientist at George Mason University. He has been at Mason since 2003, where he does research, teaches ,and advises students in the graduate and undergraduate Data Science, Informatics, and Computational Science programs. He helped to create the Data Science B.S. degree program that began in 2007. Previously, he spent nearly 20 years in positions supporting NASA projects, including an assignment as NASA's Data Archive Project Scientist for the Hubble Space Telescope, and as Project Manager in NASA's Space Science Data Operations Office. He has extensive experience in big data and data science, including expertise in scientific data mining and data systems., He has published over 200 articles and given over 200 invited talks at conferences and universities worldwide. He serves on several national and international advisory boards and journal editorial boards related to big data. In these roles, he focuses on achieving big discoveries from big data, and he promotes the use of information and data-centric experiences with big data in the STEM education pipeline at all levels. He believes in data literacy for all.?€??€? 
,
Here is my interview with him:
,
,
,
, The reality here is that ""partial, incomplete data"" has  been the norm for all of human history, and certainly for the history of science.  Consequently, traditional methods of modeling and simulation are useful here -- where you build a model that represents whatever it is you are studying.  The model includes parameters for things that you don't know and it includes constraints from the things you do know (i.e., from your partial, incomplete data). 
,
, Of course, a model is not perfect -- it is simply a partial representation of reality, from which you hope to make better discoveries and decisions than you could have made otherwise.  As the famous statistician George Box said: ""All models are wrong, but some are useful."" That's precisely the point.  The model is imperfect, but it is still useful. Similarly in the case of partial and incomplete data, our subsequent understanding, models, inferences, and predictions are imperfect, but they are still useful.

,
,
,
, Finding causality is good science, but in many applications it is more important to make a good decision.  For example, in astronomy, scientists discovered in the 1960's that there were energetic bursts of gamma-rays coming from space. We had no idea what the cause was, ,but we discovered that the spatial distribution of these bursts across the sky correlated eerily well with an isotropic model (that is, the bursts were not coming from any preferred direction or location in the sky). Nevertheless, this correlation led to improved astrophysical theories, new technologically powerful scientific instruments, and further observations for several more decades before the cause was ultimately discovered in the mid-1990's.  The cause was found to be from a massive star exploding (and then collapsing into a black hole), which occurs sporadically and randomly throughout the Universe. So, the correlation led to great physical models and fantastic improvements in space astronomy instrumentation, even without understanding (initally) the underlying cause. 
,
Similarly, in online retail stores, businesses discover correlations in customer purchase patterns, thereby enabling and empowering recommender engines to present meaningful product recommendations to their customers.  These engines are not only good at recommendations, but they are also very good at generating revenue for the business.  There is no hint in these models as to what causes a customer to have a preference for product A and also for product Z, but if the historical purchase data reveal that the products are correlated, then it is simply smart business sense for you to act on that correlation, even without the causal understanding. ,
,
,
,
, Astronomy data collections have definitely been growing ""astronomically"" for many years, but the biggest and the best is yet to come, including the LSST (Large Synoptic Survey Telescope) project that will begin construction in the summer of 2014 and the future SKA (Square Kilometer Array).  These petascale big data projects promise amazing discoveries.  From the terascale projects of the past couple of decades, there have been many important discoveries.  For example, from NASA's Kepler mission, we are discovering hundreds of new planets around distant stars via the slight variations in the time series of the stars' light emissions being tracked over several years for more than 100,000 stars.  In the first large surveys of galaxies' distances in the 1980's, we found large voids in the Universe, regions that are almost devoid of massive galaxies, leading to a better understanding of the massive large-scale structure of the Universe -- it has essentially the same structure as soap bubbles: the majority of massive galaxies and clusters of galaxies reside on the surfaces of enormous bubble-like regions of space, with almost nothing within the interior regions of the bubble-like structure.  We also found extremely rare ultra-luminous galaxies emitting enormous infrared radiation, caused by super-starbursts inside these galaxies -- these were discovered after studying the properties of millions of galaxies. 
,
More recently, we used a citizen science project called Galaxy Zoo to empower the general public to help us look at and characterize images of nearly a million galaxies (which was far more than any individual scientist or team of scientists , could look at), and our citizen volunteers found totally new classes of astronomical objects (e.g., light echos from dead quasars, and also little green galaxies, which the volunteers dubbed ""green peas""). In all of these cases, it was the study of very large databases of objects that led to the discovery of the surprising, interesting, new things.  For me, that is the most exciting and exhilarating aspect of big data science -- discovery of the new, unexpected thing.  That ""novelty discovery"" approach works in astronomy, but also in any big data domain! Finding the unknown unknowns is what motivates my work and it is my goal every day as a data scientist.
,
The second and last part of this interview: ,
,
,
,  "
"
By Bala Deshpande, May 2014.,
,
The number of computing devices has far exceeded the number of humans on this planet ?€? this milestone was in fact achieved back in 2008. Today there are , in the world. Which devices in common use today use the most processors and computing power? The answer ?€? the automobile - may surprise many people. The average car has anywhere between 50-60 microprocessors which handle everything from entertainment systems and climate control to performing advanced engine diagnostics and airbags during crash safety.,
,
,
With mobile becoming an integral part of our daily lives, the information processed by these devices becomes useful not only to the manufacturers, but also to consumers. Data from a large and growing number of embedded processors can be used in a variety of ways: at the driver level to select a more personalized, convenient and efficient mode of driving, at a macroscopic (e.g. traffic) level, information collected in this way then can be consolidated to find solutions to problems like traffic jams, helping to improve the flow of traffic should city officials want to leverage this data as well. From a manufacturing perspective, close attention to fine grain data can significantly improve vehicle quality, by ,. The U.S. government has dedicated significant research to this effort and , on this topic at the upcoming PAW-Manufacturing conference.,
,
,
This story is repeated across other non-consumer applications, particularly on the shop floor, from small contract manufacturers to the largest original equipment manufacturers of the industry. For example, , to carefully account for the activities of their assembly line workers. How much time is spent by worker #1 on project #2 which requires inspection versus project #1 which requires assembly? The data generated by the sensors is captured and processed in near real time to predict machine usage, unexpected delays and downtimes and improve overall process efficiency.,
,
Once again, the chief output of this connected network is high quality data in huge volumes, which needs to be converted into useful data products. , ?€?We are moving away from just creating and accessing data to more intimate, automated, knowledgeable, actionable insights about everything. This allows us to improve machine performance and the efficiency of the systems and networks that link them.  As the devices and environments around us become aware and intelligent, they will play a key role in making decisions for us or nudging us toward specific actions?€?.,
,
Historically manufacturing has been no stranger to either data or analytics. Statistical quality control, operations research and related technologies such as six sigma, all originated in manufacturing and could be considered the predecessors of today?€?s predictive analytics. However much has changed since these technologies became standards. Our ability to collect far more data and in real time has increased significantly. ,, writes that this upcoming data avalanche from the connected internet of things in a manufacturing plant can , by being able to ?€??€?do things that were impossible in the past [?€?] such as identifying and removing defective products while they are still on the conveyor belt. As a result, we had to rely on approximations, such as mean time to failure and scheduled maintenance that inflated costs and often missed outlier conditions that might lead to premature failure?€?.,
,
The goal of PAW-Mfg is to help tie the advances in big data, analytics and visualization to persistent issues to drive orders of magnitude improvements in manufacturing performance and to prepare an ?€?elder statesman?€? of industry verticals for a connected and data driven future.,
,
,
, Bala Deshpande is the co-chair of the inaugural Predictive Analytics World ?€? Manufacturing conference. He is the founder and principal at SimaFore, a custom analytics developer and consulting company. He has nearly two decades of experience in data analytics and related fields. He began his career as an engineering consultant, following which he spent nearly a decade analyzing data from automobile crash tests and helping to build safer cars at Ford Motor Company. He holds a PhD in Bioengineering from Carnegie Mellon and an MBA from Ross School of Business (Michigan).,
,
,
,  "
"
,
,
Across many industries, large and small organizations are using analytics and data science to offer greater insight and customer service. The gaming industry is almost well placed in that - particularly with online and social gaming - the companies already keep a vast amount of data on gamers. The challenge remains to make use of this data in a way that offers true value for money whilst enhancing the user experience.
,

,I had recently attended the 
, (May 1-2, 2014) at San Francisco, CA, organized by the Innovation Enterprise. The summit brought together acclaimed speakers and attendees for deep insight into how the gaming industry uses analytics and data science. For speakers, it had a line-up consisting of 20+ leading executives working in Analytics, Data Science & Business Intelligence in gaming.??Through real-life business case studies and deep-dive discussions, the summit offered solutions and insight from the leaders in the Gaming space.

,

To help its readers succeed in their Analytics pursuits, KDnuggets provides concise summaries from selected talks at the summit. These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key talks for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.

,
,

,
Here are highlights from selected talks on day 2 (Fri, May 2):


,shared the challenges and gratification he got through his journey until now at Ubisoft., At first he shared some facts about gaming industry:
,
, ?? ,
He emphasized that today?€?s game enterprise needs: acquisition, retention, monetization, performance, real-time actions, predictability and delivery of an enhanced 360-degree consumer experience. Contrasting with traditional BI, he characterized Big Data Analytics as innovative, iterative, real-time, predictive and interactive process.

,

While listing the challenges, he stressed upon bridging business and production data silos, data governance, master data management (MDM), tactical vs. strategic goals, and ROI(to account for sustainable investment for growth). In his assessment, he observed a lot of opportunities including highly scalable workforce, highly scalable technologies, 360-degree consumer insights, and enterprise data programs. He concluded his talk mentioning that the biggest goal of predictive analytics at gaming enterprises should be to understand the player and deliver actionable insights.

,

,talked about ?€?Navigating the Blue Ocean Opportunity Using Data Science & Analytics?€?. He mentioned that the gaming industry has witnessed rapid growth over the last decade, due to increasing number of devices, improving user experience and advanced capabilities of gaming devices. The market would be growing to $70B+ this year, driven by emerging markets and platforms. The business models have been evolving as per the following trends leading to a new way of how gaming firms think about access and usage of their IP:
,
,

EA is transforming its core competencies in Big Data & Predictive Analytics. EA Digital Platform is comprised of four components called: Descriptive, Diagnostic, Predictive and Prescriptive Analytics. In the past year, EA had a dominating market share of 35% due to its corporate digital transition continuing at full-speed & early leadership in next generation Analytics.?? Realizing true business value from Big Data requires transformation in both analytics skillset and business culture.

,

For EA, Analytics is an intersection of Art (Art of Game Design) and Science (Science of Product Management). The Analytics-based Games-as-a-Service business model is transforming the development and business culture at EA. In the end, he recommended:
,
, ?? ,

,delivered an interesting talk on ?€?Mobile User Acquisition with Big Data?€?.???? He defined Mobile User Acquisition (UA) Analytics System as an information system that uses mobile?€?s data to enable efficient allocation of marketing capital. He discussed how to build an automated ROI focused marketing analytics system using: Back End infrastructure that is easy to scale, 3rd party attribution and integration methods, UA analytics solutions, Game Data real time funnels necessary to make marketing decision and a simplified version of a nLTV(Life Time Value) calculation. ??He explained the system that is currently employed by SEGA and is used for multiple existing titles including highly popular titles like Sonic Dash.
,
Mobile User Acquisition systems have seen considerable changes within the last two years, such as:
,
, ?? ,
Marketing Analytics System helps make decisions for capital allocation in order to turn marketing into a profit center. Giving an overview of Mobile UA Ecosystem, he explained the following key steps in the Analytics process: Ingest & Normalize, Store & Process, Analyze, Visualize, and Optimize.
,
The key marketing metrics to observe during data analysis are:
,
, ?? ,
Spiros ended the talk saying: ?€?More Data will lead to More Automation, which in turn will lead to More Efficient Market?€?.
,
,
,  "
"
,
,
, is a Data Scientist at George Mason University. He has been at Mason since 2003, where he does research, teaches ,and advises students in the graduate and undergraduate Data Science, Informatics, and Computational Science programs. He helped to create the Data Science B.S. degree program that began in 2007. Previously, he spent nearly 20 years in positions supporting NASA projects, including an assignment as NASA's Data Archive Project Scientist for the Hubble Space Telescope, and as Project Manager in NASA's Space Science Data Operations Office. He has extensive experience in big data and data science, including expertise in scientific data mining and data systems., He has published over 200 articles and given over 200 invited talks at conferences and universities worldwide. He serves on several national and international advisory boards and journal editorial boards related to big data. In these roles, he focuses on achieving big discoveries from big data, and he promotes the use of information and data-centric experiences with big data in the STEM education pipeline at all levels. He believes in data literacy for all.?€??€? 
,
First part: ,
,
Here is second and final part of my interview with him:
,
,
,
, In the spirit of full disclosure, let me begin by announcing that SYNTASA's CEO Jay Marwaha has recently appointed me as the first member of the SYNTASA Advisory Board.  I see their offering as a unique Marketing Automation solution that applies the best aspects of data science:  integrating a variety of big data sources from multiple channels (including weather and location-based data, along with customer data), advanced machine learning algorithms, customer modeling, leading-edge computational technologies, predictive and prescriptive decision analytics (governed by business rules and business goals), autonomous intelligent operations, and generally a full scientific approach to the problem.  The real value proposition that they bring to the field of digital marketing is this: Marketing Analytics-as-a-Service (MAaaS). SYNTASA addresses three main customer engagement pain points for marketers (customer awareness, acquisition, and retention) through the application of machine learning, decision science, and the science of optimization on an open scalable platform.

,
,
, I have a bias toward the science part of Data Science, but that is not the only direction one can go.  For the data scientist, I encourage courses in statistics, machine learning, applied math (including linear algebra), databases and data 
,structures, data and information visualization, scientific modeling and simulation, programming (Python, R, or Matlab, at a minimum), and even some Physics (to learn and sharpen problem-solving skills).  For the big data analytics profession, focus more on the algorithms (data mining, statistics, and machine learning), programming skills, and computing technologies (such as Hadoop).  For the business or marketing analytics profession, include some of the above things while also learning the key concepts of business, marketing, finance, organizational management, social and behavioral science, leadership, entrepreneurship. , In general, of course, the concentrations in your own curriculum will vary naturally as your domain changes:  health informatics, data-driven journalism, digital humanities, learning analytics, scientific data science (X-informatics, where X might be bio, geo, astro, or any other), or cyber-security analytics.  But the core data science components are essential (databases, statistics, machine learning, and programming).

,
,
,
, Yes, I totally agree.  I don't mean it personally, but I mean it corporately.  That is, because of the attractiveness of the profession, the enormous job opportunities, the super-interesting problems that you get to solve?€?, the fascinating people you get to work with, the really interesting insights and discoveries that you can make, and using the coolest scientific algorithms and computing technologies on the planet, the field of data science is therefore attracting some of the best and the brightest people to the profession.  That is a very good thing!  And so I would say that I emphatically agree with the characterization of the Data Scientist as the sexiest job of 21st century! , , If you perhaps do not have strengths in all of those areas, then identify where your weaknesses might be, nurture those talents, grow them, feed them -- then find data science activities (do consulting, apply for internships, join a team), take classes, solve some Kaggle.com problems, and push yourself.  The rewards will be great, but the fun and satisfaction will be even greater.  
,
,
,
,My wife would say that I am always working.  Of course, she knows best.  But, apart from my ""day job"" doing research, teaching, and advising students at the university, I spend a lot of time as an independent big data science consultant with businesses, plus I spend too much of my spare time on Twitter -- spreading the good news, the discoveries, the great insights, and cool stuff from the world of big data and data science, 140 characters at a time. Beyond that, we have 3 new grand-babies in the past 16 months.  That is an awesome change of life experience.  They are a fantastic joy and I am happy to be busy with them at any time.
,
,
,  "



"
,



,The US Open Data Action Plan, which was originally due last November, was finally released on May 9, 2014, the one year anniversary of President Barack Obama?€?s executive order to open up more government data.

,

The Plan is an integral part of the Open Data Charter endorsed by the G8 (now G7, excluding Russia) nations last year. The Open Data Charter had set out five strategic principles:
,
, ?? ,
,The 2013 Open Data Census, run by the ,, had identified US as the world leader, followed by UK, with regards to open data and transparency. The census measured the openness of data in ten key areas including those essential for transparency and accountability (such as election results), and those vital for providing critical services to citizens (such as transport timetables).
,
,
, ?? ,
,The US Open Data Action Plan outlines government?€?s priorities, action plan and current progress in ensuring that government data across certain selected categories is more available and useable to the public. It also identifies four open data commitments:
,
,
To help with the fourth commitment, US Government will recruit a third round of Presidential Innovation Fellows to work on a series of Data Innovation projects. These projects include both open data initiatives and ?€?My Data?€? initiatives. ?€?My Data?€? initiatives, such as the Blue Button (for healthcare) and Green Button (for energy usage), are designed to give Americans secure electronic access to their own personal data.
,
, ?€?Freely available data from the U.S. Government is an important national resource, serving as fuel for entrepreneurship, innovation, scientific discovery, and??,. Making information about government operations more readily available and useful is also core to the promise of a more efficient and transparent government.?€?
,

- U.S. chief information officer Steven VanRoekel and U.S. chief technology officer Todd Park??(on WhiteHouse.gov blog)

,

Here is a list of selected data that has been released so far:
,
, ?? ,

,These datasets are being made available on federal agency websites as well as on the federal open data repository, Data.gov, which celebrated its fifth anniversary a few weeks ago. More than 100,000 datasets are available for download on Data.gov (,). Moreover, some of the datasets are also available in an interactive form where you can perform basic analytical (such as filter, sort) and visualization (such as map, chart) operations online (,).
,


Besides releasing data, the White House has developed , ?€? a collection of code, tools, and case studies ?€? to help agencies adopt the Open Data Policy and unlock the potential of government data. Project Open Data is planned to evolve over time as a community resource to facilitate broader adoption of open data practices in government.??Anyone can use these tools and contribute to them.


,
Despite push from the President, it still remains unclear how readily the open data by default standard will be implemented by federal agencies. The delays have already come up due to the government shut down and it seems most likely, that the release of all non-classified data online will be a gradual process. Of course, the concerns such as privacy and security cannot be ignored and thus, while this data has been released in public interest, government agencies and regulatory bodies will have to stay alert on any potential misuse.

,

In recent times, we have witnessed a plethora of Analytics startups with many more launching every month. The Open Data provides a great opportunity for these startups to prove themselves while working on real-life data and developing innovative solutions.


,
Though this is just the first step, if followed with commitment this journey can undoubtedly lead to greater government transparency and greater public participation (and trust), while generating new business opportunities through innovation.

,

The complete PDF of the US Open Data Action Plan can be accessed at: ,
,
,
,  "





"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
Latest ,, (Jun 10, 2014) ,:
,
,??,
Also
, (3) |
, (14) |
, (6) |
, (3) |
, (1) |
, (9) |
, (6) |
, (1) |
, (6) |
, (13) |
,
,
Confirmation bias is one of the easiest ways for a scientist to fool himself. Richard Feynman, 1974.  "
"
, is an open source modern C++ library implementing many machine learning algorithms and supporting functionality like threading and networking.,
,
,
,
,
, implements numerous machine learning algorithms:,
,
DLib also features utility functionality including,
,
DLib includes extensive unit testing coverage and examples using the library. Every class and function in the library is documented. This documentation can be found on the library's ,. DLib provides a good framework for developing machine learning applications in C++.,
,
DLib is much like , in that it provides a generic high-performance machine learning toolkit with many different algorithms, but DLib is more recently updated and has more examples. DLib also contains much more supporting functionality.,
,
What makes DLib unique is that it is designed for both research use and creating machine learning applications in C++.,
,
The official paper describing the machine learning part of the toolkit can be found ,.,
,
,
,
DLib works on Windows, Linux, and OS X.,
,
DLib is licensed under the Boost Software License.,
,
,
,  "
"
Most popular 
, tweets for Jun 9-10 were
,
Numeric matrix manipulation: cheat sheet for MATLAB, Python NumPy, R, and Julia #DataScience ,
,
The First Law of Data Science: Do Umbrellas Cause Rain? 
,
,
Numeric matrix manipulation: cheat sheet for MATLAB, Python NumPy, R, and Julia #DataScience ,
,
,  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
San Francisco, CA, Summer 2014 Only,
,
Zipfian Academy is offering a six-week data fellowship program for those with significant data science skills to fill in knowledge gaps and connect fellows with their hiring partners.,
,
The curriculum includes:,
,
This program is intended to serve as a bridge between high-potential individuals and companies looking for data science talent.,
,
The fellowship includes two weeks of full-time professional instruction and four weeks of projects and interviews. On Hiring Day, fellows will interview with 20+ Bay-area companies.,
,
The program is free and runs from June 30th to August 8th. Apply online ,.,
,
,
,  "
"
,
,
In the past few years, we have witnessed a significant evolution of Big Data maturity across various industries. There has been immense entrepreneurial activity (leading to an increasing number of start-ups in this space) as well as strategic initiatives in big companies, driven by the consistently increasing demand to leverage data for taking better decisions. Most of the recent assessments have highlighted the current challenges as:
,
, ?? ,
,These challenges along with the new opportunities, success stories and case studies will be the focus of , at San Jose, CA on June 22-24, 2014. The conference is a great opportunity for learning as well as networking with the leaders from industry and academia through a ,seamless exchange of information, ideas and perspectives.
,
The impressive line-up of speakers, which includes Big Data leaders from various industries (including Bill Franks, Chief Analytics Officer, Teradata), will share the best practices through real-world case studies and tutorials on a wide variety of topics. The conference program includes keynote sessions, presentations across 3 tracks (Big Data Case Studies, Big Data 101 and Emerging Trends in Big Data), technology workshops, panel discussions, facilitated networking, gala receptions and exhibits. Thus, this conference is a must-attend for all business managers, decision-makers, IT managers and technologists, and analytics professionals.
,

, ,
,
,??,
,
The organizing committee co-chairs are ,, Professor & Founding Director, Master of Science in Analytics, Northwestern University, and ,, Senior Operations Researcher, Advanced Analytics Chevron Corp.
,
Here is my interview with them regarding current Big Data opportunities and challenges, and the conference. 
,

,
,
,: For decades relational databases and SQL have been ruling the world. Then came the proliferation of the web and social media. Progressive corporations have realized that SQL cannot handle these new data sources. As a result the technology behind big data has been developed. My definition of big data is that every data set that cannot be analyzed by using relational database technologies is big data. This includes large and quick data such as tweets that simply cannot be handled by relational databases, analyses that are possible with SQL but it would take too much computing time, and problems that would not be economical on relational databases due to cost constraints.
,
Big data is definitely past early innovators and in the prime of early adopters. I want to point out that there are still several companies that use big data technologies for the sake of using it and having bragging rights. If we include such cases, then we are deeply into the early adoption phase. On the other hand, there are not many big data deployments with a positive ROI, and thus from this perspective we are barely beyond early innovators.
,
,
,
,: Operations research has always had impact in data science, in particular the fields of optimization, statistics, and data mining. On the technical side, big data spawned the search for different algorithms able to scale and tailored for the new technologies. Operations research has also always been very strong in analyzing processes and articulating business value. While manufacturing and related industries are often listed as original industries for such studies, big data created process changes in the IT and other organizational units and thus operations research practitioners and scholars have started adapting lessons learned to the data science and big data space.
,
,
,
,
,
,: In many areas the line between operations research, and predictive analytics and machine learning is blurry. For example, regression has been studied by all these communities, and also statistics. Other areas such as classification falls more under machine learning than operations research. On the other hand, optimization is an area where operations research has historically been very strong. For decades the primary applications of sophisticated optimization algorithms have been select military, transportation, and telecommunication problems. In recent years the focus has been equally important to ?€?porting?€? these state-of-the-art algorithms to machine learning related problems.
,
, ,
,

,: The most pressing issue is definitely how to identify ROI in big data projects. This is an extremely challenging problem since big data is mostly used for data discovery which by nature implies that the business value is hovering in clouds (true clouds and not computing clouds). We will also highlight key emerging technologies to educate decision makers about what to expect in the future and plan accordingly.
,
We hope that the attendees after attending the meeting will have a much better view about best practices, what problems are best for big data, and what is in the pipeline when it comes to technologies.
,
,
,
,: The technology workshops are 90 minutes long and provide our Exhibitors the opportunity to do a deeper dive into their technology solutions. The workshops also provide the attendees a chance to have a technical discussion with the vendors and hear the questions and perspectives of other attendees.?? Some workshops are hands-on.?? Leading analytics companies will be showcasing their software solutions and approaches.?? There is no additional charge for these workshops.
,
,
,  "
"
By Miles Greenford, Jun 11, 2014.
,
,
I am currently working up a PhD research proposal which involves the analysis of data in uploaded video format. I expect the complete video to be too much to analyse so am opting to reduce it to speech. I am looking for any university department currently working with , developing text analysis software, who would be interested in partnership working. The university department needs to be in the EU as my proposal involves ,.
,
I have a health professional background and want to explore a particular method for its potential to improve the health over the short, medium and long term, of those who use it. The data capture would be from the video logs of critical incidents from those who used the method (and a second group who have used a 'decoy', as a comparator). I have a particular interest in the 'complex systems theory' and would be extremely grateful to hear back from any department developing UIMA text analysis software using any of the complex systems theory approaches. I can be contacted on miles_greenford at hotmail.com.  "

"
        ,
By Gregory Piatetsky,  
,, Jun 11, 2014.
,
,, which stands for The Institute for Operations Research and the Management Sciences, is the largest professional society in the field of operations research (O.R.), management science, and analytics. Recently it has been moving strongly into the field of Analytics, and wants to 
,.
,
With over 11,000 members, INFORMS has a significant audience, and so I was very pleased that INFORMS has featured my profile of KDnuggets on its homepage 
,, in OR/Analytics at Work Blog (orange box).  The profile will be on the homepage from June 9 to 23, 2014,
and the direct link is
,.
,
Here is the profile, with an overview of KDnuggets present, history, and highlights.
,
,
,
, is a popular site covering Business Analytics, Big Data, Data Mining, and Data Science.
,KDnuggets is published and edited by 
,,  Ph.D.,
a leading expert in Data Mining and Data Science, Ph.D., a co-founder of KDD conferences, and co-founder and past chair of ACM SIGKDD association for Knowledge Discovery and Data Mining.
,
In 2013 KDnuggets had over 770,000 unique visitors, according to Google Analytics, and 279,000 unique visitors in Q1 2014 (over 100K/month).  KDnuggets was recently ranked by Alexa in the top 70K sites globally, top 35K sites in US.
,
The chart below shows growth of KDnuggets audience, which more than tripled from 2010 to 2014.
,
,
,
,
KDnuggets
, is emailed 2-3 times a month to all the subscribers.
,
KDnuggets also has a
,,
a ,, 
and a very popular
, Twitter account.
,
KDnuggets News, published 
since 1993, is a unique mirror of the history and present state of predictive analytics, data mining, and data science field.
,
All KDnuggets News issues from the very first one are  available online at
,
,
The most recent posts are at 
,
,
,
KDnuggets was started by
,  in 1993 as a way to
, attending the
, (organized by Gregory and others at AAAI-93 in Washington, DC).
,
With the appearance of World Wide Web and Mosaic, Gregory Piatetsky-Shapiro and
Chris Matheus have created a website called
, hosted at GTE Labs (info.gte.com/~kdd site no longer active), which was at the time
the second site in the world covering data mining and knowledge discovery field.
,
While Gregory was at GTE Labs, the newsletter was published as
""Knowledge Discovery Nuggets"".  It served as an unofficial publication of
, (started by Gregory Piatetsky-Shapiro in 1989),
and helped them to grow and become conferences.
,
,
When Gregory left GTE Labs in 1997, he created KDnuggets site, 
with ""KDnuggets"" standing for ,nowledge ,iscovery ,,
(conveying the mission of covering the field with short, concise ""nuggets"" of useful information),
and moved the Knowledge Discovery Mine to 
,
,
Gregory was working from 1997 to 2000 as a Chief Scientist for Knowledge Stream Partners (KSP), which were providing data mining and customer analytics consulting to major banks and financial institutions, and editing KDnuggets late at night after work.
,
When Y2K crisis caused most of KSP to freeze any future projects (including any data mining projects), KSP lost most of its revenue and and cut salaries and work hours for all employees, including Gregory.  The extra time and reduced salary motivated Gregory to put the first ads to support his work on KDnuggets.
,
KSP ,  in April 2000, and Gregory worked for Xchange for about a year as a Chief Scientist, Analytics.  During that time Xchange market value reached $1B (briefly), and then fell almost to zero.  
Gregory left Xchange in May 2001, during the collapse of the dot com bubble, and has been publishing KDnuggets as an ad-supported site since then.
,
,
,
KDnuggets website includes a directory of main areas
of data mining and data science, including
,
,??,
,
A popular feature of KDnuggets is 
,, especially the annual software poll asking ""Which Analytics, Big Data, Data mining, Data Science software you used in the past 12 months?"".  
,
Here are the results of the latest
,.
,
Another popular feature is
,, which take a humorous look at Data Mining and Data Science.
,
Here is KDnuggets Feb 2014 Valentine's Day cartoon:
,
,:
,
,
Data Scientist: ""Sweetheart, my neural net predicts that you and I are 98.9% compatible. Will you be my Valentine?""
,
,
,
KDnuggets also keeps a large directory of current events on Business Analytics, Big Data, Data Mining, and Data Science, including:
,
,??,
,
,??,
KDnuggets also offers a free
, - the teaching modules for a one-semester introductory course on Data Mining, suitable for advanced undergraduates or first-year graduate students.
The course, created by Gregory Piatetsky-Shapiro and Prof. Gary Parker,
was funded by a grant from W. M. Keck Foundation and Howard Hughes Medical Institute,
and is freely available on the web.
,
In November 2009, KDnuggets added blog-style, daily publication and switched to a custom PHP-style platform that Gregory wrote.  With the continued growth in visitors and popularity, KDnuggets converted most directory and news pages from 2013 forward to WordPress and switched to publishing using WordPress platform in Jan 2014.
,
KDnuggets team, besides Gregory, now includes Anmol Rajpurohit, Grant Marshall, and Ran Bi.  
,
In Feb 2009,
Gregory also added a
, Twitter, and later also a
,, and a 
,.
,
KDnuggets LinkedIn group is among the
,.
,
,
KDnuggets.com and 
, Twitter have received wide industry
,, including:
,
,??,
,
Whether the #BigData buzzword will stay or be replaced, the amount of data in the world will only grow, and so will the demand for people who can understand this data.  That is the audience KDnuggets addresses, so the future looks bright.
,
,
,
,
,
 ,  "






"
,
,
With the Big Data Technology and Services market set to grow to $16.9 Billion in 2015 organizations are becoming more aware of the need to begin large scale investments in order to successfully adopt Big Data Strategies. With the focus within organizations on integrating more data sources and the ability to analyze larger volumes of data, the speed by which they can obtain these answers is vital. The theories underpinning current developments in Big Data don't necessarily pan out in the real world, particularly when examined in a global context.
,
, (May 14-15, 2014) was recently organized in London by the Innovation Enterprise. The summit helped businesses understand & utilize data-driven strategies and discover what disciplines will change because of the advent of data.
,

, These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key talks for a deeper understanding and people who could not attend the summit.
,
Here are highlights:
,
, gave a talk on ?€?Transforming Data Architecture Complexity at Sears?€?. He started his talk citing the following from a recent Wikibon survey:
,
Enterprise practioners believe the potential value of Big Data is significant. However, many are struggling to derive maximum value from their Big Data investments (45% have realized only partial value).
,
Wikibon reported that there are three major reasons behind this:
,
, ?? ,
Focusing on the third reason, he stated that when considering implementing Hadoop into enterprise data architecture, it is very important to understand what Hadoop can and cannot do, specifically for your business. When implemented carefully, the Hadoop ecosystem helps in making smart business decisions quickly through faster processing on larder data sets for deep analytics.
,
Traditional data warehouses pose a variety of challenges such as high ETL (Extract-Transform-Load) costs, data latency and redundancy, batch window limits, etc. Ankur described how Sears uses Hadoop as a data hub to minimize data architecture complexity ?€? resulting in a reduction of time to insight by 30-70% - and discover Big Data ?€?quick wins?€? such as ETL modernization and mainframe MIPS reduction.
,
Based on his experience, he identified the following as key to achieve Big Data success:
,
, ?? ,
He explained how Social Analytics with Hadoop can be leveraged for understanding product perception and brand sentiment analysis. By implementing Hadoop and Cassandra into a traditional data warehousing environment, Sears is able to provide more accurate and real-time inventory, pricing, sales and return data as well as predict ideal floor plans.
,
In the end, he mentioned that Hadoop can help answer questions that were difficult or cost prohibitive to answer preciously. However, it needs a clear strategy and long-term plan with proper attention to data governance.
,
,delivered an insightful talk on ?€?With Big Data comes Big Responsibility?€?. He emphasized that while we all are excited about the innovation opportunities through leveraging Big Data, we also need to be cognizant of the underlying Privacy concerns in order to stay compliant with privacy and data protection laws across the world.
,
Users are losing control of their data. Even worse, they are not even aware about how much of data about them is being recorded and used for what all purposes. Thus, as we make progress using Big Data technologies we need to understand the importance of getting the balance right between the progress of mankind and the thread to some of the very basic rights of all humans.
,
Getting consent from user before using every single piece of user?€?s information is cost-prohibitive and thus, we need more advanced technological solutions to protect privacy. He explained the privacy implications of a Big Data world, the challenges presented by the current legal framework and what compliance with the law will involve in the future. Nations are still debating how to future proof Big Data policy making? Should we regulate technology? Or rather regulate user behavior?
,
He recommended that we need to promote transparent use of data along with having privacy and security as default priorities in all business plans.
,
,??delivered a very interesting talk on how to explore data with a touch of fun. He shared his experience of turning raw consumer data into compelling lively stories, enriched with powerful visualizations by designing & developing easy-to-use, appealing yet insightful online tools to enable end-users to instantly understand the data in the right context, enabling them to instantly spot patterns, trends and relations in a self-service manner.

,

Data scientists are busy analyzing the huge volume of structured and unstructured data ?€? referred to as Big Data ?€? to discover patterns and find solutions to existing problems. But, to propel the innovation engine of Big Data we need to make it accessible to the end consumers and make it highly intuitive to explore, understand and use. Data Visualization helps the end consumers participate in data exploration while staying away from technical details. In data visualization functionality and entertainment are deeply entangled. Carlos explained how the ?€?fun layer?€? in data exploration helps the user to get familiar with and truly enjoy the journey through data. He compared Form vs Beauty vs Fun to describe the science of data visualization.

,

We need intuitive visualizations that will enable users to see patterns and connections that matter. In conclusion, he stated that if we make data analysis fun for end consumers, they will enjoy the process and will be able to make their own discoveries.

,

,??delivered a talk on ?€?Big Data to Drive Decision-Making at Ticketmaster?€?.??,??She started her talk citing the following trends:
,
, ?? ,
Sophie quoted following words from Virginia Rometty, CEO, IBM: ?€?Information is going to be the basis of competitive advantage for every company in the world. What you will see with rapid data and social sharing is the death of the average and the era of you. Businesses will be able to truly the individual?€?
,
Ticketmaster has a lot of Big Data business use cases, the most prominent ones being: managing risk & compliance, reducing cost, getting more tickets, and selling more tickets. The biggest challenge is to manage data ownership, consent and governance across multiple markets, multiple platforms and multiple clients.
,
Discussing about data she mentioned that Big Data is massive, messy and muddled. She talked about analysis showing immense need for powerful insights on demand and supercharged segmentation.????Engagement scoring is also an important factor to consider. In action, they perform timely analysis, mass automation, and enable real-time match offers. She concluded the talk by stressing that data projects are the major drivers of change leading to a better world.
,
,
,  "




"
,
,
I?€?m back from the , and experiencing a significant amount of Hadoop overload. But in spite of the exhaustion, this annual conference in Silicon Valley has turned into one of my favorites. In a single venue for a modest 3 days time, I?€?m able to get a good pulse for where the big data industry is headed ?€? very valuable for my position as a practicing data scientist.,
,
,
One of the distinct messages I gleaned from the show was how much the leading Hadoop distributions are banking on , - a resource-management platform responsible for managing compute resources in clusters and using them for scheduling of user applications. YARN is a sub-project of Hadoop at the Apache Software Foundation, first introduced with Hadoop 2.0 last year, that separates the resource management and processing components. YARN was born of a need to enable a broader array of interaction patterns for data stored in HDFS beyond MapReduce. YARN incorporates different applications such as Apache Tez for interactive/batch applications, and Apache Storm for stream-processing.,
,
[Tweet ""Many in the Hadoop ecosystem tout YARN as the next generation compute framework for Apache Hadoop""]. 
,
And for good reason, it provides a generic data processing engine beyond MapReduce and enables the Hadoop compute layer to be a common resource-management platform that can host a multitude of applications. YARN provides a flexible resource sharing model that makes it attractive for many services to co-exist on a single cluster without worrying about resource management, isolation, multi-tenancy issues etc.,
,
Making the optimal use of a YARN cluster requires a number of best-practices considerations to be addressed:,
,
,
,
So far, YARN is faring well in deployments large and small, and stands apart from the batch-processing-only world of Hadoop 1.0. YARN has a single point of failure in the form of its master ?€? the ResourceManager (RM). The RM keeps track of all the slaves, schedules work in the cluster, and handles all client interactions. Unanticipated events like node crashes and planned events like upgrades may reduce the availability of this central service and YARN itself. At the Summit, Cloudera and Hortonworks teamed up to describe their recent work on Highly Available Resource Manager (HARM) in YARN.,
,
,
,
,
,
Another important component that was prevalent at the Summit was Apache Spark, designed to let users build unified data analytic pipelines that combine diverse processing types. Databricks demoed Spark by building a machine learning pipeline with three stages: consuming JSON data from Hive, training a k-means clustering algorithm, and applying the model to a live stream of Tweets ?€? all to classify raw Tweets in real-time. Typically this kind of pipeline might require a separate processing framework for each stage, but the demo showed how to leverage the versatility of the Spark runtime to combine Shark, MLlib, and Spark Streaming and the same time perform all of the processing by a single, small program. This arrangement allows us to reuse code and memory between the components, improving both development time and runtime efficiency. Spark as a platform integrates seamlessly with Hadoop components, running natively in YARN.,
,
Yahoo provided a great presentation about how they?€?ve been working with open source communities to bring MapReduce and additional applications onto YARN. They?€?re working to empower Spark applications via so-called ,. Spark-on-YARN enables Spark clusters and applications to be deployed on existing Hadoop hardware without creating a separate cluster. Spark applications can then directly access Hadoop datasets on HDFS. In Spark-on-YARN, Spark applications are launched in either standalone mode, executing the Spark master in a YARN container, or in client mode which executes the Spark master within the user?€?s launcher environment. Spark-on-YARN has been enhanced to support ?€? authentication, secure HDFS access, Hadoop distributed cache, and linking YARN UI to Spark UI.,
,
,
,
From what I can tell, the future of YARN is destined to be exciting ?€? its features are making YARN a first-class resource-management platform for enterprise Hadoop. There?€?s a long list of future promises for YARN including rolling upgrades with no/minimal service interruption, high-availability, support for long running services alongside applications like Apache HBase, Apache Storm natively on YARN without any changes, fine-grained isolation for multi-tenancy, powerful scheduling features like application priorities, preemption, application level SLAs, and other usability tools like application-history-server, client submission web-services, better queue management, developer tools for easier application authoring.,
,
,
,
Apache Hadoop YARN brings us a step closer to realizing the vision of Hadoop providing a single grid to run all data processing applications. This year?€?s Hadoop Summit set the stage for the evolution of YARN as an important enabler for allowing the Hadoop platform?€?s continued acceptance in the enterprise.,
,
,
,
,
,  "
"
,
,
This poster was created by analysts at ,.,
,
If you are an analyst, enterprise architect, CIO, CTO, CISO, CFO or even a business executive seeking insights into the nature of modern data solutions you will find the poster below to be a fantastic resource.  
,
Click to expand it to full view. Download and print and post on your wall to periodically review and to continue to ensure you are thinking of the many related elements of analytical solutions. The graphic was produced by three of the most highly regarded practitioners of real world data solutions I know, with contributions and mentorship of by one of the nation?€?s great enterprise architects.,
,
,
,
,
,
, is a partner and co-founder of Cognitio Corp. 
He is the Editor of CTOvision.com, the award winning technology blog, winner of InfoWorld top 25 CTO of the year for 2007,
and a was named to top 20 Influencers in Big Data by Forbes. He served as the CTO for the Defense Intelligence Agency.,
,
This post was first published on ,, and is reprinted by permission.,
,
,
,
,    "

"
,
By Alex Jones, June 2014.
,
As a graduate student in Business Analytics, I have worked the better part of a year to become a ,, ,, ,, ,, predictive analytics architect. While the skills I have developed have been invaluable, taking a year of computer science, advanced mathematics, engineering and business classes, is simply not feasible for most people.,
,
Although the challenge of collecting and analyzing "","" requires some complex and technical solutions, the fact is, that most businesses do not realize what they are currently capable of.,
,
Specifically, there are a number of exceptionally powerful analytical tools that are free and open source that you can leverage today to , ,.,
,
Rather than just leave you to navigate the frightening and giant world of IT tools and software, I have put together a list of what I see as the Top 10 Data analysis tools for Business. I picked these because of their ,(for personal use), , (no coding and intuitively designed), , (beyond basic excel), and ,(if you get stuck, you can Google your way through).,
,
,
One of my favorite data related quotes is:,
,
,
---James Governor, Founder of Redmonk.,
,
Although these tools make analysis easier, they're only as valuable as the information put in and analysis that you conduct. So take a moment to learn a few new tricks, challenge yourself, and let these tools enhance and complement the logic and reasoning skills that you already have.,
,
, is a Graduate Student at 
,.
,
,
,
,
,
 ,  "

"
Most popular 
, tweets for Jun 11-12 were
,
Huge Big Data Poster and Reference 
,
,
Proposed ethical guidelines for Twitter data mining: clear objectives, protect anonymity, don't link to other data ,
,
""Data science"" misses half the equation: you also need ""decision science"" ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Introducing the comprehensive new book on Data Classification, edited by Charu Aggarwal.
,
,
,
Published: July 25, 2014 by Chapman and Hall/CRC
,Content: 707 Pages | 84 Illustrations
,
See preface/intro  and table of contents at 
,
,??,
,
,
Research on the problem of classification tends to be fragmented across such areas as pattern recognition, database, data mining, and machine learning. Addressing the work of these different communities in a unified way, Data Classification: Algorithms and Applications explores the underlying algorithms of classification as well as applications of classification in a variety of problem domains, including text, multimedia, social network, and biological data.
,
Use promo code , to receive a 25% discount and FREE shipping for this title, or other computer science titles at 
,  "
"
,
,
,??,??
,??,??
,
,
,
,??,??
,
,  "
"
,
,
,
?€?The algorithm that runs the world?€? said the cover story of a 2012 issue of the ,. And yes, it was talking about the simplex algorithm. This topic is back in the news due to a major advance in the speed of the simplex algorithm as delivered by a commercial solution. The algorithm can now solve massive problems much more efficiently, frequently twice as fast as was previously possible.
,
Frankly, it is , that runs the world. By and large, running the modern world requires making many kinds of decisions: governments make decisions to allocate finite resources to advance society; companies make decisions to invest limited capital to generate more profit; and we all make personal decisions to spend our limited time on meaningful activities. This decision-making can take infinite forms. But to a mathematician, all decisions can be represented by mathematical models -- the allocation and limitation of the resources are regarded as variables and constraints, while the outcome is described by an objective function.
,
With this model ready, the next step is to solve it to optimality by identifying the decision that leads to the best possible outcome. The solution technique is called mathematical programming, or optimization. The simplex algorithm is at the very core of most optimization techniques. You could say the simplex algorithm runs the optimization world. It is either used to directly solve the relatively straightforward linear programming (LP) problems, or applied ubiquitously inside other more complicated optimization techniques.
,
The simplex algorithm is an iterative algorithm for solving LP problems. The concept of LP can be illustrated by a small two-variable example in Figure 1.
,
Here, we want to maximize a linear function (which could be the profit of investments) within the feasible region given by the linear constraints (which could be limitations of available resources). To solve such a simple example, one can draw its graphic representation and then easily use a pen or pencil to sweep through (pointing to the correct direction) the feasible region, until reaching the optimal extreme point. To solve the same problem using the simplex algorithm, the fundamental idea is still the same. The difference is that, rather than sweep through to the optimal point in a single shot, the simplex algorithm walks along the boundary of the feasible region, moving from one vertex to a more profitable one, until reaching the optimal vertex. Solving the same problem using the simplex algorithm is illustrated in Figure 2.

,

Although it is convenient enough to solve trivial problems (up to 3 variables with a 3 dimensional graph) with pen or pencil, for anything larger than that, we need the simplex algorithm. As a matter of fact, nowadays, LP problems often consist of millions of variables that require an efficient simplex implementation.
,
While the theory behind the simplex algorithm is relatively simple, how to efficiently implement it in practice is more complex and has long been a major topic for research. Looking back at how the theory evolved since its introduction by George , in the 1940s,?? new ground-breaking research has been published roughly every decade, with the latest one being , in the first decade of the 21, Century. The simplex world has been relatively quiet since that publication, as if hyper-sparsity was the final brick to the well-polished simplex building.
,
Besides the algorithmic improvement, advances in CPU performance have continuously boosted the power of the simplex algorithm. For example, a problem that took approximately 300 seconds to solve optimally in the 1970s can now be solved easily within 0.2 second -- some 1,500 times faster. A large fraction of the improvement came from the advance of CPU speed. However, ?€?,?€?. The turning point was 2004, when the single-core speed of CPUs ceased to increase and mainstream manufacturers turned instead to increasing the number of cores in a CPU. This trend, together with the performance of the simplex solver, is plotted in Figure 3.
,
In the graph, the red line illustrates the best-known single-core CPU clock frequency of mainstream manufacturers. From the 1990s to 2004, it increased some 40-fold from approximately 100 MHz to 3.8 GHz. We have seen hardly any increase since then, and ,was Intel able to pass the 4 GHz barrier, a mere 5% improvement in 10 years. Note that it is not the CPU clock frequency alone that determines its performance, but it is widely accepted as a fair measure. On the other hand, as the yellow line shows, the number of cores in mainstream CPUs has shown a continuous growth in the last ten years as an alternative to further improve CPU performance.
,
What?€?s even more interesting is the performance of the simplex algorithm ?€? by the standard measure of how many medium-sized LP problems can be solved in an hour ?€? over this time period. The improvement has been a combination of the continuous CPU performance improvements and several leaps in ground-breaking algorithmic variants (linking the advances to the year the research was published). In the past, newer CPUs mean better performance. However, this stopped being true in 2004. Nowadays, the simplex solver needs to go parallel to still take advantage of the hardware evolution trend.
,
As a matter of fact, the simplex community has long tried parallelization, long before parallel computers were introduced to general daily usage. In the last 20 years, the many attempts have ranged from massively distributed machines in universities and national laboratories to modern GPUs in personal computers, using either dense or sparse matrix computations. However, good speed-up ratios always came from parallelizing less efficient sequential implementations of the simplex algorithm. As a result, the overall performance of these parallelization efforts is still inferior when compared to world-leading sequential simplex solvers.
,
A meaningful simplex parallelization has to be designed and developed based on a state-of-the-art sequential simplex solver and still allow for significant parallelization. This has been achieved in the most recent version of the ,. As the first commercial parallel simplex solver, it is able to solve various large-scale problems 2X faster (speedup of two) than other methods.
,
The key idea behind this implementation is to create more parallelizable scope by combining multiple simplex iterations into one ?€?mega?€? simplex iteration. To implement this efficiently, several innovative techniques were developed. A detailed introduction to this approach is available ,. The implementation in FICO Xpress is an adaption and further development of this key idea.
,
With this implementation, medium- to large-sized linear programming problems can often be solved 30% faster on average, in the best case up to 2 times faster. This introduced yet another performance leap to the simplex algorithm ?€? while the search for other advances continues.
,
,
,
, is the ?€?simplex guy?€? behind the ,. Before joining FICO, he obtained a BSc in Computer Science from Beihang University in Beijing. He then earned an MSc in High Performance Computing and PhD in Mathematics from the University of Edinburgh. His PhD research topic was the parallel dual simplex method linked above. Notably, his PhD supervisor in Edinburgh, , is the one who published the research on hyper-sparsity, resulting in the previous simplex performance leap.
,
,
,
,
,
 ,  "
"
Most popular 
, tweets for Jun 13-15 were
,
Book: Data Classification: Algorithms and Applications ,
,
#BigData companies to watch, top analytics experts select most promising companies #BigDataCo ,
,
Book: Data Classification: Algorithms and Applications ,
,
,  "
"
,
,
, ??is responsible for using machine learning and data mining algorithms to bring structure to Slice?€?s ??purchase data. ??Prior to Slice, he earned his B.S. in Symbolic Systems and M.S. in Computer Science at Stanford University where he did research in the intersection of social network analysis and network language processing. ??He examined sentiment flow through hyperlink networks as well as building a content prediction system for Twitter users.??You can see how predictable his content is by following??him ,.
,
Here is my interview with him:
,
,
,
, In this age, graphs are becoming quite popular for exploring a domain.?? Google popularized the ?€?information graph,?€?, by modeling web pages as nodes and hyperlinks as edges.?? Facebook popularized the ?€?social graph?€?, treating each user as a node and friendships as edges.?? In the purchase graph, you have products as nodes and affinities between products as edges.?? You can determine the affinity between purchases using collaborative filtering, i.e. examine the number of shoppers by who bought both products compared to the number of shoppers who bought each individual product.
,
This is crucial for not just online retail companies, but for all kinds of companies.?? If you understand the affinities between products, you have opportunities to advertise,?? cross-sell and up-sell.?? I?€?m sure most users are not fully aware of the entire catalog from all retail companies, so using the purchase graph, you can hint at what purchases the users might be interested in and what they?€?re like (and in an automated method!).
,
,
,
, A Slice user can learn a lot about their own shopping habits - how much they spend, what they buy the most of--and we are currently working on new features , that will enable Slice users to get new utility from their own purchase data.?? That said, most online shoppers aren?€?t as interested in digging deep into their own data as we might be...so we dedicate most of our product features to help them keep track of their receipts, monitor their shipments and deliveries, be informed of price drops and get alerts when something they?€?ve bought has been subject to a recall.
,
The way we really bring our data to life, though, is through our partners, who can tap into the Slice API to create new features and experiences.?? Slice captures all activity around a consumer?€?s?? purchase--what they bought, how much they paid, how they paid, where they live and where they shipped it-- over time, and how these behaviors change. We enable our partners--retailers, web publishers and service providers--to harness this valuable data to create personalized customer experiences, such as product recommendations and targeting offers?? based on a visitor?€?s unique shopping history. And we?€?re actively working on new products to bring even more insights to light, which we plan to launch in the coming months. Stay tuned!
,
,
,
, We actually looked into that and found that paper is quickly becoming a thing of the past. I?€?m sure you?€?ve noticed this as well, as a large number of retailers are beginning to e-mail in-store receipts.?? Companies like Apple, Nordstrom, Bloomingdale?€?s and Macy?€?s, for example,?? allow users to have the option of getting their receipt e-mailed to them instead of or in addition to a paper copy of their receipt.?? In addition, we are seeing more e-receipts with the rise of mobile payments, such as PayPal and Square.?? We believe more and more retailers will send more e-receipts as it is more environmentally friendly, less hassle for the customer, and it facilitates a communication channel between the retailer and the customer.
,
,
,
, In the machine learning team, we build models for prediction and categorizing data.?? As we iterate on these models, we establish metrics from the start, so as we experiment with new features and algorithms, we learn which ideas help and which ideas don?€?t help.??,
,
,
,
, We have not, and here?€?s why -- we are clear with our customers that our technology only identifies and analyzes the receipts in their inboxes--and nothing else--and that we never, ever release their personally-identifiable information, full stop.
,
,
,
, In the past several years, the sheer volume of data has grown immensely--it?€?s mind-boggling.?? In addition to that, we now have better technology to store and mine this data to come with interesting analyses and to improve the lives of people.?? What?€?s most amazing--and inspiring-- to me is that we can do much of this in an automated fashion through machine learning and software engineering!
,
What I like most about my job is dealing with such a unique, high-definition data set ?€? a longitudinal, cross merchant purchase graph as well as working with smart people in a small startup that moves quickly.
,
What I don?€?t enjoy??? Picking restaurants to eat on University Ave in Palo Alto!?? There are just too many great options.?? Also, I do not enjoy when my coworkers (rarely) beat me in ping pong--which is pretty serious at Slice!?? Seriously speaking though, sometimes data sets can be very noisy and messy, especially with the data set comes from so many sources.?? This both makes the problem fascinating and sometimes frustrating as it makes it harder to build algorithms and training sets that can generalize. But that is what makes what we are doing so valuable and so challenging.
,
,
,
, ,
,
,

, The last book I read was , by Malcolm Gladwell.?? It was fascinating to read a social scientist discuss how epidemics and ideas spread.?? While reading, I couldn?€?t help but turn his problem into a graph with nodes and edges.?? While not working, I like to sing and play the piano, as well as hiking and biking.
,
,
,
??  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 16 and later.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky,  
,, Jun 17, 2014.
,
This year, World Cup predictions are everywhere and come from many sources, including Nate Silver FiveThirtyEight, Bloomberg, and 
,.
,
What can the players do in the face of the power of Big Data?
,
New KDnuggets cartoon from Ted Goff has one idea.
,
,
,
,
See other cartoons on
, page.
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,, Jun 17, 2014.
,
, asked 
,
,
It received a lot of attention, both from vendors and from the analytics community.
However, regardless of who was in what place, the votes from over 3,000 participants give a lot of interesting data to examine - see our initial analysis below.
,
We prepared an anonymized version of the poll votes - scroll down to the end of the post for instructions on how to download the data. Let me know what else you find!
,
One interesting question is what tools are used by the same user.
,
The poll question asked about usage over a period of 12 months, so these 
tools were not necessarily used on the same project, but data scientists,
like all craftsmen, tend to learn particular set of tools and use them repeatedly. Indeed, meaningful correlations we find confirm that co-occurrence of tools in the same vote is meaningful.
,
One way to observe tool co-occurrence is with a heat map - see below.
This heat map was prepared by Ran Bi from NYU.
,
Darker colors mean more usage.  We note that R is used a lot with other tools,
SAS is used frequently with SAS Enterprise Miner (not a surprise), SAS is used less frequently with R. 
,
,
,
,
Next, we examine for each tool, what other types of tools were also used.
,
We note that most ""lonely"" tools are
,
,??,
The overall average was 29.6% alone, Ntools=3.7.
,
The least ""lonely"" tools are below (0% lonely - never used alone): 
Hadoop, Microsoft SQL Server, MATLAB, Unix shell/awk/gawk, Rattle, 
Other Hadoop/HDFS-based tools, Gnu Octave, KXEN, Pig, Orange, Spark 
Pentaho , Mahout, Mathematica, and IBM Cognos.
,
The second group also correlated with tools that were part of larger platforms.
Users of Pig, Mathematica , Mahout , Perl , Other Hadoop/HDFS-based tools , and Orange have used at least 8 tools (vs avg of 3.7).
,
The largest number of commercial tool used was for users of free tools
,
,??,
while the smallest (just slightly above 1) are users of
,
,??,
Very surprisingly, the largest number of different free tools was among users commercial tools: 
,
,??,
with N free tools ranging from 4.4 to 3.6.  Does this suggest a gap in capabilities of those commercial tools that users want to fill in with free tools ?
,
The table below has entries for all tools with at least 50 votes.
For each tool, we show 
,
,??,
,
,
,
,.
,
The anonymized data (3285 records) is in the file ,/,/kdnuggets-2014-software-poll-anon.tab (tab-delimited), 
where , is the name of this site, and ,=tmp. 
,
The fields are:
,
,??,
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, Jun 17, 2014.
,
Hurwitz and Associates is less famous than Gartner and Forrester, 
but it is a respectable group of analysts who also cover Analytics space.
,
Its latest report, named
,
examines the top trends for end users and analyzes 10 vendors across four dimensions: vision, viability, validity and value.
,
The report is based on a survey of over 400 advanced analytics users conducted in Q1 2014.
,
,
,
,
,
,??,
,
,
,??,
,
,
,??,
Among key Study Findings:
,
,??,
For full report ($), visit
,
,
,
,
is a strategy consulting, market research, and analyst firm that focuses on how technology solutions solve real world customer problems.
,
,
,  "
"
Most popular 
, tweets for Jun 16-17 were
,
Cartoon: Big Data and World Cup Football 
,
,
You cannot afford to ignore next #AI wave. Early leaders - IBM Watson, Narrative Science, Microsoft, Apple, Google ,
,
You cannot afford to ignore next #AI wave. Early leaders - IBM Watson, Narrative Science, Microsoft, Apple, Google ,
,
,  "
"
,
,
supported by the EU FP 7 FET Open Project ICON
,
,
In industry, society and science, advanced software is used for solving, planning, scheduling and resource allocation problems, collectively known as constraint satisfaction or optimization problems. At the same time, one continuously gathers vast amounts of data about these problems.,
,
This summer school starts from the observation that current software typically does not exploit such data to update schedules, resources and plans and aims at introducing a new approach in which gathered data is analysed systematically in order to dynamically revise and adapt constraints and optimization criteria. Ultimately, this could create a new ICT paradigm, called Inductive Constraint Programming, that bridges the gap between the areas of data mining and machine learning on the one hand, and constraint programming and optimization on the other hand. If successful, this would change the face of data mining as well as constraint programming technology. It would not only allow one to use data mining techniques in constraint programming to improve the formulation and solution of constraint satisfaction problems, but also to employ declarative constraint programming principles in data mining and machine learning.,
,
One remarkable example is society, where human activities mediated by ICT generates big data in the form of digital traces that cannot only be used to evaluate the performance of the underlying constraint satisfaction and optimization models but also to automatically revise and improve the underlying models so that the solutions are automatically adapted to the behavior of the people. As one example, consider a public transportation schedule that continuously adapts itself to the real mobility patterns of people represented by the digital traces of individual travels. Thus  the knowledge  mined in (big) data can help to adapt schedules, resources and plans to the real dynamics in the real world.,
,
So far, constraint solving has evolved quite independently from machine learning and data mining. There has recently been a growing interest in the integration of these two fields, which can work in two ways: (a) constraint solvers can be included in machine learning and data mining algorithms; and (b) machine learning and data mining can help in addressing and formulating constraint problems. Promising initial results have been achieved in both directions, in the ICON project and beyond and further research is ongoing to  establish a full integration.,
,
The summer school ""Constraint Programming meets Data Mining"", organized by the FP7-ICT FET Open project ICON ?€?Inductive Constraint Programming?€? (,) provides an intensive training opportunity to learn the essentials of recent research on constraint solving, machine learning and data mining, and the key aspects related to their integration.,
,
Students will follow lectures from top experts of the fields, and will receive personalized training on selected exercises in hands-on labs.,
,
,
,
,
,
Deadline for registration: June 25, 2014,
Summer school starts: September 1st, 2014,
Summer school ends: September 5, 2014,
,
*** Venue ***,
The venue for this event is Sampieri, an old fishing village, perhaps the most picturesque in the province of Ragusa, Sicily, Italy. This village is characterized by stone houses and very small streets in old stones.,
,
The Summer school is held at the Marsa Sicl?? Residence in Sampieri, which is composed by 82 apartments and by the Club House including bar, restaurant/pizzeria, and other collective services. Residence facilities include: swimming-pool, volley court and a tennis court, soccer court, ping-pong table, etc.,
,
For more details see ,
,
We are looking forward to your participation!,
,
,
,  "
"
,
,
June 30 - July 3, UCLA, Los Angeles, CA.
,
The open source R language has become the tool of choice for data scientists in the last few years. The main annual event of the R community, the useR! conference will start soon on June 30, 2014 in Los Angeles. 
,
Don't miss this opportunity to learn about the most widely used R packages, hear the latest developments and connect with the best of the best in the R community. 
,
Register here: ,
,
Best regards,
,
Szilard Pafka
,the useR! 2014 Organizing Committee 
,
,
,  "
"
,
Latest ,, (Jun 18, 2014) ,:
,
,??,
Also
, (3) |
, (6) |
, (2) |
, (1) |
, (1) |
, (7) |
, (1) |
, (2) |
, (3) |
, (6) |
,
,
Researchers too frequently commit the ,. Gregory Piatetsky, 2014.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
KDnuggets readers can save $30 from already low registration fee - use ""KDNuggets"" promotional code when registering for the conference at 
,.
??
,
??
,
??
,
,
,
,
,
??
,
,
,
,
,
,
,
,
,
For more information, visit
, homepage.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Jun 18-19 were
,
Netflix: Senior Data Scientist, Streaming Science & Algorithms ,
,
Report on @DataKind London: what 100 data scientists did to help 4 fantastic charities: big savings, new insights ,
,
Nice tutorial ! #BigData Analytics with Google Big Query and R by @Gliders_Coach #rstats ,
,
,  "
"
,
,
NEW YORK, NY - (June 19, 2014) - The Association of Computing Machinery's Special Interest Group for Knowledge Discovery and Data Mining (,), today announced a partnership with Bloomberg to host the ""Unleash Data: Accelerate Impact - KDD at Bloomberg"" event on August 24, 2014 in New York. 
It will be part of the society's annual flagship 
,
that will have a special focus on applying data science to improve civic and social outcomes.
,
Here is 
,: short talks, paper presentations, and demos are welcome.  
,
Abstracts are solicited in areas including 
,
,
Submissions due July 15.
,Format: Short Talks = Abstract; Paper Presentations = Working Paper; Demo = Abstract + Screenshots
,Submit to kdd2014@bloomberg.net with subject ""name | affiliation | title""
,
,
,
Through this Bloomberg sponsored event researchers will be able to engage with practitioners, policy makers and activists engaged in NGO's to explore opportunities to utilize data science in socially relevant application domains like public health, medicine, urbanization and geopolitical disparity. The motivation is to explore how technical innovation around data-driven analytics can demonstrate new opportunities and accelerate solutions to pressing societal issues like climate change or the spread of disease.
,
,
,
The full 
,
(August 24-27) features four keynotes, 151 research track papers, 44 industry and government track papers, seven invited talks, 12 tutorials and 25 workshops including the KDD CUP challenge. Consistent with the theme, more than 300 teams are currently volunteering their data skills to help NYC based DonorsChoose connecting teachers to donors to provide funding for educational projects in this prestigious competition.
,
To register, please visit: ,.
,
For more information, visit 
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Jun 19, 2014.
,
,
,
Here are the results and analysis of this poll: 
,.
,
This is one of the longest running poll series on KDnuggets - see results of similar past polls below, and while the median answers grows each year, the percentage of data miners which work with really large data (many terabytes and petabytes) grows surprisingly slowly.
,
This trend suggests that really Big Data sets remain the exclusive domain of data miners at web giants and some really large companies and government organizations.
,
,
,- ,
,- ,
,- ,
,- ,
,- ,
,- ,.
,- ,
,- ,.
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
Most popular 
, tweets for Jun 20-22 were
,
Good list of R function to access, manipulate, summarize, plot and analyze data #rstats ,
,
Great visualization: English letters in the beginning, middle, and end of words ,
,
,
Watch: Practical Deep-Learning Lecture: Machine Perception & Its Applications, by Adam Gibson, cofounder of Blix.io ,
,
,  "
"
By Gregory Piatetsky,  
,, Jun 23, 2014.
,
The amazing Italian researchers,  Roberto Battiti and Mauro Brunato, 
authors of 
, software,
have recently written 
, textbook, and made it freely available on the web.  This textbook presents important materials on machine learning and optimization, but does it in a very entertaining and readable style, with excellent illustrations.
,
Now they also created 
, with materials on machine learning and optimization made available by their lab and a community of active researchers and users.
,
This includes 
,
,??,
Check all those amazing resources and more at ,
,
You can also use 
,, a comprehensive Machine Learning and Intelligent Optimization tool, freely available for non-profit, academic use.  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 23 and later.
,
See full schedule at , .
,
,  "
"
By Krishna Prasad, June 2014.,
,
The article mainly focuses on how to use R to access and visualize census data.?? There are contributed packages that greatly enhance your ability to interact with the graphs you create in R. I will mainly focus on obtaining data from the US Census via an API connection and plotting data on different types of US maps.,
,
,
,
,

Then we use the acs.lookup function to find the required data in all tables using key words.,
,
For example, the following are the search results for the keywords owner, occupied, and median.,
,
,
,
An object of class ""acs.lookup"",
,
,
,
,
,
variable.code???????????????????? table.number???????????????????????????????????????????????? table.name,
1?????? B25021_002???????????? B25021???????????????????????????? MEDIAN NUMBER OF ROOMS BY TENURE,
2?????? B25037_002???????????? B25037???????????????????????? ????MEDIAN YEAR STRUCTURE BUILT BY TENURE,
3?????? B25039_002???????????? B25039 ????????????????????????????MEDIAN YEAR HOUSEHOLDER MOVED INTO UNIT BY TENURE,
4?????? B25119_002???????????? B25119???????????????????????????? Median Household Income by Tenure,
,
                variable.name,
1 ????????????????????????????Median number of rooms -- Owner occupied,
2???????????????????????????? Median year structure built -- Owner occupied,
3?? ????????????????????????????Median year householder moved into unit -- Owner occupied,
4 ??????????????????????????????Median household income in the past 12 months (in 2012 inflation-adjusted dollars) -- Owner occupied (dollars),
,
,
,
Using choroplethr simplifies the creation of choropleths (thematic maps) in R. It provides native support for creating choropleths from US Census data. This functionality is available with the choroplethr_acs function.,
,
,
,
The choroplethr package does not store any data locally. Instead, it uses the R acs package to get ACS data via the Census API. This means a few things for users of choroplethr.,
,
,
,
,
Table B01002 has 3 columns.?? Please choose the column to render:,
,
1: Median Age by Sex: Median age -- Total:,
2: Median Age by Sex: Median age -- Male,
3: Median Age by Sex: Median age -- Female,
,
Selection: 1,
,
,
Fig. 1 US Census - Median Age of Home Buyers,
,
According to the , (NAHB) study,the average buyer is expected to stay in a home for 13 years. To know the major cost paid by home buyers, we combined median home price data and , over a period of 13 years, and plotted the data on the US map to give a clear view of the total costs by state.,
,
#Downloading median home price data,
,
,
Downloaded Average Latitude and Longitude for US States from MAX MIND,
,
,
,
,
,
,

,Krishna Prasad is a Data Analyst with experience programming in Python and R. He is a Computer Science Engineer from JNTU, Hyderabad.,
,
,
,
,
,
,
,
,
,  "
















"
By Gregory Piatetsky,  
,, Jun 23, 2014.
,
People love round numbers, and as a data scientist I pay a special attention to numbers.
So, I was very excited when in early June the number of 
, followers crossed 20,000 threshold. 
,
,
,
I was able to catch who was the follower number 20,000 and she turned out to be 
Liyang Tang,
,, 
studying for a Ph.D in data mining and business intelligence in China.
,
Here is my interview with her.
,
,
,
,
,: 
I followed most accounts about data mining and data analytics, because that is my research interest. Also, I knew about your KDnuggets site long time ago, and the first attraction is some open datasets.
,
,
,
,
My major is information management and information system for undergraduate. Then, I continue with my Ph.D program with a professor whose research interests are data mining and business intelligence. During that time, I also entered Cisco Webex Hefei branch as an intern, and my job is mainly about social networks. After that, I applied for study abroad, and worked with a professor who has put all his efforts on data mining in US. All those reasons together contribute to my firm decision of continuing with data mining field. However, my major is not technically CS, but more like business school, which means conducting data mining techniques with business insight. For now, my study is focused on social networking applications. 
,
,
,
,
Part of this question is already answered above. Specifically, I'm working on recommender systems and community discovering, with Twitter as my experimental data source. I try to consider problems with business insights and motivations, and then apply data mining or machine learning algorithms to that. Technically, recommender systems methods, time series analysis, and community detection techniques are involved.
,
,
,
,
In China, there are three big leaders: Baidu, Tencent and Alibaba. All of them put lots of efforts in big data and data mining. For example,  Baidu and Tencent are mainly about offering cloud storage, and Alibaba tries to achieve infrastructure and data sharing.
,
,
,
,
Personally, I think the current DM and BI in China efforts are mixing with the cloud computing concept. Every time a company claims about data mining, they bring forward cloud computing as well. I guess this would still hold on for next 5 years. Besides, when it comes to cloud computing, I think communication and network companies could have the advantage to build the cloud infrastructure.
,
As for global trends, I think the major trends for DM and BI would be more integrated with cloud computing and mobile computing. 
,
,
,
,
Speaking of trends, the trend for the number of KDnuggets followers seems to fit very well a polynomial
,
N = 8.898 * X, - 131.8X + 1293, (R, = 0.996)
,where X=1 for Jan 2010, 2 for Feb 2010, etc.
,
which gives an estimate of 25,400 for the number of followers at the end of 2014,
and 30,000 in May 2015.  Stay tuned.
,
,
,  "
"
By Tom HC Anderson, 
,, June 2014.
,
While text analytics has been around for quite some time and has reached mainstream to the point of becoming a buzz word, few really know what it is. It's not a word cloud. It is not a qualitative tool. IT IS data mining.
,
,
,
We've long felt the need to clear the confusion around text analytics in our industry. Surprisingly there aren't really any good videos on the subject.
,
In making a video to explain what text analytics is, we first had to decide who our audience was. So many business videos these days are trying to reach such a broad audience that they become totally void of any real information. On the other hand, we didn't want to make a geeky video just for 'data scientists' either.
,
We hope that the middle ground we chose to communicate to here, basically our core customer base (the consumer insights analyst/manager/research director), will provide the right level of detail within a reasonable amount of time, about 4 minutes (down from our original 7 minute version).
,
For more information, visit ,.
,
,  "

"
,
By Grant Marshall, June 2014.,
,
The CRN 25 Big Data Infrastructure companies includes notable companies who provide the tools for other companies to handle and analyze big data.,
,
One interesting observation about the companies in this list compared to the companies in the rest of the Big Data 100, is that the average age of a big data infrastructure company (20) is older compared to the data management companies (8) and business intelligence companies (10).,
,
This is also the only category containing companies founded before 1950 (IBM and HP). This would seem to imply that big data infrastructure rewards companies with more extensive experience in the field.,
,
Here are the CRN 25 Big Data Infrastructure companies:,
,
,
Original post: ,
,
,
,  "
"
,
,
The role and importance of Chief Data Officer has been increasingly gaining attention across industries. Data is being viewed as the most competitive tool that organizations have in order to maintain relevance and growth in today?€?s complex environment. While the CDO office holds enormous potential, it is currently faced with a wide variety of issues around data integration, ROI, data quality, etc.
,
,(May 22-23, 2014) organized by the Innovation Enterprise at San Francisco, CA covered the above topics through insightful talks from leading experts across various domains. Along with a deep dive into the role of the Chief Data Officer, the summit covered innovation, data management and data governance. Through real-life business case studies and discussions on major issues, the summit offered solutions and insight from the CDOs and other business leaders.
,
Despite the great quality of content as well as speakers, it is hard to grasp all the information during the summit itself. , These concise, takeaway-oriented summaries are designed for both ?€? people who attended the summit but would like to re-visit the key sessions for a deeper understanding and people who could not attend the summit. As you go through it, for any session that you find interesting, check KDnuggets as we would soon publish exclusive interviews with some of these speakers.
,
Here are highlights from selected talks on day 1 (Thu, May 22):
,
, shared her experience of shift from a private sector DW/BI role to data management of several state agencies at Colorado. She started her talk titled ""The Chief Data Officer Role - Knowledge Means Change"" with a story about why it is important to share information. She described how different aspects of citizens' lives have varied needs across time and to fulfill those needs they interact with a wide range of state agencies. This results in citizens data being captured in varying form and sizes across different agencies using different computer systems. It is only by sharing and integrating data that we can correlate information to generate insights, leading to impact through informed decisions. She illustrated the key differences between traditional data warehouse and Big Data analytics, while referring to the latter as IDI(Intelligent Data Insights) platform.
,
In conclusion, she emphasized that in data warehouse oriented systems you find data to support whatever you believe, on the contrary Big Data analytics enables you to know what you do not know.
,
, gave an interesting talk on ""Three Keys to Building Success and Traction with your CEO and C-Suite Peers"". Quoting an IBM study, she mentioned that over the next 3 to 5 years, 66% CEOs are focusing on higher business value, whereas 23% are focusing on efficiency and cost reduction. About one-fourth of CEOs believe that their C-suite do not understand customer and market changes. In terms of influence on CEO's strategic vision and business strategy, the customer is just second to the C-suite. CMOs need CDOs help to finally activate their digital strategies; especially on social media analytics and advanced analytics.
,
She recommended the ""Start Small. Scale Fast"" approach to generate traction for Big Data initiatives through the following three key activities (based on the findings from IBM Institute for Business Value C-suite studies):
,
,
, talked on ""Great Data by Design - Great Data Isn?€?t An Accident; And You Know It!"" While we are witnessing a sharp rise in the volume, velocity and variety of data, there are huge challenges posed by data fragmentation and data quality. This trend has been driven by the desire of individual business unit to build their own custom analytics applications(with marginal attention to the enterprise IT strategy or architecture). Traditionally, the winning strategy has been: identify the critical business function; invest in competency and efficiency for core applications & processes around the critical business function; and winning through a harmony of focused alignment of people, process and technology.
,
However, in today's world, business hardly have a single critical business function, rather it is an interconnection of a few business functions that is deemed critical. Moreover, this critical business function needs to be integrated to even more business applications at enterprise level as well as business unit level. This implies that solving tough business problems depends on data from more than one system. Thus, business agility depends on data integration ability. Emphasizing on the need for ""Great Data by Design"", he mentioned that 80% of the work in Big Data projects is data integration and data quality. He concluded explaining the Data Governance Maturity Model Assessment (available at http://www.governyourdata.com) and how the industry can benefit from Informatica solutions.
,
, delivered an insightful talk on analytics in Media & Entertainment (M&E) industry titled ""Maximizing the organization: turning data into a competitive advantage"". Business models, content strategies, and company definitions are changing rapidly. Traditionally, the media & entertainment firms have been offering siloed, static, and unidirectional consumption experiences across different media such as theatrical, television and home entertainment. However, with the increasing proliferation of devices and media platforms, today we are witnessing focus on new immersive consumer experience delivered in a connected fashion across various platforms. M&E industry needs include data creation & consumption capabilities throughout the content life-cycle, and data availability for all business workflows (such as pre-production, product development, fulfillment, finance, legal, etc.).
,
The underlying data has varying levels of complexity ranging from structured master data (such as global unique media identification, party-to-party relationship management, etc.) to unstructured social media & consumer data (such as user reviews, user-generated content, crowd-sourced metadata, etc.). Entities must pursue three axes of change to successfully cultivate a data-driven culture: technology, process, & organization. She concluded saying that multifaceted transformation is required to obtain the Holy grail of competitive advantages & optimal positioning for future growth.
,
,
,
,
,  "
"
        ,
,  "
"
,
By Grant Marshall, June 2014.,
,
The CRN 50 Big Data Analytics companies includes companies that help businesses leverage their big data resources and produce tools that make use of those resources.,
,
Compared to the companies in the other Big Data 100 lists, the 50 business analytics have the lowest average age (10) and the oldest company on the list, SAP, was founded as recently as 1972.,
,
This data seems to show that business analytics is a more recently developing field than the other categories. This is backed up by the fact that 44% of the companies in this list were founded in this decade! Based on this data, business analytics appears to be one of the most accessible fields to break into with respect to big data.,
,
,Here is a word cloud with the most common words, which shows that CA (California), San Francisco, MA (Massachusetts) are most common locations.
,
,
Here are CRN 50 Big Data business analytics companies:,
,
,
,
Original Post: ,.,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
  "
"
By Robert Vidra, XLMiner, June 2014.
,
Almost every business analyst can use Excel, and it often plays a role in an 
analytics project.??But can you actually do 
, for a , problems in Excel? This might surprise you, but analysts in over 
, 
know the answer is ,??For years, they've been solving large-scale 
industrial problems with big payoffs, using our Excel-based and SDK tools.
,
You might have heard of 
,
as developers of the Excel Solver and our
, upgrades.??But for 7 years, our
, products 
have given analysts the power to solve problems involving risk analysis, 
decision analysis, and resource allocation under uncertainty in Excel -- 
using state of the art, parallelized??,, simulation 
,, stochastic programming and , optimization methods.??This week, we're introducing a new, powerful
, 
product, which rivals the best-known statistical software for performance and accuracy on large datasets.
,
,
,
But Excel ,, right? 
, -- with 
,, 
the SQL Server Analysis Services engine in memory with Excel, it's 
, to bring in 100 million rows, joining tables from enterprise and 
public databases, and let XLMiner draw a sample for training its wide 
range of data mining algorithms.
,And with ,
in Excel -- the , based on SQL Server Integration Services -- 
plus XLMiner's , and transformation tools, you'll
be surprised at how much easier the ""data wrangling"" task can be.
,
But you , up-to-date models and data visualizations online, right? 
, -- with Microsoft 
,,
it's as simple as File Save As... 
to publish your Excel workbook, with its own cached Power Pivot database 
and Power View visualizations -- in Office 365 and Excel Online.?? And with 
Power BI's , plus our , and 
,, you can update 
and re-optimize decision models via your Web browser.
,
, is part of
,, our integrated software for predictive and prescriptive analytics -- forecasting, data mining, optimization and simulation in Excel. 
And that makes it part of a ,: How , and ,, with , and our 
,, have emerged as a surprisingly powerful ""complete stack"" for business intelligence and advanced analytics, with easy deployment on the Web. 
,  "
"
Most popular 
, tweets for Jun 23-24 were
,
What is Text Analytics? ,
,
Machine learning in the cloud: the brains behind Microsoft Azure #MachineLearning - in-depth look ,
,
,
Machine learning in the cloud: the brains behind Microsoft Azure #MachineLearning - in-depth look ,
,
,
  "
"
,
By Grant Marshall, June 2014.,
,
The CRN 50 Emerging Big Data Vendors includes companies from the infrastructure, business analytics, and data management lists that are up-and-coming.,
,
Since these companies were chosen to be those that are showing great promise for their relatively young age (the oldest company on this list was founded in 2008), their average age is very young (<4 years).,
,
These companies come from a variety of different backgrounds; some of them handle the infrastructure side of big data (e.g. Pivotal and Xplenty) while others are focused more on business analytics (e.g. Alpine Data Labs and Numerify). 
,
Silicon Valley (CA) dominates this list with 34 companies, top locations being San Francisco (12 companies), Mountain View (7), and Palo Alto (5). Outside of California, top locations are MA (Massachusetts) - 4 companies, and Israel - 3 companies.  
,
Below is the word cloud from the companies descriptions, with most common words - data, CA, Hadoop, software, develops, analytics, and platform.  Thus the prototypical emerging Big Data company is , Competition will be tough. 
,
,
,
Here are the CRN 50 (actually 52) Emerging Big Data Vendors:,
,
,
,
Original Post: ,
,
,
,  "
"
,
Latest ,, (Jun 25, 2014) ,:
,
,??,
Also
, (1) |
, (2) |
, (2) |
, (1) |
, (1) |
, (6) |
, (7) |
, (1) |
, (3) |
, (10) |
,
,
For all the networks we studied, for each sample, we always manage to generate very close, visually indistinguishable, adversarial examples that are misclassified by the original network. Google and NYU researchers, see ,.  "
"
, aims to establish a status quo of smartphone usage.
,
To this end, we devised an App that collects interaction-events on the
phone and processes these on a server. It then provides aggregated feedback
to the user regarding his or her time spent using the phone. By these
means, we managed to attract a substantial number of users, knowingly
donating their data to science.
,
The data set describes the smartphone usage of >100k users since January
2014. It records when and which App is started or stopped, when the
phone is locked or unlocked, as well as meta-data for calls and texts.
Furthermore, users have answered a questionnaire regarding age, sex,
education and personality.
,
This data allows research from numerous perspectives, employing a wide
range of techniques. Our small team cannot possibly envision many of the
research questions that could be answered using this data set.
Similarly, we would be new to many of the more advanced
techniques. Hence, we would like to open this amazing data set to the
research community. Due to the highly sensitive character of the data,
we cannot ship disks or provide remote access. Any interested researcher
would have to physically visit Bonn, Germany.

,We are thus looking for one to four highly motivated researchers to
visit Bonn. In particular, you are invited to conduct research on the
Menthal data set for a period of 2 ?€? 8 weeks in collaboration with the
local team of students and scientists. Any applicant would need to be a
Post-doc, or later stage PhD student. You need to be able to work, research
and publish independently.
,
We are looking for applicants from a broad range of behavioral and data
sciences, such as:
,
, ?? ,
We actively promote a multidisciplinary approach to analyze our data. To
this end, we try to have researchers from more than one discipline
present at any given moment in time.
,
You will be given access to raw Menthal data, as well as first
authorship of any paper resulting from your work. Other than that, you
will receive a desk, a window, and fun colleagues. Plus, we guarantee an
insanely creative and busy atmosphere. We will assist with
administrative issues. However, we are a small and under-financed
research team. Hence, there are NO funds available, you will need to be
self-funded.
,
In order to apply, send a brief email to ,. Be sure
to include your CV, publication record, and an one page summary of your research idea.
,
,
,
We are the authors of the Menthal App, which records smartphone interactions. Using the collected data, we study smartphone habits and depression, and provide feedback to the users.  
,
,
,  "
"
,
By Grant Marshall, June 2014.,
,
The CRN 25 Big Data Management companies include those who are known for helping manage an organization's data through databases or data storage solutions and leverage that data for the development of applications.,
,
One interesting thing to note is that there are nine companies in this list that either make use of or develop their own NoSQL solutions for data management. Compared to the four companies in this list that develop tools based on traditional SQL, that number is huge. Big data is one area where NoSQL is really beginning to gain mind share.,
,
,
Here is a word cloud from top 60 words in company descriptions.,
,
Here are the CRN 25 Big Data Management companies:,
,
,
Original post: ,
,
,
,  "
"
By Nick Elprin (Founder, ,),
,
As more organizations invest in maturing their analytical capabilities, data scientists are increasingly applying best practices from software engineering to make their workflows easier to manage (e.g., version control). ??Unfortunately, data analysis and software development have fundamental differences, which can lead to friction when transplanting tools and practices from the software world. ??As a result, data analysis teams either awkwardly graft tools on to their workflows, or they build their own custom toolset ?€? or, lacking the resources for custom development, they simply tolerate inefficiency.,
,
This article introduces ,, a platform we have built from the ground up to support modern data analysis workflows. ??Domino is language agnostic (supports Python, R, MATLAB, Perl, Julia, shell scripts and more) and has rich functionality for version control and collaboration (think Github for data science) along with one-click infrastructure scalability (think Heroku for scripts), and deployment and publishing (like yhat) ?€? all in an integrated end-to-end platform.,
,
,
,
The differences between software development and data science illuminate critical requirements of modern analytical workflows:,
,
,
,
,
,
,
,
Domino addresses the above issues with a platform built around three key insights:,
,
,
,
We have received a ton of positive feedback from data scientists who don?€?t have the time or engineering training to manage their own infrastructure or set up tools to facilitate best practices. ??We hope , will be useful to you, and we?€?d love to hear what you think!,
,
,
,
,
Nick Elprin is a founder at ,. ??Prior to Domino, he built software at a large hedge fund. ??He has a BA and MS in computer science from Harvard.,
,
,
,  "
"
,
,by Jean-Paul Isson, Jesse Harriott
,Publisher: Wiley, 2012
,
With today's explosion of digital information, companies are inundated with overwhelming volume of data from multiple sources. While some businesses don't even know from where to start others are struggling to move beyond buzz words of Big Data and reporting. And in some cases executives don't have a clear understanding of data intelligence or don't see the value of analytics.
,
Packed with case studies, , provides a new way of looking at integrating different types data intelligences into business practice.
,
Written clearly for the non-technical professional, this definitive guide shows you how to gain the most opportunity and value from every type of advanced business analytics. It is an essential guide to bridge the communication and implementation gaps between the Business and the Technical teams.
,
Helping managers and executives to understand what it takes to successfully implement and leverage analytics at work; it would be essential to technical professionals eager to speak the business language to successfully communicate their findings to the business world. 
,
Readers will also enjoy ""The Seven Pillars of Business Analytics Success"", an outstanding analytics framework that a CEO can get behind, yet the individual analyst or manager can also use as a blueprint to take analytics to the next level at her organization. 
,
, is a Global Vice President BI & Predictive Analytics Monster Worldwide, an internationally recognized speaker and an expert in advanced business analytics.   "
"
,By Alex Jones, June 2014. 
,
For years, business leaders and IT organizations have had a ,. On one hand, IT organizations struggle to keep up with rapidly changing technology, poor communication, talent gaps, and unforeseen challenges. While on the other hand, business leaders face project completion dates that seem to slide across the calendar, cost overruns, and an ever increasing appetite for technology to support and drive business.,
,
Now, with the rise of ""Data Scientists"" we see the same dynamics play out--- but this time, the new ""must have"" tech project is people, not software or systems. The parallels are humorous and hopefully shed some light on the confusion that often arises between IT and Business.,
,
Let's explore the development of business analytics and data science.,
,
In 2011, McKinsey published , and in that one report hundreds of companies put on their running shoes and joined the race for analytical talent. Although this was certainly not the first report on the subject, it served to illustrate the business value and potential opportunities that are available (and not just to technology companies).,
,
After describing the opportunity, McKinsey highlighted their forecast that there would be a severe shortage of ""Deep Analytical Talent"" by 2018 and they outlined the type of professional that would be needed to implement big data initiatives.,
,
,

Just a few months after McKinsey's report, DJ Patil and Jeff Hammerbacher wrote ,, in which, they discussed their experiences at LinkedIn and Facebook.,
,
More importantly, DJ Patil gave McKinsey's ""Deep Analytical Talent"" a name--- Data Scientists. Below is an excerpt from the article describing how the title came to be:,
,
,
,
With title in hand, DJ Patil added some key characteristics to look for in a Data Scientist:,
,
,
,
For the executive looking for a ""Deep Analytical Talent"" this article was a welcome expansion to the job description. However, much like IT projects, it also increased the expectations for what a Data Scientist was suppose to be.,
,
From there, the floodgates opened. Over the next few months and years, the job description expanded into what is often referred to as a ,.,
,
Everyone had a few things to add. Below are just a few examples. (Sources: Drew Conway's ,, ,, Drew Tierney's ,),
,
,

,

,
,
With each iteration, Data Scientist began to look more and , and less like ""Deep Analytical Talent"".,
,
Ironically, the expanded expectations of Data Scientists are a product of their own success. The ability to advise executives, understand deeply technical problems, communicate, ,illustrates that business leaders see Data Scientists as a bridge that can finally align IT and Business in a much more permanent and productive manner.,
,
Unfortunately, many technical focused professionals, see the obligation to develop business skills as a trivial and unneeded task. However, it doesn't have to be!,
,
In an effort to bridge the gap between Business and IT, I expanded from simply looking at the , to developing a concise guide to get IT professionals and business leaders to structure the problem solving process (which can be challenging after spending a few hours data cleansing or coding) and ensure that both groups understand the strategic need for a project.,
,
In some cases, simply structuring out the problem and working through the objectives can be enough to come to a quick and simple solution.,
,
,
,
Essentially, this serves as a basic framework for working through business problems. Although it won't make you a strategy expert, it will help to advance the conversation, align your goals with the business, understand the expected return (which can help to prioritize between projects), or even speed up talking points for a meeting so that you can get back to the comfort of coding.,
,
As you would expect, it flows from top to bottom. ""Basics"" serve as a reminder of the steps to consider with any problem. For instance, the first item "",is a reminder that spans customer/ revenues/ products/ etc (ie: segment out your customer base, breakdown products by volume and profit margin, segment revenues by streams, etc).,
,
From there, Finance, Marketing, and Operations are very simplified and loosely defined groupings that help to guide discussion. All of which leads down to the ultimate goal of any project--- the profits!,
,
Here is a link to a , of the cheat sheet.,
,
Original LinkedIn post ,.,
,
, is a Graduate Student at ,.,
,
,
,
,
,
 ,  "
"
,
,
, is part of the ?€?Customer Connect Data Science Engineering?€? team at eBay that transforms customer service data into actionable information. She is working on topic and sentiment models to identify and summarize customers?€? pain points from user feedback. Samaneh Moghaddam holds a PhD in Computer Science with a thesis on aspect-based opinion mining. She has published several papers in the area of sentiment analysis in the top international conferences such as WWW, ACM SIGIR, ACM CIKM, and ACM WSDM. She has also, presented tutorials at ACM SIGIR 2012 and ,WWW 2013 to introduce this research area, to the community.
,
Here is my interview with her:
,
,
,
, Mining opinions at the document-level or sentence -level is useful in many cases. ,However, these levels of information are not sufficient for the process of decision-making (e.g. whether to purchase the product). In a typical review, the reviewer usually writes both positive and negative aspects of the reviewed item, although his general opinion on the item may be positive or negative. Aspect-based opinion mining addresses the needs for detailed information.
,
Given a set of reviews about an item (e.g., product, services, organization, person, etc.), the task is to identify the major aspects of the item and to predict the rating of each aspect. Aspects (also called features) are attributes or components of the item that has been commented on in a review, e.g., display, battery life, zoom for a digital camera. Estimated rating of an aspect is a numerical value (e.g., in the range from 1 to 5) indicating the quality of that aspect. A sample of extracted aspects and their estimated ratings for a camcorder is shown in the following figure:
,
There are various challenges that make the problem of aspect-based opinion mining hard. For example, there are many aspects/sentiments that are understandable for a human reader but hard to be extracted by a machine, e.g., ?€?fits in my pocket pretty easily?€? that implicitly implies positive sentiment about the aspect size. Another challenge is noisy information as reviewers normally include a large amount of irrelevant information, e.g., opinion about the manufacturer. Finally, identifying aspects and ratings for cold-start items is a critical and challenging problem.
,
,
,
, Items with few numbers of reviews are called cold start items. In real-life data sets a large percentage of items are cold start (in some data set around 90% of items). A cold start item can be a recently released item (a new smart phone), a rarely reviewed item (an Inn resort in a small city), a very unique item, etc. Our experiments on three real-life data sets [1] show that the distributions of number of reviews per items and number of reviews per reviewers follow a power law. ,
,
,
,
,In the last decade, several latent variable models have been proposed to address the problem of aspect-based opinion mining. All of these models are applied at the item level, i.e., they learn one model per item from the reviews of that item. Learning a model per item is logical as the rating of an aspect depends on the aspect quality, which usually differs for different items. However, an issue that has been neglected in all of the current works is that latent variable models are not accurate if there is not enough training data.

,
Factorized LDA is our proposed solution to address the cold-start problem. FLDA is a probabilistic model based on LDA (Latent Dirichlet Allocation) that models not only items but also reviewers. This model makes the following assumptions:
,
, ?? ,
,This model assumes that both items and reviewers can be modeled by a set of latent factors. Item?€?s/reviewer?€?s factors represent the item/reviewer distribution over aspects and for each aspect its distribution over ratings. Each review in the FLDA model is generated based on the learned factors of the corresponding item and reviewer. It first samples aspects in a review from the aspect distributions of the corresponding item and reviewer, and then generates the rating of each aspect conditioned on that aspect and the rating distributions of that item and reviewer.
,
For cold start items, the aspect and rating distributions are mainly determined by the prior aspect distribution of the category and the rating distribution of the reviewer (or the prior rating distribution of all reviewers), respectively. For non-cold start items, the aspect and rating distributions mainly depend on the observed reviews of that item.
,
,
,
,
,  "
"
,
,
Here is my interview with 
,, co-founder and CEO of RapidMiner. 
,
Ingo Mierswa is an industry-veteran data scientist since starting to develop RapidMiner at the AI Division of the University of Dortmund, Germany.  Mierswa has authored numerous award-winning publications about predictive analytics and big data.  Mierswa, the entrepreneur, is the founder of RapidMiner.  Under his leadership RapidMiner has grown up to 300% per year over the past five years.  In 2012, he spearheaded the go-international strategy with the opening of offices in the US.
,
,
,
,: Everything at RapidMiner follows three simple principles: 
,, collaboration and simplicity. ""Predaction"" represents the fact that, contrary to popular belief, 
,
,
""What is the weather predaction for tomorrow?"" - ""I will bring my umbrella!"" 
,
,
The value is not in the knowledge that it is going to rain; the true value lies in determining the best option for you when this happens so you can be prepared. Should you stay home? Do you bring your umbrella? RapidMiner is a platform that can create those millions of predictions and trigger the right business actions as a result.
,
,  means that we offer teams of people with different backgrounds a white board on which they can express their ideas on data integration, transformation, and modeling and turn them into reality with a single click. It's true that RapidMiner makes predictive analytics easy for business analysts since the product requires no programming and users can build analytics processes with a drag and drop interface. And indeed, across our customer base, the business analyst is often in the driver's seat. But if business users can contribute the business problem, then data scientists are freed up from standard tasks and can focus on the specialized algorithms where needed, and IT professionals can contribute the data and control access rights. RapidMiner empowers such a team to effectively reach the best solution together. 
,
,
We identified a speed-up of up to a factor of 40-50 times compared to pure scripting approaches for data integration, transformation, modeling, deployment and maintenance. It is amazing how RapidMiner achieves this ease-of-use and performance gain given that it supports more than 1,500 analytical operations, including hundreds of methods for data integration, data transformation, data modeling and data visualization - with access to data sources including Excel, Access, Oracle, IBM DB2, Microsoft SQL, Sybase, Ingres, MySQL, Postgres, SPSS, dBase, text files and more.
,
,
,
, We're proud of our open source roots and have a very large community of dedicated users that create add-ons to our software, which we will continue to support. But a pure open source-based business model poses a lot of challenges - and handling of intellectual property rights is only one of them. It is important to find a balance between being open and embracing innovation as well as offering extra value for paying customers. 
,
Over the last six years, we evaluated multiple open source business models before settling on our existing 
,. We started with a pure open source model where the software was completely open and customers would only pay for guarantees and technical support. While this might work for mission-critical infrastructure parts like Red Hat's operating system, it is not a good model for business applications like RapidMiner. We then moved on - together with other large open source vendors like Jaspersoft, Pentaho and Talend- to offer our software in an open core model where the core of the software was open and freely available but customers had to pay for premium features.
,
But this model has disadvantages as well, including the fact that it is disconnected from the original concept of open source since those premium features will never be available for the community. It also poses challenges in product strategy since each feature could only support either community growth or conversion, but not both.
,
Our business source model represents a perfect balance of this community support and a scalable business model. The idea behind business source is incredibly simple: the latest and greatest version of RapidMiner is available under a freemium model, while previous versions are available under an open source license.
,
Popularized by Michael (Monty) Widenius, one of the founders of MySQL and an investor in RapidMiner, business source is a commercial software license model that offers many of the benefits of open source, but with a built-in time delay on users being able to access new versions of our products. This allows us to deliver feature-rich versions of the software to all groups of users, while commercial, paid users are able to analyze larger data sets and connect the software to more data sources. 
,
,
,
, We started the development of RapidMiner under the name of ""YALE"" in 2001, and although we basically rewrote the complete product three times, we cover a history of more than ten years and have been through all phases of the market.
,
We started with a pure computation engine for analytical processes in 2001, and the first big turning point was when we introduced the graphical user interface, which was flexible enough to perform, for example, loops and cross-validation for preprocessing to measure its effect, and many other complex analytical tasks - all without the need for writing a single line of code. We achieved a flexibility known from programming languages like Base SAS or R but combined it with the ease of use of a SAS Enterprise Miner or Clementine, which later on became SPSS Modeler.
,
Believe it or not, the second turning point was the creation of a Windows installer which helped to overcome the technical hurdles of installing RapidMiner and the necessary underlying technologies. This doubled and tripled our community growth immediately.
,
The next significant phase was the development and release of RapidMiner 5 in 2010. We made a complete paradigm shift from our process trees to workflows. However, at the same time, we kept RapidMiner-specific features like the ability to propagate metadata through the process and detect errors during the design phase. This was also a major turning point towards usability and analyst support. With RapidMiner 5, we also introduced our Server product which allowed for remote execution, scheduling and one-click integration into business processes.
,
The fourth and fifth key milestones took place in 2013. The fourth was the change in our business model to business source, which I described previously. And fifth, the release of RapidMiner 6, which included our new ""accelerators"" that allow users to get the first predictions and predictive insights within five minutes after installation. All users have to do is pick a problem type like ""churn"" and throw in their data, and RapidMiner builds the necessary process. Users can then use this process as a starting point for optimizations, add more of their own data, or deploy it directly on the server for integration. Accelerators are a perfect blend between the ease-of-use of a shrink-wrapped vertical solution and the power and flexibility of a platform like RapidMiner.
,
,
,
, In a broad sense, cyber-crime covers all types of crimes committed with the help of computers and networks. Typical examples are fraud detection in transactions, phishing attempts, identification of offensive content, attacks against computers or networks and even spam, which is considered unlawful in many jurisdictions.
,
,
,
We have seen applications of RapidMiner in all of these named examples. RapidMiner is often used as a central part of an intrusion detection system to analyze web logs for unusual patterns. We have also seen a lot of applications in the field of fraud detection, where both supervised and unsupervised machine learning methods of RapidMiner have been modeling fraud cases in financial transactions, and also in insurance claims, mainly in the health insurance sector. 
,
Additionally, RapidMiner is used in the banking sector to identify logins that are a consequence of a phishing attack. All of these examples clearly show how predictive analytics, and RapidMiner, can help to identify cyber-crime. But we are still not seeing a ""true"" predictive approach since all identifications are re-active in nature. Personally, however, I would not like the idea of the movie Minority Report becoming a reality, so keeping a human being in the loop is the way to go.
,
,
,
, I am surprised at how many users are deploying RapidMiner for sports analytics. This was definitely unexpected and came as a big surprise. We have seen applications of RapidMiner in sports betting, in predicting the winner of the World Series or the World Cup, and for identifying underrated players on the transfer market. We have even seen the use of RapidMiner for creating predictions during the games themselves, such as predicting the most likely landing zones for a certain combination of a pitcher and batter in baseball.
,
,
,
, We constantly see cases where optimal results are achieved when each member of an analytical team can focus on their key strengths. R programmers are true data scientists and as the name ""scientist"" suggests, they are best when they focus on the creation of a new algorithm for a complete new task. But in order to do work on this time-intensive task, they need to be freed from standard tasks related to data integration, transformation and visualization.
,
One of our customers in the automotive sector has optimized this approach. The complete team for advanced analytics is working as a center of excellence and supports multiple other business units with predictive intelligence and prediction-based action. The team consists of approximately 10 people with the majority being business analysts, plus one database expert and one data scientist creating new algorithms. In just two weeks, this team created and deployed the overall workflows for predictive maintenance of their machines. The data scientist used R to create an ensemble of methods using the preprocessed sensor data coming from RapidMiner. The overall workflow, as well as the notifications, are all governed by RapidMiner while the actual prediction was created by a specialized new algorithm created in R. This is a perfect example of how a collaborative team can blend the easier maintenance and reproducibility of RapidMiner with the programming possibilities of R to efficiently achieve a solution within a shorter time frame.
,
,
,
, I think the biggest challenge for our field, as I mentioned above, is that many organizations believe that they can only rely on data scientists to work on predictive and advanced analytics. We are focused on changing this paradigm by empowering a team of experts with different backgrounds to help us achieve the results we're looking for, and other organizations should be using this approach as well in order to be successful. 
,
,-----,
Ajay Ohri is the founder of
, and the author of , (Springer 2012).
,
,
,  "
"
,
,
,??,??
,??,??
,??,??
,
,
,??,??
,
,  "
"
,By Jean-Paul Isson,?? Monster.com, June 2014.,
,
Data is the foundation upon which analytics can have an impact on your business. However, data is necessary but not sufficient in order to unlock business value for your organization. For this, actionable insight is required. I speak to business leaders on a regular basis about their data assets and challenges. It never fails?€?a common sentiment I hear consistently is that their organizations are drowning in data but lacking in understanding and action from that data.,
,
Said a different way, they have a lot of numbers and stats but aren?€?t really sure what they mean.,
,
Providing analytical services is somewhat like being a professional translator, helping to convert numbers and trends into something the business can understand and act upon.?? However, often analysts are very comfortable deep in the data and not as comfortable translating that data into action.,
,
How do you get your analysts to pull their heads up from the data and focus on the business? It?€?s not always an easy task, and it?€?s a bit of an art and a science. Based on experience leading?? and building analytics team from the ground up , as well as input I received from researching successful analytical organizations, I developed the IMPACT Cycle to guide your analysts through the process of ensuring they are insightful business partners, rather than just purveyors of data.,
,
,
,
,
The IMPACT Cycle provides the analytical professional with a guiding framework for thinking about the steps of being an effective analytical consultant. It can also be a tool to help you drive effectiveness through your analytical teams.,
,
For more on the IMPACT Cycle and effective use of business analytics, check out our book, ,
,
, is a Global Vice President BI & Predictive Analytics Monster Worldwide, and the author of ,.?? He is an internationally recognized speaker and an expert in advanced business analytics.,
,
,
,
,
,
,
,
,
,
,
,
,
 ,  "
"
Most popular 
, tweets for Jun 25-26 were
,
,  "
"
,
,
We analyze the lists included in the CRN Big Data 100 to find trends related to Big Data companies.,
,
First, we will look at the content in the descriptions of these companies. Below is a word cloud generated from the descriptions of the companies on the CRN Big Data 100 lists.,
,
,
,
Ignoring the obvious words like software, information, technology, etc., some standout words include Hadoop and NoSQL. In fact, 21 out of 100 companies in the CRN Big Data 100 contain the word Hadoop (most notably 11 out of 25 Big Data Management companies). This is just evidence of how integral Hadoop has become in the infrastructure of many companies that handle big data.,
,
NoSQL is an interesting term because it is also quite common, being featured in descriptions of 7 out of 100 companies. These companies include Amazon with their highly-scalable DynamoDB and the widely-known MongoDB. This, coupled with the recent ,, a major DBaaS vendor based on Apache CouchDB, goes to show that NoSQL database systems are faring quite well in today's market.,
,
Now we will look at some interesting trends in the geographical distribution of these companies.,
,
,
,
What this image makes clear is that the majority of the 92 US companies in the CRN Big Data 100 are located near either the East or West coast. What it doesn?€?t show, however, is just how active the Bay area is.,
,
,
,
Zooming in, we see here that a huge number (51) of the CRN Big Data 100 companies lie within the Bay area, with San Francisco being the single most represented city on the list. This means that approximately 55% of CRN Big Data companies were founded in this relatively small region of California. This figure grows to 67% if only considering the emerging companies.,
,
This all goes to show just how active the bay area is for big data companies, especially those that are just now emerging. It will be interesting to see if this trend continues in the future.,
,
Now, we will look at when these companies were founded.,
,
,
,
,
Looking at this chart, there are two stories that stick out.,
,The first is the lack of companies founded between 2000 and 2003. This can be accounted for by the fact that this was in the wake of the bursting of the dot com bubble, and serves as a warning of what can happen when caught up in a bull market.,
,
The second is the , number of companies (53) founded since 2008. More than half of the companies on this list were formed in the past six years, showing that industries built around big data are booming and are quite approachable to small companies with fresh ideas. The downside to this is that the space is getting very crowded, and we are likely to see some fierce competition among the various players in the coming years.,
,
The companies chosen by the CRN for their Big Data 100 are very illuminating. By keeping up with these kinds of lists, we can try to keep up with the development of what is definitely an exciting industry.,
,
,
,
,
,
 ,  "
"
,
By Sean McClure, ThoughtWorks Inc, June 2014.,
,
As humans, we navigate our lives largely by the recognition of patterns. These patterns include the sound of a mother?€?s voice, the appearance of a dangerous animal or poisonous food, the familiarity of kin, and the attraction to potential mates.,
,
Accurate pattern recognition is key to an animal?€?s survival and progress, and has allowed humans to become the socially complex and advanced species we are today.
,
,
It should come as no surprise that scientists and engineers have long been fascinated with the mind?€?s ability to rapidly and accurately recognize patterns, and much research has been geared towards attempting to recreate this ability in a machine as a demonstration of Artificial Intelligence (AI). ??One of these abilities is facial recognition; something humans do with relative ease, but which has been exceedingly difficult to mimic using programming logic and advanced algorithms.,
,
In Ray Kurzweil?€?s book , it is argued that the human brain contains approximately 300 million ?€?general pattern recognizers?€? arranged in a hierarchical pattern. ??Input signals feed through various layers, and concepts are learned in an increasingly abstract sense until a new pattern is learned or an existing pattern recognized.,
,
The closest parallel to this in machine learning are artificial neural networks (ANNs). ??ANNs use mind-inspired mechanics including simulated neurons and layering to learn concepts from experience (exposure to data). ??ANNs have been applied to a number of problems in pattern discovery including facial recognition but until recently have had limited success in real-world facial recognition systems.,
,
This is now changing with the use of Deep Learning; a set of algorithms in machine learning that attempt to model high-level abstractions using an approach more closely aligned with how our minds recognize patterns. Using a large number of layers, so called Deep Neural Networks (DNNs) can mimic human learning by adjusting the strength of connections between simulated neurons within many layers, just as the human mind is believed to strengthen our understanding of a concept. ??Each layer can model an increasingly abstract concept built from the more basic concepts learned at earlier layers.,
,
Deep Learning is breathing life back into the use of ANNs and some researchers consider the use of deep learning neural nets to be a small revolution in AI. ??What makes deep learning so attractive is its property of getting better by simply throwing more data through its networks. ??More data to a DNN is like more experience to a child. Although these networks have been around for decades, only now do we have the volume and variety of data to expose them to enough information for a DNN?€?s ?€?understanding?€? to rival that of humans.,
,
In science and industry, deep-learning computers are being used to search for potential drug candidates and to predict the functions of proteins. Companies like Google and Facebook are using their massive data stores of images and text to ,. A small start-up in Las Vegas called NameTag has developed a beta version Facial Recognition App for Google Glass. The App uses the , database to search through millions of photos, learn what features define a human face, and match them to facial features detected from the App. ??The exact use of this in , is still unknown and Google has yet to approve the use of facial recognition Apps for Google Glass.,
,
,
Facebook is moving forward with a facial recognition project called DeepFace. ??DeepFace can detect whether two faces in different photos are of the same person and is reporting accuracies that rival a human?€?s ability to do the same. ??DeepFace uses DNNs to model highly complex data and multiple features. ??What makes DeepFace so exciting is its ability to detect faces regardless of lighting or camera angle; two factors that have stumped most facial recognition algorithms to date. This could lead to new photo tagging applications and authentication technologies. DeepFace uses a 3-D modeling technique to rotate a single flat image in 3-dimensions, thereby allowing the algorithms to ?€?see?€? different angles of the face. ??Using a DNN with over 100 million connections, and its database of millions of photos, DeepFace teases out the features that can be used to recognize human faces, and uses this knowledge to discover very high-level similarities between 2 photos of the same person. Facebook has reached an accuracy of 97.35% on the so-called Labelled Faces in the Wild (LFW) dataset, and has reduced the error of the current state-of-the-art by more than 27%, approaching human-level performance.,
,
The advantage of DNNs over other learning approaches such as Support Vector Machines and Linear Discriminant Analysis (LDA) is its ability to scale to extremely large datasets; something required for problems in speech and facial recognition. This means large, inexpensive clusters of computers are required to process and calculate all the data and make learning using DNNs feasible. ??In the now famous ,, Google scientists used 16,000 processors and the internet as a data source to recognize the appearance of a cat. ??What makes the experiment so game-changing is that the algorithms were ever told what a cat is or looks like. The DNN came up with the concept of a cat all by itself.,
,
Many researchers and start-ups will be jumping onto the DNN bandwagon to see where we can push machine learning. Any application requiring the most important features to be detected among myriad variables is a potential candidate for DNNs. Neural networks have two big advantages; they deal well with many ill-defined features, and they scale efficiently, meaning extremely large datasets can be used. This is the latest approach to machine learning and the true point to Big Data; that throwing more data at the problem bypasses the problems associated with traditional statistical analyses on small datasets. Although the somewhat blackbox nature of DNNs take us further from understanding true causality, they do offer the exciting opportunity to mimic human intelligence and automate more sophisticated tasks. More abstract areas like psychology may soon see benefits in the application of DNNs in both targeted marketing and in helping us understand how the human mind works.,
,
With the application of DNNs to facial recognition, a new era of AI is being ushered in and will lead to new and exciting technologies. It is already raising, however, some ethical concerns surrounding privacy. ??The use and acceptance of such technology will bear out over the next few years.,
,
,
,
, Sean McClure, PhD, is a data scientist at ,, a global technology company solving some of the world?€?s toughest problems. Learn more about the , practice at ThoughtWorks.,
,
,
,
,
,
,
 ,  "
"
,
,
, is part of the ?€?Customer Connect Data Science Engineering?€? team at eBay that transforms customer service data into actionable information. She is working on topic and sentiment models to identify and summarize customers?€? pain points from user feedback. Samaneh Moghaddam holds a PhD in Computer Science with a thesis on aspect-based opinion mining. She has published several papers in the area of sentiment analysis in the top international conferences such as WWW, ACM SIGIR, ACM CIKM, and ACM WSDM.
,
,
,
Here is second part of my interview with her:
,
,
,
, At eBay, I am part of the Customer Connect team. The Customer Connect team is in charge of transforming customer service data into actionable information. I am mainly working on sentiment analysis models to identify and summarize customers?€? pain points from user feedback. The project that I am working on currently is mining and summarizing actionable information from eBay App reviews. Our goal is to make the experience easier and more favorable for our app customers.
,
,
,
, Sentiment Analysis or opinion mining represents a large problem space. Sentiment analysis can be done at document-level (e.g., opinion spam detection, opinion quality and helpfulness estimation, etc.), or at sentence-level (e.g., opinion question answering, opinion mining in comparative sentences, etc.) or at the phrase level (e.g., aspect-based opinion mining). Some challenges like dealing with noisy information or co-reference resolution are common in the whole problem space. However, each specific problem has its own challenges. 
,
One of the main challenges in the area of aspect-based opinion mining is identifying implicit aspects and sentiments. There are usually many types of implicit aspect expressions in a review. Adjectives and adverbs are perhaps the most common types because most adjectives describe some specific attributes or properties of items, e.g., expensive describes ?€?price,?€? and beautiful describes ?€?appearance.?€? Implicit aspects can be verbs or complex phrases too. Although there have been some works considering extraction of implicit aspects, further research is still needed. 
,
,
,
,
,
, I got my PhD in computing science and I always enjoyed finding and solving challenging problems. In fact, finding and defining a problem satisfy my curiosity and attacking and solving it satisfy my creativity. I usually take time to learn what needs to be solved, and how it will be used and by whom. When I come up with a clear problem definition, I attack it by studying the relationships between the data. Finally, I attempt new problem-solving approaches and potential solutions. 
,
,
,
, ,
,
,
,
, Strong background in statistics, algorithms and machine learning as well as knowledge of diverse technologies such as Hadoop, Java, Hive, Pig, Python, R, etc. Innovation, problem-solving skill and presentation ability are also must-have. Depending on the topic that one is working on, other skills and knowledge will be necessary. For example, a scientist in the area of sentiment analysis/opinion mining, needs to also have strong background in natural language processing, information retrieval and text mining.
,
,
,
, I recently read ?€?How to talk so kids will listen & listen so kids will talk?€?.  I am a new parent and I found this book really helpful. I recommend every new parent to read this book. ,
I love spending time with my family. I also like reading short stories, biking and hiking.
,
,
,


  "




"
,
By Gregory Piatetsky,  
,, Jun 27, 2014.
,
,
has many active discussions, and recently 
one such discussion was prompted by a question from Alok Sharma:
,
,
,
This discussion is now going for 4 months, and got responses from many leading data scientists and professors, including 
Mark A. Biernbaum,
Goutam Chakraborty,
Michael Fahy,
Myles Gartland,
Vincent Granville, 
Daniel Dean Gutierrez,
Steven Miller,
Greta Roberts, 
and myself.
,
The consensus seems to be that good practical skills can take the place of a MS degree, but there are many interesting comments and practical tips - see below.
,
In case you are interested in Masters, here are 
, options, including
,
,??,
Here are selected and most interesting answers from the 
, in answer to the question:
,
,
,
, To learn data science -- absolutely. A few schools are building undergraduate programs that will be akin to a computer science degree. You will learn core skills, but they won't make you a scientist who will be advancing the field.
,
,
, Remember that Data Scientist is just a Title. (A media hyped title) Some give themselves or have this title because that's the work they do, not because they have a particular degree. Some may hold degrees in Statistics, Mathematics, Computer Science, the disciplines vary.
,
You can learn data science anywhere. No single Masters Program could cover all the disciplines needed in significant depth for one to be an expert in all these areas. Selecting an area or two or three and having depth and expertise in those is common. Many companies do not have just a ""Data Scientist"" but teams comprised of experts from the different disciplines.
,
While some institutions are offering or creating Masters Programs with this title, most of the current field of Data Scientist have no such Degree. Check out the following link , to see a list of disciplines considered within the Data Science area.
,
The new Columbia Data Science Institute is offering a new Masters in Data Science Program , as well as a certificate program for those who already hold advanced degrees.
,
There is the Johns Hopkins University Online Data Science Certificate program available on Coursera ,,
,
Experience and doing in my opinion is the best way to become a ""Data Scientist"". There are many ways to do this with or without an advanced degree program. There are many doing great ""Data Science"" work under other titles.
,
,
, I agree, Data Scientist is just a title. A good Masters to get you in the arena is Operations Research. My experience was, I had to understand the system to apply the math or tools, not just how to do the math or work the tool. Good luck.
,
,
, There are many analytics certificates - see , . I also recommend you take part in some Kaggle competitions - a good result there shows your competence (but it is not easy - competition is stiff!).
,
,
,You may want to check out the hands on / practical / certificate that comes with a SAS certification at the end (a very valuable certification). All done online - in your own time. 
,
,
,
,
Big data knowledge is not very difficult to obtain, and anyone with some needed pre-requisites like existing knowledge of statistics, programming and databases concepts can become a big data professional
,
Based on job requirements, the skills in most demand are Hadoop/Big Data, tools including R and SAS, and some domain knowledge. Theory is assumed as a prerequisite, but usually good data selection and engineering is more important than advanced algorithms.
,
However, there is a strong demand for analytic talent and a shortfall in supply. If you have a master's degree, it will be add on for you but if you don't have, many companies will overlook this as long as you have the right skills.
,
Please check the below link for India's top 10 Analytics institutes.
,
,
,
,    I was at a Meetup last night where a guy, a ""programmer for 31 years,"" said that a last year he decided to call himself a data scientist. He wanted to take advantage of the new hype. He said it was really easy to become a data scientist. He started by taking Andrew Ng's class in machine learning through Coursera, but was ""destroyed"" by the class and had to drop. Then he took the Coursera ""Computing for Data Analysis"" class, 4 weeks to basically learn R. Then he took an expensive Data Science on-premise classes. And voila! A data scientist is born.
,
Having an academic background in data science, I'm hard pressed to call this gentleman a data scientist. I think it takes more than a couple MOOC classes and and more time to take on that moniker.
,
,
, The person Daniel refers to may be a good data analyst/coder, but not yet a Data Scientist. Knowledge of R , Python or other tools is secondary to knowing how to approach the data, how to ask right questions, and good intuition about what works and what not. Those skills are critical to a good data scientist, but take more than a few weeks and
,
,
, So what if you have undergrad degrees in History, Statistics, and CS, and then advanced degrees in CS and/or SE. Experience doing actuarial analysis, financial fraud analysis, failure analysis, BioStatistics, HUMINT, OSINT, and organizational theory analysis. And experience developing software for HPC systems, information management systems, DBMS (so a lot of information theory app), etc. in languages like C/C++, PHP, Java, Python, R, and Julia. And, of course, a natural curiosity on how things work and the ability to hire and manage other folks who also have a passion for information. And finally, have run a business and made business decisions. So does someone like this qualify as a Data Scientist? Just curious...
,
,
,    I think people are really getting way too hung up on the term ""data scientist"". Due to it being completely nebulous as to what exactly it is at this point I would say it is merely a buzzword that may or may not end up getting a hard definition in the future. Speaking as someone who has minimal experience in the field and currently enrolled in Northwestern's MSPA program, I can confidently say that when I complete my degree I will NOT be a data scientist (if I had years of experience prior to beginning the program I might be singing a different tune though). However, I do believe it will give me a solid foundation to build upon.
,
In the end, I believe it requires a combination of years of experience and a relevant degree. Furthermore I would say a certain amount of aptitude is also required. I'm sure there are people with a B.S. in Computer Science that have been doing analysis for over ten years that can and should be called data scientists just as there are PhD's out there who should not.
,
So to conclude I believe it is wise for newcomers like myself just to become excellent at data analysis/mining and not worry about monikers - those will come naturally once a certain degree of success has been achieved. That's my two cents at any rate.
,
,
, Jim, what you describe is a ""unicorn,"" something many companies are seeking when hiring a data scientist.
, Matthew ... as one long-time ""data scientist"" I love the new term for what I do. I think it aptly describes what I do, what I've always done, with data.
,
,
,
Masters will eventually change and adapt. I wouldn't be surprised that some organizations/companies will soon offer a solid master, at almost no cost, online and on-demand. We are actually working on delivering such a high-quality training to practitioners with a quant background. The idea is to help interested candidates acquire all my useful experience and knowledge gathered over my 25 years career, spanning across multiple continents and various data science roles (Visa, Microsft, eBay, Wells Fargo amd start-ups), in a compact format delivered online on-demand in less than six months.
,
,
,    One doesn't become a data scientist overnight. You need to take it step by step especially if you are new to the whole analytics/data science show. To become a data scientist you need to be hands-on on various tools and technologies and these vary right from the basic MS Excel and SQL to statistical software like SAS/R/SPSS, languages like Python, Perl, C++, Java etc. and technologies that can handle Big Data like the Hadoop ecosystem. Apart from this you would be expected to have good knowledge of business to be able to eventually bring out the insights from the data.
,
To take a step by step approach an expensive Master's degree may not be the best solution as no degree will eventually cover all these requisites and there will always be newer tech coming up. The best way would be to take a modular approach in learning all this stuff through short, inexpensive certificate courses. After all it is the knowledge which matters and certificate courses probably provide more hands-on, practical knowledge at a cheaper price than a Master's.
,
Check out some certificate course providers in analytics here: 
,
,
,
,    Thank You Sal, Greg, Daniel, Vincent, Atash & others. Your Comments were really insightful. I think I will start off by taking up Data Analysis courses on Coursera followed by industry relevant course in analytics and work on developing my knowledge until I land up to a relevant job.
,
,
, To me it is also like asking do I need have a graduate degree to be a CEO. Well, no. A data scientist does not require any licensure- so technically you need so specific credential. Your degree usually gets you in the door, and your skills let you keep and excel at your job. All that said, you do not need a graduate degree to DO the job, but you might need one to GET the job (look at many of the job postings and their requirements).
,
,
,    If you don't need to sell something to someone (a real human - like selling yourself to get a job), but instead generate revenue via automated data science systems that do not require human interactions (stock trading, various arbitraging systems including keyword bidding, sport bets, data science publisher generating revenue via Google Adwords, some types of hacking), then you don't need any diploma or certifications. Not even high school, not even primary school.
,
,
,
This is a great discussion. I have taught, advised, counseled more than 500 students (in last 10+ years) of our graduate certificate program in data mining at Oklahoma State University (analytics.okstate.edu). Most of our students work in the field of data mining, predictive analytics, marketing analytics, web analytics, marketing science, data science ..... After having talked to 100's of major corporations and employers, I feel a data scientist (wanted by a company) is someone with a ""multiple personality disorder"" who can still function well! This person has knowledge and abilities in
,
,    1. Programming (SQL, Python, Java, .....) and exposure to big data via Hadoop, MapReduce...
,    2. Statistical and numerical models along with ability to do visualization and optimization (using multiple software platforms including proprietary such as SAS and open-source such as R)
,    3. Have domain expertise to understand how all these apply in the context of a business to cerate value
,    4. A good communicator so that the person can explain the models to users who are unlikely to accept the models if they do not understand them
,    5. A person with curiosity, determination, team-player, leader... you name it.
,
So, can you develop all these skills in one course? or a short program? NO!
,
How about through a series of well-designed courses (not 1 or 2 perhaps 4 or more spread over a span of 1-2 years so you have time to assimilate knowledge and put those to work) that build on each other plus hands-on experience in working with complex data and models - Yes (that is what we do at our graduate certificate program for working professionals taking courses via online). But, of course, I am biased because I run the program.
,
,
,
I completely agree that ""Data Scientist"" is just a title. A Master's degree can be beneficial, however, a set of courses that provide a balance between theoretical depth and practical breadth would be ideal. I would highly recommend Dr. Chakraborty's Graduate Data Mining Certificate program at Oklahoma State University. Having completed it myself, I know first hand that the program provides a solid theoretical foundation, a lot of experience with publishing/presenting in industry fora, and competing in industry sponsored competitions such as the annual SAS analytics shootout. It is available online, and so its quite practical for full time professionals. Hope this helps answer your question.
,
,
,
You need a combination of Mathematics, Statistics and Computer Science.
,
Here is a sample list of courses from our MS degree in Computational and Datat Sciences at Chapman University:
,
,??,
,
, to pile on to Michaels and Daniel's list- lets not forget context, domain and communication. A few classes in communication and basic business (assuming they will work for one) will go a long way too. People sitting in cubes writing models without understanding of businesses questions and ability to communicate their results lose some of their value.
,
,
,
The current crop of Data Scientists has learned the work on the job. A lot of great research work is learned on the job. A Master's or credential program could create problems for the person obtaining them once they get on the job and find that the work is as much experience as it is education.
,
,
,    If you are a self-funded entrepreneur, you don't even need a primary school education, not even kindergarden: your education and job title do not matter, only your capacity to generate value and profits. In my case, though having a PhD, I never mention my degree - nobody (except time-wasters) is asking anyway. Indeed, ignoring people asking about my (real!) credentials has been one of the best strategies to stop wasting time in discussions going to nowhere.
,
,
,
Except for companies that have just started doing a few things in analytics and dream of a hiring a non-existent master of all trades, experience is a more valuable thing than a Masters or any other degree.
,
When I started out sometime in 2003 (in India), it was called Research and Analytics because most of the analytics work were linked with marketing research studies/surveys. Quantum and SPSS were very popular software then, while sampling and significance tests were the most used analytical techniques. I learned all of these on the job.
,
Sometime around 2007, I had to learn SAS programming because the new company I joined, uses SAS for all their analytics projects. I also learned techniques like logistic regression, cluster analysis, and factor analysis at this company.
,
During 2009 till 2012, I worked on Netezza and Teradata to extract and acquire the data I needed for my predictive modeling and other analytical projects. While I continued using SAS, I had to learn SPSS Modeler because one of my biggest clients uses SPSS. I also became very good in a lot of statistical/data mining techniques - Decision Trees, Regression Models, Time Series, Mixed Models, etc.
,
And finally in my current role (starting 2012), I learned MS SQL and Tableau. I also did my first and very challenging SAS Macro programming which is more than 800 lines of code and will accomplish the same tasks that used to take weeks, in about a day.
,
All the software/programming and the statistical/data mining techniques were not learned in college or a formal coaching environment. Most of the times, it was searching and reading on Google, a good discussion with the team, and in a few cases, a colleague or someone senior who will help when the right questions were asked. What I want to say is - the media, some companies and their 'executives' try to make everything sound very technical and critical/essential - R, Hadoop, Big Data, MongoDB, Deep Learning, etc,. etc,. - but at the end of the day, you can and will learn anything when there is a requirement. And sometimes, there will be someone who will push you into the water, someone who will make you take the first step when you have your doubts.
,
So, my answer is no. :)
,
,
 ,  "












"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 30 and later.
,
See full schedule at , .
,
,  "

"
,
,
, is Innovation Strategy Consultant for Fujitsu Laboratories of America. There he has led projects ranging from digital negotiation systems to sensor-based healthcare, from automated ontology generation to novel interface methodologies.
,
Dave has worked as a writer and producer with WGBH?€?s Nova Science Team, served as a Mellon Visiting Professor at Caltech, and been involved with many Silicon Valley Startups. This includes his role as a founder, VP Production, and lead creative at Worlds Inc., and founder and VP Marketing at Disappearing Inc.
,
Dave is a Fellow of the Internet Archive. He was selected as one of Time Magazine?€?s 2001 ?€?Digital Dozen?€? ?€? one of the 12 most influential people in the digital world. He has over 70 patents granted and pending.
,
Here is my interview with him:
,
,
,
, I don?€?t want to get too hung up on semantics here. The key idea is that currently people think of ?€? yes, even define ?€? sentiment analysis in terms of the technologies underlying it. And these technologies are Natural Language Processing (NLP) and Computational Linguistics. (Take a look at the Wikipedia entry and you will see what I mean.)
,
,But technologies change ?€? quickly these days. ,This can be derived in a variety of ways from a variety of data streams. What someone might call ?€?Modern Sentiment Analysis?€? will simply involve a much broader array of technologies for figuring that out.
,
I propose that some of these other technologies may include bio-monitoring systems. Current heart monitors provide data that enable tracking psychological stress levels reasonably well. There are even several headsets coming on the market that claim to track a variety of brain states ?€? including attention, focus, engagement, interest, excitement, affinity, and relaxation. As these devices get smaller and cheaper they will find an audience. In combination with other sensors (like GPS or eye tracking) brain states can be contextualized. This will provide fertile ground for tracking subjective states in entirely new ways ?€? and go way beyond just processing blogs, product reviews, and tweets.
,
,
,
, To me it is a statement about priorities. We should be putting the people first ?€? using technology in service to people and to the society that they are a part of. At one level the shift is subtle. At another level it can completely change your perspective. For example, instead of thinking of sentiment analysis in terms of a technology such as document analysis, I prefer to think of it in terms of understanding people?€?s subjective states. It sounds simple enough, but it opens up lots of possibilities for innovation. It gets us out of the box of technique and enables us to ask the important questions ?€? questions about making people?€?s lives better.
,
,
,
,
,
, Although the majority of people don?€?t think of themselves as being ?€?instrumented?€? with sensors, anyone with a smartphone already is. These devices are loaded with very sophisticated sensors ?€? accelerometers, magnetometers, GPS sensors, multiple cameras, and, of course microphones. If you broaden the definition of ?€?sensor?€? a bit to include systems with the capability to track usage patterns (of email, texts, telephony, web browsing and other apps) we are both heavily and intimately instrumented. So, in a way, sensors have already achieved mainstream adoption with smart phones as the vehicle.
,
The reason people don?€?t see this clearly is because they have a preconception of what it means for people to adopt mobile sensors ?€? and they see it primarily as health and wellness monitoring. Even though lots of mobile sensing is already going on it is somehow invisible.
,
That said, I do think the monitoring for health and wellness will take hold. I often argue that you wouldn?€?t drive a car without instrumentation (fuel gauges, speedometer, engine lights etc.) Once the technology is there people will feel the same way about instrumenting their own bodies. It?€?s just a question of the cost benefit ratio ?€? with cost including money, but also the hassle involved. The benefits include everything from better health and lower insurance rates to public demonstration of social responsibility, community membership, and even identity construction.
,
All of this will require smaller / better / cheaper sensors to drop the various costs and richer analytics and services to increase the benefits. Both are happening rapidly.
,
,
,
, Improving the efficiency of existing systems is probably where we will see the opportunities first. Smoother traffic flows as cars communicate, more efficient electric grid utilization as appliances and homes share data, less waste as raw materials move through supply chains, and so on. These applications don?€?t require a paradigm shift on the part of customers. It just lets them do what they are already doing with greater efficiency and lower cost. Hence adoption is likely to be quick.
,
The bigger and more disruptive opportunities are harder to predict ?€? since they aren?€?t simple extrapolations of what is already going on. While I?€?m not sure what form they will take, I do believe that connecting big data and ubiquitous sensing to the dynamics of our social networks will be very important. This is where the combination of technologies will start to redefine the limits of the possible ?€? and hence how we think about what it means to be human.
,
Second and last part of the interview: ,
,
,
,  "

"
Most popular 
, tweets for Jun 27-29 were
,
Do you need a Masters Degree to,become a Data Scientist?
,
,
Google says #Hadoop era is over, Google Cloud Dataflow can do much more, and also process #BigData in real-time ,
,
Machine learning, data mining, predictive analysis, and advanced analytics are more or less synonymous ,
,
,
  "






"
Most popular 
, tweets for July 11-13 were
,
MapR CEO John Schroeder on evolution of #Hadoop market and future overlap with OLAP | ODBMS Industry Watch ,
,
Avoid Dilbert ""core competency"": automating things that you should not be doing ,
,
,
Salesforce buys Palo Alto #BigData startup RelateIQ for $390M 4 its unstructured data/social nets search #BigDataCo ,
,
,  "
"
,
,
, is a sociologist specializing in the social organization of online communities and computer-mediated interaction. Smith leads the , consulting group and lives and works in Silicon Valley, California. Smith co-founded and directs,the??,, a non-profit devoted to open tools, data, and scholarship related to social media research.
,
Smith's research focuses on computer-mediated collective action: the ways group dynamics change when they take place in and through social cyberspaces. While at Microsoft Research, he founded the Community Technologies Group and led the development of the ""Netscan"" web application and data mining engine.
,
Smith received a B.S. in??,??from??,??in Philadelphia in 1988, an M.Phil. in??,??from??,??in 1990, and a Ph.D. in Sociology from UCLA in 2001. He is an??,??at the??,??at the??,. ??Smith is also a??,??at the??,??at??,.
,
Here is my interview with him:
,
,: Q1. Who are the primary users of NodeXL? What are some of the most memorable success stories you have heard so far from NodeXL users?
,
, NodeXL is for anyone who is interested in networks, social networks and particularly social media networks. ??Our users are often scholars, researchers, students, managers, and analysts who are interested in understanding the shape, structure, and key positions within a connected structure. ??Since societies are connected structures, people interested in understanding organizations, groups, enterprises, and markets are often interested in networks. ??The challenge has been that network analysis tools have been difficult to use; most network analysis tools are programming libraries or are too complex for casual use. ??This has meant that network analysis has been out of reach for many. ??We have focused on creating a network analysis tool built for ease of use, automation, and reporting that highlights the key features of interest in a connected structure. ??By integrating with Excel, we reach people where they often are already working. ??With NodeXL, if you can make a pie chart you can now make a network chart.
,
Some notable users and success stories come from a variety of scholars and disciplines. ??Professor Diane Cline at George Washington University is a scholar of antiquity with a specialty in the life of Alexander the Great. ??She has been able to quickly master NodeXL and use it to create some of the first maps of Alexander's social network. ??He may not have had a Facebook page, but Alexander did have a web of connections and relationships that are now easier for scholars to understand. ,
,
Business professor Scott Dempwolf at the University of Maryland uses NodeXL to map the connections formed when people author patents together. ??These networks reveal clusters of innovations that define a regional economic specialty. ,
,
Lee Rainie, director of the Pew Research Internet Project, used NodeXL to map a range of Twitter social media networks. ??Along with researchers from the Social Media Research Foundation, including myself, the research team documented the existence of six distinct patterns of network structures that regularly occur in Twitter topic streams. These six network types can help people understand the conversations in which they participate and recognize the patterns of conversations that they want to emulate.
,
,
,
,
,We are focusing on improved data importers from services like Twitter, Facebook, YouTube, Sina Weibo, and beyond. ??There is a lot of social media network data on the Internet and we'd like to make it easy for end-users to extract those networks without requiring any programming skills.
,
, ?? ,

,
,
, Like Mozilla's commitment to the Firefox web browser, we share the sense that some tools are better if they are open and free. ??NodeXL enables a larger group of people to better understand the structures and dynamics of social media - a place where hundreds of millions of people now spend a great deal of time. ??We sometimes refer to NodeXL as a ""point-and-shoot digital camera for crowds in cyberspace"". Since we want to create as many pictures of social media networks as possible, to better document the range of variation in these populations, we want as many people as possible to get and use the tool.,
,
,
,
,There are many very powerful and sophisticated network tools available. ??Of them I am always very impressed by Gephi, which is a free and open Java application that can generate very beautiful network visualizations.
,
,
,
,Social media analytics is not just for analysts. ??Most of us spend a lot of our time in social media; it's where our people are. ??But there is far more social media data than any human can consume so we need to prioritize and filter our feeds.??Analytic??tools will become mainstream as people reach for tools that??resolve the torrent of posts and messages into a focused image that reveals the key people, groups, topics, and bridges. ??Social media analytics might ""disappear"" at that point, becoming a normal part of our interfaces to social data. ??Visualization will be a big part of that, a necessary method of bringing analytic insights to people with limited quantitative training or skills., ,
,
,
,
,Software development and database skills are very useful, but I expect that tools like NodeXL point the way to a time when 80% of what we now call ""data science"" is done by casual end-users in the same way that ""desktop publishing"" enabled anyone with some text to create professional looking results. ??So technical skills alone will not be enough, insight into social processes and structure will be a big way to differentiate.

,
,
,If I have free time I like to hang out with my family, we walk our dogs around the hills overlooking San Francisco bay. ??My favorite recent read was James Gleick's "","" which provides a grand overview of the rise of information technology and the way it shapes our worldview.
,
,
,  "
"
,
,
Seattle-based startup GraphLab announced today
,, a new platform to bring large-scale machine learning capabilities to enterprise. GraphLab Create already has hundreds of beta-users.
,
GraphLab key advantages are performance and scaling. GraphLab Create has optimized out-of-core scaling enabling iteration of 1TB of data on a single machine, and can easily scale to distributed compute in the cloud for production environments that have high data volume and velocity.
,
,
,
,??,
GraphLab Create 1.0 will be available on July 21, 2014.
,
Additional Resources:
,
,??,
,
,  "
"
        ,    
,  "
"
        ,  "
"
,, Oct 27-29, 2014, San Francisco.
,
Plan now to attend 
,, October 27-29 in San Francisco, to learn HOW-TO accommodate the terabytes and petabytes of data from your Web logs, social media interactions, scientific research, transactions, sensors and financial records. Learn how to index, search and summarize Big Data. Learn how to empower employees, inform managers, and reach out to customers.  In short - learn how to Master Big Data!
,
The tangible benefits of Big Data analytics are well known. You likely understand the ""why"" of Big Data. Big Data TechCon isn't a ""why"" conference. It's the HOW-TO conference for Big Data. Practical tutorials. Technical classes.
,
, is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Come up to speed on the latest big data technologies like Yarn, Apache Spark and Cascading. Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or JavaScript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes. 
,
Mingle with fellow attendees. Be inspired by keynotes, including Gloria Lau, Manager of Data Science at Linkedin.  Be impressed by the hottest Big Data tools in the Expo Hall, from more than 35 top companies.  It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code ,
when prompted.  Register early for additional savings.  
,
,  "
"
With all of us blogging, emailing and posting updates on social media, the amount of textual data is growing fast. So how do you make sense of all that big data?
,
, teaches a SAS Business Knowledge Series course,
,, where you can learn how to organize, manage and mine textual data to extract the best information to improve your business.
,
,
,??,
,.
,??,??,
,
,
,??,
Course fee: $1,650,
,
, early and SAVE with our 
,.  "
"
,
Gregory Piatetsky, 
,, July 15, 2014.
,
Ahead of MLB All-Star game, 
,, a Pittsburgh-based startup which automatically
finds interesting insights in data, and writes them in English, launched a 
,.
,
Baseball 57,000 batter/seasons through 2013 give rise to 200,000 paragraphs, all automated.  
,
Visit  ,, select the baseball option, and enter a player, team, and season, or just enter a player and select from the matches, or a team and year and select from the team's roster. You can find comparable players and contrast them. You can also move from year to year for a player - finding his best year with a single click.
,
Some examples: 
,
,??,
OnlyBoth also allows you to find interesting insights about colleges, such as 
,
,??,
OnlyBoth was co-founded in March 2014 by Raul Valdes-Perez, a former CMU Professor and Machine Learning Researcher, who previously founded a successful search and discovery startup Vivisimo. 
,??,
Read ,.
,
,
,
,
,
 ,  "
"
, - August 18-21, Boston, MA, USA,
,
Register to attend RapidMiner World Boston where business and technical users from around the globe will discuss predictive analytics, big data and data mining. Join industry leaders including Usama Fayyad and Shawn Rogers from EMA for 3.5 days of research presentations, case studies, and networking with your peers and colleagues!,
,
If you are looking to advance your knowledge of RapidMiner, or are just getting started in understanding the advantages of Advanced Analytics, you won't want to miss this event.,
,
Reasons to attend include:,
,
,
Register to attend: ,
,
,
,  "
"
,
Latest ,, (Jul 16, 2014) ,:
,
,??,
Also
, (2) |
, (7) |
, (4) |
, (3) |
, (1) |
, (4) |
, (9) |
, (5) |
, (10) |
,
,
""I was going to write an angry post about Facebook emotional manipulation study, but then I got distracted by all the happy cat pictures they showed me"", KDnuggets Cartoon on ,.  "
"
Most popular 
, tweets for July 14-15 were
,
US ""Data Scientist"" average salary up over 10%, to $112K, according to indeed ,
,
,
US ""Data Scientist"" average salary up over 10%, to $112K, according to indeed ,
,
5 R #rstats training programs: Udemy, RStudio, Revolution Analytics, Mango Solutions, Accelebrate #DataScience ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,  "
"
By Geoff Webb, Monash U., July 16, 2014.,
IEEE ICDM Research Contributions and Outstanding Service Awards, 2014 Nominations by August 15, 2014,
,
,
The IEEE ICDM Research Contributions Award is given to one individual or one group who has made influential research contributions to the field of data mining.
,
,
The IEEE ICDM Outstanding Service Award is given to one individual or one group who has made major service contributions that have promoted data mining as a field and ICDM as the world?€?s premier research conference in data mining.,
,
For the Research Contributions Award, each nomination should consist of the following pieces of information: (a) up to 3 significant publications, (b) how these publications are relevant to the data mining community in general and to ICDM in particular, and (c) the broader impacts of these publications, in terms of citations and practical applications.,
,
For the Outstanding Service Award, each nomination should include the following pieces of information: (a) up to 3 significant service activities (such as running conferences and journals and facilitating data mining research, development, and applications), and (b) how these activities have promoted data mining as a field and ICDM as the world?€?s premier research conference in data mining.,
,
A nominator can be any researcher in any country, with a possible list of endorsers. The nominator should let the nominee know before a nomination is forwarded to the nomination committee chair. A nominee can be an individual or a group of researchers who have performed the research or service activities together. A nomination for either award is valid for 3 years. An unsuccessful nomination for one year will be automatically forwarded to the following two years?€? nomination pools, whether or not the nominator adds new information in the following years.,
,
The recipients of both awards from the last three years form the nomination committee, with the previous Outstanding Service Award recipient as the nomination committee chair.,
,
,
,
,
,
,  "


















"
,
,
The Data Lake is supposed to be the architecture of choice for Big Data. Ideally, the Data Lake has the following features:,
,
,
,
,
,
In short, the Data Lake is supposed to be the grown-up version of traditional DW - minus its intransigence plus a plethora of added benefits. It should eliminate the need for upfront schema design (a major component of traditional DW costs) and yet, give a single, comprehensive view of all data for analysis (thus reducing the cost and effort in analysis). CIOs would have reason to rejoice with the Data Lake, because it would be the most cost-effective way to bring all their data to play.,
,
,
,
Unfortunately, this scenario is far removed from reality. Enterprises with Big Data ambitions are faced with a profusion of specialized storage/processing options catering to specific data types/use cases, perpetuating the very silo-ed data problem that the Data Lake is supposed to solve. CIOs who want to bring different types of data to analysis, end up procuring different sets of tools and hiring (or training) different types of expertise to work on them. The Data Lake is in fact reduced to a bunch of Data Puddles that analysts have to jump across ?€? at great cost to the enterprise in terms of tools, talent and time. 
,
For example, one study projects an additional cost of more than $500Mn over 5 years to analyse 500TB of data in a bunch of data puddles in Hadoop vs. a monolithic relational DB.,
,
,
,
,
Enter Data Virtualization (DV) - a great data abstraction concept that gives a single-format view to different types of data and is intended to reduce the cost of leveraging multiple data sources and formats. It works by connecting to various sources of data and exposing them in a virtual, tabular view in near real-time. Analysts can define specific operations on this virtual view quickly without upfront modeling or moving data to the processing layer. It should be the final piece that brings all the puddles together and makes a lake.,
,
Except that it isn?€?t.,
,
For one, DV works on small amounts of data brought to cache and hence cannot work on large data sets. It is good for near real-time transformations but cannot work on historical data, complex hierarchical or multi-structured data or in cases where changes in data need to be brought to analysis. To me, DV is a quick-fix solution to solve a small set of Big Data problems ?€? a mousetrap in the elephant cage that ignores the elephant.,
,
The concept however, is virtuous.,
,
What if we could bring the concept of Data Virtualization right down to the Data Lake, make it an integral part of it and not merely a superficial appendage? What if all Data (not just cache) were to be , in a single-format but without the kind of upfront schema modeling, that traditional DWs are infamous for?,
,
At Xurmo, we have developed a NoSQL store on Hadoop that accomplishes the following ,:,
,
,
,
,
This means that Xurmo can store any kind of input data (structured, unstructured, static, streaming and across formats) in a single materialized form without schema design. Since the natural schema of data is stored, it is possible to handle multi-structured, complex data as well as changes in it. Semantic models can be quickly layered on auto-identified syntactic relationships.(,).,
,
Xurmo has a single abstraction layer to search, query or perform operations on this data, which means that an analyst can create analytics workflows seamlessly. Data processing is not divorced from storage and is not limited to a sample set.,
,
The abstraction layer solves another problem too. It enables new operations to be created in any JVM language and ingested into the workflow without MapReduce expertise. 3, party analytics tools can be easily integrated with native libraries.,
,
To sum up, Xurmo realizes the vision of an Enterprise Data Lake by creating a single repository of all data and enabling seamless processing across. The benefits of the Xurmo Data Lake are immediate and obvious: It is simpler, quicker and cheaper than deploying a portfolio of storage and processing tools and finding a way to make them all work together.,
,
,
Sridhar Krishnan is the founder and CEO of Xurmo Technologies.??Xurmo 
(,) is a middleware company??that simplifies Big Data and Analytics workflows??by providing seamless integration between data storage and applications. Xurmo holds number of patents in the area of unified??analytics??and self learning. For more information about the post or Xurmo email Sridhar at 
,
,
,
,  "
"
,
,

, is a practicing data scientist with a background in healthcare and applied machine learning. He holds a BS in Finance and Management Information Systems from the University of Delaware, a MS in Predictive Analytics from Northwestern University, and is a professor of Text Analytics at Rockhurst University. Piero is currently the Manager of Analytics & Insights at Blue Cross and Blue Shield of Kansas City where he has oversight over cross-organizational analytical initiatives. He also helps organize a local Data Science Meetup in Kansas City with over 190 members (,).
,
,
,
,Right now there?€?s a lot of uncertainty on the payer side, especially with the advent of ACA. With slimmer margins and greater uncertainty comes the need to evolve. In this sense, evolution means more widespread adoption of analytics and data-driven decision making. As far as the insights are concerned, it really runs the gamut from more conventional applications involving targeted marketing and cross-sell/up-sell opportunities to developing clinical classification algorithms that can be used to identify undiagnosed conditions and things like Monte Carlo simulations that will help us understand a wide range of scenarios for different books of business.
,
,
,
,That?€?s fair to say. I define analytics as the transformation of raw data into information and insights with the intent of facilitating better decision support. With that in mind, I don?€?t think there?€?s enough systematic transformation (or data mining) taking place and decisions might still be made based on gut instincts and pattern recognition. The fact that the healthcare industry hasn?€?t historically been as technology-driven as the financial or the retail industries doesn?€?t help, but that is beginning to change.
,
, As technology and security evolve, we should be challenging ourselves to evolve policies and procedures as well. Of course another big challenge is the sharing of data between various players in the healthcare arena such as payers, EMR providers and hospitals, pharmaceutical companies, and other research-based organizations.
,
,
,
,Depending on how you define Big Data, it may or may not help solve the problem. If you define it narrowly as having copious amounts of structured or unstructured data that can typically be described by a handful of words that start with the letter V, then I?€?d say the answer is no. On the other hand, if you define Big Data more generally as a movement geared towards making more thoughtful use of all sorts of data by leveraging technology like the Hadoop ecosystem and various machine learning libraries in a more meaningful way, then I?€?d be inclined to agree.
,
,
,
,
,

,Although still in their infancy, a few compelling use cases come to mind. Text mining clinicians?€? progress notes and customer service calls for topics and sentiment are two solid examples. From a behavioral health perspective, it?€?s not a stretch to imagine how much more can be learned about a patient with these types of data points. At the end of the day, it all comes down to enabling a higher standard of care. More conventional use cases such as mining customer service calls for sentiment certainly have their place too.
,
,
,

,I initially sought to make sentiment analysis more digestible by measuring positive, negative, and neutral sentiment where words expressing modality can have a multiplicative effect and subsequently building favorability indexes that can be referenced through various individual-based, role-based, and topic-based hierarchies. Given a structured corpus of calls,, one could quickly gauge sentiment for a specific individual, external analyst, internal executive (by title/role), industry, or call topic with relative ease. From there, you can train models to make predictions or extract information to enrich other models. For example, if a certain analyst is always overwhelmingly negative on calls then you can ding him or her by weighting the negative input accordingly. Conversely, if an executive is consistently, unjustifiably optimistic then you could weight that as well. Right now everything is done in Pandas (I prototyped in the PyData ecosystem) so I?€?ll be working on formalizing the data model in the near future.
,
,
,
,Right out of college I was a finance guy and fortunately I took an advisor?€?s advice (before data science was a thing) and picked up an MIS (Management Information Systems) ,minor as well because 2008 wasn?€?t the best time to hit the ground running in New York City with a finance degree. In any case, I got involved in the healthcare industry as an operations analyst. I quickly realized that data was EVERYTHING and that I could automate away the majority of my work which led me further down the paths of databases and programming. From there on out, the problems that I was addressing required a deeper understanding of statistics and eventually other forms of predictive modeling and machine learning became necessary. So I went back to school.
,
Being of a mathematical mindset that?€?s laced with a hefty dose of pragmatism has served me well. I think about deriving insights like an engineer thinks about building a bridge or a doctor thinks about performing surgery; given my skill set, this is how I?€?m best suited to make contributions and create value. Just like in most exercises involving some form of predictive modeling, text mining requires a lot of data preparation that will essentially make or break your models and/or insights.
,
,
,
,First, you absolutely need to love what you do because if you don?€?t then the data preparation and feature engineering aspects of it will wear thin. Another valuable piece of advice that was given to me is the idea of not letting perfect be the enemy of good. Sometimes going to market with something that?€?s good can be better than letting an opportunity pass while perfecting something that would have been great. Last, I?€?d stress the importance of ongoing education and deepening your analytical toolbox from a software, theory, and application perspective.
,
,
,
,I have a lot of pet projects that I?€?m always working on and I?€?m trying to be more involved with a ,handful of not-for-profit and charitable organizations by volunteering. I recently helped Big Brothers Big Sisters of Greater Kansas City optimize its donation bin placement around the city using a very limited dataset that I enriched with Census data and built some models on. I also try to keep up with data science related news on Twitter, Kaggle, FastML, The Practical Quant, Flowing Data (for visualization inspiration), Scikit-Learn/R/Spark/H,O/Hadoop releases, white papers, and of course KDnuggets!
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Jul 17, 2014.
,
Latest KDnuggets Poll asked:
,
,
The results, based on 392 votes, show a pattern that has remained surprisingly stable over the last 3 years 
,
,??,
,
,
We can see the trends more clearly by grouping the answers into ranges for Megabytes (1 PB).  We will call data scientists with largest dataset analyzed in each range Megabyte analysts, Gigabyte analysts, etc.
,
The global percent of Gigabyte analysts slightly increased from 53% in 2012 to 54% in 2014.  The percent of Megabyte analysts has steadily declined, as expected, while percent of Terabyte analysts has grown slightly, from 16% to 18%.
,
,
,
Here is a similar chart just for the US, which shows decline of in percent of Gigabyte analysts and the corresponding growth in Terabyte and Petabyte analysts.
,
,
,
Regional participation was
,
,??,
The chart below shows the distribution of Largest Dataset Ranges by Region, sorted by % of TB+ answers. We see that US/Canada and AU/NZ lead, with about 30% of their data miners having worked on TB-size databases.  Next is Europe (19%), Latin America (15%), Asia (10%), and Africa/MidEast (7.7%).
,
,
,
Here are the results of past polls:
,
,??,
,
,
 ,  "
"
,
,
To keep up with the pace of development in modern business, it is essential to invest in more sophisticated and innovative manufacturing processes. Investment in analytics offers this opportunity as it allows organizations to streamline their manufacturing process, improve efficiency and reactivity to customer demand by utilizing real time data. For any company manufacturing its own products, this is an essential area to gain an advantage over competitors.
,

The , (May 21 & 22, 2014) was organized by the Innovation Enterprise in Chicago, IL to bring together analytics leaders from manufacturing sector to share their success stories and key learning. The summit addressed the issues mentioned above and brought together business leaders who are implementing and experimenting with new analytical methods for manufacturing - driving remarkable improvements in their organizations. Executives at the forefront of analytics shared their innovative approaches, providing insight into how they have gained valuable information from raw data.

,
We provide here a summary of selected talks along with the key takeaways.
,
,
,

Here are highlights from Day 2 (Thursday, May 22, 2014):

,
,??delivered an interesting talk on ""Collecting and Applying Logistics Analytics"". He started his talk with explaining what does a simple transaction means from different perspectives - plain English, business, and logistics. Supply chain has a wide variety of data objects, all of which need to be collected efficiently in order to speak the common language of commerce. 
,
Next, he demonstrated through an example how does crude oil supply chain data systems look like today, how it is slowly changing and how it will look into future; highlighting the increasing move towards cloud-based infrastructure. During data collection, it is important to provide alternatives for when it is difficult to collect certain data. Also, standards such as ISO, ANSI, IEEE or API (American Petroleum Institute) must be used to reduce data complexity and ease the integration of different systems. Next, object oriented thinking should be leveraged to design data architecture for managing all these supply chain objects.

,

Decentralize resources in order to unleash creativity and speed to market. Focus IT on the core value-add of managing data and connecting to internal/external systems. De-emphasize creation of front-end systems. Also, if a firm is not in the business of software development, simply outsource that work to focus on its key capabilities. Talking about execution, he noted that a product needs an imperfect 1.0 version to be released rapidly. Focus should be on passing the test of human intelligence and achieving the critical mass. In conclusion, he suggested that organizations should be careful to not kill innovation. Let the innovators be creative and then ""organize"" their innovation.
,
,??gave an insightful talk on ""Using Critical Metrics to Build a High Quality, Reliable & Globally Competitive US Based Supply Chain"". When time to market, product innovation and quality are critical components of a company?€?s business strategy then the supply chain process and infrastructure must be established, monitored and measured to continuously improve. The challenge is to establish strong US based symbiotic relationships within a supply chain which are structured, aligned and mutually leveraged to achieve repeatable high product quality, quick turn product innovation.
,
Drawing upon his extensive supply chain experience, Curt suggested that it is far better to focus on Critical Few metrics rather than on Important Many metrics. He suggested the following metrics for maintaining an efficient and effective supply chain:
,
, ?? ,
,??shared valuable suggestions based on his work on continuous improvement through technology in the heavy manufacturing industry, during his talk titled ""Data Collection and Transformation ?€? Video Analytics, Digital Manufacturing and Immersive Technology"". Revamping factories and designing new products need considerable amount of data from factory. ??However, majority of the existing data collection and transformation methods are expensive, time consuming and inconsistent. 
,
In its pursuit to harness technology to meet these challenges, Caterpillar has developed various systems to collect and transform data using digital manufacturing and immersive technologies to support lean product and factory development. He described how his firm is doing low cost data collection using ""GoPro"" cameras. He also explained how digital manufacturing is addressing ""Design for X"" issues. Finally, he demonstrated how the above initiatives along with others are improving quality and velocity of facility design process through immersive technology.

,

, introduced and explained the concept of OPPM (One Page Project Manager) in his talk ""Driving Operational Strategy and Excellence Amidst Solar Chaos"". New product initiatives (in Solar Energy industry) are plagued by a lot of challenges such as long customer qualification cycle, supply chain continuity, safety and long term reliability. He explained about the concept of OPPM (One Page Project Manager) defined in the book by Clark A. Campbell, and how he tweaked the OPPM concept to fit SunEdison needs (by defining deliverables and adding project metrics). Explaining the role of OPPMs, he mentioned that OPPMs set project boundaries for:
,
, ?? ,
OPPMs help the project by clearly defining the objectives, limiting scope creeps, ensuring proper communication and thoroughly evaluating the results.
,
,
,  "
"
,
,
,
,
, is a fast, effortless, and beautiful way to explore data and share your insights. This self-service business analytics solution is designed to help you intuitively access and explore data without expert assistance. Analytics Desktop lets you access data instantly and securely, discover insights using compelling data visualizations, organize information in interactive dashboards, and deliver your results via email without complex scripting.
,
No special training or experience is required as it is designed for business people who need to visualize, analyze, and deliver data on their own, but who lack the necessary tools, technical experience, or IT resources. 
,
Effortlessly build dashboards by choosing from a wide range of interactive visualizations. You can drag and drop your data to customize your view, or sort, pivot, drill, and use aggregations to apply statistical analysis, making Analytics Desktop powerful, yet easy to use. 
,
Try it yourself and see how easy business analytics can be. Download your free copy of ,.
,
,  "
"
Most popular 
, tweets for July 16-17 were
,
Revised standards for statistical evidence - should researchers/journals use a p=0.005 publication threshold ? ,
,
An awesome GitHub list of #BigData frameworks, resources, and more ,
,
,
An awesome GitHub list of #BigData frameworks, resources, and more ,
,
,  "
"
,
,
, joined , as the head of technology recommender systems in May of 2013, where he leads a dedicated engineering and research team in the development of applied machine learning software and services.  These solutions drive dynamic content delivery in both email and site-side applications.  Prior to that, Cliff was Senior Director of Engineering at CBS Interactive, where, in addition to recommendation, he delivered technology for online optimization and multivariate testing.
,
Here is my interview with him:
,
,
,
,:  Recommendation and personalization are organizationally complex.  These systems span multiple groups.  And, unlike the tactical goals ?€? the algorithm, the delivery stack, and the user experience flow ?€? the strategic goal doesn't belong to any single contributor.   The business relies on all these parts working together to create the effect it wants.  So someone has to pay attention to this.
,
,For example, if the business wants more revenue, and you?€?re recommending related products while people are purchasing, you may distract them and end up losing money.  The same applies at the top of the funnel: you don?€?t want to show an early stage laptop shopper a compatible Bluetooth mouse.  That?€?s the sort of thing they can add to cart at checkout.  These are simple examples; in practice the dysfunction can be much more subtle.  The main point is that despite the fact that the individual parts of the system work, and perhaps work very well, the business may not be getting what it wants.
,
If you are setting up a recommendation or personalization system, my advice to you is to make sure you know what the strategic goal is, and test your system against that goal as soon as possible, with the simplest version of the system you can.  Once you establish that you?€?re headed in the right direction, and you have a baseline, then you can invest as much time and effort as you like deepening various aspects of the system.
,

,
,
,We use A/B testing to see which metric or combinations of metrics work best.  I would certainly recommend that as a general approach.
,
,A popular metric is not necessarily a good metric.  For example, anyone interested in recommendation will encounter the cosine similarity at a very early stage.  I see it mentioned over and over again in papers for illustration, probably because it is conceptually simple and easy to explain.  However, in my experience it doesn't do as well as other similarity metrics, though I am sure there are people who have had a different experience.  We include it in testing anyway, in case we encounter a situation where it may work well.
,
There was a paper I liked from Google a while back looking at a bunch of similarity metrics that?€?s a decent place to start: ,. Also worth a look are the metrics for association rule mining: confidence, lift, leverage, and conviction. I would also recommend trying the root log-likelihood ratio (LLR) similarity implemented in Apache Mahout.  The history of recommendation as we?€?re discussing it here is relatively short; it is easy enough to just go through the timeline, and look at the various metrics that have been introduced over time.
,
,
,

,
,
,There are certainly idioms.  ?€?You may also like?€??€? from any sort of product browse or detail page, for example.  There are natural places on a page, places in a flow, where recommendation works; you know these from your own positive experiences with recommendation on the web.  Recommendation works well at the edges, figuratively and literally.  You don?€?t want to distract people from what they?€?re doing, but you want to be there if they ?€?pop out?€? of the flow, and decide to maybe go to something else.  So look at the stages people go through as they interact with your catalog, and look for the in-between places that emerge.  Test recommendations there, and if you get a decent response, just make sure that those clicks are also contributing to your strategic goal.
,
,We had an interesting example some years ago where we had a recommendation module at the end of a very long page, which featured an editorial product review.  Even though we were doing reasonably well, I pushed the page owner to move the placement higher on the page, because I was convinced few visitors scrolled down far enough to see the placement.  I figured that if we moved the placement up, more people would see it, and we?€?d do better. 
,
The reverse was true.  We went from an 8% click-rate at the bottom of the page to a 4% click-rate at the top.  In hindsight, we reasoned that when recommendation was at the top, the placement became a casualty of the sort of natural filtering people do ?€? they skip the ads and the navigation and so on, and just follow their line of interest. Being at the bottom of a long piece of content ends up working really well, because when a reader reached the bottom, they sort of popped out of the flow and were ready for something new.
,
We went on and used this same idea for message boards, UGC, and other sort of long form content, and it worked really well.
,
That?€?s just one example of course.  In the end, it is all about experimentation ?€? we didn?€?t come to understand this effect until we were faced with an experimental result that challenged our intuition.
,
,.
,
,
,  "
"
,
,
Understanding PEOPLE (i.e. not shoppers, not consumers, not respondents) - but people - across platforms and touch-points in an increasingly interconnected world threaded together with always-on technology, presents an opportunity for us to know people more deeply - and take strategic action.
,
Technology is the central driving force amongst the foremost mega and macro trends across industries. It's advancing at such a rapid pace it is not only changing how we do things but changing how we understand the world, business, and people.

,

The , (May 19-21, 2014) at Los Angeles explored the emerging role of decision science and the convergence of knowledge points - insights, foresights, social science, marketing science and intelligence with technology a central driving force and profound connector. ??This year's theme was ""Technology and the Humanization of Data: Synthesizing Insights, Analytics and Relational Database??Strategy.""

,

This event was about accelerating disruptive innovators in the research space and pushing people to take risks to think outside of traditional research methods and insights gathering end explore new tools and technologies.

,

We provide here a summary of selected talks along with the key takeaways.

,

Here are highlights from Day 1 (May 19, 2014):

,

, gave a talk titled ?€?Hacking H(app)iness ?€? How to Give Big Data a Direction?€?. He insisted that if one wants to understand people, it would be helpful to think about them the way they think about themselves, but people do not think of themselves as ?€?consumers?€?. 
,
He admonished the audience to stop calling people consumers. The term ?€?consumer?€? objectifies and reduces people to a single dimension. Havens explained that a shiny new Lexus may make one happy, but it?€?s a fleeting sort of happy (known as ?€?hedonic?€?) and when it fades we?€?re left searching for something else to fill the void. Consumption as a happiness engine has left people feeling empty. People want purpose and meaning, to feel a part of something bigger than themselves. And they want the brands and companies they patronize to not just reflect their values and to care about the things that really matter?€?family, community, people, the planet?€? but to walk the talk.

,

, talked about ?€?Progress in the Adoption of Innovations for Consumer Intelligence?€?. He shared findings from research conducted by Socratic Technologies along with Institute for International Research (IIR)??into the research industry?€?s tendency towards innovation. Talking about innovation adoption orientation, he mentioned that only 6% of companies in market research are innovators. 
,
Innovation adoption is motivated by quality and efficiency improvements and not by cost reduction. Cost and stakeholder acceptance are the biggest innovation barriers. Only 4% of researchers cited ?€?new and better insights?€? as an innovation driver. ?€?Fear is a problem,?€? he explained. ?€?And fear is a great way to kill any new idea. Since the economic downturn, there?€?s been a lot of defaulting to ?€?good enough.?€??€? ?€?People are also afraid of how introducing a new capability might affect their organizational structure. Where will this new intelligence function or capability sit??€? MacElroy added.

,

, delivered his talk on ?€?Building a Social Spine in Tracking Research?€?. Survey-based tracking research has come under severe criticism in the past few years, for being too slow, too expensive, and too backward-looking.????By making use of various forms of new, passively collected data ?€? including social media data and search data ?€? and integrating them with quick, flexible surveys, one can renew tracking research, turning it into a valuable management tool.??
,
Even if traditional trackers were reasonably predictive?€?which Friedman noted they are not?€?they would still be too cumbersome in their conventional form to suit the pace of change today. It?€?s like trying to turn an ocean liner when what?€?s called for is a speedboat. ??Friedman?€?s answer to both the speed and predictability problems: social media. ?€?We can build things that are more predictive using passively collected data than we ever were able to using survey data,?€? he said. Friedman provided several examples from new trackers with social spines?€?a number of which TNS is co-creating with clients?€?that appear to be quite predictive.

,

, gave an interesting talk titled ?€?The Consumer Is No More. Long Live The People: Moving from Consumer Understanding to Empathy?€?. He emphasized that as role of brands is changing, the role of research must change consequently. He said ?€?People are turning less and less to brands to define themselves or as guides to identity,?€?. He added ?€?Experience and the creation of meaning are also no longer the jobs of brands.?€? 
,
He recommended all researchers to read ?€?,?€? by Jeremy Rifkin. He also cited a great quote by neuroscientist Antonio Damasio: ?€?We aren't thinking machines. We are feeling machines that happen to think.?€? He emphasized that ?€?It?€?s a complete myth that to know what people are thinking is to understand them.?€? He asked the researchers to forget these terms: ?€?aspirational, understanding, research?€? and replace them by ?€?useful, empathy and dialogue?€? in order to facilitate connections between brands and people.

,

,.
,
,
,  "
"
,
,
, joined , as the head of technology recommender systems in May of 2013, where he leads a dedicated engineering and research team in the development of applied machine learning software and services.  These solutions drive dynamic content delivery in both email and site-side applications.  Prior to that, Cliff was Senior Director of Engineering at CBS Interactive, where, in addition to recommendation, he delivered technology for online optimization and multivariate testing.
,
,.
,
Here is second part and last part of my interview with him:
,
,
,
,: One trend I keep running into recently is deploying recommendation using search technology.  Search and recommendation are both strategies for dealing with information overload.  The technologies grew up differently, but it turns out that much of what search does well can be used for recommendation, too.  This is pretty great, since it gives you some useful tools for free; I am used to DIY technology for recommendation. Ted Dunning, a Mahout contributor now at MapR, has some nice presentations online on this topic.
,
On the presentation side, given the huge growth of mobile, and the much smaller viewport, I think recommendation and search together can really help users make the most use of the small screen ?€? the more quickly catalogs can cull low-utility suggestions, the better.
,
,
,
,I spend a lot of time looking at recommendations I don?€?t understand, and trying to decide whether I just don?€?t have the context, or if it might be a bug.  I have seen some surprising similarities for sure, but none that had the sort of explanatory power of the beer-and-diapers case.  At StubHub, we sell tickets for concerts, music, and theater.  As you might imagine, most often, the correlations stay in category ?€? sports for sports, concerts for concerts, etc.  But I did find it interesting that some artists cross over to sports audiences ?€? Jay-Z correlates with the Nets, as he has connections to that organization, for example.  And strangely, Jay-Z also shows up for WWE fans.  Not exactly beer and diapers, but we do find cross-category correlations.

,
,
,
,I think the key to this is that the system must be able to adjust to the user?€?s desired level of privacy.  A user must have some control ?€? ?€?clear my personal information?€? is a baseline ?€? and the system must be robust to a sudden loss of information.  The system must have a strategy for handling a graduated amount of information or context, from the total stranger to the customer who has more than enough data points.

,

,
,
,Like any data geek, I suppose, when I first encountered the power of machine learning as a way of extracting information from data, I was completely hooked.  I?€?d worked with data for years to that point, doing ,summaries and aggregations, and machine learning ?€? it wasn?€?t ""data science"" then ?€? just really amazed me.  I happened to be working in the Data Warehouse group at CNET at the time, and I convinced my manager to let me start my own group, so I could become a customer of the data, and apply it back to make user experience better.  Never looked back, still think it is important, and there are plenty of challenges left in doing it well.
,
Best advice ?€? I feel lucky in that I had a lot of good teachers and counselors, too many to count. ,

,

,
,
,I look for depth in one or maybe two areas depending on the level of the position, and passion.  It is such a broad discipline, chances are you are going to be out of your comfort zone, learning new things, and you need passion to get you through that.
,
,

,
,
,I really liked Ted Dunning?€?s ,; a quick read, and a really great take on the growing trend of deploying recommendation via search.
,
,
,  "
"
,
,
,
Learn Data Science in 12 weeks with in-person instruction, ongoing career coaching and job placement support.
,
Bootcamp: September 2 - November 21, 2014
,
,
,Twitter:
,
,
,
,
,
,
, Learn Data Science in 12 weeks with 100% in-person instruction with experts from Datascope Analytics. 
,
,
, Use real data from sports, business, government, or from whereever else you can borrow. Build a soup-to-nuts Passion Project to share with potential employers. 
,
,
, Leave fully qualified for an entry-level data scientist job. Placement Programs are available to all graduates.
,
,
,
Upon graduating from the Data Science bootcamp you will be prepared for an entry-level position on a Data Science team as a data scientist or data analyst. This means you will:
,
1. Have a fluid understanding of and practical experience with the process of designing, implementing, and communicating the results of a data science project.
,
2. Be a capable coder in Python and at the command line, including the related packages and toolsets most commonly used in data science.
,
3. Understand the landscape of data science tools and their applications, and be prepared to identify and dig into new technologies and algorithms needed for the job at hand.
,
4. Know the fundamentals of data visualization and have experience creating static and dynamic data visuals using JavaScript and D3.js.
,
5. Have introductory exposure to modern big data tools and architecture such as the Hadoop stack, know when these tools are necessary, and be poised to quickly train up and utilize them in a big data project.  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for July 22 and later.
,
See full schedule at , .
,
,  "
"
,
,
AWARD:??, ??|?? DEADLINE:??, ??| Innocentive Challenge 9933491,
,
Accurate estimates of life expectancy have long been an interest of clinicians, insurers, and academic researchers. What if there were a way to predict how long one is expected to live without traditional medical records, invasive tests, or examinations? The Seeker wants to identify a novel method of determining longevity.,
,
This is an Ideation Challenge with a guaranteed award for at least one submitted solution.,
,
Challenge Overview,
,
Life expectancy is a commonly used statistic to measure the health of a population. On average, the current worldwide life expectancy is approximately 70 years, but this is influenced by many factors, including gender, geographical location, family history, diet, and social habits. In our increasingly health-conscious society, people are making smarter food and lifestyle choices, but how do you know if your habits are actually prolonging your life? What if you could find out how many more years are in your future? Is there a set of data points that can be used to estimate an individual?€?s life expectancy??? The Seeker desires a novel approach for combining information to accurately predict longevity.,
,
,
,
,
Submissions to this Challenge must be received by 11:59 PM (US Eastern Time) on August 4, 2014.?? Late submissions will??,??be considered.,
,
,
,  "
"
Most popular 
, tweets for Jul 18-20 were
,
#WorldCup: Facebook Data Science analysis of rooting: lost team fans move to opponent of team that beat theirs ,
,
,
My 7 Steps for Learning Data Mining and Data Science - now in Techopedia ,
,
A good collection of #MachineLearning tools in #Python ,
,
Baby steps in learning #Python for data analysis  ,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Lisa Solomon, Salford Systems, July 2014.,
,
,
,
,
Click to Register:??,
,
,
Click to Register:??,
,
,
Click to Register:??,
,
,
Click to Register:??,
,
,
,
,
,??$35 (includes free 90-day access to the SPM Salford Predictive Modeler technology),
,
,
Data Mining Quickstart,
,
Hands-On Technical Session (Bring your own laptop),
1:00pm-4:00pm,
,
,
,
,
,
,
,
Salford Systems' Training Seminars offer crystal clear instruction and a wealth of real-world consulting experience. This seminar will provide a great opportunity to use data mining technology and to understand how to apply data mining to your business and/or research needs.,
,
Questions?
Please contact Lisa Solomon??,
,
,  "
"
        ,  "
"
,
,
To explore how people identify and respond to bad data, Software Advice created an online survey, collecting nearly 800 total responses from randomly selected U.S. adults. We also spoke to data visualization experts for advice on how you should be presenting your data. Here?€?s what we found.,
,
,
,
When it comes to making mistakes in your charts, graphs and other visual data representations, do people really notice? According to our survey, they do - 56% have correctly identified bad visualizations.,
,
,
,
,
,
The first option was an image portraying percentages of ?€?yes?€? and ?€?no?€? answers, scaled with incorrect proportions. This faulty visualization was correctly identified as problematic by 22 percent of respondents. The third option was a pie chart with percentages that added up to more than 100 percent. This poorly-presented visualization was correctly identified as problematic by 34 percent of respondents.,
,
,
,
,
,
Two of the other options (B and D) were charts that had nothing wrong with them. Nonetheless, these were still identified as ?€?wrong?€? by a combined 20 percent of respondents. The final choice, which was ?€?none of these?€? (and was incorrect, as two of the options were, in fact, wrong), was selected by 24 percent of respondents.,
,
,
Excerpted from Software Advice, a data visualization ,.
,
Read more in the original post ,.
,
,
,
,
,
,
 ,  "
"
,
,
Business Intelligence (BI) is playing a key role in modern business and is considered central to identifying new opportunities within an organization. Making use of new opportunities and implementing an effective strategy can provide a competitive market advantage and long-term stability for a business. Using Business Intelligence theories, methodologies, processes & more is paramount to the success of such pursuits.
,
The , (May 21 & 22, 2014) organized by the Innovation Enterprise at Chicago, IL covered major challenges and opportunities being observed by BI leaders across industries. The summit featured an industry led speaker line-up consisting of 25+ leading business analysts, data scientists, intelligence developers, business researchers and business intelligence leaders. ??BI leaders shared their perspective on common challenges, best practices and real-life case studies.

,

We provide here a summary of selected talks along with the key takeaways.
,
Here are highlights from Day 1 (Wednesday, May 21, 2014):
,

,??delivered an insightful talk on ""Insight-Driven Approach as a Competitive Advantage"". Information management plays a significant role in promoting an insight-driven culture. Areas like data quality, metadata, data lineage and master data must be addressed to ensure successful and systematic decision-making across the enterprise. He explained why approaching Analytics process as a discipline, as opposed to a project?€?based approach, can further organizational success.

,

He listed the pre-requisites for analytical success as: well understood problem, good quality data, choice of tools and skilled analysts. The high level benefits of Analytics are revenue generation, cost containment and process improvement. The Analytics process boosts revenue generation through:
,
,??,

Business Analytics and Information Management should always focus on business adoption, improved speed to market, data quality, and innovation. He also explained how these concepts can best be applied to achieve real business results.

,

,??described the importance and key characteristics of Open Innovation in his talk titled ""To be a Disruptor or to be Disrupted?"". 96% of all innovations fail, with some of the most common innovation challenges being fragmented conversations, too many platforms and lack of unified approach. As defined by Prof. Henry Chesbrough, Open Innovation is the use of purposive inflows and outflows of knowledge to accelerate internal innovation, and expand the markets for external use of innovation, respectively. The central idea behind Open Innovation is that in a world of widely distributed knowledge, companies cannot afford to rely entirely on their own capabilities or create new capabilities. In contrast to Closed Innovation, Open Innovation is based on the premise that External R&D can create significant value; Internal R&D is needed to claim some portion of that value.

,
He explained the various steps involved in the ideation workshops, which synthesizes the best elements of business process, trends, human collaboration, hindsight, business intelligence and broad opportunity insight to connect customers, employees, and partners into an Open Innovation strategic blueprint to become a Disruptor. Next, he gave examples of how companies such as GE, General Mills, and Philips are following the Open Innovation approach for market leadership. In summary, he mentioned that Open Innovation Model is the future - through Open Innovation model organizations keep strong relationships with external parties at different stages of the innovation development process.

,

,??shared his experience of managing the talent challenge, in his talk ""Growing a BI Practice from Within"". A constant challenge in staffing a BI team is acquiring talent that has both the technical know-how and the business acumen needed to excel in a consultative BI role. ??Retention and continuity are also of high importance in order to service the business teams. The major challenges in front of Wells Fargo BI team were poor data environment, antiquated software and inadequate staffing. The vision to overcome these challenges focused on self-service, consultative, and predictive capabilities. It is particularly hard to find good talent for BI because of the niche toolsets involved, required passion and strong soft skills.

,

In July 2013, Wells Fargo created the Experiential Learning Program. ??The program allows internal team members, who express interest and pass minimal qualifications, the opportunity to ?€?intern?€? with the team. ??As an ?€?intern?€?, they learn technical skills such as SQL, and Cognos report writing as well as basic database knowledge. ??The intention of this program is to achieve a win/win outcome for both the BI team and the intern. ??The win for the intern is the opportunity to learn a set of skills and gain first hand insight into a new career path. ??The win on the BI side is, if the intern excels at the technical aspects of the job, this coupled with the intrinsic knowledge of the business they should already possess, they become a strong candidate to fill any upcoming staffing needs on the team.

,

,??gave an interesting talk on ""Business Intelligence & Communications"". He started his talk with a very interesting quote: ""Data is growing at the fastest pace it ever has and at the slowest pace it ever will"". Next, he talked about different layers of data processing including data transformation (ETL), connectivity, reporting, analysis, performance monitoring and prediction. Given the complexity of technology and variety of options, picking the right vendors can be difficult.
,
He ended his talk describing the importance and process of getting meaningful analytics delivered in a visual message at the right time, using the following three key aspects of business intelligence:
,
, ?? ,
,.
,
,
,  "
"
,
,
, is an Assistant Vice President in MetLife?€?s Global Technology & Operations. She currently serves as the principal subject matter expert for data analysis and business intelligence. As a member of the SWAT Team, MetLife?€?s internal IT consultancy, she explores data science applications for emerging technologies across multiple lines of business worldwide. Prior to joining MetLife, Amy was a senior intelligence analyst supporting various agencies within the United States Intelligence Community and Department of Defense. 
,
Here is the first part of my interview with her:
,
,
,
, When I say that knowing about customers is not the same as knowing them, it?€?s really a statement about intentions. When we know about someone, we focus on easily quantifiable variables, such as demographic information or transactional information like purchasing behavior. This kind of information answers questions about what a customer does. What did they buy? When did they buy it? Do they have patterns of purchasing behavior? These types of data are a lot easier to digest and dashboard; that?€?s why people love NPS (Net Promoter Score) scores.
,
,The idea of being able to ask a single question and determine customer loyalty is very seductive, but that one data point won?€?t give you the whole story because it can?€?t. It captures how a customer feels at a specific point in time after a single interaction, but is that how they feel about us all the time? I doubt it. The NPS score wraps up a lot of assumptions in a tidy package. I?€?m not saying companies shouldn't use it, just that it can?€?t be their sole measure because it still only answers a what question.
,
When I say that we know someone, our research is focused on variables that are harder to quantify, like motivation. Call it qualitative. We?€?re looking for why our customers do what they do. Why did they choose one option over another? Why did they choose us over a competitor? Why do they show certain purchasing patterns? , ,
Sometimes it simply confirms what we already assume, but sometimes it yields really interesting insights, particularly about things like values. Either way, we gain a much more holistic view of the customer that gets to the heart of the why question.
,
, 
,
, MetLife Infinity is designed around the concept of digital legacy. The ability to tell your stories after you?€?re gone. The Infinity app helps ensure that our customers have a safe location to digitally store their memories and the opportunity to share them with loved ones at preset dates in the future. It works like a time capsule; you save things now so that others can view them later. 
,
Our product strategy for China was different for a few reasons. First, Chinese heritage is more strongly rooted in ancestry than in the US. It?€?s not that we don?€?t look to the past here, but Chinese families?€? legacies can span thousands of years. It was important for us to honor that in the look and feel of both the website and promotional channels. You can see the difference in how we've explained Infinity?€?s functionality: 
,
US: ?€?MetLife Infinity lets you store your photos, videos and important documents in one secure location. Share them with your loved ones now or on a pre-determined future date. Infinity ensures your digital legacy lives on, in the right hands.?€?
,
China: ?€?With MetLife Infinity, you can share your memories and send them to the people who matter most. And when they grow older, your memories can stay alive and be passed from generation to generation.?€?
,
They both convey the same message, but the US version is direct and focuses on how the app works, while the China version is more subtle and emphasizes why it works, such as for respecting heritage.
,
,
,
,This goes back to the ?€?what vs. why?€? problem. Transactions can help segment customers into tidy groups, where you can infer how they might behave based on statistical models. We can predict with some degree of accuracy what they might do, but only within that group. What happens when we want to understand what motivates them? That?€?s at the heart of the why question. This is especially true when trying to better understand outliers or groups for which you have no existing data. 
,
Behavior-based segmentation groups customers into like-mindedness, rather than like-attributed-ness., That?€?s when you begin to see clusters of people who may not seem similar from the outset, but actually share intrinsic motivations. If you can figure out which group feels most strongly about the idea you?€?re selling, the information used for transaction-based segmentation matters less. It?€?s difficult to master, but can give you a much more accurate way to develop targeted messaging.
,
,.
,
,
,  "
"
,

,

Business Intelligence (BI) is playing a key role in modern business and is considered central to identifying new opportunities within an organization. Making use of new opportunities and implementing an effective strategy can provide a competitive market advantage and long-term stability for a business. Using Business Intelligence theories, methodologies, processes & more is paramount to the success of such pursuits.

,

The , (May 21 & 22, 2014) organized by the Innovation Enterprise at Chicago, IL covered major challenges and opportunities being observed by BI leaders across industries. The summit featured an industry led speaker line-up consisting of 25+ leading business analysts, data scientists, intelligence developers, business researchers and business intelligence leaders. ??BI leaders shared their perspective on common challenges, best practices and real-life case studies.
,
We provide here a summary of selected talks along with the key takeaways.

,

,.

,

Here are highlights from Day 2 (Thursday, May 22, 2014):
,
,??gave an insightful talk on ""Analytics @Netflix: Fast, Iterative, and Insightful"". Netflix collects data from a variety of sources including usage statistics, user ratings, set top boxes, user profiles, social networks, etc. All this data is used across business units for product design, content selection, marketing, customer experience, payments and finance. The data (greater than 7 peta-bytes) is stored on Amazon S3 (Simple Storage Service) and Teradata Cloud, where it observes around 100 billion events/transactions per day. Using cloud-based architecture enables Netflix to focus on its own core competencies and not worrying about hosting issues such as maintenance, capacity planning, scaling, etc.

,
Next, he explained the architecture of Netflix data infrastructure. Most of the data processing happens on Hadoop platform and then the aggregated data is pushed to Teradata for faster queries, interactive dashboards. MicroStrategy and Tableau are currently used for reporting and visualization. Visual insights is also being assessed as a future option. In order to minimize the ambiguity due to multiple visualization tools being used across the firm, the firm is moving more and more logic into data layer, and having less of logic in the reporting or visualization tools.

,

He explained how Netflix data platforms, tools and analytics teams are evolving to keep up with growing complexity and data volumes to drive optimization and decision-making. The BI process at Netflix is focused on collaboration, responsibility, alignment and information sharing. The process encourages light documentation, design guidance and self-service tools. In conclusion, he mentioned that there are four key factors that enable Netflix to move fast: culture & people, dedicated & co-located teams, analytics culture and flat organization.

,

,??delivered an interesting talk on ""BI Innovation at Hyatt"". He started his talk with describing why the technical issues are relatively easier to solve whereas the people issues (related to control, skills, incentives, etc.) are way harder to resolve. He explained data processing in the following five steps (in increasing order of difficulty):
,
, ?? ,

In the new era of consumerization of analytics, we are all seeing that analytics is sexy again. However, there is a considerable data and statistics literacy gap, and thus, an analytics revolution has to precede in order to prepare the masses for analytics. It is very important to understand that Analytics is not the same as Reporting & Data Warehousing. Analytics is very different from typical IT projects. IT is often a cost center, lacks strategic influence; whereas Analytics needs more of scientists than engineers, and has significant strategic influence. People can be irrational. Instincts can be wrong. When that happens, Analytics is the only reliable way to identify and act on it. While applying Analytics, context and experience are invaluable.

,

,??shared his story of setting up BI framework for UT Austin in his talk ""Sizzle that Sells!! - Secrets of Building a Next Generation Business Intelligence Program"". The major data problems included data not transparent, weak reporting tools for reliable transactional systems, and widespread discrepancies - data definitions, naming conventions, business logic, etc. In order to solve these problems, various challenges had to be overcome such as showing ROI, time-to-market, data stewards availability, etc. 
,
A BI initiative, called Project Information Quest (IQ) was undertaken to deliver accurate and flexible analytical tools and management information to support University leaders in making data-driven decisions. The project was launched with one-on-one and group interviews with business users to define and document requirements. Key interview questions were: What business questions they cannot currently answer?, What business questions take a long time to answer?, What they hate about current reports?

,

He shared the key factors (tips and techniques) that were effectively handled in deploying a successful enterprise Business Intelligence (BI) solution in a world class higher education institution. He discussed how to shorten time-to-market to exceed customer analytical needs and to delight customers. He mentioned that selling a project in current technology trend is the easy part but making it part of the daily routine is harder.

,

Talking about the project, he mentioned that collaboration was critical as the BI program was based on grassroots approach. The focus was on answering the business questions, while shortening time-to-market for analytical needs. Next, good governance helped in creating a shared, clear understanding about strategy, roles, resources and priorities. The success of the project provided a strong foundation to expand on, avoided duplication of similar efforts across campus, and positioned IQ as integral part of university information ecosystem. He stated the following seven golden rules that he learned from his experience:
,
,??,

,??talked about GE Capital's BI pursuits in her talk ""Customer Facing Analytics"". Significant advances in big data, predictive analytics and location services are redefining the role of IT. At GE Capital, Americas ?€? the North American commercial lending and leasing arm of GE Capital ?€? IT innovation pro-actively helps the business and its customers achieve their goals and objectives. IT has expanded boundaries and rapidly evolved from an important back-office function to an integral part of the overall P&L.

,

GE Capital, Americas is striving to better serve customers and provide a differentiated offering. In order to create disruption, one needs commercial intensity as well as market speed. She walked through the firm's focus areas for IT: Geo-spatial, User Experience, Predictive Analytics and Social. She talked about the GE Capital Social Media Command Center to offer free, value-added services to customers. The center provides real-time intuitive visualizations, sentiment analysis, and reputation management. Lastly, she mentioned the key take-aways from her experience as: focus on customer ""stickiness"", embrace new revenue models, continuously analyze success and establish right partnerships for innovation.

,
,
,  "
"
        ,
,
,
,
,??,
,  "
"
Most popular 
, tweets for Jul 21-22 were
,
#BigData drives better decision making, says @TheEconomist Intelligence Unit ,
,
Haskell Data Analysis Cookbook - A practical and concise guide, from data collection to visualization, with examples ,
,
10 types of regressions. DataScienceCentral guide to which one to use ,
,
Microsoft: Data Scientist 
,
,
,  "
"
,
,
, is an Assistant Vice President in MetLife?€?s Global Technology & Operations. She currently serves as the principal subject matter expert for data analysis and business intelligence. As a member of the SWAT Team, MetLife?€?s internal IT consultancy, she explores data science applications for emerging technologies across multiple lines of business worldwide. Prior to joining MetLife, Amy was a senior intelligence analyst supporting various agencies within the United States Intelligence Community and Department of Defense. 
,
,.
,
Here is the second and last part of my interview with her:
,
,
,
, The Synapse website was built because of a strong need to recruit the best technologists. Our application system wasn't much different from other companies and we wanted to change that., On our side, resumes don?€?t give us a good idea of a candidate?€?s skills, especially for developers. Even if a candidate?€?s resume says that they?€?re an expert Java developer, how do we really know? We wouldn't be able tell from their resume unless it was actually written in code, so why not give them a place to do that? And thus, Synapse was born. The condensed application process allows candidates to show off their coding skills and lets us reduce wait time by quickly validating their entries. Synapse creates a user-friendly experience on both sides, which is what we?€?re all about.  
,
,
,
,Data science as a field is maturing, so it?€?s great to see some consensus around the idea that there can be multiple types of data scientists. ,When data science first emerged, it was so focused on the technical; if you couldn't code, there wasn't a place for you. That wasn't necessarily bad, per se, but it was definitely limiting and probably ended up excluding people who were really great strategic thinkers or pattern recognizers. I also think that?€?s why so many companies see disappointing returns on their data science investments. I've recently seen a few companies switch to multi-functional data science teams, so it looks like we?€?re finally rounding that curve. It?€?s amazing to me that an insurer could beat a lot of tech firms to this team formulation strategy, but we did it.
,
,
,
,The best advice that I've gotten is something people hear me say a lot: Stop caring so much about what other people think. It can come across as harsh, but the real message is that people tend to be pretty self-absorbed and they?€?re too busy being concerned with themselves to think about you. , ,Plus, most people are going to forget a ?€?catastrophic?€? presentation, email, whatever, well before you do, so stop beating yourself up about it and move on. Adopting that mindset builds resilience and courage.
,
,
,
,There are two big ones. The first is empathy. This is where so many data science teams fail to deliver and end up frustrating the business. It?€?s impossible to help your business customer if you can?€?t understand a problem from their point of view. , The other one is humility. I've met quite a few data scientists who come across as know-it-alls. The worst part is that they probably don?€?t realize it (and if they do, they?€?re in the wrong job). The best ones understand that no matter how smart they are, they still have a lot to learn. It turns them into mental sponges who are open to the idea of getting outside the safety of the tech bubble. As a result, their worldviews expand and they start thinking about problems in different ways. 
,
,
,
,I?€?m a huge fan of Adam Grant?€?s ,. We tend to think people who give a lot of themselves aren't successful, but Grant?€?s research shows that isn't true?€?to a point. It turns out that the most and least successful people are both classified as givers, but how and why they give are critical determinants for who succeeds and who becomes a doormat. It?€?s a great read for people who are natural givers, but also for those who aren't.
,
I?€?d be remiss if I didn't also mention Simon Sinek's ,. It should be required reading for people managers (or for those aspiring to be one).
,
,
,  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
, July 2014,
,
The Hadoop Summit 2014 in San Jose (June 3-5) brought many innovations to the Hadoop ecosystem, but the one I was most eager to hear about was what was happening with the MLlib component of Apache Spark. Spark 1.0.0 was released just before the conference on May 30 (and a new 1.0.1 release found its way out on July 11).,
,
In case you?€?re not familiar with Spark and MLlib, let me get you quickly up to speed. Spark is a distributed in-memory computation framework, and the project is almost a year old. Apache Spark provides primitives for in-memory cluster computing which is well suited for large-scale machine learning purposes. MLlib is a Spark component focusing on machine learning, with many developers now creating practical machine learning pipelines with MLlib. It became a standard component of Spark in version 0.8 (Sep 2013). The initial contribution for the Spark subproject was from UC Berkeley AMPLab. Due to the rapid adoption of Spark, MLlib has received more and more attention and contributions from the open source machine learning community. At this time, 50+??developers from the open source community have contributed to its codebase. MLlib has features for classification, regression, collaborative filtering, clustering, and decomposition (SVD and PCA).,
,
,
,
,
,
With the release of Spark 1.0, there are some exciting new features in MLlib. Here is a run-down of all the available machine learning functionality in Spark v0.8 and what?€?s new in v1.0. Note that the new decision trees feature includes support for both continuous and categorical features variables.,
,
, ?€? v0.8: logistic regression, linear support vector machines (SVM), and new in v1.0: na??ve Bayes, decision trees.,
, ?€? v0.8: linear regression, and new in v1.0: regression trees.,
, ?€? v0.8: alternating least squares (ALS).,
, ?€? v0.8: k-means for unsupervised machine learning applications.,
, ?€? v0.8: stochastic gradient descent (SGD), and new in 1.0: limited-memory BFGS (L-BFGS). This is a limited memory version of an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden?€?Fletcher?€?Goldfarb?€?Shanno (BFGS) algorithm. It is a popular algorithm for parameter estimation in machine learning.,
, ?€? new in v1.0: singular value decomposition (SVD), and principal component analysis (PCA).,
,
Here?€?s what else is new with MLlib for Spark:,
,
, ?€? The new release has improved organization, and the code examples are useful as templates for standalone applications. The \examples folder has examples for various algorithms along with sample data sets.,
, ?€? Following Spark core, MLlib is guaranteeing binary compatibility for all 1.x releases on stable API. For changes in experimental and developer APIs, a migration guide between releases will be provided. There is now availability of unified API docs for various Spark components.,
, - MLlib now includes full support for sparse data in Scala, Java, and Python (previous versions only supported it in specific algorithms like alternating least squares). It takes advantage of sparse features in both storage and computation in methods including SVM, logistic regression, Lasso, naive Bayes, k-means, and summary statistics.,
, ?€? A distributed matrix has long-typed row and column indices and double-typed values, stored in a distributive manner in one or more RDDs. It is very important to choose the right format to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Spark v1.0 implements three types of distributed matrices in this release and will add more types in the future ?€? RowMatrix, IndexedRowMatrix, and CoordinateMatrix.,
,
,
,
Spark has a 3-month release cycle with a cut-off date of July 25 for new features. Here?€?s what is in the works for the next release:,
,
,
,
,
Most everyone who has taken a look at MLlib expects it to continue to evolve quickly. There was much banter thrown around at the Hadoop Summit regarding what might be beyond MLlib v1.1. Here are some areas that may receive attention moving forward: scalable implementations of well-known and well-understood machine learning algorithms, user friendly documentation and consistent APIs, better integration with Spark SQL, Streaming, and GraphX, addressing practical machine learning pipelines. If only a fraction of these areas come to fruition, the future of MLlib is destined to be bright.,
,
,
,
,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,  "
"
,
,
Written documents are an intuitive and easy way for humans to store, share, and interpret data. Except, when the ,consist of the 30 million scanned books in the ,, or even the 400,000+ pages in long established journals like ,. In these cases we turn to data scientists and their natural language processing tools. , takes a query of words and generates a graph of their popularity in the Google Books Library from 1500 to 2008; however, it is only capable of searching up to twelve words, and even then the graph can be difficult to interpret. Other tools like ,, ,, ,, and , can also provide insights into corpora, but are also limited by query size or the inability to provide temporal word relevance. With these limitations in mind, and the desire to identify previously unknown ?€?buzz-words?€? without an , list, I developed the WordSwarm program.,
,
, generates dynamic word clouds in which the word size changes as the animation moves forward through the corpus. The top words from the preprocessing are colored randomly or from an assigned pallet, sized according to their magnitude at the first date, and then displayed in a pseudo-random location on the screen. The animation progresses into the future by growing or shrinking each word according to its frequency in the corpus at the next date. Clash detection is achieved using a 2D physics engine, which also applies ?€?gravitational force?€? to each word, bringing the larger words closer to the center of the screen.,
,
WordSwarms have been used to show , as inferred from ,, the , as inferred from their frequency in books found in the Google Books library, and the , from records of babies born each year. The , WordSwarm demonstrates the tool?€?s insightful power by showing that the 1980?€?s maintains a biological focus that begins with animal testing in rats that shifts to the basic science of understanding genes, which remains the focus through the 90?€?s. The new millennia brings more advanced understanding of proteins and genomes, but also non-health issues like energy and atmospheric carbon. Finally, the current decade brings forward new knowledge in quantum theory and methods for controlling systems. These few examples only begin to showcase a WordSwarm?€?s ability to easily display chronological trends in diverse corpora.,
,
,
,
,
Try creating your own WordSwarms by downloading the open-source program at ,, where you will also find a tutorial on how to quickly get started. In an age where time and relevance are at a premium, dynamic visual presentation of trends through a free tool like WordSwarm can fuel new insights for data scientists and the general public alike.,
,
,
Dr. Michael Kane is currently serving as a fellow at the U.S. Department of Energy?€?s Advanced Research Project Agency ?€? Energy (ARPA-E). His focus is on technologies for controlling, monitoring, and managing infrastructure systems in order to improve energy efficiency and production.,
,
,
,
,
,
,  "
"
,
,
The Healthcare industry is at a turning point. Huge amounts of analytical talent is flowing into healthcare and by 2016 half of hospitals will be using advanced analytics software, compared to 10% today. This trend is driven by the realization that the best way to help physicians make better treatment decisions while decreasing cost is by leveraging data and predictive modeling. With data and data scientists at the core, the healthcare industry is evolving. Healthcare providers and payers are increasingly turning to big data and analytics, to help them understand their patients and the contexts of their illnesses in more detail. 
,
The , (May 15 & 16, 2014) was organized by the Innovation Enterprise at Philadelphia. A wide range of topics were discussed including clinical decision support, high cost patient treatment, creating physician friendly systems, consumer healthcare engagement, bundled payments, health information exchanges, clinical integration and improving patient safety, genomics & personalized medicine and population healthcare management. 
,
We provide here a summary of selected talks along with the key takeaways. 
,
Here are highlights from Day 1 (Thursday, May 15, 2014): 
,
, delivered a highly insightful talk on ""Innovation, Agility & Generating Value Through Data Integration"". Currently, the major industry challenges are patent cliff / generic pressures, reimbursement requirements becoming more stringent, increased regulatory complexity, and increasing costs to develop new medicines. At the same time, there are disruptive data opportunities because we now have more data than ever before, more efficient and affordable tools to process the data, and open collaboration opportunities.
,
Despite such enormous opportunities, the process of extracting business value from data has been far from easy, particularly due to problems such as data complexity, return on investment (ROI) concerns, unconnected infrastructure, ethical concerns(privacy), economies of scale, lack of data standards and lack of holistic patient understanding. He stressed on connecting to the community to overcome problems such as ""I don't want to share my data, but I would like everyone else to share their data""; ""I don't really trust the data other people generated, I guess I'll do it myself""; and ""I spend most of my time pulling data together instead of analyzing the data"". Knowledge is a commodity, which can be used to broker relationships with partners.
,
Establish a bold, yet credible strategic plan that delivers incremental value over time towards business goals with measured investment. He asked firms to establish a simple framework to inspire, identify, incubate and industrialize innovation, thus, quickly moving from ideas to solutions. During incubation, stay focused on a question of value, keep costs low and undergo quick iterations for continual improvement. This approach reduces risk of strategic investments and enables innovation by making it okay to 'fail'.
,
He discussed principles and practices GSK has followed to quickly mobilize the necessary capabilities (people, processes, systems and data) to enable 6 key business and scientific areas of investigation across the R&D value chain, each focused on a different area of data analytics. No matter how great, monolithic solutions would not work. An information ecosystem needs to be set up to harness data. Be aware of vendors re-branding their legacy products as 'big data' solutions when they really aren't. In the end, he summarized the learning through the following three points:
,
, ?? ,
, talked about current state of healthcare and Big Data opportunities in his presentation ""Leveraging Big Data for Improving Population Health Management"". Health Care reform redefined how individuals can obtain health insurance. Providers will receive incentives on positive outcomes which will lead to their increased interest in improving the health not only of the patients they visit in their offices but the patients they seldom see. The information available about their patients is growing rapidly and can be harvested from sources that are not typically linked to medical records. 
,
Demographic, technological and economic forces are changing the face of health care. Buyers expect greater value, improved quality and better outcomes - at a more affordable cost. Volumes of data create an opportunity for deeper insight, earlier intervention and engagement (1 billion health-related apps will be downloaded by the year 2016). Demand to connect health care and social services is driving formation of new partnerships ($500 billion in avoidable costs with medication adherence).
,
In the healthcare industry, enterprise analytics and Big Data has moved to center stage. Healthcare deals with a variety of data such as machine-to-machine data, transaction data, biometric data, human-generated data, etc. In order to deliver efficient solutions, healthcare needs a 360-degree view of the customer which should include all relevant details such as living conditions, diet, education, social behaviors, driving, sleep schedule, physical health, family medical history, physical activity, and more. 
,
The immense growth in personal, portable and affordable bio-sensors is creating a new era of unprecedented opportunities. Big Data and Analytics can lead to the proactive improvement of population health and wellness by helping healthcare industry improve quality & efficiency, detect diseases early, detect frauds, deliver personalized solutions, and manage population health. His key recommendations were: engage providers, foster transparency, provide for flexibility in information transference, prefer in-house solutions over vendor-generated solutions, and close the quality loop.
,
, gave an interesting talk on ""Healthcare Analytics Driven Optimization for Patient Safety and Physician Experience"". The major healthcare initiative that is predicted for 2014 is healthcare analytics to drive quality efficiency and revenue. The drastic changes in the healthcare payment systems from fee for service, to payments for quality, preventive care and population health drives this initiative.
,
Hospital mergers are sky rocketing to leverage economies of scale to boost efficiency and effectiveness. A staggering 43% of physician's time is being taken up by data entry. There is a threefold increase in EMR adoption in the past 2 years. About 40% of the US hospitals have some form of EMR in place and have been collecting data for the past few years. He described how Adventist Health System is leveraging their data to improve patient safety, quality, cost, physician efficiency and experience.  
,
The number of patients that die of preventable medical errors is similar to having a 747 plane crash every day. Analytics can help identify these errors in time by raising alerts. Monitoring physicians' response to these alerts should be recorded and reported back to help understand what is going wrong. The Analytics system must fine tune the alerts to reduce false positives. The alerts should identify the physicians to engage in a dialogue, and when possible, also recommend the next steps. Data Analytics helps explore opportunities for improving efficiency by engineering time stamp probes in the EMR (Electronic Medical Record). Concluding his talk, he recommended a Physician Dashboard with key metrics such as total number of orders written, total number of notes documented, order set utilization and evidence based order set usage against LOS (Length Of Stay) and cost of care. 
,
, talked about relevance of Big Data to clinical research in his talk titled ""The Era of Big Data Informatics for Clinical and Translational Research"". Despite considerable progress in prevention and treatment, cancer remains the second leading cause of death in the United States. Cancer researchers around the world are generating massive amounts of clinical and genetic data, although due to its volume, complexity and lack of centralization, much is left unanalyzed. 
,
Big Data backed by powerful analytics holds the key to gain important insights from such high volume, variety and velocity data enabling a new understanding of cancer from molecular biology through clinical management. It provides opportunities to ask complex questions and identify novel knowledge from existing data including the study of genetics of an individual?€?s cancer cells, on her response to treatment and sensitivity to side effects. 
,
He talked about the recent developments in Big Data Analytics platforms at Mayo Clinic and how this transformative technology can be harnessed to leverage multidimensional data for developing new preventive measures, diagnostic tools and interventions in cancer research. He stated Mayo Clinic's vision to move away from the current application-centric approach for managing data to data-as-a-service/unified data approach. The initial focus for Big Data efforts is Clinical NLP(Natural Language Processing), which is highly data and process intensive. In conclusion, he stated that Big Data approaches allow us to quantify the biological and non-biological determinants of health and disease. Big Data creates actionable information, particularly for personalizing treatment interventions and identifying new therapeutic potential.
,
,.
,
,
,  "
"
,

,

The Healthcare industry is at a turning point. Huge amounts of analytical talent is flowing into healthcare and by 2016 half of hospitals will be using advanced analytics software, compared to 10% today. This trend is driven by the realization that the best way to help physicians make better treatment decisions while decreasing cost is by leveraging data and predictive modeling. With data and data scientists at the core, the healthcare industry is evolving. Healthcare providers and payers are increasingly turning to big data and analytics, to help them understand their patients and the contexts of their illnesses in more detail.

,

The , (May 15 & 16, 2014) was organized by the Innovation Enterprise at Philadelphia. A wide range of topics were discussed including clinical decision support, high cost patient treatment, creating physician friendly systems, consumer healthcare engagement, bundled payments, health information exchanges, clinical integration and improving patient safety, genomics & personalized medicine and population healthcare management.

,

We provide here a summary of selected talks along with the key takeaways.


,
,.

,

Here are highlights from Day 2 (Friday, May 16, 2014):

,

,??talked about how data can empower health care consumers in his talk titled ""Engagement Strategies, Segmentation, Micro-Targeting and Understanding 'Health Journeys'"". The emergence of an empowered consumer in health care will change many traditional dynamics in the industry. This change will put pressure on all stakeholders across the health ecosystem to individualize and personalize the interaction with their customers or risk losing their loyalty. By leveraging big data and applying marketing tools and techniques that are coupled with predictive analytics, we are able to segment the consumer population with more precision. This segmentation will allow us to align tools, resources and experiences that will help drive consumers to reach their optimal health journey.

,

Today, there is too much generic information, but not enough relevant information, and that makes it difficult to be data empowered. The outreach, engagement, and support strategies need to be personalized based on strong understanding of individual customer's medical needs and lifestyle. He described the ""health journey"" value chain as comprising of data, analytics, care plan (genomics, diagnostics, transition of care), life plan (caregiver, lifestyle, values), and health agents (virtual health, bio-sensors, remote monitoring, public health). Sustainable change will be driven by inter-connectivity across the healthcare ecosystem. Leveraging data to its fullest potential can deliver insights necessary to operationalize and personalize across partners. Health is personal, and thus, personalization is the key for effective healthcare.

,

,??shared insights on how Analytics is impacting the research on Parkinson's disease, in his talk ""Big Data & the Largest Ever Study of Parkinson's Disease"". Since 2009, the National Parkinson Foundation has been conducting the Parkinson?€?s Outcomes Project at 20 centers of excellence around the world. ??Drawing insight from connecting clinical practice to patient outcomes, project leaders have had to confront issues ranging from differences in organization of diverse clinics to cognitive challenges in injecting new data into existing care models. ??A strategic approach to communications has led to widespread acceptance of findings and the potential to change patient outcomes for the better, both within expert clinics and more broadly.

,

A study of the quality of health care delivered to adults in the United States has revealed that only 55% of recommended care is actually delivered. Referring to Atul Gawande's book ""The Checklist Manifesto"", he described a similar checklist for Parkinson's. The checklist has 35 individual items covering points such as psychiatric assessment, cognitive evaluation, autonomic dysfunction, sleep disturbance, falls and rehabilitative therapy. Next, he described the Parkinson?€?s Outcomes Project's size, scope and success achieved so far. He emphasized that evidence-based medicine is not practicable without Big Data.

,

,??gave a talk on ""Using Big Data & Analytics to Drive Business Value"". Quintiles has been on a decade long journey to harness Big Data across research, real-world and directly with patients, to drive value through analytics both internally and external for Biopharma, Payers/Providers and Patients. It is the intersection of clinical trial, real world testing (as compared to Randomized Clinical Trials), and patient insights where value will appear. All stakeholders win when they gain insights across the continuum.

,
He described a reference data architecture to obtain value from data through the value chain: data -> information -> knowledge -> value. Next, he described the data architecture for managing research and real-world testing. True business value is enabled by end-to-end integration and actionable analytics. He emphasized that the key success formula is to understand that one size definitely does not fit all use cases. He recommended the following:
,
,??,

,??talked about the need for a ""Continuous Learning Ecosystem"" in his presentation titled ""Aligning the Healthcare Ecosystem: Data Innovations to Improve Clinical Outcomes and Reduce Costs"". He presented a holistic view of the broader healthcare ecosystem with a special emphasis on the role of data analytics and innovations across various stakeholders in the ecosystem. He discussed use case scenarios and examples that demonstrated: (a) Understanding drivers of costs and outcomes; (b) Understanding drivers and impact of quality measures; (c) Predicting disease trajectory and targeting health management interventions; and (d) Predicting readmissions, length of stay and hospital ?€?never events?€?.

,

Optimal health interventions will require collaborations between stakeholders across the ecosystem to optimize on outcomes, cost, toxicity, efficacy, utilization and economic benefit. He described the framework of a collaborative healthcare supply chain. We need to move from the ad-hoc, informal collaboration prevalent currently, to a business driven industry-wide collaboration established through formal contracts and business relationships. A brief description of the underlying machine learning platform was presented along with the detailed discussion of use case scenarios. He concluded his talk with the following points:
,
,??,
,
,  "
"
,
,
, co-founded Graphistry in early 2014. Previously, he researched programming language design at UC Berkeley and Brown University. His PhD introduced the first multicore web browser (3 PLDI SRC awards) and led to browser parallelization at Mozilla, Samsung, Google, Microsoft Research, and Qualcomm. Leo also performed the largest scale analysis of programming language adoption and social underpinnings (OOPSLA best paper) and, with security researchers at Google, Microsoft, and Brown University, designed several secure web scripting languages.
,
Earlier, he designed Flapjax, the first functional reactive language for highly concurrent web software (OOPSLA best paper). His research was supported by the first Qualcomm Innovation Fellowship (winner among 50 Ph.D. teams at Berkeley and Stanford), the NSF GRFP, and grants from Samsung, Nokia, Microsoft, NVIDIA, Intel, and others.
,
Here is my interview with him:
,
,

,

,??Superconductor is our language from UC Berkeley for visualizing big data sets that runs in the browser. , , You load it in as just another JavaScript library, write layouts in its CSS-like language, and script interactions with normal JavaScript. ??Underneath, Superconductor takes care of generating high-performance code that leverages multicore and GPU hardware.

,

,

,

,??Superconductor makes several architectural leaps over JavaScript libraries like D3 and what browsers try to do natively:

,
,

, ?? ,
,

,

,??At Graphistry, we are building tools for exploring time series and graph data. Our initial focus is monitoring and analyzing data from hardware sensors and software services.

,
One of my favorite early experiments was for exploring election fraud. A tree map showed 100,000+ polling stations ,and sliders let you filter on demographics like voter turnout, which is a key fraud indicator. We knew there was suspicious activity, but this revealed where in the pipeline. We?€?ve had a lot of excitement for opportunities in genetics (especially heatmaps and circos plots) and finance (more fraud heatmaps, large correlation matrices, and ??a lot of time series).
,
,

,
,??The WebCL+WebGL backend is already available under the permissive BSD3 open source license. We want to release the web workers + WebGL backend later this year. The technology is important enough that we?€?re started Graphistry as a company to support and accelerate even more aggressive next steps.

,

,

,

,??We?€?ve been working in this space at Berkeley for 6 years, ,and started Graphistry, Inc. ??a few months ago to support making some even bigger leaps. I can?€?t say much, but Superconductor is going to the cloud. The result is more flexibility, running even on tiny devices, and scaling beyond today?€?s already impressive ~1 million data points to 10-100 million.

,

I invite people needing help with their graph and time series data to join our private beta. This year?€?s focus is on enabling internal dashboard users to get significantly better visibility. Year-end, we will tackle web-scale public deployments.

,

,

,

,??Visual analytics is scaling from tiny charts to deep exploration. With Superconductor, we?€?ve  made leaps in getting the raw performance ,needed for that. However, as part of our work at Graphistry, a full solution requires scaling the visual and interactive design. We see a lot of eye candy that plots big data sets directly on to maps, but in practice, that mostly gives a population heatmap. In contrast, smarter designs like force-directed layouts would reveal clusters, and putting graph mining algorithms at the fingertips of users enables exploring even more nuanced relationships.

,

,

,

,??I switched from designing interactive media to building programming languages as a way to take the pain out of going from idea to code. I repeatedly hit that same roadblock as a scientist exploring quantitative data using Python, R, and Excel. When we realized that my Ph.D. work on parallelizing web browsers could be applied to scaling data exploration, I leapt at the chance.

,

,
,

,

,

,??Collaborate with industrial teams already doing it on real data sets, at true scale, and with big goals in mind. For data visualization in particular, stay up to date with people pushing the field like Jeff Heer and Carlos Scheidegger.

,

,

,

, Not the last book, but definitely the one I?€?d recommend for anyone building high-impact technologies: , by Everett Rogers.

,

Running a company is intense, so I have to make time to read, write, and code. The Bay Area is wonderful if you love food and running.
,
,
,  "
"
Most popular 
, tweets for Jul 23-24 were
,
81% of retail firms gather #BigData, only 34% use analytics to drive pricing optimization - @alteryx infographic ,
,
81% of retail firms gather #BigData, only 34% use analytics to drive pricing optimization - @alteryx infographic ,
,
,
81% of retail firms gather #BigData, only 34% use analytics to drive pricing optimization - @alteryx infographic ,
,
Google Brain project: Google is not really a search company. It's a #MachineLearning (on #BigData) company ,
,
,  "
"
,
,

, is the Lead Strategist for Data at eBay. She was also the Lead for Behavior Insights and Science at eBay, responsible for Behavior Data Platform and Behavior Analytics. Prior to eBay, she was the Director of Media and Front Doors at Yahoo!' - and launched Yahoo! Cricket and Entertainment media properties for Yahoo!?€? in emerging markets. Prior to Yahoo!, Aparna was a Director of e-Commerce, at Omnicell, where built a B2B e-Commerce Platform for Healthcare., Aparna is an accomplished technology and product expert, with a diversified Technology, Product and Business Management experience across several domains in the Internet space. With her strong background in customer experience, in her current role at eBay she is focused on creating Analytical products that leverage eBay?€?s rich and multivariate data.
,
Here is my interview with her:
,
,
,
,: This is a matter of ,terminology. Web Analytics laid the foundation for analyzing user behavior through a user?€?s interaction on a website. , Per Comscore, smartphone subscribers are about 66% of mobile subscribers. The fine line between online and offline will eventually dissolve. Therefore, at eBay, we recognized the need for a more all-inclusive, comprehensive term to define this field of analytics.
,	
,
,
,The metrics depend on the business domain and use-case. The most commonly used are: Repeat visits or visitors and time spent.
,
,
,	 
,The most interesting one ?€? that image size for an item does matter.
,
,
,
,Data Quality and Data Governance. As the demand for data and analytics increases ?€? ensuring the availability and high quality of data is crucial. Data stewardship needs to become a collaborative effort, a joint ownership of data creators, curators, and consumers.  Governance should be strategic, in that it is best achieved when it is interwoven into the end-to-end flow of data creation to consumption.
,
,
,
, For ex., what devices will users use predominantly for search, what device for posting Facebook comments, or to post or upload pictures?  Ability to mine these behavior data points and leverage them to develop and hone customer acquisition, activity, and retention is key.  The combination of versatility and variability of data presents an interesting challenge and describes a complex problem domain. The next 2 years will probably see more theories and developments in this space. Internet of Things is an emerging area that should see some interesting developments. 
,
,
,
,The most important skill for practitioners of analytics in general, not just Online Marketing Analytics, is the ability to ask the right questions to explore with data.  
,
,
,
,I spend time exploring data. eBay has a such a rich and interesting portfolio of data, it is a goldmine for data enthusiasts and can be addictive. I also spend time browsing and shopping on eBay for Collectibles. I love eBay Deals.
,
,
,
,My family. My husband, two daughters and Polo, our Maltese.
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Jul 25, 2014.
,
This week I attended the 
, (Chief Data Officer and Information Quality) symposium. 
Parking was easy to find and the traffic seemed to be less than usual - many in Boston area were probably on a well-deserved summer vacation. 
,
The symposim had an excellent program and was well organized by Richard Y. Wang.
The audience dress was typical of business and management, with most people in suits and ties, ignoring the summer outside.
,
Jeff Kelly and Dave Vellante from SiliconANGLE brought TheCUBE to Cambridge,
and were broadcasting and taking interviews, which you can see at
, playlist.
,
Check informative symposium tweets (including many from ,) at 
, hashtag.
,
,
from MIT Sloan School, one of the luminaries who opened the symposium, emphasized the importance of data interpretation.  He gave an entertaining anecdote about a study (apocryphal?) done at MIT many years ago, which measured the IQ of a class of freshmen, 
and then 4 years later of a class of graduating seniors. The study found a small, but significant decline in IQ. 
(see Dave Vellante tweet /photo
,)
,
What happened? Were there so many formulas crammed into freshmen heads that their IQ declined?  Was there an special vacuum cleaner that sucked IQ from unsuspecting freshmen? 
,
Madnick explanation was that the smartest freshmen graduated in less than 4 years (or perhaps dropped out).  Another data science lesson - 
,
,
U. Georgia Prof. 
, gave an excellent keynote on 
,
He identified 6 forms of capital - Natural, Economic, Human, Organizational, Social, Symbolic and gave good examples of digitization of each type of capital.
,
,??,
The key role of Chief Data Officer, according to Richard Watson, is 
,
,??,
Prof. 
, from MIT has expanded on her article in HBR entitled 
,.  
,
Her point, partly obscured by the catchy title, is that companies need to learn how to use small data effectively before working with Big Data.  She also said
,
,
She divided big opportunities for Big Data into
, 1. extraordinary insights 
, 2. better decisions every day
,
,
While there are companies that get extraordinary insights from Big Data like comScore or UPS, she argued that the biggest impact would be from the second opportunity, making better decisions every day.  As an example, she used 7-Eleven stores in Japan, which are independently operated of the US stores.
,
There are now over 16,000 stores.  Each store is very small, but has about 1,000 items, with items delivered daily and fresh items delivered up to 3 times a day.  The stores use very analytical approach and analytics counselors to maximize their turnover and to decide what to order, and are among (or the) most profitable retail stores in Japan. 
,
Other presentations in the afternoon discussed the role of Big Data is US Department of Defense, Navy, and the Army. 
,
However, my feeling was that something important was missing from the discussion about Big Data great features. 
,
I am as big advocate of Big Data benefits as anyone - after all KDnuggets was 
,. 
,
,
However, I am also very concerned about people not understanding when data is effective and when it is not.  Big Data is great in cases when there many similar examples and many small decisions that need to be automated, like 7-11 Japan or for increasing ad clicks.  Big Data can also create entirely new platforms or products, like Google or Facebook. 
,
However, when there are few similar examples, predictions based on data are not very reliable.
,
While Nate Silver was able to perfectly predict US 2012 presidential elections in every state (using results of many state polls), he was much less accurate in predicting Oscars or 2014 World Cup.
,
No #BigData would have predicted that Brazil would lose to Germany 1:7 in the World Cup semifinal.  Steve Jobs did not rely on data for deciding to create iPhone.  There is no data to help US Air Force reliably decide about next planes it wants to order - that is why we routinely hear about planes behind schedule, over budget, and not needed after all.
,
Even for more trivial cases, like predicting how a person will rate a Netflix movie, 
the best algorithms still have an error of about 0.86 stars out of 5 - 
see my HBR blog
,.
,
Data helps for making decisions for situations that are frequent enough and similar enough, but how can this be quantified ?
,
I would like to see more emphasis and discussion about the boundary where the Big Data is effective and where it is not.
,
What do you think?
,
See also 
,
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
  "














"
,
,
,??,??
,
,
,??,??
,??,??
,
,  "



"
,
,
, is a new non-profit seeking to show off the more altruistic data science projects and resources that are out there. By using a format similar to Hacker News or Reddit, users are able to submit links to relevant sites showing off how data science can provide social good. Projects posted to Data for Good are supposed to be approachable to audiences wider than just data scientists, with the intent being to show decision-makers in governments what data can do for them.,
,
,
,
The projects shown on the site are quite diverse and cover a number of topics, from APIs, to videos, to reports showing how cities were able to use data to balance their budgets. As we can see above, there is a focus on projects that empower ?€?citizens?€? or contribute back to the ?€?community?€?. These socially oriented terms are what dominate the titles of the projects that get posted to the site, showing that the site is adhering to its goal of promoting data science projects aimed at providing social good.,
,
For example, on the front page of the site right now is ,, an API for accessing crisis data from multiple sources in a single standard format. Another example of the sort of post that would go on Data for Good is , showing how the Mayor?€?s Office of Data Analytics tackled the problem of illegal grease disposal using data science in New York City.,
,
,
,
As the diagram above (created July 22, 2014) shows, though the number of posts for the first week was inconsistent from day to day (they were largely by only two unique users), over the past few days, the number of posts per day has stabilized. In addition to this, the posts are now coming from a variety of users, showing that the site is beginning to develop a more involved community. This is a good sign for the site, and hopefully this trend of community involvement continues.,
,
This project comes at a very auspicious time for data science focused on providing social good, with KDD 2014?€?s theme being , and , seeking to connect data scientists and NGOs for social good. Interest in applying data science toward to improving society seems to be gaining momentum.,
,
Overall, Data for Good is tackling the noble mission of showcasing just what data can do for society. If they continue to grow and a community forms around the site, Data for Good could grow into a valuable resource for discovering opportunities and resources in the data science community for socially minded data scientists.,
,
,
,  "
"
,
,
,
Please notice: Upcoming deadline for registration is 30th of August.,
,
More info and registration at:,
,
,
Date: 29th of September to 2nd of October.,
Location: TU Dortmund University, Germany,
,
,
Earth is hit every day by countless particles from outside the solar systems. Research collaborations from all over the world are building gigantic telescopes to capture these events.

But even a standard CCD sensor as included in every smartphone is able to detect these phenomena. While in normal operation regarded as noise,tiny variations in CCD cell charging due to particle impact can be attributed to these events. After constantly capturing images these need to be processed to differentiate interesting events from background noise as caused from heating or faulty pixels. Join our experts on stream data mining and learn how to set up processes for analyzing the imaging data, independently of the underlying system, being it a smartphone or a compute cluster.,
,
Further topics of the summer school:,
,
The summer school is open for international PhD, advanced master students and practitioners, who want to learn cutting edge techniques for machine learning with constrained resources.,
,
,
,
,
,  "
"
,
, (,),
,
Granted, the fear of public speaking is often considered the most common of all phobias. In some (non-scientific) studies, evidence suggests that people fear public speaking more than death itself. It is not unlikely that analysts fear public speaking even more ?€? as they are often distinctly more introverted than your average Joe. As with death, the natural human reflex is to avoid such fearful events. But this natural reaction is probably not the most fruitful, for two major reasons.,
,
,. In our projects, the most frustrating experiences for analysts were created when the results of the analysis were not adopted. Analysts often give their heart and soul in analyzing complex matters. They often spend much less mental energy in trying to ?€?sell?€? the results of their work. In general, however, analysts really care about their work being used and being useful ?€? hence they should care about maximizing the odds of adoption of their work within the organization. This often implies stepping away from the formulae and the data and translating the results of complex procedures into intuitively acceptable results. In order to present a research project successfully to non-experts, public speaking skills play a major role.,
,
,. Today, there is an interesting lack of clarity into the ?€?natural?€? career path for analysts. In our network, we?€?ve seen it all: analysts who become managers of analytical teams (who are suddenly challenged on the core competencies needed to fulfill their new role), analysts who become managers of other teams, and analysts who will eternally remain analysts. In a reality where people are ?€?encouraged?€? to work longer, one needs to ask the crucial question: will I remain forever happy as an analyst? A lack of public speaking skills has been considered a major factor??,??in general - and this is probably not much different for analysts.,
,
,??There is one way to learn public speaking, and that is by doing. Practice makes perfect. And even if I still often feel the urge to flee the five minutes before I have to step into the spotlight, I set the target for myself to present at least twice a year in front of an unknown crowd in order to work on my (public) speaking skills. Working to transform the facts and figures into a story that makes sense both to experts and non-experts is one of the most rewarding tasks for an analyst ?€? if one dares to take a (calculated) risk. But I?€?m guaranteed to walk away stronger, with more experience, and often valuable feedback to enhance my future presentations for clients and colleagues.,
,
,??There?€?s an excellent upcoming opportunity: Predictive Analytics World London is still looking for speakers (analysts and their managers) for their next event, October 29 & 30 in London. Additional details on??,
,
, is a Managing Partner at , and Program Chair PAW London.,
,
,
,  "
"
,

,

, is the Founder of , a company that provides technologies for summarization of opinions, crawling of entity-specific reviews and enabling opinion-driven search. She has over 10 years of experience in research and development of intelligent information systems, particularly in the domain of text information management and analysis. 
,
She received her Ph.D. from the University of Illinois at Urbana-Champaign where she finished a dissertation on opinion-driven decision support system proposing a suite of novel and highly general algorithms for online review crawling, abstractive and concise opinion summarization, and opinion-based entity ranking. She is very passionate about using research in practice and with that focuses on developing techniques that are general and scalable.
,
Here is my interview with her:
,

,
,
,From the time I joined my PhD program, I have been very passionate about developing novel algorithms to solve interesting real world problems. I have always wanted research to be ""usable"" rather than just be on paper. This is what got me into launching , - to turn research into usable technology solving real world problems.
,
,
,
, An Opinion-driven Decision Support System is basically a platform consisting of tools and technologies that would facilitate users and businesses to leverage opinions more efficiently for all sorts of decision making tasks. For example, for a user, this can be a decision making task on which product to purchase based on all available opinions. And for a business, this can be what problems of their very own product to fix based on opinions of other users. To facilitate such a platform there is actually a multitude of interesting research problems ranging from data mining problems to human computer interaction problems. Example of research problems:
,
,
,
One of the easiest ways to analyze the abundance of unstructured opinions is through the summarization of all these opinions. There is a whole range of methods to actually summarize opinions with each method having its pros and cons. A lot of details on the different methods for structured summarization (for eg, opinion through rating scales) can be found in ,. Then you also have unstructured summaries where these are basically textual summaries, trying to summarize the key opinions in text. In recent years, researchers have actually been looking into abstractive micro-summarization (micropinion) format rather than sentence extraction methods, where you try to generate concise, abstractive and readable summaries on key opinions. The reason for this is because full sentences can become verbose and may not be suitable for hand-held devices. The example below shows what a micropinion summary looks like when run on reviews of Acura 2007. This was run using a variant algorithm based on several research projects: , and ,.
,
,
,
More examples: ,, Opinion Acquisition,(,)
,
,
,
,??Based on my experience, the biggest challenge with all these scattered opinions is noise and duplicates. Since opinions can be highly redundant, we have the benefit of volume to actually surface important opinions for analysis. Along with this, we often times would have ?€?noise?€? and duplicates ,that can be highly distracting and can throw algorithms and crawlers off-track. For example, because TripAdvisor allows other sites to use their review APIs, a crawler may regard the reviews from the TripAdvisor site and the site that ?€?borrowed?€? TripAdvisor reviews as two separate sources of reviews when technically they are the same reviews. In addition, if we consider social media content, not all content contain opinions. Some posts are links to articles or videos. Some are just stating what people are currently doing and some posts have a mix of opinions and links. Thus, the irrelevant content is often the ?€?noise?€? and it is very important to offset this noise or be robust to such noise. If we start building applications and analysis around the ?€?noise?€? and the duplicates, then we would end up with false analysis or be frustrated with all the distracting content which surfaces instead of the desired content.
,
,
,
,??Unlike typical sentiment analysis tools that tag text to contain positive or negative sentiments or full-scale market research type of sentiment analysis applications, the goal of FindiLike is to provide pre-requisite ,tools needed in order for any type of opinion-driven analysis to happen. For example, we provide review feeds to companies for their own sentiment analysis tasks.?? We also provide API tools so summarize reviews, tweets and opinions in general so that users and businesses can understand what people are actually saying within such content. We also have a framework that would facilitate opinion-driven search where the user can provide specific opinion requirements, and the framework would actually rank the entities of interest based on how well these opinion requirements are matched. For example, when looking for a laptop, a user may want a laptop that is said to be ?€?lightweight?€? and has ?€?bright screen?€?. FindiLike uses extensions of state-of-the-art research methods to achieve this general goal.
,
,
,
,??, For example, ""iPhone 5s design: positive"" is not as informative as ?€?the iphone 5s is sleek, fits easily in the pocket and has a beautiful interface?€?.
,
Also, in the industry, the current focus of sentiment analysis is primarily restricted to opinions within social media content. However, opinions are far more ubiquitous. You have an abundance of opinions in the form of user reviews which actually contain a lot of details, then you have opinions within user comments (e.g. comments on articles, videos, etc.), you also have opinions within forums. So the value would soon come from all these other sources and not just social media content.

,

,
,
,??Be brave, take risks! You never know where a new adventure would take you. It may take you to a better place than you envisioned or to a place that you have always dreamt about.
,

,
,
,??Well my favorite book on IR is ,, it starts from the very basic and has a lot of information right from inverted indexing to crawling. It is usually my go-to book for basics in IR and text mining. I also enjoy lectures by Andrew Ng on machine learning - he does a great job in explaining obscure concepts. I have also been following Hal Daume?€?s blog (,). He explains specific Machine Learning + NLP related topics in an intuitive manner which is great when you need more of a high-level understanding before you dive into the details or just to get some ideas for your own work.
,
,
,  "
"
,
,
In the emerging model of ,, several projects, including YARN, , and more recently ,, are undertaking the effort of building an operating system for this ?€?new computer.?€??? These new operating systems in turn need the equivalent of the early multi-user time sharing systems to support a multi-tenant environment of diverse application and user ecosystems over the distributed resources of a data-center.



,Hadoop itself is evolving from its MapReduce roots into a data-center operating system for running and managing scalable data-centric applications.?? A variety of applications such as Spark and Hbase have already been implemented on Hadoop-YARN, and initiatives such as Slider, Twill and Spring YARN framework are explicitly chartered to accelerate this evolution.

,


Transitioning from a dedicated resource manager for a single application to a shared platform that can securely host a variety of independent applications requires a built-in mechanism to isolate the tasks of individual applications from one another and the host.?? Indeed, providing isolation among applications is one of the immediate challenges that need to be overcome for this transition to be successful.

,

Current approaches used by YARN to resolve conflicts among applications are incomplete.?? While YARN can use ?€?control groups?€? to provide a rudimentary level of performance isolation, it does not have mechanisms for isolating run-time environments, software dependencies, or security contexts.?? An ideal solution would provide strong isolation that extends beyond compute and memory resources to all potential sources of interference between the applications, such as system configuration, software environment, file system etc. It should also be able to seamlessly integrate with YARN without requiring a disruptive new model or imposing unacceptable overhead.
,
,
,

Containers not only meet the requirements perfectly, but also offer a number of additional advantages with one simple and elegant solution.?? The rest of this article introduces containers and the advantages they deliver in the context of YARN.
,
,
,
Containers are essentially isolated abstractions of the underlying operating system, its resources and their names.?? The keyword is ,.?? Unlike traditional virtual machines that need an additional layer of the guest operating system to virtualize the application, containers are a native operating system abstraction that doesn't need any substantial state.?? It makes them extremely lightweight and scalable.?? Since they look and feel exactly like private instances of the operating system, they are completely transparent to the applications.?? They require no new interfaces to be adopted.?? Containers are particularly suited for Hadoop because they are able to provide strong isolation with almost imperceptible run time overhead and startup latency.

,

, builds on containers by providing a repository of container images that represent self-contained application packages. These can be readily instantiated as application containers on any platform regardless of what software is installed and how it is configured.
,
,
,

The powerful combination of Docker containers and YARN delivers many benefits.

,

,YARN is typically deployed as a multi-tenant environment in large organizations with multiple groups sharing a common IT-managed cluster.?? Tasks from different tenants could potentially be scheduled on the same host. Containers securely isolate those tasks by limiting the privilege scope of a task to the container in which it runs.?? Root in the container is distinct from root on the host.?? Even though the root in a container could run privileged operations, it only affects the container counterparts of the host resources but not the host directly.?? Specific Linux Capabilities possessed by the task, devices accessible to it, etc. are adjusted for each container.

,

When combined with Software Defined Networking techniques, containers isolate the network traffic of different tenant applications. Then the tasks of one customer would not be able to maliciously or unintentionally snoop the traffic of another tenant.

,

,Containers provide resource accounting and enforce resource limits on the processes running within them to prevent applications from stepping on each other. For fine-grain control, resource limits associated with CPU, memory and I/O bandwidth can be tuned on-the-fly as decided by the resource manager.

,
,In a multi-tenant environment, applications have varying resource needs.?? While some tasks are compute-intensive, others could be I/O-bound.?? When the tasks of an I/O bound job are scheduled on a node, its compute resources go unused and vice versa.?? Due to the security risk of co-locating the tasks of different tenants on a shared machine, the idle resources are not allocated to other tenants even if they are able to utilize them.?? Containers prevent such resource under utilization by securely isolating tasks from one another, so that they can be safely co-scheduled on the same host.

,
,Distributed YARN applications consist of tasks that need to run on different cluster nodes deployed with an identical host environment.?? Any discrepancies may cause application misbehavior.?? Containers ensure that all the tasks of an application run in a consistent software environment defined by the container and its image, regardless of the state of the host.?? For example, an application could run in an Ubuntu environment making use of Ubuntu-specific software, while the host itself runs RHEL.

,

,YARN is designed to be modular, with well-defined interfaces between applications and its core.?? This allows applications to be built as independent binaries, which often rely on third party software.?? For example, an application that predicts consumer spending based on linear regression might have a dependency on Matlab.?? Since the tasks of an application could be potentially scheduled to run on any host in the cluster, these software dependencies would have to be installed on all the cluster nodes.?? A variety of applications all sharing the same YARN cluster can quickly clutter the nodes with their respective software dependencies. Installing all dependencies across all hosts is an unscalable approach.?? In some cases, the software dependencies and their versions may be mutually conflicting.

,

With applications encapsulated in Docker containers, software dependencies and the system configuration required for them can be specified independent of the host and other applications running on the cluster.

,

,?? Docker supports a mechanism to programmatically build out a consistent environment required for YARN applications.?? The build process can be run offline with its products stored in the central repository of container images.?? At the time of deployment, the image bits are quickly streamed into the cluster without incurring the overhead of runtime configuration.

,

,The central repository of container images decouples software state and configuration from the hardware, enabling a relatively stateless base platform to be rapidly provisioned for a YARN application, by automatically pulling the right container image on demand.?? When the job finishes the containers are simply removed, returning the cluster to its pristine state.

,
Realizing these benefits requires extensions to Docker as well as to YARN, and bringing them together through right interfaces.?? We are grateful to both communities for their enthusiastic support for the needed ,.?? These new features not only benefit YARN and Docker but also other ambitious efforts addressing the problem of data center resource management through containers such as Mesos and Kubernetes.
,
, is responsible for the multi-tenancy and virtualization infrastructure at ,. He developed the notion of Operating System level virtualization as a part of his Ph.D., which later came to be known in the industry as Containers. Published in OSDI 2002, his work showed for the first time that enterprise applications can be virtualized and live-migrated. Dinesh applied that research to drive industry's first Container virtualization product for enterprise Linux applications at Meiosys, the company behind Linux Containers that IBM acquired in 2005.
, 
He authored over 35 patents and papers in the areas of virtualization, storage and operating systems, and holds a B.E. degree in computer science from BITS-Pilani, India and M.S., M.Phil., and Ph.D. degrees in computer science from Columbia University, New York. He also had teaching experience both from his time at Columbia, as well as multiple meet ups and industry conferences around the world.
,
,
,  "
"
        ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for July 29 and later.
,
See full schedule at , .
,
,  "
"
,
,
, is currently the Chief Architect at ,, an , company, responsible for the overall technology architecture, strategy and direction, including Big Data Platform development.
,
Sastry is a veteran technologist with two and half decades of experience developing, leading and architecting various highly scalable??and distributed systems. Before transitioning to StubHub, he led the architecture transformation of eBay from its monolithic architecture to the distributed, and scalable service oriented architecture that it is today. Prior to joining eBay, Sastry was co-founder and CTO of OpenGridSolutions, Founding member and Architect at SpikeSource, and an architect at Oracle.
,
Here is my interview with him:
,
,
,
,Our vision is to make StubHub a worldwide destination for end-to-end fan experience that includes discovering, purchasing and ,sharing post event experiences. In order to achieve that lofty goal, we need to understand our customers and their interaction patterns better. We need to be able to personalize their experience and recommend events based on their preferences and interests. We also need to understand how well is the experience working out for our customers, for continuous feedback loop to improve the experience. Of course, we need to keep an eye on fraudsters in the process.
,
In order to successfully do all of the above, we need to analyze our data coming from many sources and the big data platform is a great place to start doing that and feeding the results of the analysis (done via what are called Map Reduce Jobs) to appropriate systems. So we began our journey on the Big Data Architecture last year and have made good progress so far.
,
,
,
,
,
The 6 key aspects of the architecture that we think gives us this leverage are the following (not in any particular order)
,
, ?? ,
,
,
,Great question. ,
,
,
, ?? ,

I think that over a span of next 3 to 5 years, organizations will mature using the Hadoop technology and will slowly deprecate data warehouses for active data processing. However, the results of the data analysis may still continue to be stored in a data warehouse for easier access by existing reporting tools. This hypothesis is applicable for organizations that have already begun their big data journey.

,
,

,The challenges can be categorized into two buckets, namely, Technical and operational. The technical challenges are relatively easier to deal with, while the operational challenges take time to address.
,
Technical challenges
,
, ?? ,
Operational challenges
,
, ?? ,
,
,
, The growth of personal Mobile devices is dramatically changing this picture too, making it possible to capture a user?€?s implicit preferences and behaviors and personalizing the content. The distinction between search and recommendations is also dwindling, as search systems are smart enough these days to return personal recommended results.
,
,
,
,When you are dealing with large volumes of data, it is important to be able bucket the data into ,different ?€?sets?€? that are naturally connected through some association. Said it differently, we need to classify the data based on some metadata. For example, in the StubHub use case, we have lots of events and the data associated with them. But lets say, someone is trying to look for events that are ?€?family friendly?€?. That?€?s one example of a meta classification. How do we know which events are family friendly? What determines family-friendliness? This classification typically initially happens through a manual process and then automated through machine learning.
,
,
,
,My simple advice would be to be cognizant of the fact that the technology landscape is rapidly changing in this space and be patient and ready to adapt as appropriate. But sky is the limit in terms of the value they are going to get from the big data systems.
,
,
,
,I don?€?t specifically focus on data science and algorithms per say, but more on the platform/frameworks aspects that enable data scientists to do their analysis. While I read some books on this, I usually get a lot more information and latest trends from the Apache Hadoop and related websites.
,
,
,  "
"
Most popular 
, tweets for Jul 25-27 were
,
Does Apple slow down old iPhones when new ones are released? Hard to believe, but Google trends #BigData supports it ,
,
Does Apple slow down old iPhones when new ones are released? Hard to believe, but Google trends #BigData supports it ,
,
,
From Data Mining at NASA to Teaching #BigData Science at GMU: Interview with Data Scientist ,
,
,
Does Apple slow down old iPhones when new ones are released? Hard to believe, but Google trends #BigData supports it ,
,
,  "
"
By Shane Clyburn, Morgan & Claypool, July 2014.,
,
I am pleased to announce the latest title in Morgan & Claypool?€?s series on Data Mining and Knowledge Discovery:,
,
,
,
Nicola Barbieri, ,
Giuseppe Manco, ,
Ettore Ritacco, ,Paperback ISBN: 9781627052573, $45.00,
eBook ISBN: 9781627052580,May 2014, 197 pages,
,
,
,
The importance of accurate recommender systems has been widely recognized by academia and industry, and recommendation is rapidly becoming one of the most successful applications of data mining and machine learning. Understanding and predicting the choices and preferences of users is a challenging task: real-world scenarios involve users behaving in complex situations, where prior beliefs, specific tendencies, and reciprocal influences jointly contribute to determining the preferences of users toward huge amounts of information, services, and products. Probabilistic modeling represents a robust formal mathematical framework to model these assumptions and study their effects in the recommendation process.,
,
This book starts with a brief summary of the recommendation problem and its challenges and a review of some widely used techniques. Next, we introduce and discuss probabilistic approaches for modeling preference data. We focus our attention on methods based on latent factors, such as mixture models, probabilistic matrix factorization, and topic models, for explicit and implicit preference data. These methods represent a significant advance in the research and technology of recommendation. The resulting models allow us to identify complex patterns in preference data, which can be exploited to predict future purchases effectively.,
,
The extreme sparsity of preference data poses serious challenges to the modeling of user preferences, especially in the cases where few observations are available. Bayesian inference techniques elegantly address the need for regularization, and their integration with latent factor modeling helps to boost the performances of the basic techniques.,
,
We summarize the strengths and weakness of several approaches by considering two different but related evaluation perspectives, namely, rating prediction and recommendation accuracy. Furthermore, we describe how probabilistic methods based on latent factors enable the exploitation of preference patterns in novel applications beyond rating prediction or recommendation accuracy.,
,
We finally discuss the application of probabilistic techniques in two additional scenarios, characterized by the availability of side information besides preference data. In summary, the book categorizes the myriad probabilistic approaches to recommendations and provides guidelines for their adoption in real-world situations.,
,
,
,
Series: Synthesis on Data Mining and Knowledge Discovery,
Series Editors: Jiawei Han, Lise Getoor, Wei Wang, Johannes Gehrke, and Robert Grossman,
,
This book can also be purchased in print from Amazon and other booksellers worldwide.,
Amazon URL: ,
,
, to Synthesis are available for just $99.00 per year. This subscription will provide individuals with unrestricted access to all Synthesis titles: ,
,
Available titles and subject areas:,
,
,
Information for librarians, including pricing and license:,
,
,
Please contact , to request your desk copy  "
"
,
,
Understanding PEOPLE (i.e. not shoppers, not consumers, not respondents) - but people - across platforms and touch-points in an increasingly interconnected world threaded together with always-on technology, presents an opportunity for us to know people more deeply - and take strategic action.
,
Technology is the central driving force amongst the foremost mega and macro trends across industries. It's advancing at such a rapid pace it is not only changing how we do things but changing how we understand the world, business, and people.

,

The , (May 19-21, 2014) at Los Angeles explored the emerging role of decision science and the convergence of knowledge points - insights, foresights, social science, marketing science and intelligence with technology a central driving force and profound connector. ??This year's theme was ""Technology and the Humanization of Data: Synthesizing Insights, Analytics and Relational Database??Strategy.""

,

This event was about accelerating disruptive innovators in the research space and pushing people to take risks to think outside of traditional research methods and insights gathering end explore new tools and technologies.

,

We provide here a summary of selected talks along with the key takeaways.
,
,.
,
Here are highlights from Day 2 (May 20, 2014):
,
, gave a keynote titled ?€?Data, Data Everywhere ?€? The Need for BIG Privacy in a World of Big Data?€?. She reviewed the immense shift occurring on the privacy front and the implications for businesses. She is the architect behind a landmark resolution approved by international data protection and privacy commissioners in Jerusalem in 2010 dubbed ?€?Privacy by Design?€? (PbD).
,
The PbD framework sets out seven foundation principles aimed at ensuring that privacy is embedded into new technologies and business practices from the outset and boils down to three key tenets:
,
, ?? ,
Privacy is really becoming a battle for personal control of one?€?s data?€?a concept entirely at odds with the service-for-data business models of many social media companies, email providers, search engines and mobile services (where location data is becoming a big business). She said that privacy policies are becoming meaningless to people, and warned the audience not to hide behind them. ?€?Privacy isn?€?t something people think they should have to ask for; it?€?s a presumption,?€? she added.  A privacy policy may protect you from a lawsuit, but it won?€?t help you in the court of public opinion, where the stakes may be much higher.
,
She also stressed that it?€?s much more difficult and costly to build privacy onto what already exists. So from a programming standpoint, make sure it?€?s in your code from the get-go. Talking about Big Data, she exhorted the audience to abandon zero-sum thinking and embrace win/win systems, emphasizing that Big Data and Big Privacy can co-exist.
,
, talked about ?€?Location and the Art of Business Analytics?€?. He quickly explained how location-based business analytics powered by GIS (geographical information science) and geo-informatics are transforming the world of intelligence. He mentioned that approximately every 21st century data set contains locations and by 2020, it?€?s estimated that there will be 50 million connected devices that are location aware. Moreover, the consolidation of gadgets into smart portables - from TVs to stethoscopes to POS systems- paired with location data is opening a world of intelligence possibilities. ?€?Location is the new ?€?cookie?€??€?, he added.
,
?€?Place impressions outperform page impressions.?€? He said we?€?re so fixated on looking at the individual that we miss all of the insight to be gained from aggregated location data. ?€?Big Data is not the problem; it?€?s the people who use it,?€? he remarked. He said we often confuse correlation and causality. (Ex. Google Flu Trends). ?€?Intent and sentiment are not the same things?€? he said. ?€?Think similarly about location data as providing context.?€? He advised marketers to stop focusing on location-based coupon-ing. ?€?No one goes to the store to save money,?€? he said. ?€?We go to buy or to research. So find a way to use location data to improve my experience based on my needs.?€?
,
, delivered a talk on ?€?Insights to Integration?€?.  She proposed a way of thinking about pursuing insights that entails a pronounced departure from the cool detachment that underpins research traditionally. Jandie (a P&G insights alum) said researchers have over the years increasingly distanced themselves from people in a misguided attempt to establish objectivity and at the expense of making the connections needed to really understand other people. 
,
About 93% of communication is non-verbal, but observation alone won?€?t get you there, either. She said the job of researchers should be to advocate for the consumer, which means influencing decision makers. ?€?When you really ?€?get?€? me, you feel compelled to act, and that?€?s how a brand discovers where it can fit in someone?€?s life. When we find that fit, we show a person that we understand them and a bond is formed?€? she said.
,
,.
,
,
,  "
"
By Gregory Piatetsky,  
,, Jul 28, 2014.
,
As a media partner for Strata 2014 Conference, KDnuggets is pleased to offer 
our subscribers a free, 2-day conference pass to
,
,, 
,Tools and Techniques that make data Work
,
Oct 15-17, 2014. 
,New York, NY, USA
,
Strata + Hadoop is one of the most important and probably the largest conference on Big Data. 
Quoting the conference homepage,
,
,
To win this pass, please email to , 
,
with the subject: ,
,
and please specify, what in your opinion, are 2-3  
,
,
The winner will be chosen with a random number generator and announced on Aug 12, 2014 along with the most interesting answers.  
,
Please enter only if you are actually able to attend the conference, and note
that you will be responsible for your own travel, hotel, and other expenses. 
,
Apart from one lucky person that will get KDnuggets ticket to Strata NYC, 
others can get 
, by July 31, 2014.
,
Also, get a 
,.
,
,
,  "
"
,
,
October 27-30, 2014, Washington DC, USA,
,
In recent years, ?€?Big Data?€? has become a new ubiquitous term. Big Data is transforming science, engineering, medicine, healthcare, finance, business, and ultimately society itself. The IEEE Big Data has established itself as the top tier research conference in Big Data and IEEE BigData 2014 continues the success of the IEEE BigData 2013. We expect to have an exciting program -??IEEE BigData 2014 has received 271 paper submissions for the main conference and 37 paper submissions for the industry and government program.,
,
There are 21 workshops covering many emerging research areas, so if you miss the deadline to submit a paper to the conference, you are encouraged to submit your research work to one of the workshops or poster program,
,
, (most deadlines are in late August),
,
,
,
Poster abstracts are limited to one page, must be camera-ready, and must follow the same formatting requirements as the main papers,
,
Online Submission:,
,
,
,
,  "
"
        ,  "
"
,
,
, started out with a doctorate in Theoretical Physics and String Theory from the University of Pennsylvania in 2006. His postdoctoral studies in cosmology and string theory, where he wrote 19 papers garnering 650+ citations, then took him to NYU and finally UBC. In 2012, he decided to move into industry, and took on the role of Senior Data Scientist at ,. Thomas has been involved in diverse projects such as behavior analysis, social network analysis, scam detection, bot detection, matching algorithms, topic modelling and semantic analysis.
,
Here is my interview with him:
,
,

,

, is the world?€?s largest online dating site with over 80 million registered users.
,
Everyday 3.6 million unique users log into the site and send between 20k and 30K messages per minute. But what we?€?re most proud of, is all of the relationships that are created as a result of the site.

,

We use data for a lot of things at PlentyOfFish, both internally and externally. We collect data on successful couples that have used the site and use that information to train and update our matching algorithms via neural networks among other things. Internally, we use data for diverse projects like scam detection, predicting user on-boarding/churn, user behavior as well as more advanced social network analysis to understand how users message and cluster together on the site.

,

,

,

,One of the most interesting things about working for PlentyOfFish is getting to work with a huge amount of data on real people. ,People behave very differently when they know they are answering questions on a survey rather than their natural behavior or descriptions of themselves. In the case of looking at the interests users put on their profiles, which is what I focused on for this project, there?€?s no system to game or reason to not simply write what you?€?re actually interested in. This stands in sharp contrast to the sort of BuzzFeed like questionnaires where you might be trying to tailor your answers to get certain results. In our case, we can see how users and interests cluster together by things like location, gender or age. You can start to ask questions like what do people do for fun in Texas vs. California? What group of people is most romantic? Nerdy? Hipster? Sometimes those results can surprise you.

,

,

,

, We?€?re using it as a form of feature reduction. For example a user might list ?€?skiing?€? as an interest, while others might enter things like ?€?snowboarding?€?, or ?€?ski touring?€?. A human looking at that list would likely conclude that all of those people are into mountain based winter sports or outdoor sports. LDA allows us to take all of the things a user can write for their interests and group them naturally into a much smaller number of categories which can be used to search and match on.

,

There are a few properties of LDA that make it a particularly good choice for finding user archetypes. The first is that the topics and what words have a high lift in them are determined organically?? that is the actual users on the site determine them. This eliminates any bias that might be introduced. If every user who listed ?€?snowboarding?€? also listed ?€?puppies?€? then those would be very likely to occur high in a topic together (spoiler alert: they don?€?t). The second property is that LDA is a mixture model. What that means is that each user does not end up as just one thing, but can be a mix of various topics with different weights. If you think about how people actually are, that?€?s a good description. For example, I enjoy outdoor sports, but also nerdy TV, books and video-games. Simply listing that I?€?m into one of those things doesn't capture me as a whole. This model correctly labels me as a mix of all of these things.

,

We?€?re not currently using this model live yet, as we?€?re in the process of discussing implementations for it. It can be used as the basis for a matching algorithm on its own, as a factor in some of our other matching algorithms, or as a way of showing similar users to one someone is viewing. I believe its strongest potential is in search, as we can allow our members to insert whatever they want their potential match to be interested in and show them thematic matches, e.g. typing in ?€?skiing and Netflix?€? will get you matches interested in outdoor sports and ??TV/movies. If your readers want to see it, they should let us (or me) know.

,

,

,

,Our challenges fall into three rough categories. The first is the technical challenge of data gathering and what we now call data engineering. Building out a system to measure every action ,users can take on a site at our scale is an extremely complex task. It?€?s made even more so because as a dating site, users can interact with each other so, much like Facebook, network and social graph effects add another layer of complexity. Once that system is built, we have to store the data, and store it in a way that a Data Scientist like myself can query and work with. I work closely with the team building out this system and I've done a lot of work making custom packages in things like R to handle some of the data.

,

The second is a business goals set of challenges. With all of that data, and all of the things we can do, how do I make the most impact? What?€?s highest value? I?€?m currently the only Data Scientist on the research team (though our research developers do a lot of data science too) so choosing projects is not always an easy task.
,

The third challenge is educational or cultural. ,
,
,.
,
,
,  "
"
,
,
As the amount of data that companies are able to effectively collect, store and analyse increases, organisations now face new challenges in making use out of this vast new resource. Big Data offers all companies the opportunity transform their organisation into a data-driven culture, promising a more efficient business and offering a more informed decision-making process.
,
While data analytics and modeling can offer an accurate picture of what is happening now, predictive analytics gives an extra benefit in bringing together information to offer an accurate prediction of future action. Effectively knowing how customers or markets will behave before they do offers a new opportunity for companies, and if they are able to capitalize on this before competitors then they will gain a crucial advantage in driving success.
,
The , (May 14 & 15, 2014) was organized by the Innovation Enterprise at London, UK. The summit brought together Analytics leaders from various industries for interactive sessions and thought-provoking discussion on how they are using Analytics to gain a clearer picture of their customers, market and organisation as a whole.
,
We provide here a summary of selected talks along with the key takeaways. 
,
Here are highlights from Day 1 (Tuesday, May 14, 2014): 
,
, delivered an insightful talk on ""Leveraging on Inventory Insights to Support Merchant Growth"". In the last 4 years eBay evolved its strategy, leveraging on its strengths to move from being a pure ?€?auction site?€? to being the place where buyers can find things they need and love. In order to achieve this ambitious goal eBay started changing the core of its user experience to create an environment in which large merchants can supply their products and, taking advantage of a uniquely big and diversified audience, create a solid sales channel. He described the role analytics played in this pivotal change, as well as how analytics are helping merchants compete and win in this scenario. 
,
He explained how eBay is supporting increasingly complex shopper journeys, which he broke down into five phases: awareness, research, purchase, fulfillment, and loyalty. He described eBay's progress in the past through the following three phases: Pioneering Phase (1995-2000, focused on  C2c, predominantly auction); Rapid Growth Phase (2000-2005, growth of B2C, online payments); and Retail Phase (2005-2012, category focused, many big retail brands using ebay platform). The next phase is Strategic Partnerships (2012-2015, delivering online/offline value, support with fulfillment/payment and loyalty).
,
Talking about the merchant development efforts, he mentioned that eBay partners with merchants on each step of customer's journey to deliver consumer insights and support business expansion. Analytics builds a forecast based on all we know about sales in a particular category, going down to day-level. Then, we create reports to track progress and help the business investigate any deviation from the target. Towards the end of his presentation, he walked through a few examples to demonstrate how Analytics has helped in acquisition analysis (assessing brand/manufacturer value and identifying the right market, right price).
,
, shared his experience and learning in his presentation ""Data Calling: Transforming an Organisation into an Insight - Driven Business"". Data, Analytics and Technology are just the start of a transformation into an insight driven and user centric organization. Addressing the elements of decision making, the integration with Business Strategy, Organisational structure, processes and tools issues, and the obvious cultural change is a complex effort. That complexity increases when you add to the mix a major acquisition, a significant enterprise re-organization, leadership changes and new competition in your category.  Skype's small strategic team has facilitated this transformation in a company with over 300 million customers in 200 countries. 
,
Though the idea of transforming to data-driven attitude originated in the product team, soon it was realized that this transformation is required across the organization in order to boost innovation, design strategy and improve execution. Defining strategy as the process of moving from current state to the desired state, he mentioned that most organizations have a good understanding of the desired target state; however, they do not understand their current capabilities and challenges well. 
,
Big Data is not just a technical problem, rather it is a human problem, as deriving real benefits from Big Data needs organizational transformation, which includes dealing with feelings, behaviors, mindsets, motivation, etc. Thus, culture is one of the most important levers for Big Data transformation of an organization. To make Big Data initiative work for Skype was like a big puzzle with several components such as culture, strategy, politics, and capabilities assessment. It was important to do the proper due diligence and get the buy-in from the highest level. It is equally important to customize Big Data to align with firm's business strategy. Big Data needs executive support in order to ensure that it does not gets treated by the organization like just another IT initiative. Big Data initiative is also about Branding and Marketing both within the organization as well as outside.
,
Big Data and Analytics cannot live on its own. It needs supporting business infrastructure and transparency-supporting attitude to execute actions based on analytical insights. Big Data is not merely about saving time or streamlining operations. When applied correctly, it can lead to real measurable business benefits on key metrics.
,
, gave a very interesting talk on ""To Be or Not be Engaged: What is the Question (to Ask)?"". In the online world, user engagement refers to the quality of the user experience that emphasizes the phenomena associated with wanting to use an application longer and frequently. User engagement is a complex phenomenon that requires a number of approaches for its measurement: we can ask the user about their experience through questionnaires, we can observe where they look or move the mouse, and we can calculate various web analytic metrics. 
,
In addition to utilitarian factors, such as usability, we must consider the hedonic and experiential factors of interacting with technology, such as fun, fulfillment, play, and user engagement. Large scale measurement of user engagement can be divided into two categories - Intra-session engagement, which measures success in attracting user to remain on site for as long as possible; and Inter-session engagement, which is measured by observing lifetime user value. The former is focused on user activity, whereas the latter is focused on loyalty and popularity. User engagement varies widely based on the nature of online site, such as gaming, search, social media, news, etc. It is important to not just measure user engagement, but also to interpret it well, because sustainable value is obtained only from meaningful customer engagement.
,
User engagement is the quality of user experience that emphasizes the phenomena associated with wanting to use a technological resource longer and frequently. It is an emotional, cognitive and behavioral connection that exists, between a user and a technological resource. The measurement of user engagement can be self-reported, cognition-based or interactive. User interest in web page content is a good predictor of focused attention, which itself is a good predictor of positive affect. In the end, she noted that no one measurement is perfect or complete. Measurement should be applied consistently with attention to reliability.
,
, talked about Analytics over the mobile platform in his talk ""Mobile Analytics - Decisions Driven by Personal Mobile Data Usage"". The mobile is central to our lives in so many different ways.  The data that are generated as a result of its usage tells a story about the shape and pattern of our days. This data can provide the opportunity to understand with detail and precision where, when and how tech companies, service providers and brand marketers can most powerfully engage. 
,
The rise in the popularity of mobile platform has created several dilemma for marketers around targeting, timing, channel, message, and spend optimization. The key challenges in using mobile data are its massive data size, unstructured app data and rapidly changing mobile browsing behavior of users. He explained how analytics can guide the process of identifying patterns, helping structure diverse and unstructured data sets and build frameworks that allow organisations to use them to address the real life marketing challenge of how to best leverage mobile to connect with consumers. 
,
,.
,
,
,  "
"
,

Award: ,
Deadline: ,
,
,
,
With the growing amount of health care data available, there are opportunities to inform policy decisions like never before. VizRisk, sponsored by the US Department of Health and Human Services, is the first behavioral health care visualization challenge, intended to help policy makers make decisions on the future of health care policy.,
,
,
,
If you are , and interested in participating, follow these steps to partake in the challenge:,
,
,
,
,
,
Submissions consist of a video pitch, a demo, and a 250-word written explanation of the project. Winners will be notified by November 24, 2014.,
,
The winners will be decided based on the innovation, design, relevance, and scientific excellence of their project.,
,
,
,
The awards are structured as follows:,
,
Grand Prize - $6000,
Second Prize - $3000,
Third Prize - $2000,
,
In addition to the three top prizes, the top project in each of the four judgment areas (innovation, design, relevance, and scientific excellence) will be awarded $1000.,
,
,
,
For more information, visit the challenge website at ,
,
,
,  "
"
,
,
,
,
,
,
  "
"
,
Latest ,, (Jul 30, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
A small but significant gap in 1-100 PB range separates analysts who work with Terabyte-size commercial data warehouses and 100 PB+ Internet-scale data stores, KDnuggets ,.  "
"
Most popular 
, tweets for Jul 28-29 were
,
Top countries w. >$1B Internet firms: US 87, China 26, Japan 11, UK 10, S. Korea 7, Israel 6 
,
,
Key paper on #DeepLearning: Dropout: A Simple Way to Prevent Neural Networks from Overfitting ,
,
KDnuggets Free Pass to Strata Conference + Hadoop World, New York, Oct 15-17, 2014 
,
,
Data Scientists are among the highest-paid professions (and I think the most fun), but it is really hard to do ,
,
,  "
"
,
,
Understanding PEOPLE (i.e. not shoppers, not consumers, not respondents) - but people - across platforms and touch-points in an increasingly interconnected world threaded together with always-on technology, presents an opportunity for us to know people more deeply - and take strategic action.
,
Technology is the central driving force amongst the foremost mega and macro trends across industries. It's advancing at such a rapid pace it is not only changing how we do things but changing how we understand the world, business, and people.

,

The , (May 19-21, 2014) at Los Angeles explored the emerging role of decision science and the convergence of knowledge points - insights, foresights, social science, marketing science and intelligence with technology a central driving force and profound connector. ??This year's theme was ""Technology and the Humanization of Data: Synthesizing Insights, Analytics and Relational Database??Strategy.""

,

This event was about accelerating disruptive innovators in the research space and pushing people to take risks to think outside of traditional research methods and insights gathering end explore new tools and technologies.

,

We provide here a summary of selected talks along with the key takeaways.
,
,.
,
,.
,
Here are highlights from Day 3 (May 21, 2014):
,
,, Author, Inventor, Director of Engineering, Google gave a keynote titled ?€?The Web Within Us: When Minds & Machines Become One?€?.  He presented an inspiring vision of our ultimate destiny in which we will merge with our machines, can live forever, and are a billion times more intelligent...all within the next three to four decades. ?€?Contrary to popular belief, change is not a constant,?€? Kurzweil explained, ?€?because it is accelerating. If you plot it on a graph, it isn?€?t linear.?€? He explained that since change accelerates exponentially, we have reached a point where the underlying technology that moves our world?€?it may not be on your radar, yet, but it?€?s already out there?€?radically changes every six months, and that rate will only increase. 
,
What may be particularly interesting for researchers, according to Kurzweil is that what?€?s ahead is relatively predictable. There?€?s a formula (formulae, actually). For example, using an algorithm he developed based on historical patterns, Kurzweil was able to predict the advent of the Internet when it was limited to a few scientists?€? computers talking to each other via modem.
,
His predictions?€?many of which were considered quite audacious when he came up with them?€?have come to pass with uncanny accuracy (~86% as of 2009). ?€?If you look at the world around you and apply the simple laws of exponential change, you can then tell what the world will look like at a specific point in time,?€? Kurzweil said. ?€?Maybe not to the level of specificity you would like, but you can get very close.?€? 
,
Kurzweil also noted that people always underestimate the pace of change because ?€?human intuition is linear, not exponential,?€? he said.  ?€?We think in a straight line, so it?€?s difficult for us to grasp how something that seems absurd or impossible today could become a reality almost overnight.?€?
,
, gave a keynote on ?€?Making Data More Human?€?. He taught audience how adding meaning and narrative to huge amounts of data can help people take control of the information that surrounds them, and revolutionize the way we utilize data. Thorp recommended: ?€?think about visualization as problem solving, the act of revealing, not as an end product or a noun, but as a verb.?€? A self-confessed astronomy geek, Thorp recounted the frustration he felt trying to wrap his brain around the white paper results of NASA?€?s Kepler Mission?€?a galactic survey to discover potentially hospitable planets, which has turned up about 4300 exoplanets so far.
,
?€?NASA is good at science, but they?€?re not good at communicating it,?€? he said. So Thorp created a visualization distilling all of the Kepler data, which is absolutely stunning both aesthetically and in its elegance as a communication solution. He explained: ?€?Visualization is truer to the nature of data than charts and graphs.?€? Thorp went through several examples to explain how data can be visualized to provide fresh perspective.
,
Thorp advised companies to start thinking not in terms of data ownership, but in terms of data custodianship and to position themselves as ?€?data ethical?€?, which entails:
,
, ?? ,
, delivered a keynote speech on ?€?When the Future Begins: A Guide to Long-Term?€?. He discussed how technology is not only changing how we do things, but also how we understand the world, business, and people as well as the emerging space of marketing science. He said ?€?In 1800 it took 6 weeks to move an idea from Chicago to New York. Today, anyone can go anywhere within 47 hours, or 47 seconds if you use email?€?. Time lag is our best friend today as it makes intention easy. He gave full form for abbreviation R&D as ?€?Rip off and Duplicate?€?.  
,
A good measure of progress is liberation. Only ideas with true potential make you frown or smile. He exhorted to look for secrets, non-articulated needs. We look for ideas in the wrong people sometimes, just focusing on people who say the right things and sound intelligent. He stressed that we should look for secrets, experiment, recycle failures, be patient and persistent. Companies often feed problems and starve opportunities. Keep ?€?making enemies?€? at the top of the list and remember that ideas that sound strange have potential to succeed.
,
,
,  "
"
,
,
, started out with a doctorate in Theoretical Physics and String Theory from the University of Pennsylvania in 2006. His postdoctoral studies in cosmology and string theory, where he wrote 19 papers garnering 650+ citations, then took him to NYU and finally UBC. In 2012, he decided to move into industry, and took on the role of Senior Data Scientist at ,. Thomas has been involved in diverse projects such as behavior analysis, social network analysis, scam detection, bot detection, matching algorithms, topic modelling and semantic analysis.
,
,.
,
Here is second and last part of my interview with him:
,
,

,

,It?€?s ,: This has always been one of our strongest, differentiators, particularly in an industry in which some of our biggest competitors follow a subscription model. We?€?ve been able to acquire users at a faster rate because we offer a free service.

,

It offers ,: We?€?re the largest online dating site in the world, which means a large selection of potential matches for our users. Even when you filter your search down by location (if you live in small town), or by religion (if you want to meet someone who shares your religious beliefs), you?€?ll still find a huge selection of users.

,

It?€?s always generated ,: As one of the Google AdSense pioneers, we were the first to break the $1M/month milestone with Google. Years later, we built a proprietary self-serve online ad platform that is widely used by affiliate marketers and advertisers.

,

It?€?s a , company: Having achieved this level of success with no investment, Markus Frind remains the sole shareholder and helms one of the few independently held dating sites in the world. Today, unlike any of its competitors, Markus Frind continues to lead the company's day-to-day operations as founding CEO. With just 70 employees, PlentyOfFish is still in ?€?startup?€? mode the team can iterate quickly and effectively, and continues to dominate in a space where the top competitors have TV marketing budgets exceeding 250 million dollars.

,

,
,

, This study was based on the LDA interests algorithm mentioned above. When the model was being built out, we noticed that there were categories focused around romantic interests, i.e. ???€?Candlelight dinners?€? and ?€?long walks on the beach?€?. Another researcher I work with had the clever idea that we could average over peoples?€? archetypes based on location to determine which states had the highest membership in those categories. The result is that study. In the future we could look at a host of other things, including which states are good ?€?matches?€? for each other or where a person should live based on their interests.

,

,

,

A large chunk of it was negative things having to do ,with academia and string theory. The job market for full time faculty positions on the tenure track at elite research universities is pretty grim these days. In string theory there have been a few years where only two or three people are hired in that specialty in all of North America. The alternative, and where I was career wise was to continue doing postdoctoral fellowships which are quite low paying, two to three year contract positions. The work life balance for me was way out of whack and I found myself wanting more control over where I lived and some stability. It?€?s frowned on to talk about money, but I won?€?t pretend that wasn't a factor. I've eaten a lot of ramen in my life.

,

I also found myself wanting to work on something more concrete and connected to the everyday world. String theory is amazing, and I loved spending my time thinking about how the universe began and the building blocks of space and time. That said, it?€?s not likely to produce a testable result in my lifetime (never say never though!). I wanted something where I could actually see the impact of my work on regular people. I thought a bit about finance, but tech just appealed to me more. I liked the idea of creating something, and in the case of PlentyOfFish being able to look at how people interact and meet each other is incredibly interesting to me. I?€?m also a closet romantic, so it sounds corny, but I really like bringing people together, especially ones who otherwise wouldn't have met.

,

There are a lot of connections between data science and string theory, though not at the obvious level. Yes, both things involve math and analysis, but I haven?€?t gotten to the stage of doing algebraic topology to understand dating quite yet. I think the approach to asking questions and solving problems is very similar. Nearly every problem I've solved at PlentyOfFish I had to go off and learn a lot of things. For the interest matching we?€?re talking about here, I taught myself LDA and some Monte Carlo techniques to solve it. In addition, I didn?€?t just hit on LDA, I spent a couple of months trying out different approaches and exploring various options, learning as I went. That has a lot in common with how I worked in physics. When I decided to move more towards cosmology, I had to teach myself modern cosmology and inflationary theory, and rapidly come to grips with the current state of the art. I also had to boil large questions down to something I could actually write equations for and make progress. The same is true in data science.

,

,

,

, The best advice I've received is to constantly make sure you?€?re happy doing what you?€?re doing. I decided at 16 to be a theoretical physicist. I waited until I was in my 30s to question if that was still what I wanted and whether it was making me happy. , Reading that over, it sounds a bit cliche, but I really do believe it.

,

,

,

, On the technical side, I?€?m a big fan of ?€?,?€? by Larry Wasserman. It gives a great crash course in statistical thinking, with proofs and lots of examples for all of the key concepts. I also really like David Barber?€?s ?€?,?€? as I can?€?t emphasize enough how important conditional probability and Bayesian statistics are for my job. I couldn't make a list of books for a Data Scientist and not throw in Tom Mitchell?€?s ?€?,?€?. It?€?s a classic. On the less technical side I think every aspiring Data Scientist and just about anyone in business should read Nate Silver?€?s ?€?,?€? It?€?s a popular audience level book on how to understand probability, think about it and what goes wrong if you don?€?t. It?€?s also a pretty entertaining read.

,

On the just for fun side of things, Patrick Rothfuss and his Kingkiller Chronicles is possibly the best series I've read, and the first book ?€?,?€? might just be the greatest novel I've ever read. I could say similar things about Ernie Cline?€?s ?€?Ready Player One?€?.

,

Right now, I?€?m reading Michael Lewis?€? new book ?€?,?€? about high frequency trading, and his earlier effort ?€?,?€? is another great read.
,
,
,  "
"
,
,
,
,
I would bet that soon, if not today, most uses of statistical modeling methods are going to be in a data science context. ??The guidelines include a section on data science. ??However, I think the importance of data science is going to be such that statistics courses need to go further and not just teach data science as a separate strand, but integrate it throughout the curriculum.,
,
For example, regression is a tool, and it can be used in research statistics to explain data (in which case R-squared and other goodness-of-fit statistics are important), or in data mining to predict new values (in which case predictive performance on a hold-out sample is the key metric).,
,
Statistics courses generally teach regression in the former context. ??Any data science (predictive modeling) angle comes later, if at all. ??When approaching a data mining problem, ""statistically-minded"" analysts are trained to get tangled up in various technically elegant but substantively unimportant issues -- this reinforces the perception in the data science community that statisticians are not relevant to their needs.,
,
We need to embrace the idea that there are (at least) two communities that use the contents of statistical toolkits -- data scientists and research statisticians. We should be the teachers of the tools, and how to use them appropriately in the two distinct contexts in ways that make sense given the real-world needs of the two communities.,
,
, is the President of The Institute for Statistics Education at Statistics.com. He is the developer of Resampling Stats software (originated by Julian Simon in the 1970's), and taught resampling statistics at the U. of Maryland and elsewhere. He is the co-author of , (Wiley, 2006, 2nd ed. 2010), , (Wiley 2014) and many journal articles.,
,
,
,
,
,
 ,  "
"
,
,
As the amount of data that companies are able to effectively collect, store and analyse increases, organisations now face new challenges in making use out of this vast new resource. Big Data offers all companies the opportunity transform their organisation into a data-driven culture, promising a more efficient business and offering a more informed decision-making process.
,
While data analytics and modeling can offer an accurate picture of what is happening now, predictive analytics gives an extra benefit in bringing together information to offer an accurate prediction of future action. Effectively knowing how customers or markets will behave before they do offers a new opportunity for companies, and if they are able to capitalize on this before competitors then they will gain a crucial advantage in driving success.
,
The , (May 14 & 15, 2014) was organized by the Innovation Enterprise at London, UK. The summit brought together Analytics leaders from various industries for interactive sessions and thought-provoking discussion on how they are using Analytics to gain a clearer picture of their customers, market and organisation as a whole.
,
We provide here a summary of selected talks along with the key takeaways. 
,
,.
,
Here are highlights from Day 2 (Wednesday, May 15, 2014): 
,
,??gave an intriguing talk on ""Managing Experiments at Spotify"". Spotify strives for team autonomy and independence. This means that no team should be blocked by others and they should be able to move as fast as they can. The autonomy has proven to be a challenge for managing a centralized and coordinated experimentation infrastructure and analysis. He shared his story of setting up the experimentation infrastructure for Spotify and how he handled experimentation in a complex multi-platform environment.

,

Spotify has over 40 autonomous teams working on features, over 7 platforms with multiple features, and over 3000 source repositories. It is important to align on metrics rather than actual tests. Metrics undergo their own evolution - so, patience and determination is key to identifying the right metrics and the best way to use them. Many organizations miss to focus on functionality as a metric, which is a serious blunder. He also talked about how to find the right balance between speed of development and proper experimentation and data driven culture. In conclusion, he emphasized that:
,
,??,

,??shared some great Big Data learning in his talk ""Big Data Solutions for Marketing Research at ING Retail Netherlands"". The retail banking market demands now more than ever to stay close to the customers, and to carefully understand what services, products, and wishes are relevant for each customer ??at any given time. This sort of marketing research is often beyond the capacity of traditional BI reporting frameworks.
,
There is an immense need to humanize data - through storytelling, visualization and other means - in order to make it truly usable. Data is the fabric of our lives. So, let's give more meaning and context to data. He explained the human expectations from data through the well-known Maslow's Hierarchy of Needs, in context of the retail banking customers. He mentioned that the top 3 tasks of a data scientist are: dimensionality reduction, clustering segmentation and predictive analytics. In data science, it is important to keep it scientific (by cross-validating models and keeping it measurable), and play with it (creating new features and exploring available data).

,
The goal should be to earn the customer's trust by understanding them deeply. The key challenges are that there is not much time to reach (customers want everything NOW, no more room for latency), and there is a lot of information to process (larger context is required for deep learning). By using Hadoop and open source statistical language and tools such R and Python, firms can execute a variety of machine learning algorithms, and scale them out on a distributed computing framework. He described the following as key lessons learned:
,
,??,

,??delivered an insightful talk on ""Predictive Analytic Modelling of Clinical Trial Operations"". Drug development process is highly complicated and costly. The majority of existing tools in pharma companies still use ad-hoc simplified or deterministic models, which leads to inefficient design, under-powered studies, extra costs and drug waste. Efficient design and forecasting clinical trial operation require developing predictive analytic techniques accounting for major uncertainties in input data, stochasticity of the enrolment & events, and hierarchic structure of operational characteristics. Next, he discussed some innovative statistical techniques for predictive modelling: patient's enrollment, trial/site performance, risk-based monitoring, and operational characteristics & costs.

,

He discussed innovative analytic techniques for predictive modelling operational processes in late stage clinical trials. Patient enrollment modelling forms an underlying methodology. As the next stage, more complicated processes on the top of enrollment are modeled by using hierarchic evolving processes. The technique for evaluating predictive distributions is developed. It allows closed-form solutions for many practical scenarios; Monte Carlo simulation is not required. ??Developed tools are applied to dynamic modelling of the number of patients in trial, events in event-driven trials, operational costs, and site risk-based monitoring. Finally, he shared some case studies which involved novel predictive modeling solutions built using C#, R and RExcel.

,

,??talked about the problem of duplicates and his approach for solving it, in his talk ""Data Finds Data ?€? The Rest is Math"". The success of Riot Games is due in no small part to its team?€?s ability to respond to changing landscapes. Achieving this level of adaptability requires talented people with a vast amount of responsibility and ownership. It?€?s an approach that?€?s common across successful companies in the various industries. Modular and agile development enables teams to make rapid progress and adapt to changes, but for a data scientist, they can lead to a painful truth: incompatible data models, each with their own way of identifying a game player.

,

As a start-up, MySQL and Excel were good enough for firm's analytical needs. But after witnessing strong growth, these basic tools were no longer able to handle the workload and thus, the firm had to switch to Hadoop, since it was cost-effective, scalable, and open-source. Peter?€?s Big Data challenge was de-duplication, given that databases were using different IDs for the same player. Player interactions are complex, yet they need to be understood well in order to uniquely identify the player, which is the first step towards providing them a great gaming experience. 
,
Duplicates are a common problem arising from various reasons. Data schemas tend to evolve, leading to duplicates. Sometimes it's nice to keep data separate, such as for security or performance reasons, or simply to decouple project dependencies. In other words, the challenge is that whenever a new record is observed, we need to determine if this matches anything we have seen so far, i.e. the historical records in the database. This can be solved by ""Like Routing"" process for identifying the entity using nGram partitions. Describing the technical part of the solution, he explained the benefits of using Hadoop, Accumulo (distributed column store) and Intelligent keys.

,
,
,  "
"
,
By Gregory Piatetsky,  
,, Jul 31, 2014.
,
,
With hard-to-find ""Data Scientist"" proclaimed the sexiest profession of the 21st century,
data science education is a hot topic, and among the most popular on KDnuggets.
,
KDnuggets has a large directory of different education options under ,
and we will be updating and posting different sections throughout the month of August.
,
If you see some additions or missing programs, please comment below.
,
Here is the first part - 
,
,
,
,
,??,
,
See also 
,
,??,
,
,
,
,??,
,
,
 ,  "
"
,
,
, provided a great opportunity for students, scientists, engineers, data analysts, and marketing professionals ,to learn more about the applications of Big Data.  Session topics included ?€?Enabling Science from Big Image Data,?€? ?€?Engineering Cyber Security and Resilience,?€? ?€?Cloud Forensics,?€? and ?€?Exploiting Big Data in Commerce and Finance.?€?
,
Held at the Tresidder Memorial Union at Stanford University, the ASE International Conference on Big Data Science took place from Tuesday, May 27 ?€? Friday, May 31, 2014. The conference kicked-off with 4 workshops which were held in parallel. Here are highlights from selected talks from the following 2 workshops which were held together:
,
, ?? , 
, gave a keynote titled ?€?From JSON Logs to Latent Variable Models: Knowledge Mining Massive Open Online Courses (MOOC) Data?€?. MOOCs provide data pertaining to hundreds of thousands of students as they navigate through the web based platform, submit assignments and participate in forums. The data comes with a promise of enabling us to find patterns that could help MOOC instructors teach more effectively and improve student engagement. That data also presents us with a unique opportunity to learn about how students learn and helps us improve on-campus instruction as well. 
,
He briefly discussed the predictive models that his group is building which would enable to predict student stop out. His group is consistently working to build innovative platforms that enable data science at scale. He shared details on how they compiled good predictors for MOOC outcome variables. While explaining his data strategy, he mentioned: ?€?We care more about getting the variables right than we care about the models themselves.?€?
,
, delivered a keynote on ?€?Big Data Challenges in the Biosciences?€?. He started with mentioning that rapid decline in the cost to sequence a complex organism?€?s DNA (from $100 million down to $1000) has enabled many new applications in human health, agricultural biotechnology, etc. 
,
This has happened by the advent of a number of high-throughput sequencing technologies, collectively known as next generation sequencing. This is leading to an explosive growth in the number of organisms sequenced, and in the number of individuals sequenced in search of important genetic variations. Next-gen sequencers enable diverse applications, each requiring its own class of supporting algorithms. He shared big data challenges arising from these developments in the context of microbial communities, agricultural biotechnology, and human health. Here are few open problems he shared:
,
,??,

,
,
, delivered a talk on ?€?Code with Locality for Data Storage?€?. Talking about erasure codes for Big Data storage, he mentioned that it is very important to ensure that error patterns which occur frequently (such as single disk failure) can be corrected efficiently. This has motivated the study of codes with good locality, where any data symbol can be reconstructed using a few other code word symbols. He summarized what we know about rate distance trade-offs for such codes. The main challenge in constructing such codes is to maximize their reliability while keeping the field size small. 
,
, talked about ?€?Polynomial Construction of Sector-disk Code?€?. Shum described two types of disk errors: entire disk failures and disk sector failures. He mentioned that in conventional storage codes, a disk sector failure is considered as an entire disk failure, even though most of the remaining sectors in the disk remain intact. 
,
Sector-Disk (SD) code, proposed by Plank et al. recovers a mixture of these two types of failure patterns. He presented a detailed construction of SD codes which specifies the generator matrix using bi-variate polynomials. This new construction can repair any number of disk failures with up to three sector failures.
,
Highlights from conference proceedings will be published soon.
,
,
,  "


























"
,
,
Here is the company, startup, and acquisition activity for July 2014 from 
,.
See the latest under hashtag
,.
,
,
,
,
,??,
,
,  "
"
,  "
"
,

,

As many organizations are now working with unmanageably large data sets, the importance of using and maintaining an analytics platform which can cope with this scale of information is essential. This presents both a challenge and opportunity as organizations must identify patterns and gain actionable results in order to gain a crucial advantage over competitors. Big Data Innovation will help businesses understand & utilize data-driven strategies and discover what disciplines will change because of the advent of data. With a vast amount of data now available, modern businesses are faced with the challenge of storage, management, analysis, visualization, security and disruptive tools & technologies.

,

, (June 4 & 5, 2014) was organized by the Innovation Enterprise at Toronto, Canada. Illustrated intermittently with case studies, interactive panel sessions and deep-dive discussions, this summit offered solutions and insight from the leaders operating in the Big Data space.

,

We provide here a summary of selected talks along with the key takeaways.

,

,.

,

Here are highlights from Day 2 (Thursday, June 5, 2014):
,
, delivered an interesting talk on ""Finding Focus Areas in Big Data"". It was famously said that a wealth of information brings a poverty of attention. Big data can accompany this paralyzing effect that not only stalls progress but undermines confidence in the value of analytics. While the sheer volume of big data overwhelms the most scientific analytical methods, pockets exist that accommodate these scientific methods quite well.
,
Masoud elaborated on a framework that helps find focus areas in big data, and demonstrated that the problem is not information overload; it's filter failure. Citing research and surveys, they stated that market is currently bullish on Big Data.
,
Shirin stated that there are 2 approaches in Analytics, which she referred to as ""Stone Age"" approach and ""Information Age"" approach. She shared the following anonymous quote:
,
,

The ""Stone Age"" approach in Analytics uses Basic Tools + Clever People. Quite a few companies follow this approach which does hold true to some extent - as far as you are looking for the ""known unknowns"" only. Whereas, the ""Information Age"" approach uses Clever Tools + Basic People. By putting the majority of technical expertise in the tools, such organizations have a better distribution of departments with analytical capabilities. Also, this approach is better suited for Big Data, where you look for a signal in huge amount of noise, and are often looking for the ""unknown unknowns"".
,
It is very important to identify the focus areas and ask the right questions. The focus should be on the biggest and highest value opportunities, and within each opportunity, start with questions, not data. Early focus should be on areas that involve no more than first or second degree inferential analysis.
,
Finally, by walking through a practical example, they outlined the following suggestions for Analytics processes:
,
, ?? ,

, gave a thought-provoking talk on ""Big Data Gone Bad, or Bad Data Gone BIG?"". Over the course of his talk, Dave explored various aspects of Big Data.?? Challenging the paradigm and industry hype, he presented a provocative view of analytics. After providing a quick overview of the Royal College, he talked about the role of questions and user's beliefs in Big Data. The big change implied by Big Data is bound to resistance, as the employees worry about loss of control, uncertainty, competence concern, etc.
,
He asked: Is Big Data really the answer or are we just taking bad data creating massive analytics? As a result, are we taking small problems and turning them into BIG ones? Addressing the ultimate question: ""how to generate true business value?"", he shared some tools, techniques and thinking models, to help the audience understand various perspectives when it comes to analytics and the roles they play in decision-making.

,

, talked about ""Big Data In Healthcare ?€? Issues & Opportunities"". For a number of reasons healthcare lags decades behind other sectors in leveraging and adopting information and communication technologies. Big Data has potential to contribute to health service transformation by enabling improved access, quality, safety and personalization of care while containing costs and improving operational efficiencies.
,
He outlined the Big Data opportunities in Healthcare as: Quality, Safety, Access, Efficiency, Cost, Outcomes (reporting and optimization), Analytics (personalized medicine, clinical decision support) and R&D. The new sources of health data include genomic data (gene sequencing data), streamed data (home monitoring, tele-health, bio-sensors) and clinical data (80% unstructured documents, images, clinical or transcribed notes). He highlighted the following major challenges:
,
, ?? ,
Most legacy systems were not initially designed for clinical care. They were focused on visits rather than patients. Those systems had complicated work-flows and poor inter-operability.?? Currently, the healthcare industry is facing major challenges around standards, inter-operability, customization, marketplace fragmentation, regulation and user adoption. Big challenges require big resources, and thus, need for the power of community (eg. World Computing Grid). In conclusion, he recommended eHealth initiatives to focus on: Design (empower patients to boost self-efficacy, connect providers to reduce medical errors), Technology (leverage economies of scale, support & fund innovation), and Governance (national standards & strategy, international comparison & benchmarking).

,

, gave an interesting and insightful talk on ""Why We Should All Love Graph Analytics & Stop Worrying"". Today's complex data is big, variably-structured and densely connected.?? Hashmat explained how size, structure and connectedness have converged to change the way we work with data. Connected data is prevalent in social networking (as you mention), logistics networks (for package routing), financial transaction graphs (for detecting fraud), telecommunications networks, ad optimization, recommendation engines, bio-informatics, and in many other places. He shared the new opportunities for creating end-user value that have emerged in a world of connected data, through illustrations with graph analytics examples implemented using graph database.
,
Graphs are a very powerful tool for dealing with more complex data. He defined data complexity through the following equation:
,
,
The benefits of a graph database include ""miutes to milliseconds"" performance, fit for the domain and business responsiveness (easy to evolve). Then, he walked through several examples such as geographical routes graph, internet networking graph, friendship graph, e-commerce graph (buying patterns & relationship among customers).
,
A graph database is one that uses graph structures with nodes, edges, and properties to represent and store data. Graph databases provide index-free adjacency. Examples of popular graph database: Neo4j, FlockDB, AllegroGraph, InfiniteGraph, OrientDB, etc. Neo4j is a NOSQL graph database with powerful traversal framework. It works with the Cypher query language over HTTP. Cypher is a human readable language that was purpose built for working with graph data (with inspiration from SQL syntax). It's a primary tool for building graph applications. Finally, he suggested that Graph Theory is particularly useful when we first want to gain some insight into a new domain and understand insights to extract from a domain.
,
,
,  "
"
,

,

, has been developing software for 15 years. He is a hands-on expert on Hadoop, NoSQL and Cloud technologies. He consults and teaches Big Data technologies. Sujee has authored a few open source projects and has contributed to Hadoop project. He is an author of open source Hadoop book called ?€?Hadoop illuminated?€?

,

Sujee is the founder of ?€?Big Data Gurus?€? meetup in San Jose, CA. He has presented at various meetups and conferences.
,
Here is my interview with him:
,
,

,

,: Elephant Scale is a boutique company that offers expert consulting and training around Big Data eco system.?? We focus exclusively on Big Data technologies (Hadoop, NoSQL, Cloud, etc.).

,

Elephant Scale was founded by Mark Kerzner and Sujee Maniyam. Both Mark and Sujee are veterans in Big Data space. We provide enterprise support for FreeEed (,) - an open source eDiscovery Engine.?? We are seeing a lot of interest and adoption of FreeEed.?? We are in the process of enhancing FreeEed.
,
We are a very open source friendly company. Look at our Github : ,
,
And we have written an open source book on Hadoop: ?€?hadoop illuminated?€? :

,

,
,

,
, ,

,

There are so many resources now available to learn Hadoop.?? Some of them are online and free! Here are some pointers (in no particular order):
,

,
, ?? ,
And finally, I?€?d like to mention a program that I am involved in -- , (,).?? It is a free, intensive, 6-week, full time training fellowship designed to train data engineers.

,

,

,

,??There is a skills chasm in Big Data.?? There is a huge demand for experienced Big Data developers.?? However someone who just learned Hadoop will have to ?€?prove himself?€? to land a ?€?good job?€?

,

One technique I recommend is, once you have learned the basics of Hadoop, you should try to solve a substantial real world problem.?? Find a data set and try solve an interesting problem.
,
We have a list of publicly available big data sets here:
,
,

,

Even better, if you can do this as an open source project. This will go a long way in helping you with your interview process.

,
Also, getting certified may not be a bad idea.?? Hadoop certifications offered by Cloudera & HortonWorks are pretty affordable.?? And having a certification might add some weight to the resume (especially if your real world expertise is light).

,

,

,

,??Hah :-) 
Both Mark and I were approached by publishers to write a book on Hadoop.?? We thought it would be interesting to write a book on Hadoop, but do it open source, out in the open.
,
And that is what we did.
,
Since we were completely in charge of the content, we could write it at our own pace. And since it is a ?€?living book?€? we can have chapters like ?€?Hadoop Use Cases?€? and ?€?Big Data Eco System?€? -- we keep adding to these chapters.

,

We wrote the book to make Hadoop accessible to a wider audience, not just the deeply technical.?? We have been getting lot of ?€?thank you?€? emails from all around the world -- tells us we did something right :)
,
We like to think of the book as our little contribution to the Hadoop project. Plus the book has given us name recognition also.?? Sometimes we will meet a prospective customer and they will tell us that they enjoyed reading our book! Pretty good :-)
,

The entire book is freely available here: ,
,
And the book content is open source: ,
,
We have released it under a Creative Commons license (same as MIT open course-ware).
,
Second and last part of the interview will be published soon.
,
,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,  "
"
,
By Gregory Piatetsky,  
,, Aug 8, 2014.
,
Data Scientists are in big demand, and as a result there is a boom in Analytics and Data Science Education.  Data Science education, not surprisingly, is among the most popular topics on KDnuggets as well.
,
,
,
Last week I updated the KDnuggets directory  for 
,.
,
Today I updated KDnuggets entry for 
,.
,
Here is the updated information. 
,
,
,
,??,
,
,
,
,??,
,
,
 ,  "
"
Most popular 
, tweets for Aug 6-7 were
,
Watch: #BigData & Brews: Mike Olson, co-founder of Cloudera shares an interesting perspective on SQL ,
,
Statistics is the *least* important part of data science - you can do tech w/out stats, but not w/out coding and DB ,
,
Singapore IDA 1st Open Online Training for #DataScience and Analytics starts Aug 6, with Coursera #MOOC ,
,
Becoming a Data Scientist: MS Program, Bootcamp, or MOOCs? ,
,
,  "
"
,

,

, has been developing software for 15 years. He is a hands-on expert on Hadoop, NoSQL and Cloud technologies. He consults and teaches Big Data technologies. Sujee has authored a few open source projects and has contributed to Hadoop project. He is an author of open source Hadoop book called ?€?Hadoop illuminated?€?

,

Sujee is the founder of ?€?Big Data Gurus?€? meetup in San Jose, CA. He has presented at various meetups and conferences.
,
,.
,
Here is second and last part of my interview with him:
,
,

,

,: Most of the cutting edge technologies are open source.  And as big data developers, we use these open source technologies routinely. So on a ?€?do good?€? level , contributing to open source is a wonderful way to give back.
,
,
,
Most open source contributions take up a lot of personal time.  So it demonstrates one?€?s passion for technology (so that people are not just doing this gig because it pays well :-) )
,
I see that companies have started seeing GitHub profiles of applicants.  If you have a solid track record in open source contributions, you will definitely land on your ?€?dream job?€?.
,
I highly recommend developers to start contributing to open source.  Best way to get started is to join an existing project that you are using and care about.  Most projects welcome new developers.  And the veterans really help out of the newbies.  For example: Cassandra project has a tag on their bug tracking system ?€?low hanging fruits?€? :-)  Start with these and move forward.
,

,
,
,I encourage managers to learn the fundamentals.  They don?€?t necessarily need to learn to code.  But they need to understand the complexities of Big Data systems, so they can manage their teams effectively. 
,
I expect managers to have good domain specific knowledge (e.g. wireless networking, security, etc.). Managers should develop a sense of how to apply Big Data technologies to solve their domain specific problems.  I believe this will really help with their careers.
,
,
,
,Oh :-),
If I can do this, I?€?d be very rich man?€?. *smile*
,
** need to think about this a little more **
,
There is still so much hype around Big Data.  But now companies are starting to adopt big data technologies (Hadoop, NoSQL) a little more. Big Data technologies will continue to mature and become easier to use -- the enterprise features in Hadoop 2 are a good example.
,
,
,
And we will see more ?€?real time?€? or ?€?near real time?€? applications on Hadoop (think Spark, Storm, etc.).
,
,
,
,I know more about ?€?Data Engineering?€? than ?€?Data Science?€?, so let me speak from that perspective.
,
A good data engineer is a 
,
, ?? ,
,I look for people with broad experience in programming plus admin -- generalists than specialists.  When things don?€?t work right, you need to jump in, scan through the log files and figure out what is going on.  Plus being handy in system / performance monitoring tools is key (e.g. why this is machine so slow?).  And they should be very comfortable in scripting (shell or python, etc.).
,
Soft Skills are
,
, ?? ,
,
,
,Network like crazy :-)
,

,
,
,I have two young kids ?€? enough said :-)
,
,
,  "
"
,
,
,
When Edward De Bono spoke about the , way back, he definitely did not visualize the emerging field of data science. The hats were meant to be an aid for lateral thinking and brainstorming. Over the years, the hats have seen many creative uses in the corporate world. 3M used the six thinking hats for new product development, a new type of duct tape. Boeing used the six thinking hats to resolve a management vs. union deadlock. And J. Walter Thompson is reported to have used the method to develop an ad campaign for the Ford Focus.
,
Can the 6 thinking hats really enable such lateral thinking or am I talking through my hat? And where do data scientists come into this argument?
,
An average data-scientist does have to wear multiple hats to do his job, from mining through data to delivering insights that make a difference. But the six thinking hats are not just tools for brainstorming, they are the tools for his job; hats she/he needs to juggle everyday, all 6 of them, to be precise, to be successful at work.
,
Here?€?s how:
,
The white hat:
,
, calls for information known or needed. ""The facts, just the facts.""
,
The data scientist needs: Data, just the data, in all its myriad shapes and sizes from historical customer transactions to syndicated research papers, from the unstructured text data in social media to the king of all data, Big Data. Data is his morning caffeine fix, the starting point for his work, his job, his life.
,
The blue hat:
,
, is used to manage the thinking process. It ensures that the 'Six Thinking Hats' guidelines are observed.
,
The data scientist needs: The blue hat to generate insights from the cartloads of data he sees every hour. The blue hat applies to the statistical algorithms he needs to use; shuffling from logistic regression one day to perfecting Natural Language processing the next; from analyzing normality one day to researching Black Swan events the next, as easily as if they were multi-colored candies to choose from a candy store.
,
The black hat:
,
, is judgment, the devil's advocate or why something may not work.
,
The data scientist needs the black hat to develop hypothesis based analysis. The fanciest statistical model may not work if the assumptions behind it are not sound. Donning a black hat, the data scientist can play the roles of the lawyer and the accused, both at the same time and free himself from the paralysis that data can lead to without arguing the whys and hows of analysis at every step of the way.
,
The yellow hat:
,
, symbolizes brightness and optimism.
,
The data scientist needs large doses of the yellow hat, everyday. Just when the deadline approaches, the project scope gets changed; just when he thought his model was working perfectly, a new variable gets literally dropped out of a hat, not one of Bono?€?s 6. Yellow hat, is not just a hat, it?€?s a survival tool for the data scientist?€?s day in the highs and lows of the data valley.
,
The red hat:
,
,: signifies feelings, hunches and intuition.
,
Intuitively, the data scientist does not depend on intuition, right? He has his data to back him. But then there are dark spaces that even the biggest of big data cannot reach, there are business rules that need to be applied for every statistical model; just that right way of looking at data that develops into the ?€?Eureka?€? beer and diaper insights moment every data scientist dreams of making on the road to creating new ?€?Moneyball?€? history. Call it trial and error, or simply, red hat.
,
The green hat:
,
,: focuses on creativity: the possibilities, alternatives and new ideas.
,
,
The data scientist needs the green hat to think out of his self-made data box. In today?€?s data rich, insight poor (DRIP) world, the green hat helps the data scientist to think of new innovative techniques, use visualization and infographic tools and make his data and insights show the real business value to busy executives who need something that stands out from the data clutter in their constant effort to hang on to their hats and take decisions that spell ROI and business impact.
,
There it is. 
,
6 thinking hats in the life of the data scientist. Does it make you want to toss your hat into the ring? Juggling six hats all day long to get an ounce of insight from an ocean of data?
,
Pablo Picasso once said, ?€?To draw, you must close your eyes and sing.?€? But juggling six hats? That might deserve a resounding hats off, all 6 of them.
,
Reposted with permission from ,
,
, is an Analytics Leader at BRIDGEi2i Analytics Solutions in Bengaluru Area, India, focusing on Financial Services.
,
,
,  "
"
,

,

, was a great opportunity for students, data scientists, ,engineers, data analysts, and marketing professionals to learn more about the applications of Big Data. Session topics included ?€?Enabling Science from Big Image Data,?€? ?€?Engineering Cyber Security and Resilience,?€? ?€?Cloud Forensics,?€? and ?€?Exploiting Big Data in Commerce and Finance.?€?

,

Held at the Tresidder Memorial Union at Stanford University, the ASE International Conference on Big Data Science took place from Tuesday, May 27 ?€? Friday, May 31, 2014.

,

,.

,

,.

,

,.

,
,.

,

Here are highlights from Day 4 (Saturday, May 31, 2014):
,
, kicked off the last day of the conference by delivering a talk on ?€?Randomized matrix algorithms and large-scale scientific data analysis?€?. He started with mentioning that randomization has proved to be a valuable resource for the development of better algorithms. Matrix problems are ubiquitous in many large-scale scientific data analysis applications. He discussed algorithms (in RAM) for least square and low rank approximation. He described the underlying theory and gap between randomized matrix algorithms of theoretical origins and practical applications. He also emphasized on the critical need of bridging the gap between the two.
,
Depending on the situation, better might mean faster in worst-case theory, faster in high-quality numerical implementation, e.g., in RAM or in parallel and distributed environments, or more useful for downstream domain scientists. He mentioned that although a lot of recent progress has been made on theory, implementation, and application of randomized matrix algorithms, there is still an immense need of a great model to bridge the gap between large-scale scientific data analysis and more general large-scale data analysis.
,

, talked about ?€?Discovering Community Structure in Dynamic Social Networks using the Correlation Density Rank?€?. Bidnoi mentioned that a ?€?community?€? in context of social network is defined as a sub graph with a higher internal density and a lower crossing density with respect to other sub graphs. 
,
Community detection is an important research issue in social network analysis (SNA), where the objective is to recognize related sets of members such that intra-community associations are denser than inter-communities associations. She introduced a novel and efficient distance based ranking algorithm, called the ?€?Correlation Density Rank?€? (CDR), which is utilized to derive the community tree from the social network and to develop a tree learning algorithm that is employed to construct an evolving community tree. She also presented an evolution graph of the organizational structure, through which new insights into the dynamic network may be obtained. The experiments, conducted on datasets, both synthetic and real, demonstrated the feasibility and applicability of the framework.

,
, delivered a speech on ?€?Using Interactions in the Quantification of Media Bias?€?. He started with mentioning that media outlets portray themselves as ?€?neutral?€? or ?€?nonpartisan?€? but how can the bias - which really exists - be quantified? Interpretation of coverage of media outlets can lead to evaluation of media bias. However the evaluation is not as reliable as the ground truth because ?€?true reports?€?, ?€?false reports?€?, and ?€?lack of reports?€? compose the media coverage. Media outlets are packed with what is called ?€?spin"" - a type of propaganda used to sway public opinion in favor or against an organization or public figure. 
,
One clear example of favoritism happens in politics; the Pew Research Center has shown that different media outlets attract audiences with different political ideology, which in turn can put pressure on outlets to satisfy what they want to hear leading to spinning the news: a typical vicious cycle. He proposed a mechanism to quantify media bias based on the analysis of relationships between people or organizations in the real world. 
,
He proposed a metric called ?€?coverage?€? that indicates how much the media outlet can be trusted and then showed how the coverage can be applied to the case of party and individual favoritism. He also applied the proposed approach to the US Senate using collaborations between senators in bills' co-sponsorships as the ground truth; the assumption was simple, Senators working more should get more coverage on the media. Their results indicate that most media outlets favor the Democrats and only one favors the Republicans.
,
, gave a talk titled ?€?Information Relaxation is Ultra-diffusive?€?. They investigated how the overall response to a piece of information (a story or an article) evolves and relaxes as a function of time in social networks like Reddit, Digg and YouTube. 
,
They found that the temporal evolution of popularity can be described by a universal function whose parameters depend upon the system under consideration. Whether it is the inter-arrival time between two consecutive votes on a story on Reddit or the comments on a video shared on YouTube, there is always a hierarchy of time scales in information propagation. The hierarchy of time scales led them to believe that the dynamical response of users to information is ultra-diffusive in nature. They showed that an ultra-diffusion based stochastic process can be used to rationalize the observed temporal evolution.
,
,
,  "
"
,
,Sep 30- Oct 2, 2014
,Berkeley, CA
,
The Berkeley MFE Program is joining forces with ENSAE ParisTech (University Paris Saclay) to host the inaugural DataLead conference from September 30th to October 2nd at the Clark Kerr Campus Conference Center at the University of California, Berkeley. 
,
This year's conference will focus on big data applied to real world business issues with particular focus on finance and marketing. It will also cover many other facets of data science with a focus on applications to industries ranging from technology, retail, education, science, and government. 
,
Please visit 
, for more information or to register. 
,
Sponsorship opportunities are also still available and are explained in detail on the website.  
,
Please join us for what will be an exciting and informative 3-day conference!  "
"
By Lisa Solomon, Salford Systems, Aug 2014.,
,
,
,
,
Click to Register: ,
,
,
Click to Register: ,
,
,
Click to Register: ,
,
,
,
,
, $35 (includes free 90-day access to the SPM Salford Predictive Modeler technology),
,
,
Data Mining Quickstart,
,
Hands-On Technical Session (Bring your own laptop),
1:00pm-4:00pm,
,
,
,
,
,
,
,
Salford Systems' Training Seminars offer crystal clear instruction and a wealth of real-world consulting experience. This seminar will provide a great opportunity to use data mining technology and to understand how to apply data mining to your business and/or research needs.,
,
Questions?
Please contact Lisa Solomon ,
,
,  "
"
,
,
?€?Above all else show the data,?€? quoted visualization guru, ,, way back in 1983. While Tufte might have been referring purely to visualization, we can extrapolate his concern very easily to the metrics we use to ?€?show the data?€? as well. Defining the right metrics beforehand that help dashboards users derive truly actionable insights from the data, is a slippery slope unless you have prepared beforehand and bring the right gear with you. 
,
Tableau, a company that produces interactive data visualization products and leader in the 2014 Magic Quadrant for Business Intelligence, included ?€?choosing metrics that matter?€? as the first point in its whitepaper on ,.
,
While this might seem fairly obvious and/or self-explanatory, this blog entry seeks to flesh out the premise and present a simple example that illuminates the difference between a dashboard that contains the right metrics and one that doesn?€?t.
,
Design-thinking is a human-centered approach where the focus is on users and their needs and not on a specific technology. Keeping this in mind, here?€?s a quick checklist to arriving at the right metrics:
,
,
,??,
We worked on a Sales Effectiveness project for a Fortune 100 technology client where we needed to understand the ingredients for top-notch sales performance. As we conversed with potential users of the dashboard, the first issue that arose was the ambiguity in how different managers in the sales organization defined sales performance. While some of them looked at overall revenue as an indicator of great performance, others tended to view revenue in comparison to the initial sales plan. The business impact of this was immediate and compelling ?€? did a Manager dish out bonuses to his Sales team if it had out-performed the company average or if it met sales expectations? What information percolated up the system? We liaised among the users to define metrics that were standardized and immediately understood across the organization and actionable in the context of their overall sales effectiveness strategy.
,
Let?€?s look at a simplified test scenario loosely based on the above assignment. The dashboard below analyzes the sales effectiveness of 19 sales teams (A1-19), covering Sales Areas 1-19 respectively. Can you consider the below dashboard interesting? Maybe, but does it really present actionable insights? Questionably so.
,
,
,
,??,
A much simpler and enormously more useful way to view the same data would be as follows, where Sales Preparedness is a measure of the average no. of training modules that the employees in a Sales Area in a district have completed as a percentage of the total modules available.
,
,
,
,
,
Using this, a district sales manager can immediately:
,
,
,??,
By focusing on metrics that matter and showing the right data, the dashboard becomes much more meaningful. But how does one choose the right visualizations once the metrics have been dealt with?
,
,
,
One example is the ?€?Show Me?€? functionality in Tableau Desktop. This is a distillation of best practices in choosing the right visualization based on the metrics defined and guides lay business users or even developers without an advanced degree in visualization theory and/or design in the right direction.
,
While this nifty feature does not help in deciding the right metrics, one can make use of the same to arrive at interesting and powerful visualizations if one chooses to use Tableau. Once the few dimensions and measures are selected, Show Me automatically generates a visualization that uses all the fields selected. And if that is not satisfactory, one can roll over among the in-built styles with the pointer and identify one that suits specific requirements better.
,
Whichever tool or visualization one decides to go ahead with, care needs to be taken beforehand to start with the fundamental premise ?€?choose metrics that matter.?€?
,
Reposted with permission from: ,
,
,
,
,
,  "
"
        ,
  "
"
By Gregory Piatetsky,  
,, Aug 9, 2014.
,
, is the founder and president of KNIME.com AG, makers of the popular , open source data mining and processing platform. 
,
I have known Michael for many years, and have watched his transition from a leading researcher and organizer of 
,
to the founder and president of a successful company.  In our meetings at different conferences Michael always had interesting comments and observations, so I was glad he found time to answer my questions. 
,
In the first part of the interview I have asked him about KNIME the company, KNIME the software platform, coming new features, his research, Big Data Hype, and more - read on! Michael's bio is at the end of this post. 
Here is
,.
,
,
,
,: There are many differences, the license, the GUI, the breadth of functionality.
KNIME comes under an open source license like Weka, so we can integrate other open source projects such as R and Weka. We also have a super active community adding cool new features continuously as well. Our community contributions are a huge asset, adding highly specialized functionality that you have a hard time finding in other, closed tools.
,
Like many others, KNIME started at a university - however, it didn't start as a research project but from day #1 as a well-designed software project since we knew from the beginning that we needed to build a professional scale platform - a software architect was part of the founding team! That has resulted in KNIME consistently getting very positive ratings when it comes to stability and scalability.
,
,
,
Another, lesser known difference is the graphical workflow editor. All of the tools you list above have that, too, but it was added afterwards. Only KNIME really runs underneath the hood what you model visually. Many of the other tools are based on a different (scripting...) representation that the visual representation must be matched to. And the result you can see in the ,, , and other reports: users consistently rank KNIME highest for ease of use.
,
Finally, KNIME has probably the most comprehensive set of ETL nodes (in addition to lots of analytics and visualization nodes) out there.Thanks to the extensive and powerful ETL component, thanks to its professional open architecture, and thanks to its active community, KNIME enables quick, easy, and seamless integration of tools and data.
,
,
,
,
KNIME creates revenue by licensing additional tools around the open analytics platform KNIME. Those tools allow you to run KNIME more productively in larger teams (such as the KNIME Server), deploy KNIME workflows to others in your group (via the KNIME WebPortal) or make it easier to access Big Data (via the Cloudera certified Big Data Extensions). This is all stuff you can also do with the open source KNIME, but our commercial software just makes your life easier or simply saves you time. We strongly believe in this concept - others provide outdated or crippled, light weight versions that you cannot use for real work - with open source KNIME you can do everything you want. And this model is working well, KNIME is profitable and growing strongly.
,
,
,
,
That's an impression from the past. KNIME initially found very quick traction in life science research because people there had big data problems before that term even became popular. Nowadays our user base in pharma still grows but it grows much faster in other areas, such as customer intelligence, predictive maintenance, and others.
,
I'd like to believe that KNIME was a critical part of the discovery of a new drug but that's hard to validate. Drug discovery is a complex and long process. Plus I am sure nobody would ever admit that out of fear that we'd demand royalties :)
,
,
,
,
We are working hard on a lot things - the Big Data Extensions will grow further to embrace other platforms as well and we will be reaching out to some of the machine libraries there too. We are also making all of our visualizations ""web aware"" so that KNIME can ultimately be run on the web. And we are also working on our Enterprise Server setup, so that big companies can share workflows using distributed servers around the globe. And we will continue to add lots of smaller and bigger nodes to KNIME, of course. We have a completely new Python integration in the works and will also add better JSON support in the next version, to name just two categories.
,
,
,
,
As I said before, we knew from day #1 that we were building a professional scale, open analytics platform (note that I am not claiming we knew from day #1 what a successful business model around that platform would look like, though). So the transition to create a spin off and later move the company to Zurich was pretty natural. At some point in time it became obvious that work needed to be done that was not really useful for an academic research group anymore.
,
Were there surprises? On the business side we all learned a lot of new things. But also on the SW side. To be quite honest, I didn't initially expect that the visual workflow editor would have such an impact. I found it the most natural way to do things but I underestimated what impact it would have for others to document what they were doing and creating blue prints of sophisticated analyses that others could use as a template.
,
,
Michael Berthold got his PhD from Karlsruhe University, Germany, and then spent over 7 years in the US - at CMU, Intel, UC Berkeley, and as director of an industrial think tank in San Francisco.
,
Since August 2003 he holds the Nycomed-Chair for Bioinformatics
and Information Mining at Konstanz University, Germany where
his research focuses on using machine learning methods for the
interactive analysis of large information repositories in the
Life Sciences.
,
Most of the research results of Michael Berthold's group are
made available to the public via the open source data processing
platform KNIME. In 2008 Michael Berthold co-founded KNIME.com AG,
located in Zurich, Switzerland. KNIME.com offers consulting and
training for the KNIME platform in addition to an increasing
range of enterprise products.
,
M. Berthold is a Fellow of the IEEE, Past President of the North
American Fuzzy Information Processing Society, Associate Editor
of several journals and Past-President of the IEEE System, Man,
and Cybernetics Society. He was involved in organization of various conferences,
most notably the IDA-series of symposia on Intelligent Data
Analysis and the conference series on Computational Life Science.
With David Hand, he co-edited a successful textbook
""Intelligent Data Analysis: An Introduction"".  He is also a
co-author of the ""Guide to Intelligent Data Analysis"" (2010). 
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Aug 08-10 were
,
,  "

"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "














"
By Alex Jones, August 10, 2014.,
,
Between rising Data Scientist salaries, rapidly evolving software and hardware needs, and mounting technical hurdles, enterprises are keenly aware of the , of data analytics.,
,
While there is generally little doubt the , of data science, realizing that value can be more cumbersome than anticipated (as with most IT projects). That said, analytics professionals who firmly demonstrate the net gain of their projects, will find themselves with greater organizational support, higher degrees of trust, and more freedom to pursue the innate intellectual curiosity that drives so many data scientists.,
,
As someone who sees beauty in simplicity, I often jot a note or two from reports, articles, and personal experiences. Most recently, I took moment to distill some of the common themes and advice about how data science and analytics initiatives can have the greatest ,.,
,
,
Even with an amazing internal team, occasionally an external recommendation or insight can be just what is needed to spur ideas, enhance cost-savings/ gain assumptions, and improve business buy-in.,
,
Let's take a moment to explore some of the references and sources that can help data science teams conceptualize, estimate and prioritize project opportunities. The , list was compiled based on specificity and robustness of recommendations (not simply buzzword reports), quality and breadth of analysis, frequency of updates, and recognition/ respect among business leaders.,
,
,
,- McKinsey?€?s 2011 report arguably spurred the race for Big Data Analytics. I appreciate McKinsey?€?s specificity in projected cost savings, details and breakdowns of assumptions, and a willingness to make bold recommendations.,
,
,
,Industry specific recommendations provide high level perspective and assumptions are well explained. Case-studies embedded into BCG's reports help to understand the problem at an operational level.,
,
,
,PWC?€?s acquisition and rebranding of Booz and Co to Strategy& may be somewhat confusing, but the insights and specific recommendations help to conceptualize opportunities. I appreciate Booz & Co/ Strategy&'s level of detail in industry-specific reports,
,
,
,Deloitte?€?s industry specific reports and recommendations provide context around a number of analytics initiatives at numerous managerial/ operational levels.,
,
,
,Accenture?€?s recommendations span operational levels from executives to implementation leaders.,
,
,
,
,IBM?€?s expertise in analytics provides both thought leadership and interesting case studies.,
,
,
,Bain?€?s comparisons of major firms and their performance speaks volumes to the value of analytics and a motivating factor for executives.,
,
,
,
,Booz Allen Hamilton?€?s Field Guide to Data Science stands out in my mind as one of the best and concise resources to use to introduce business leaders to data science. Leaders in government recommendations and reports.,
,
,
,Investment Banks are excellent resources for deep analysis into industry specific trends and performance. Although reports are often rather dense, the high level perspective and detailed review can be instrumental in designing high value analytics projects.,
,
,
,- I purposefully steered away from software, hardware and data providers. However, I realize that they can provide exceptional insights. Therefore, below are additional resources to keep an eye on (in no particular order).,
,
,
Hopefully these resources will complement your understanding and intuition to identify new opportunities. Further, consider your industry and operational area to hone in on those thought leaders and resources that are most relevant.,
,
Finally, I'd love to find additional quality references, where do you go for case studies, ideas, and recommendations for big data analytics projects?,
,
, is a Graduate Student at ,.,
,
Reposted with permission from:,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Aug 12 and later.
,
See full schedule at , .
,
,  "
"
        ,  "
"
By Gregory Piatetsky,  
,, Aug 12, 2014.
,
, is the founder and president of KNIME.com AG, makers of the popular , open source data mining and processing platform. 
,
Here is the 
,.
,
,
,
,: 
Two come to mind - first the learning in parallel universes. The idea that objects can have different representations has been floating around for a while (multi view / multi instance learning) but creating a model that combines the patterns from across those representations rather than learning one representation from all those was a new way to think about this. It resulted in a couple of neat algorithms that are now, of course, also part of KNIME.
,
,
The other one created a larger EU consortium and resulted in a nice LNCS volume: 
,, the idea that creativity is triggered by connecting diverse domains. The concept is not new (Arthur Koestler published about this in the 60s) but creating computer based methods to mimic or at least support it was. Some of the output of the project is now also available in KNIME, for instance as part of the network and text mining extensions.
,
,
,
,
,:
Well, see above. We were (and are) not shooting for a truly creative system but more for a system that supports creativity. Ideally you have a system that presents interesting connections/patterns to a user and she says ""what the heck is that???"" and, after some thinking and exploring where that connection comes from, creates an entirely new question that is then answered using the data. 
,
So, in a way, we are trying to build a system that helps you come up with completely new patterns to look for or questions to ask. That would already be a huge improvement over current systems that really only provide answers to questions that you already know how to phrase.
,
,
,
,
,:
It's hype. And as I said above, it's also a problem that pharmas had for at least a decade without calling it ""big data"". I see two problems: 
,
,
Many people seem to believe that if they have more data than they don't really need to understand the data and/or problem anymore. And that leads to the 
,??
,
,
Quite a few of the applications we see really boil down to doing Big ETL and running the analysis on a carefully (and meaningfully!) created aggregate of the data. Don't get me wrong, there are some Big Analytics problems but I agree with David Hand who said something along the lines of ""only a small fraction of all problems are Big Data problems"".
,
So I think Big Data will cease to be such a hype but it will stick around. Fact is that people are now collecting lots more data and now at least they stuff it into Hadoop instead of burning it onto DVDs. Getting meaningful answers from this pile of data will require a combination of ETL and analytics, not necessarily all run in a ""big"" mode, though. So I think the real power will come from combining Big Data tools with the classic, very powerful analytics environments - one of the reasons why we recently added our Big Data Extensions. We have seen people who accessed huge amounts of data in Hadoop, ran some analyses in KNIME and even handed samples over to our R integration to run some bleeding edge algorithm.
,
As a side note: I am curious to see how the algorithm development will change in the coming years - we are working on something called ""Widened Data Mining"", where we invest distributed resources not to speed up analysis but to find better results. I think in the long run we will need to revisit some of the assumptions we have made in the past. Why use a greedy algorithm to find a decision tree, for instance, if I have a million cores that can explore the search space much better than that sequential, greedy algorithm?
,
,
,
,:
I am curious about that myself. I don't think we can go back to before and simply ignore the problem. I think we have good ethical guidelines (and if not - use your common sense). But I am not sure they can be implemented so easily. For instance I am not sure if forbidding Google to deliver certain results is a good way forward, I just don't think that's feasible to enforce. Honestly? I don't know. I am looking forward to interesting discussions about that with my kids. I do wonder what someone thinks about that who grew up with ""all"" information out there in the net...
,
,
,
,:
Practice. 
,
You need to really understand the theory but at the same time you also need to exercise your gut feeling. I often have really smart students who really understand the concepts and then they apply algorithms and you just look over their shoulder and say ""think! Can this result even make sense?!"". And then they say ""ah, you are right, that's impossible!"".
,
This actually triggered our data generation nodes in KNIME - we wanted to have data sets accompany our , textbook that had a lot of those weird cases built-in. So that students can already fall into a lot of traps and gain this experience first-hand, so to speak. But in the end you need to practice running real world data analyses. Over and over again. And then you learn to trust your instincts that tell you ""wait, this is too good to/can't be true!""
,
,
,
I don't have or need much free time. I have a wonderful family and I have a hobby that I love and was lucky enough to turn into a job. Ok, if I had a bit more flexibility in my hobby/job I would go back and write a few more algorithms for KNIME. That would scare the hell out of the other KNIMErs, though.
,
,
, holds the Nycomed-Chair for Bioinformatics
and Information Mining at Konstanz University, Germany where
his research focuses on using machine learning methods for the
interactive analysis of large information repositories in the
Life Sciences.
,
Most of the research results of Michael Berthold's group are
made available to the public via the open source data processing
platform KNIME. 
In 2008 Michael Berthold co-founded KNIME.com AG,
located in Zurich, Switzerland. KNIME.com offers consulting and
training for the KNIME platform in addition to an increasing
range of enterprise products.
,
M. Berthold is a Fellow of the IEEE, Past President of the North American Fuzzy Information Processing Society, Associate Editor of several journals and Past-President of the IEEE System, Man, and Cybernetics Society. He was involved in organization of various conferences, most notably the IDA-series of symposia on Intelligent Data Analysis and the conference series on Computational Life Science. With David Hand, he co-edited a successful textbook ""Intelligent Data Analysis: An Introduction"". He is also a co-author of the ""Guide to Intelligent Data Analysis"" (2010). 
,
,
,  "
"
,
,
Presented by: ,
,
10am PT, San Francisco | 1pm ET - New York | 18:00 - London
,
Duration: Approximately 120 minutes.
,
Cost: Free
,
We data scientists love to create exciting data visualizations and insightful statistical models. However, before we get to that point, usually much effort goes into obtaining, scrubbing, and exploring the required data.
,
The command line, although invented decades ago, is an amazing environment for performing such data science tasks. By combining small, yet powerful, command-line tools you can quickly explore your data and hack together prototypes. New tools such as GNU Parallel, jq, and Drake allow you to use the command line for today's data challenges. Even if you're already comfortable processing data with, for example, R or Python, being able to also leverage the power of the command line can make you a more efficient data scientist.
,
We will make use of the ,, which is a free, open-source virtual environment that allows everybody to get started with data science in minutes. The Data Science Toolbox runs not only on Linux, but also on Mac OS X and Microsoft Windows, so everybody can participate with this hands-on webcast. In about two hours we will cover the following subjects:
,
,??,
Whether you're entirely new to the command line or already dreaming in shell scripts, by the end of this webcast you will have a solid understanding of how to leverage the power of the command line for your next data science project.
,
,
,Jeroen is a Senior Data Scientist at YPlan in New York City. He has an M.Sc. in Artificial Intelligence and a Ph.D. in Machine Learning. He is authoring a book titled ""Data Science at the Command Line"", which will be published by O'Reilly in summer 2014. Jeroen enjoys biking the Brooklyn Bridge, building tools, and eating stroopwafels.   "
"
,
Latest ,, (Aug 13, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
""only a small fraction of all problems are Big Data problems"", David J. Hand, quoted in ,  "
"
,
,
,
Before conducting any major data science project or knowledge discovery research, a good first step is to acquire a robust , to work with. One of the benefits of the social media explosion that has taken place in recent years is that with it has come a profusion of large, free, open data sets, often accompanied by graph/network information and large amounts of metadata. If you?€?re interested in text analytics, graphical models, computer vision, or just need a huge amount of data to stress test your new algorithm/procedure, social media data can be a great resource.
,
,
,
Not all social media data is created equal, however. Whether a particular dataset is a good match for your project depends on what kind of relationships you want to model. Probably the most obvious differentiator between social networks is whether their relationships are , (think follower/followee on Twitter) or , (think friends on Facebook). Depending on the type of model you want to use, and the question you want to ask, this can be relevant and you should consider this when choosing a dataset.
,
Another important criterion to remember is the specific constraints placed on the data based on the particular social media site in question. For example, Twitter artificially imposes a 140 character limit on tweets that can make tasks like sentiment analysis and parts of speech tagging challenging, though there are some , designed specifically for these challenges. Just make sure to keep these restrictions in mind before committing to a dataset.
,
Now where would you get such datasets? There are a couple of sources that contain a variety of datasets to experiment with:
,
,
These two resources have a great number and variety of datasets and should satisfy the needs of a diverse range of data science projects. But what if you have more specific needs?
,
There are many more specific datasets available as well:
,
,
,
,
,
Of course, this list is not comprehensive, and if none of these datasets satisfies your needs, there are KDnuggets directories of , and , to consider. In the end, you should choose the dataset that fits the needs of the project best, and an open social media dataset may be exactly what you need.,
,
,, is a Research Assistant at Arizona State University, working on Data Mining and Machine Learning projects.  He won ASU President's Award for Innovation for his work on the TweetTracker system.
,
,
,
,
,
,
,
 ,
   "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
Over the years, our firm has had many discussions with employers on the eve of a new talent analytics project. Often, it is the firm?€?s first deep-dive look at employee data. Sometimes we act as a strategic sounding board, and sometimes we can help them move directly forward into predictive analytics. It is always interesting.
,
This article will discuss two analytics approaches that we have seen. We will describe the value of each approach, and why you might want to begin with one over another. Though we are speaking about employee analytics, the same logic applies to any kind of business analytics.
,
,
,
,
,
Basic business awareness leads most firms to the conclusion that (a) employees are their largest expense, and (b) employee behavior is a significant driver of corporate success or failure. Other human behavior domains (customer analytics, patient analytics, voter analytics) continue to demonstrate clear ROI from using predictive analytics, and it is natural for employers to ask how the practice can be applied to employees and job candidates for similar ROI.
,
This potential often results in a decree to ?€?look for something interesting in the HR data.?€? We often speak with the professionals who have been given this vague task: to explore HR?€?s system of record: hires, terminations, payroll, home information, demographics, performance reviews. Sometimes, the agenda can also include unstructured data, such as internal email, chats, and social media in or outside of the company.
,
Often the effort is scoped to include a wide swath of employees in diverse roles: all managers, all professional roles, high and low potentials. The objective is almost always a vague ?€?something interesting?€? rather than any specific objective or business outcome.
,
Inevitably, with enough data, enough time, and an expensive visualization package, , will be found. Let it be noted that you can almost always find something of interest in any dataset. Whether it is actionable or not is another story. For example, analysis of HR data may find that:
,
,
The problem is, of course, that these types of projects are fishing in a vast sea of input variables, with no known business outcomes. The problem is that the HR data is essentially meta-data of the employee?€?s life at a company, absent the results of their work.
,
The interesting, high value data - the work, the performance - is generated on the job, in their department after they?€?ve been hired and on-boarded. HR doesn?€?t typically track this data it is tracked by the Line of Business.
,
The real business outcomes, the KPIs, the reasons the business hired the employee, are lived out and documented at each line-of-business for each role: in the sales department, or in the call center, or in the bank branches, or in software development.
,
The problem with the ?€?fishing expedition?€? type of analytics effort is that it wasn?€?t framed correctly to begin with. An effective talent analytics effort simply cannot span , lines of business, across , job description. It needs to be focused and framed.
,
Without measurement of actual business performance from the line-of-business itself, the analysis is fated to discover trivial relationships between input variables. Even with performance data, such an effort can?€?t hope to understand and normalize the subtleties of Key Performance Indicators (KPIs) in each role, line-of-business and location.
,
Our experience with these fishing expeditions is that the results are unlikely to yield patterns that are actionable - if you want greater sales, do you limit hiring to a small number of schools??? No ?€? this limits your sourcing options.?? If you want higher influence scores, do you simply start hiring from more expensive zip codes? No. And is there any proof yet that influence is tied to any kind of performance, in this role, in this company? No.
,
The result of a fishing expedition is descriptive analytics - glorified reporting - not predictive analytics. The only thing that is predicable is that employees may be less than comfortable finding that their employer has been reading their email and social media profiles. It seems obvious, but only by considering , in the analysis?€? can one predict ,.
,
However beautiful interactive dashboards and visualizations of these patterns may be, fishing expedition projects are unlikely to receive follow-up executive sponsorship. And sadly, the reputation of talent analytics projects at the firm becomes trivialized or suspect.
,
,
,
A more narrowly focused ?€?business win?€? that can be implemented to save hundreds of thousands of dollars a year will win over a vague, ambitious effort just about every time. These more focused wins are indicative of millions that can be saved when implemented at full scale.
,
Targeted wins can be led by either HR or line-of-business management (or together).?? They identify a business ?€?pain?€? in a particular role, such as:
,
,
The successful talent analyst goes beyond HR?€?s carefully manicured ?€?system of record?€? and merges actual performance data from the line-of-business. Business relevance is here. Actual outcomes are being tracked in the sales office, where the sales are being made.
,
The data are often quite structured. Even so, performance and KPIs can be confusing and contradictory. It can take effort to untangle signals to devise simple data experiments that deliver results, measured in line-of-business terms (not HR terms). It takes a data scientist with keen control over the data to tease patterns apart to find the truth and present it in a way that can be understood by the business.
,
As in all analytics efforts, a targeted effort with a targeted group is needed for the effort to yield strategic results - not just interesting results.
,
,
,
In marketing analytics, the costs of acquiring and maintaining customers are a vital component of a predictive decision. Likewise for employees, the costs of acquiring, on-boarding, and maintaining staff in a role matter.
,
We don?€?t understand why every line of business or HR department doesn?€?t already know and publish these cost curves, at least for high-volume roles. Most managers know that attrition is expensive, but still only count employee costs in terms of compensation and perhaps the cost of the job advertisement.
,
To understand attrition cost, one must factor in training, productivity, and break-even as outlined in , , and soon churn 203. The cost information provides a vital link to place a value on employee turnover, prioritize efforts, streamline improvement, and to tune predictive models. Managers who care about performance or attrition need this information.
,
,
,
,
,
,
Analysts, HR and line-of-business executives need to obsess over finding incremental fact based solutions to business problems, rather than grandiose patterns seen on a dashboard. HR and analytics teams need to earn their way into bigger budgets and bigger toys.
,
Employee analytics projects should focus on a targeted win.?? Cost modeling is an example of a targeted win and is the ideal way to get started.
,
,
,
,
,
,  "
"
Most popular 
, tweets for Aug 11-12 were
,
The World Bank sums up the entire global economy in one chart: GDP vs Population  , 
,
,
,
Watch: ""Building Recommender Systems"" by Netflix Research/Eng Director Xavier 
, 
Amatriain ,
,
The World Bank sums up the entire global economy in one chart: GDP vs Population  ,
,
The World Bank sums up the entire global economy in one chart: GDP vs Population  ,
,
,  "
"
,
,
The Data Visualization Summit returns to the Westin Copley Place in Boston in just 6 weeks time, uniting data visualization experts, visual designers and business executives alike.
,
View the schedule here: 
,
,
Data visualization now represents an important piece of the puzzle for organizations that want to take control of their data.
,
Set to share their visualization strategies and success include 25+ speakers from companies including 
, and  more.
,
Wondering what to expect? Check out a presentation from the Senior Data Visualization Scientist at Twitter from a previous summit, ,
,
,
Data Visualization provides the lens to bring data insights into sharper focus. Join 200+ of the most innovative minds for two days of interactive discussions, keynote presentations, skill sharing and networking.
,
If you are interested in attending, or for more information, please contact Emma Pawlowski at , (+ 1 415 670 9814). 
Alternatively you can confirm your place here: 
,.
,
We hope to see you in Boston,
,
Innovation Enterprise  "
"
,
,
,
The conversation around Big Data has mostly shifted from ?€?what is it??€? to ?€?how do we handle it??€? and with this shift there has been ,. But while data scientists are adept at many things, a large enterprise hoping to truly capitalize on the value in their data needs more than a team of brilliant data scientists ?€? it needs a strategic leader capable of governing and managing the data, with the authority to enact strategy across departments.
,
At some organizations this has involved appointing a ,, and many more have appointed a senior leadership position with the same focus ?€? but without elevating the role to the C-suite. Although the individual may not be called a CDO, it is more about the scope of responsibility than the title itself. Someone in the organization has to be ultimately responsible for the data.
,
Although many have been quick to brush this latest addition to the C-suite as just another fad, David Linthicum addresses this skepticism aptly ,:
,
,
,
Data is not a fad. In fact, data is exponentially increasing every day, hour, and , of the day, for every business. This means many things: increasing data management challenges, increasing opportunities to better understand customers, increasing privacy concerns, increasing advantages for marketing, and much more. Of the many uncertainties surrounding Big Data, its existence now (I?€?m referring to the data itself, not the buzzword) and going forward should not be one of them. When the conversation surrounding Big Data dies down, it will most likely be because massive data has become the new normal, not because it has disappeared.
,
,
,
,
I was invited by Peter Anlyan to speak as a panelist at , (CDOIQ) in July, discussing how the industry is bridging the talent gap in analytics and data science. As we in the industry are all well aware, there is more focus than ever before on quantitative professionals, but the shortage of qualified analytics professionals and data scientists?? has made hiring a significant challenge for many companies.
,
The talent shortage is great enough, in fact, that some company representatives at the symposium expressed concern about sending their teams to Master?€?s programs for deeper training, lest they be poached away, defeating the investment of time and resources. While high attrition may be a frustrating symptom of the times, I?€?m not sure they have a choice.
,
Luckily, the increase in MOOC?€?s (Massive Online Open Courses) and various bootcamps across the country could offer an alternative to companies not willing to risk investing in a time and money into a full-fledged Master?€?s program. The efficacy of those methods however, depends on the strength of the program as well as the learning style of the individual, as Irmak Sirer of Datascope Analytics ,.
,
Having just read Karen O?€?Leonard?€?s report from Deloitte, ,I was also eager to hear her thoughts on HR and talent analytics at CDOIQ, as well as attend some of the other events to hear more about the?? development of the Chief Data Officer position. You can read more about Karen?€?s thoughts from CDOIQ ,, and??Gregory Piatetsky of kdnuggets also had some ,.
,
,
,
,
My thoughts on the longevity of the CDO role are that the responsibilities are the important part, not the title. , that by 2015, 25% of large global organizations will have appointed Chief Data Officers, so it will be interesting to see if that holds true.,If we?€?re predicting the future in C-Level hires though, , to throw their hat in the ring?
,
, is the head of Burtch Works Executive Recruiting, which specializes in marketing research, marketing science, credit and risk analytics.
Reposted from
,.
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Aug 14, 2014.
,
As a media partner for Strata 2014 Conference, KDnuggets had raffled among our subscribers a free, 2-day conference pass to
,
,, 
,Tools and Techniques that make data Work
,
Oct 15-17, 2014. 
,New York, NY, USA
,
Strata + Hadoop is one of the most important and probably the largest conference on Big Data. 
,
Check 
, to see why!
,
Congratulations to Sima Yazdani from Cisco who won the KDnuggets pass!
,
All other readers can get a 
,
,
As part of the raffle, we have asked our readers to name top 2-3 most important trends in Analytics, Data Science, and Big Data in 2014-2015, and here are the answers.
,
Most popular trends were
,
,??,
Comparing with responses for
,, a similar raffle KDnuggets conducted before Feb 2014 Strata Santa Clara conference, we see only one new topic - ""Deep Learning"".  The other 3 topics - Spark, Democratization of Analytics, Real-time processing were also mentioned in February.
,
Here are the selected responses.
,
JL:,
,
,??,
SH:,
,
,??,
GG:,
,
,??,
JMK:,
,
,??,
syc:
,
,??,
SJ:,
,
,??,
RJ:,
,
,??,
PON:,
,
,??,
KB:,
,
,??,
DP:,
,
,??,
SY:,
,
,??,
w2s:
,
,??,
PL:,
,
,??,
SD:,
,
,??,
KO:,
,
,??,
PG:,
,
,??,
AS:,
,
,??,
,
,
,??,
,
,
 ,  "
"
,
,
,
,
,
Let's grow a community of researchers and science-lovers who are passionate about understanding complex systems. We will be working on an adaptation of Thermostatistics together for this purpose! Your contributions will provide a year-long runway for a collaborative infrastructure as well as educational materials for this emerging field.
,
,
,
In the late 1800s, a growing belief in the atomic theory of matter solidified the importance of Statistical Mechanics as a means of deriving Thermodynamics. Thermostatistics focuses on this connection between the ""microscopic"" Statistical Mechanics of atoms and molecules and the ""macroscopic"" Thermodynamics of the behavior of systems at our scale and beyond.
,
Thermodynamics was originally developed during the industrial revolution to better understand the properties of steam engines. With great accuracy, it is assumed that the molecules of steam in the engine do not interact. Unsurprisingly, scientists have had trouble applying Thermostatistics to ""complex"" systems, where interactions between the system components overwhelmingly determine the macroscopic system behavior that emerges
,
,
,
The emergence of this field coincides with the rapid growth of complex internet technologies and the ""big data"" they collect. As a data scientist at Radius.com, I engage every day with a database of 27 million small businesses in the United States - an incredibly complex system. Many important systems are complex, such as metabolism, social networks, the behavior of cities, and food webs. Thermostatistics is increasingly referenced as potential bedrock for a theoretical understanding of these types of systems.
,
It's time that we apply the scientific method to this big data we are accumulating. This project is a push to generalize the classical Thermostatistics of steam engines so that we might fit this data we are gathering and pave the way toward a physics of complex systems.
,
,
,
This project hopes to empower and grow a community of researchers who are passionate about adapting Thermostatistics to better describe the behavior of complex systems. If you would like to join our research community, check out our public Facebook group, our curated Google group, and our public GitHub repo.
,
If this campaign succeeds, I will give away:
,
,??,
Additionally, you can expect the following for contributing:
,
,??,
Contribute to this experiment at 
,
,
,
,
 ,  "
"
,
,
, is a Senior Data Scientist at ,, where she built out their cross-platform data infrastructure and oversees the statistical techniques used for analyzing features and optimizing revenue. Blue Shell Games is a casino gaming studio best known for Lucky Slots, one of the top grossing casino games of 2012 and 2013. Prior to Blue Shell Games, Pallas was on TinyCo's Data Team, where she worked on in-game economy optimization and monetization for Tiny Zoo, one of the top grossing mobile games of 2011. 
,
Here is my interview with her:
,
,
,
,:  Blue Shell Games is social casino gaming company that develops for iOS, Android, ,Kindle Fire and Facebook. We are best known for Lucky Slots, one of the top grossing social casino games on both Facebook and iOS.  We use data science to optimize our live games in order to maximize revenue and retention. Data Science informs which features to build and how players will experience new features. When we run sales or live operations, we use data science to personalize each event.
,

,
,
,Lean Data is my way of saying in-house Big Data analysis can be ?€? and should be ?€? a priority for small studios. Big Data can sound intimidating, so I prefer Lean Data. The only requirement for Lean Data is that the entire company be on board. A small data team is not very effective if they work in isolation. If studios want to maximize data insights, take the Lean Data approach and make sure every department that touches a product knows how to do simple data analyses. I've even seen medium size studios of 100-150 employees benefit from the Lean Data approach, wherein everyone has basic analysis skills.
,
Metrics are often seen as a side task, often implemented last because they are neither user-facing nor intrinsic to the core game mechanics. In having every department use the data and have a personal stake in the quality of the data, metrics become part of each feature instead of an after-thought. I personally benefit from having everyone consume data because I am a one-person team with no checks and balances on my work. I love it when a UI artist comes to me and says this doesn't make any sense. , This quick turnaround time both improves Blue Shell?€?s bottom line and makes my job incredibly rewarding.

,
,
,
,At the end of the day everything comes down to Revenue. However, you have to retain users long enough to monetize off of them. If nothing else, a studio needs revenue, Daily Active User(DAU), and install data per player. From here, you can derive n-day retention. What percent of the players that installed on June 1st also played on June 4th? Answering that question gives you 3-day retention for the June 1st install cohort. You also want to look at n-day Average Revenue Per User(ARPU). It?€?s similar to Average Revenue Per Daily Active User(ARPDAU), but normalized for size of the install cohort. 
,
Of the users that installed June 1st, how much money did they spend on June 1st, 2nd, and 3rd? Taking these three days of revenue and dividing by the number of installs on June 1st gives you 3-day ARPU. You?€?ll start to see trends and might notice 6 and 7 day ARPU are much higher than 5 day ARPU. You can then look at 6 and 7 day retention and make sure users are sticking around long enough for your game to monetize off of them.  
,
,
,
, ,Step one is documentation. ,Every game has a design document that all of the teams refer to when a game is being built. Each team gives feedback about what is reasonable and unreasonable to implement and this shapes the final product. You have to take a similar approach to data. Each game has a metrics spec and every team gets a say in the final output. This enables each department having a stake in metrics and their implementation.
,

As for cross-product projects, at the end of the day I work for Blue Shell Games, not a single game like Lucky Slots. Having cross-product projects gives me an opportunity to push myself and make sure that the recommendations I?€?m making are in the best interest of the company?€?s bottom line. A good example is cross promotion. While the source game might suffer, Blue Shell Games comes out net positive. This is why cross promoting users from one game to another is standard practice.
,
,.
,
,
,  "
"
,
,Trevor Hastie and Robert Tibshirani, Stanford University
,
Harvard Conference Center, Boston, MA - October 27-28, 2014
,
This 
,
gives a detailed overview of statistical models
for data mining, inference and prediction.  With the rapid
developments in internet technology, genomics, financial risk
modeling, and other high-tech industries, we rely increasingly more on
data analysis and statistical models to exploit the vast amounts of
data at our fingertips.
,
In this course we emphasize the tools useful for tackling modern-day
data analysis problems. From the vast array of tools available, we have
selected what we consider are the most relevant and exciting. 
,
Our top-ten list of topics are:
,
,??,
Our earlier courses are not a prerequisite for this new course. Although there is some overlap with past courses, our new course contains many topics not covered by us before.
,
The material is based on recent papers by the authors and other researchers, as well as the new second edition of our best selling book:
,
,, ,
Hastie, Tibshirani & Friedman, Springer-Verlag, 2008.
,
A copy of this book will be given to all attendees.
,
The lectures will consist of video-projected presentations and discussion. Go to the site 
,
,
for more information and online registration.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
The process of identifying and developing a relevant business use case for Hadoop continues to stump IT managers and stall their attempts to derive new value from their data and achieve big data success. Figuring out the right proof-of-concept (POC) use case for your organization can help bring stakeholders together to build a long-term strategy for successful big data implementation.
,
Use case development requires education on big data technologies. Hadoop is a departure from the traditional enterprise data warehouse approach, both from an architecture and skills standpoint. You need to gather some concise materials that explain what Hadoop is, why it is important to your organization and what the benefits are. There is a wealth of information readily available. The key is to collect materials that are tailored to your organization?€?s specific needs and pain points.
,
Once your team has educated themselves and the necessary executives and business stakeholders, you need to setup a Hadoop cluster if you have not done so already. Setting up a Hadoop cluster is relatively easy. It can be done on some old, discarded equipment (with some local storage) or inexpensive commodity hardware.
,
Next, identify a small, low impact use case for implementation on the cluster. The application should be a good candidate for Hadoop so a successful POC will be assured. When selecting a ,, identify the problems you are attempting to solve. Pick something small and achievable ?€? do not attempt to boil the ocean. Most important, identify pain points that can be solved with Hadoop. Things like cost reduction or processing constraints are problems that Hadoop excels at solving.
,
Like all new technology, Hadoop is the ?€?shiny new object?€? that you will want to test and explore, and try to solve all kinds of problems with at once. Your Hadoop environment can become complex very quickly, so it is imperative you stay focused on your initial use case. Once the POC is successful, you will have the ammunition to demonstrate Hadoop?€?s value to the organization and build stakeholder buy-in.
,
Long-term planning is essential to a successful big data implementation. You need to decide what the Physical Hardware Architectures are going to be. You also need to decide whether this is something your team will create and deploy, will be created and deployed by other departments within your organization, or outsourced to a solution provider (such as ,).
,
There are several factors that need to be considered when designing the architecture. For starters, setup the initial logical and physical architectures, and incorporate data layers into your design. Target de-normalizing data and persisting data as much as possible to enable the creation of your enterprise data hub. You also need to formulate system development plans for functionality.
,
,
,
Big data technologies such as Hadoop require a fundamental shift in your approach to data management. In order to manage risk, MetaScale recommends that organizations bring in outside help to get started. Developing a partnership with a proven solution provider allows you to focus on your core competencies of extracting value from your data while compressing the time to value of your big data initiatives.
,
,
Bio: ,, ,, leads Marketing and Operations at MetaScale, a big data company of Sears Holdings Corporation. As a member of the MetaScale Team, Nathan is committed to helping IT and business professionals understand how big data tools such as Hadoop and NoSQL can benefit their organization.
,
,
,
,  "
"
,
,
I first met Pedro at KDD-95 in Montreal, the first international conference on 
Knowledge Discovery and Data Mining.  His modest manner did not conceal his brilliance, 
and he impressed me with his rare talent of being able to explain difficult technical issues quite clearly.
I quite enjoyed a dinner conversation we had then in a small Montreal restaurant, 
but enjoyed our interaction even more when,
in my capacity of KDD Awards chair, I presented him with the 
, in New York City. 
When Pedro again won the best paper award next year at KDD-99, I half-jokingly asked him to give others a chance.  
,
However, Pedro has won numerous awards since then for research in areas spanning Machine Learning, Data Mining, Artificial Intelligence, Statistics, and Databases, and his outstanding contributions were recognized by 
,, the highest awards in the field - a  Data Mining/Data Science ""Nobel Prize"".
,
Many of his award winning research ideas are implemented in software, which Prof. Domingos has made freely available.  This includes 
,
,??,
To learn more about his research, here are some of his most cited papers via
, and
,.
,
This is the first part of the interview. Here is
,
,
,
,
,: 
My work fits into all three, and I gave up long ago trying to parse
the differences between them. But from a practical point of view these
are different - if overlapping - communities, and it's good to have a
map of the territory. Machine learning, because of its origins in AI,
is more concerned with making the computer as autonomous as possible.
,
,
There's a lot of emphasis on scaling up, of course, but also on
dealing with real data in all its messiness. Statistics provides a lot
of the underpinnings of this whole enterprise. As a field, however,
it's also more conservative than computer science. So it's good to
have both a common core and different emphases in these fields.
,
,
,
,
,: 
The problem MetaCost addresses is that most classifiers assume all
errors are equally costly, but in reality this is seldom the case. Not
deleting a spam email will cost a fraction of a second of your
attention, but 
,
Now you could go and redesign every classifier from scratch to be
cost-sensitive, but that would take a long time. Instead, 
,
,
What it does is relabel examples so that the frontier
between the classes becomes the optimal one according to the cost
measure. For example, perhaps an email is labeled as spam in the
training set, but it looks enough like a legitimate one that MetaCost
relabels it as such, just to be safe. Then it just runs the cost-free
classifier to learn the frontiers according to the relabeled examples.
,
There was a tremendous follow-up to MetaCost, and today cost-sensitive
classification is a large field, with many techniques that are much
more sophisticated than MetaCost. The bottom line is, you want to use
either a natively cost-sensitive learner or an algorithm like
MetaCost, or your system will be making a lot of costly mistakes.
,
,
,
,: 
The motivation for the work was that companies spend a lot of effort
trying to predict the value of a customer, because that determines how
much they're willing to spend to acquire and keep him. But they ignore
(or used to) what is often the single biggest component of that value:
the customer's influence on other customers. If I switch to a
different cell phone service provider, that makes my friends and
family more likely to switch as well, particularly if I'm a persuasive
guy and keep telling them how much better my new carrier is. 
,
,
,
So the question I asked myself was: can we model the influence among
customers, mine these models from data, and then use the models to
choose the optimal customers to market to, in the sense that they'll
generate the highest expected return for least marketing expenditure?
We measured what we called the network value of a customer, which is
the expected number of other customers that that customer will
directly or indirectly convert, and found that it was often huge. For
example, in the ""web of trust"" of the Epinions review site, which we
mined, the network value of the top user was over 20,000. That was
pretty exciting! And today's social networks take it to a whole other level.
,
,
,
I think the social influence ranking industry has a great future - the
power of traditional mass marketing and direct marketing is waning,
and increasingly social influence is what determines the fate of your
product. Having said that, the industry is still in its infancy - a
lot of the modeling is still quite ad hoc, as far as I can tell, and
misses a lot of important effects. But the better it gets, the more
widely it will be used, so watch this space.
,
,
,
,
,: 
The goal in VFML is take any traditional batch learning algorithm -
which has random access to all the data, and typically runs in time
that's quadratic or worse in the size of the data - and easily turn it
into a stream mining algorithm - which has fixed memory, can only see
each sample once, and needs to run in time that's at worst linear in
the size of the data, but ideally independent of it. And we want
statistical guarantees that the model learned by the stream miner is
essentially equivalent to the batch one. 
,
This may seem like a tall order at first, but is in fact quite feasible, 
for a simple reason. 
,
If you want to predict who will will the next presidential election, you
don't need to ask every single voter who they'll vote for; a sample of
a few thousand suffices, if you're willing to accept a little bit of
uncertainty. 
,
,
,
Once I do, I can move on and use the
remainder of the stream to pick the tests for that node's children,
etc. And I can aggregate all the uncertainties in the individual
decisions into the overall uncertainty that I've learned the same
model I would if I had used infinite time and data, and use just as
much data as needed to keep this uncertainty within limits set by the user.
,
,
,
,: 
Most learning algorithms only look at a single relation - a single
table of data - and this is very limiting. Real databases have many
relations, and we want to mine straight from them. This requires a
pretty substantial rethink of how to do statistical learning, because
samples are no longer independent and identically distributed, due to
joins, etc. The solution in Markov logic networks is to use
first-order logic to express patterns among multiple relations - which
it can easily do - but treat logical formulas as features in a Markov
network or log-linear model, which sets up a probability distribution
over relational databases that we can then use to answer questions.
,
,
,
, is an open-source implementation of the learning and inference
algorithms we've developed for Markov logic. The goal is to provide
for machine learning and data mining something akin to what the
relational model and SQL provide for databases.
,
,
,
,
,: 
I remember that! Made me feel quite guilty. Choosing where to submit
my papers is always tough. I think everything I've published in the
last ten years could have creditably appeared in KDD or another data
mining venue. The reason I've published mostly in machine learning and
AI venues is that Markov logic networks draw heavily on ideas from
those fields, and so the audience in those communities is more primed
to like and follow the work. Also, because of the difficulty of the
issues involved, we had to at first focus on data sets that are fairly
small by KDD standards. But we're now rapidly scaling to larger and
messier data, so expect to see more KDD papers from my group in the
future.
,
, , is Professor of Computer Science and Engineering
at the University of Washington. His research interests are in machine
learning, artificial intelligence and data mining. He received a PhD in
Information and Computer Science from the University of California at
Irvine, and is the author or co-author of over 200 technical publications.
,
He is a member of the editorial board of the Machine Learning journal,
co-founder of the International Machine Learning Society, and past
associate editor of JAIR. He was program co-chair of KDD-2003 and SRL-2009,
and has served on numerous program committees. He is a winner of the
SIGKDD Innovation Award, the highest honor in the data mining field.
He is a AAAI Fellow, and received a Sloan Fellowship, an NSF CAREER Award,
a Fulbright Scholarship, an IBM Faculty Award, and best paper awards at
several leading conferences.
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Aug 16, 2014.
,
In honor of the upcoming 
,, we looked at the research in this field, using a very good 
tool from Microsoft Academic Search.
,
Overall, it finds 37,377 publications and 269,627 citations. 
Both publications and citations started growing dramatically around 1996.  The Microsoft Academic search data shows that the growth peaked in 2009, and declined dramatically to almost zero around 2011.  Such change cannot be explained only by term ""Data Mining"" becoming less popular and being replaced by ""Data Science"" and ""Big Data"". 
Since the total number of conferences and publications in the field has only grown since 2011, it seems that Microsoft Academic search data for 2011 and later years in incomplete.  See more detailed analysis at the bottom of this post. 
,
,
,
Here is a list of 20 top researchers in the field, according to Microsoft Academic Search for 
, for all time.
The authors are ranked using field rating, which is similar to H-index in that it calculates the number of publications by the author and the distribution of citations to the publications, but it only calculates publications and citations within a specific field to show the impact of the scholar within that field.
,
,
,
There is little correlation between field rating and total publications (R,=0.26).  There is, as expected, good correlation between field rating and the number of 
, publications (R,=0.69)
,
,
,
We note that most of correlation is driven by top 2 researchers - Jiawei Han and Philip Yu who have over 300 publications each.  If we exclude them, the correlation between field rating and Data Mining publications drops to R,=0.26.
,
,
,
Seven of these researchers have received 
,
,
,??,
and 5 have received 
,
,
,??,
Next I looked at the top researchers in the last 10 years.  Jiawei Han, Philip Yu, and  Christos Faloutsos are still in the top, but the rest of the list has many new names, showing showing rapid evolution of the field. 
,
,
,
Researchers with the largest gains in field ranking are:
,
,??,
so we can see that researchers from China (including Hong Kong) are becoming the leaders in data mining field.
,
Microsoft Academic search shows a decline in papers on ""Data Mining"" starting around 2010 (chart above).  This is especially clearly seen in the 
, on Knowledge Discovery and Data Mining, which is the first and most-cited conference for this field.
,
,
,
The chart shows that the number of publications in KDD conference related to Data Mining peaked at 247 in 2009 (probably including workshop papers), but then dropped to 125 publications in 2010, and only 3 in 2011, and 1 in 2012.  
,
While some of the change could be due to other terms like ""Data Science"" or ""Large Scale"" replacing ""Data Minig"" in the paper titles and session topics, this cannot explain such drop and having only 1 ""Data Mining"" paper for KDD-2012.
Since KDD conference main focus is on 
,, all of KDD papers should be considered as relevant to Data Mining.
,
We also note that KDD 2011 conference had 
,
and KDD-2012 had 
,, so a lot of data is missing from Microsoft Academic search for Data Mining starting with at least 2011.
,See also ,, which shows that after 2011 Microsoft Academic Search is missing a lot of papers.  
Thus analysis using Microsoft Academic Search option ""Last 5 years"" will be unrepresentative. 
However, the all-time list of researchers is still very good, as evident to all who follow the field.
,
,
,
,
,
 ,  "
"
,
,
,
Sometimes the high-level data science platform is not enough for a particular analytics task, and data scientists need to go to a lower level statistics / programming language. 
,
The last KDnuggets poll asked ,
,
,
The results show that the main 4 languages - R, Python, SAS, and SQL - hold a commanding lead - 91% of all respondents used one of them.
,
Comparing with similar KDnuggets Polls
,in 2013: ,, and in ,, we note several changes and trends.
,
,
perhaps partly driven by growth and change in KDnuggets readers composition, 
and likely also by increased visibility of this poll among SAS users. 
SAS users had a high percentage of ""lone"" votes - in 2014, 58% of them said they used only SAS, compared to 26% in 2013. The fraction of ""lone"" votes in 2014 was 20.5% for R, 14% for Python, and only 4.5% for SQL.
,
, -
R, SAS, Python, and SQL. 91% of all voters have used at least one of them.
Almost all other languages declined in their popularity for data mining tasks, including Java, Unix shell, MATLAB, C/C++, Perl, Octave, Ruby, Lisp, and F#.
,
Here is a Venn diagram that shows significant overlap between R, Python, and SQL. 
The percentages indicated how many voters chose that option, eg 20% of all voters have used both R and Python, while 10% have used R, Python, and SQL.  The areas of the circles and intersections approximately correspond to the fraction of voters.
,
,
,
Here is a similar Venn diagram showing overlap between R, Python, and SAS.
We see that SAS is much more independent from R and Python, with about 2/3 of of SAS users not using R or Python.
,
,
,
,
,
,??,
,
in share of usage were
,
,??,
Here is the table with more details: 
,
,
,
Among other programming languages William Dwinnell mentioned Compiled BASIC (PowerBASIC).
,
Regional participation was
,
,??,
,
This is similar to 2013, but with more participation from Asia and Africa/Middle East (led by Israel and Turkey), and less from Latin America (main decline from Brazil, perhaps still depressed from the World Cup loss).
,
,
 ,  "




















"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Aug 19 and later.
,
See full schedule at , .
,
,  "
"
By Gregory Piatetsky,  
,, Aug 19, 2014.
,
,This is the second part of my interview with Prof. Pedro Domingos, a leading researcher in Machine Learning and Data Mining, winner of 
,, widely considered the Data Mining/Data Science ""Nobel Prize"".
,
Here is the first part:
,.
,
Many of Prof. Domingos award winning research ideas are implemented in software which is freely available, including
,
,??,
To learn more about his research, here are some of his most cited papers via
, and
,.
,
,
,
,: 
Yes! 
????
,
,??,
There's many more, and I'll have more to say about some of these in my award talk at KDD-2014.
,

,
,
,: 
It's a popular science book about machine learning and big data, entitled 
,
,
It's almost done, and will come out in 2015. The goal is
to do for data science what ""Chaos"" did for complexity theory, or 
""The Selfish Gene"" for evolutionary game theory: introduce the essential
ideas to a broader audience, in an entertaining and accessible way,
and outline the field's rich history, connections to other fields, and
implications. 
,
Now that everyone is using machine learning and big
data, and they're in the media every day, I think there's a crying
need for a book like this. Data science is too important to be left
just to us experts! Everyone - citizens, consumers, managers,
policymakers - should have a basic understanding of what goes on
inside the magic black box that turns data into predictions.
,
,
At MIT I worked with 
, on a joint research project we
have. The goal is to be able to go all the way from raw sensor data to
a high-level understanding of the situation you're in, with Markov
logic as the glue that lets all the pieces come together. Josh is a
cognitive scientist, and his role in the project is to bring in ideas
from psychology. In fact, one of the funnest parts of my sabbatical
was to hang out with computer scientists, psychologists and
neuroscientists - there's a lot you can learn from all of them.
,
,
,
,: 
I'm working on a new type of deep learning, called 
,
SPNs have many layers of hidden variables, and thus the same
kind of power as deep architectures like DBMs and DBNs, but with a big
difference: 
,
it takes a single pass through the network, and avoids all the
difficulties and unpredictability of approximate methods like Markov
chain Monte Carlo and loopy belief propagation. 
As a result, the learning itself, which in these deep models uses inference as a
subroutine, also becomes much easier and more scalable. 
,
,
,
,
,
In other deep models, the inference is an
exponentially costly loop you have to wrap around the model, and
that's where the trouble begins. Interestingly, the sums and products
in an SPN also correspond to real concepts in the world, which makes
them more interpretable than traditional deep models: sum nodes
represent subclasses of a class, and product nodes represent subparts
of a part. So you can look at an SPN for recognizing faces, say, and
see what type of nose a given node models, for example.
,
I'm also continuing to work on Markov logic networks, with an emphasis
on scaling them up to big data. Our approach is to use tractable
subsets of Markov logic, in the same way that SQL is a tractable
subset of first-order logic. 
,
One of our current projects is to build
something akin to Google's knowledge graph, but much richer, based on
data from Freebase, DBpedia, etc. We call it a TPKB - 
, - and it can answer questions about the
entities and relations in Wikipedia, etc. We're planning to make a
demo version available on the Web, and then we can learn from users'
interactions with it.
,
,
,
,
,: 
That's what my wife keeps asking me. Seriously, I do think there's a
startup in my future. There are two reasons I haven't done it yet.
First, I want to do a startup that's based on my research, and in the
last decade my research has been fairly long-term. This means there's
a longer arc until it's ready for deployment, but hopefully when it is
the impact is also larger. 
,
Second and related, I want to do a startup
that has at least the potential to be world-changing, and many stars
have to align for that to happen. I often see colleagues do a startup
without giving much thought to all the non-technical issues that are
even more important than the technical ones, which is not a recipe for
success. In the data science space, it's rare for a startup to be a
complete failure, just because the acqui-hire value of a company is so
high, but if that's all you wind up with then maybe it wasn't the
greatest use of your time.
,
, 
(Note: Gartner latest ""Hype Cycle"" report has ,).
,
,: 
There's a fair amount of hype, but at heart the big data boom is very
real. I like the ""army of ants"" metaphor: it's not that any single big
data project will drastically change your bottom line - although it
does on occasion - but that when you add up all the places where data
analysis can make a difference, it really is transformative. And we're
still only scratching the surface of what can be done. 
The bottleneck really is the lack of data scientists.
,
Machine learning is booming along with big data, because if data is
the fuel and computing is the engine, machine learning is the spark plugs. 
,
To date machine learning has been less of a meme in industry or
the public's mind than data mining, data science, analytics or big
data, but even that is changing. 
,
,
,
,
,
,: 
,
,
Learn everything you can, but don't necessarily believe any of it; your job is to make some of those things outdated.
,
,
,
Listening to the data - doing experiments, analyzing the results, digging
deeper, following up on surprises - is the path to success. 
,
,
,
Talk continually with people from not just one company or industry, but many, and try to figure out what problems they have in common. That way you know you'll have a lot of impact if you solve one of them. 
,
,
,
Work with tomorrow's computing power in mind, not today's. 
,
,
,
,
,
And of course, have fun - no field has more scope for it than this one.
,
,
,
,: 
I like to read books and listen to music. I'm a movie buff, and I
enjoy traveling. My tastes in all of these things are pretty eclectic.
I'm also a swimmer and long-distance runner. And, most of all, I spend
time with my family.
,
A fascinating book I've read recently is 
,
by Alison Gopnik, Andy Meltzoff and Pat Kuhl. Infants and small children go through an
amazing series of learning stages, assembling piece by piece the
consciousness we adults take for granted. I can't help thinking that
the answers to a lot of our questions in machine learning are right
there in the baby's mind, if only we can decode them from the
often-astonishing experimental observations that Gopnik and Co.
summarize in the book.
,
On the fiction side, the best book I've read recently is probably 
,  by Cormac McCarthy. It's about a father and son trying to
survive in a post-apocalyptic world, and it's a powerful, unforgettable book.
,
, , is Professor of Computer Science and Engineering
at the University of Washington. His research interests are in machine
learning, artificial intelligence and data mining. He received a PhD in
Information and Computer Science from the University of California at
Irvine, and is the author or co-author of over 200 technical publications.
,
He is a member of the editorial board of the Machine Learning journal,
co-founder of the International Machine Learning Society, and past
associate editor of JAIR. He was program co-chair of KDD-2003 and SRL-2009,
and has served on numerous program committees. He is a winner of the
SIGKDD Innovation Award, the highest honor in the data mining field.
He is a AAAI Fellow, and received a Sloan Fellowship, an NSF CAREER Award,
a Fulbright Scholarship, an IBM Faculty Award, and best paper awards at
several leading conferences.
,
,
,  "
"
By Gregory Piatetsky,  
,, Aug 19, 2014.
,
,
One of the most interesting sessions at KDD-2013 conference on Data Mining and Knowledge Discovery (Chicago, Aug 2013) 
was a panel
,
,, with well-known entrepreneurs and Data Scientists
,
,??,
The panel was moderated by two top researchers: Foster Provost (NYU) and Geoff Webb (Monash U.).
,
,
has just published
,an extensive report on this panel:
,
,
,
In August 2013, we held a panel discussion at the KDD 2013 conference in Chicago on the subject of data science, data scientists, and start-ups. KDD is the premier conference on data science research and practice. The panel discussed the pros and cons for top-notch data scientists of the hot data science start-up scene. In this article, we first present background on our panelists. Our four panelists have unquestionable pedigrees in data science and substantial experience with start-ups from multiple perspectives (founders, employees, chief scientists, venture capitalists). For the casual reader, we next present a brief summary of the experts' opinions on eight of the issues the panel discussed. The rest of the article presents a lightly edited transcription of the entire panel discussion.
,
,
,
,
They include the excitement caused by the speed and unpredictability of events; the opportunity for real-world impact; the benefits of working in a small, focused team with a ""can-do"" attitude; the rewards of being an integral component of something big, interesting, and worthwhile; the thrill of creating something big from nothing, and, of course, the potential of substantial financial reward.
,
,
and, no matter what happens, you will gain valuable data science experience. Also, you can negotiate remuneration to balance equity (i.e., potential long-term profit) against salary (i.e., certain current income). It is critical to negotiate a good deal when you join any company.
,
,
Once a start-up is reasonably established, joining it may be no more beneficial financially than joining a very established company. On the other hand, an established start-up can provide many of the same non-financial rewards (see point 1), as well as better work-life balance. 
,
,
A great team can make something from very little. A poor team is unlikely to succeed no matter how good the vision. The most critical member of the team is the chief executive officer (CEO). The team must be coupled with an idea that addresses some real pain or major opportunity. To get major venture capital funding, the business plan should be for a $1 billion-plus business.
,
5. If you want to assess the success prospects of an established start-up, 
,
If it is funded by a top venture-capital firm, then you know that it has been assessed as a good bet by an informed and likely competent team.
,
,
One strategy for companies is to make yourself publicly visible as a top data-science company, as top-notch data scientists benefit considerably from working with other top-notch data scientists. When assessing potential staff, look for passion, vision, and excitement.
,
7. ,
A PhD definitely adds substantial value, but it is not clear that on average this is any greater than the value of 5 years of focused industry data-science experience. One of the key factors either way is the mentorship - a great PhD with a great advisor is hard to beat in terms of skill set, critical thinking, and independence; these also can be developed in an industry position with a great mentor. On the other hand, you are unlikely to gain great skills if you go straight from a master's degree to leading a data science project. Data science is a craft, and, as with most complex crafts, one learns best by working with top-notch, experienced practitioners.
,
,
If you want to be a technical founder, then it is essential to partner with a great business-savvy co-founder.
,
,
,
Read the full paper at
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Aug 13-14 were
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is CTO and co-founder at ,, a company that matches job seekers to jobs through analyzing big data from Knack games to discover a person's unique combination of strengths, talents, and personality traits.  John was previously Director of Engineering at Netflix.  Before joining Netflix, John co-founded AiLive, a real-time machine learning company that became synonymous with automated motion recognition middle ware that was used in top-selling games for the Wii and PS3. 
,
John grew up in the UK, obtained his MS from Oxford University and his PhD from Toronto University, he has authored three books on Artificial Intelligence and computer games (Game AI), and taught at the University of California Santa Cruz where he was an Adjunct Professor.  
,
Here is my interview with him:
,
,

,
,: The original idea came when Guy Halfteck (CEO and co-founder) started to think about how hiring could be improved after he was turned down for a ,hedge fund job.  Early discussions and experiments made us realize just how rich game play data is for learning about people?€?s personalities and traits.  The journey since 2012 has been about building an amazing team, proof of concepts, and then consumer and enterprise-ready products.
,
,
,
,We define Predictive Human Analytics as the ability to accurately predict personality and traits.  In an employment setting, that means being able to predict job performance based on game play data.  Right now we are focused on hiring, but we realize there are many other areas, such as college admission and team building that we are interested in working on in the future.
,
,
,
,
,
For example, we have used our games to infer cognitive ability, conscientiousness, leadership potential, creativity as well as predict how people would perform as surgeons, management consultants, and innovators.  Game data is behavioral and so it makes intuitive sense that it tells you a lot about a person?€?s actual behavior in real life.  Moreover, we have performed many rigorous validation studies to make sure our inferences are applicable to the real world.
,
,
,
,Our games provide a high volume of data and so we need to rapidly distill that information and make inferences to provide a compelling customer experience where people play games and get timely feedback.
,
,
,
,For exploration we make heavy use of Matlab (and the open source version Octave).  In production, we have taken advantage of many of the great tools that AWS offers for horizontal scaling in dealing with big data such as load-balanced auto-scaling groups for EC2 instances, the DynamoDB NoSQL database, and SQS.  We have used Clojure running on the JVM for some data processing but are currently excited by some of the great analytics tools available for Python.
,
,
,
,We like to hire people who can perform great exploratory analysis but then also take those ideas and put them in production.  Those people are hard to find but so far we've been fortunate to hire some outstanding data scientists, the best I?€?ve ever worked with in fact.
,
,
,
,That you need to constantly strive to hire great people who ?€?raise the bar?€? and not just settle for people who might be useful.
,
,
,  "
"
,
,
, is a Senior Data Scientist at ,, where she built out their cross-platform data infrastructure and oversees the statistical techniques used for analyzing features and optimizing revenue. Blue Shell Games is a casino gaming studio best known for Lucky Slots, one of the top grossing casino games of 2012 and 2013. Prior to Blue Shell Games, Pallas was on TinyCo's Data Team, where she worked on in-game economy optimization and monetization for Tiny Zoo, one of the top grossing mobile games of 2011. 
,
,.
,
Here is second and last part of my interview with her:
,
,
,
,: Whales. Our highest value customers are whales, but they skew the data and their spending behavior is erratic. You can exclude them from an analysis, but then you aren't accounting for one of the most profitable segments of your user base. If you analyze and cater only to whale preferences, you risk alienating the bulk of your user base and turning off players that may become whales next month. The other problem with whales is that statistically, you need a lot of them before the central limit theorem applies and you can use classical statistical techniques. I?€?m happy if I can get a p-value of 0.2. Data Scientists in most fields would find this preposterous.
,
,
,
,If a user uninstalls your game, they are essentially terminating their relationship with the game. That?€?s very different than leaving an app installed, but untouched for several months. A user that still has the game installed just needs the right enticement for re-engagement. Trying to reactivate a user that has uninstalled the app is probably a lost cause. The best we can do, currently, is assume every user is waiting to be re-engaged. To some extent this is true, but our marketing tactics would be much more effective if we had uninstall information.

,
,
,
,I?€?m really interested in Machine Learning and predictive analytics. The gaming industry is very young. Retail, finance, and pharmaceuticals have been using predictive analytics for decades. The gaming industry is still trying to figure out how to appropriately apply these techniques. Our games evolve weekly. It can be hard to justify investing time in predictive solutions when the conclusions may be no more informative than business intuition. Furthermore, by the time a solution is found, the game may have evolved to a point that the original problem is moot., I?€?m really excited to see where these techniques will lead and what types of questions I?€?ll be able to answer a year from now. 

,
,
,
,It?€?s a freaking video game! (The actual language was more colorful.) When I started working on my first game, it was six weeks away from losing its engineering resources due to under-performing revenue. In my mind the game was a dying patient, six weeks away from having the life support pulled and it was my job to save the patient. Lives were at stake! I?€?m proud to say that we did turn the revenue numbers around and the game kept a full-time engineering team for another year. Despite extending the life of the game, it?€?s important to maintain perspective and not take work too seriously. I've seen people burn out very quickly in this industry and I think many people would benefit from hearing it?€?s just a game, not heart surgery.

,
,
,
,A statistics, programming, or math background is very useful. However, none of those backgrounds teach you how to be both stubborn and creative. The data will always be imperfect. , When your data looks like a Jackson Pollock painting, you can?€?t be deterred. You also need a really thick skin. Nobody else really understands what data scientists work on. Even with a p-value ,

,
,
,On a weekday, I?€?ll probably watch a movie with my cat. On the weekends, I?€?ll take short trips to wine country or go to the theater. I like to keep things laid back and pamper myself a bit in my down time.
,
,
,  "
"
Most popular 
, tweets for Aug 18-19 were
,
#BigData moves to ""Trough of Disillusionment"" in Gartner 2014 Hype Cycle for Emerging Tech ,
,
,
#BigData moves to ""Trough of Disillusionment"" in Gartner 2014 Hype Cycle for Emerging Tech ,
,
#BigData moves to ""Trough of Disillusionment"" in Gartner 2014 Hype Cycle for Emerging Tech ,
,
Four main languages for Analytics, Data Mining, Data Science ,
,
,  "
"
Most popular 
, tweets for Aug 15-17 were
,
Is HBase slow and steady approach winning the #NoSQL race, gaining ground on MongoDB? #Hadoop #BigData ,
,
The top 5 Questions a Data Scientist should ask during a job interview ,
,
Course: Strategic Management in the Era of #BigData Analytics, at Tufts, Medford MA (online & oncampus)  ,
,
The top 5 Questions a Data Scientist should ask during a job interview ,
,
,  "
"
By Gregory Piatetsky,  
,, Aug 21, 2014.
,
Deep Learning is the hottest machine learning method and it is very much in the news for achieving remarkable results.  In the recent 
Large Scale Visual Recognition Challenge
(,, 
,),
the accuracy has almost doubled (to 43.9% from 22.5%) and the error rate fell to only 6.6% from 11.7%.
,
,
Significantly, almost all of the entrants used a variant of an approach known as a convolutional neural network (,), an approach first refined in 1998 by Yann LeCun, NYU professor, recently hired to head Facebook AI Research Center.
,
Here are additional resources to learn more about Deep Learning and convolutional neural network
,
,??,
Book draft on 
,, by Yoshua Bengio, Ian Goodfellow and Aaron Courville
,
Software:
,
,??,
,, including 
,
,??,
Fundamental papers on Deep Learning - from 
,.
,
Here are selected papers:
,
,??,
Excellent 
,, by Ivan Vasilev at Toptal
,
Another excellent presentation
,, by Adam Gibson and Josh Patterson.
,
,
,  "
"
,
,
,??,??
,??,??
,
,
,??,??
,??,??
,
,
,??,??
,??,??
,??,??
,??,??
,
,  "






"
,
,
,
,
,??,
,
,
,??,
,
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,  "




"
,
Latest ,, (Aug 20, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
No amount of analytics will make customers want something that doesn't suit their needs.,Tom Davenport, in ,  "



"
        ,  "






"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,  
,, Aug 20, 2014.
,
,
is an information and education portal focusing on data analysis, business intelligence, and big data. 
It is based in Ukraine, and ably run by Yevhen (Eugene) Dwortsyn, entrepreneur and IT specialist,
and editor-in-chief Larisa Shuriga, a journalist and IT enthusiast.
DataReview.info publishes mainly in Russian, but also translates some of its content in English. 
,
I have recently given them an interview - we talked about how I arrived to data mining, the difference between data mining and KDD, the history of data mining,  interesting problems I solved with the help of data mining, typical problems faced by aspiring data scientists, and more.
,
Here is the full interview in English:
,
,and in Russian:
,.
,
Here are some excerpts (illustrated by my photo in ""Data Miner"" hat)
,
,
,
Data scientists are some of the most demanded apecialists in the IT-market.  What tasks the solve? What challenges they face? DataReview has addressed these questions to one of pioneers of data analysis, the founder of KDD concept, the president of KDnuggets, Gregory Piatetsky-Shapiro.
,
,
,
,: Thank you, Larisa, but you are much too kind. There are now many thousands of excellent data scientists, and I am glad if I am considered somewhere among them.
,
I am probably best known as one of the pioneers in this field. I organized the first 3 workshops/meetings on Knowledge Discovery and Data Mining (KDD-89, 91, 93), co-edited the first 2 books in this field (1991 and 1996), helped launch KDD Cup - the first large data mining competition in 1997, co-founded the SIGKDD association - ACM group for Knowledge Discovery and Data Mining in 1999, and served as SIGKDD chair from 2005 to 2009.
,
,
,
,: As a child, I loved science fiction, especially A & B. Strugatsky, Isaac Asimov, and Stanislaw Lem, and that , along with mathematical inclination inherited from my father Ilya Piatetski-Shapiro - one of the leading mathematicians in Moscow - led me to study computers and being interested in Artificial Intelligence and Machine Learning.
,
My PhD at New York University was on application of machine learning method for database optimization, and my first job was working with databases. So perhaps working with databases and being interested in Machine Learning naturally led me to try to combine the two, which led to my work on knowledge discovery in data.
,
I described my journey to data mining in my chapter in Journeys to Data Mining: Experiences from 15 Renowned Researchers , Mohamed Medhat Gaber (Editor) Springer, 2012
,
,
,
,: Of course I did not invent data mining - analyzing facts and finding patterns is probably one of the basic human traits. Statisticians have been working on data analysis for centuries.
,
Regarding the different names of this field - Data Mining, Knowledge Discovery, Predictive Analytics, Data Science - here is a very brief history.
,
In 1960-s, statisticians have used terms like ""Data Fishing"" or ""Data Dredging"" to refer to what they considered a bad practice of analyzing data without a prior hypothesis. The term ""Data Mining"" appeared around 1990s in the database community. I coined the term ""Knowledge Discovery in Databases"" (KDD) for the first workshop on the same topic (1989) and this term became popular in academic and research community. KDD conference, now in its 21 year, is the top research conference in the field and there are also KDD conferences in Europe and Asia.
,
However, the term ""data mining"" is easier to understand it became more popular in the business community and the press.
,
In 2003, the term ""data mining"" acquired a bad image in the US because of its association with US government program called TIA (Total information Awareness) which was closed by US Senate after protests by privacy advocates. In 2006, the term ""Analytics"" jumped to great popularity, driven by introduction of Google Analytics (Dec 2005). According to Google Trends, ""Analytics"" became more popular than ""Data Mining"", as measured by Google searches, around 2006, and continued to climb ever since.
,
The term ""Data Science"" appeared in early 2000, but became used in its current meaning only since 2012, and we can see a huge demand in jobs for ""Data Scientist"" on indeed.com, a popular platform for jobs. ...
,
Read more in the full interview
,
,
,
,
,  "
"
,
,
, is Principal Data Scientist at , working on analyzing the online social behavioral data of hundreds of millions of users. He is leading the data science efforts on reliably identifying users across devices and building user models for online advertising. Prior to ShareThis, he was at Intuit where he used data mining and machine learning techniques to build innovative offers and recommendation products from small business and consumer transactional data, and analytics for increasing conversion and customer retention in various Intuit products. 
,
He obtained his Ph.D. from Stony Brook University and bachelors from the Indian Institute of Technology and is a frequent speaker in data science conferences.
,
Here is my interview with him:
,
,
,
,:  , is a social media company. We provide social tools to publishers so that users can share their favorite content to social networks of their choice. Currently we integrate with more than 100 social channels and 2.4 million publishers. At the same time, we help advertisers buy ,media and find the right audience for their message. Analytics is extremely critical to what we do with data at ShareThis. Our platform is based on finding the right insights for our publishers and advertisers. That insights can range from, for example, informing publishers about what content is resonating with their users to finding the right users for advertisers at the right time.
,
,
,
, Social TV is typically defined as online social activity about TV shows. For example, you might be tweeting about the show while watching an episode of ?€?Breaking Bad?€?. In this case, it is real time but it need not be real time activity and could be social activity before or after the show has aired.
,
,Since ShareThis has an extensive coverage of all the major social channels, we are in a great position to measure social TV activity across channels as opposed to analysis on just any particular channel. We found that Facebook and Twitter indeed dominate the social TV conversation in the ShareThis network data but there are also interesting niche players in Pinterest, Reddit, and Tumblr. For example, ,
,
,
,
, We have looked at device usage (i.e. is the activity from an iPhone or a PC, etc.), distribution of TV show genres, demographics, and geographic location of users across social channels in our current analysis. While that itself gave us a lot of interesting insights about user behavior, we can go much further particularly on analyzing the impact of the social activity on other users.
,
,
,
, One of the challenges in doing these kinds of large scale analysis is on using data from different sources - both first party as well as third party. There is the usual challenge on integration as well as trying to figure out how good the data really is and what kind of conclusions can be reliably drawn from it. For social TV in particular, being able to associate a piece of social content to a TV show can be challenging too. For example, the content can refer to the show name or it might refer to the main cast and a particular episode of the show so there are lots of variations.
,
,
,
, Mobile has been a surprise. While it?€?s commonly known how content consumption, is increasingly becoming mobile and how networks such as Facebook and Twitter are successfully riding that wave, it has been surprising to see how other social channels are becoming so mobile-centric. For example, we found that almost 75% of the social TV activity in Pinterest is mobile compared to 55% for Facebook.  ,
,
,
,
, I think, from a device standpoint, it will be interesting to see how social plays out in mobile apps. From a marketing perspective, I think we are beginning to understand how social affects a user?€?s path to purchase and I think there will be interesting advances over the next few years in how marketers use this data to reach their audience of choice.

,
,
,
, I think it?€?s important not to fixate on any one particular social channel but have a mix in the bag. Different social networks tap into different demographics and the usage, behavior, in terms of content, devices, location and time, also vary across them. Hence it?€?s important to target audiences across networks.  I think some of the metrics around social are still developing and we might very likely go beyond counting likes to metrics more connected to brand awareness and advertising campaign performance.
,
,
,
, I agree that it?€?s a great job and very fulfilling! ,
,
,
,
, Again to be a good data scientist, you need to be very aware of your particular industry. I try to read as much as I can of adexchanger, adotas, adage which are blogs/publications in the advertising industry. As far as data science goes, I am a hadoop stack + python + linear models kind of guy so try to catch up as much as I can on books and blogs in these areas.
,
,
,  "
"
        ,
,
  "
"
,
,
,, held on June 22-24, 2014 at San Jose Convention Center, focused on "","" and educated the attendees on the best way to prepare themselves for the growing opportunities in the field of Big Data. The conference was a great opportunity for learning as well as networking with the leaders from industry and academia through a seamless exchange of information, ideas and perspectives. The impressive line-up of speakers, which ,included Big Data leaders from various industries, shared the best practices through real-world case studies and tutorials on a wide variety of topics such as getting from data discovery to return on investment and real business value, bridging the gap between decision makers, IT managers, and analytics professionals, etc. The conference program included keynote sessions, presentations across 3 tracks (Big Data Case Studies, Big Data 101 and Emerging Trends in Big Data), technology workshops, panel discussions and exhibits.
,
Here is a summary of the key takeaways from selected talks on Day 1:
,
, kicked off the event with keynote speech titled ?€?Putting Big Data To Work?€?. He said that today one can?€?t avoid being exposed to discussion around big data and its analysis. However, the downside of this attention is that there is a lot of hype and misinformation in the marketplace. Many companies are also confused about how to get started, what actions to take and what pitfalls to avoid. Based on his popular book ?€?Taming The Big Data Tidal Wave?€?, he addressed many organizational and cultural points that must be considered while taking big data initiatives. 
,
He insisted that there is indispensable need of augmenting traditional analytics with new approaches. The data science team should constantly expand and evolve. Building team that knows what one needs in total is very essential. He argued about external resources saying that we should leverage external resources optimally. One can outsource tactical execution, but should not hand off analytic strategy and design. Talking about organizing analytics team, he said ?€?Unlike many other common corporate teams, there is no standard model for organizing analytics teams.?€? He recommended a hybrid model along with a center of excellence for mature organizations. 
,He strongly suggested to leverage analytics in diverse ways by performing discovery analysis alongside confirmatory analysis to maximize profits. He said it?€?s high time to move IT from serving to enabling, giving example of traditional server prepared yogurt cups to model self-prepared yogurt cups. He also gave this mantra: ,
,
,
, talked about ?€?Getting Started with Hadoop and Big Data?€?. He started with mentioning that big data analytics can be a very daunting task because of the complexities of technologies and the fast evolution of the software ecosystem. 
,
He exposed the attendees to established open source big data technologies. He also discussed practical methods to prototype and develop big data analytics without requiring deep knowledge of software development or distributed computing. Giving an overview of Hadoop and Map Reduce, he discussed alternatives to writing Java code to develop analytics on Hadoop. He also mentioned how one can get started with big data technologies today.
,
, delivered a talk titled ?€?Taming Big Data with Berkeley Data Analytics Stack (BDAS)?€?. Today?€?s data analytics tools are slow in answering even simple queries, as they typically require to sift through huge amounts of data stored on disk, and are even less suitable for complex computations, such as machine learning algorithms. 
,
He discussed today?€?s state-of-art analytics stack and mentioned need to manage the following three stacks as biggest challenge: Interactive, Batch and Streaming. To address some other challenges as well, his team is developing BDAS, an open source data analytics stack that provides interactive response times for complex computations on massive data. He briefly discussed Apache Sparks and merits such as fast, easy and generic. Spark also unifies real-time and historical analytics. He briefly discussed unification of graph processing and ETL.
,
, talked about SIPmath Modeler Tools. The Stochastic Information Packet or SIP allows uncertainties to be used in interactive calculations. The Open SIPmath standard facilitates uncertainties to be communicated as big data for driving interactive simulation in native excel and other environments without add-in software. He demonstrated some SIPmath Modeler Tools which facilitate generation of such models in Excel, and also how to import and export results from Crystal Ball, Risk Solver and Matlab to leverage those packages.  

,
"
"
,, a complete Data Science software tool for developers and analysts, significantly shortens the load-clean-train-test-deploy cycles that are time-consuming when building predictive applications. We, at Dataiku, have built this software by trying to capture what data scientist do on a day-to-day basis. We feel  statistical software needs to adapt to the time of big data, where data integration and data pipelining capabilities are more important than ever.
,
,
,
First of all, DSS enables direct and fast connection to the most common sources (Hadoop, SQL, Cassandra, MongoDB, S3, ?€?) and formats (CSV, Excel, SAS, JSON, Avro, ?€?) ??for data today.
After connecting to a data source, the first step in any serious modeling job is cleaning the data. As you know, this process typically takes up as much as 80% of data scientists?€? time. To speed this up, our studio comes with strong data integration capabilities. First, DSS automatically infers smart data types over data (such as Gender, Country, IP Address, URL, Date, ?€?).
Analysts can leverage these smart data types to validate and transform the data in an automated way (like getting the country out the IP, the day of week out of the date, and so on). They can also perform more mundane tasks such as replacement, grouping, splitting, calculating, and others on DSS?€?s interface which shows them an instant visual feedback of any operation.
,
,
,
Once data is clean and rich, users can train supervised or unsupervised models. It automatically creates a tunable features engineering pipeline with missing value imputation, dummy variable encoding, impact coding, feature generation, and reduction.
,
,
,
It then trains different models using algorithms from the popular Scikit-Learn and H2O frameworks. Users can compare models, analyze their performance, and tune all the parameters from the web UI. Committed to a White Box approach, DSS lets advanced users generate the full Python source level code for the whole pipeline (in Python), so that they can modify it at will.
,
,
,
Once models are built, users can create robust and repeatable workflows that enable retraining or scoring.
These workflows include the full data flow from raw data to predictions. The workflow engine is incremental (for instance, it can work incrementally on hourly updated data) and robust (it can intelligently recover from situations were part of the data was temporarily available). These workflows can be extended with custom code in Python, R, SQL or Pig, enabling full Hadoop based workloads.
Predicted values can be accessed through a REST API or published directly by DSS to a variety of destinations (e.g. ElasticSearch, FTP servers, internal Data warehouse).
,
We have built Data Science Studio in order to shorten the load-clean-train-test cycles without dumbing them down; it is a tool for qualified data scientists while remaining accessible for less technical business intelligence or marketing profiles.
,
Free community edition is available (limited to 100,000 rows and one user).
A free trial version of the tool is also available at ,
,
,
,
,
,
,
,
,
,  "
"
,
,
,,
to be held August 24-27, 2014, New York, NY, USA is the 
, in Data Mining, Data Science, and Knowledge Discovery,
and with over 2,000 people expected, it will be the largest conference in the field up to now!
,
KDD 2014 will bring together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. 
,
KDD will also recognize the leaders in research and community with several significant awards.
,
,
,
This winner in 2014 is Prof. Pedro Domingos from U. Washington. The SIGKDD Innovation Award recognizes outstanding technical contributions to the field of knowledge discovery in data and data mining that have had a lasting impact. This award recognizes Pedro Domingos?€?s work in data stream analysis, cost-sensitive classification, adversarial learning, and Markov logic networks, as well as applications in viral marketing and information integration.
,
Read the ,.
,
,
,
, is the winner of the 2014 Service Award. The Service Award recognizes individuals for their outstanding service contributions to the field of knowledge discovery in the community. Senator earned this award for his work influencing the direction of major conferences, helping define the distinction between research and applications in knowledge discovery, and highlighting the challenges specific to the field for outside entities.
,
New in 2014 are the
,.
,
The three Test of Time Awards are granted to papers from past KDD conferences beyond the last decade that have had a profound influence on the data mining research community. The winning papers are as follows:
,
,. This paper introduced density-based clustering to the data mining community. Since its introduction, density-based clustering has become one of the prominent clustering paradigms. Since the publication of DBSCAN, density-based clustering has been extensively studied and has been successfully used in many applications.
,
,. This paper pioneered the research of using association rules for classification by integrating classification and association rule mining. It also proposed an efficient algorithm and built the first system (called CBA) for the purpose. This work triggered a large number of follow-up works and applications.
,
,. This paper considers questions involving the spread of information, innovations, and behaviors in social networks. The paper identifies a technically rich structure inherent in the problem, and establishes a framework that has subsequently been used in areas ranging from social media and marketing to the diffusion of innovations and the study of inequality.
,
,
,
The overall best research track paper was ,, which presents an approximate sampler for topic models that theoretically  and experimentally outperforms existing samplers thereby allowing topic models to scale to industry-scale datasets.
,
The best student paper was ,, which presents algorithms for tackling the non-convexity that arises in using the hierarchical lasso when regularizing parameters of models that attempt to capture non-linear feature interaction.
,
,Targeting Direct Cash Transfers to the Extremely Poor,
,by Brian Abelson, Enigma; Kush R Varshney, IBM TJ Watson; Joy Sun, GiveDirectly; 
,
,
,Style in the Long Tail: Discovering Unique Interests with Latent Variable Models in Large Scale Social E-commerce 
,by Diane J. Hu, Rob Hall, and Josh Attenberg (Etsy) 
,
,
,Winner:  Reconstruction and Applications of Collective Storylines from Web Photo Collections.  
,Gunhee Kim (student) and Eric Xing (advisor)
,Runner-up: Human-Powered Data Management.  
,Aditya Parameswaran (student) and Hector Garcia-Molina (advisor)
,
,
,  "
"
For those of you who are not lucky enough to be at 
, in New York, here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Aug 26 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Aug 20-21 were
,
,  "
"
,
,
Sept. 18-19, 2014, 
,University of Colorado at Boulder
,
The second annual Leeds Analytics Conference brings top names in
business analytics to the Leeds School of Business at CU Boulder,
creating a unique opportunity to learn practical applications of
analytics today in enterprises and to network in one of technology's
highly in-demand fields. 
,
The conference is an interactive forum for
students, faculty, and professionals, whether you are simply curious
about the growing role of analytics in business, an aspiring analyst,
or a practicing data professional.
,
Details are available at 
,.  "
"
,
,
, is the founder and CEO of , which he started after participating in numerous conferences, meeting with several customers and having worked at senior leadership roles at Rackspace, Amazon, Yahoo, Oracle, shy of 33 patents and publications with a bachelors of engineering in Computer Science, MBA in Finance and PhD course work in Data Mining.
,
Arpit is an active speaker at conferences and have interests in Big Data, eCommerce, Ads, Search, Risk, Fraud, Mobile. He actively advises multiple companies in the above mentioned areas.
,
Here is my interview with him:
,
,
,
, ,(AA) is building decision as a service(DAAS) platform on the cloud for big data. ,It allows customers to build, deploy, manage predictive models and operationalize predictions. It has been a challenge to draw insights from data. As the data grows, the need to get actionable insights becomes very important. DAAS allows rapid experimentation framework, that is easy to use and affordable, which is a must when working with huge data sets.
,
,
,
,Big data technologies are not good fit for all enterprises. There has to be a business buy in and the right tool should be used to solve it, be it a big data solution or tradition data solution.
,
Prototyping seems to be the way to go. Go for the low hanging fruits and generate the momentum to run big ticket projects.
,
,
,
,Cloud has made the storage of data very cheap. It offers the scale for distributed computing at a pay as you go pricing model. Both computing and storage help lower the cost of analyzing big data.
,
,
,
,People tend to measure what has happened. Metrics such as KPI, Dashboards, trends all tell what has happened. They answer what happened. But real analysis is to determine why it happened, what were the reasons for the numbers. This is the most challenging part, actionable insights that not only tell you what occurred but why its happening and what you need to do to keep or alter the course.
,
,
,
,Earlier Internet companies such as Yahoo, Google, Facebook used to use these terms but now there is broader adoption even if its in proof of concept stage. Now people have started to ask ROI questions, its no longer an experiment outside of the traditional big data companies. We will we some consolidation in industry in coming quarters.
,
,
,
,Study stats, data structures, economics, DBMS and maths. Take online courses on machine learning and experiment on pen source tools such as Weka, Mahout, Hadoop etc. Be part of industry groups. Apply in big data companies to start the career there.
,
,
,
,I mainly get my content through LinkedIn or sometimes I search on web. I was very fascinated with use of data in Indian Elections recently and analysis done by Rediff Labs.
,
Apart from work I enjoy debates and discussions on economics, politics, culture and relationships. Motivational speaking, mentoring, singing, dancing and volleyball are some of my interests that I actively pursue.
,
,
,  "



"
Most popular 
, tweets for Aug 22-24 were
,
#kdd2014 Words most correlated with accepted industry track papers: Production, Large Scale, Deployed; Rejected papers: Search, Traffic
,
#kdd2014 Words most correlated with accepted industry track papers: Production, Large Scale, Deployed; Rejected papers: Search, Traffic
,
How to keep yourself up to data on Data Science & Data Mining - best blogs, Quora people, apps,  ,
,
A Day in the Life of a Functional Data Scientist - interesting presentation ,
,
,  "









"
,
By Alesia Siuchykava, August 2014.
,
Prior to the ,, I wanted to share the 2 best received presentations from our last year?€?s event
,
You can ,.
,
,
,
,
,
,
,
,
,

  "
"
Most popular 
, tweets for Aug 25-26 were
,
#kdd2014 Cynthia Rudin (MIT): A good transparent model that is used is better than a more accurate black-box model that nobody will use
,
Quora: most important APIs every Data Scientist should know: Twitter, Facebook, LinkedIn, Maps - docs and tutorials ,
,
Dataiku Data Science Studio ,
,
Best Text Analytics Summit Presentations ,
,
,  "








"
,
,
,
Kaggle is holding a new prediction challenge in which participants will create a seizure forecasting system to attempt to improve the quality of life for epilepsy patients. If successful, these seizure forecasting systems could help patients lead more normal lives.
,
, Monday, August 25, 2014
,
, Monday, November 17, 2014
,
,
,
The data provided for the competition comes from electrode sensors in the brain as part of intracranial EEG. These readings come from both dogs and human patients. EEG data from ,ieeg.org is also permitted to be used.
,
The effectiveness of the predictor will be judged based on the area under the ROC curve. More information can be found on the ,.
,
,
,
The prizes for first, second, and third are as follows:
,
,
In addition, the top ten teams will be invited to co-author the academic paper presenting the results of the challenge.
,
,
,
,  "
"
,
,
An unconference or camp is an event that involves the audience members taking 1 minute turns proposing unconference sessions.  The audience is polled for their interest in attending the given session, and assigned to a time slot and room size accordingly.  A ""session matrix"" of 4 time slot rows by about 8 room columns, with each cell listing the session title and moderator/speaker.  The terms camp and unconference are used interchangeably.  A camp is a bottom up, user participation driven event.
,
Historically, we held 
, since 2009.  This year, we are renaming it to ,
to be more inclusive of Big Data as well as Data Science.  The event is organized by the San Francisco Bay Area chapter of the ACM (Association of Computing Machinery).  ACM is the professional society of Computer Science. SFbayACM has been continuously running events since 1957.
,
Proposed Sessions: 
,
,
Sessions can be proposed in advanced on our event web site, and registered attendees can discuss and vote on sessions - to share your preliminary interest.  In the session descriptions, the organizers invite the use of SESSION TAGS, such as:
,
,??,
,
,
,
The San Francisco Bay Area Chapter of ACM was founded in 1957 and combined with the San Francisco Peninsula Chapter in 1970.  "
"
,
,
, is Sr. Director of Personalization and Principal Data Scientist at ,. Before moving to StumbleUpon, Debora was Senior Scientist at Yahoo! Labs. Her research interests include User Behavior Analysis, Recommendation Systems, Web Information Retrieval, Link Analysis, Algorithms for the Characterization of the Web, Complex Networks and Social Networks. 
,
Debora obtained a Ph.D. in Computer Engineering in 2005 from the University of Rome ""La Sapienza"". She has published more than 50 scientific papers and she has been serving on the program committee of top tier conferences in the area of Data Mining and Information Retrieval.
,
Here is my interview with her:
,
,
,
,: , is the easiest way to discover new and interesting things across the web. Over 30 Million people turn to StumbleUpon across desktop and mobile to be informed, entertained and surprised by content that is recommended based on declared interests and activity.
,                                                       
Partners use StumbleUpon to distribute their content to influential and socially-engaged audiences by maintaining active accounts, sharing links, employing StumbleUpon badges, and creating StumbleUpon Lists.
,
The Data Science team works behind-the-scenes to understand the millions of people who use StumbleUpon.  Gained insights are then leveraged to further our business objectives and help other teams across Advertising, Personalization, Site and Community understand how our users are interacting on StumbleUpon.
,
,
,
,
,
StumbleUpon?€?s personalized recommendation engine is an attempt to solve this problem.  Relevant sources are selected and shown to users based on their declared interests, explicit ratings and other implicit signals.  Users join StumbleUpon to be entertained, to learn but also to discover useful resources. 
,
,
,
,To study and quantify User Engagement, I took inspiration from recent work in the field.  WWW 2013  Tutorial on ?€?Measuring User Engagement?€? brought to my attention the work of Attfield et all. 2011 where user engagement was defined as a ?€?quality of the user experience that emphasizes [?€?] the fact of being captivated by the technology.?€?
,
,I often see engagement measured as return rate or retention, but these metrics focus on the effect of the engagement instead of looking at the engagement itself. To quantify engagement at StumbleUpon, we relate each user experience to retention. We separately monitor time spent engaging with the core stumbling experience and engagement with other features like Lists, Ratings, Sharing, etc.  
,
,
,
,Users can express their opinions by submitting new content and rating recommended content.
,
,We deeply rely on explicit ratings and other forms of implicit and explicit feedback to determine content quality. Although recent studies have related beauty, novelty and interestingness to statistical properties around content, we are not interested in judging quality without considering context. Therefore, we assess relevance for each segment of our user base. 
,
We can also identify ?€?Experts,?€? i.e., users who are trustworthy and likely to provide high quality feedback, and exploit their contributions at a level that may be higher than average. 
,
,
,
,With the oversaturation of traditional advertising in the form of banner ads, homepage takeovers, etc., we have seen an emergence in native advertising - or sponsored content ?€? used as a means of breaking through the clutter. Native content does this by delivering brand and publisher messaging to consumers in a more digestible way.  It reads more editorially and helps keep users engaged because the content is not a traditional advertisement.
,
In an ideal scenario, users should not be able to discern between organic and sponsored content, and it is a better experience since the ad does not disrupt the flow of the user?€?s activity. In the case of StumbleUpon, sponsored content is just ?€?another Stumble?€? that can be rated or shared. 
,
,
,
,I have seen increasing interest in multimedia recommendation systems.  For example, the first workshop on Recommender Systems for Television, and online Video (RecSysTV) will be held in conjunction with RecSys. Although video recommendation faces unique challenges, the growing number of users who consume videos justifies the increasing demand for it. 
,
Another research opportunity could come from the ?€?Internet of Things?€? concept. We?€?re faced with challenges like the need to integrate and query a vast range of diverse content formats (from text to sensor data). Although we have witnessed some research efforts in the areas of data management and data integration, I have not seen progresses able to make the ?€?Internet of Things?€? a reality.  
,
,
,
,
,
,
,
,I look for people with experience in Machine Learning and Data modeling with a tangible passion for managing big data, curiosity and capacity of mining the opportunities beyond the data.  
,
,
,
,The last book I read was a classic: ?€?,.?€? I am currently reading  ?€?,.?€?  Before the birth of my son, Luca, I enjoyed dancing tango, but now I mostly spend my free time (that is not much) with him.
,
,
,  "
"
,By Gregory Piatetsky,  
,, Aug 28, 2014.
,
I have just attended 
,,  held August 24-27, 2014, in New York City. 
,
This is part 1 of the report - here is 
,.
,
With an amazing 2,300 people attending (twice the size of KDD-2013), KDD showed that it was not just 
, in Data Mining, Data Science, and Knowledge Discovery, but also the largest conference in the field. Kudos to KDD-2014 general chairs Claudia Perlich and Sofus Macsk??ssy for excellent organization.
,
There were 151 research papers accepted from a record 1,036 submissions.
,
For the first time at KDD, over 50% of attendees were from the industry. Attendees came from a record 52 countries, with most from the US (1506 people), followed by China (86), Japan (57), and Germany (40). More data mining on KDD statistics in the second part of my post.
,
For a detailed overview of the papers, review process, keywords were more likely to get papers accepted/rejected, and other statistics, see a great presentation compiled by PC Co-chair 
, Leskovec: ,
,
,
Jure and PC co-chair Wei Wang created a very full program, but with up to 6 parallel tracks I frequently wanted to be in 3 places at once!
,
,
The success of the conference was 
very evident in the first day, with part of the workshops held in the 
,.  Bloomberg corporation and its CEO 
, deserve a lot of gratitude from data scientists for their support of KDD-2014 and its theme,
,. 
,
There were many presentations and talks about social good, including
,
,??,
To remind us that, alas, not all is nice and good in our world, 
Rand Waltzman from DARPA gave a rather scary talk about Information Environment Security, how rumor spread in social media can be started and detected, and how some terrorist organizations are surprisingly effective users of social media for their ends.
,
However, the core of the conference was serious and excellent data science.  Many popular sessions and workshops were packed wall-to-wall, with barely a place to lean against!
,
Some technical topics that I found especially notable/popular include:
,
,??,
KDD has presented several significant
,, including Innovation, Service, Test of Time, Best Papers, and Best Dissertation awards.
,
See also my interview with
,.
,
Pedro Domingos Innovation Award Talk on Sunday, Aug 24, was on Scalable Data Science and Very Large Scale models.  He made an analogy that 
, - we now begin the transition to very large scale models.  He introduced and explained 3 principles for very large scale models and proposed Markov-Logic based Sum-Products Networks as a way to building Sum-Product models efficiently.
,
Here are my selected tweets from his talk (with prefix ""#kdd2014 Pedro Domingos"" removed) 
,
,
Conference 
, was packed with interesting research and industry presentations, sometimes with up to 6 parallel activities, so I can only mention a few.
,
On Monday, Aug 25, Oren ,, a distinguished researcher, entrepreneur, and now the director of 
,, gave a controversial keynote on
""The Battle for the Future of Data Mining"".
,
He argued that the traditional data-driven approach, including Deep Learning, is limited in its potential and proposed a knowledge-driven method -  building a very complex knowledge base. 
As a test of their system, he is developing an open question answering system which will be able to answer questions from 4th grade science exams.  Here are some of my tweets from his talk (with ""#kdd2014 Oren Etzioni"" removed from 2nd & following tweets)
,
,??,
On Tue, Aug 26, Eric Horvitz, 
,, 
Director of Microsoft Research, talked about ""Data, Predictions, and Decisions in Support of People and Society"".
Here are some of my tweets from his keynote:
(with repeated part ""@EricHorvitz #kdd2014 keynote"" deleted for brevity)
,
Here are his
, (PDF).
,??,
Both keynotes were covered in NYTimes: 
,.
,
Also, just found a very nice page with 
,, created using seen.co platform.
According to that page, top twitter accounts for #kdd2014 were @Bloomberg, @kdnuggets, @DataKind, @DanDoctoroff, and @erichorvitz.
,
Here is ,.
,
,
 ,  "
"
,
Plug&Score, a business division of Scorto Corporation, has announced a release of the updated version of its , ?€? an easy-to-use credit scorecard development software designed for credit risk professionals in banks and other financial institutions.
,
,
,
,
,
,
Further information is available at ,.
,
,
,
,:
,
Plug&Score is a business division of Scorto Corporation, a globally recognized provider of decision management and risk mitigation solutions and services. Scorto has served lending and credit based organizations since 2005 by providing the most precise and sensible tools and platforms, allowing its clients to make accurate and timely decisions, quantify credit risk, increase the profitability of credit portfolios, and improve operational efficiency.
,
,
,
,  "
"
,
Unstructured data is the most prevalent form of information on the planet. It also underpins much of our communication. It exists in our e-mails, surveys, social media accounts, call center logs, etc. With a strong text analytics strategy in place, companies can get critical information from this data to drive better business decisions. 
,
,Data Driven Business has compiled a free white paper which focuses on the business benefits (and challenges!) of text analytics from the perspectives of 4 experts from 
,, and ,, who are slated to speak at the 
,
on November 4-5 in San Francisco. 
,
Download the free white paper here: 
,.
,
,
,
,??,
,
,
,??,
Download your free copy today: 
,.  "
"
By Gregory Piatetsky,  
,, Aug 30, 2014.
,
The 
, has raised many millions of dollars for ALS charity, 
but in the summer of 2014 it also has become a 
,.
,
,  held in hot August days in New York 
has not escaped it, with several KDD-2014 organizers
, by Ronny Kohavi.
,
Here was our response.
,
Before, 
,
,
,
during,
,
,
,
and after.
,
,
,
From left to right, 
, Leskovec (KDD-2014 PC Co-Chair), 
, (KDD-2014 General Co-Chair), 
, (KDD-2014 General Co-Chair), 
, (KDD-2014 Industry/Gov. Track Co-Chair, and Obama 2012 Campaign Chief Data Scientist),
and 
, (KDnuggets Editor and KDD co-founder).
,
The moment when ice water is dumped on your head is stunning!
,
Thanks to Tanya Berger Wolf for taking these pictures and  
, (although I have serious doubts that it will spread virally).
,
In the video, Claudia challenged the 
, organizers. 
,
I challenged data scientists to think how they can help their community, their city, and the world.
,
So donate to ALS or a charity of your choice and may the ice-cold water help
you objectively assess the data and avoid overfitting !  "
"
Most popular 
, tweets for Aug 27-28 were
,
Worst Venn Diagram ever? Thomson Reuters needs a new graphic designer (HT 
,)  
,
,
Worst Venn Diagram ever? Thomson Reuters needs a new graphic designer (HT @CBinsights) ,
,
New Microsoft Azure SQL Database service tiers with reduced pricing and enhanced SLA, in Sept ,
,
Worst Venn Diagram ever? Thomson Reuters needs a new graphic designer (HT @CBinsights) ,
,
,  "
"
,
,
,??,??,??,
,
,
,??,??
,
,  "















"
By Gregory Piatetsky,  
,, Sep 10, 2014. 
,
This past week I was on vacation, and tweeting a lot less tweets than usual,
so here are the most popular for the last 7 days - Sep 3-9.
,
,
What Is #BigData? Definitions from 40+ thought leaders, including 
,, 
,, 
,, and 
, 
,
,
,
Choosing the right estimator scikit-learn #CheatSheet: classification vs regression vs clustering vs dim. reduction ,
,
,
What Is #BigData? Definitions from 40+ thought leaders, including @DrewConway, @kdnuggets, @SethGrimes, and @hmason ,
,
,
Fewer companies are hiring Data Scientists but #DataScience is still hot ,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
By Grant Marshall, Sept 2014
,
Today, we look at the top 25 most viewed text mining lectures on ,
,
The way popularity is determined is by looking at the ?€?,?€? sort on the text mining video listing. These are the videos, including authors, length, and venue, sorted by views:
,
,

First, we will look at the contents of the titles of these lectures to get a feel for the important topics covered.
,
,
,
Looking at this visualization, it is clear how prominent the web is in these text analytics lectures. It is also clear that many text analytics lectures cover the information retrieval and machine learning aspects of the problem as well.
,
Now we will look at how the length of the videos correlates with popularity.
,
,
,
As with all other topics covered thus far, text analytics lectures correlate longer length with higher popularity. This is the case whether we measure popularity by videolectures?€?s metric or by pure view counts.
,
One notable thing about the text analytics lectures, though, is the number of lectures under one hour. Sixty percent of these top 25 lectures were under one hour in length, which compared to the Data Mining (36%) and Machine Learning (44%) lectures is very high, and is only beat by the Big Data lectures, which has 64% of its lectures under one hour. This could indicate that text analytics lectures tend to be focused on more specific topics that don?€?t demand longer lectures.
,
,
,
,  "
"
,
,
, is becoming critical. Customers are more empowered and connected than ever. And becoming more so. Customers have access to information anywhere, any time ?€? where to shop, what to buy, how much to pay, etc. That makes it increasingly important to predict how customers will behave when interacting with your organization, so you can respond accordingly. 
,
, (June 19-20, 2014) was organized by Innovation Enterprise in Chicago. It brought together analytics executives and data scientists working in retail, ecommerce and consumer goods, offering unique insight into the innovations that are revolutionizing their relationship with customers.
,
We provide here a summary of selected talks along with the key takeaways. 
,
,.
,
Here are highlights from Day 2 (Friday, June 20, 2014):
,
, kicked off the second day of summit delivering a talk on ?€?Emerging Market Insights Combining Big Data & Mobile Market Research?€?. Browsing on mobile is pleasurable in US due to availability of 4G. However, many markets remain network challenged in delivering a quality internet experience to 4G Smartphone users. Feature phones continue to represent the majority of consumers in emerging markets. 
,
Giving a quick introduction of Xpress Browser, he mentioned that the service uses advanced, cloud-based technology with high speed Internet connections to render and optimize web pages. When user requests a web page using Xpress, the request is routed through a Microsoft Mobile server on its way to the destination website. The Microsoft Mobile server receives the page from the destination website, renders and optimizes it for user?€?s device, compresses the data and sends the response back to user?€?s device. This optimization can shrink web pages substantially, and as a result user can experience faster browsing and browse more web pages with your data plan. They also conducted a survey which resulted in these key findings:
,
, ?? ,
, talked about ?€?Using Data Mining and Machine Learning in Retail?€?. He said ?€?Big Data can no longer be defined by the amount of data, but by the type, speed and storage capacity needed to compute and analyze that data.?€? He stated the problem with large scale data processing is that using traditional computer processing it can be difficult to compute everything, due to storage space, processing time, and cost. 
,
Hadoop brings infinite scalability, extremely large storage capability and fast data processing. Discussing about Big Data Analytics in retail, he mentioned Mahout ?€? an Apache Foundation software project using scalable machine learning algorithms. He briefly discussed three primary algorithms: Clustering, Recommendation Systems, Market Basket Analysis. He illustrated with examples that how these algorithms can be help enhance sales of retail stores. He concluded the talk discussing various layers of Big Data stack.
,
, ?? ,
, delivered a talk on ?€?Using Analytics to Help Win the Presidential Election?€?. He mentioned that the similarities between customer analytics in the Obama Campaign and in the corporate world are quite striking. He mentioned lines from The New York Times dated Oct 13, 2012:  ?€?And as the race enters its final month campaign officials increasingly sound like executives from retailers like Target and credit card companies like Capital One, both of which extensively use data to model customers?€? habits.?€? During campaign several large sources of data were joined together. The campaign had 3 main goals for 2012 election: Register, Persuade and Turnout. Statistical models helped during the campaign to achieve these goals. A support model was built to predict who like Obama over Romney and then a turnout model to predict who would actually vote. 
,
, ?? ,
He briefly discussed how the team came up with a persuasion model to target undecided likely voters. A lot of fundraising emails were sent. They found that emails subject line ?€?Hey?€? raised majority of money. He also discussed about Obama Facebook app and polling. Peter concluded the talk predicting the following for 2016 elections:
,
, ?? ,
,
,  "
"
,
,
, is Senior Principal at ,, where he advises executives on business value creation from technology innovation. He leads strategy engagements with executives, helping them use Advanced Analytics to gain a sustainable competitive advantage in their industry. In addition he is a thought leader and evangelist for big data analytics, writing practical guides for CMOs, CFOs, and the nascent Chief Analytics Officer roles.
,
Prior, Mr. Siddiqi was at CEB, a best practices research and analysis firm, where he held multiple roles including Senior Director of Strategic Marketing, Chief of Staff to the CEO, and Senior Director of Research. Prior to CEB, he held roles in strategy consulting at Bain & Company and at Kaiser Associates; he started his career in FP&A at Engro Corp.
,
Mr. Siddiqi is passionate about teaching and education, and is active in the community through nonprofit work, including serving on the Board of Directors of Ingenuity Prep, a charter school in Washington DC.
,
,.
,
Here is second and last part of my interview with him:
,
,
,
,: Look at what your organization is already doing that could be improved ?€? or what others in your peer group are doing that could be emulated. What could be done better with Big Data techniques, such as mashing up social data with product data, or developing new cost models based on consumer profiles? Ask your partners / vendors to give you a baseline portfolio of use cases, ranked by level of maturity ?€? but make sure it is not a generic list. My team here at SAP, known as Industry Value Engineering (IVE) has developed this for different industries already and mapped each use case to a level of maturity. 
,
,
,
At SAP we do a lot of educating of our customers ?€? and part of that is a service called the Big Data Strategy Engagement. It?€?s a systematic, collaborative way of surfacing the right use cases, benchmarking the maturity of your current and aspirational abilities, building a business case containing an ROI and also a storyline that answers the Why and the So What for the Board and leadership. 
,
We build it in direct collaboration with customers in a simple 4-step process, and from our side we bring a team of strategists, design thinkers*, data scientists, technologists and architects. This helps our customers choose how they want to proceed, and then we can get into architecture, roadmaps and technology choices. Our goal is to help you craft a vision or if you already have one, to strengthen it and then accelerate it to make it real.
If this sounds like a strategy consulting engagement to you, then you are correct ?€? it is. However we do not charge for this service: we are in the software and cloud business and our goal is to help customers figure out how to capture value in their business using our platforms, solutions and expertise.
,
* Quick note about ?€?Design Thinking?€? ?€? it?€?s something that we leverage in addition to business thinking. 
,
,
,
,Don?€?t fall into the trap of diving into the specific tools you have or want without first thinking through the complete ?€?logical?€? architecture you will need. Your use cases will be what drives what elements you need in each layer.
,
This is an important task that needs the right amount of time to do but most importantly it needs an objective ,lens. Depending on how you look at it there are as many as 9 layers to consider. We have a chart that shows the elements within each of these layers ?€? and there are no product names on there, just what the element does. The readers interested in a quick assessment of their set up along these lines, can use the Big Data Architecture Assessment which is a quick check up ?€? it?€?s available ,
,
Disaggregating in this way helps to really understand what you need ?€? and what you don?€?t. Most important, it also helps you later determine whether what you already have in-house is up to the task or not. Once you have done that and validated it with the stakeholders, then you can start to actually put names and logos in. This way, you know exactly what you need, and where the gaps are. Then you can more easily do a trade-off analysis of how best to fill those gaps. For our customers many of those gaps can be filled relatively easily because of the engines already built into SAP HANA, its connections with all kinds of data science packages, plus of course it?€?s an in-memory columnar database and with SAP Smart Data Access getting data in and out is not a problem anymore and so on. 
,
The key to a sustainable information architecture is:
,
, ?? ,
,
,
,That's basically coming from a product management and strategy perspective. I'm calling out two things here, both of them about building big data / advanced analytics as a real business capability, rather than a series of one-off experiments.
,
First off, it's critical to assign business outcomes and quantified value to use cases. So for example if you are working on a product affinity analysis, what are the KPIs in your business that will be improved ?€? and what is the range of improvement you should expect to see over a period of time. 
,
Second point I'm making is that a big data capability really pays out when you scale it up. Once you decide to invest in the people, skills and culture, in the right standards and processes, in the platform and tools, then to scale it you need to ensure that your use cases are ambitious enough. The classic cost/volume/profitability logic applies - your costs will stay relatively fixed so it behooves you to ramp up the ambitiousness of your use cases. Don't settle for use cases that everyone else is doing- do those but also target the ones that will provide you competitive advantage.
, 
,
,
,OK to answer this correctly let?€?s first discuss two key ideas that are absolutely key when it comes to Big Data, but which people are only now starting to appreciate.
,
One is Scale, and the other is Optionality.
,
Now that data science is front and center for Boards and CEOs, they want it to quickly scale and grow ?€? but to get a competitive advantage using big data, you need to have both the ability to easily ?€?scale?€? from small projects to large deployments, as well as the ?€?optionality?€? of growing into future use cases, connecting to all kinds of data sources in real-time to do a variety of data science work.
,
For 40 years , has been all about providing an end-to-end system: both our customers and our engineers demand no less. The SAP HANA platform provides both scale and optionality for growth as you need it, without having to ,re-architect and change over all the time. Why? Because it?€?s built specifically to support big data and advanced analytics ?€? running at scale an In-memory columnar database, with engines for predictive analytics, geospatial and text analytics ?€? we also have something known as Smart Data Access which allows you to connect to virtually any data source. You can have a ?€?Start Anywhere, Go Everywhere?€? approach because now that ERP is running on SAP HANA, the artificial divide between ?€?transactions?€? and ?€?analytics?€? has also started to go away.
,
So you don?€?t have to go to multiple vendors for all those things and then to yet others for data storage, data access, information lifecycle management, middleware, security, etc., and then spend time and effort on trying to integrate everything. All that before you even get to model deployment ?€? a key area in which we really stand apart because as world leaders in enterprise mobility we have the technology to let you get the insights from your models into the hands of users, customers and machines. 
,
When you can do all that with one system, you can scale and you can have optionality. 
,
Finally, we have always done things with a deep appreciation and understanding of industry. In fact our entire company is organized by industry, so you have hundreds of people whose entire focus and mission in life, is to look at business challenges and SAP solutions from one industry perspective, so they develop deep expertise and best practices, all of which our customers get not just after they buy our products but as part of the evaluation cycle itself. And we do that for something like 26 or so industries and the public sector. 
,
Between the SAP portfolio, our industry focus, our partners and our startup community, it?€?s quite likely you will find what you need to innovate, scale and grow.
,
,
,
,I?€?ve recently started re-reading , by Jared Diamond.
,
As far as Work-Life balance goes, I try to remind myself as often as possible that we must choose whether we want to ?€?live to work?€? or ?€?work to live.?€? As long as I have the privilege of doing what I enjoy, that helps maintain a balance.
,
,
,  "
"
,
,
,
,??,??
,??,??
,??,??
,
,
,
,??,??
,??,??
,
,  "

"
,
,
,, held on June 22-24, 2014 at San Jose Convention Center, focused on "","" and educated the attendees on the best way to prepare themselves for the growing opportunities in the field of Big Data. The conference was a great opportunity for INFORMS The Business of Big Data 2014learning as well as networking with the leaders from industry and academia through a seamless exchange of information, ideas and perspectives.  ,The impressive line-up of speakers, which included Big Data leaders from various industries, shared the best practices through real-world case studies and tutorials on a wide variety of topics such as getting from data discovery to return on investment and real business value, bridging the gap between decision makers, IT managers, and analytics professionals, etc. The conference program included keynote sessions, presentations across 3 tracks (Big Data Case Studies, Big Data 101 and Emerging Trends in Big Data), technology workshops, panel discussions and exhibits. 
,
,.
,
Here is a succinct summary of the key takeaways from selected talks on day 2:
,
, started the second day of conference giving keynote speech titled ?€?Big Data in Action: Applying Analytics to the Internet of Everything?€?. In the Internet of Everything world, systems, devices and physical objects are constantly communicating with one another. He mentioned that there are more than a trillion connected and instrumented things such as cars, appliances, cameras, etc. His talk focused on how organizations can drive business outcomes in the connected world using big data analytics. 
,
Big Data in enterprise should target driving from issues to outcomes. He also discussed steps involved in ?€?Analytics Journey to ROI?€?: ??Issues -> Data -> Analytics -> Insights -> Actions -> Outcomes. He also shared solution to problem of water leakage and management (Refer picture below).
,
,
, talked about ?€?Optimizing Media Purchasing through Big Data?€?. ??AMG used lessons learned from the 2012 Obama presidential campaign to bring new, data-driven insights into the world of media buying. Using various modelling and data mining techniques in conjunction with large and rich datasets such as billions of set top box records. AMG discovered who is most likely to ?€?convert?€? to a product or candidate at the person-level. AMG then takes these desirable targets and uses a trove of set top box data to produce a near-optimal solution to problem of purchasing the most valuable placements given a limiting budget. 
,
He briefly discussed AMG?€?s techniques for identifying targets, strategies for efficiently storing and retrieving tens of billions of TV viewing records, and heuristics for finding a near-optimal media buy plan. AMG?€?s optimization software played a big in shaping the media planning of the Obama 2012 campaign, allowing them to stay competitive despite being outspent.
,
,, gave a talk titled ?€?Big Data and Big Analytics ?€? So Much More Gunpowder!?€? Paul?€?s talk focused on four themes: abundance, Hadoop, SAS on Hadoop, and Big Data ideas for organizations. We find ourselves in an ?€?era of abundance?€? because the cost of storing information has become less than the cost of making the decision to throw it away. We can use this data to answer questions that have not even been formulated at the time of data collection. Paul summarized the Hadoop ecosystem which supports the collection and processing of such data. He went on to describe several SAS offerings which interact with Hadoop in various ways.
,
Paul provided his own definition of Big Data as: ?€?the amount of data or complexity that puts you out of your comfort zone?€?.
,
,talked about ?€?Big Data Analytics Application to Jet Engine Diagnostics?€?. The immense explosion of information that we have witnessed is not just contributed by a large number of data sources, but also by large amount of data originating from these sources. Together they have created the so-called ?€?Big Data?€? environment. The challenge of information explosion is how to extract the right information at the right time from the data environment which we live in. The analytic process spots trends and patterns so as to derive predictive indicators. These indicators are then used to make proper recommendations or to take timely actions. Aircraft engines represent the type of machines that are most complicated and safety-critical. He discussed a case study of an aircraft engine diagnostic problem. 
,
Giving a quick background of the aviation industry, he mentioned various characteristics of machine diagnostic problem, various data elements and algorithms. The problem is posed as a classification problem in machine learning. After experimenting with different algorithms, the best algorithm was to be nonlinear SVM with a hierarchical dimension reduction technique (kernel sliced regression). He also shared results of analytics. The solution approach discussed in this use case is also applicable to various industrial sectors.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Sep 16 and later.
,
See full schedule at , .
,
,  "
"
, on Coursera
,
,
,
,
,Sep 29 - Dec 1, 2014
,
Instructors
,
,??,
,
,
We introduce the student to modern distributed file systems and MapReduce, including what distinguishes good MapReduce algorithms from good algorithms in general.  The rest of the course is devoted to algorithms for extracting models and information from large datasets.  Students will learn how Google's PageRank algorithm models importance of Web pages and some of the many extensions that have been used for a variety of purposes.  
,
We'll cover locality-sensitive hashing, a bit of magic that allows you to find similar items in a set of items so large you cannot possibly compare each pair.  When data is stored as a very large, sparse matrix, dimensionality reduction is often a good way to model the data, but standard approaches do not scale well; we'll talk about efficient approaches.  Many other large-scale algorithms are covered as well, as outlined in the course syllabus.
,
,
,There is a free book ""Mining of Massive Datasets, by Leskovec, Rajaraman, and Ullman (who by coincidence are the instructors for this course :-).  You can download it at ,  
,
Hardcopies can be purchased from Cambridge Univ. Press.
,
Enroll at ,  "
"
,
Data Driven Business is excited to announce that the 
,
West will be back in San Francisco on November 4-5, with pre-conference workshops on Nov 3. 
,
For the past decade, this event has been the meeting place for text analytics practitioners, vendors, visionaries and thought leaders.
,
You can access this year's summit brochure here: 
,
,
What's new in 2014? 
,
,??,
,
Check the full agenda, workshops, and speaker line up here: 
,
,
,
Use code ,
to receive the exclusive $100 discount on top of the Last Chance Discount of $100 if 
,  "
"
,
Most Viewed Machine Learning Talks at Videolectures ,
,
,
Most Viewed Text Mining Lectures 
,
,
,
3 Marks of Real #DataScience: Algorithms, not queries; Rich data model; look for top 10 attributes used by algorithm ,
,
,
Most Viewed Text Mining Lectures ,
,
,
,  "














"
By Graham Williams, (Togaware), Sept 2014.
,
,
The latest release of the , package for Data Mining in , is now available.
,
, continues to be the platform of choice for the data scientist. , is a freely available and open source graphical user interface for ,, wrapping up the use of over 100 R packages that together provide the most popular algorithms for the Data Scientist.
,
Rattle is used widely by Data Scientists across industry and by many independent consultants. It is also used for teaching the concepts of Machine Learning and Data Mining, and as a pathway into the full power of R for the Data Scientist -  an important feature of Rattle is that all functionality accessed via the graphical user interface is captured as a structured R script that can be run independently of Rattle to repeat every step performed by Rattle. In addition to being a useful tool for learning R it transparently supports repeatability of all activity in scripts that can extended or automatically be run at a later time.
,
The related popular text book , is available from Amazon.
,
Rattle version 3.3.0 is available on ,. Rattle is regularly updated with new user supplied features and bug fixes and the latest beta versions are always available from within R:
,
,
,
Also note that feature requests and bug fixes are now captured and voted on and resolved using the new , service where users are encouraged to nominate features they would like to see in Rattle. Such requests are then put up to the community for implementation.
,
,
,
,  "



"
Most popular 
, tweets for Sep 15-16 were
,
Best #BigData Deal: Mining Massive Datasets on Coursera, by top Stanford researchers, starts Sep 29 and is free ,
,
,
14 Awesome (and Free) #DataScience Books on Statistical Learning, Machine Learning, Mining #BigData & more ,
,
Best #BigData Deal: Mining Massive Datasets on Coursera, by top Stanford researchers, starts Sep 29 and is free ,
,
14 Awesome (and Free) #DataScience Books on Statistical Learning, Machine Learning, Mining #BigData & more ,
,
,  "
"
        ,  "
"
        ,  "
"
,
The , is set to take place at the Bellagio Las Vegas on Oct. 20-21. 
,
If you're serious about analytics, you won't want to miss it. Here are the top 10 reasons why you should attend.
,
,
,
Get registered today! And don't forget the KDnuggets News subscribers' special discount. Enter , on the , and receive $500 off standard conference fees.  "
"
,
Latest ,, (Sep 18, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
Statistically, you are a more likely to be killed by a statistician than by a shark, Tweet by ,, Aug 2014  "
"
By ,, Sept 2014.
,
To understand how to build and lead an effective business analytics (BA) team, we studied the sharp decline of one. We spoke with several people involved, before and after a change in leadership.
,
Let's start with a synopsis.
,
,
,
,
Now, let's take a look at what happened.
,
The original CAO built and managed the practice using a delegation-style approach, which depends on hiring, trusting, and supporting smart people. The CAO, who possessed a strong business acumen and had great relationships, acted with integrity by placing the company's interests first. The CAO had light training in statistics but no data analysis experience.
,
The CAO chose lieutenants who had the courage to report the facts and viewed the BA group as objective stewards of information. The lieutenants needed a supportive CAO to help navigate any mercurial opinion-based clients.
,
In the fullness of time, this CAO moved on, and the BA group merged with its sister group. The ""replacement CAO"" -- an experienced general business manager with no BA experience -- insisted on reviewing all findings before passing them on to the business units. This CAO insisted that findings matched business opinions and projected desired perceptions; anything inconsistent was discarded. Because the CAO didn't support lieutenants as they dealt with opinion-based clients, these clients felt free to push harder to make results fit their opinions and to make their performance look good.
,
One business unit continued to underperform, but objective information that would help guide performance wasn't available any longer. Nevertheless, all the opinion-based clients looked like high performers.
,
The following table summarizes the contrast between the two CAOs:
,
,
,
In the new environment, the lieutenants became so undermined that they were irrelevant. As one said, ""Whatever you analyzed, they [the opinion-based clients] were going to do what they wanted anyway.""
,
This gap provided opportunities for yes-men to serve up nonsense numbers to fit conclusions by hiring outside name-brand management consultants. The yes-men were too na??ve or disinterested to realize the analysis they were buying at a premium price was dubious. For their part, the management consultants were happy to supply whatever findings would fulfill the desired conclusions?€? and their sales quotas.
,
The yes-men began outsourcing everything, whereas the two lieutenants had believed in having some native BA capabilities. At times, the management consultants failed in an obvious way, and internal resources had to be repurposed to rescue them. Meanwhile, the fact-based decision makers became wary enough of the CAO's spin that they hired their own outside consultants.
,
Eventually, the lieutenants left, and BA stopped providing the services it was expected to provide. All that remained from the BA group was the dubious external consulting, which was filtered through the replacement CAO. This CAO wasted a pile of money on the dubious analysis and later on a big-data fishing expedition.
,
There are four observations to draw from this example.
,
,
Do you take away any other lessons from the dissolution of this analytics group? How else might the BA group have survived while serving a useful purpose? Are you surprised that this sort of situation happens within companies today? Why or why not? Share your opinions below.
,
Original:,
,
,
, is a Business Analytics Author & Thought Leader.
,His latest All Analytics blog:,
,Analytics Magazine: ,; Amstat News: ,
,: ,
,
,
,
,  "
"
By Gregory Piatetsky,  
,, Sep 18, 2014.
,
,, a top-rated Caltech course by a leading Caltech professor 
,, is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. 
,
Course starts on edX on Sep 25, 2014 and will last 10 weeks.
,
,
ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to automatically learn how to perform a desired task based on information extracted from the data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects. The lectures follow each other in a story-like fashion:
,
,??,
The topics in the story line are covered by 18 lectures of about 60 minutes each plus Q&A.
,
MATLAB and LIONsolver are offering free licenses to all enrolled students for the duration of the course. 
,
Register at
,
, 
,
The course lecture videos have been very popular with over 1.2 million hits on the videos on the Caltech Youtube and iTunes channels. 
In fact, the top 3 videos in all categories in the 
,  are 3 lectures from the course. 
,
The most popular lecture videos from this course are: 
,
,??,
,
,
  "
"
By Markus Hofmann, Sept 2014.
,
With flexibility of a tool comes complexity. Our videos help to understand simple as well as complex aspects of RapidMiner tools.
,
A set of free videos are available on 
,
,??,
, offers a large range of RapidMiner videos focusing on important and fundamental aspects of this great software suite. 
,
In addition, we have released our first complete course which consists of 43 videos. This course called 'RapidMiner Analyst Preparation Course' aims to prepare learners to take the official RapidMiner Analyst exam. 
,
A set of advanced videos are also available online.
,
The next set of videos will focus on advanced features and concepts of RapidMiner including Text and Web Content Mining. We aim to release this batch of videos at some stage late this year.
,
, is Lecturer in Informatics at 
Institute of Technology Blanchardstown (ITB), 
Dublin, Ireland.
,
,
,  "
"
Most popular 
, tweets for Sep 17-18 were
,
Business #Analytics Returns $13.01 for each dollar spent, up from $10.66 3 years ago, Nucleus Research Finds ,
,
The Science of a Great Career in #DataScience, entertaining presentation, esp. for Star Wars fans ,
,
Business #Analytics Returns $13.01 for each dollar spent, up from $10.66 3 years ago, Nucleus Research Finds ,
,
20 questions you should ask a Data Scientist for a good conversation ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,:??the complete description of the structural connectivity of an organism?€?s nervous system, often used in connection as a comprehensive circuit diagram of the human brain
,
, A , of proteins, transcription factors that assemble cooperatively at the , region of a ,;?? the region of , that can increase ,.
,
, ??refers to the study of the amount of environmental contaminants that a person is exposed to from conception onwards during their lifetime, covering their eventual impact on genomes and ultimately health.
,
, Informatics for Integrating Biology and the Bedside, a framework facilitating the design of targeted therapies for individual patients by enabling clinicians to use existing data for discovery research
,
, a type of genetic variation involving either an insertion or deletion mutation of a specific nucleotide sequence
,
, ??comprise protein complexes implicated in physiological and pathological inflammation, that are activated upon cellular infection or stress to facilitate innate immune defences.
,
, Two organisms are isogenic when possessing the same genetic composition,
,
,
,
,?? an individual's collection of chromosomes. It is equivalent to the number and appearance of 
, in the 
, of a 
, (somatic) 
,. ??It also refers to a lab technique that produces a photomicrograph of an individual's chromosomes
,
,?? the genome subset comprising the protein kinase genes expressed in a cell,
,
, describes the functional and sequence-based genomic analysis of the collective microbial genomes contained in an environmental sample by direct extraction and cloning of DNA
,
,?? the total genetic content of microbes, that are collectively referred to as microbiota, ??colonising a given environment, especially the human body,
,
,: the study of related sets of biomolecules, interpreted by a specified computational model.
,
,: A group of distinct genes that are expressed and regulated, functioning as a single 
, unit.
,
,:?? A small, mobile DNA sequence that can move from one genomic location to another by producing RNA that is transcribed by reverse transcriptase back into DNA, which is then inserted at a new site. It is also called a retroposon.
,
, Small interfering RNA (siRNA) are small pieces of double-stranded (ds) RNA?? ??20-25 nucleotides long that can interfere with the 
, of proteins i.e. with the expression of a specific gene
,
, the full range or complement of all RNA molecules, including 
,, 
,, 
, and other , in 
,.
,
,
,
,?? a segment of DNA that inserts and independently replicates itself in another place in the genome.
,
, comprises the variants in an individual's genome, the total set of 
, found in populations of 
, that have experienced ??a relatively short 
,ary change
,
,  studied Chemistry with Biochemistry for BSc at Brunel University, London then obtained an MSc in Bio-Organic Chemistry from the University of St Andrews.  His bioinformatics experience includes 5 years with Thomson Corporation as a scientific indexer involved in the updating of pharmaceutical databases. Currently he is working as a freelance information scientist with interests in both chem.- and bioinformatics.
,
,
,
,  "
"
,
,
,??,??
See also past ,.
,??,??
,
,
,??
,
,  "
"
By Gregory Piatetsky,  
,, Sep 20, 2014.
,
Here is a Machine Learning gem I found on the web: 
,
a free online book on 
,, written by 
,, a scientist, writer, and programmer. 
,
The book covers:
,
,??,
Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. 
This book teaches the core concepts behind neural networks and deep learning.
,
The book is written in a nice, visual, explanatory style and provides good examples while not shying away from math. 
Currently, the book is a draft, and has
,
,??,
Planned chapters include
,
,??,
,
,
,  "
"
,
INFORMS, the leading professional association in advanced analytics, has introduced the , to help you take a good, frank look at how you are using analytics and how you can improve your organization.
,
The INFORMS Analytics Maturity Model (IAMM) is designed to provide useful information whether your organization is at an initial, developing, or advanced level of analytics. It helps you assess your strengths and weaknesses in three broad areas of analytics:
,
,
and drills down with a dozen peering questions that provide broad analytics understanding.
,
The INFORMS Analytics Maturity Model not only lets you score your organization's abilities in these 12 encompassing areas; it also lets you set a goal and a timeline to reach a higher level of analytics maturity that will set your organization apart. You'll be able to benchmark against others in your industry. The model includes visual tools that add to your understanding. You can use these tools when you take the assessment a second and third time as you chart your progress.
,
Try it out free at
,
,
,
,
,
,  "
"
,
,
A WCAI Research Opportunity Sponsored by an International Beauty Retailer
,
October 17, 2014,
Noon ?€? 1pm US Eastern Time
,
,
,
The Wharton??Customer Analytics??Initiative is excited to announce a unique dataset from an International Beauty Retailer that allows for an unprecedented view into how customers?€? relationships with a retail brand evolve.
,
This dataset contains comprehensive customer interaction information for 85,000 randomly selected customers and prospects from 6 different countries, and includes:
,
,
,??,
The research sponsor sells a unique, exclusive line of beauty products and their ideal customer develops a strong loyalty to particular products or lines of products. This anonymized data will be provided by competitive proposal to 6-10 research teams who want to study these problems (or any others related to the sponsor?€?s business).
,
Through this Research Opportunity, the sponsor wishes to better understand:
,
,
,??,
To learn more about the company and the dataset, interested faculty, doctoral students, and other researchers can attend a live webinar??on October 17, 2014 from 12:00-1:00pm US Eastern. During the webinar, the data will be described in detail, and executives from the Corporate Sponsor will be available for Q&A.????
,??for the webinar here.
,
Interested researchers should submit proposals online through the??,by November 3, 2014 to be considered for access to the data. Proposals will be evaluated based on their potential for academic contribution and the researchers?€? ability to address issues of strategic importance to the program sponsor. A committee including representatives from the Corporate Sponsor, Rajdeep Grewal (Penn State University), Eric Bradlow (The Wharton??School), Peter Fader (The Wharton School), and Elea McDonnell Feit (Drexel University) will select the teams to receive the data.
,
Researchers are encouraged to review??,??and??,before submitting their proposal. Additional questions on the data or the process can be directed to,.
,
,
,
,  "
"
,
,
Gartner's , (below) shows the relative expectations of various technologies, including Big Data, Data Science, In-Memory Databases, and Prescriptive Analytics. In my mind, this illustrates it's time to stop tossing around buzz-words and start realizing value.
,
,
,
To give some perspective, it is important to realize that analytics is an evolution of skills and capabilities. ,
,
,
,
,
,
With that little bit of context, let's take a moment to cut the buzz-words and get into the nuts and bolts of data science techniques.
,
This particular post is one that I have held in draft for some time now, simply because generalizing the complexities of mathematical models, computational efficiency, and sophisticated techniques is most definitely going to lose some accuracy. With that, this isn't meant to teach or to guide a data scientist, it is meant to ,. All the while, providing a reference for Data Scientists and technical leaders to use as they try to distill ,.
,
Let's begin!
,??,
,
,
,Solver/ SolverTable within Excel
,
Linear programming is an , that allows users to maximize (or minimize) an objective function (a metric defined by an equation). In the graph below, the objective function is to maximize profits, given the trade off of manufacturing tables or chairs within a certain number of production hours. Although the example below is quite rudimentary, linear programming allows for many constraints/ factors and is incredibly fast, because it simply draws a number of ""lines"" to represent each constraint (green lines) and then identifies the peak ""feasible"" point (the optimal).
,
As you might expect, non-linear programming doesn't require linear constraints. However, non-linear is known to be much more computationally challenging as the program runs through each potential point (or uses an approximation parameter/ gradient).
,
,
,
,
,
,:
Requires users to ""know"" the constraints, influential variables and their relative impacts.
,
,
,:@Risk,
An engineering and MBA classic, Monte Carlo simulations allow for users to designate randomization functions and distributions to represent unknowns. This is used to simulate problems that are not deterministic (meaning that they ,). This ultimately estimates both a cone of uncertainty and the most-probable outcome.
,
,
,
,
Since Monte Carlo runs simulations based on estimates and user defined inputs, the output has ample opportunity for human error. Monte Carlo simulations can get pretty darn complex, but don't mistake complexity for accuracy. Instead, with Monte Carlo we must apply a veil of reason and constantly work to eliminate error by testing and benchmark against real world outcomes. Furthermore, whenever possible we should derive our estimations from historical data.
,
With that said, there are certain realms where Monte Carlo is phenomenal and best suited. As with any technique, simply urging caution. Take a look at some of Monte Carlo's variations-- such as Markov Chains, but that's for another post.
,
,
,StatTools,
The classic. Good ole regression. Fitting a line to a set of points (ax+b=y). Regression provides insights into the relative importance of variables and the drivers of a given outcome. Today, regression takes many forms linear, logistic, polynomial, MARS, etc. One of the major differences is the , Most people are familiar with SSE, the sum of squared error. However, there are many more exciting options! Below is an image of a few of the flavors.
,
,
,
,
,
,
The key limitations are the input data being independent, well chosen, and interpreting the output. Regression can be deceptively confidence boosting particularly on large datasets. If you're feeling extra nerdy, check out , (,).
,
,
, Numerous,
Decision trees are easy to interpret and often output a great visual. Decision trees work well in situations where they are predicting a binary outcome. For instance, buy or not buy (1=buy, 0=not buy) based on certain characteristics of a consumer/ customer. As we progress, the examples I use will focus on marketing because it is relatively easy to follow, however, these models are all ,, they don't care what functional area or industry the data comes from! Below is an elementary example of a decision tree.
,
,
,
,
Decision trees aren't always good with datasets that are dynamically changing. In other words,when what's happening or going to happen doesn't match what happened in the past. Also, they have a tendency to ""overfit"" the data. ,, they're well aware of these problems and are able to ""tune"", adjust, reconfigure, and test against a holdout dataset.
,
,Great question! Essentially, by randomly splitting the data or using cross validation, analysts can build a model with one set of data and then get the accuracy stats with another set.
,
Another concern is that interpretation is limited because variables exist at different ""steps"" in the decision tree and errors propagate forward. In other words, mistakes made at the beginning can impact the entire model!
,
,
,
,
, knn package in R
,
Although there are a ton of classification algorithms, we'll focus on K-Nearest Neighbor, simply as a means to convey the logic.
Let's say we have a dataset of buyers and non-buyers with lots of characteristic columns (things like age, gender, income, etc).
,
Technically speaking, it would be more accurate to describe our data as-,-- but life's too short for that many two-dollar words in one sentence.
,
We'll stick with ,. So let's say we have a new list of ""prospects"" and we have the columns of characteristics but we don't know if they'll become customers. Well, KNN can help predict! In the visual below, let's say that we have customers-- blue squares and non customers-- red triangles.
,
Then along comes ""Green-dot-man"". Will he be a customer or not? Well, in this case, it depends on a few things as to what we would predict.
,
,
,
First, how big is ""K"", in other words,,? If we look at K=3, then we would look at the points inside the, and see there's 2 red/non-customers and 1 blue/customer, so we'd predict green-dot man is a non-customer. However, if we look at K=5, we'd look within the dotted line circle and find 3 blue/ customers, 2 red/ non-customers.
,
What can we do? Well, we could weight by distance. In other words, we could say, let's consider those points that are closest (aka most like Green-man) more than the points further away. In that case, it would likely be a toss-up. , As our model would give us a ""probability"" of being a customer. For things like mail campaigns that is highly relevant!
,
So when is this a good option? Well, let's think about Amazon for a second. Currently, Amazon recommends products that are ""associated"" with the product you are looking at or your browsing history. However, that's a pretty loose model.
,
Rather, a K-Nearest Neighbor model is likely to find that handful of weirdos just like you, those guys that also buy red silk suspenders, rent movies at 9pm on Friday nights, look at pocket protectors that are dishwasher safe, and write shamelessly about their ,. Those recommendations will drive tons of sales! ,
,
,
The true downside is that KNN calculates the distance between each Green-dot-man (new point) and every other point. , Fortunately, there are some binning, parallelization and generalization strategies that can speed up the process.
,
Here is the
,.
,
, is a Graduate Student at ,.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Sep 22 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Sep 19-21 were
,
Dilbert 20 funniest cartoons on #BigData, data mining, data privacy, data security, ... #humor ,
,
Neural Networks and Deep Learning, free online book (draft) ,
,
Dilbert 20 funniest cartoons on #BigData, data mining, data privacy, data security, ... #humor ,
,
Randomness: One pattern is random, the other is machine-generated. Can you guess which ? ,
,
,
,  "
















"
,
,
A recent billion-dollar forecasting error in Walgreen?€?s Medicare-related business has shocked the company and investors. The CFO was pressured to leave after he cut his 2016 pharmacy-unit earnings forecast from $8.5 billion to $7.4 billion. The company explained that it had not factored in a spike in the price of some generic drugs that it sells through its annual Medicare Part D contracts.
,
While an error of this size draws the attention of the press and sends shock waves through the industry, Healthcare companies are regularly buffeted by armies of similar but smaller scale issues.
,
Walgreen?€?s forecasting mistake highlighted a common problem: ever-growing business complexity, with interconnections between many market, business and regulatory drivers. Addressing this new level of complexity with traditional business methods is no longer sufficient. To remain competitive and successful, Healthcare companies need robust analytical tools that can cut through gigabytes of data, dozens of issues and find connections between many different factors.
,
When it comes to advanced analytics, Healthcare is still behind many other industries such as Finance and Consumer Goods. However, the industry is rapidly discovering that Predictive Analytics can help to solve some of the most critical issues it faces today:
,
,
,
,
The total demand for Healthcare services is projected to grow 14 percent by 2020. About 80 percent of this additional demand is likely to come from aging, with the remaining 20 percent from expanded coverage under ACA.
,
At the same time, the number of primary care doctors is projected to grow only by 8 percent, leaving a likely shortage of 20,400 primary care physicians. How can the industry close this gap?
,
,
,
,
,
,
,
While we will have more people to cover, health care budgets will not increase proportionately, translating into fewer dollars per patient, and putting pressure on pricing and margins throughout the entire value chain from pharma and device manufacturers to health care providers.
,
,
,
The industry tax will further compound margin pressures, taking billions of dollars of revenue from branded market and device companies.
,
Changes to payment models will likely cascade across the industry and affect all participants, from private insurers and government which will develop new models, to providers and patients who will change their behavior accordingly.
,
Each of these steps can affect the eventual decision to prescribe or use a specific health care product or service, as well as how much this product will cost and how it will be paid for.
,
And it is clear that ACA will result in additional future price pressures, the impact of which will vary greatly by product, therapeutic area, service and state.
,
,
,
,
,
,
,
,
Medical decisions will be more regulated and regimented, with payees increasingly mandating what doctors can prescribe, and doing so first and foremost from the standpoint of cost effectiveness. The resulting increased formulary restrictions will impact large areas of branded portfolios, but the degree of formulary pressure will vary by therapeutic area and health channel.
,
,
,
,
,
,:
,
OTC switches will continue to grow and take a big bite out of the current Rx market.
,
Key drivers of OTC growth include insurance / reimbursement changes and pressure on prices.
,
Many Rx blockbuster products are in the late stage of their lifecycle, coming off-patent and losing 90 percent + of their volume to generics right away. The OTC switch offers a ?€?second chance?€? for these brands. Finally, more informed consumers are increasingly interested in self-treating simple or chronic conditions
,
,
,
,
,
Healthcare organizations today are constantly pressured by contradictory goals: to see more patients in less time, but become more patient-centric; and cut costs while improving quality and outcomes.
,
At the same time, it is clear that the industry is plagued by entrenched inefficiencies and suboptimal processes.
,
,
,
,
Successful pharma companies will have to change their ?€?blockbuster?€? and ?€?high margin?€? mentality and rely on having more products, with lower revenues and lower costs.
,
That means having to overhaul the entire business model starting with a more nimble drug discovery and R&D process and ending with the ability to effectively target more fragmented ?€?specialist?€? networks.
,
,
,
Looking forward we can expect that in the next five to ten years the Healthcare industry will undergo monumental changes. ACA related changes (direct and indirect) are estimated to have a negative 10-15 percent impact on Healthcare revenues and at least double that (20-30 percent) on gross margins. To survive and navigate through these unprecedented changes and complexities, many Healthcare companies will need to find a new path forward, as well as cutting edge management tools to stay on that path. Advanced Analytics capabilities will become increasingly important in these conditions, providing clarity, promoting objectivity, and enabling faster and better decisions.
,
,
,
Original:
,
,
,
,
,
,  "
"
        ,  "
"
,
By Gregory Piatetsky,  
,, Sep 23, 2014.
,
The results of the latest KDnuggets Poll:
,
,
,
,show that 
,, with 62% of respondents working ONLY or MOSTLY on data science for humans.  Only 14% have worked only or mostly on machine data.  The ""Other"" votes could represent people working on non-human biological data or purely scientific data, like astronomy.
,
At the same time, about 60% of respondents worked with both human and machine data, casting doubt on the assertion that there are 2 types of data scientists: ones that work with machine data, and ones that work with human data.
,
,
,
The table below gives a regional breakdown for 4 regions with sufficient representation, with 5 ""Other"" votes excluded.
,
We note that Europe and Asia have higher machine-data experience than the US, while in Latin America almost all data scientists who responded work with human data.
,
,
,
,
,
,??,
,
,
,
 ,  "
"
,
at 20th International Conference on Management of Data
,
COMAD 2014,
17th-19th December, 2014,
Hyderabad, India,
,
COMAD 2014 announces a programming contest on mining densest subgraphs from a given graph, which correspond to communities in a variety of social networks.
,
The important dates are as follows:
,
Registration deadline: November 3,
Submission of code: November 5,
Announcement of results: November 12,
,
For details, please see the website at
,
,
,
or the pdf at
,
,
,
The problem of finding relatively highly dense sub-structures in various social networks such as communication networks, collaboration networks and web networks has received a lot of attention [,,,,,,,,,]. Experiments have suggested that such subgraphs correspond to , in those networks. Formally, given an undirected un-weighted graph, the problem is to find the ,.
,
The combinatorial algorithms based on max flow computation for finding maximum density subgraphs have been studied extensively [,,,]. However, the time complexity of exactly finding the densest component using these algorithms is cubic or worse, and hence, they are not scalable for large graphs in real-world settings. Although the previous stated results are the best that any deterministic algorithm can achieve, there exist fast linear time algorithms for computing approximate solutions [,,,]. However, they are destructive and, thus, return only a single subgraph.
,
,
,
,  "

"
,
,
,
,
,
Presented by 
,PACE: 
Predictive Analytics 
,Center of Excellence
,
Each day, our society creates 2.5 quintillion bytes of data (that's 2.5 followed by 18 zeros). Conventional statistical analysis and business intelligence software are not designed to capture, curate, manage and process large quantities of data generated by most enterprises.
,
Learn the critical predictive data analytics techniques and tools that contribute to accurate, actionable and agile insights. Boot Camp training includes:
,
,
,??,
The PACE Boot Camps provide the , community with conceptual and hands-on training for the critical predictive data analytics techniques that help discover patterns and relationships which in turn contribute to accurate, actionable and agile insights.
,
,
to be held October 15-16, 2014 ,
at the San Diego Supercomputer Center, UC San Diego,
,. 
,
,
,
,
For over 25 years, SDSC has been at the forefront of new trends and developments in high-performance and data-intensive computing. SDSC's large-scale data systems include Data Oasis, a 5 petabyte parallel file system; the SDSC Cloud, consisting of 5.5 petabytes of disk (with replication); and the Gordon system with 64 terabytes aggregate memory, 300 terabytes flash memory, and 341 teraflops of computing power.  "

"
,
Latest ,, (Sep 24, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
62% of data scientists indicated in KDnuggets Poll that they worked ONLY or MOSTLY on data science for humans, while only 14% worked primarily on data science for machines. See ,.  "
"
Most popular 
, tweets for Sep 22-23 were
,
The Future of Food is in #BigData: former YouTube chief data scientist heads the effort to model new food creation ,
,
Machine Learning for (Smart) Dummies: 7 week #MachineLearning Course taught at Yahoo Labs ,
,
,
Ex-Googler shares his #BigData secrets, creates Quest, like Google Dremel - SQL Big Data querying tool ,
,
Machine Learning for (Smart) Dummies: 7 week #MachineLearning Course taught at Yahoo Labs ,
,
,  "
"
        ,
,  "
"
,,
by Brendan Tierney
,
Published: August 5, 2014 Paperback, 464 pages ISBN?? 0071821678 / 9780071821674
,
The book has 3 main aims.,
First is to introduce the concepts of Data Mining and Predictive Analytics to the mainly Oracle community.
,
Second is to show how to use the graphical workflow based Oracle Data Miner tool (that is part of SQL Developer) to create models and how to apply these to new data.
,
Third is how to perform all the steps and tasks of data mining and predictive analytics using the SQL and PL/SQL functions and procedures.
,
The book is available in Print and e-Book formats from Amazon and the McGraw-Hill website
,
Each site will offer discounts so check out which one is the best for you.
,
For USA locations (enter promo code Tierney to save 20% and free delivery)??,
,For UK & Ireland locations (enter promo code Tierney to save 20% and free delivery)??
,
,
,
?€? Charlie Berger, Senior Director Product Management, 
,Oracle Data Mining and Advanced Analytics
,
,
,
,  "
"
,
,
Ed. Charu Aggarwal, and Jiawei Han, Springer, 2014
,
TABLE OF CONTENTS and INTRODUCTION: ,
,
Comprehensive Coverage in the form of surveys on  the entire area of Frequent Pattern Mining
,
Frequent pattern mining is one of the problems which serves as one of the distinguishing problems of the data mining area, separate from statistics and machine learning. In fact, the early work in frequent pattern mining provided  an important impetus to the establishment of a separate  field of data mining. Since then, the problem and its solutions have matured, along with the subsequent study of a wide variety of data types.  This comprehensive book studies the field of frequent pattern mining from several perspectives:
,
Methods: The book first describes common techniques used for frequent pattern mining, such as Apriori, TreeProjection, Vertical Methods, FP-growth etc. Methods for  long pattern mining, interesting pattern mining, compression methods, negative pattern mining, and constraint-based mining are studied separately.
,
Domains: The book then examines specific methods used for data domains such as discrete sequences, spatiotemporal data, graph data and uncertain data. It also covers large data sets and data streams due to the recent importance of the big data paradigm.
,
Applications: The book concludes with several important applications. Two chapters are dedicated to clustering and classification. The final chapter of the book discusses a wide variety of applications of frequent pattern mining along with pointers to resources for the practitioner.
,
Features
,
,
,
The table of contents and the introduction may be found at ,
,
,
,
,  "
"
By Alex Jones, Sept 2014.
,
This is a second part of the post - here is the first part:
,
,
,
,R's neuralnet package,
Although a somewhat older method, Neural Networks have only recently truly been utilized, thanks to an exponential decrease in the cost of computing. Neural networks take in input data and create ""hidden layers"" capable of learning and self-improving through trial and error-- heck, it's ,! IBM's Watson, Image Recognition, and Handwriting software often are driven by neural networks and they often are well suited for tasks that have huge amounts of information and are poorly suited for traditional programming. The computational complexity can be immense and interpretation of the ""importance/ influence"" of any given variable is difficult.
,
,
,
,
Its not easy to interpret the influential variables, works best with large amounts of information, and can become very computationally complex.
,
,
Deep Learning is the stuff of sci-fi movies and artificial intelligence in which computers parse through unstructured and unlabeled data (including images, videos, language, speech, etc) and eventually make sense of the information.
,
Recently, Google developed an artificial neural network across 16,000 CPU cores and ""watched"" YouTube videos for 3 days. Without guiding or limiting the model to particular videos, they found it was able to recognize cats. While this may serve as testament to the massive number of cat videos on YouTube, the finding itself is extraordinary.
,
,
,
Essentially rather than being told what ""features"" or ""characteristics"" to look for, the model identified repetitive shapes and ,.
,
While humans are incredible at these sort of tasks, Deep Learning methods are catching up... and in some cases beating us. In 2011, a contest in Germany put, to accurately recognize street signs (many of which were blurry and dark). , with 99.4% while humans achieved 98.5% accuracy. Want to learn more about , artificial intelligence? Check out ,.
,
,
The computational complexity and data requirements are massive, but it represents the progressive evolution of data science and that is exciting.
,
,
, R's kmeans package
,
Clustering has been around for decades. However, Clustering has greatly evolved since the days of hierarchical agglomerative (,).
,
Clustering is a lot like K-nearest neighbor, except clustering , and is often used to identify things like customer segments.
,
Below is a visual of a handful of the more popular clustering algorithms and how they would cluster the respective dataset. As you can see, some algorithms have difficulty discerning data that isn't well separated from other clusters while others are perfectly capable of identifying such relationships.
,
,
,
,
Depending on the algorithm you use, it can be challenging to interpret what you're looking at. Also, most methods begin with a ""random"" starting point, which means you can ,
,
One of the advantages of open-source software (R, KNIME, Rapidminer--and other tools listed in ,) is that when new methods come out, they are often quickly released. Whereas, on traditional software, development can be a long process. Finally, Complexity is a function of the number of columns, therefore, very ""Wide"" datasets, can take some time.
,
,
,
,
Ensembles are a little unique in that they're the combination of existing. Ensembles are phenomenal, intuitive, and relatively quick. Essentially they take multiple models and , into prediction-ultron (related to Google Ultron). Ensembles allow you to balance out models weaknesses/ biases and improve predictive performance.
,
For this post, we'll look at an ,. Essentially this randomizes the development of decision trees (to create some variability), then at each ""Leaf"" node (aka the end of the decision tree/ flow chart) it fits a regression line!,
In other words, rather than simply saying everything in leaf node number one is a buy, this model would use a regression equation within that node to say something like ""there's a 92% chance that Bob will buy, a 99% that Ron will buy, a 82% chance Tod will buy, etc"".
,
,
,
, Not only is this pretty neat, an Ensemble Regression Decision Tree creates X number of trees (let's say 50). Then makes 50 predictions for each point.
,
, Oftentimes simply averaging (or other aggregation methods such a committee vote, majority vote, confidence weighted averaging, etc) across multiple models can , by a few percentage points (which translates into real money).
,
,Finally, since each one of the decision trees/ algorithms are independent, ensembles are wonderfully parallelizable. What does that mean? Basically, it is pretty darn fast.
,
,
Good luck interpreting it. Although the model eventually spits out a single averaged/ aggregated prediction, it is hard to tell what is driving the changes.
,
,
, Adaboost in Weka,
Adaboost is one of the more famous (and rightfully so, it is quite powerful) algorithms that implements an ensemble method. Essentially, it fits a classification parameter (shown below with a line), then aggregates each prediction into a single parameter (the bottom box). What makes Adaboost interesting is that it focuses on ""weak"" classifiers. In other words, those variables that weren't really ""used"" in the initial model are now called upon to fit a micro-level model. In essence, , *,* *,*
,
,
,
,
Although there's no perfect algorithm, Adaboost is one of those pretty-darn good ones, that does well without too much extra work.
,
,
Support Vector Machines are black magic/ voodoo. I say that because it often appears counter-intuitive if you try to understand it first from a mathematical perspective. Rather, this is ,. In the most basic sense, SVM separates data into two classes (think buyers - Blue, non-buyers - Red). Take a look at the visual below.
,
,
,
Looks good to me? Both sides are separated! That's true. But take a look at how the line leans a little bit and there's red/ non-customers close on the bottom and blue/ customers close on the top., Now, let's say we run a marketing campaign and we predict purchases using this line. We'll most likely get something like this:
,
,
,
Pretty good. Essentially, we just got that Red customer wrong. In other words, although that customer didn't buy, we predicted that it would. No big deal, right? Maybe... ,
,
So let's see what this SVM thing is all about.,We're going to project the points into a higher dimensional space that maximizes the distance between the two groups. ,
,
Ultimately, we'll end up with a line like this. Rather than ""tilting"", this line tries to maximize the Margin/ space (that weird yellowish colored area around the line) between the two groups.
,
,
,
Ok now let's see what happens when we predict new customers.
,
,
,
,Boom, accuracy improved. Below is a visual that's a little more accurate in illustrating what is actually going on and how the whole "",s"" works. Rather than ""lines"" SVM works with hyperplanes and there's a whole slew of ""kernel functions"" that add to the complexity. But again, for our purposes, not really important.
,
,
,
,
SVM can be a computational nightmare. So why use SVM? Because it's , accurate and ""learns"" rapidly from additional input data. Sadly, it is simply not possible to interpret and challenging to tune.
,
,
Below is a ,reference table for determining what algorithm will , be useful. However, think for a moment about the problem at hand, reflect on the conceptual understanding of your goals versus the methods available, and realize that most times , Having said that, reach out for support-- data cleansing, prepping, and modeling can be daunting!
,
,
,
At a macro-level, you may be thinking that these don't seem that novel or revolutionary. I agree. ,
,
What has changed is the sheer size and scale of data, the scope of metrics being collected, and the ability to analyze it with relatively inexpensive, scalable, and fast computing.
,
Over time, more industries will fundamentally change or be disrupted as companies begin to leverage analytics, enhance efficiency, and allow data to drive decisions. Simply put, the ,
,
My hope is that this will help to develop an intuition and trust for data science among business leaders. Ultimately allowing the nerds/ data scientists to focus less on how to present complex topics and business to begin realizing value!
,
,
,
Here is the
,.
,
, is a Graduate Student at ,.
,
,
,
,  "
"
Most popular 
, tweets for Sep 24-25 were
,
Watch: Statistical & Machine learning with R, great 15 hour online course, by Stanford profs Hastie & Tibshirani ,
,
Watch: Statistical & Machine learning with R, great 15 hour online course, by Stanford profs Hastie & Tibshirani ,
,
Watch: Statistical & Machine learning with R, great 15 hour online course, by Stanford profs Hastie & Tibshirani ,
,
50 Data Science and Statistics Blogs worth reading ,
,
,  "
"
, is today the, open source project in the Big Data ecosystem ?€? with over 300 contributors in the past 12 months. Spark is available through,, including all of the major Hadoop distributors, with a rapidly growing volume of production use cases. Design patterns are emerging that leverage Spark together with other popular frameworks, such as,.
,
There are , recent important pieces of news about Spark...
,
, Databricks and O?€?Reilly Media have partnered to launch Developer Certification for Apache Spark, ?€? a new program that leverages Databricks?€? Spark experts and the O?€?Reilly Media editorial team. The formal exam (90 min) and subsequent certification establish the industry standard for measuring and validating technical expertise in Spark.
,
The first certification exam will be held next month at,. Get certified as a Spark Developer so you can demonstrate recognized validation for your expertise, and be a part of the Spark developers community building the next-generation of Big Data applications.
,
, To stay up to date with the latest advances and training in Spark, and help prepare for the certification exam, Databricks is offering, worldwide. The program in major cities throughout the US was launched in April of this year, and is now expanding into EU and APAC. Upcoming EU training for Spark will be held in London, Paris, and Barcelona:
,
, ?? ,
The Introduction to Apache Spark workshop is for developers to learn the core Spark APIs. This course features hands-on technical exercises to get developers up to speed using Spark for data exploration, analysis, and building Big Data applications. Course materials are authored by Databricks, and the instructors for these workshops are trained and certified by Databricks.
,
, Databricks has initiated a program of partnering with major universities to host, events ?€? multi-day programs open to a mix of industry and academic audience. These offer deep-dives into the latest research related to Spark, with more academic material included and featuring speakers from the core Spark development team. A recent tutorial hosted by, was a big success, and the upcoming tutorial at, sold out almost immediately.
,
Databricks would like to partner with other universities for similar tutorial events, particularly those which have strong programs in distributed systems, functional programming, data science, and machine learning ?€? or other academic areas that leverage Spark, such as genomics and physical sciences. Please contact: ,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
A worldwide leader automotive company, faced a daunting challenge for its After Sales Service business and more specially for its Authorized repairers: ""How to reduce risk exposure, better serve and market to its customers""?
,
In EU economic crisis had a negative impact for after sales market, a market that was already in a transition between a promising potential and a difficult reality as client?€?s cant perceive easily maintenance as product that brings a tangible value, because new cars are built to last longer and require maintenance less frequently and due to new regulations of European Commission (The new competition law 2010).
,
In this context Life Time Cycle of each client is critical and specially Churn as it affects the length of the service period and, hence, future profit generation.
,
,
,
Based on market actual situation and historical data?? , we defined our prediction objective as follow:
,
,
,
,
,
,
,
,
Data analysis is a vital component in strategic planning for companies that are aware of worldwide competition, ever-shorter production cycles and increasing customer requirements. Due to actual speed of communication through internet of things it is important to identify meaningful patterns quickly within the collected data.
,
DIRECTING mission is the design of knowledge architectural plan as part of business engineering in each enterprise and the creation of data mining applications offering to users that not necessarily have a statistical background to assess and understand the identified patterns.
,
This has been accomplished by the initial concept of DATACTIF??, a Data Mining Platform able to generate concept-applications tailor made for each enterprise needs and in same time bringing a 15 years experience of learning processes, accumulating knowledge and finally finding solutions to problems in industrial, financial, retail sectors.
,
DATACTIF?? uses machine learning methodology and algorithms such as neural network, Kohonen SOM with U-Matrix visualization, fuzzy systems, genetic algorithms, Support Vector Machines, etc?€? and contains visualization methods that allows a global view on the domain that is under analysis, and an analytical view to all details offered by existing data.
,
,
,
Predictive modeling is used to forecast a particular event. It assumes that an analyst has a specific question ask. The model provides the answer by assigning ranks, which determines the likelihood of certain classes. ??In our case that the Automotive Company to predict which customers are likely to stop maintenance or not, it have to prepare for predictive modeling by feeding data about two types of customers into the data mining tool: data of customers who have stopped and data of customers who have continued. We applied predictive modeling to a sample of 500.000 customers with their maintenance historical data concerning the period from 2009 to end 2013.
,
We used also January 2014 data for one training set and for verification of our models real visits for maintenance between 1/2/2014 and 31/7/2014
,
In more detail, the training data consists of the state x that describes certain instances of the problem and the desired response.
,
In the case of cars <4 years we selected randomly 3.000 customers of our data base as training set of whom 652 made a maintenance service on January 2014 and the rest (2.348) they did not.
,
In the case of cars >4 years and <7, we defined churn customers as those who have not made any visit for service in the past three years (2011, 2102, 2013) and not-churn customers who made service every year over the past three years and that combined with January 2014 results. We selected randomly 5.000 customers as training set. We used as input variables:
,
,
,
After the training phase our model predicts the parameter values for new cases (not included in the training data). We used the polynomial kernel with , (SVMs) that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.
,
,
,
2014 Churn Prediction Results
,
,
Average accuracy: 76.8%
,
,
,
This is part 1 - here is 
,.
,
,is the principal at ,, based in Athens Greece, which creates Business Driven Analytics, applications of machine learning theory,?? tailored to specific industries and in each sector of industries to specific needs and business requirements of every company.
,
,
,
,  "
"
,
,
This is part 2 of the post - here is
,.
,
,
,
The need of a macroscopic point of view allowing us to see the relation between clients and maintenance evolution through years and the need to ?€?visualize?€? this relation made us decided to use neural networks and self organizing map. As data and input variables we used the same as in the prediction model.
,
A self-organizing map consists of components called nodes or neurons. Associated with each node is a weight vector of the same dimension as the input data vectors and a position in the map space.
,
There is a visualization technique called the U-matrix or unified distance matrix that visualizes the distance between adjacent units in the SOM. It represents the map as a regular grid of neurons as illustrated in ,.
,
,
,
In order to interpret the map, and in particular the characteristics of each cluster, we used the component planes of the map that show the distribution of values across the map, according to one variable at a time ,
,
Based on features values of each cluster and on a clusters similitude?€?s analysis we observed that 4 Hyper Clusters are formed , Hyper Cluster A : clusters 4, 5, 10, 15, Hyper Cluster B : clusters 1, 2, 6, Hyper Cluster C : clusters 16, 17, 21, 22, Hyper Cluster D : clusters 19, 20, 23, 24, 25.
,
,
,
,
,
Based on real visits of cars who made maintenance service between 1/2/2014 and 31/7/2014 we observe that Hyper Cluster D is more important for car ages till 6 years, Hyper Cluster C for car ages between 6 and 10 and Hyper Cluster B for car ages over 10 years (,
,
,
,
Considering past years history we observe that from 2009 to 2014 there is a gradual movement from Hyper Cluster A to Hyper Clusters C and D, this movement allows us to predict that in 2015 we will have Hyper Cluster D as the only dominant. So Hyper Clusters allows us a macroscopic point of view of Life Time Cycle evolution through time (,
,
,
,
Now concerning cars in warranty we observe a dominance of Hyper Cluster D independently of models (,and for cars out of warranty we have a dominance of Hyper Cluster D for the Model_6 as it is the news of all and for the rest we have : Models 8, 1, 5, 4, 10 dominant Hyper Clusters are B and C, Models 2, 7 and 9, Hyper Clusters C and Model 3, Hyper Clusters B and D (,
,
,
,
,
,
,
,is the principal at ,, based in Athens Greece, which creates Business Driven Analytics, applications of machine learning theory,?? tailored to specific industries and in each sector of industries to specific needs and business requirements of every company.
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Sep 30 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "

"
Most popular 
, tweets for Sep 26-28 were
,
Any data scientist worth their salary will say you should start with a question, NOT the data, says 
, 
,
,
Any data scientist worth their salary will say you should start with a question, NOT the data, says 
, 
,
,
CNN embarrassing lack of ""Data Quality"" - this #Scotland Independence poll adds up to 110% ,
,
,
Top KDnuggets tweets, Sep 24-25: Statistical & Machine learning with R, great 15 hour online course ,
,
,  "
"
,
,
There has been a lot of talk about the Internet of Things lately, especially since the purchase of Nest by Google, officially opening the run towards 
,. Companies working in this field have multiplied over the last few years, like SmartThings, later 
,. However, it is not only households that can be intelligently interconnected: cities ?€? i.e. smart cities ?€? represent another area for the application of the Internet of Things. One of the first smart cities around the world has been Santander in the north of Spain (,). Sensors have been installed around the city to constantly monitor temperature, traffic, weather conditions, and parking facilities.
,
The Internet of Things poses a great challenge for data analysts, on the one hand because of the very large amounts of data created over time and on the other because of the algorithms that make the sensor-equipped object (house, or city) capable of learning and therefore smarter.
,
We decided to take up this challenge and put KNIME (,) to work to:
,
,
,
As an example of an Internet of Things application, we need a use case that is easy for everybody to understand and with publicly available data to reproduce. After some search, we found a bike share system in Washington DC called Capital Bikeshare (,)
,
,
,
Here is an example of sub-workflow that submits the requests to the Google API RESTful service is shown in Figure 5 below.?? This sub-workflow consists of two parts: a loop posting a request and collecting the response for each data row and a metanode, named ?€?extract?€?, also containing a loop to interpret all received responses
,
,
The results of this work, together with white paper, data, and workflows, have been made available in the KNIME white papers pool at ,.
,
In this kind of cutting edge problems, where a very large amount of data is generated, it is imperative to adopt a scalable approach that can grow together with the application. A scalable approach means not only handling bigger data faster, but also reaching out to new external data sources, integrating different complementary tools to refine the analytics with the newest emerging algorithms and techniques, and collaborating within the analyst team to exploit the group?€?s collective competence. Only an , can provide such a flexible environment, to expand and adapt the tool bench in unpredictable ways (,).
,
The Internet of Things is a very good example of the data explosion that is occurring in most fields, from social media to sensor-driven processes. But how much information can more data actually convey? This is of course highly dependable on the amount of intelligence we apply to it. Pure data plumbing and systematization do not generally produce more intelligent applications. Only the injection of , algorithms from statistics and machine learning, can make applications capable of learning and therefore ?€? smarter.
,
, has been a researcher in applications of Data Mining and Machine Learning for over a decade. Application fields include biomedical systems and data analysis, financial time series (including risk analysis), and automatic speech processing.
,
,
,
,  "
"
,
,
As the data deluge continues it becomes increasingly obvious that we will always need machines to leverage our comprehension of a confused and confusing world. While steady progress is being made on supervised learning techniques for structured data the same cannot be said for unstructured text analysis, which is primarily restricted to the reduction of text to structured data, so that supervised techniques can be applied. This represents a major challenge to data miners, because unstructured text constitutes 70% of all data and it is that 70% where knowledge and ideas can be found.
,
Luckily help is at hand from a rather unexpected quarter, namely Association Rules. Just as it is possible to say that ""people who bought these things also bought that thing"", we can also reason that ""if a document contains these words then it is likely to contain this word"". Readers familiar with such techniques might quake at the combinational problem entailed by natural language, but this can be scaled down by the intelligent reduction of input texts. According to OED just 100 words, such as pronouns and prepositions, account for half of the English corpus, carry little meaning, and can be ignored.
,
However, even if we reduce the input texts by removing those words, two problems remain, how to make the rules quickly with finite hardware resources, and how to select the best when interestingness measures do not always rank rules in the same order.
,
Although FPgrowth algorithms produce trees at fantastic pace, they are optimised towards sequential construction; as a consequence semi-parallel approaches such as the APriori algorithm compare poorly in a conventional hardware environment. That's unfortunate because the flow of the APriori algorithm is ideally placed to generate and score Association Rules on the fly, as it carries the context counts that drive all measures of importance. These are also the numbers that can be ""Iteratively Proportionally Fitted"" so that all measures will rank the rules in the same order. Parallel hardware might therefore solve both the speed and grading problems.
,
Step forward the Nvidia GPU beloved by all gamers. Currently powering the second fastest computer on the planet it also comes complete with its own language and libraries to ease the programming task.
,
Using these offerings a prototype news analyser has emerged at
,
,, which extracts the most significant themes from newsfeeds 24/7.
,
It addresses the memory space problem by progressively reducing the frequency threshold until the top rules altogether contain a given number of words. This is made feasible by the speed of the GPU, a parallel version of the APriori algorithm, and Iterative Proportional Fitting to select the top rules on the fly.
Here are some examples of raw news text
,
,
,
The graph generated from top stories
,
,
,
and the generated rules
,
,
,
The commercial advantages of a bias-free comprehensible knowledge extractor are not difficult to see, from News to Patents to Medical Science, the list is as long as human enterprise. The author has a long standing and productive relationship with Ingo Mierswa at RapidMiner, and is currently working to provide this capability as an extension to that software.
,
Visitors to , can check out the Accumulator, a Neo4J database into which all rule observations are aggregated, and which can be interrogated via Prolog; this represents a second strand to the work, and one which hums the Big Data anthem, that noise cancels itself out.
,
We have a self-improving oracle growing out there, and one that you can question right now. Enjoy!
,
,  read Classics at Oxford, majoring in Philosophy and Logic. For the last twenty five years he has pursued the notion of Meaning, both in Economic patterns and free form text.
,
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "








"
,
,
As organizations have developed the capacity to gain greater insight into customer behavior, it is essential to use innovative analytics practices to succeed. For large and small retailers, it is essential to be able to not only react to, but also accurately predict the trends and nuances in the market.
,
,

, (June 19 & 20, 2014) was organized by the Innovation Enterprise at Chicago. ??Illustrated intermittently with case studies, interactive panel sessions and deep-dive discussions, this summit offered solutions and insight from the leaders operating in the Big Data space.
,
We provide here a summary of selected talks along with the key takeaways.

,

,.
,
Here are highlights from Day 2 (Friday, June 20, 2014):

,

, explained the role of location intelligence in marketing analytics during his talk ""Location Intelligence: The Missing Component in Retail?€?s Use of Data Analytics"". Location intelligence brings the ""Where"" factor to performance analysis and enables a deeper analysis of business information; all designed to assist companies in reaching conclusions and making the next, best business decision.?? He outlined the applications of location intelligence to improve data analytics. 
,
By harnessing information already within Business Intelligence (BI), Customer Relationship Management (CRM), Supply Chain Management (SCM), and other enterprise applications, location intelligence enables companies to integrate location dimensions with performance management, reporting, predictive analytics and campaign management solutions to increase sales and optimize marketing operations. Location Intelligence assists companies in making complex location-dependent business decisions by representing and displaying business intelligence data on maps to identify hidden trends, customer relationships and behaviors, as well as enable a deeper marketing analysis.

,

Consumer's purchase decisions today are being influenced by a wide variety of factors, which makes marketing extremely difficult and complex. Even though marketers need to study a lot of factors such as Price, Weather, Distribution, Promoted Volume, Radio, TV, Print, Seasonality, and Macro Trends; in order to succeed they must have a simple view of the world through conceptual frameworks to decompose the effects. It is easy to study these factors separately, however, in real life, they all act simultaneously. Decoding this simultaneous effect is critical to better understand how any investment to drive sales is actually performing. Understanding the full picture is vital to have an insights driven supply chain.

,

Shoppers have many choices. Sale occurs only at the conjunction of availability, right relevant message and value proposition. All consumer interactions are important. Thus, they must be measured and included in the Analytics for marketing decisions. Location intelligence empowers retailers to optimize Site Selection and Store Performance.

,

, gave an interesting talk on ""Architecting an Intelligent Organization"". Getting the right information to the right person in the right way to make the right decision. Seems logical, right??? But ?€?insight logistics?€? is much easier in theory than it is in practice. In adopting the technologies to accommodate new forms of data and analytics, we?€?ve unconsciously skipped over foundational elements required to make analysis meaningful for decision-makers.

,

Through simple examples, Jason illustrated the imperatives for architecting today?€?s intelligent organisation and the cultural nuances to make it relevant. An intelligent organization is one that is able to successfully implement the right balance between Performance (quickness & accuracy) and Innovation (learning & experimentation).

,

In order to succeed, business managers need to think about:
,
,??,

Companies need to be very careful while making business decision such as whether to build or buy analytics solutions, and whether to build their data architecture as centralized or decentralized. He concluded his talk with the following quote:

,

,

,

, delivered an informative talk on ""Content, Context, Community, and Clarks"". With the emergence of Omni Channel it is more and more imperative that data is distributed and consumed correctly. With so many moving parts to the puzzle a business must be ready to adapt and be ready for when the next challenge arrives.?? Adapting an agile approach to reporting and to the use of data, Clarks is smartly using web analytics to shape the future of the Business and the Brand.

,

Mark revised the Analytics at Clarks, driving it towards understandable, actionable analytics through rigorous A/B testing, continuous measurement of not just conversions but the entire consumer experience. He stressed on the need of simple pilot tests, intuitive visualization and getting business buy-in.

,

Based on his experience, he strongly recommended the marketers to focus on Content, Context, and Community.

,

,gave an insightful talk on ""Advanced Text Mining of Online Product Reviews for Better Visibility of Merchandise Focused Customer Sentiments"". All online retailers gather valuable information from their customers in the form of Product Reviews online. This information is collected using review form on the website and through post purchase survey emails which are sent to the customers. We can leverage this data to improve the product selection process and improve the overall merchandise quality. However majority of the data is in unstructured text format, which is fundamentally different from quantitative score data. We need to perform advanced text mining to derive these sentimental insights.

,

Through a live example, Murtaza demonstrated how online retailers can harvest this qualitative information. His team created a sophisticated methodology to measure customer sentiments across a retailer's product assortment, in order to inform the business: what was going wrong! They categorized product reviews across parameters such as Fit, Delivery, Durability, Price and General Comments. Text mining was done using RapidMiner and text string clusters were created for deeper analysis.

,

Sentiment trending across top categories helped pin point why certain categories were losing revenue share over the years. The overall category sentiment score cards helped rate merchandising effectiveness. Besides, sentiment mining was also used to learn the language customers were using to describe their sentiments. The sentiment describing keywords were then used to describe products in all outbound marketing communications. This helped with building customer relevance in messaging and brand damage controls when needed.

,
,
,  "
"
,
,
Here is the company, startup, and acquisition activity for September 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are tweets from KDnuggets (with #BigDataCo removed).  I was on vacation and not tweeting Sep 5-15.
,
,??,
Here are previous months activities:
,  "
"
By Frank Pauw, Develve, Oct 2014.
,
, Statistical software Develve 2.00 is out and with the new function ,, for measurement system analysis. And is becoming a full featured Six Sigma tool.
,
,
,
Between version 1.00 and 2.00 various function are added to make the tool more complete.,
For instance:
,
,
The aim of Develve is to make a easy tool suitable for less experience users but advanced enough for more demanding users. When applying a statistical test Develve tries to look if all the boundaries are met. For instance when applying a t-test Develve also checks the sample size and the shape of the distribution.
,
Beside The new Gauge R&R mode Develve is having two other modes. Switching between these modes changes the layout to fit characteristics of the statistical mode.
,
,: If there is a difference or relation between datasets,?? shape of a dataset and if it is within specification limits.
,
,: DOE test setup with more factors on different levels in a balanced matrix. To analyzing complex problems with a more of influence factors in once. When editing a DOE matrix the program checks if it is in balance.
Develve is free for non commercial use!
,
Get it at ,.
,
,
,
,  "

"
        ,Here is an excellent, very comprehensive, and entertaining overview and history of  deep learning and recurrent neural network, given at Berkeley in August 2014 by Professor 
, of IDSIA, Switzerland.
,
,
,Machine learning and pattern recognition are currently being revolutionized by ""Deep Learning"" (DL) Neural Networks (NNs). This is of commercial interest (for example, Google spent over $400 million on the start-up ""DeepMind,"" co-founded by our student). My talk will summarize our work on DL since 1991. Our recurrent NNs (RNNs) were the first to win official international competitions in pattern recognition and machine learning; our team has won more such contests than any other research group or company. In particular, our RNNs represent the state of the art in connected handwriting recognition, and aspects of speech recognition. We also built the first artificial RNN-based agent that learns from scratch complex control based on high-dimensional vision.
,
,
,
More information about the talk:
,
,??,
,
,
Deep Learning survey with over 850 references:
,
,??,
,
Since age 15 or so, Professor Jurgen Schmidhuber's main scientific ambition has been to build an optimal scientist through self-improving AI, then retire. His AI team has won nine international competitions in machine learning and pattern recognition and seven independent best paper/best video awards, achieved the world's first superhuman visual classification results, and established the field of mathematically rigorous universal AI and optimal universal problem solvers. His formal theory of creativity and curiosity and fun explains art, science, music, and humor. 
,
He also generalized algorithmic information theory and the many-worlds theory of physics, and introduced the concept of Low-Complexity Art, the information age's extreme form of minimal art. Many famous companies are now using the techniques developed in his group at the Swiss AI Lab IDSIA (a Business Week Top 10 AI Lab) and USI & SUPSI. Since 2009, he has been a member of the European Academy of Sciences and Arts. He has published more than 300 peer-reviewed papers, and is recipient of the 2013 Helmholtz Award of the International Neural Networks Society. Home page: ,
,
,
,
,  "
"
,, by Paulo Cortez, Springer, 2014
,
,
,
The goal of this book is to gather in a single document the most relevant concepts related to modern optimization methods, showing how such concepts and methods can be addressed using the open source, multi-platform R tool. Modern optimization methods, also known as metaheuristics, are particularly useful for solving complex problems for which no specialized optimization algorithm has been developed. These methods often yield high quality solutions with a more reasonable use of computational resources (e.g. memory and processing effort).
,
Examples of popular modern methods discussed in this book are: simulated annealing; tabu search; genetic algorithms; differential evolution; and particle swarm optimization. This book is suitable for undergraduate and graduate students in Computer Science, Information Technology, and related areas, as well as data analysts interested in exploring modern optimization methods using R.
,
Keywords: R statistical tool - evolutionary computation - metaheuristics - particle swarm optimization - simulated annealing - tabu search
,
The R source code is available at:
,
,
,
,
,  "
"
,
,
I?€?m currently active in IT. At a certain moment I was curious as to what people of my age are earning so I could compare.?? The easiest way is to go and check on standard job sites or google it. Still not all data is released there, only some figures. And most of all there is no fun in doing this. Luckily we have message boards where users can anonymously post their salaries and bunch of other interesting data. And luckily we have some easy tools to collect, refine and plot this data thus enabling us to do our own research.
,
In the picture below you find an example of a post on the message board as you can see it?€?s divided in topics: personal, labour agreement, terms of employment and working conditions. It?€?s interesting to know what one person is making but it would be more interesting to know what the average is, what people earn versus age is, to plot it into a histogram to know the distribution of the salaries. The people posting on this forum are mostly young(er) and they?€?re more active in engineering jobs, so interesting for myself to compare but not representative for the entire population. Below an example of the template people could fill in(in dutch).
,
,
,
Template post
,
The techniques and tools that are described below require minimal technical knowledge and almost no programming skills. This was my first time using any of the mentioned tools and they were easy to learn.
,
,
,
To collect the data I?€?ve used import.io. Import.io enables you to turn any website into a table of data or an API with no coding required. Import.io can be freely downloaded at??,. In the example used here the following steps were taken:
,
1. Create an extractor and start with training the rows, in this case every post is a row. 20 rows per page, this could differ on what your posts/page setting is.
,
,
,
2. Next up is training the columns, this is the time consuming part. Add a column and mark where the data is to be found. Most of the time you?€?ll have to train one column a few times with different rows, so it really learns correctly where to find the particular data.
,
,
,
Click add column
,
,
,
Name your column and click done
,
,
,
Mark the data to be extracted
,
,
,
Repeat for other columns
,
3. You do this for two example pages, train rows and train columns.?? After training two it should be enough.,
,
4. Create a Dataset and select the extractor you created
,
,
,
select dataset
,
5. Profit
,
In the screenshot below we now have our dataset for the first two pages of the thread. On the left you can add more pages from which you want to extract data. There?€?s is a way to automatically paginate through more pages but unfortunately and also understandably this is limited to 10 pages. Adding them manually is what you can do.
,
,
,
Table with the data
,
,
,
Even though they are using templates these posts on the message board are still like free text fields. There are lots of different ways of saying how much you make, if you have a company car or not and so on. So before anything useful can be done with the data we need to ?€?refine?€? it. For this I used OpenRefine, formerly known as GoogleRefine. OpenRefine offers a nice toolset to clean your data. Their website: ,
,
What I did in this example is cleaning the gross and net wages column. OpenRefine offers basic transform actions. It also let?€?s you do custom transforms written yourself with Clojure, Jython or GREL (Google Refine Expression Language).
,
,
,
Basic transforms
,
,
,
Custom transformations
,
OpenRefine has some other great features, for example logging all the actions you did on a dataset and then replaying them on a similar dataset. You can also extend your data with more data to derive more from it. Check http://openrefine.org/ to learn how.
,
,
,
As said in the introduction, now that we have collected the data and cleaned some of it we can get some statistics. Plot.ly will enable you to plot your data in a lot of insightful ways. For this example I got some basics like an histogram showing the distribution of the salaries, salary versus age and if it correlates. As you can see here you can embed your plots into your blog or any website you want. It?€?s interactive and you can click through if you want to have a look at the data or the code itself. No knowledge of programming is required to make plots like these.
,
Distribution of gross salaries:
,
,
,
Gross salary versus age:
,
,
,
Thank you for reading and hopefully you?€?ll use some of it.
,
Content from ,
,
Bio: Stijn Diependaele is an IT Project manager and blogger in Belgium.
,
,
,
,  "
"
,
,
Whether secretly or openly, sports organizations around the world are increasingly getting on board with analytics. Whether it's to identify an edge in performance from players or finding one away from the field of play, using data to inform decisions is becoming essential.

,

, (September 10 & 11, 2014) was organized by the Innovation Enterprise at San Francisco. ??From the bleeding edge of performance enhancement to fan engagement & business analysis, a wide spectrum of topics were discussed. Experts shared their thoughts on how analytics are constantly evolving the way the sports industry operates, in every sense.
,
We provide here a summary of selected talks along with the key takeaways.
,
,.
,
Here are highlights from Day 2 (Thursday, September 11, 2014):
,
, gave an exciting talk on ""Using Analytics to Define the Ticketing Experience"". Data and analytics have become the new buzz word in ticketing.?? Dynamic pricing took all the data and produced information that allowed us to properly price our tickets.?? Now our analytical team is providing us with recommendations to better run our business. We rely on this information for promo dates, special events, season ticket, risk assessment and keeping the sell-out streak alive.
,

Based on his over 25 years of experience with San Francisco Giants, he remarked that it has been astonishing to see the change from a world where decisions were predominantly made by gut instinct to today's Analytics era - where there is number crunching behind even the small decisions. Today, San Francisco Giants' analytics architecture comprises of a data warehouse, Salesforce customer management tools and Tableau reporting.

,

SF Giants has been doing dynamic pricing for about 6-7 years. Currently, they use Qcue as the front-end management tool for pricing decisions. There has been substantial benefits of modifying price dynamically based on real-time monitoring of demand and several factors that impact demand. Sometimes, the urge to continue the sell-out streak obstructs revenue maximization through dynamic pricing. Sell-out streak has some reputation factor, and thus, one needs to be careful about trading it off for greater revenue.

,

All games are not created equal. Real-time monitoring of demand can help determine the effective ways to add value to soft games. The increased price from big games allows the firm to have softer prices on smaller games. Monitoring and measuring the effectiveness of promotions and special events can deliver unprecedented insights to make the most of marketing dollars. Analytics plays a key role in improving fan engagement, which is a top strategic priority, to maximize season ticket retention.

,

, delivered a highly informative talk on ""NBA Optical Analytics in the Real World"". He shared his half a decade of hands-on experience with the SportVu data to present practical insights on how best to extract value. The four main cross-dependent components are: asking the right questions, computing the answer, presenting the results, and automating the process. He also talked about the tools, programming languages, and skills are needed to store and compute the data. It's important that projects be simultaneously quickly prototyped and yet managed to build an extensible system with real value for the organization rather than a collection of one-off results.

,

Talking about the current scenario, he described the hierarchy of optical adoption. The zero level comprises of the standard public reports which includes data on speed, distance, touches, passes, drives, catch-and-shoot and pull-up. The first level comprises of the standard team-only reporting. This is very basic level of reporting, and mostly, the teams are stuck here since it is hard to get started on deploying analytics effectively. The second level comprises of cases where custom one-off reports are being generated through external contracts. The third level, the ideal state, comprises of custom in-house tools and reports, nightly runs, entire data and analytics system incorporate all traditional analytics as well, live visual interactive tools, cloud-deployed, extensible and production-level system.

,

Talking about the size of data, he mentioned that SportVu data per game is 40 MB and per season it is 50 GB. These numbers might seem drastically low compared to the data processing at Walmart or Facebook. So, the data at hand is not ""big data"". A better approach is to treat SportVu data as binary data, and not as relational data.

,

There are a lot of questions that cannot be answered satisfactorily without optical analytics. Number crunching can provide deep insights to answer the questions on player evaluation, coaching strategy, shot selection, team chemistry, etc.

,

,shared his insights on baseball injuries in his talk ""Professional Baseball Injury Analytics"". Baseball has a rich history of statistics and with the advent of Sabermetrics, the world of baseball performance analytics has exploded.?? However, research and analytics of injuries falls far behind even compared to other professional sports.?? He discussed the epidemiology of baseball injuries, the trends, patterns and costs of injuries in baseball. Using the example of specific injury trends such as Tommy John Surgery, he showed where injury analytics presently stand and what can be done in the future.

,

Injury analytics plays a very important role in structuring prevention programs, monitoring outcomes and assessing risk. In 1996, it took him 2.5 years to get the data on past injuries. The only reliable source from information was the disabled list, which is a roster management tool, and is used to replace injured players. Even today, we lack clean data on injuries in baseball.

,

His paper published in American Orthopedic Society of Sports Medicine in 2001, revealed that the injuries had increased sharply from 1989 to 1999. The MLB Injury Surveillance System was established in 2010 by MLB. It is an event based system that does data collection and reporting. Over 180 people enter data every day, which is a good news (greater quantity of data) as well as bad news (poor quality and reliability of data). This has enabled a lot of data analysis such as severity of injury vs days lost, pitcher injury rates, average injury rate by position, etc. Today, this system enables rigorous medical risk assessment based on factors such as medical history, demographics, biometrics, performance metrics, and usage. 
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
Whether secretly or openly, sports organizations around the world are increasingly getting on board with analytics. Whether it's to identify an edge in performance from players or finding one away from the field of play, using data to inform decisions is becoming essential.

,

, (September 10 & 11, 2014) was organized by the Innovation Enterprise at San Francisco. ??From the bleeding edge of performance enhancement to fan engagement & business analysis, a wide spectrum of topics were discussed. Experts shared their thoughts on how analytics are constantly evolving the way the sports industry operates, in every sense.
,
We provide here a summary of selected talks along with the key takeaways.
,
Here are highlights from Day 1 (Wednesday, September 10, 2014):
,

, described the role of Analytics in Football through his talk ""Implementation of Analytics: What Actually Benefits Teams"". With 22 players involved and the specific goals of each play not always defined, football provides a challenge for analytics. The NFL anti-technology rule prevents the use of computers during games so that the best athletes and coaches, not the best software developers, determine the outcome. Decisions such as when to use a time-out, punt, try a two-point conversion, spike, kick a field goal on 1st down or kneel are the aspects of the game which can be measured and defined. These are also areas where a game or two each week is won or lost.

,

Through an interesting role playing example, he demonstrated what kind of Analytics go into making the crucial decisions in the game in order to maximize the chances of winning. Coaches often need to decide within seconds whether to go as per the advice from Analytics guy or go by their own instinct. No matter how promising Analytics might seem, if it does not work, it can easily get them fired.
,
Talking about the history of analytics in the NFL, he mentioned that the original draft trade value chart was a retrospective analysis of what had happened so far, and had nothing to do with strategy (or what gives you an optimal trade). More than 80% teams still use it today (though they would keep it secretive), even though the numbers on paper do not necessarily work well on the ground.
,
Analytics can be very useful to coaches for several purposes, such as Offense can have more refined goals for a given down/distance, and Defense can focus on decreasing the opponent?€?s chances of winning. However, there are also things where Analytics can hardly offer any advice to the coach, such as: besides gaining yards, what were the intentions of the play call?
,
, gave a thought-provoking talk on ""Using Data Correctly - How Do You Tell the Team?"". 5 years ago, we were focusing on data collection tools and methodology. Today, this process is seamless; today, it is extremely easy to get hold of vital, physiological and activity data. Our present challenge is really about the system that holds the data together and to answer a very simple question: how might we help our clients/partners benefit from reliable and easy-to-understand information. Within this context, human-centered design and lean development principles can prove to be major assets. Mounir shared some stories from the US Olympic teams inspired by these assets.

,
Talking about future, he described ""tactile internet"" which will improve internet reaction time from 25 millisecond to 1 millisecond. This will make information transfer a commodity, and the innovation will be based on real-time execution of the learning from Analytics. 21st century will observe ""Industrial Internet Revolution"", where minds and machines operate collaboratively in a seamless manner.
,
Analyzing the performance of athletes in standardized environments is something of the past. Today, we need to capture performance in the natural field of play. Wearable technology is making an unprecedented impact on the field of sports.
,
To advance sports, Analytics must:
,
, ?? ,

, shared her experience of applying Analytics to Soccer in her talk ""Managing the 'Art' & 'Science' of Data Analytics in the English Premier League"". The expansion of data collection and Sport Science in English football over the past few years has been unstoppable.?? The challenge to those working in it is to manage, interpret and communicate this information to colleagues in a valuable manner, whilst balancing with the ?€?art?€? of football. Jo presented case studies of some successful (and less successful) applications of data in this high pressure and unpredictable environment from the past five years and discussed the lessons that have been learnt.

,

As sports science has grown in influence, there is increased tension between managers and their backroom staff. Sports scientists want to dictate, believing that their data should be unquestionable. Analytics also needs to be used carefully, because improper or excessive use may stifle the enthusiasm of players. Performance in Soccer comprises of a lot of factors such as physical, tactical, emotional, psychological, technical and circumstances. The art and science of analytics impacts several aspects of the game including training analysis, fitness fatigue, physical profiling, youth development and injury rehabilitation.

,

She emphasized on the contextual, environmental and cultural factors that need to be considered along with Analytics, in order to drive towards the desired results. She concluded her talk saying that data can add value, but soft skills are as important as data itself.
,
,
,
,
,
  "
"
,
,
Technology (think big data) presumes that everything is in some form of indecipherable encrypted code that by itself means nothing unless it can be magically transformed by technology. In a sense and from a technology perspective the aforementioned statement is true. It is not true that everything is in some form of indecipherable encrypted code, but rather since technology does not truly understand the methods by which people communicate, to technology it is indecipherable in its native form.
,
What this means is that technology (read technologists) must manipulate content into a form suitable for technology to process. This abstraction is not so much intended to achieve a greater understanding of the subtlety and nuance of what is contained, but to make it manageable for processing. This is what I describe as the big data version of the scientific method, where you define the result and configure the experiment to produce it. But the desired product defines the process and the process defines the outcome produced.
,
The missing element in all of this algorithmic and statistical logic is at the very essence of how we communicate, that is context. Big data produces great insight when data is involved, but data is the output of a prior activity or operation as data does not exist in nature. Human communication does exist in nature as does human emotion, and as humanity has evolved so has the form, style, and locution of how we express ourselves. Big data is perceived as being the next Esperanto, a modern Lingua Franca, the Rosetta Stone of this phase of our continual sprint toward the next big innovation. But despite the illumination cast from this most shiniest of objects, when it comes to language big data's capability remains immature.
,
,
,
It seems completely incongruent that something virtually everyone is so familiar seems absent from the contemporary strategy and dialog. Soon after we can lift our head as an infant we learn to discern the emotive expressions of those around us. This expression defines the initial context of our immediate environment. Later when we begin to expand our communication skills and learn to read we are offered pictures and illustrations so that we may better understand the content. As our vocabulary expands the amount of visual support diminishes. Why, because we can now discern context from language. It is all about context, news, opinion, education, literature, all function because of context.
,
Context is the basis of comprehension and understanding, which is perhaps the reason why I find the application of big data where human communication and expression are concerned so perplexing. Sadly virtually all technology is illiterate and lacks the capacity to comprehend, it's the same condition that encumbers efforts in AI and machine learning. In language a great deal of information can be conveyed in one or two words, we disambiguate to be consistent with context. It's what we think of when we consider the meaning of a word.
,
Take ""School"" for example. Is the context related to fish or education? A school of fish connotes a body of water, perhaps multiple of fish moving in unison, whereas school in an educational context might include a building, desks, students, teachers, and the like. This semantic reasoning is something we do through our waking lives, but systems have not achieved the ability to be capable of understanding context.
,
Finally I often ask, people I encounter when discussing the importance of context, have you ever received an email from someone who was obviously upset but did not explicitly state that they were? Of course you did, that's context.
,
,
Bio: Ron Ezsak is Director for Expert System (www.expertsystem.com). A software industry veteran, Mr. Ezsak has more than 30 years of experience, including deep domain expertise in global industrial, financial, and commercial sectors. He is based at the company's corporate headquarters in the Chicago area.
,
,
,
,
,
,
,
,
,
,  "
"
Most popular 
, tweets for Oct 08-09 were
,
IBM #Watson presentation: Clinical data determines only 10% of health; Genomic: 30%, Rest (Behavior, economic) ~ 60% ,
,
,
Data analysis (with R) the data.table way is faster: introducing DataCamp newest course #rstats ,
,
Big Email Mistake even the experts make: leaving the subject line to the last moment - need to start with it #SEO  ,
,
A Kaggle hero 100-line #Python code for online logistic regression + 2 smart tricks that beats the Criteo benchmark ,
,
,  "
"
,
,
,
,Semantic similarity of arbitrary pairs of lexical items, from word senses to texts!
,
The Linguistic Computing Laboratory of the Sapienza University of Rome is pleased to announce the first release of ADW.
,
ADW is a software for measuring semantic similarity of arbitrary pairs of lexical items, from word senses to texts. The software is based on ""Align, Disambiguate, and Walk"" [1], a WordNet-based state-of-the-art semantic similarity approach presented in ACL 2013.
,
Features in ADW:
,
,??,
To obtain the software, please visit ADW's github repository at:
,
,
,
You can also try ADW's online demo at:
,
,
,
Reference
,
[1] M. T. Pilehvar, D. Jurgens and R. Navigli. Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity. Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 4-9, 2013, pp. 1341-1351.
,
,
I tried the online demo, and 
the semantic similarity of
,
,??,
so the computed values have to be used for relative comparison, not in an absolute sense.
,
,
,  "
"
Most popular 
, tweets for Oct 10-12 were
,
7 Most Data Rich Companies in the World: GE, IBM, Amazon, Facebook, Google, Cloudera, Kaggle #BigData ,
,
R and #DataScience Webinar slides - status, why, code examples #rstats ,
,
7 Most Data Rich Companies in the World: GE, IBM, Amazon, Facebook, Google, Cloudera, Kaggle #BigData ,
,
Another list of 200+ #BigData thought leaders to follow on Twitter ,
,
,  "
"
,
By Gregory Piatetsky,  
,, Oct 13, 2014.
,
,
I was recently asked about including a 
, in a book, and this prompted me to update this poll. 
,
Methodologies like 
, and 
, (from SAS) seemed to decline in visibility, perhaps because they are not as well adapted to Big Data. CRISP-DM was a prominent part of early versions of SPSS Clementine, but I don't know how prominent it is in current versions of IBM Modeler.
,
Google Trends can barely find enough data for CRISP-DM, and Google Trends for SEMMA are not for SAS methodology but for results from Latin America (since ""Semma"" is a popular shopping center in Brazil and something else in Dominican Republic) .
,
This poll is over - here are the results of KDnuggets Poll:
,
,
Additional relevant links for methodologies:
,
,
,
 ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Oct 14 and later.
,
See full schedule at , .
,
,  "
"
        ,  "
"
,
More and more we hear that analytics shouldn't be done just for the sake of ""beautiful predictive models"". Data scientists' time is expensive, it should be used in the most productive manner to help answer important questions and help your business grow. But how do we achieve it?
,
The 
,
(San Francisco, November 4-5) opens with a session which drills down into driving business value with text analytics.
,
Text analytics practitioners from LinkedIn and Toyota and the author of the book Data Mining for Dummies will cover:
,
,??,
This is just one part of an in-depth two day agenda dedicated to the growing importance of text analytics. Check the full conference agenda to see 
,
,
Use code , to receive the exclusive $100 discount if you register 
,  "
"
,
,
By Richard Boire
,
Oct 2014.
,ISBN-13: 978-1137406170
,ISBN-10: 1137406178
,
In today?€?s business and marketing worlds, there?€?s big talk about big data.
,
As companies?€? capacities to amass information continue to grow and improve, the process of
mining the data has become more and more vital.
,
All are abuzz about data mining?€?s importance and potential?€?and for good reason?€?but the field
is still in its infancy, and there?€?s an urgent need to spread and grow the skills, know-how, and
strategies to best optimize data mining?€?s results.
,
In ,, industry veteran Richard Boire provides streamlined insights and techniques for making the most of
the masses of information and mining techniques that technology has enabled. Chock-full of
engaging stories and case studies involving some of the world?€?s top companies,
,sets itself apart in more ways than one.
,
A guide for business managers who need to understand the concepts of data mining
as well as the potential it has for providing strategic guidance, Boire delivers a uniquely
simple, 4-step process for identifying when data mining is the appropriate tool and then designing,
implementing, and measuring your mining.
,
Through hands-on analysis of best practices,,demonstrates how
to interpret your results into actionable learning and target your mining to achieve appropriate
business solutions?€?solutions that lead directly to optimized customer ROI and other tangible
results. Boire also takes pains to outline the common pitfalls of data mining and detail
vital approaches for sidestepping them.
,
Among other warnings, he advises managers about the investment in intellectual capital required
for effective data mining, urging them against focusing solely on technology and unenlightened,
de-contextualized numbers analysis.
,
,is a book for marketers, IT professionals, analysts, and anyone
else who wants to ride the revolution of big data?€?not just get swept along by it. It?€?s an
invaluable handbook for those looking to learn more about how to convert data mining into
actionable insights and business solutions.
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
The M.S. in Analytics at the University of San Francisco is an intensive one-year program that provides students with the skills necessary to develop techniques and processes for data-driven decision-making - the key to effective business strategies.
,
,
An innovative, interdisciplinary program offered jointly by the College of Arts and Sciences and the School of Management, the Analytics Program delivers rigorous training in the mathematical and computational techniques of Big Data. It provides mastery of the analytics tied to strategic decisions - and the skills to effectively communicate these results in business settings.
,
You will study 
, such as data mining, machine learning, statistical models, predictive analytics, econometrics, optimization, risk analysis, data visualization, business communication and management science.
,
USF's analytics program is highly technical and students write R and Python programs for most of their courses. Throughout their program, students work with an industrial partner in the practicum.
,
,
,
Practicums are special features of the program that provide you with the professional skills, experiences and networking needed to succeed in a business setting. Each semester, students engage in a project working with an industrial partner (some of which are paid internships). Students have worked with
,,
,,
,,
and 
,,
among others; see 
,  for more information.
,
,
,
The M.S. in Analytics program starts once each year in July (summer admission). The early deadline for admission and scholarship consideration is December 4 and the final deadline is March 1. The online application for summer 2015 admission is now available. 
,
,
,: ,
,415-422-2966
,
,
,
,??,
,
,
Please provide your information 
,
,
,
,  "
"
,
Make-believe costume company, WigWart's Costumes, is launching a new glow in the dark range of costumes in time for Halloween 2014. Susan Scary is the marketing manager who is lists with WigWart's own database and each has data in keen to promote the costumes to Halloween party goers in October through an email campaign, sounds easy right? Unfortunately it isn't. Sue is struggling to combine different formats.  It's driving her crazy!  So Sue needs your help!
,
,
and help Sue by analyzing the data. You could win one of five 
,. Sue acquired an external list from a data provider to enhance her own database for her promotional campaign, but she is experiencing data quality issues, and doesn't know how to solve them. 
,
Your task is simple, fun and will take just 15 minutes to complete. To get started, simply 
,
Then, using the Lavastorm Analytics Engine  import the lists provided, solve a few data quality issues using the step-by-step instructions provided and send in your list to Sue so she can start to promote the new glow in the dark costumes.   
,
Remember, five lucky helpers will receive a , for their effort. So 
, and enter soon, the contest closes on October 30. 
,  "
"
        By Gregory Piatetsky,  
,, Oct 15, 2014.
,
As a media partner for Strata 2014 Conference, KDnuggets is pleased to
bring you a live streaming player for 
,
,, 
,Tools and Techniques that make data Work
,
Oct 15-17, 2014. 
,New York, NY, USA
,
Strata + Hadoop is one of the most important and probably the largest conference on Big Data. 
,
The conference is sold out, but you can watch it on the 
,, on a separate streaming page (,) or down below.
,
Livestreaming begins the second day of the conference, on October 16.
,
,
,
,
,  "
"
Most popular 
, tweets for Oct 13-14 were
,
,  "
"
        ,  "














"
By Sarah Kline, Oct, 2014
,
,
Leading executives and technologists will join together in the second annual ,, , , Nov 3 - 8.?? The events will be held at multiple locations: Thomson Reuters, Cambridge Innovation Center/Venture Caf??, Harvard Business School, hack/reduce, Microsoft NERD Center, DataXu, and Foley Hoag.
,
There will a series of energetic and insightful events, presented by an all-star cast of speakers. The speakers represented are from start-ups and established companies, technologists and entrepreneurs, academia and industry, all driving the big data revolution forward.
,
Over the course of the week, a wide variety of data-centric topics will be covered including: data visualization, data analytics, machine learning, big data, and predictive analytics, with more information at ,.
,
Monday night?€?s Networking/Kickoff event, held at Thomson Reuters, will include two distinguished speakers: Owen Zhang, Chief Product Officer of Data Robot, and Ranked #1 on the Kaggle leader board; and Andy Palmer, Co-Founder of Tamr, working with renowned research database researcher Mike Stonebraker, PhD.
,
During the rest of the week some of the topics include:
,
and more.
,
Sponsors for the Boston Data Festival include: Thomson Reuters, MaxPoint, BigR.io, DataRobot, Booz Allen, VoltDB, and Startup Code Works.
,
Visit , to learn more about the 2014 Boston Data Festival.
,
,
,
,  "
"
By Deepak Agarwal (LinkedIn), Oct 2014.
,
LinkedIn?€?s vision is to create economic opportunity for every member of the global workforce. Facilitating economic empowerment is a big task that will require bold thinking by smart, passionate individuals and groups. Today, we?€?re kicking off an initiative that aims to encourage this type of big thinking: the ,.
,
The LinkedIn Economic Graph Challenge is an idea that emerged from the development of the ,, a digital mapping of the global economy, comprised of a profile for every professional, company, job opportunity, the skills required to obtain those opportunities, every higher education organization, and all the professionally relevant knowledge associated with each of these entities. With these elements in place, we can connect talent with opportunity at massive scale.
,
,
,
We are launching the LinkedIn Economic Graph Challenge to encourage researchers, academics, and data-driven thinkers to propose how they would use data from LinkedIn to solve some of the most challenging economic problems of our times. We invite anyone who is interested to submit your most innovative, ambitious ideas. In return, we will recognize the three strongest proposals for using data from LinkedIn to generate a positive impact on the global economy, and present??the team and/or individual with a $25,000 (USD) research award and the resources to complete their proposed research, with the potential to have it published.
,
If you?€?re interested in ,, here?€?s what you need to know:
,
,
The LinkedIn Economic Graph Challenge has been carefully planned with our member?€?s privacy and security in mind. All access to and usage of select data from LinkedIn will only be granted to the winning teams or individuals, and will be done so under the supervision of a LinkedIn employee collaborator. Additionally, all research award recipients??will be required to undergo security training and work on LinkedIn issued laptops on the LinkedIn network within a monitored sandbox environment.
,
Creating economic opportunity is not something we can do alone. As we continue to scale the Economic Graph, and work with others through initiatives like the LinkedIn Economic Graph Challenge, we can uncover insights to remove barriers to economic opportunity everywhere.
,
We look forward to your submissions! For more information, please visit the ,.
,
,
,
,  "
"
DataMatch, the flagship software suite of 
,, provides deduplication, standardization, and fuzzy matching technology for companies of all sizes. 
,
The study done by Curtin University provided results on the match accuracy of several data sets, spanning 40,000 to 4 million records. The match accuracy exceeded all other software tools, including both IBM's Infosphere QualityStage software as well as SAS Dataflux in all categories.
,
,
,
Tests were completed on data provided by
,, Australia, and were done using Data Ladder's proprietary algorithms (no pre-processed algorithmic results were used.) The tests included:
,
,??,
,
,
With the substantial growth in data and data linkage activities over the last several years, there has been increasing demand for independent assessment of performance and suitability of software tools. Evaluations for record linkage software had previously been based on past experiences of the user rather than on the objective, formal evaluation of available products. By being independently confirmed as the company with the highest match accuracy, 
, is well poised for growth in the future.  "
"
Most popular 
, tweets for Oct 15-16 were
,
GraphLab Create v1.0 brings scalable machine learning, #DeepLearning to #BigData, works with Apache #Spark, #Hadoop ,
,
Watch: R wizard 
, 
dplyr tutorial at useR! 2014 conf #DataScience #rstats ,
,
Strata Hadoop World NYC - Watch Live, Oct 16 ,
,
BAH launches online course Explore Data Science: ""Explore the Art of Turning Data into Actions"" ,
,
,  "


"
By Ryan A. Rossi, Oct 2014.
,
, Exploratory Analysis & Visualization
,
Unlike other data repositories (e.g., UCI ML Data Repository, and SNAP), the network data repository (,) allows users to interactively analyze and visualize such data using our web-based interactive graph analytics platform.
,
Users can in real-time analyze, visualize, compare, and explore data along many different dimensions. Our goal is to make it easy for people to discover key insights into the data extremely fast with little effort while also providing a medium for people to share data, visualizations, and insights. Other key factors that differentiate us from the state-of-the-art repositories is the number of graph datasets, their size, and variety.?? While other data repositories are static, they also do not allow user discussion, feedback, comments, etc.
,
We are striving to incorporate these social and collaborative aspects tightly into our platform. For instance, users can create a profile to share and/or view their recent visualizations, uploaded/constructed graphs, previous graph queries, etc.
,
Ryan A. Rossi is a PhD Fellow (NSF/NDSEG/Andrews) in Computer Science / Machine Learning
at Purdue University,
,
,
,
,
,  "
"
,
While there is great value locked deep in companies?€? textual assets, mining this information can be both time consuming and expensive. These costs often serve as a barrier to entry, preventing companies from capitalizing on the business value inherent in text.
,
I?€?m excited to share Janine Johnson?€?s (ex-Director of Analytics at ISO) presentation 
,
, with you, in which she outlines cost effective ways to unlock value from unstructured data.
,??,??
,
,
This topic is being discussed in more depth at the??, in San Francisco, November 4-5!
,
Use code??,??to receive the exclusive $100 discount if you register??,  "
"
,

,

, (Sep 19-21, 2014) was organized by , at Los Angeles Convention Center. This 3 day event gave a fast paced,??vendor agnostic, technical overview of the Big Data landscape. The participants included a good number of beginners, who had no prior knowledge of databases or programming. The event was targeted towards both technical and non-technical people who wish to understand the emerging world of Big Data, with a specific focus on Hadoop, NoSQL & Machine Learning. Along with the lectures, attendees got first hand practical experience of implementing and using real Hadoop clusters and the latest Hadoop distributions.

,

Here is a brief summary of the class on ?€?Big Data & Hadoop?€?:-
,
The event kicked off with introduction to Big Data. Sujee Maniyam from ElephantScale was the instructor. He mentioned that based on nature of data, data can either be human generated or machine generated. Although we are generating both kinds of data at a rapid pace, machine data has exploded in recent years. Lots of intelligence lies in big-data, such as user behavior. Therefore, all major online companies are following this mantra: ?€?log everything and ask questions later?€?. The logs help in providing users with targeted ads, recommendations, etc. Describing data as the new ?€?gold?€?, he suggested collecting as much data as possible and then applying analytics in different ways over it to generate profits. He briefly described big data challenges as: volume, variety and velocity.
,
Hadoop is a software stack which runs on cluster of machines providing distributed storage and processing. Before Hadoop, parallel computing concept included storage and compute clouds as separate entities. Hadoop merged the ,two by moving code to data. Talking about cost of storage, he mentioned that it has reduced significantly and it costs about 5 cents/GB these days. A modern disk can support about 150 MB/sec read speed i.e. reading 1 TB in just 2 hours. 10 GB of sequential data can be read in about 20 hours. How about 10 TB spread across 10 disks? Reading 10 disks of 10TB in parallel takes just 2 hrs, leading to IO throughput of 5GB/sec. Now, if we imagine 10 machines, each with 10 disks then we can read 100TB in 2 hours, leading to IO throughput of 50GB/sec. This is exactly how Hadoop scales.
,
,
Hadoop technology is widely used by all major online players, such as Yahoo, Facebook, Twitter, eBay, etc. Maniyam briefly discussed some of the popular Hadoop distributions by Apache(official), Cloudera, HortonWorks, MapR, etc. Hadoop in cloud is offered by AmazonEC2, Google Compute, RackSpace Cloud, Microsoft, etc. 
,
Big Data in cloud is pretty popular among startups as there is no initial cost and they can pay per use. A click can create any sized cluster. However, Big Data in cloud has several challenges such as cost, security and getting data into cloud. Running a permanent hadoop cluster in the cloud can be costly. So, ?€?Hadoop on demand?€? came as a solution. ??The model works as following: spins up a cluster, processes data, shuts down the cluster and you can pay for usage. 
,
Apache Whirr is an open source tool to easily manage clusters in the cloud. Hadoop has a large number of use cases: analyzing click stream data to optimize ad-serving, fraud detection, find influencers in Twitter sphere. Enterprises are now adopting Hadoop to get new insights and compliment their existing data-warehouse and processing. With the help of following image he described scaling and access time for various database technologies.
,

Talking about Hadoop ecosystem, Mr. Maniyam briefly described following technologies:
,
, ?? ,
Extended Hadoop Ecosystem includes following:
,
, ?? ,
Technologies involved in Hadoop ETL can be classified as shown in following figure:

,

Maniyam summarized the lecture by mentioning that Hadoop is mainstream, solving very real and difficult problems; however, it is still making inroads into enterprises.
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,,
@kdnuggets, Oct 23, 2014.
,
,
,
New IBM??, is open to developers around the world, and starts on Nov 10.
,
Using city data developers can create applications for social good and win great prizes.
,
, Build applications that drive social good using IBM's Hadoop platform
,
,Global, online challenge/hackathon
,
,City of Boston, City of Los Angeles, City of San Jose and others
,
,Cash and trees
,
,
,
, Launch 11/10
,
, All developers will be eligible; ,
,
,
,
,  "
"
,
,
,Oct 30, 2014 from 11:30 AM to 8:00 PM (ET)
,at O'Reilly Media, 10 Fawcett Street, Cambridge, MA 02138
,
, is an open platform for developers and sysadmins to build, ship, and run distributed applications. 
,
Join other Boston-area developers and Docker fans for 
,
at O'Reilly Media in Cambridge, MA. We'll livestream the announcement  and have plenty of food, coffee, and wifi for this day-long hack day. The fun continues with a Docker Meetup with plenty of interesting talks and demos, as well as judging of the winning hack.  Boston teams can submit their hacks for consideration to win conference and plane tickets to DockerCon Europe in Amsterdam!
,
Here is more info, including 
,
,
Here is a the recap from 
,
,
,
,  "

"
Most popular 
, tweets for Oct 17-19 were
,
Air traffic data is being analyzed to predict Ebola's Spread , #EbolaOutbreak @hey_anmol 
,
Some cool public data sources you can use for your next data science project. , 
#datamining #bigdata ,
,
Some cool public data sources you can use for your next data science project. , 
#datamining #bigdata ,
,
Some cool public data sources you can use for your next data science project. , 
#datamining #bigdata ,
,
,  "
"
        ,  "
"
By Sumit Sidana, Oct 2014.
,
Currently I am working with Dr. 
, on a project, in which we are trying to find correlations between various demographics (Geo-location, age , gender) and nutrition. We are also looking for relationships between these demographics and health using ,.
,
,
For this purpose, we first need to recognize important features of health and nutrition tweets using the power of crowd. We need to understand what general population thinks about health and nutrition. Good quality of health and nutrition related tweets will help us to mine them further in an efficient way.
,
Therefore, we need your help! We request you to open these links and label as many tweets. Tasks are self-explanatory. We hope to put your labeling to good use such as finding correlations between health and nutrition.
,
,
,
,
,
Bio: I am a PhD. Student at ,.,
My research interests include Data Mining, Machine Learning, Natural Language Processing and Text Mining.,
Some of my projects include designing Search Engine, Page Ranking, Developing Insights on huge data sets, Face Recognition System, Opinion Mining and Machine Translation.,
I completed my Masters from IIT HYDERABAD (2011-2013). Then, I had a significant amount of industry experience as research engineer in Xurmo Technologies where I designed and wrote code for machine learning algorithms for the platform.
,
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,, Oct 20, 2014.
,
Ebola epidemic remains a major global health threat, and continues to increase rapidly. 
The top world epidemiologists and medical professionals are already examining the Ebola data from all angles, so my goal in this post is more to draw some data science lessons rather than to discover something new. Still, we do find new results on changes in Ebola death rate in Liberia and Sierra Leone, and the implications it has on the extend of undercounting of Ebola cases.
,
Ebola is presently an example of ""small data"" (let's hope it stays this way), but its data has many problems which lend themselves to useful lessons that can be applied to Big Data as well.
,
I have extracted the data for the 3 most affected countries (Guinea, Liberia, and Sierra Leone) from
,
,??,
The first lesson is to ,, since the report date is usually a few days later than ""data"" date, and the dates for each country sometimes differ.  Thus we have to keep a separate date for each country data.  Next figure shows the growth in the number of , cases.
,
,
,Fig 1. Total Number of Ebola Cases in West Africa, as of Oct 14, 2014
,
First observation we can make is that the growth in the number of 
, cases in Liberia - the most affected country - has slowed down since the middle of September. 
,
Is it because the epidemic has slowed down or because the cases are not being reported by overwhelmed local health officials? 
If the epidemic has slowed down, we can expect the slow down in number of deaths as well. 
,
,
,Fig 2. Total Number of Ebola Deaths in West Africa, as of Oct 14, 2014
,
The grim figure 2 of the number of death does not show any slowdown in Liberia but shows
a significant jump in Sierra Leone deaths, starting in late Sept. 
,
,
Next, we examine Liberia in more details and look at the reported number of cases/day and number of deaths/day there.
,
,
,Fig 3. Liberia Ebola Cases and Deaths per day (reported), Jun-Oct 2014.
,
The raw data is very noisy, since the reports come at different intervals.
Next idea is to ""smooth"" the data by examining changes at regular intervals, and weekly intervals is the next logical choice.
,
Since we see changing trends, having one global estimator will not work, even if we can fit a polynomial function to the total number of cases with R=0.987.
,
Instead, we will take the simplest approach and use local interpolation. 
,
There is actual data for Liberia for Oct 7, Sep 30, and Sep 23 (all Tuesdays), but 
not for Sep 16.  There is data for Sep 17 and Sep 9, so we can use these 2 points to estimate data for Sep 16. Since the number of cases appears to grow exponentially, we will not use  linear extrapolation. 
,
Instead we will take logarithms (which do look piecewise linear) and do a linear interpolation of logarithms.  For example, to get the estimate for the log of cases for DateB between Date 1 and Date 2, we use  
,
,
,
and then convert the log to the number of cases.
,
The following chart confirms that the estimated cases fit very nicely into the overall pattern.
,
,
,Fig 4. Liberia Ebola reported and estimated Cases using log-linear approach, Jun-Oct 2014.
,
The Data Science Lesson here is ,
,
Now we can look at the data at weekly intervals, which are indicated on the X axis of Fig 4.
,
,
,Fig 5. Liberia Ebola Weekly Data (reported and estimated), Jun - Oct 2014
,
The number of cases/day declines sharply, but the number of deaths does not decline correspondingly.  Since cases are more likely to be underreported than deaths, the more likely conclusion is that the number of cases is very much under-reported. 
,
So the data science  lesson here is to 
,
,
,
We can also do a similar analysis for Sierra Leone for July - October (in June the number of cases there was not reported reliably).
,
,
,Fig 6. Sierra Leone Ebola Weekly Data (reported and estimated), July - Oct 2014.
,
We note a continued growth in cases/day since Sep 10 and a sharp jump deaths/day in early October - perhaps sharpness of the jump is due to late reporting.
,
,
Next, we examine what is the Ebola death rate.
,
It was reported that the Ebola death rate has
,, but no supporting data was provided.
,
The death rate in some analyses I saw compared the number of cases diagnosed on day X with number of deaths reported on the same day.  
This, however, is incorrect, since it takes Ebola a number of days to kill its victims.
,
Wikipedia says death from Ebola usually occurs 
,.  
,, first Ebola patient diagnosed in the US, succumbed to Ebola in about 13 days after his first hospital visit. The treatment for Ebola victims in West Africa is certainly less advanced than what he received, so  a reasonable range for death from Ebola is about 1-2 weeks from diagnosis.
,
So we can compute death rate with 1 week delay, by dividing death(date=X) with cases(date=X + 1 week), etc. 
,
Next chart looks at Ebola death rate in Liberia with same day (under-estimate), 1 week, and 2 week delays. The chart does not show data before July 8, when there were less than 110 reported cases, which produced both 1 and 2-week death rates higher than 100% - suggesting significant undercount of initial cases.
,
,
,Fig 7. Liberia Ebola Death rate, same day, 1-week, and 2-weeks delay, July - Oct 2014
,
We see that with more cases, death rate converges to around 60-65%, with delay between 1 and 2 weeks.
,
Here is a similar chart for Sierra Leone death rate.
,
,
,Fig 8. Sierra Leone Ebola Death rate, same day, 1-week, and 2-weeks delay, July - Oct 2014
,
In Sierra Leone there is a consistent gap between the same day, 1-week, and 2-week death rates, which suggests more consistent reporting than in Liberia.  All 3 death rates  decline from July up to Oct 1, then increase in the last 2 weeks. Sierra Leone death rate is lower, which suggests better treatment or more under-reporting.
,
What can we infer about spread of Ebola from this data?
,
Not much, since we have not used additional sources of data, such as geo-location.
,
The 
, has a very good chart showing the latest geographic spread of Ebola by counties - see below.
,
,
,Fig 9. Geographical distribution of new cases and total cases in Guinea, Liberia, and Sierra Leone, as of Oct 12, 2014.
,
We can see where the disease grows faster.
,
Other analysis was done using air traffic data 
, via airline travel - see below.
,
,
,Fig 10. Air traffic data to predict the Ebola spread
,
,
,
What do you think about Ebola analysis and Data Science lessons?
,
Let me know in the comments below.
,Here is  
, in CSV format.
,
See also
,
,
,
,
,??,
,
,
 ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Oct 21 and later.
,
See full schedule at , .
,
,  "
"
,
,
You Can 
,
,
,
,
Discover the power of tree-structured data mining during this popular introductory seminar, geared toward statisticians and IT audiences who are interested in understanding the conceptual basis of decision tree technology - what is it, why it works, how it has been used, and how it can help you make better business decisions. 
,
,
,
Using real-world datasets we will demonstrate Stanford Professor Jerome Friedman's advances in nonlinear, regularized-linear and logistic regression. This workshop will introduce the main concepts behind 
Generalized PathSeeker (GPS) and Multivariate Adaptive Regression Splines (MARS), a nonlinear automated regression tool
,
,
,
This workshop discusses key algorithmic details of Breiman's Random Forests and Friedman's TreeNet, and important extensions to bagging/boosting technology. We will show how the software is used to solve real-world problems, cover theory, discuss what is novel, illustrate how to select an ideal balance between model complexity and predictive accuracy, and show where the software fits in terms of other data mining software.
,
Learn more and register at 
,  "
"
By Gregory Piatetsky,  
,, Oct 21, 2014.
,
Join me and my fellow panelists for
,
,,
,noon PT, 3 pm ET.
,
Once upon a time, only a handful of major vendors offered tools for predictive analytics. That time has gone. Today, there are dozens, even scores of companies wading into the world of predictive analytics. Some of the new tools use machine learning algorithms that can vastly improve the accuracy of certain predictions. How, where and when can these new tools be leveraged? And how much has the cost come down? Register for this episode of DM Radio to find out!
,
,
,
,??,
,.  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
,
,
, (Sept 17-18, 2014) was organized by the Innovation Enterprise in Sydney, Australia. It brought together a large gathering of senior business executives leading Big Data initiatives in Australia. The summit brought together business leaders and innovators offering solutions and insight in the Big Data space. Big Data Innovation helps utilizing data-driven strategies and discovering disciplines that change because of the advent of data. With a vast amount of data now available, modern businesses are faced with the challenge of storage, management, analysis, visualization, security and disruptive tools & technologies.
,
Here are highlights from Day 1 (Sept 17, 2014):
,
, talked about how combining digital analytics with user experience research can help optimize customer experience. He emphasized that companies should go for valuable data rather than just huge amount of data. Today, digital channels are opening up more ways for one to engage with one's customers than ever before, and these channels can be a very fast cost-effective way to drive business performance. However, if one gets the customer experience wrong, then digital channels become a very fast way to destroy value instead. Missed conversion opportunities, opt-outs, do-not-solicit requests, and negative brand experience, all have a real impact on the bottom line. 
,
Opportunities to engage with customers are precious and one needs to ensure these interactions are optimized. Using the power of digital analytics, one can reach a deeper understanding of one?€?s customers?€? behavior and allow every hypothesis to be tested. Combining this with User Experience (UX) research, one can understand why customers behave the way they do and how they would like their digital experience be improved. He concluded saying that we should not forget/ignore the human side of big data and decisions should not be taken without taking into account customer experience.
,
, talked about how real-time visual data discovery is demystifying big data. Giving a quick overview of Datawatch, he differentiated traditional BI/data-warehousing with real-time analytics visualization as the former one presents what you already know but in a very nice way and the latter one shows what you don?€?t know but should know.  Talking about return on real-time visualization, he mentioned organizations utilizing real-time data visualization have outpaced all others in several key metrics. Event streams come from various sources such as IOT (Internet of Things), Web click streams, commercial data, log files, etc. He gave some interesting use cases and data visuals. 
,
Data is increasingly available at real-time speeds, and from disparate sources such as databases, PDF files, web pages, machine data and text files. This trend is only going to grow as organizations demand the ability to make quicker decisions with all the information available. It is no longer feasible to analyze only structured data sources, and to wait until the end of the month, or week to do so for reports to become available. Things can go wrong intra-day!  Organizations should learn how Visual Data Discovery enables the identification of patterns and outliers in data at any speed, and from virtually any source, to highlight 'what you don't know, but should'. 
,
, delivered a talk on ""Ensuring Production Success for Hadoop"". Google recently invested $110 million in MapR. He briefly explained basics of Hadoop and its working. He mentioned that big data trends are forcing a revolution in enterprise architecture. 
,
2014 was expected to be the year when Hadoop goes mainstream. A majority of companies moved Hadoop from small-scale test environments into full-fledged production deployments. However, enterprises should complement Hadoop production success with an architecture designed specifically for business-critical applications. MapR ensures seamless data access and integration. It runs both online, to support analytical processing and applications reliably on one platform.
,
,.
,
,
,  "
"
Most popular 
, tweets for Oct 20-21 were
,
The 'Internet of Things' #IoT will be the world biggest device market, save companies $ Billions #BigData ,
,
4 ways to become a Data Scientist w/out getting a PhD: BA+ experience, MOOC courses, Bootcamp, MS Degree (preferred) ,
,
Cazena, #BigData startup from former Netezza execs, raises $8M from Andreessen Horowitz 
,, 
others #BigDataCo ,
,
4 ways to become a Data Scientist w/out getting a PhD: BA+ experience, MOOC courses, Bootcamp, MS Degree (preferred) ,
,
,
  "
"
        ,
,  "
"
,
Latest ,, (Oct 22, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
,
Ebola is presently an example of ""small data"" (let's hope it stays this way), but its data has many problems which lend themselves to useful , that can be applied to Big Data as well. Gregory Piatetsky, Oct 2014.  "
"
,
,
,
,
A European Leader Supermarket chain decided to design and implement a Business Intelligence Strategy and applications in order to increase competitiveness and profitability.
,
,
,
,
,
Consumers are the ultimate arbiters of enterprise ability to identify and predict market trends and to procure and distribute products and services that represent desired customer value, at the right price and through the right channels.
,
Firms must be aligned to consumers?€? continually evolving needs and expectations of value.
,
As a result, the ability to innovate successfully to create customer-centric differentiation is critical to the overall success of the sector and increasingly decisive in the survival of individual enterprises.
,
In order to achieve a Customer-Centric framework, we created a Business Intelligence architectural plan that analyzes the interferences (input) of all external factors on customers and the consequences on their final purchase decision (output).
,
,
,
In the heart of our Business Intelligence applications platform is Customers Segmentation based on purchase behaviour.
,
The biggest problem with segmentation is that a supermarket has a?? huge, continuously changing number of product codes (new products, seasonal products, promotions that makes a product appears with a different code, etc?€?) that makes any segmentation based on purchase behaviour almost impossible.
,
In the other hand using only categories of products make decision makers loose information that only products detailed description offers. For example customers who prefers white yogurt 0% are different from the those who prefers a yogurt with fruits 2%. And customers who prefers Danone yogurt differs from those who prefers Nestle yogurt.
,
,
,
Our objective concerning segmentation was to obtain a scientific state of the art segmentation and in same time useful for business decision making.
,
In order to solve this problem we opted for a two layers approach, training Self-Organizing Maps first with an intermediary categorization (sub categories of products plus brands) and finally with detailed products.
,
We used as data customers annual transactions and unsupervised learning and Self Organized Map that allows to find groups of clients (Clusters) that exhibit a certain degree of similarity in respect to a number of features that describe these objects.
,
We opted for 25 clusters solution (5 X 5) as it is important for a retail company to have the less possible groups of customer in order to design cost effective business and marketing campaigns.
,
,
,
Features extracted values allows us to examine each cluster separately, finding how and why it was formed as in bellow Figure, where we see that Cluster 11 is made of families with young children, that prefer biological products.
,
,
,
By classifying clusters based on data such as : total sales, net profit, etc... we obtained the economical impact of each cluster on enterprise profitability.
,
,
,
,
,
Customers Segmentation observed through time, allows comparison for the same clients between two time periods?? and offers a macroscopic point of view on customers evolution in a social and economic context, measuring in same time the efficiency of the Enterprise's strategy .
,
In the following Figure (comparison between 2009 and 2010), we observe that 41,1% of clients from Cluster 5 (new and basic clients) remain in the same cluster and have the same consumption habits between 2009 and 2010.
,
A significant part of the rest, moves horizontally from cluster 5 to cluster 25 (all products from the same SM, that means they became heavy and loyal clients) and another part moves vertically from cluster 5 to cluster 1 (fruits and vegetables, biological and healthy products in general)
,
,
,
,
,
,  "
"
,
By Michael Brodie, Oct 2014.
,
,
,??monumental??,??has deep challenges and lessons beyond the world of economics.
,
In adhering to??,??Piketty achieved the nearly impossible with no guidance from??,??let alone tools. He instinctively followed many as-yet unwritten principles of this emerging field. As an unwitting Data Science pioneer, Piketty?€?s work took a decade. Following his model, emerging data curation tools may enable others to achieve similar results in two years or less.
,
Let?€?s look at his challenge:
,
For each of his many hypotheses in his ~700-page book, Piketty had to:
,
,
For Piketty, faithful data science here was a dynamic process, requiring him to explore, acquire and curate the data he needed; model and verify it; modify his hypothesis and all the data that went with it; document its provenance; and then and only then publish it.
,
Rigorous? Yes. Protracted? Yup ?€? a decade. Worth it? Absolutely, especially in the context of the??,??that ensued.
,
,
Typically, debates about science, medicine and economics focus around modelling and the related assumptions. However, the rapidly growing role, use, and value of Big Data adds??Data Science to the debate, and whether the appropriate scientific, statistical and economic methods were followed in conducting the research. Specifically, did the Data Curation ?€? the discovery, analysis and combination of data sources into a curated data set for use in the analysis ?€? achieve the veracity, data quality and significance requirements of the economic analysis? Does the data curation provenance demonstrate that adequate data governance was applied?
,
Let?€?s address these questions through the example of economics.
,
,??(a term coined or at least endorsed by Piketty, joining Computational Biology, Computational Social Science and many more) should be based on sound data science principles just as scientific experiments are based on the principles of the scientific method.
,
Piketty?€?s work is an example of using computational economics to test collections of economic hypotheses against available data. Good data science practice in these cases involves determining data requirements from the economic hypotheses including the veracity, data quality, and significance requirements. These??,, as they are called in physical science experiments and clinical studies, are derived from the model, the intended analysis, and should be verified together with the model and the intended analysis by expert economists. The economic analysis proceeds in two steps: data curation and data analysis.
,
As the Piketty data controversy illustrates, raw data sources are seldom in a form required for analysis. Raw data sources are replete with errors, inconsistencies, and gaps and must be augmented and combined to meet the requirements of the analysis. The economic facts that are analyzed are those curated from the raw data sources.
,
The data curation step involves discovering, analyzing, cleaning, transforming, combining, and de-duplicating data sources to produce target data sources that meet the requirements for input to the analysis. Every data curation step should be documented as data provenance that is then compared against the controls to determine the extent to which the appropriate data governance was followed and the required data quality was achieved.
,
All of this must be verified by reviewers, supporters and detractors alike. The analytical results should be accompanied by the measures of data quality supported by the data provenance, and data governance, hence the extent to which the requirements were met. Variations from the requirements should be considered in establishing a level of confidence in the analytical results.
,
,. Anything less can be assumed to conceal poor data practices.
,
In conclusion, as the example of computational economics suggests, data curation (as supported by Tamr) must be thought of as an activity that precedes ?€? as ,but??,???€? analysis. Approached this way, the analyst has a far better chance to remain faithful to strong data science principles.??Alternatively, the analyst may violate the principles of their analysis and domain, in Piketty?€?s case economics.
,
Have a look at Piketty?€?s??,??in economics and data science.
,
Previous version published at ,
,
,
,
,
,
,
 ,  "
"
,
,
Twitter data has recently been one of the most favorite dataset for Natural Language Processing (NLP) researchers. Besides its magnanimous size, Twitter data has other unique qualities as well ?€? it comprises of real-life conversations, uniform length (140 characters), rich variety, and real-time data stream. Advanced analytics on Twitter data needs one to go beyond the words and parse sentences into syntactic representations to develop a better contextual understanding of the tweet content. This can now be done conveniently through the tools developed by , and his team at Carnegie Mellon University.
,
,Prof. Noah Smith?€?s informal research group (commonly known as ?€?,?€?) at the Language Technologies Institute, School of Computer Science, CMU has developed the following NLP tools/utilities to analyze Twitter data:
,
,: A fast and robust Java-based tokenizer and part-of-speech tagger for tweets that breaks down the tweet into a sequence of tokens (which roughly corresponds to ?€?words?€?) and identifies the part-of-speech (such as interjection, proper noun, preposition, subordinate conjunction, etc.) that the token belongs to. Importantly, the tool also identifies misspellings and corrects them before deciding which part-of-speech the token would belong to.
,
Source Code is available ,.
,
,: A dependency parser for English tweets, TweeboParser is trained on a subset of Tweebank dataset. Given a tweet, TweeboParser predicts its syntactic structure, represented by unlabeled dependencies. Since a tweet often contains more than one utterance, the output of TweeboParser will often be a multi-rooted graph over the tweet.
Sample output of TweeoParser:
,
Parser (along with pre-trained models and annotated data) is available here on ,.
,
,: From over 56 million English tweets (837 million tokens), 1000 hierarchical clusters over 217 thousand words. The source data comprises of sample of 100k tweet/day from 9/10/2008 to 8/14/2012.
,
Sample clusters:
,
If interested, you can check out the ,.

,

For more details, refer to the following recent publications:
,
,
Olutobi Owoputi,??,,??,,??,,??,??and??,.
In??,.
,
,
,,??,,??,,??,,??,, and??,. In ,.

,
,
,  "
"
By Slater Victoroff, Oct 2014.
,
It?€?s that time of year again! The birds are leaving, the picket lines are up in force, and people are telling me once again that I should wear something other than flip-flops. That?€?s right. It?€?s voting season: the season where citizens around the country are inundated with countless political slogans, and smear campaigns in the hopes of shifting a few votes.
,
,
,
In Massachusetts there are four ballot initiatives, each comprised of some uncountable number of incomprehensible legal documents, and only a few short paragraphs to help inform the average voter. How then, in this world of carefully targeted political sloganeering is a person supposed to inform himself or herself without turning it into a full-time job?
,
The 20th century answer is to research harder and do real due diligence on each ballot initiative to come to a reasonably informed opinion. Today however, we have more options. indico just so happens to have a pre-built machine-learning tool to help out with a 21st century approach.
,
Let?€?s start with Massachusetts?€? Question 1 initiative to change the way that Mass. handles gas taxation 
,(,). Scrolling through the arguments section, it becomes pretty difficult to tell which side is the most reasonable, and despite the fact that the two arguments are in direct conflict, they both seem to present pretty reasonable cases.
,
The first thing that the indico political API helps us pull out about these two arguments is that they are both very clearly intended for a Liberal audience. Quickly running any of the pro or con arguments through the API yields a clear Liberal signal.
,
Since Massachusetts is a Liberal state, it?€?s unsurprising that most arguments are constructed with a Liberal audience in mind. However, with the power of the internet, we can go a little deeper on the issue and actually look up some more informal speech to get a better sense of the actual leanings on this bill. If we look up an interview (,) with the author of that argument, Steve Aylward, we find a decidedly more conservative tone in his quotes.
,
,
,
It?€?s not a new way to vote, but hopefully indico?€?s political API
, (,) can help give you some insight into political figures and movements.
,
Summary: Election season is coming! Take a deeper look at some political dynamics with indico?€?s political analysis API.
,
Suggested Tags: Analytically Speaking; Data Analytics; Machine Learning; startups; text analytics
,
,
Author Bio: Slater Victoroff is the CEO of indico (,). In previous lives Slater has been an MMA fighter, a buddhist monk, and a poet, though the last several years have been spent in solemn dedication to software. He is currently based in Boston.
,
Twitter: ,
,
,
,
,  "
















"
Most popular 
, tweets for Oct 22-23 were
,
Great viz: chances of survival of #chess pieces in average game. Most long lived: h2,g2,a2 
,
,
,
Baidu, 'Chinese Google', had big revenue jump after it started using  #DeepLearning to target ads #MachineLearning ,
,
Baidu, 'Chinese Google', had big revenue jump after it started using  #DeepLearning to target ads #MachineLearning ,
,
Great viz: chances of survival of #chess pieces in average game. Most long lived: h2,g2,a2 ,
,
,  "
"
,
,
The IEEE ICDM Research Contributions Award is the highest recognition for research achievements in Data Mining, and is given to one individual or one group who has made influential research contributions to the field of Data Mining. The 2014 IEEE ICDM Research Contributions Award goes to Professor Jian Pei of Simon Fraser University, Canada.
,
, is a Canada Research Chair (Tier 1) and a Professor of Computing Science at Simon Fraser University, Canada, and a Fellow of the IEEE. His research has focused on effective and efficient approaches to analyze and capitalize on big data in various applications. He has published prolifically over the core frontiers of data mining, including pattern mining, classification, clustering, anomaly detection and outlier analysis. His 
, have been 
,, tens of thousands of times.
,
Professor Pei has made prominent contributions to the foundation, principles and applications of data mining. Together with his collaborators, he developed a series of ground-breaking frequent pattern mining methods, which have been adopted by popular data mining textbooks and used by many researchers and in industry applications. He is also proactive and strategic in extending the boundary of data mining. He contributed the major ideas of multidimensional skyline analysis, a well-seasoned integration of ideas and methods from data mining and databases.
,
In addition to his fundamental technical contributions, Professor Pei has also made exemplary and invaluable contributions in service to the data mining research community through journal editorship and key organizers of premier data mining conferences.
,
,
,
,??,
,
,  "
"
,, a highly innovative, peer-reviewed journal,  provides a unique forum for world-class research exploring the challenges and  opportunities in collecting, analyzing, and disseminating vast amounts of data.  
,??,
Stay connected with these top-read Open Access articles:
,
,
,??,
,??,
,
,  "
"
,
,
The IEEE ICDM Outstanding Service Award is the highest recognition for service achievements in Data Mining, and is given to one individual or one group who has made major service contributions that have promoted Data Mining as a field and ICDM as the world's premier research conference in Data Mining. The 2014 IEEE ICDM Outstanding Service Award goes to Dr. Ramamohanarao (Rao) Kotagiri, Professor of Computer Science at The University of Melbourne, Australia.
,
,
has been a Professor at The University of Melbourne since 1989. He is a Fellow of the Australian Academy of Science, as well as the Australian Academy of Technological Sciences and Engineering. He has a long history of promoting ICDM and the field of data mining. He was a founding member of ICDM and has been a Steering Committee member since its inception. He was Honorary Chair for ICDM 2012 in Sydney and has also undertaken roles such as Panels Chair and Awards Chair for the conference.
,
Professor Kotagiri's service to the broader data mining community includes service on a range of editorial boards, including IEEE Transactions on Knowledge and Data Engineering, Knowledge and Information Systems, The VLDB Journal and Statistical Analysis and Data Mining. He is recently a General co-Chair for IEEE ICDE (2013) and ACM SIGMOD (2014), venues fostering top research in databases and knowledge discovery. He has also been active in the Asia-Pacific region, being co-PC Chair for the 2nd PAKDD Conference in 1998 and shortly afterwards, being one of the co-founders of the PAKDD Steering Committee.
,
Professor Kotagiri has also made significant contributions to the research areas of pattern mining and classification, intrusion detection and text mining. He has published over 300 research papers and supervised 48 PhD students.
,
,
,
,
,  "
"
By Molly Larkin (Zipfian Academy), Oct 2014.
,
,The best way to learn data science is by doing data science, and the focus of Zipfian academy is hands-on work extracting actionable information from real-world data sets. The program teaches data science as it's practiced in industry, from statistical analysis to machine learning and software engineering, with a focus on working with real data sets.
,
Zipfian Academy is now accepting applications for the Data Science Immersive program in San Francisco. 
, of the program join some of the most respected data science teams in the world at companies like Facebook, Twitter, Tesla, Airbnb, Uber, and more.
,
Graduate Alex Mentch, now a Data Scientist at Facebook, cites the hands-on curriculum as a core strength of the program: 
,
,
,
,
Alex sat down to chat with us about his experience in the program, the project he built, and the process of interviewing for data science roles. 
Read the 
,
,
Zipfian Academy provides participants with a world-class network of mentors, alumni, and hiring partners. The program culminates in a Hiring Day, a private recruiting event where graduates connect with top companies. As a result, 93% of participants find data science roles within 6 months of graduating.
,
Zipfian Academy selects candidates with quantitative backgrounds in engineering, data analysis, statistics, and (sometimes) professional poker. Curiosity, motivation, and excellent communication skills are also part of the selection process.
,
Dates for the upcoming Winter 2015 cohort are January 5th - March 27th, 2015. Applications are due November 14th, 2014 and admissions are rolling.
,
To learn more about the data science immersive at Zipfian Academy, we're hosting an online info session for the program on October 28th. In the information session, an instructor from the program will provide a curriculum overview and host a Q&A for listeners. 
,, or 
,
,
To find out more about Zipfian Academy, watch our 
,
where we described the program in more detail and answered questions. Applications are now open.   "
"
        ,
,
This blog picks up after ,, and describes the remaining steps of the data transformation process, detailing how we used Trifacta to structure, clean, enrich and distill Jeopardy data for analysis.
,
,
,
Data lacking traditional tabular structure can be difficult to initially understand. Hundreds, if not thousands of rows can display themselves as a single string of run-on characters in end-user tools. For this semi-structured data, analysts need to know the content of the data in advance to quickly uncover delimiters in the data that reveal its formatting. Rather than forcing the analyst to manually figure this out, Trifacta introspects the data and infers as many delimiter patterns as it can in advance of displaying the data, automatically and proactively recommending initial data structure. By automating this process, Trifacta speeds up an analyst?€?s ability to understand what?€?s in the data and how it may need to be further refined for analysis.
,
,
Fig 1. Initial inferred structuring of Jeopardy Contestants data set
,
After gaining an understanding of the data profile, we began pulling out attributes or , relevant to our analysis.
,
For the contestants data set, we worked through the free text bios of the contestants to split the contestant information originally provided in a single data column, into separate columns for each contestant?€?s name, state and occupation.
,
,
Fig 2. Example of extracting Jeopardy contestants?€? occupation into a separate column
,
To create indicator columns not present in the original data set, we also prepared the data for aggregations and calculations by leveraging the columns on show number and contestant final score. This is often called variable reshaping and will help us mathematically identify whether a contestant was a winner or loser of their particular episode.
,
,
Fig 3. Jeopardy contestants data set (partial view) after the structuring process has been completed
,
In Trifacta, it?€?s important to note that the majority of this structuring is done through interactions directly on the content of the data itself (such as clicking and dragging) and selecting the appropriate recommended transformation for that particular segment of the data.
,
,
,
Since the Jeopardy questions data set originated from someone?€?s scraping efforts, there were some data quality issues that we needed to clean up. As shown in the screenshot below, we were able to use Trifacta to remove elements that aren?€?t accepted by downstream analytics tools such as formatting inconsistencies for numbers and names. This type of data cleansing is not only commonly required for data that may have been gathered publicly, but also often required for tabular data due to mistakes made during data entry or additional values erroneously created during the data processing pipeline in large organizations.
,
The scraping process pulled in html tags for links on the pages that we scraped. To clean up these tags, we used a combination of highlighting data elements visually and accepting Trifacta recommendations to build a transform to identify and remove any text elements in that column that were enclosed by tags. The (partial) screenshot below shows how this was performed in Trifacta.
,
,
Fig 4. Cleaning Jeopardy Questions data set by removing html characters
,
,
,
The preparation process of the two data sets for this Jeopardy analysis required blending the two data sets to start answering questions. Trifacta?€?s interface allows you to perform this blending fairly seamlessly within the interface. You simply open the Jeopardy contestants data set and connect it to the Jeopardy questions data set.
,
,
,
,
Fig 5. Enriching the data by blending the Jeopardy questions & Jeopardy contestants data sets
,
Once you determine the appropriate columns to match on and which columns to include in the final output data set, you?€?ve completed enriching the Jeopardy contestants data set with additional attributes or features of each show from the Jeopardy questions data set.
,
,
,
Depending upon the type of analysis you?€?re performing, a data set can be prepared a variety of different ways to answer slightly different business questions. Whether it?€?s performing a lookup on a certain element in the data, creating indicator columns or blending different data sets, the business case for the data will determine the preparation process of the data.
,
At the end of the data preparation process, typically you want to share your insights with a broader audience. This is where data analysis shifts into telling a story about your data. Data visualization technologies from companies like , have become very robust platforms for sharing data stories across a wide audience of users. However, they often have constraints on the size and shape of data that they can accept and often prefer tabular structured formats. This is where Trifacta is able to transform data for analysis and filter the data down to a size that your data visualization tool of choice can support. Trifacta has optimized integration with tools like Excel and Tableau by delivering data in output formats native to those platforms.
,
At Trifacta, we?€?re solving the bottleneck between organizations and discovering insights from data by facilitating the process of data transformation. The Trifacta Data Transformation Platform is enabling data analysts, data scientists and anyone who regularly works with data to increase their productivity when transforming data for analysis.
,
,
,brings a wealth of field experience to Trifacta's product management team with his experience in product management, alliances and sales engineering.??Prior to joining Trifacta,??Alon??worked at both GoodData and Google.??As Product Manager at Trifacta,??Alon??works closely with Trifacta customers and partners to drive the product roadmap and requirements for Trifacta's Data Transformation Platform.
,
,
,drives both content and product marketing efforts at Trifacta having spent the past five years managing the marketing initiatives for several high-growth data companies. Prior to Trifacta, Will worked with a variety of companies focused on data infrastructure, analytics and visualization, including GoodData, Greenplum and ClearStory Data. Will develops and executes Trifacta?€?s marketing and content strategies to rapidly expand business growth and brand awareness.
,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
A WCAI Research Opportunity Sponsored by a Fortune 500 Software Company,
November 21, 2014,
Noon - 1pm Eastern Time,
,
,
The Wharton Customer Analytics Initiative is pleased to announce an extraordinary dataset from a Fortune 500 Software Company that allows for deep analysis and exploration of how customers use and renew software licenses.
,
This dataset contains 8 cohorts of 10,000 customers each. The cohorts are randomly selected from customers who made their first purchase in each calendar year, 2006-2013. Included in this dataset are all licensed products for every year, purchase history (including free trial conversions and channel source), and 4 years of weekly ""heartbeat"" data to indicate software use & renewal communication. In addition to using a subscription model for licensing, the data sponsor also has implemented ""auto-renewal"" in the past, requiring customers to opt-in or opt-out of automatically extending their subscriptions.
,
Through this Research Opportunity, the sponsor wishes to better understand:
,
,
To learn more about the company and the dataset, interested faculty, doctoral students, and other researchers can attend a live webinar on ,. During the webinar, the data will be described in detail, and executives from the Corporate Sponsor will be available for Q&A. Register for the webinar ,.
,
Interested researchers should , through the ,by , to be considered for access to the data. Proposals will be evaluated based on their potential for academic contribution and the researchers' ability to address issues of strategic importance to the program sponsor. A committee including representatives from the Corporate Sponsor, Eric Bradlow (The Wharton School), Peter Fader (The Wharton School) and Elea McDonnell Feit (Drexel University) will select the teams to receive the data.
,
Researchers are encouraged to review , and ,before submitting their proposal. Additional questions on the data or the process can be directed to ,
,
,
,
,  "
"
,
By Peter Bruce (Statistics.com), Oct 2014.
,
There was an interesting??, a couple of weeks ago in the New York Times magazine section on the role that Big Data can play in treating patients -- discovering things that clinical trials are too slow, too expensive, and too blunt to find.?? The story was about a very particular set of lupus symptoms, and how a doctor, on a hunch, searched a large database and found that those symptoms were associated with an increased propensity for blood clots.
,
,
However, a search of the medical literature turned up nothing on the subject.?? What to do??? The patient was treated with anticoagulant medication, and did not develop a blood clot.?? Of course, this does nothing to prove that the association was there in the first place. And on the flip side of the coin lies recent research about the non-replicability of scientific research.
,
A recent??, looked at over 4 dozen health claims that researchers arrived at by examining existing data for possible associations - not by conducting controlled experiments.?? These 4 dozen claims all had one thing in common - they were tested later by controlled experiments.?? Astonishingly, ,.
,
Various reasons have been posited for the parlous state of scientific and medical research, including fraud and outright error, but a key issue is what statisticians call the ""multiple comparisons problem.""?? Even in completely randomly-generated data, interesting patterns appear.?? If the data are big enough and the search exhaustive enough, the patterns can be very compelling. ,
So was the lupus association for real, or a fluke of Big Data??? There's no way to know, ex-post.?? The best we can do is to conduct what Lopiano, Obenchain and Young call "",.""?? One principle is that the researcher should begin with an hypothesis to be tested, then proceed to test it on the available data, without letting any knowledge of the outcomes guide the analysis.?? This eliminates erroneous results that happen when you simply ""look for something interesting until you find it.""
,
There remains the problem of hidden differences among patients, a problem that, in large controlled experiments, is effectively ""washed out"" by the random assignment process.?? Random assignment of treatment is not possible in observational data, so Lopiano et al propose the idea of clustering the patients into relatively homogeneous, and possibly quite small, clusters, where the effects of treatments can be examined for groups of similar patients.?? In this way, different treatment effects for different sorts of patients can be identified.
,
The moral??? Rapid growth in the digitization and availability of patient data and health data in general holds great potential for medical research and personalized medicine.?? However, appropriate statistical methodology and sound study design are needed to unlock this potential, and guard against error.
,
Peter Bruce is the President of Statistics.com.
,
,
,
,
,
,
 ,  "
"
By Courtney Burton, Oct 2014.
,
, is the culmination of the growing need for users of the very popular Open Source Machine Learning Engine H2O to come together and share their knowledge, experiences, and best practices to build smarter applications that are fast, scalable, and easy to deploy into production.  Over the past year, H2O has grown the community with over 100 Meetups.  Our community has solved the most demanding problems of our time with machine learning algorithms including fraud detection, risk assessment, pricing elasticity, mix model marketing, advertising technology, and have competed on Kaggle to place in the top ten.
,
,
To start things off, we'll have a full day of training on November 18th starting with an introduction to the world of machine learning in R, Python, Scala, and via our Web-Based UI if you're not into programming.  Next, we'll take you into the depths of algorithms with a solid use case of our work horse algorithm Generalized Linear Models or GLM.  Finally, we'll get into the really cutting edge stuff with a ""Deep Dive"" into Deep Learning.  
,
,
Industry, Research, and Data Science veterans will assemble the next day on November 19th to give their point of view on Gradient Boosting by Dr. Trevor Hastie, Predictive Marketing by Satya Ramachandran of Marketshare, Unsupervised Learning by Alex Tellez of Robert Half, R for Big Data by Nachum Shacham of Paypal, and many many more from Cisco, ShareThis among many others.  With this many great speakers we're likely to run out of space quickly so head over to 
,
,
,
now to reserve your spot.  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Oct 24-26 were
,
Academic Uprising: Less prestigious journals publishing more of high-impact papers, finds Google Scholar team ,
,
Open Source Distributed Analytics Engine with SQL interface & OLAP on Hadoop-Kylin , 
,
Free! 3 Great Data Science Books You Can Read Now , #DataViz #BigData ,
,
Here is why #DeepLearning is likely to make other Machine Learning algorithms obsolete  ,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Oct 28 and later.
,
See full schedule at , .
,
,  "
"
,
By Gregory Piatetsky,  
,, Oct 28, 2014.
,
Latest KDnuggets Poll asked
,
,
,
,
Compared to 
,, the results are suprisingly stable.
,
CRISP-DM remains the top methodology for data mining projects, with essentially the same percentage as in 2007 (43% vs 42%).  
However, it is reported to be used by less than 50%.
,
CRISP-DM was conceived around 1996 - I remember attending a CRISP-DM meeting in Brussels in 1998 (dont repeat my mistake and never eat ,.)
,
The 6 high-level phases of CRISP-DM are still a good description for the analytics process, 
but the details and specifics need to be updated.  CRISP-DM does not seem to be maintained and adapted to the challenges of Big Data and modern data science.  The original crisp-dm.org site is no longer active, and IBM SPSS Modeler is probably the only tool that still includes it.
,
One response to this lack of modern methodology is the significant increase in people using their own methodology and other methodologies (together 35.5%, up from 23% in 2007) 
,
There are other methodologies being developed - see James Taylor comment below and additional links at the bottom of this post.
,
We also note a big decline in SAS SEMMA methodology (from 13 to 8.5%) .  
,
Perhaps most encouragingly, in the era of Big Data when lack of methodology is likely to produce random and false discoveries, zero people reported using no methodology.
,
,
,
Regional distribution of voters was 
,
,??,
,
,
,
,Ralph, Business (domain) understanding is not binary - you can always have more! Part of the knowledge discovery and CRISP-DM process is to increase your business understanding
,
,
,I always thought of the Business Understanding part as a chicken or egg problem. You either have it and you can mine it, or you need to mine it to define it, if you don't.
,
,
,I like CRISP-DM because it puts business understanding front and center at the beginning of the project. We have had some success with using decision modeling - based on the new Decision Model and Notation standard - as a way to express understanding of the business problem by modeling the decision that the analytic is designed to improve. More focused than simply saying ""improve this metric"", decision modeling helps focus analytic projects on improving the way the business acts today while providing great assets for planning deployment and adoption.
,
See , analytic-projects for more.
,
,
, 
,
Robin Way, in his model deployment red paper, outlines a very nice ongoing methodology that covers not just model deployment but model maintenance as well. Model maintenance is a very important aspect for financial institutions.
,
,
,In past, I looked for a data mining methodology and found crisp-dm, but it was not updated for a long time. Is there any initiative to update that methodology, and where i found documentation about it (specification, book or paper)?
,
Additional relevant links:
,
,??,
,
,
 ,  "
"
,
,
, (Sept 17-18, 2014) was organized by the Innovation Enterprise in Sydney, Australia. It brought together a large gathering of senior business executives leading Big Data initiatives in Australia. The summit brought together business leaders and innovators offering solutions and insight in the Big Data space. Big Data Innovation helps utilizing data-driven strategies and discovering disciplines that change because of the advent of data. With a vast amount of data now available, modern businesses are faced with the challenge of storage, management, analysis, visualization, security and disruptive tools & technologies.
,
,.
,
Here are highlights from Day 2 (Sept 18, 2014):
,
, offered valuable insights in his talk ""Smart Phones & Big Data"". The future is here, it's just not evenly distributed yet. Daniel Nelson described how the mobile revolution is one of the most significant frontiers for big data and offers both massive opportunity and significant hazards for brands. Commerce is undergoing enormous disruption caused by the exponential growth of smart phones. Smart Phones have already taken over online commerce and the long awaited offline takeover is building momentum. 
,
72% of Australians said they never leave home without their smartphone. They check their smartphones an average of 150 times per day. 60% of total time on social media is spent on smartphones. Smart phones come bristling with sensors, connectivity and their owner?€?s unrivaled attention, making them a Data Scientist?€?s dream. By 2017, there will be over 20 billion mobile connected devices. The smartphone will be the interface to the ""Internet of Things (IOT)"". Smartphones and Cloud are symbiotic trends. Smartphones are the collection and delivery point for Big Data. Big Data is redefining e-commerce in many significant ways - analytics to optimize traffic & conversions, dynamic & optimized pricing, recommendation systems, pay-for-performance advertising, attribution modelling, etc.
,
He ended his talk stating that smartphones enable Big Data strategies to break free from online and to flourish and grow in a mobile-enabled world.
,
, gave an interesting talk on ""Making the Big Data Connection"". Telecom industry is facing the challenge of addressing the seemingly endless increase in data collection and the expectations on businesses to manage it in a meaningful way. The Big Data challenge threatens to increase IT budgets and staff requirements by an estimated 40% and potentially cause organisational disruption instead of graceful evolution. 
,
Citing Big Data and 4G as the major telecom trends, he mentioned that several lessons from the 4G roll-out apply to Big Data, such as the importance of speed and user experience, data-centric packages, the transforming profile of data usage and increasing trend of digitization. Big Data, a new productive force within society, can generate significant financial value across sectors. However, value discovery from Big Data is the biggest challenge en-route. Today, Big Data is triggering wide-scale commercial reform - disruptive products and services are emerging, new business models are launching at an unprecedented pace, etc. From telecom industry perspective, customer experience is driving Big Data. Most customers associate Big Data with real-time information.
,
Through real-life case studies, he explained how Big Data can provide great business insights such as consumer profiling, deeper understanding of purchase decisions, etc. For Business-to-Consumer (B2C) segment, business value can be reaped through incremental offers (in terms of bandwidth and download limit) in order to grow user consumption. In the Big Data era, user centrality and awareness drive value growth. Thus, it is important for companies to gain insights into user behavior analysis through harnessing Big Data.
,
He described an ICT platform to drive decision making into real-time operations; along with B2C as well as B2B use case scenarios. In conclusion, he stated that Big Data brings new challenges as well as new opportunities; and an appropriate strategy will enable monetization.
,
, delivered an insightful talk on ""Text Mining Offers the Best of Both Worlds"". Text Mining (TM) is being talked about more and more in the marketing research space, but how does it compare to traditional research approaches such as qualitative and quantitative research? Qualitative research can exact great richness of insights but is not generally projectable to the population. Quantitative research is generally projectable to the population but lacks the richness of qualitative research. However, text mining analytics contain the richness of qualitative research and the capability of projecting to the population! 
,
Currently there are many approaches towards text mining - word clouds via content, sentiment towards brand, thematic representations, text time-series analysis, concepts via clustering, etc. Very often, the key problem is to build predictive models to predict satisfaction, recommendation or NPS (Net Promoter Score).
,
He worked on this problem using text data only, specifically from just one question posed to the subjects, to demonstrate that a single verbatim open ended question can provide predictors of satisfaction. He presented a case study, which compared qualitative and quantitative research outputs, to those obtained via mining and modeling of verbatim text data. Through the case study, he demonstrated that TM provides advantages in cost; time; resources; and scalability, when compared to more traditional methods such as structured qualitative and quantitative research. TM produced key client specific drivers, in addition to insights on how to implement those drivers. 
,
Explaining the mechanics of text mining, he said that before applying analytics the text data needs a lot of pre-processing - stemming, punctuation removal, number removal, applying dictionaries, applying thesaurus, case changes, stop word removal, etc. He stressed that the text mining approach must be faithful to each body of text because of the extreme distributional demands of concept and theme generation.
,
The results of his approach showed that text derived variables contain latent structure closer to reality than loose scale construction that is commonly used in the industry. He also noted the importance of open ended questions which do not contain respondents to the researcher's framework, and are a lot easier than other many types of scales.
,
He believes that TM offers the marketing research industry another arrow in its bow for two reasons. First, text mining offers insights that would otherwise go unnoticed because it is based on free-form text. Second, TM is fast, low-cost, and can have results to clients within days as opposed to weeks. 
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Oct 29, 2014.
,
With the upcoming Halloween holiday,
KDnuggets cartoon looks at the appropriate Halloween costume for Big Data and its companion - ""No Privacy"".
,
,
,
,
,
,
,??,
,
,
 ,  "
"
By Vishal Kumar, Oct 2014.
,
,, a Boston Data Analytics Conference, Nov 6-7, 2014
,
2 Days, 45 Businesses, 16 Panels, 10 talks, 6 Keynotes and lots of learning and networking. A must attend for anyone on a lookout for latest and greatest in the field of data analytics.
,
,30% Off Discount Code: AWKD
,
Before the conference, check
,
,, Analytics Workshop, Nov 5, 2014
,
, is hosting a 6 hour long workshop covering wide array of topics relevant to data professionals. Looking to upgrade the skills? #OpenAnalyticsDay workshop could help.
,
,, A Boston Data Analytics Career Fair, Nov 5, 2014
,
A job fair for data analytics professionals. Data Analytic's top recruiting companies looking for sharp and dynamic data analytics professionals. If you are looking, #AnalyticsFair could help.
,
Discount Code (for recruiter 30% off): AWKD  "
"
,
Latest ,, (Oct 29, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
,
Various reasons have been posited for the parlous state of scientific and medical research, including fraud and outright error, but a key issue is what statisticians call the ""multiple comparisons problem.""  Peter Bruce, ,.  "
"
Most popular 
, tweets for Oct 27-28 were
,
""Breakout detection in the wild"": Twitter open sources, explains R anomaly detection tool ,
,
""Breakout detection in the wild"": Twitter open sources, explains R anomaly detection tool , 
,
Marc Andreessen ,, web pioneer & VC 
, on 
#BigData and upending the world of finance ,
,
Marc Andreessen ,, web pioneer & VC 
, on 
#BigData and upending the world of finance ,
,
,  "
"
,
By Gregory Piatetsky,  
,, Oct 30, 2014.
,
,, a leading expert on Machine Learning, and a professor at UC Berkeley, recently gave an extensive 
,, where he touched on many topics.
,
I am not much concerned about his critique of people using ""brain metaphors"" when talking about computing. Of course, current artificial neural networks are very different 
from one in a human brain.  However, the brain provided an inspiration for neural approach,
just like the bird flight provided an inspiration and proof of possibility for building heavier-than-air flying machines.  Airplanes don't flap their wings, but they fly faster than birds!
,
However, I share Michael Jordan concern about Big Data winter, due to 
simple-minded and statistically unsound approaches which will produce too many false positives. 
,
,
,
When asked about adverse consequences might if Big Data remain on the trajectory he described, he added
,
,
This is hardly a novel warning - many experts have been warning of dangers of overfitting (see related posts below), but it is a very serious one.
,
What do you think?  How likely is the Big Data Winter?
,
,
,
,
,??,
,
,
 ,  "
"
,
,
These days capital markets are heavily interested in Big Data - as the hype is now eventually leading to practical monetary gains, there is lot of interest in evaluating the future potential and identifying the stars of tomorrow. On October 16, 2014, Wikibon released its research on the state of Big Data in the enterprise, the implications for IT heavyweights (Oracle, SAP, Teradata, etc.) and what savvy investors need to consider when placing their bets on the Big Data market.
,
,
Jeff Kelly, Principal Analyst, WikiBon shared great insights and summary of how capital markets are reacting to Big Data, in his presentation ""Big Data, Big Winners (and Some Losers)"". Describing the industry overview, he mentioned that the major companies are under heavy pressure to develop a Big Data strategy. Markets events that have led to this state include Oracle missing earnings and revenue targets, Teradata losing $3 billion in market cap last year and SAP still struggling with HANA.

,

The Big Data revenue is estimated to almost double from $28.5 billion for 2014 to $50.1 billion for 2017. The ""Real"" Big Data - Hadoop & NoSQL (Software/Services Only) - market is estimated to grow from $1.4 billion for 2014 to $3.4 billion for 2017. He provided overview of recent success and challenges faced by some of the leaders such as Splunk, Tableau and Qlik. He mentioned that there are 3 Hadoop distributions that matter: Cloudera, MapR and Hortonworks. These 3 are also strong IPO candidates, along with MarkLogic, MongoDB and Datastax.

,

One of the biggest questions that enterprises are facing today is to decide whether to use a free/open source Hadoop distribution or be a subscription customer of one or more of the commercial Hadoop vendors. Citing Wikibon 2014 survey, he mentioned that out of the respondents, 51% are using open-source Hadoop, 24% are using a free Hadoop distribution from a vendor and 25% are a paying customer of one or more commercial vendors.

,

With the Big Data ecosystem exploding rapidly, he called the business landscape - ""The Wild West"". There are 2 big questions for capital markets:
,
, 
,
Big Data is still in its early days. Most of the organizations are still evaluating their Big Data projects or deploying pilot projects for testing. Not many companies are yet using Hadoop, rather most of them are still using the conventional tools.

,

Practioners are the biggest winners in Big Data. The future belongs to the companies who will come up with novel business models leveraging Big Data technology to enable disruptive innovation. Such practioner companies will not be confined to IT industry and would most effectively use Big Data technology vendors to execute their business strategy. A big majority of such companies (72% of survey respondents) are looking out for external guidance.
,
The practioners are expecting big returns - they expect a return of $3.5 on every dollar spent; however, today they are getting a return of just 55 cents. This is no surprise given the numerous challenges around data, technology, people, processes, and IT-business alignment. However, the practioners are upbeat about the future prospects, and with a great attitude, they are all set to create exponentially more value than vendors.
,
Lastly, he encouraged companies to assess their ?€?Data IQ?€? by answering the following questions:
,
, ?? ,

If interested, you can watch the complete presentation video below:
, ,
,
,
,  "
"
Most popular 
, tweets for Oct 29-30 were
,
Data Blending for Dummies - free ebook from Alteryx - explains how to access, clean, and join #BigData ,
,
13 Machine Learning Books recommended by Berkeley machine learning expert Michael Jordan @AnalyticBridge ,
,
The #strataconf debate: If You Can't Code, You Can't Be a Data Scientist - the ""agree"" side with 
, 
won easily ,
,
,
The #strataconf debate: If You Can't Code, You Can't Be a Data Scientist - the ""agree"" side with @hmason won easily ,
,
,
  "

























"
, tweets for last week, Nov 3-9 were
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
iMath Research, a start-up from Research Parc of UAB (Barcelona), offers access to intensive computational high technological solutions, such as big data analysis, for small and medium companies.
,
IMath Research is presenting their first product, 
,
In the initial phases, the creation idea, and its corresponding demo, was finalist at Code_n14 International start-up competition, and later on, it was selected as one of the 15 start-up winners at London Technology Week 2014. Now, iMath Cloud is published in beta version and it is available for data scientists in companies, universities and research centers.
,
Try it free at
,
,
,
iMath Cloud permits to develop in any Python programming language
,
iMath Cloud has three important features:
,
,
You can sign in iMath Cloud at iMath Research's official web site:
,
,.
,
Technical documentation:
,
, .
,
Send questions , to contact directly iMath Research's technical service.
,
,
,
,  "
"
        ,  "
"
, 
,
,
,, save 15% with code ""KDDNuggets"".
,
by Nikolaos Vasiloglou, Technical Committee Chair, MLConf, Nov 2014.
,
Machine Learning is changing the way businesses operate today, the same way that databases changed the corporate world more than 30 years ago. There are currently many prestigious conferences in the space, including: KDD, NIPS and ICML that cover recent inventions in machine learning.  Although information is readily available, time constraints prohibit busy data scientists from staying current with the various advances in ML. Even if somebody manages to attend all the sessions or go through all the proceedings, it is impossible to implement and discover the algorithms and business processes that really work and fit your needs.
,
This is the gap that MLconf is here to bridge. MLconf speakers come from top machine learning companies and share their experience from use cases and distilled knowledge of solutions to real machine learning problems. MLconf talks cover useful information such as: how Google is mining time series, how Yahoo is mining infinite streams of email data, which algorithms Twitter and Netflix are using for recommendations, or what type of data advertisers find useful for targeting. Apart from big established companies, MLconf hosts smaller startups that innovate in the ML space, attacking non-traditional high tech domains, like law and energy or developing new machine learning platforms and tools that are in great demand. At last, every MLconf always includes pioneers from research labs and universities that present the future of Machine Learning.
,
Although it is possible to fathom the the ML trends by reading blogs, articles or watching videos on the web from the MLconf speaker roster, attending MLconf offers you the opportunity to network with the speakers and a crowd of 300+ data scientists, business executives, engineers and grad students that deal with machine learning in their everyday life. All of that happening in a nice environment, with plenty of coffee, food, and about 70 titles of machine learning books to browse.
,
So, whether you are a busy professional or a student who wants to find out about Today's adventures in applied Machine Learning, it is worth investing a day for broadening your horizons in ML by attending MLconf in the city near you. 
,
, now for MLconf SF, which is this Friday, 11/14/14. Mention ""KDDNuggets"" and save 15% on tickets!  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
Despite outspending other countries per student, the US continues to lag behind global competitors in educational outcomes. Finding a better way to classify resources for schools and districts may help clarify different spending approaches and drive toward strategies to use the dollars we spend on education more effectively and efficiently.
,
ERS was formally established as a non-profit consulting firm in 2004. Their primary work is helping school districts use their resources more strategically. In this competition, the goal is to use data to predict aspects of a school's budget, and use that to make more effective decisions.
,
,
More specifically, the goal is to predict the likelihood of attaching a label (i.e., R&D, Recruitment, Legal, etc.) to a particular budget item using the features (mostly textual) provided in the dataset.
,
The prizes for the competition are as follows:
,
,
,
For more information, visit the , website.
,
,
,
,  "
"
Most popular 
, tweets for Nov 10-11 were
,
Statistical language R on its way to the top 10 - #TIOBE Index , #rstats #BigData #Statistics 
,
,
,
Using #MachineLearning to Detect Abnormalities in Time Series Data , #AzureML #Analytics 
,
,
,
Statistical language R on its way to the top 10 - #TIOBE Index , #rstats #BigData #Statistics 
,
Using #MachineLearning to Detect Abnormalities in Time Series Data , #AzureML #Analytics 
,
,  "
"
,
,Sydney, Australia
,
Feb 16 - 25, 2015
,
,
,
Important dates (AEDT):
,
,??,
,
,
Machine Learning is a foundational discipline that forms the basis of much modern statistical data analysis. As data science emerges to meet the challenges of ""Big Data"", understanding the theory and practice of machine learning becomes a crucial asset in academia and industry.
,
The machine learning summer school provides graduate students, academics and industry professionals with an intense learning experience on the theory and applications of modern machine learning. Over the course of eight days, a panel of internationally renowned experts in the field will offer tutorials covering basic as well as advanced topics. In addition, MLSS Sydney 2015 will feature hands-on sessions aimed at reinforcing the learned concepts through practice.
,
,
,
The school will have a strong focus on probabilistic inference, large scale learning, Bayesian non-parametrics and applications to recommender systems, vision and document analysis.
,
,
,
,??,
,
,
,??,
General inquiries should be directed to ,  "
"
        ,  "
"
Most popular 
, tweets for Nov 12-13 were
,
Japanese AI program did well enough on tests to enter 80% of private universities ,
,
Great writeup! Hacker's guide to Neural Networks and Machine Learning from Stanford PhD Student ,
,
Workday #BigData Analysis can predict which employees likely to leave 
, 
,
,
,
Workday #BigData Analysis can predict which employees likely to leave 
, 
,
,
,  "
"
,
,
,, the open source private cloud software firm focused on building clouds compatible with Amazon Web Services APIs, was acquired by HP in September 2014, citing common vision for the future of cloud in the enterprise. With this, Eucalyptus CEO Marten Mickos moved to HP, leading the HP Cloud organization and building out the , portfolio, based on OpenStack technology.
,
Marten recently delivered a keynote at , on ?€?Full victory for open source?€?. The video of his talk is available ,.
,
Here is my interview with him, preceded by his short bio:
,
, is the head of the HP Cloud business unit. In this role Marten leads the cloud strategy and oversees development of the HP Helion portfolio of cloud products and services, as well as the company?€?s involvement in the OpenStack technology and other open source communities.
,
Mickos joined HP in September 2014. Prior to HP, Mickos was CEO of Eucalyptus Systems, provider of the only AWS-compatible open source cloud computing platform. Before that as CEO of MySQL AB, Mickos grew that company from a garage start-up to the second largest open source company in the world, ultimately serving as head of Sun Microsystem's Database Group. 
,
Marten holds a M.Sc. in technical physics from Helsinki University of Technology in his native Finland.
,

,
 ,
, , had (and has) a very sharp focus: AWS-compatible private clouds that are very easy to install, use and operate. That focus allowed the company to grow.
,
,
 ,
, The Eucalyptus product will become part of the Helion family. This will allow for workload mobility to and from AWS. Additionally, HP will use the know-how that the Eucalyptus team possesses to make Helion OpenStack easier to use and operate, and to create various hybrid solutions.
 ,
,
 ,
, I see three main meanings of ?€?hybrid cloud?€?. As a vendor, we (HP) listen to our customers and let them define what the terminology means. The three main meanings are:
,
, ?? ,
Different hybrid environments require different hybrid solutions. In category 1 above, the key is to have the same API on the public and the private cloud. In category 2, you hybridize with a service catalog (such as HP Cloud Service Automation) that can deal with both environments. In the third category, you typically connect the two environments through the use of PaaS such as Helion Developer Platform. The PaaS layer masks the underlying differences in the clouds.
,
Nearly every customer will have a hybrid environment of one or the other type. That?€?s why vendors must have solutions to these hybrid requirements. The benefits are workload mobility. As a customer,  you can choose where to run your app workloads. This can save you money and make you more agile.
,
,
,
, You should always select vendors who are aligned with your own strategic goals. If you believe in open source, you need to select a vendor that is committed to open source. If you want full deployment flexibility, you need to choose a vendor that can offer private cloud, managed cloud and public cloud. If you are a large organization, you should select a vendor that can help and guide you on your journey to cloud.
,
Second and last part of the interview is ,.
,
,
,  "
"
,
,
,
Our guest Dan Ariely, author of best-sellers , and ,, uses simple experiments to study how people actually act when making real-life decisions. His interests span a wide range of daily behaviors such as buying (or not), saving (or not), ordering food in restaurants, pain management, procrastination, dishonesty and decision making under different emotional states. His TED talks on these topics have been viewed more than 4.8 million times.
,
,
,
,
,
,??,
On-demand webcasts featuring additional thought leaders are also available.
,  "
"
, in conjunction with??,.
,
The challenge is about predicting the items that a user will buy given a sequence of clicks he performed within an online shop. It is a real-world problem, and the dataset is coming from a big retailer in Europe. It is an opportunity to prove that you can deal with a large scaled and heterogeneous noisy data set and two fold recommendation prediction.
,
All the information, such as downloading the dataset and submitting the solution, is available at the challenge website: ,.
,
The challenge is managed and hosted by ,.
,
We encourage all, industry and academia members, to participate in the challenge and submit solutions. Money prizes are set and publication of selected and winning solutions is guaranteed.
,
We looking forward to seeing your team score in the leaderboard.
,
Good Luck,,
YooChoose Team,,
2015 RecSys Challenge Organizers
,
,
,
,  "
"
By Grant Marshall, Nov 2014
,
Slideshare is a platform for uploading, annotating, sharing, and commenting on slide-based presentations. The platform has been around for some time, and has accumulated a great wealth of presentations on technical topics like Data Mining.
,
,
,
,
Today, we will look at some of these top Data Mining presentations found on Slideshare. These presentations were retrieved by using a Python script and the , API, and then hand-curated to select the best, most relevant presentations. The slideshows and their associated metrics are shown below:
,
,
,
Here are some quick stats about the 24 slideshows in this table: there is an average of approximately 29,000 views, 621 downloads, 5 comments, and 38 favorites per slideshow. These aggregates can be deceptive, however.
,
With the comments, for example, a large number of these comments came from , and ,. In the first case, most of these comments were requests for the slides (the author chose to disable downloads) and in the second case, most of the comments were requests for code that was excerpted in the presentation. Similarly, because some presentations had downloads disabled, the average download count is misleading. Adjusting for these comments and some downloads being disabled, the real averages are approximately 994 downloads and 3 comments per presentation.
,
Regardless, it appears that the social features are being put to use, and with some presentations, , for example, the author can be seen responding to the comments. This shows how the format provides an interesting potential for people to interface with experts in data mining.
,
,
,
,
In this chart we see the diversity in the audiences that these different slideshows can draw. While there is a general upward trend in the number of favorites compared to the number of views, there are some exceptions. My hypothesis is that the more generally applicable lectures, like ,, might draw a larger general viewership, the viewers will be less likely to favorite the slides. On the other hand, a more specific slideshow like , might not draw as large of a general crowd, but the viewers may be more likely to favorite it.
,
,
,
,  "
"
,
,
,, the open source private cloud software firm focused on building clouds compatible with Amazon Web Services APIs, was acquired by HP in September 2014, citing common vision for the future of cloud in the enterprise. With this, Eucalyptus CEO Marten Mickos moved to HP, leading the HP Cloud organization and building out the , portfolio, based on OpenStack technology.
,
Marten recently delivered a keynote at , on ?€?Full victory for open source?€?. The video of his talk is available ,.
,
,
,
Here is the second and final part of my interview with him:
,
,
,
,: First I could perhaps state that a benefit of open source is that you are aligned with your competitors, ,not just differentiated. You share a lot of basic platform technology. We work very closely with Red Hat, for instance, on hardening the OpenStack code.
 ,
Once in a commercial environment where we sell our offerings, we are of course happy to compete against anyone. HP?€?s open source strategy is more comprehensive than that of other vendors. We also focus more on reliability (including high availability) and on security.
 ,
,
 ,
,: I have always said that open source is not a business model. It is a production model.
 ,
The main difference is that 10 years ago, open source was novel and only the bravest would use it. Today, enterprises state open source as one of their first criteria for any infrastructure they will use.
 ,
,
 ,
,: Yes, absolutely. The LAMP stack won the web war, marginalizing players like Microsoft and Oracle in the market for web infrastructure. Now we are about to do a similar thing in cloud. We must build a strong ecosystem of open source projects, and we must encourage each project or product to excel in its own area. Too much coordination is not good; each project must succeed on its own merits. But we also cannot let the overall architecture become sloppy. So we must have a clearly thought-out balance between freedom and control.
 ,
,
 ,
,: We have great tools and processes for handling code commits, bug reports, bug fixes, and continuous integration and deployment (CI/CD). But the art of designing the architecture of a software product is still just an art form. Ideally we should invent new tools and governance processes for achieving the best possible software architecture in a meritocracy. It?€?s not easy, but I think it can be done.
 ,
,
 ,
,: Everything is changing in the shift to cloud, so the list will be very long! But perhaps an area that deserves special mention is the telecom side of cloud. We see huge focus on Network Functions Virtualization (NFV) these days. This bodes well for carriers, long-term. Another hot topic is containers. They bring a fresh new approach to the granularization of application workloads.
 ,
,
 ,
,: Jump in, learn fast, and don?€?t be afraid of making mistakes!
, ,
,
,
,: , by Richard Rumelt. A brilliant book.
,
Mostly I just work. But when I don?€?t work, I read books, I enjoy photography, I ride my bike, and I go hiking. And I enjoy good food and wine in the company of great friends.
,
,
,  "
"
By Emily Ritter, Nov 2014
,
, is a free, interactive tutorial built by Mode Analytics. Unlike many existing SQL tutorials targeted at database administrators or engineers, SQL School is written by analysts for aspiring analysts.
,
,
,
The shortage of data scientists and analysts shows no sign of letting up any time soon. Programs like 
,
and 
,
are popping up to train data scientists and resources like Clare Corthell's 
,
help folks going at it alone. But there's another formidable solution all around us: increasing data literacy in roles throughout our companies. And that starts with teaching SQL.
,
R and Python may be hot topics in data science education, but SQL can be the most empowering step towards working with data. More than any other tool, SQL helps analysts quickly gather and understand data, even if the task ultimately demands other tools. Plus, SQL logic can feel familiar to Excel power users. By ""teaching others to fish,"" analysts can free up time for more complex data tasks, simply by decreasing requests for simple SQL queries and data pulls.
,
As we began building Mode, a SQL-based analytics tool, customers often mentioned workshops they've created to teach peers-and repeated a common frustration with the lack of SQL resources focused on analysis. As a side project, we created SQL School to fill this void. By leveraging data hosted in Mode's public data warehouse, each lesson encourages practice with real data rather than simply explaining syntax. For example, students work with Top 40 music data as they learn about operators such as 'LIKE' and 'OR' and college football stats as they master joins. 
,
,
,
Like any language, syntax knowledge doesn't equate mastery. SQL learners need practice answering real-life questions to develop analytical thinking skills. In talking with students who've used SQL School to train for interviews and land dream jobs, we've heard that it's difficult to prepare for life as analyst. They find the biggest hurdle to be one of access: proprietary company data simply doesn't exist publicly. We're filling this void with the help of the Mode community by building fake data sets and case studies to mirror on-the-job situations.
,
Take a moment to check out SQL School at , and share it with folks looking to learn new skills. If you've recently tackled an interesting problem using SQL, heard a challenging SQL-based interview question, or would like to join us in giving back to the burgeoning analytics community, we'd love to hear from you. 
Drop us a line at ,.   "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
In statistics, bootstrapping can refer to any test or metric that relies on random sampling with replacement.?? In simple terms, it allows a way to measure the accuracy of the sampling distribution often used in constructing a hypothesis test.?? In business, bootstrapping refers to starting a business without external help or capital.?? Bootstrapping in general parlance refers to an absurdly impossible action, ?€?to pull oneself over a fence by one?€?s bootstraps.?€??? R and Hadoop are very much ,technologies having received zero direct investment capital and relying on what might appear to be a random group of contributors over the past 20 years in practically every industry and use case imaginable.
,
,
,
R first appeared in 1993 when Ross Ihaka and Robert Gentleman at the University of Auckland released a free version as a software package.?? Since then, R has grown to over 3 million users in the US alone according to the download site , last year.
,
In addition, R surpassed SAS with over 7,000 unique packages you can view on ,.?? It is no wonder it has found wide use in many industries and academia. In fact, during the summer of 2014, R surpassed IBM SPSS as the most widely used analytics software for scholarly articles according to ,.?? For this reason, R is now the ?€?Gold Standard?€? for doing all sorts of statistics, economics, and even machine learning.?? Furthermore, from my experience I found that many if not most people use R as a complimentary tool today for spot checking their work even when using other far more expensive or popular enterprise software.?? It is no wonder R is quickly taking over as the go to tool for Data Scientists in the 21st century.
,
What is fueling R growth is predominantly the community for making the core software useful and relevant by providing answers to common questions via many blogs and user groups.?? In addition, it is clear there is an underserved job market according to data from LinkedIn (see image below).?? Due to this demand, R is now offered in practically all major universities as the de facto language for statistical programming and many new online courses are starting each day.?? , is one such example having built an interactive web environment with rich lessons that non-programmers can easily get started without ever touching a command line.
,
,
Businesses too are flocking to statistics and embracing the probabilistic vs the deterministic nature of problems that arise when data is expanding at an increasing size and rate where tradition Business Intelligence cannot keep pace.?? For this reason, many turned to Hadoop to open up the data platform to unlock the world of enterprise data management that had been kept away from business analysts for many years.?? Gone are the days of pre-filtered, pre-aggregated dashboards and excel workbooks that are emailed around haphazardly to executives and decision makers left to little interpretation or devoid of any ?€?storytelling?€? to guide the business to make informed decisions.
,
,
,
Apache Hadoop came almost 10 years after R first hit the scene in 2005 and wasn?€?t widely adopted until as late as 2013 when more than half of the Fortune 50 got around to building their own clusters.?? The name ?€?Hadoop?€? is after a toy elephant of famed Yahoo! engineer Doug Cutting who along with Mike Cafarella originally developed the technology to create a better search engine, of course. Along with its ability to process enormous sums of data on relatively inexpensive hardware, it also made it possible to store data on a distributed file system (HDFS) without having to transform it ahead of time.?? As with R, many open source projects were created to re-imagine the data platform. Starting with getting data into HDFS (sqoop, flume, kafka, etc.) to compute and streaming (Spark, YARN, MapReduce, Storm, etc.), to querying data (Hive, Pig, Stinger / Tez, Drill, Presto, etc.), to datastores (Hbase, Cassandra, Redis, Voldermort, etc.), to schedulers (Oozie, Cascading, Scalding, etc.), and finally to Machine Learning (Mahout, MLlib, H2O, etc.) among many other applications.
,
Unfortunately, there is not a simple way to see all of these technologies and easily install with one line of code like R.???? Nor is MapReduce a simple language for the average developer.?? In fact, you can clearly see the shortage like R of Hadoop and MapReduce skilled workers to the number of jobs available thanks to LinkedIn.?? It is for this reason Hadoop has not fully caught fire in the same way R has and there is talk of its demise at the recent Strata Hadoop World conference in NYC this past fall.
,
,
,
,
,
Over the past few years, the issues of data have cropped up in the field of data science as the , faced when working with the vast variety and volumes of data.?? I?€?d be remiss to not mention the velocity of unrelenting data waves crashing against our fragile analysis environments.?? In fact, it is projected that the volume of data is expected to exceed the number of stars in the universe by 2020 ,.?? Fortunately, there is an entirely new approach to this problem that has until now escaped us in our persistent habit of wanting to constrain data to our querying tools.
,
,
,
Put simply, ?€?Machine Learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data.?€? It is a quantum shift in the standard way of simply counting things; instead, its the start of a fantastic journey into the deeper pools of the unknown.
,
,
,
So here comes the really interesting part of the story.?? According to my LinkedIn analysis, Machine Learning and Data Science are actually very well matched to the overall demand in the job market to the people available (unlike R and Hadoop).
,
We?€?ll need another plot to really understand what is going on here of the actual number of jobs that exist for Data Science vs Machine Learning.?? My interpretation of this graph is that the job of a data scientist today is synonymous to that of an analytics professional or analyst and the real opportunity is in the growing area of Machine Learning.
,
,
,
,
Data Science was first described as the intersection of programming or ?€?hacking skills?€?, math and statistics along with business expertise according to Drew Conway?€?s ,.?? As it turns out, programming is too generic a term and what is really meant is applied math to large scale data through new algorithms that can crawl through this tangled mess.?? To search for answers in this jungle, simply flying over the canopy will not reveal the treasure boxes hidden just beneath the canopy. It is evident to me from the number of machine learning projects that have cropped up and the maturity of the market accepting probabilistic information not only deterministic marks a new era in the race to find value in our data assets.
,
,
,
,
,
Many people have tried to claim that Hadoop 2.0 had arrived with MR2 or YARN or high-availability HDFS capabilities, but this is a misnomer when considering the similarly named Web 2.0 that brought us into the age of the web applications like Facebook, Twitter, LinkedIn, Amazon, and the vast majority of the internet.?? According to John Battelle and Tim O?€?Reilly of now Strata fame defined the shift as simply ?€?Web as a Platform?€? meaning software applications are built upon the Web as opposed to the desktop. Hints of this change is coming from Apache Spark making it easier to develop applications on Hadoop and , making it finally possible for R and Hadoop to work together.?? It is clear to me having worked in the ?€?Big Data?€? industry for some time, software developers and statisticians want to program in their language, not MapReduce.?? It's an exciting time to move off the desktop and onto the cluster where the constraints are lifted and the opportunities are endless.
,
As an open source project like R and Apache Hadoop, H2O must not only demonstrate continuing value to the community, but also stand up to the most stringent industry use cases imaginable.?? Having started in 2012, H2O focused first on creating the community through over 120 meet-ups to grow the user base to over 7000 strong.?? H2O is a bridge for R users to move beyond their laptop and onto a large scale environment to train data that doesn?€?t fit inside the memory of their laptop.?? It is a fundamental shift in how we approach the data problem and is producing reliable results for numerous production customers ranging from retail to insurance to health to high tech.?? To learn more about how you can get started go to , to get started for free.
,
,
,
Many people have conducted research as of late on the growing popularity of statistical software using , like academic research citations, job posts, books, website traffic, blogs, surveys like KDnuggets annual ,, GitHub Activity and many more.?? However, all of these methods have generally been focused on the technical crowd.?? Where the rubber meets the road is in the business context which in my mind LinkedIn represents as it is highly representative of the business world.?? Further, if you want to go to an even more general audience you can perform the same trick with Google Adwords.?? Reverse engineering Ad platforms is a good way to get back of the envelope market sizing information.?? I wrote a complete blog on this subject on my ,.?? In the following instructions, I?€?ll walk through how I used LinkedIn as my sample and R to analyze the business market for my analysis above.
,
,
,
There are two ways to gather data from LinkedIn.?? One is to use the ad shown in the left image below or the other is to use the direct search functionality shown to the right below.
,
,
,
In this case, I went the manual route and used the search function. For each product category that I search there is a count of results that show up and use that as a proxy for demand.?? See below:
,
,
,
From this example, we can see the phrase ?€?R Programming?€? has 1720 results.?? I?€?ve also included ?€?R statistics?€? and other ?€?R?€? relevant terms.
,
,
,
As a new R user myself, I manually created each data frame to hold the data by first creating the individual vectors for the people and jobs:
,
,
,
after I created a simple data frame:
,
,
,
to get the ratio, I simply use the transform function:
,
,
,
,
,
R comes with many visualization packages, the most notable one being ggplot2. ??For this situation, I used a built in barplot as it was much easier out of the box.?? Frankly, the visualizations that are produced in R may not seem the most compelling to the general audience, but it does force you to consider what you?€?re plotting making for more informed visuals.
,
to get the bar graph (in H2O colors):
,
,
,
Thats it! Pretty simple and I am sure there are ways of doing this analysis more elegantly, but for me this was the way that I can be sure the analysis makes sense.
,
To get the full script you can , to try yourself or add to my analysis.
,
,
,
,
,
,
 ,  "
"
By Seth Grimes (AltaPlana), Nov 2014.
,
Next month?€?s , will be the third occasion I?€?ve invited , to speak at a conference I?€?ve organized. She?€?s that interesting a speaker. One talk was on Goal-driven Sentiment Analysis, a second on Fusing Sentiment and BI to Obtain Customer/Retail Insight. (You?€?ll find video of the latter talk embedded at the end of this article.) Next month, at LT-Accelerate in Brussels, she?€?ll be speaking on a particular topic that?€?s actually of quite broad concern, ,.
,
As part of the conference??lead-up, I interviewed Lipika regarding consumer and market analytics, and ?€? given her research and consulting background ?€? techniques that best extract practical, usable insights from text and social data. What follows are??a brief bio and then the full text of our exchange.
,
,

Dr. Lipika Dey, senior consultant and principal scientist at Tata Consultancy Services
,
, is a senior consultant and principal scientist at ,, India with over 20 years of experience in academic and industrial R&D. Her research interests are in content analytics from social media and news, social network analytics, predictive modeling, sentiment analysis and opinion mining, and semantic search of enterprise content. She is keenly interested in developing analytical frameworks for integrated analysis of unstructured and structured data.
,
Lipika was formerly a faculty member in the Department of Mathematics at the Indian Institute of Technology, Delhi, from 1995 to 2006. She has published in international journals and refereed conference proceedings. Lipika has a Ph.D. in Computer Science and Engineering, M.Tech in Computer Science and Data Processing, and 5 Year Integrated M.Sc in??Mathematics from IIT Kharagpur.
,
,
,
Q1: The topic of this Q&A is consumer and market insight. What?€?s your ??personal background and your current work role, as they relate to these domains?
,
,
,
Q2: What roles do you see for text and social analyses, as part of comprehensive insight analytics, in understanding and aggregating market voices?
,
,
,
Q3: Are there particular tools or methods you favor? How do you ensure business-outcome alignment?
,
,
,
Q4: A number of industry analysts and solution providers talk about omni-channel analytics and unified customer experience. Do you have any thoughts to share on working across the variety of interaction channels?
,
,
,
Q5: To what extent does your work involve sentiment and subjective information?
,
,
,
Q6: How do you recommend dealing with high-volume, high-velocity, diverse data ?€? to ensure that analyses draw on the most complete and relevant data available and deliver the most accurate results possible?
,
,
,
Q7: So what are the elements of that rethinking and that prioritization?
,
,
,
Q8: Could you say more about??data and analytical??challenges?
,
,
,
Q9: Could you provide an example (or two) that illustrates really well what your organization and clients have been able to accomplish via analytics, that demonstrate strong ROI?
,
,
,
Q10: I?€?m glad you?€?ll be speaking at ,. Your talk is titled ?€?E-mail Analytics for Customer Support Centres ?€? Gathering Insights about Support Activities, Bottlenecks and Remedies.?€? That?€?s a pretty descriptive title, but is there anything you?€?d like to add by way of a preview?
,
,
,
Thanks Lipika, for sharing insights in this interview and in advance for your December presentation.
,
Original:
,
,.
,
,
,
,  "
"
,By Gregory Piatetsky,  
,, Nov 17, 2014.
,
,
This poll is closed - poll results coming soon.
,
Here are the results of past polls:
,
  "
"
Most popular 
, tweets for Nov 09-16 were
,
,
8 Steps to Becoming a #DataScientist , #BigData #MachineLearning #DataMining #Practice #Engage 
,
,
8 Steps to Becoming a #DataScientist , #BigData #MachineLearning #DataMining #Practice #Engage 
,
,
Learn How Google Understands You ,  #Infographic #BigData #Search #Analytics 
,
,
A Very Good #DeepLearning Tutorial , #MachineLearning
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Nov 18 and later.
,
See full schedule at , .
,
,  "
"
,

,

,After the success of its Hadoop-based Azure HDInsight and PowerBI for Office 365, Microsoft took yet another step towards market leadership in Big Data through the public preview release of Azure Machine Learning (also known as ""Azure ML""). Taking the predictive analytics to public cloud seems like the next logical step towards large-scale consumerization of Machine Learning. Azure ML does just that, while making it significantly easier for the developers. The service runs on Azure public cloud, which means that users need not buy any hardware or software; and also, need not worry about deployment and maintenance.
,
Through an integrated development environment called ML Studio, people without data science background can also build data models through drag-and-drop gestures and simple data flow diagrams. This not only minimizes coding, but also saves a lot of time through ML Studio's library of sample experiments. On the other hand, seasoned data scientists will be glad to notice how strongly Azure ML supports R. You can just drop existing R code directly into Azure ML, or develop your own code using more than 350 R packages supported by ML Studio.

,

Azure ML is built on top of the machine learning capabilities of several Microsoft products and services. It shares many of the real-time predictive analytics of the new personal assistant in Windows Phone called Cortana. Azure ML also uses proven solutions from Xbox and Bing. Outshining Nate Silver's lauded FiveThirtyEight blog, Bing Predicts recently astonished many by ,. Thus, it might be worth checking out Azure ML to see what its powerful cloud-based predictive analytics can do for you.

,

During the Strata+Hadoop World Conference in New York a month ago, Azure ML was being compared to IBM's Watson. In his keynote, ,, Microsoft CVP of Machine Learning, announced ,, which has evolved significantly over the last few years (since its launch in October 2010). Azure Marketplace hosts various exciting APIs that use ML, including the Bing Speech Recognition Control, Microsoft Translator, Bing Synonyms API and Bing Search API. As of today, Azure Marketplace has 25+ machine learning APIs.
, 
,

The Marketplace is a convenient platform for data scientists to build custom web services, publish APIs and charge for its usage. Azure ML users can search for these APIs and subscribe to them. Thus, along with other Analytics Marketplaces, Azure Marketplace is another good platform for data scientists to monetize their expertise and creativity, very similar to how developers do on iOS or Android app stores.

,

In the efforts to boost the adoption of the still new Azure ML service, Microsoft has taken several steps to make it easier for people to try it. Recently, Microsoft executives at the PASS Summit in Seattle, announced a free tier of access to Azure ML (i.e. now you can try Azure ML without providing any credit card information). This has encouraged DBAs, developers, BI professionals and amateur data scientists to try Azure ML for building models to incorporate into applications. Besides, Microsoft also launched the , offering research grants to both seasoned researchers and students to try Azure ML.

,

Here is a short video on how Azure ML solves the common challenges in developing and deploying predictive analytics projects:

,

,

,
If you are interested in trying Azure ML, there are a lot of resources on , to help you get started.

,

Microsoft Azure Machine Learning as a Service has gone a long way since its launch in June 2014 and it sure looks like Microsoft is investing on its long term success.
,
,
,  "
"
        ,  "





"
,
,
, recently joined Microsoft from Amazon where he was the VP for the Global Inventory Platform and CTO of the core retail business. In this role he had responsibility for the science and software behind Amazon?€?s supply chain and order fulfillment systems, as well as the central Machine Learning group which he built and led.
,
During his 9 years at Amazon, he managed a variety of teams including forecasting, inventory, supply chain and fulfillment, fraud prevention systems, data warehouse and a novel data-driven seller lending business. Prior to Amazon, Joseph worked for Fair Isaac Corporation as VP of R&D. Joseph is passionate about Machine Learning and its applications and has been active in the field since 1990.
,
Here is my interview with him:
,
,

,

,: Thank you. Our customers are most excited by how fast they can deploy a machine learning solution. That?€?s where Azure Machine Learning is really ,differentiated. The integrated experience between data sources, modeling, and operationalizing to a web service allows deployments to take hours or days, when they would have previously taken months. For instance, we launched a collaborative effort with Dell Statistica and Azure Machine Learning recently. The Dell team was able to create a working integration with Statistica the same week they met with us and learned about the service for the first time. Customers have also been interested in being able to put R code into production as REST APIs.
,
,
,
The service is production ready today, as evidenced by customers and partners such as , Imports, , and ,. But we are continuously adding new features. For instance, we introduced a machine learning category in the , recently. Solutions on this marketplace, such as recommendations and forecasting APIs, are built by Microsoft data scientists and our partners and customers. This category in the Marketplace is designed for anyone to be able to post and monetize their solution.

,

We?€?re also always working to reduce the barriers to access the service, so we recently made available a , that allows users to try Azure Machine Learning with just a Microsoft account ID.

,
We?€?re continuously updating Azure Machine Learning, so keep an eye on the , for the latest updates and other posts about machine learning at Microsoft.
,
,
,
,One of my favorite use cases is , as it shows the power of machine, learning and how it can impact everyone?€?s daily lives in positive ways. I take an elevator every day to my office and I don?€?t think about it -- I just assume it will work flawlessly. For ThyssenKrupp, ensuring millions of elevators worldwide are maintained in top notch condition is a huge opportunity. Through the power of Internet of Things and machine learning, they can predict and prevent an elevator maintenance issues before it even happens so they can service the elevator proactively. This allows them to both reduce maintenance cost and improve customer service -- a combination that is hard to achieve.
,
,

,

,It?€?s really our Azure customers that are benefiting from this new capability. With Azure Machine Learning, customers who have SQL Server data in a VM, Azure SQL Database, Azure Table ,storage, Azure Blob storage or Azure HDInsight can now easily bring in that data into our modeling studio and perform predictive analytics. This is especially valuable for our Azure HDInsight customers who are managing and seeking to get business value from huge quantities of data. By using Azure Machine Learning to build predictive analytics APIs on the cloud, any application on any device can use predictive intelligence. ??Users of business intelligence tools such as Power BI and Excel can easily call these APIs -- whether on-premises or on the cloud -- and integrate predictive analytics into dashboards and visualizations.
,
,


,
,Azure Machine Learning is focused on empowering data scientists and developers to deliver great business value by operationalizing predictive analytics very easily using their domain knowledge and business context. To enable a rich collection of capabilities for them, we have built open source tools such as R into our solution, with over 400 R packages provided for use in ,our product, plus the ability to easily drop in custom R code. The resulting model can then be easily operationalized as a web API for any end application to use, anywhere, for driving business outcomes. We created a , recently that describes how this solution empowers data scientists.


,
Our partners and customers have taken this ball and run with it, developing custom solutions for predictive maintenance, customer churn, fraud detection, online marketing optimization, and other business challenges. Over time, as we engage with solutions for several industries, we expect to create toolkits that make industry-specific solution building much easier.
,
Second part of the interview will be published soon.
,
,
,  "









"
,
By Gregory Piatetsky,  
,, Nov 18, 2014.
,
,
Earlier this month I moderated a panel on the ""Frontiers of Analytics"" at
,, Boston Data Analytics Conference, which was organized
by Vishal Kumar,
,.
,
We had a great team with 5 panelists from Boston-area companies that are prominent in analytics and Big Data:
,
,??,
Unfortunately, we did not have enough time to adequately get into the issues.
,
To share their wisdom with KDnuggets readers, I have asked the panelists to send me the answers to 2 questions we discussed at the panel, and a third question added later: 
,
,
Some of the important themes that emerge from their answers are 
the importance of machine learning, dealing with streaming data, 
unstructured and text data, data preparation, 
security, and privacy.
,
Here are the full answers, in alphabetical order of panelists.
,
,, a  data scientist at DataXu, an ad-tech company in the programmatic marketing space. He works on projects related to machine learning and data mining. His background is in Biology and text mining.  He has published a number of journal articles and medical notes from hospitals.
,
,
,
,
DataXu uses machine-learning algorithms to make decisions about the correct ads to show to an audience. This happens in less than 100 milliseconds, because these decisions need to be made while the page is being loaded on the user's end.
,
Traditional ""direct response"" digital campaigns tend to focus on a single goal, be it to drive clicks, a purchase, or a conversion action like ""request a quote."" Increasingly, I've noticed more advertisers, especially ones focused on improving awareness or favorability for their brands, are looking to optimize towards multiple objectives together, and that's what I find to be the most interesting frontier. For example, was the ad in view on the user's end (think above the fold/below the fold), or did the ad serve to a bot, or did the ad show up on an appropriate publisher. Often, these need to be optimized simultaneously. Since we use machine learning at DataXu, this involves having multiple algorithms optimizing towards multiple objectives on the same opportunity to show an ad, and making a decision based on the priority of the objectives.
,
,
,
,
There are a couple of frontiers that I find to be extremely important.
,
,
ETL (Extract, Transform, Load) is a process through which logs that are collected over some time period (hour/day/week) are read (Extract), parsed (Transformed), and loaded onto a database (Load), so that they are available to downstream analytics processes or decision engines. Most companies, including DataXu, run this process either hourly or daily. However, in a real world setting, where thousands of transactions are taking place every second, an hourly or daily delay in having the latest information might be unacceptable. A case from DataXu's perspective: if we try to serve an ad impression, and discover that the ad was served to a fraudulent bot, then 20 seconds later, when that bot tries to generate another fraudulent impression, we want to be able to block it based on the data gathered from the first impression.
,
,
In marketing, as in other fields, we see different sources of data, and they can all have different attributes. For example, an ad opportunity on a mobile device will include information on the device vendor, or carrier, which won't be seen on an ad opportunity seen from a web browser on a laptop. Or, we might start getting a new attribute that we had not seen before. In the current scenario of a schema-based data warehouse, we expect all the attributes to be known and defined before we start receiving data. This makes it difficult to adapt to the changing data. Also, adding new ""columns"" to handle a specific attribute (think carrier for mobile) means that for data where this information is not present, you end up filling nulls.
,
Schema-less architectures, such as the MongoDB and CouchDB are based on JSON and avoid these issues. However, one must be careful to ensure that attributes that can be normalized are being normalized. For example, geo location in Europe might include an attribute ""postal_code"", whereas in the US, it includes ""zip_code"", which are essentially the same thing, and should be normalized.
,
,
,
,
Many companies feel that they need to jump on the big data bandwagon, and hire a team of data scientists, when in reality, the amount of data they have can fit on a single hard drive or even in memory. This results in them not deriving the value that they originally anticipated, and could result in them discounting the true value of big data and data science. Moreover, even if the data is truly large, and stored in a data warehouse, not extracting actionable insights from the data is also a risk. 
,
,
,
,
,,
,, is a Strategy and marketing lead at Tamr, a starup offering Big Data curation, and a founding board member Cloud vLab. 
Short bio: Nidhi Aggarwal leads strategy and marketing at Tamr. Prior to joining Tamr, Nidhi founded Cloud vLab, makers of qwikLAB, a software-learning platform used to create and deploy on-demand lab environments. In the years before Cloud vLab, Nidhi worked at McKinsey & Company, advising Fortune 150 companies on Big Data Strategy. Nidhi holds a Ph.D. in Computer Science from the University of Wisconsin-Madison.
,
,
,
,: 
The most important frontier of analytics/Big Data Tamr is solving is simplifying the process of preparing the data for consumption by analytics and visualization tools. Most of the time is spent stitching the data from various silos, cleaning it, transforming it. This process if called curation. Tamr dramatically reduces the time and effort required to do curation and speeds up the time to analytics. Putting data in data lakes gives the illusion of unification but those data lakes can turn into data swamps really quickly. 
,
,??,
,
,
,: 
Tamr is plumbing 
,)
so I am really excited about companies that are upstream and downstream for us. For eg. Basis Tech for unstructured data, Recorded Future for predictions based on data.
,
,
,
,: 
The most significant risk is a lack of understanding of the process of going from raw data to analytics and not putting together a scalable process to get the data prepared. 
,
,
,
,, CTO and co-founder of Sqrrl, is responsible for ensuring that Sqrrl is leading the world in Big Data Infrastructure technology. Previously at NSA, Adam was an innovator and technical director for several database projects, handling some of the world's largest and most diverse data sets. He is a co-founder of the Apache Accumulo project. 
,
,
,
,:
At Sqrrl, we feel that Linked Data Analysis stands to change the game for making sense out of Big Data by putting it in context.  What that involves is transforming raw data into linked information, and being able to explore and traverse the intricate relationships between different entities of interest.  Not only that, but the attribution of derived knowledge back to its original source is a key feature in terms of data quality and provenance.  There are obviously back-end technologies that are instrumental in making this happen, but just as important are effective ways to visualize and interact with the data and analytic results.  These are all key challenges we're tackling today.
,
,
,
,:
Unstructured entity disambiguation is a huge area of analytics that includes, among other things, finding and normalizing concepts found in corpuses of human language text. Figuring out that the ""Bill"" in one document refers to the same being as the ""William"" in another relies on decades of research in automating what humans are naturally pretty good at. The best approaches to this problem today require scalability to corpuses in the Big Data range. Basis is doing some great work in this area, as well as another Boston company, Diffeo.
,
Another area of analytics that is dear to my heart is software analysis. This area includes proving that certain properties hold of programs and detecting behaviors that may not match expectations for what a program should do. From a cybersecurity perspective software analytics can be used for both attach and defense operations, detecting and exploiting vulnerabilities in the Big Data space of code that makes up a typical enterprise network. This is an area where I would like to see more commercial research and development. Many of the top cybersecurity firms are working in this problem space, but I think computer science departments at many universities are still leading the charge.
,
,
,
,:
Obviously, given our heritage in the US Intelligence Community and current focus on cybersecurity, we feel that security is the biggest risk to Big Data and Analytics projects today.  When you consolidate all your data in one place, as is the case with many Big Data efforts, you are compounding the risk of protecting that data.  Coupled with the increased sophistication of motivated cyber attackers, many new Big Data efforts seem ripe for the picking as a potential target.  As an industry, we're lacking both ways of securely keeping/accessing data as well as good tools for quickly investigating the impact of security incidents at scale.  At Sqrrl, we're addressing both of these concerns.
,
,
,
,, Chief Architect, Nutonian, responsible for driving the strategy and development of Nutonian's suite of cognitive computing products. Previously, as an Engineering Lead at Vertica (acquired by HP), Andrew designed, implemented and supported the distributed query planning and execution engine, SQL language features and performance optimizations for the flagship Vertica Analytic Database. 
,
,
,
,:
Lowering the skills needed to go from data to understanding and actionable insight. Specifically, Nutonian is focused on automating the process from data to insight with *software* rather than throwing more people at the problem. Other modeling techniques may give you predictive power, but you have to know which to try (or how to evaluate multiple options), and interpreting the results into English that you can understand what is going on with your business.  
,
,
,
,:
,
,??,
,
,
,: 
Security - as we continue putting all this data into centralized systems that make it easy to understand and access, those systems will become very tempting targets for hackers and others.
,
,
,
,, VP, Engineering at Basis Technology which he joined in 2005. He leads the engineering team responsible for text analytics. He has been building natural language processing systems since 1998.
,
, 
,
,:
Basis's expertise is in analytics of unstructured data, especially text. The frontier we're addressing now is better integrating unstructured and structured data by focusing on entity-centric systems, or as some say 'things, not strings'. That is, by resolving chunks of text to references of real-world people, places, organizations, products etc., analytics can be done over those items rather than by counting words and the like. Transforming the data to this level allows more effective user-computer collaboration since users think in terms of those entities not specific variations of their names. An example of this work is our recently launched Entity Resolver (,).
,
, 
,
,:
One important frontier is the ever more nuanced trade-off between privacy and features. An example is the advent of Ello as an alternative to Facebook. Airbnb vs a hotel is a similar dichotomy with a differing motivation for decreased privacy: more trust in the case of Airbnb vs. more interaction in the case of Facebook.
,
Another frontier is analytics for social good. Firms like , and 
, are good examples. Hopefully the continuing advance and simplification of open source tools developed for more commercial applications will enable these sorts of uses.
,
,
,
,:
Commercial analytics projects need to avoid appearing 'creepy'. Individuals anthropomorphize companies so when it's revealed that a company has been tracking an individual across many channels the individual naturally thinks 'this feels like stalking'. Giving users transparency, control and clear value from the use of their data seem important steps to address this.
,
,
,
What do you, readers, think? 
,
,
,
,??,
,
,
 ,  "

"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions (note: Las Vegas in December is guaranteed to run)
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
        ,
,  "
"
,
Latest ,, (Nov 19, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
When you consolidate all your data in one place, as is the case with many Big Data efforts, you are compounding the risk of protecting that data. Adam Fuchs, CTO of Sqrrl, ,.  "
"
By Edward Rotenberg, Nov 2014.
,
The final word in decision making belongs to humans. To simplify understanding of machine algorithms outputs or arbitrary data sets, we need to take advantage of the most advanced of all the human cognitive systems ?€? the visual cortex. Our version of the Visual Data Discovery Framework attempts to do just that.
,
To visually present different types of data, we employ a Unified Visual Data Representation Space (UVDRS), where a data set is visually represented as a set of objects and relations between them. This is high level of abstraction and it let us represent almost any data set. We discover data ?€?insight?€? by finding meaningful relations in data sets together with maximizing amount of information about data objects inside the same screen space.
,
The UVDRS design was prompted by searching for potential linkages between even the most unthinkable relations. Initial studies of transformative discoveries such as Nobel Prize winning discoveries are particularly promising.
,
To further simplify visual recognition of the relative strength of relations, we moved our Discovery Framework to a 3D space where we can:
,
,
In some cases the UVDRS is the natural abstract data representation, especially when it comes to financial portfolio visual modeling. It even has a commonly used name: visualization of portfolio diversification.
,
There is one other important application - it is very difficult to detect, explain or predict , composed of multiple simultaneous local events and involving multiple relations amongst multiple distributed domain objects. The financial ?€?bubbles?€? are examples of such events. We used our framework to visualize the stock market crash of October 1987.
,
The following are screen shots from our demo located on , under Products menu item.
,
,
Initially the user selects an appropriate data set classification. It could be
,
,
In the process of research the user could create miscellaneous types of classifications in order to discover meaningful relations in one or more of them.
,
After choosing data set organization users can search for valuable relations in a data set by:
,
,
The development of UVDRS has resulted in one patent granted: US 8,423,445 B2 (2014). A second one is pending: #13784611.
,
,
,
,
,
,
,
,
,: ,, Ph.D.,??is a principal at Rational Solutions Group specializing in analytic data visualization and data-mining. His work experience includes AT&T Bell Labs and Microsoft.
,
He holds a PhD degree in EE and an MS in theoretical cybernetics. Dr. Rotenberg is the author of more than twenty patents in analytic visualization, probabilistic modeling systems, analytical models and methods in process control, adaptive and training systems.
,
,
,
,  "
"
By Mohammed J. Zaki, Nov 2014
,
,
We are pleased to announce the availability of supplementary resources 
for our textbook on data mining.
,
Data Mining and Analysis: Fundamental Concepts and Algorithms
Mohammed J. Zaki and Wagner Meira, Jr.
Cambridge University Press, May 2014
,
Companion Website for Online Resources:
,
,
,
The fundamental algorithms in data mining and analysis form the basis
for the emerging field of data science, which includes automated methods
to analyze patterns and models for all kinds of data, with applications
ranging from scientific discovery to business intelligence and
analytics. This textbook for senior undergraduate and graduate data
mining courses provides a broad yet in-depth overview of data mining,
integrating related concepts from machine learning and statistics. 
,
The main parts of the book include exploratory data analysis, pattern
mining, clustering, and classification. The book lays the basic
foundations of these tasks, and also covers cutting-edge topics such as
kernel methods, high-dimensional data analysis, and complex graphs and
networks. With its comprehensive coverage, algorithmic perspective, and
wealth of examples, this book offers solid guidance in data mining for
students, researchers, and practitioners alike.
,
The supplementary resources for the book include:
,
,
,
,
,
Gregory Piatetsky-Shapiro,
,President, KDnuggets
,Founder, ACM SIGKDD, the leading professional organization for Knowledge
Discovery and Data Mining
,
,
Professor Christos Faloutsos, Carnegie Mellon University
,Winner of the ACM SIGKDD Innovation Award
,
,
, 
  "
"
,
,
, recently joined Microsoft from Amazon where he was the VP for the Global Inventory Platform and CTO of the core retail business. In this role he had responsibility for the science and software behind Amazon?€?s supply chain and order fulfillment systems, as well as the central Machine Learning group which he built and led.
,
During his 9 years at Amazon, he managed a variety of teams including forecasting, inventory, supply chain and fulfillment, fraud prevention systems, data warehouse and a novel data-driven seller lending business. Prior to Amazon, Joseph worked for Fair Isaac Corporation as VP of R&D. Joseph is passionate about Machine Learning and its applications and has been active in the field since 1990.
,
,.
,
Here is second and last part of my interview with him:
,
,?

,

,: We are really differentiated in our ability to build production-ready machine learning APIs with a few clicks. No other solution provides the ability to use drag and drop visual tools and open source code like R along with sophisticated machine learning algorithms and build reliable cloud hosted APIs that are ready to be hooked into production applications. Democratization of complex technology is something Microsoft is known for and machine learning is an area that really benefits from this expertise.

,

In a broader sense, our differentiation is that Azure Machine Learning is part of a broad, connected data platform that includes services for every stage of the data lifecycle. This includes not only machine learning, but also Hadoop services such as Azure HDInsight, data orchestration and ETL services such as Azure Data Factory, cloud-based complex event processing such as Azure Streaming Analytics, and business intelligence tools such as Power BI. This combination enables advanced analytics at an entirely new level of ease and sophistication.

,

,

,

,The goals of the initiative are to make machine learning more accessible and user friendly so that any developer anywhere in the world can use it as another tool in application building. I've been in this space a long time and haven?€?t seen ,advances in ease of use in generations. In the short term, we are empowering our partners and early customers of Azure Machine Learning with Machine Learning University. In the long term, we expect developers, data scientists, and students to get practical training for Azure Machine Learning through this avenue.

,

,)?

,

,The explosion of data has a lot to do with it. And the growth in the number of scenarios that benefit from machine learning ?€? from fraud detection to search ranking and ad targeting online. Software like R has also helped empower a lot of data scientists.

,

, Now, you can run compute for pennies on the dollar on the cloud and connect to systems and services that were previously stand-alone. The explosion of online data opens up opportunities for our customers to glean insights and make good choices to improve their business.
,
,

,

,I believe we are in the early phase of creating the infrastructure to handle big data. We are currently in the phase where we are analyzing current data; the next phase will be about predicting future outcomes, which is where solutions such as Azure Machine Learning come in. I recently gave a , at the Strata conference in New York about the emerging data science economy. My hypothesis is that we will eventually have millions of analytic APIs in the cloud, just as there is an application economy today with millions of apps. It will be ultimately up to the data scientists and advanced analytic developers themselves to embrace easy cloud tools like Azure Machine Learning and pursue this possibility to make it real.
,
,
,
,

,The key skills are a real understanding of data science, a great ability to develop services in the cloud, and facility with tools such as R, Python and Hadoop. I also think a key to success on our team, and really for any emerging market like this, is customer focus and a bias for action. I want everyone on my team, not just the data scientists, but sales, marketing, everyone, to be asking, ?€?What is the customer problem we are really solving??€? Then, have a strong bias for action, for building prototypes and iterating with the customer towards an end-solution in an agile manner.

,

,

,A book I really liked recently is ?€?,?€? by Ash Maurya. It?€?s a great book about an iterative and lean approach to building a successful startup. The agile methodologies mentioned in that book are a great guide to any product innovator.
,
,
,
  "
"
Most popular 
, tweets for Nov 17-18 were
,
,
,
,
,
Keep this #Python Cheat Sheet handy when learning to code , 
,
,
,
Is #BigData The Most Hyped Technology Ever? No (at least not yet) , http://t.co/RaiurvM7h5
,
Keep this #Python Cheat Sheet handy when learning to code , http://t.co/lEPdSvuX8A
,
,  "
"
Most popular 
, tweets for Nov 19-20 were
,
Spurious #Correlations-20 Insane Things That Correlate W/ Each Other , #BigData #MachineLearning 
,
,
,
Spurious #Correlations-20 Insane Things That Correlate W/ Each Other , #BigData #MachineLearning 
,
,
Spurious #Correlations-20 Insane Things That Correlate W/ Each Other , #BigData #MachineLearning http://t.co/xG2bFF3Pbp
,
10 Most Profitable Industries According to #BigData , 
,
,
,
,  "
"
Data analytics in Human Resources is quickly becoming a new area of innovation and focus because of the insights it can provide around workforce management. Workforce analytics has emerged as the key avenue for HR to become a proactive force for managing human capital and strengthening business strategies.
,
,
Data Driven Business has compiled a free white paper which focuses on the changing nature of HR in the workplace due to increased analytics and the improved ability to integrate data and systems from the perspectives of 4 experts from 
,
and one large financial firm.
,
,
,
Highlights include:
,
,??, 
,
,
,??, 
Download your free copy today: 
,  "
"
,
,
, is Assistant Professor of Finance and Risk Engineering at the NYU School of Engineering. He is also the founding managing editor of??,??and the co-founder and co-editor-in-chief of the??,. He has also been an analytics consultant with several NBA teams.
,
He holds a Ph.D. in Finance from the University of Chicago, a Master's in Applied Mathematics from Harvard University, and a Bachelor's in Computer Science from Harvard University. He also holds a J.D. and is an attorney-at-law admitted to practice in California.
,
He has been a portfolio manager at Long-Term Capital Management, Ellington Management Group, and his own hedge fund, Maymin Capital Management.
,
He has also been a policy scholar for a free market think tank, a Justice of the Peace, a Congressional candidate, and a columnist for??,, the??,??and??,. He is also an award-winning journalist and the author of??,, ,, and??,. He was a finalist for the 2010 Bastiat Prize for Online Journalism.
,
Here is my interview with him:
,
,
,
,Optical analytics extracts useful information from the high-frequency tracking data which reports the court location of all ten players and the ball, twenty-five frames per second, using an array of video cameras above the court. This is a new and very hot area for basketball decision makers because last year was the first year the data was available to all 30 NBA teams, under a groundbreaking deal between the league and STATS LLC, the innovators and providers of the data.
,
I believe this kind of data will ultimately prove to be as significant as the three-point line and shot clock in its effect on basketball. It is the most fundamental data: play-by-plays and box scores can in principle all be derived from the optical data. It lets you answer some questions that are otherwise impossible to even address: How does the average small forward?€?s shooting percentage from the left elbow depend on his distance to the nearest two defenders? Which players have the fastest first step? Who sets the hardest picks?
,
,
,
,: The optical analytics are not always simple. Traditional analytics are simple: points, rebounds, assists. You can see how it changes over time, you can rank players, and you can compare to historical greats. 
,
Some of the optical stuff is relatively simple: drives, contested rebounds, hockey assists, etc. Those will be integrated by all teams quite soon, I believe.
,
But the more advanced questions are sometimes hard to formulate in the typical analytics tools available to coaches. That?€?s the toughest and most important and valuable challenge. 
,
,Every team has at least one member of the coaching staff who gets lots of ideas from watching the games that he wants to check. Is this player really dogging it on defense, or is that an illusion? Who are the league leaders in getting the ball up the floor, or slowing down transition offense, or closing out on wing shooters? And how do our guys compare to them? Which pick-and-roll partners are most effective vs. which ones are we actually using most often? What areas of the post generate the most consistent offense, and does it come from kick-outs or fouls or plain old field goal effectiveness? What situations cause the most defensive lapses?
,
The toughest challenge is not just getting answers to them quickly, but presenting them with an interactive tool through which they can ask other or deeper questions, filter by game outcome, or shot clock remaining, or quarter, or any other reasonable parameter. 
,
, There are technical challenges in doing that properly but if it is done right, if an answer to a brand-new unique question can be answered just as quickly as opening up a web page, that?€?s suddenly an enormous advantage. Then you have not merely an optical analytics report, or even a tool, but a complete system. ,
,
, ?? ,
That means the skills needed for analytics are no longer purely statistical but also include data science, programming, visual interface design, cloud storage and computation, quality assurance and usability testing, automation and error handling and recovery, and in general a far more polished and production-level approach. Except for the rare case when those skills happen to coincide in a single person, this can be a hiring and/or training challenge for analytics departments more used to producing regular statistical reports. 
,
Some teams consider outsourcing some analytics to third-parties who offer similar services simultaneously to multiple teams, but ultimately a team?€?s chief analytics officer can?€?t really be outsourced any more than its general manager. They need to be an integral part of just one team. And yes, I think teams should eventually appoint chief analytics officers once they realize that analytics is not just another viewpoint among many, but can actually be a comprehensive framework for a team?€?s overall decision making.
,
,
,
,: Gut instinct is great for forming hypotheses. Analytics are then used to backtest those hypotheses. If the coaches can formulate and backtest new hypotheses on their own as easily as pulling up video, that would make things much easier. But if they have to sit through meeting after meeting and repeat over and over what they want, they?€?ll stop asking.
,
,
,
Coaches are perhaps the most adaptive people on the planet. They might have to rework their entire offensive and defensive schemes mid-season following a major trade. If there is a tool that can help them easily and quickly run objective what-if scenarios based on mountains of historical evidence, I think they would love that.
,
,
,
,: Sure. Most fans now know how great the corner three is. They know that driving into the paint is usually a good thing. They know that some players tend to get more traffic rebounds than others. But it?€?s not their primary focus. I think fans are mostly engaged, and rightly so, on the truly amazing plays that the world?€?s greatest athletes perform on a nightly basis. 
,
The NBA has just signed a deal with FanDuel, and Commissioner Adam Silver has recently expressed support for extending and allowing sports betting. I think optical analytics will impact both of those, in proportion to how much optical data is available direct to fans for analysis.
,
The second and last part of the interview is ,.
,
,
,  "
"
by Burtch Works, Nov 2014.
,
,
Over the past year, interest in data science has soared. , is a household name, companies everywhere are ,, and professionals in many different disciplines have begun eyeing the , as a possible career move.
,
In our recruiting searches here at Burtch Works, we?€?ve spoken to many analytics professionals who are considering adapting their skills to the growing field of data science, and have questions about how to do so. From my perspective as a recruiter, I wanted to put together a list of technical and non-technical skills that are critical to success in data science, and at the top of hiring managers?€? lists.
,
Every company will value skills and tools a bit differently, and this is by no means an ,, but if you have experience in these areas you will be making a strong case for yourself as a data science candidate.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
The next question I always get is, ?€?What can I do to develop these skills??€? There are many resources around the web, but I don?€?t want to give anyone the mistaken impression that the path to data science is as simple as taking a few MOOCs. Unless you already have a strong quantitative background, the road to becoming a data scientist will be challenging ?€? but not impossible.
,
However, if it?€?s something you?€?re sincerely interested in, and have a passion for data and lifelong learning, don?€?t let your background discourage you from pursuing data science as a career. Here are a few of the resources we?€?ve found to be helpful:
,
,
,
,
,
I?€?m sure there are items I may have missed, so if there?€?s a crucial skill or resource you think would be helpful to any data science hopefuls, feel free to share it in the comments below!
,
Original: ,
,
,
,
,  "
"
,
,
, is Assistant Professor of Finance and Risk Engineering at the NYU School of Engineering. He is also the founding managing editor of??,??and the co-founder and co-editor-in-chief of the??,. He has also been an analytics consultant with several NBA teams.
,
He holds a Ph.D. in Finance from the University of Chicago, a Master's in Applied Mathematics from Harvard University, and a Bachelor's in Computer Science from Harvard University. He also holds a J.D. and is an attorney-at-law admitted to practice in California.
,
He has been a portfolio manager at Long-Term Capital Management, Ellington Management Group, and his own hedge fund, Maymin Capital Management.
,
He has also been a policy scholar for a free market think tank, a Justice of the Peace, a Congressional candidate, and a columnist for??,, the??,??and??,. He is also an award-winning journalist and the author of??,, ,, and??,. He was a finalist for the 2010 Bastiat Prize for Online Journalism.
,
First part of interview: ,
,
Here is second and last part of my interview with him:
,
,
,
,Our current box score will soon seem to be a relic of a more innocent bygone era. There will be far more interesting columns, and many more of them. Even the idea of identical rows might eventually disappear, since for most players, many fields will just be zero, or not particularly interesting. 
,
Ultimately we?€?ll see more focus on what I think are the two most basic questions you want to ask about a player: what does he do really well, and what does he do really often? The optical data allows you to give much more nuanced answers.
,
I think we will also soon see the next evolution in measures of player performance. We will have optical counterparts to things like PER, Win Shares, Wins Produced, etc. All of the current methods of player evaluation will seem quaint in hindsight. I think even plus-minus and its extensions might be fine-tuned: perhaps a probability or expected value weighting per possession will give more useful and less noisy results.
,
As a result of all those changes, I think eventually the market for players will change, and there will be a greater focus on total contributions to a team?€?s chances of winning than simply paying for total point or rebound production. Agents will want to use the optical data to make a case for their clients. Perhaps once colleges start collecting optical data too, we will be able to see which skills translate to the pros and which do not.
,
Hopefully, we will see a lot more experimentation on the court. Some on-court plays require a certain amount of randomness already. It?€?d be nice to see some randomness in lineups too. Does the cost outweigh the benefit? Without running the experiment it is hard to find out.
,
,
,
,This isn't new in sports, let alone athletic sports. The same was once even said for chess. Allowing people to train on computers will ruin the beauty of the game! People will stop playing! In fact, chess has become much more interesting and championship games much more dramatic. The way Magnus Carlsen plays is, and should be, inspirational to all humans. Without computers, Carlsen wouldn't have had much of a chance against the hegemony of Russian players.,
,
Plus, the concern that any voluntary activity can ruin basketball?€?s true essence is very insulting and disrespectful to the robustness and beauty of basketball?€?s true essence! 
,
,No team is forced to use analytics, any more than they are forced to conduct practices or have morning shootarounds before a game. But if it helps, why shouldn't they seek every advantage? The point is to use any legal means to go out and get that ring. What?€?s the alternative? Ban analytics? That?€?s like banning thinking.
,
If the results from analytics are more boring basketball games, then rule changes will and should be forthcoming. But it seems as if the analytics approach tends to create better games: high-scoring, fast-paced games with lots of threes, lots of dunks, and lots of passing; in short, lots and lots of highlights. If you see a team where a single player consistently brings the ball up, dribbles it to death, dances around a bit, then takes a contested long-two, you?€?re probably not watching an analytically minded team. And you?€?re not watching one that is likely respecting the true essence of the game of basketball.
,
,
,
,There are a lot of machine learning applications now, because standard libraries and tools are now widely available and easy to use. And it is being applied at a breakneck speed to all sorts of sports analytics issues, which is good. I think that will run its course as it has in every industry until it is no more exciting than a linear regression. However, I do think it is in general more useful than a linear regression, so I imagine it will eventually become the standard approach to much of analytics. Part of the machine learning applications are analyzing existing data to draw interesting and useful conclusions, and part of it is actually generating new data for further analysis. I think both approaches will make a lasting impact.
,
,
,
,Be a master of all trades and skills rather than a narrow specialist.
,
,
,
,Be useful. Make useful things. Build your human capital. Learn many skills. Go to the annual Sloan ,Sports Analytics Conference and other conferences, at least virtually. DVR games and skip commercials, except for the important ones, because Twitter will ruin the ending for you. Be kind to people. Write. Code. Produce. Apply the same analytical thinking you would use on sports to help you manage your own career. Have a backup plan. Watch and allow yourself to genuinely enjoy the Top Ten plays every day. Sleep. Eat healthy whenever you can because you?€?ll end up bingeing at arenas. Apply for every job opening you reasonably can and sign up for updates. Take advantage of Sloan?€?s career resources.
,
,
,
,I like team-specific ones a lot, even if not all of their posts are quantitatively analytical. With so many eyeballs and neurons focusing on a specific team, you can get a lot of great insight that you can?€?t always find in a more general blog. 
,
When I?€?m not working I like to spend time with my family! Gotta go! :)
,
,
,  "
"
,,
has quickly become a top-tier conference.
,
The latest event, held Oct 27-30, 2014 in Washington, DC was a great success, with close to 600 participants.
,
The highlights of the program include:
,
,??,
The tutorials, keynote speeches, funding agency program director presentation (PPT format) are now available for download from the links below
,
,
,
,??,
,:
,
,??,
,
,
Chair :  Prof. Xiaohua Tony Hu 
,Panelists: Dr. Chaitanya Baru (NSF), Dr. Yuan Liu (NIH), Dr. David Kuehn (DoT), Dr.Tsengdar Lee (NASA), Dr. Sudarsan  Rachuri (NIST), Mr. Matti Vakkuri (DIGILE)
,
The IEEE Big Data 2015 will be back to  Santa Clara, California in November, 2015.  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,  
,, Nov 24, 2014.
,
We revisit KDnuggets Thanksgiving cartoon, which takes a look at 
what could be predicted from Big Data and Google Trends?
,
,
,
,
,
Here are other 
,.
,
and KDnuggets posts tagged
,.
,
As a further example of life imitating art, the actual 
Google Trends for 
,
look very similar to the cartoon! (However, only if we select US region, otherwise we find an earlier peak in October reflecting Canadian thanksgiving). 
,
,   "
"
Most popular 
, tweets for Nov 17-23 were
,
Is #BigData The Most Hyped Technology Ever? No (at least not yet) , 
,
,
,
,
Keep this #Python Cheat Sheet handy when learning to code , 
,
,
,
Is #BigData The Most Hyped Technology Ever? No (at least not yet) , 
,
,
Keep this #Python Cheat Sheet handy when learning to code , http://t.co/lEPdSvuX8A
,
,  "
"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Nov 25 and later.
,
See full schedule at , .
,
,  "
"
        ,
By Gregory Piatetsky,  
,, Nov 23, 2014.
,
We update our 
, (thanks to Anmol Rajpurohit for collecting the stats) and find several interesting trends:
,
,??,
Here are the 10 largest groups (by membership as of Oct 31, 2014), and their annualized rate of growth 
,
,??,
We note that the 6 largest groups (in bold above, each with over 35,000 members), were in the same order as on Mar 31, 2014.  
While the overall growth rate slowed down, as expected, the 6 largest groups grew at an annual rate of 57%, faster than the bottom 24 groups (32%) - see Fig 1.
,
,
,
,
Here are 10 groups with the fastest annualized growth (Nov 25, 2013 to Oct 31, 2014)
,
,??,
The chart below shows group growth vs group size. Color corresponds to growth rate in 2014 vs 2013 - redder is faster, bluer is slower.  Group name abbreviations are in the table below.
,
,
,
,
The two main measures of group activity are discussions (posts)/week and comments/week. 
,
The figure below compares the average group size with discussions and comments/week.
We note that although group size and discussions are growing, the absolute number of comments is actually steadily declining, perhaps because members are feeling overwhelmed with too many posts.
,
,
,
,
Since these numbers clearly depend on the group size, it is more interesting to compare them relative to group size (per 1000 members). The next figure shows that both comments and discussions relative to group size are declining.
,
,
,
,
The chart below shows group comments/week per 1,000 members vs group discussions/week per 1,000 members. Group name abbreviations are in the table below.  Since the distributions of comments and discussions are very asymetrical, we use median values of rather than average. 
,
,
,
,For better separation of groups, both axes are on logarithmic scale
,
The median lines on each dimension create 4 quadrants:
,
,??,
The groups with the highest comments/week per /1000 members:
,
,??,
The groups with the highest discussions/week per /1000 members:
,
,??,
The details are in the table with below, with groups ordered by the number of members.
,
The growth, comments, and discussions are in 
, if that value is 25% above median, 
, if 25% below median, and in black otherwise. 
We note that there are 7 ""triple green"" groups, that are at least 25% above median on all 3 key parameters: growth, comments, and discussions:
,
,??,
,
,Commments and Discussions are for period Mar - Oct 2014.
,
,
,
Here are past analyses:
,
,
,??,
I have used this set of 30 groups for consistency since 2013.  What large and relevant groups am I missing? What other trends do you see? 
,??,
,
,
 ,  "
"
,
Latest ,, (Nov 25, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
While machine learning has been around for a long time, usage was primarily restricted to people with deep skills and deep pockets. The cloud changes this dynamic completely. Joseph Sirosh, ,.  "
"
By Arun Swami, special to KDnuggets, Nov 2014.
,
,
Oxdata (,) provides software to allow data scientists to quickly and easily run machine learning models at scale. The intended audience seems to be people familiar with R who are limited by the scalability of R. Using H2O allows data scientists to distribute machine learning algorithms over a cluster. Not all machine learning algorithms are currently supported but the supported list is quite impressive. Many R functions and structures are supported but this will never likely be a clone of R. Oxdata seems to use a freemium model: the basic software is free and open source. Enterprises can choose to buy a premium license that provides them with 24/7 support, help with optimizing and scaling clusters, etc. For details, please refer to the company Web site.
,
H2O World took place on November 18-19 in Mountain View, CA at the Computer History Museum. This is a report on Day 1, primarily devoted to sessions that were significantly ?€?hands on?€? to help attendees get a feel for how they could use the H2O suite of tools.
,
The day started with an introduction to the company and its mission by Sri Ambati (CEO, Co-founder). The rationale for H2O can be summarized by:
,
,
Cliff Click (CTO, Co-founder) gave a high level presentation of the architecture. He showed how data and computation are distributed (the platform is written in Java). According to him, 100GB datasets can be handled easily and they are moving towards handling 1TB datasets. Analysis can be run using either a Web UI or R Studio.
,
Amy Wang gave a fast paced tutorial on running different machine learning models on H2O. They have a number of models out of the box that can run on a distributed cluster and more are being added. For many models, only some of the features are supported. For example, Generalized Linear Models do not support weights and Gradient Boosting Machine (GBM) do not support different loss functions.
,
Tom Kraljevic (VP Engineering) gave a talk on Using H2O in Big Data Environments. H2O can be run on Hadoop (YARN) and there is a project (Sparkling Water) to run H2O as an application on top of Spark. They plan to interoperate with Spark MLLib.
,
Arno Candel gave a comprehensive tutorial on doing deep learning using H2O. Free booklets on using R and on running deep learning are available at 
,. 
,
Arno Candel gave an interesting talk on using auto-encoders for anomaly detection. In this case, the auto-encoder is a deep learning model where the number of input neurons is the same as the number of output neurons. The model learns the identity function using a hidden layer that as many fewer neurons than input or output. He also talked at a superficial level about using H2O for feature engineering.
,
,
,Here are Sri Ambati and Arno Candel on stage.
,
,
Yan Zou and Vijay Iyengar talked about using H2O for Marketing and CRM. They also announced that they would run a competition using the KDDCup 1998 data set where participants would use H2O and would be ranked on how much better they performed than the baseline. The contest details will be posted on ,.
,
Bio: , is a Bay Area entrepreneur and tech leader, who created innovative systems using text mining, ranking algorithms, heuristic approaches, data mining, personalization technology, database algorithms and optimization algorithms. Arun was a key member of the team that started IBM's research in data mining and has published seminal work in this area. His classic data mining paper with Rakesh Agrawal , is ranked among most cited CS papers.
,
,
,
,  "
"
, at U. of Minnesota
,
The University of Minnesota is now accepting applications for Fall
2015 for a new Master's of Science degree in Data Science.  
,
This is a rigorous degree for the modern digital age, for students who want to learn state-of-the-art methods to manage and analyse Big Data, and the theory behind the methods.  
,
Students in this program will
learn cutting-edge methods to manage, analyse, and find patterns
hidden within big data, through the same regular coursework taken by
graduate students in Statistics, Computer Science, Biostatistics,
Electrical Engineering, etc.  Most of the coursework is open to remote
off-campus students through our interactive instructional video
system.  A Capstone project will enable students to use what they
learn on real-world problems.
,
Go to
, for information and to apply.  "
"
,
,
Slideshare is a platform for uploading, annotating, sharing, and commenting on slide-based presentations. The platform has been around for some time, and has accumulated a great wealth of presentations on technical topics like Data Science.
,
,
,
This visualization of the tags associated with the tag ?€?data science?€? on SlideShare illustrates the essential interactions between data science, big data, and open datasets.
,
Today, we will look at some of these top , presentations found on SlideShare. Similar to how we collected data for the previous ,, these presentations were retrieved by using a Python script and the , API, and then hand-curated to select the best, most relevant presentations. The slideshows and their associated metrics are shown below:
,
,
,
Some quick stats: the average views, downloads, favorites, and comments for a slideshow in this category are approximately 20000, 165, 29, and 2 respectively. Comparing these figures to those of the data science slides, we see lower numbers across the board. This is partially because many big slideshows in with the data science tag were actually more data mining oriented, and subsequently filtered out of this list. By doing this, many of the most viewed slideshows were eliminated.
,
Looking at the publication dates of these presentations, it?€?s interesting to note that 2013 is the most commonly occurring date. Considering the fact that SlideShare has been around since its launch eight years ago, this indicates that in recent years, data science has been increasing in popularity, making new presentations more popular.
,
,
,
Note that most of the presentations with 0 downloads have downloads disabled. There is an interesting lack of correlation between the number of views on a particular presentation and the download count. It seems that there are certain types of presentations that elicit downloads that aren?€?t necessarily more/less popular than those that do not. It seems instructive presentations, , for example, seem to encourage downloads, perhaps for offline studying.
,
,
,
,  "
"
By Roberto Battiti , LIONlab director, Nov 2014
,
News about LION-related activities in November
,
1. ,
,
After adding ,in the previous month, the flexibility to add and orchestrate components in different languages continues with full support for ,. Integrating external models and data sources within the LIONoso workflow is a way to develop complex applications with limited effort and extremely rapid prototyping.
,
2. ,helps to understand that machine learning is not the end, but the mean to reach generalization when new cases are considered (prepared for the ,).
,
,
,
3. ,(after some physical preparation) - a fantastic video of speed-flying through the Alps
,
4. ,participates in the KDP Select and Kindle MatchBook programs (if you qualify, the digital version is for free).
,
,
,
,  "
"
By Josie King (Innovation Enterprise), Nov 2014
,
,
,
Big Data has seen a huge leap forward in 2014 regarding how it has been represented and used across companies. The adoption rates have grown and the importance of Big Data as a business function has increased, but what are we going to see in 2015?
,
,
,
As the use of data has increased in the past year, the speed at which results are needed has grown with it. When this hasn?€?t been the case, people want to be more informed than before or have the ability to make decisions in real time, rather than through the use of reports reporting on historical data.
,
In-Memory databases allow companies the freedom to access, analyze and take actions based on data much quicker than regular databases. This in turn means that either decisions can be made quicker as data can be analyzed faster or more informed as more data can be analyzed in the same amount of time.
,
,
,
As we as a society have become data driven, one of the main aspects that has become clear is that finding the necessary talent has become difficult. This means that companies are often reliant on either too few staff or outsourced consultants.
,
Therefore, 2015 is likely to see more automated platforms that can allow employees who may not have as much skill with data as others, to collect, analyze and make decisions based on this data. This could be anything from simple to use interfaces with more complex backends or simpler tasks that could create business results.
,
,
,
The internet of things is evolving and more companies are using it, but it may well hit its tipping point in 2015. This would be sensor-to-sensor data being collected, collated and analyzed through purely sensor based collection. This can be done in multiple ways from the way that objects are interacting with another object, to the settings that people are using on particular devices.
,
Sensor based data is unlikely to out perform transactional data (which currently makes up the majority of data collection) but is still likely to see a marked increase. It is likely that we are going to see more device-to-device data being created and collected in 2015, which could see this number grow beyond transactional data within the next 5 years.
,
,
,
Despite the fact that transactional data is still more numerous that sensor data, 2015 may be the year that we see it being truly looked at in multi-dimensional ways to create even deeper customer insight.
,
This could be anything from geographical data to a deeper understanding of purchasing trends according to different, oblique factors. With new technology allowing metrics to be tracked across even more areas and wearables creating even more possible trackable actions, deeper customer understanding is inevitable.
,
,
,
Once thought of as the definition of making your employees ?€?just a number?€?, HR Analytics are being shown to have significant benefits to both the company and its employees. We believe that 2015 is going to see more companies wake up to the positives that an effective HR strategy can bring.
,
From optimizing workflow to tracking overall employee happiness, we are likely to see an increased use of HR analytics in 2015 as those who have adopted it within the last couple of years will be gaining ground on their competitors and those who don?€?t have it will need to catch up.
,
,
,
Data will become a commodity that is not just kept in one department alone and used purely by senior company leaders. 2015 is likely to see a democratization of data throughout the organization, meaning that more departments will become adept at using the insight that it can bring.
,
Rather than working towards a central strategy that is created by senior management, day-to-day activities will be based on data and the insights created from it.
,
Original: ,
,
Bio:,
, is President, Innovation Enterprise, which organizes Big Data Innovation Summits and many other meetings.
,
,
,
,  "
"
,
,
SlideShare is a platform for uploading, annotating, sharing, and commenting on slide-based presentations. The platform has been around for some time, and has accumulated a great wealth of presentations on technical topics like Big Data.
,
,
,
This visualization of the tags associated with the tag ?€?big data?€? on SlideShare shows how integral Hadoop is and the rising importance of the cloud for big data.
,
Today, we will look at some of these top Big Data presentations found on SlideShare. Similar to how we collected data for the previous article, these presentations were retrieved by using a Python script and the , API, and then hand-curated to select the best, most relevant presentations. The slideshows and their associated metrics are shown below:
,
,
,
Looking at the data quickly, we have averages of about 5300 views, 200 downloads, and 8 favorites. Keep in mind, this is with the presentations with downloads disabled being filtered out. This is impressive ?€? compared to the data science figures of 20000 views and 165 downloads, there are many more downloads/view on these big data slideshows. ,, a very comprehensive presentation on Visible Technology?€?s big data stack, alone accounts for 1770 downloads.
,
Looking at the authors in this table, we see that there are exactly two users with more than one presentation in this list: , and ,. The presentations by msitpro cover the big data use cases of Microsoft Azure. AsterData, on the other hand, covers topics involving the use of big data in business decisions.
,
,
,
This chart shows the number of views/week presentations received against their publication date. This shows that recent publications and older publications receive similar traffic over time, and over time there are particular presentations that receive exceptionally large numbers of views across the entire time range. One other interesting property of the top big data presentations is that they are no older than 2009, even though SlideShare has been around since its inception in 2006.
,
,
,
,  "
"
Most popular 
, tweets for Nov 24-25 were
,
#BigData Top Trends in 2015: In-memory DBs, Non-Data Scientists, Sensor Data, Deeper customer insight, HR Analytics ,
,
Great course: ""Intro to Statistical Learning, with Applications in R"" #rstats, free on 
,, starts Jan 19 
,
,
#BigData Top Trends in 2015: In-memory DBs, Non-Data Scientists, Sensor Data, Deeper customer insight, HR Analytics ,
,
Why So Many Data Ideas Fail: beware of building on public or others data - unsafe biz model , 
,
,
,
,  "
"
Most popular 
, tweets for Nov 26-28 were
,
Facebook's #AI team hires Vladimir Vapnik, father of popular #SVM algorithm , #MachineLearning 
,
,
,
Starting data analysis/wrangling with R: Things I wish I'd been told ,
,
Facebook's #AI team hires Vladimir Vapnik, father of popular #SVM algorithm , #MachineLearning 
,
,
Starting data analysis/wrangling with R: Things I wish I'd been told ,
,
,  "
















"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Lise Getoor, Nov 2014.
,
Join us for an in-depth, lively and thoughtful discussion about 
,
,
,
What it is now, what is needed (and what isn't!), and what it can be in the future.   We have brought together data science leaders from diverse fields and backgrounds to share their thoughts, and we will invite the audience to join in the discussion as well.
,
The panel will take place from 6pm-9pm PST on Thursday, Dec. 4th, at the BNY Mellon Innovation Center, Deer Creek Rd, Palo Alto, just off of Pagemill Rd.   
,
It will also be broadcast on ,
,
We have a terrific panel line-up:
,
,??,
Moderator: Mehran Sahami, Stanford
,
Agenda
,
,??,
Address: Pivotal Building 3495 Deer Creek Road, Palo Alto, CA 94304
,
Organizer: Lise Getoor, UC Santa Cruz
,
Hosted by Silicon Valley Financial Services Cloud meetup group and Data Science Santa Cruz (DSSC)
,
If you plan to attend in person, please sign-up here (it's free!):
,
,  "








"
,
,
,
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
Most popular 
, tweets for Dec 01-07 were
,
Hilarious ! If programming languages were vehicles: on C, C++, Java, Python, Perl, Lisp, ...  , 
,
,
,
Data Scientists who know #BigData tools like Storm, Apache #Spark have the highest salaries 
, 
,
,
,
Hilarious ! If programming languages were vehicles: on C, C++, Java, Python, Perl, Lisp, ...  
, 
,
,
Hilarious ! If programming languages were vehicles: on C, C++, Java, Python, Perl, Lisp, ...  
, 
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Dec 9, 2014.
,
Latest KDnuggets Poll asked readers to select
,
,
,
Based on the results, the top 3 areas are
,
,??,
However, a number of significant new areas also appeared.
,
These were not asked in 2012 poll, but added to the poll based on job ads on , and in similar places.  
,
The most notable new sectors are
,
,??,
Here are the poll results - see additional analysis below.
,
,
,
Comparing to 
, poll,
we note that the biggest increases, computed as (pct2014 - pct2012) / pct2012, were for
,
,??,
The industries with the biggest decline in share of applications were
,
,??,
Average number of industries selected per voter was 2.8, slightly higher than  2.6 in 2012.  However, we also note that about 40% of the voters only worked in one industry.
,
We can also measure affinity of industries by looking at applications most commonly selected together by the same voters. While this is not perfect, the results seem to be very meaningful.
,
For,, most close fields were Banking, Retail, E-commerce, and Direct Marketing/ Fundraising.
,
For ,, most close fields were CRM/Consumer analytics, Retail, Credit Scoring, and Insurance.
,
For ,, most close fields were Science, Medical/ Pharma, Biotech/Genomics, and Insurance.
,
The regional distribution was surprisingly similar to 2012 poll for US, Europe, and Asia, but with more participants from Latin America and fewer from Africa/Middle East:
,
,??,
,
,
,
,
I miss mining industry. There are many data mining application in this industry: predictive maintenance, quality and production optimization, among others.
,
,
,IT Application and Network Infrastructure
,HelpDesk/Customer Support
,
,
,Scientific application of DM for Agriculture
,
Here are the results of past polls:
,
,??,
,
,
 ,  "
"
,  "

"
        ,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
,
Latest ,, (Dec 10, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
""I think the success of deep learning gives a lot of credibility to the idea that we learn multiple layers of distributed representations using stochastic gradient descent. However, I think we are probably a long way from understanding how the brain does this."" Geoff Hinton, ,  "
"
Kim Carter, Dec 2014.
,
Australian businesses and government departments crave analytics professionals, but talent is still hard to find, according to the Institute of Analytics Professionals of Australia?€?s 2014 Skills and Salary Survey Report.
,
,
IAPA has conducted its second annual salary survey to explore the employment opportunities, education, experience, skills and salary ranges of analytics professionals. Nearly 500 people who work with data responded, reporting from a variety of industries across the country.
,
The key findings were:
,
,
,
,
Nearly 90% of respondents said recruiting analysts was harder than or as hard as last year. The increasing demand for analytics across more business units and in more industries has boosted demand for these professionals, which continues to drive up salaries.
,
?€?The global trend of demand for talented analysts outstripping supply remains true in Australia with the majority of managers reporting difficulty filling positions,?€? said Jodie Sangster, ADMA/IAPA CEO. ?€?This is an area we need to address through education and upskilling to ensure we have the future professionals required in this field.?€?
,
,
,
The report also shows analytics professionals earn almost twice the median Australian salary ?€? $125,000 annually. Respondents in the workforce for up to 3 years and now working in analytics had a median salary of $72 - $75K while those working for 14 to 16 years had a median salary of $152 - $194K. (Variations are based on whether analysts are in industry or the supply side).
,
,
,
Those who analyse and mine ?€?social media and social network data?€? are earning a 50% premium over the average respondent?€?s salary. This group has a median salary of AU $190K, almost three times the median Australian salary.
,
?€?With social analytics typically located in the marketing domain and marketing nominated as the most common ?€?owner?€? of the analytics function, the CMO is quickly becoming the key analytics decision maker,?€? said Ms Sangster.
,
,
,
?€?Analytics is becoming a career magnet,?€? said Doug Campbell, IAPA Chairman. ?€?The report shows that respondents are moving into the field from other disciplines, evidence that analytics is emerging as an attractive career option,?€?
,
Keeping skills up to date was flagged as both the biggest challenge by 43% of respondents as well as their biggest interest. Some 75% of respondents were interested in receiving further non-degree training and 40% were pursuing further degree-based education.
,
,
,
Despite all the hype about big data, only 15% of those surveyed ranked handling big data as a challenge. ?€?Either we?€?ve yet to really hit the big data iceberg or we?€?ve sailed straight past it. Either way, respondents seem to see data as data, big or little,?€? said Evan Stubbs, Chief Analytics Officer-SAS, and the report?€?s author.
,
,
,
This year IAPA introduced a segmentation analysis to better understand the types of professionals in the Australian market. Twenty-four percent, dubbed ?€?Data Science Professionals?€?, showed a unique set of skills: they?€?re more multi-skilled, more technical and use change management, persuasion, business case development and communication skills more than other respondents.
,
?€?Without a universally agreed definition of ?€?data scientist,?€? perhaps the characteristics of this segment could act as a proxy,?€? suggested Stubbs.
,
The other three groups identified were classified as:
,
,

,
,
Thirty-nine percent of respondents said a bigger challenge was ?€?convincing their company of the value of analytics?€? while ?€?getting the company to act on insights?€? was cited as a challenge by 38%.
,
?€?There?€?s a disconnect between what companies say they want and what?€?s actually happening ?€? everyone wants answers and yet analysts struggle to get people to act on those answers. It?€?s this ?€?decision inertia?€? that respondents seem most frustrated with,?€? added Mr Stubbs.
,
?€?Business leaders will need to improve the use of evidence from analytics to guide decision-making, and understand how marrying insights and experience improves performance. Analytics professionals need to directly link their efforts to business priorities and communicate insights in ways that are easily assimilated and understood by business decision-makers. IAPA will be beside both groups to support, encourage and facilitate their needs,?€? concluded Mr Campbell.
,
The IAPA report can be downloaded free at:??,
,
,
,
,  "
"
,
,
, has over 20 years of experience in analyzing and taking actions on very large data. Trained in data analysis and simulations on molecular systems, he gained extensive expertise in customer centric marketing, optimizing for all stages of customer acquisition, conversion and retention. He has worked on segmentation and predictive modeling for banner ads, web logs, search keywords, emails, transactions, call center, and customer life time values. 
,
Daqing is Director of Advanced Analytics at Macys.com, leading the predictive analytics, test and experimentation and data science teams. He previously held senior management and technical leadership positions at Ask.com, the University of Phoenix, Tribal Fusion, Yahoo, Digital Impact, and Bank of America. He also worked on client analytics projects for Intel, HP, Wells Fargo Bank, SBC, Dell, T-Mobile, MSN Search and Travel, Intrawest, PayPal, wine.com, MasterCard and others. 
,
Daqing received his Ph.D. from Stanford University, specialized in scientific data processing, simulations and optimizations.
,
Here is my interview with him:
,
,
,
,: First, let me thank KDNuggets for an interview regarding my talk at Big Data & Analytics for Retail Summit in Chicago this year. I have been a user of KDNuggets for data mining related information since the late 1990's and have great respect for the site and the team. 
,
Analytics play an important role in Macy's customer centric strategies.  Our senior management puts great emphasis on ,competing in analytics. Macy's omni channel customer strategies are aimed to provide superior customers shopping experience, store or online, with interaction through web sites, emails, using desktop, tablet and mobile devices. A customer may buy some products online and exchange or return in store. Or she may also go to a store and an associate may help order online an item out of stock, with free shipping. 
,
From improving customer experience to optimizing our business processes, data and analytics are very important assets. We use data to build powerful tools to improve the experience of our customers. We strive to innovate on how to apply data science in these areas. There are two types of data scientists, one being those who make sure data are collected, and accessible in time and provide the tools for us to process the data and to take action. There is another kind who are domain experts, focusing on solving specific data driven business problems such as those customer or supply chain analytics and modeling, using the data and analysis and modeling tools. Using an example in the medical field, there is a need for MRI technologists (physicists, chemists and electrical engineers) but there is also a need for medical professionals who use MRI to treat patients.  We are more like the latter. We need both types. Senior management needs to differentiate the two types of data scientists. Being an expert in one does not imply having the knowhow for the other. 
,
We have many high priority projects, such as real time site personalization, email personalization and ad spend optimizations, as well as supporting other company wide initiatives, support supply chain optimization, customer values, providing insights to business decision maker. 
,
,
,
,At Advanced Analytics at Macy's, we work on all projects beyond reporting and database ad hoc analysis.  Big data is a natural growth out of traditional analytics.  Traditional BI has a set of processes from schema design, ETL (Extract, Transform and Load), standard reporting, multidimensional reporting, simple analysis, predictive modeling, and knowledge discovery.  There is a maturity process for companies in adopting BI.  Most companies never go beyond simple analysis.  Due to technology limitations, only after loading data into a database can we start querying and analyzing the data. The value of data at per gigabyte level had to be very high. For example, only transaction data, limited customer information and product data, as well as financial data were stored and analyzed. 
,
Now in the Big Data era, with Hadoop and NoSQL, we have a paradigm shift.  In order to store and read the data efficiently we need to use distributed commodity computer clusters and ,distributed computing software. The key differences are that data are not only big but also raw including free format data. Value of data in dollars per gigabytes of data can be much lower than before.  We now can include web analytics data, raw log, ad serving data and conversion data, and also free text and image data. It is often subjective on how to transform free format data into structured one, so data transformation is now part of the analysis and modeling. 
,
With the increasing volume and the disk IO bottleneck, persistent data are stored raw as read and append only. We can only read and often analyze from the raw data using distributed tools like Map Reduce.  Data storage is cheap and computation power is in abundance.  In the new paradigm, we should analyze the raw data first and do some insight discovery, before we use domain knowledge to decide on the best ETL design. We also can have transformations and load processes in a much more iterative and agile fashion. This are the software development principles in the Big Data context. 
,
,
,
,We need to work with the customers of the data models and understand their objectives, how the models are used and how successes are measured. It is crucially important for our analysts to understand the data, their context of collection very well.  And understand the strengths and weaknesses of the data.  If we see a pattern, we need to know if it is real. We need to understand what data can and cannot be collected, and consider cost and benefits of collection of the data. Some data cannot be collected. It can be due to privacy policies or behavior, for example, customers may pay cash. 
, 
,
,
,With scalable models, we have to be very careful about leakages in the model target definitions.  We need to rely on test and experimentation to identify champions, and optimize and learn over time. We may have some assumptions, but we cannot be sure of their correctness unless we can prove in the market that the model works, in that they improve customer engagement or generate lifts in conversions. This is another art and science and a constant challenge. 
,
,
,
,From an analyst's perspective, we want more access to more data and quick turnaround time for transformations.  We also want the freedom and capability to conveniently interpret, transform free format data.  We want a more interactive environment with quick turnaround time for modeling building and comparisons. 
,
On model building, the old way of model building is very human and time intensive. Data sets were small and expensive to collect.  It may take months or longer to build a model, and patterns ,captured in the models would have to be long term. Now, with low cost of storage and computing, and fast collection and processing of data, we can build many more models and build models for short time behaviors. Building so many models, with each model having so many predictors and some of the categorical variables having a large number of values, an analyst just cannot inspect all variables individually. The danger of leakages and other problems is much higher.  We have to rely on techniques of automated modeling, such as out of sample testing, cross validations, robust modeling tools that can handle missing values and outliers. 
,
,
,
,Different companies may have different challenges.  The senior management has to be sold on the idea. More than ten years ago, Yahoo had an Idea Factory where employees proposed ways to ,improve customer engagement and monetization.  From my earlier experience in 1:1 emails at Digital Impact (now part of Acxiom), I proposed a 1:1 Yahoo front page to reduce clutter.  But Yahoo front page was the most valuable page on the Internet.  Right information to the right Yahoo user at the right time wasn't getting any attention.  Personalized front page didn't become reality until recent years and after several changes in management.  Right now, most companies are convinced that the only way to help customers on relevance of messages and information in scale is to use automated data driven personalization solutions. 
,
Data are always a challenge. The most important data, the smoking gun, may not even be collected.  Data availability, data quality is hugely important.  In addition, the right transformation may not be invented.  There may always be a better way to transform the data. 
,
,
,
Modeling platform needs to be scalable. We need to have fast enough turnaround time to capture and influence customer?€?s real time behavior.  Model deployment has to be fast and easy enough in order for the business to utilize the models.  Data visualization tools can help the analyst explore and explain the models.
,
It takes more than either programming or the standard data mining algorithms.  Google still offers a long list of search results.  I hope one day when we search for something, we should get one item we want, in the context of what we are doing or searching, and not by luck.  We are getting there but not there yet.  Applying data mining and artificial intelligence methodologies in targeting is still far from being mature. We need more innovations. 
,
,
,
,
,  "
"
Most popular 
, tweets for Dec 08-09 were
,
Alan #Turing Institute for #DataScience to be based at British Library 
, #UK #BigData #London 
,
,
,
Great post: An Introduction to Unsupervised Learning via Scikit Learn #Python #PCA #FaceRecognition #MachineLearning 
,
,
Bottlenose, a Los-Angeles pioneer in Real-time Trend Intelligence, gets big investment from @KPMG Capital #BigDataCo 
,
,
One of the very first well documented reports on the effects Analytics bring to Enterprises 
,
,
, 
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, has over 20 years of experience in analyzing and taking actions on very large data. Trained in data analysis and simulations on molecular systems, he gained extensive expertise in customer centric marketing, optimizing for all stages of customer acquisition, conversion and retention. He has worked on segmentation and predictive modeling for banner ads, web logs, search keywords, emails, transactions, call center, and customer life time values. 
,
Daqing is Director of Advanced Analytics at Macys.com, leading the predictive analytics, test and experimentation and data science teams. He previously held senior management and technical leadership positions at Ask.com, the University of Phoenix, Tribal Fusion, Yahoo, Digital Impact, and Bank of America. He also worked on client analytics projects for Intel, HP, Wells Fargo Bank, SBC, Dell, T-Mobile, MSN Search and Travel, Intrawest, PayPal, wine.com, MasterCard and others. 
,
Daqing received his Ph.D. from Stanford University, specialized in scientific data processing, simulations and optimizations.
,
,.
,
Here is second and last part of my interview with him:
,
,
,
,: The reasons can be many things, such as data availability and scarcity and scalability of data analytics on millions of keywords, billions of ad pages and large numbers of customers. The data we get from media vendors are limited.  We need time to figure out how to use that data. The market is changing all the time, due to advertiser behaviors as well as consumer behaviors., 
,
Let me give a simplified example. If we average say two ad words in SEM (Search Engine Marketing). Let's say keyword A has a value per click of $5 and keyword B at $10.  The average of the two is $7.5. As a group of the two, in order to be profitable, we cannot bid over $7.5.  What happens if we bid just below $7.5 on both keywords? We lose traffic on keyword B and we lose profit on keyword A.  I call this a double loss, on both traffic and profit.  
,
In addition, in an auction market, say there are two advertisers competing for these two keywords. Company 1 knows the values of the clicks of keyword A and B separately, and Company 2 does not.  What happens?  Company 1 will bid lower on A and get less bad traffic and bid higher on keyword B, to get more the better traffic. Company 2 on the other hand, not knowing the values in detail, will bid the same for both. Because the good traffic from keyword B has gone to the higher bidder, Company 2 will get more unprofitable traffic from A.  So a company with better analytics can cherry pick in the market, putting other companies without good analytics at a severe disadvantage. This is especially the case if profit margin is very low. 
,
,
,
,The most important thing is to make sure the model is relevant to the business objectives. We need to make sure that the insights and results can benefit the customers and the business in some way. We can build models for a lot of things, but many of them may not be relevant or actionable. 
,
,

,
One may have many ways to do clustering, but if I add or remove some dimensions, the results may have much larger impact than various algorithms may produce. 
,
From my experience, as we analyze the data more and more, we continually find data issues.  Every wrong day can be wrong in its own way. So in order to ensure data quality, we cannot just ,collect data or only see some reports such as dashboards.  Some data issues may not be apparent in those simple reports.  For example, in search engine marketing, we have a very long tail search keyword set with very sparse traffic counts data which can be very volatile in number.  If we miss the capture of 10 percent of the conversion counts, it may not be obvious from simple reports.  But if we use that information to bid in the media auction market, our ROI would be wrong.  If we didn?€?t model and take action on the data, we might never have found the data issue.  The more we analyze and use the data, the cleaner we data will be.
,
We need efficient model building environment, where analysts can build, refine, deploy, compare, and analyze the results of many models easily. In traditional statistics, data sets are small in size and high in value, and an analyst can afford to spend a long to research on a model. Now data are cheap and models may degrade in weeks or shorter, and the market is changing all the time, we need to be able to scale our models. 
,
It takes time for an organization to get used to the data driven decision process. It takes a long time to cumulate creative assets, and train business decision makers to digest data insights and act on them. A misconception is that data analysis makes online making easier. The truth is that often it is harder for the marketing managers, because now there is visibility on performance, and the insights can be constraining on media spend decisions.  Spending money wisely is much harder than spending money. 
,
,
,
,Yes. Mobile customers are different from desktop users. There are demographic factors, in adoption and usage of ,smart phones. There is also this decentralization of human interaction to the Internet. We used to have desktop as the only device to get online. Now we have not only desktops, but also tablets and smart phones, often not shared as much as desktops. We each may have multiple, many types of devices, each used in a different context. Our interactions with the Internet overall are higher, but for each device, we may exhibit less activities. It is very important to understand customer behavior across the devices. Any one device is just part of the puzzle.  Here, we have challenges not only for attributions but also for behaviors.
,
Another challenge for smart phone is about locations.  It is hard not only to collect data on location and context but also to analyze and refine the targeting algorithms. When we get data from a partner, we still need to understand what we get, what the match rates mean, etc.  I see most efforts are around simple rules like if the customer is near where and then do what. This type of targeting is not scalable and less effective. 
,
,
,
,We use the principle of separation of concerns. It is far better to let software companies and open source communities build and refine the lower level data science infrastructure. We definitely have to own the design of the solutions specific to our data, our customers, and our products and businesses. We may work with consultants on some areas in the middle. We use commercial and open source tools, and we also have internal data technical teams as well as analytics teams. There are some types of work, due to for example, privacy protection and business knowledge, we have to do internally and cumulate expertise in house. 
,
We have Hadoop, and Hbase, and are in efforts to test solutions using Spark and H2O. We also have SAS, and use R, Mahout, SAP/KXEN, and Tableau for visualization, etc. We also work with research groups at SAS, IBM and others on some solutions. We evaluate products to suit our solution needs and everything is dynamic. In the end, we optimize the overall benefits and cost of ownership. 
,
,
,
,We don't have specific questions for interviews. I like asking description of a project the candidate worked on recently and from there to see his or her technical and business skills. In addition to the ?€?must-haves?€? such as basic math ,skills, computer skills, and SQL skills, and the ability to communicate ideas and insights, we like candidates who have handled and not intimidated by large data sets and complex analysis strategies. We also like candidates who enjoy learning new things and can think outside of the box. Continuous lifelong learning is not easy, but there are people who want to learn new things and are eager to face new challenges. This is a very important trait working in a new, fast changing field.  
,
,
,
,
,
,
,One book I read a while ago was Walter Isaacson's book on Steve Jobs, but the book keeps coming back to my mind. I am amazed at the incredible inventor, his fascinating way of thinking and an interesting personality.  Jobs has qualities that differentiate from many of the pure technical people and pure business people, and the book offers interesting insights about the dynamics of people with different backgrounds. There are a lot of things for us to learn, being in a new, ever changing field, combining technical innovations and impact on the social side.  
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Dec 11, 2014.
,
On Dec 10, 2014 I attended the 2015 Analytics Predictions Webinar by
,, with IIA Co-Founders Thomas H. Davenport and Jack Phillips, and Faculty Members Robert Morison, Bill Franks and Mike Lampa.
,
The poll conducted before #2015Analytics webinar had 26% of organizations say their analytics maturity is high, 47% medium, 11% dont know.
,
The IIA Panelists evaluated their 2014 predictions made a year ago
,
,
,
and found that they were mostly right on predictions 1-6, except that use of facial recognition technology is not mainstream yet.  The last 3 predictions (nos. 7-9) were judged as stay tuned.
,
Here are the tweets from , and others under 
, hashtag.
,
The predictions for 2015 were divided into 4 categories.
,
,
,
Prediction 1:
,
,??,
Prediction 2 generated the most discussion:
,
,??,
,
,
Prediction 3:
,
,??,
Prediction 4:
,
,??,
Prediction 5:
,
,??,
,
,
Prediction 6:
,
,??,
Prediction 7:
,
,??,
,
,
Prediction 8:
,
,??,
Prediction 9:
,
,??,
Prediction 10:
,
,??,
This prediction raised the most serious question - if decisions are becoming automated, how do people keep expertise? 
,
How does a person become an expert in any field, if all entry-level work is done by machines?
,
Below is a snapshot ofe IIA predictions for 2015
You can also 
,
,??,
,
,
What are your 2015 predictions for Analytics?
,
,
,
,??,
,
,
 ,  "
"
,
,
My monthly summary of the company, startup, and acquisition activity for November 2014 from 
,.
See the latest under hashtag
,.
,
,
,
Here are KDnuggets tweets
,
,??,
Here are previous months activities:
,  "
"
,
By Gregory Piatetsky,  
,, Dec 12, 2014.
,
The idea that creatures created by us will threaten us is not new.
It has already appeared in medieval Golem stories and 1818 Frankenstein novel.  
In the very first story where the word ""Robot"" appeared, 
1920 play 
, by  Karel Chapek, the robots rebel against humans.
,
Many popular movies, such as The Terminator series, 2001, Blade Runner, feature humans in a not always successful fight against AI entities.
,
However, the rapid rise and successes of AI and Machine Learning
has caused prominent scientists to sound alarm.
,
Recently
, said ""AI could be threat to human race"" and
, warned about ""AI being an existential threat"" to humanity.
,
On the other hand, full AI is still far away, and may turn out to be a humanity guardian and benefactor, as in Asimov's Foundation stories.
There are projects launched to
, and be safe.
,
Here is a BBC report 
, which examines both sides of the story.
,
What do you think?  Please vote if you are human.
,
,
,
,
,
 ,  "
"
,
By Mathias Golombek (EXASOL), Dec 2014.
,
The concept of data science and data scientists is nothing new. It?€?s been talked about for years and has even been called one of the sexiest jobs around. As the amount of data available today only continues to grow, companies are faced with the challenge of properly analyzing information to create better customer experiences and make better business decisions. Enter data scientists. In order for companies to realize the full potential of their data, we predict the following things around data science and data scientists to come to fruition in 2015:
,
,
,
,
2015 will see data science becoming a mainstream career choice. Most universities are already offering courses in data science in preparation of the dramatic rise of this new profession. High demand for data scientists will snowball in the coming year, with nearly every enterprise having data scientists doing more than just studying customer behavior - heralded so much in 2014 - but expanding into new areas such as data forensics to combat rising cyber threats, fraud and risk as well as the creation of new types of businesses based on data.
,
Data science adoption will be driven by the business and its requirements, which will lead to an explosion of new tools and services focused on specific industries and departmental needs. In addition, the gap between expectations and reality will increase in 2015. Because many technologies are still very immature, newly hired data scientists are often very inexperienced regarding business questions and face the challenge of bridging the gap between business departments?€? operational needs and the capabilities of the technical infrastructure which is getting more and more complex.
,
,
,
Data science as the science of ?€?optimizing business by exploring/mining data?€? is becoming a mega trend, but there is a lot of confusion in the market. Regarding the job of data scientists, many big companies have already implemented complete data science labs without any clear strategy. The role of a data scientist has to evolve next year by becoming a kind of human interface between ?€?data business incubators?€? and the actual business departments who need to gain competitive advantages over their??competitors. In 2015, the need for additional human resources in that area will accelerate, but the number of skilled people to fulfill these requirements cannot be satisfied. Therefore the gap between expectations and fulfillment will open even more.
,
While a lot can happen in a year and we?€?ve only scratched the surface of the data science profession, we can be sure that it is going to evolve and grow significantly over the next year. Whether or not these specific predictions materialize, we can say with confidence that the data scientist of today will not be the same data scientist of tomorrow.
,
,
,
,
,
,  "
"
,
,
,, B.A., M.B.A., is the Vice President 	     of Technical Sales for IBM?€?s Information Management 	  	     division and additionally leads its World Wide                	     	     Competitive Database and Big Data teams. Paul is an 	  	     award winning writer and speaker with more than 20 years 	     of experience in Information Management and is seen as a global expert in Big Data and Analytic technologies. Independent groups often recognize Paul as a thought leader with nominations to SAP?€?s ?€?Top 50 Big Data Twitter Influencers?€?, Big Data Republic?€?s ?€?Most Influential?€?, Onalytica?€?s ?€?Top 100?€?, and Analytics Week ?€?Thought Leader in Big Data and Analytics?€? lists. Big Data Made Simple noted him as a ?€?Top 200 Big Data Thought Leaders on Twitter?€? and Technopedia listed him one of its ?€?Big Data Experts to Follow?€?. 
,
Paul has written more than 350 magazine articles and 19 books, some of which include ?€?Big Data Beyond the Hype?€?, ?€?Hadoop for Dummies?€?, ?€?Harness the Power of Big Data?€? and more.
,
Here is my interview with him:
,
,: Q1. In your keynote at Strata + Hadoop World 2014, you encouraged people to think of Big Data environment as ""polyglot"". Can you explain that statement and provide a few examples?
,
,: A person that is polyglot can speak multiple languages - so when I talked about a polyglot analytics architecture, I'm was talking about multiple ,technologies. Too often people get caught up in the hype and think Hadoop = Big Data. But just take a look at what's going on with Spark - I mean, is anyone even talking about MapReduce anymore? In a polyglot environment, I would have other technologies as well; for example a graph store such as Titan or perhaps a document store such as Cloudant. Of course, the RDBMS is not going away and it frustrates me when I see over-zealous IT folks act in such a manner. Now add to this things like Swift Object Stores and so on.

,
,
,
,Lots of myths out there. I alluded to one earlier, ?€?Big Data is solely Hadoop?€?. Others include ?€?Big Data means lots of data?€? - it really mean more data than you are used to, and perhaps different kinds and the speed of accumulation. , Big Data doesn't mean death to the RDBMS, that's an unfortunate one that I feel I still have to displace. Finally, Big Data isn't just social sentiment. There are so many use cases, from machine data (what I like to call data exhaust) to image and feature extraction...it's just fun to talk about social.
,
Pain points? I've alluded to some of them already, but I will toss skills out there. Look, my clients are not all LinkedIn and Facebooks - they don't have the budgets to hire ,PhDs in Math and Java programming. Even some clients I work with that are well travelled on BigData lose sight of the fact that the true value comes when you move Big Data from the privileged few to the empowered many. So skills. You're going to need varying BigData skills, from maths, to visualizations, to business communication around the topic. I find most companies, whether they want to admit it or not, need to train internally as well as add to the bench from external sources.
,
,
,
,Well, you look at Twitter and it truly is the 'pulse of the planet'. One of the things I said in my keynote is that Big Data without analytics is...well...just ,a bunch of data. So, I think it's a natural evolution to bring the pulse of the planet with the most advanced analytics platform on the planet. I mean step back for a moment and look beyond Big Data today. IBM is doing the hard Big Data work of tomorrow with its Watson cognitive computing. I mean, eWeek named IBM as one of the top 10 technology companies that is proving innovation isn't dead (Nest, Tesla, Amazon in that group, no other traditional or new age Hadoop vendor was in there). So it's a natural marriage if you will. We are going to offer the ability in both PaaS and SaaS styles to interact with this Twitter data, for free(depending on the service); as well as deliver expert consulting around the use of Twitter data to bolster that business.
,
The most significant value is really about attribute fullness in my humble opinion. I mean sure, there are things like reputational risk assessment and monetizable intent, but what about using Twitter posts to classify folks, understand location, and those kinds of things. IBM has a technology called BigMatch - it's the world's (as far as I know) first probabilistic native Hadoop matching engine. Clients ahead of the industry are using it to match system of record with Twitter system of engagement for better attribute fullness.
,

,
,
,This is a broad topic, so let me narrow it on Hadoop. Enterprises are running with scissors. Look, , So things like activity monitoring for audit, masking, meta-data ... these things all matter. And if you look at pending legislation around consumer protectionism (for example the pending ""Right to be forgotten"" legislation in Europe) it's going to get even more important. But most important, at the end of the day, consumers are going to punish you if you mishandle their 'stuff'. We've seen that a number of times. 
,
, 
,
,
,  "
"
Most popular 
, tweets for Dec 10-11 were
,
Predictions for $125B #BigData Analytics Market in 2015: Security becomes killer app, #IoT will be hot, data markets 
,
,
10 Data Science Newsletters to Subscribe to, from BigData-Startups to KDnuggets 
,
,
Predictions for $125B #BigData Analytics Market in 2015: Security becomes killer app, #IoT will be hot, data markets 
,
,
Which one is the bunny? Google new CAPTCHA bot-trap trains #AI for #image #recognition 
, 
,
,
,
,
,  "
"
By Joe Krakoviak, Dec 2014.
,
, Kaggle competition
,
In the spirit of the season as well as business analytics and optimization, FICO is sponsoring a Kaggle competition ?€? for prizes up to $20,000 ?€? seeking an algorithm that will mathematically make Santa?€?s elves?€? production schedule most efficient.
,
Santa?€?s grueling production schedule has a strict deadline for on-time Christmas delivery while complying with the strict rules of the Elfin Worker?€?s Union.??Can Santa?€?s operation be optimized mathematically? Is it possible to create a job scheduling algorithm that not only meets Santa?€?s toy target, but keeps his elves healthy and happy? Can Santa?€?s workshop be brought out of the dark ages of magic and holiday spirit, and transformed into a highly optimized modern operation?
,
Competitors can win in three categories, and, of course, earn a spot on Santa?€?s nice list.
Competition details are available on the ,. There is also $5,000 reserved for the best solution using ,. More than 100 teams have already signed up.
,
,
,
,  "


















"
,
,
,, B.A., M.B.A., is the Vice President of Technical Sales for IBM?€?s Information Management 	  	     division and additionally leads its World Wide Competitive Database and Big Data teams. Paul is an award winning writer and speaker with more than 20 years of experience in Information Management and is seen as a global expert in Big Data and Analytic technologies. Independent groups often recognize Paul as a thought leader with nominations to SAP?€?s ?€?Top 50 Big Data Twitter Influencers?€?, Big Data Republic?€?s ?€?Most Influential?€?, Onalytica?€?s ?€?Top 100?€?, and Analytics Week ?€?Thought Leader in Big Data and Analytics?€? lists. Big Data Made Simple noted him as a ?€?Top 200 Big Data Thought Leaders on Twitter?€? and Technopedia listed him one of its ?€?Big Data Experts to Follow?€?. 
,
Paul has written more than 350 magazine articles and 19 books, some of which include ?€?Big Data Beyond the Hype?€?, ?€?Hadoop for Dummies?€?, ?€?Harness the Power of Big Data?€? and more.
,
,.
,
Here is second and last part of my interview with him:
,
,

, Big Data is relevant to everyone because of what I said earlier, it's a little more data than before. But it's a discipline, it's a mindset of enquiry. Personally, because SMBs tend to be more nimble, I think they can outdo their larger peer groups and take larger market share by finding efficiencies faster. It's a great time to be an SMB.
,

,

,For this simple reason. In the next few years, you are going to see a shift from the billions of people generating PBs of data today to hundreds of billions of devices generating ZBs of data - this is massive scale change and computers have to learn with us to take advantage of it. 

,
,

,I am just going to give you one (other than the obvious, which is governance). Don't start with a science project. Use BigData to solve business problems. And Big Data can't solve everything - for example, I don't expect the Toronto Maple Leafs to win the Stanley Cup anytime soon because they are getting into the data science game.

,
,
,
,Well there are multiple competitors out there. I think however the biggest thing that IBM does to distinguish itself from others are the following. First, we don't just talk about analytics ,for data at rest (be it in HDFS or RDBMS, or both) but we talk about taking the harvested insights there and getting the focus to analytics on data in motion. Second, cognitive. Third, governance. Fourth, consumability. Put that all together, and the stuff I didn't mention, and the point I'm making is we offer a platform. That platform accords for things like data integration, governance, reporting, search, data science, and more. I've not seen one vendor bring to market this kind of platform and capability.
,

,
,
,I will say this - the landscape is going to change like crazy. I alluded to this with Spark. I think HDFS is going to ,change - I really feel that file system has a lot of shortcomings. I think we will see more high level languages, more Apache projects, but I also think we will see a lot of consolidation. It's the wild wild west in Apache land. I mean look at Sentry and Falcon for security for example. Look at Hive on Tez and so on. I hope we will see even more zaniness in some of the names of these projects too.
,

,
,
,Someone once told me ""Don't let good enough...be good enough..."". And I never have...
,

,
,
,It's funny you asked me that, because I have a good track record here and people have been asking the same question. I don't spend a lot of time on qualifications. I mean, really, you are going to give me a CV and references - are they going to make ,you look bad? No. I didn't tell my wife my bad points on our first dates (I never told her, she had to find out that hard way...). Anyway, so I meet them. I get a vibe. Can you communicate? I'm not looking for some stats person that can't speak. I'm looking for someone that is well dressed, but has a hard time figuring out what to do on a Friday night: see a movie, play Wizard of WarCraft, or download the latest R module on CRAN. Then I get them to do something ... I put them out of their comfort zone. That's when I see the kind of character you have.

,
,
,
,The last book I read and liked was my own: ,. I get that sounds bad...but it's true, and I really do like it :). When I'm not working you are going to find me in the gym or doing hot yoga, unless I'm with my kid, who affectionately calls me ""Big DaDa"".
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "

"
, 
,
IKANOW's Community Edition is an open source, big data analytics platform that is built with industry-leading technologies such as Hadoop, elasticsearch, and MongoDB. This platform provides enterprises the flexibility, scalability, and openness to quickly and easily search and visualize data in meaningful ways. 
,
IKANOW recently integrated the ELK stack (elasticsearch, logstash, and kibana) to provide not only document-style intel analysis but also records or log-based analysis. 
,
Click the link below to download IKANOW's Community Edition for FREE!
,
,
,
Follow IKANOW at 
,  "
"
By , (BRIDGEi2i Analytics Solutions), Dec 2014
,
On the face of it, John Kotter?€?s seminal book ?€?Our iceberg is melting?€? is a simple tale of a group of penguins who are scared about losing their home, their iceberg, and yes, even more scared of the changes that could entail. But through that simple story and their struggle for finding their new home, the story delivers a more powerful message that could be increasingly relevant for today?€?s companies as they search for their isolated Icebergs of Analytics that don?€?t melt.
,
,
,
,
,
Fred, an unusually observant, curious and creative penguin. , Well, Fred, using his powers, observed that their iceberg, their home, was melting. Not one to just wait for his daily quota of squid, he spoke to one person in the leadership council who he felt would listen to him.
,
Enter Alice, one of the leaders of the colony, a practical and tough bird. ,. Of course Alice initially wondered if Fred was suffering from a personal crisis or if he missed his morning fish meal. But she did give him a patient hearing which rapidly changed to alarm when she saw for herself the data - the cracks and the fissures in their iceberg.
,
Alice brought Fred?€?s concern to the leadership council and here entered the other cast of characters in the book.
,
Louis ?€? Formal colony leader, well-respected and yet conservative. ,
,
NoNo- One of the colony leaders and negative influencer who kept trying to poke holes in Fred?€?s theory. , They are typically conspicuous by their alarming presence and doomsday predictions, all the time probably sipping their cup of coffee in the office watering hole.
,
Buddy ?€? Well liked, influencer, but not very powerful. T,
,
The Professor ?€? Intellectual influencer. ,
,
Scout and Sally Ann: Helping, excited, part of the change. ,
,
Well, the unlikely team of penguins waddled their way to a miraculous solution in the book, enjoying quite a few squids on the way as well, showing that in order to drive change, you need a vision and a team that can drive that change.
,
,
,
Now let?€?s cut to our world. The CMOs' world where the Marketing Iceberg is melting today in a plethora of confused paradigms. Alice, the CMO, know she probably has an average tenure of less than 60 months in her role. Data is staring at her across channels from transaction information to marketing automation and digital marketing platforms. She has to balance the Art of Marketing; historically the branding and the positioning of the company with the emerging Science of Marketing driven by data to personalize customer insights and drive improved Marketing Effectiveness. And maybe, there is no Fred in sight. What can Alice do?
,
Let?€?s look at the 8 steps for change outlined in the book and see how they will apply for an organization trying to improve their Marketing Effectiveness Strategy
,
,
,
Let?€?s hope Alice finds Fred and the magical team of penguins that can drive data driven Marketing change and impact over a nice meal of tasty squids. Would love to hear from all of you who are seeing the melting icebergs in your companies.
,
Can we stop the marketing icebergs from melting and learn from the nomadic seagulls as our penguin friends did? Is it time to look for Fred and his team?
,
Original: ,
,
,
,
,  "
"
By Michael Berthold (KNIME), Dec 2014.
,
,
We don?€?t want to join the usual discussion about what Big Data really is here ?€? ultimately everybody?€?s interpretation contains a grain of truth: Big Data is essentially about huge and/or highly heterogeneous and/or semi or unstructured data. However the real issue is: what do we use Big Data for?
,
To answer this question we need to take a look at the different stages of innovation or discovery. Initially all we do is to try and poke holes in the darkness. We look for connections of some sort such as correlations or clusters, basically anything that can help us gain insight no matter how small into the underlying system; sometimes that?€?s enough to already create business value. Secondly, later-stage data analysis goes on to create more abstract descriptions by focusing on parts of the system. This is classical data analysis, which utilizes more advanced data analysis methods. In the third and final stage we obtain a complete picture of the underlying system and our goal is to match the model?€?s parameters. This last stage is no longer really part of data analysis as we already fully understand how the system works. Typical advanced analytics focuses mainly on the second phase whereas exploratory data analysis is more focused on the first phase.
,
Big Data drives us back towards this first phase of that process. We, again, don?€?t really know much, if anything, about the underlying system and are looking for interesting connections in the underlying data to gain initial insights. Establishing these connections can create more value than before as they are now derived from much bigger/more complex (i.e. ?€?big?€? ?€? see above) data sources. This sometimes leads to the claim that in the age of Big Data all we need are such correlations. True, correlations based on much more and more diverse data are likely, albeit not always, more meaningful (beware of spurious correlations!). But that?€?s only part of the story ?€? we should not lose sight of the big picture: finding a model that describes as much as possible of the underlying system is the goal of data science. Big or small.
,
Tools to create these initial insights from Big Data are all the hype right now. However this also raises an interesting question: if we know how little we know about the Big Data world, how can we trust any one monolithic, proprietary platform provider to know what will keep us innovating and discovering new insights now and in the future?
,
,
The need for open platforms in classic data analytics is therefore even more pressing now, in the age of Big Data, when data analysts have easy access to an ever-growing number of internal and external data sources. To tackle this challenge they need quick and easy access to best-of-breed tools to intuitively explore new analysis ideas unburdened by the artificial barriers of closed environments.
,
Therefore also here, the five key pillars of an open analytics platform are vital for success:
,
,
And here is another thought: history keeps repeating itself. Expert Systems were supposed to be the solution to knowledge capturing ?€? but ended up ?€?only?€? being an important piece of a larger puzzle. Data Warehouses were supposed to solve the need for, I guess all of ETL, once and for all; they ended up being a solution for fairly static data structures but never really captured all of the data. And now we believe that by pumping all of our data into a large, distributed data storage environment we will solve all of our ?€?data problems?€??
,
I much rather foresee an interesting mix of unstructured, messy, heterogeneous, distributed Big Data storage facilities playing in concert with more organized, much better structured data repositories. Do we already know what this mix will look like? Do any of the analytic tool vendors know? Do we really want to repeat that mistake of locking ourselves and/or our data in with one single vendor and trust that vendor will know what we will need in a year or two from now?
,
My personal bet is on an open platform that allows selection of the best resources (data, tools, or expertise), unconstrained by a proprietary toolbox. Now and in the exciting years to come.
,
Bio: Michael Berthold is co-founder of ,, the open analytics platform used by thousands of data experts around the world.??He is currently president of KNIME.com AG and a professor at Konstanz University, where his research interests include bisociative data analysis and widening of mining algorithms.
,
Original: ,
,
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,, Dec 15, 2014.
,
This year saw continued strong demand for Data Scientists. 
Although ""data scientist"" Job Trends from indeed show the keyword peaking in 2013 and remaining flat in 2014, it may reflect changing titles and Data Scientists becoming more of a mainstream job.
,
, 
,
Many articles have been written about 
, - see also at the bottom of this post.
,
KDnuggets Job board , has received the all-time high number of jobs in 2014 (about 245 so far),  which is a sufficiently large sample to analyze and actually quantify the skills in-demand.
,
About 85% of the KDnuggets 2014 jobs are from US, but there are also jobs from 14 other countries: Canada, China, Estonia, Germany, India, Israel, Luxembourg, Malta, Portugal,
Serbia,
Singapore,
Switzerland,
the Netherlands, and the UK.
,
About 33% of the jobs had the title 
,, up from about 25% in 2013 and 19% in 2012.  
,
The second most common job title was , - with many different versions, such as BI Engineer, Machine Learning Engineer, Software Engineer, and more - see word cloud below.
,
,
,
Next we looked at the skills most in demand by examining the most frequent keywords in job descriptions.
,
The most common were general terms: 
,
,??,
This suggests that a data scientist job is a team effort focused on business analytics, with research, design and development playing a major role.  
Statisics, Machine Learning, and Data Mining are used almost synonymously. 
,
Looking at terms corresponding to more specific skills / languages we have 
,
,??,
NoSQL was mentioned in 11% of job ads.
,
The US map below shows the distribution of US-based jobs, with the size of the circle corresponding to the number of jobs and color corresponds to the log of ratio of SAS (blue) vs R (red) jobs.  
,
,
,
The next figure shows the distribution of SAS vs R for US cities.
,
,
,
We see that New York, San Diego, Rochester, Portland, and Dallas have more SAS jobs than R, while Seattle, Boston, Redmond, and San Francisco, CA are more R oriented. Chicago, Cupertino, and Palo Alto have about equal number of R and SAS jobs.
,
We also looked at interaction between top languages/systems, by measuring the 
,
, = actual number of jobs with the pair X,Y / expected number of jobs if X, Y were independently distributed.
,
We see that the strongest pairing is between R and Python (1.61 lift), but also between R and SAS.  The only negative lift is between SAS and Hadoop - less likely to be required together.
,
,
,
Finally, we looked at Education.
,
Almost all of the jobs required a graduate degree (Masters), and 48% of the jobs required or preferred a PhD.
,
What do you see as most-demanded skills for Data Scientists?
,
,
,
,??,
,
,
 ,  "
"
Most popular 
, tweets for Dec 07-14 were
,
Important paper! Researchers evaluate 179 #MachineLearning classifiers, find Random Forests & SVM give best results ,
,
Important paper! Researchers evaluate 179 #MachineLearning classifiers, find Random Forests & SVM give best results ,
,
Which one is the bunny? Google new CAPTCHA bot-trap trains #AI for #image #recognition , 
,
,
,
10 Data Science Newsletters to Subscribe to, from BigData-Startups to KDnuggets ,
,
,  "
"
Traditional relational databases weren't invented with mobile, social, and big data types -- or extreme scale -- in mind. These 16 NoSQL and NewSQL databases were engineered to address these types of challenges.
,
,
Original: ,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Dec 16 and later.
,
See full schedule at , .
,
,  "
"
By Blaise Aboh (Bizinvent), Dec 2014
,
Nigeria's presidential elections coming up in February 2015 is one of the most talked about topic today in thousands of tiny chatter across the media and social media platforms, the data lake created by this topic is overwhelming and the stats are continuously increasing, grossing millions of impressions.
,
Nigerians at home and in diaspora are communicating like never before, ranting and sharing their opinions about the elections, its major players, and most of all their choice and the changes they need.
,
Interestingly, the world at large has tuned in, and is listening and for this reason , created , website, a , aimed at visualizing information ?€? facts, ideas, topics, issues, statistics, queries available in volumes, velocity, veracity and variety ?€? all to a bare minimum of words and very beautiful imagery.
,
The idea is for the designed information to aid us understand the elections, cutting through the noise and revealing the hidden connections, patterns and the ordinary Nigerian citizen's dilemma underneath it all which in turn would lead to necessary implementation and transformation after all we all know a well informed electorate is a prerequisite for democracy.
,
Today, images have surpassed text as the preferred currency in social conversations and for this reason the platform was created, to smartly accumulate, assemble, manage, present, analyze and visualize data from thousands of these tiny stories and chatter about the Nigerians elections out there overtly fueling and engaging the social ecosystem in ways never seen before.
,
,
,
,
,
,is a creative and technology company that is focused towards providing enterprise solutions for small and medium scale businesses. Bizinvent is also the creator of ,, a publishing platform fostering social economic independence by advancing business, education and entrepreneurship amongst other verticals.
,
,
,
,  "
"
,

,

,

,

,
,
,
,
,
,??,
,
,
Based on above analysis, Derek Ruths and Jurgen Pfeffer provide a checklist of approaches when using social media data.
,
,??,
,
,  "
"
        ,
,

,

,

,

,

,??,
,

,

,

,

,

,

,

,

,

,??,
,

,

,

,
,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,

,
,
,
 ,
 

 
  "
"
,
,
Bringing the benefits of Big Data technology to the Operations department needs robust handling of two particular dimensions: space (or location) and time. A key requirement for optimizing logistics is to have a clear understanding of ?€?when?€? and ?€?where?€? information for all inventory. While most of today?€?s data architectures do include both these dimensions, there are significant limitations to spatial and temporal analysis that could be performed on these architectures simply because of their traditional design. SpaceCurve has developed a promising solution to help overcome these limitations and obtain deep insights through space-time analysis.
,

SpaceCurve has developed the first Spatial Data Platform specifically engineered to organize and enable the analysis of large-scale spatial data. The platform ,delivers unprecedented time-to-value using space and time as the key indices for all associated data. SpaceCurve continuously fuses geospatial, sensor, IoT, social media, location, and other streaming and historical data while making the data immediately available for analytics and operational intelligence.
,
The Internet of Things (IoT) requires immediate analysis of sensor representations of the real world for a wide range of purposes, from ,understanding consumer behavior to optimizing oil and gas production. For example, telecommunications companies can now monitor network sensor data in real-time, detecting patterns of human migration to understand the density of people in a given location, physical or virtual, at a particular point in time to improve services. In transportation and logistics, the movement of people and products from one location to another can be optimized by fusing real-time environmental conditions, traffic flows, and any other operational factors to constantly determine the most economical routes possible.
,
SpaceCurve?€?s ability to continuously index and store this data concurrent with queries enables fast, interactive spatial analytics that cannot be achieved using technologies currently on the market. This is combined with a computational geometry engine that is uniquely capable of performant and ultra-precise spatial analysis operations to ensure maximum fidelity for applications with global scope.
,
,
,
The product includes a database engine designed to support the continuous ingestion and high-dimensionality indexing of spatial data at the extremely high data rates typical of machine-generated data sources. It also enables ad hoc queries and complex operations that run concurrent with data ingestion, immediately reflecting all new data. SpaceCurve has implemented a new approach to computational parallelism that enables highly scalable ingest, storage, and analysis of complex space and time relationships across diverse, very large data sources. This is all accomplished on commodity Linux clusters, either on customer premise or in the cloud.
,
Here is my short interview with SpaceCurve CEO Dane Coyer and CTO Andrew Rogers:
, ?? ,
,
,
Big data platforms were originally invented to allow people to analyze the Internet. A decade ago, I was involved in some of the earliest attempts to bring live sensor data into these analyses. There was a big open question at the time: how could we analyze the real world in the same way we were analyzing relationships in the virtual world? We actually proved that this was not possible using big data technologies like Hadoop. SpaceCurve was created to be the first platform capable of analyzing the real world in real-time.
,
When the company was first launched in 2009, we really had no idea what the shape of the market would be for our platform. The recent emergence of the Internet of Things, ubiquitous sensors, and an increasingly mobile-centric world is creating an enormous market for applications that require a platform with SpaceCurve?€?s unique capabilities.
,
,
,
SpaceCurve overcomes two major limitations of traditional data architectures.
,
First, traditional data architectures are designed for the simple entity relationships common in enterprise, web, and social media data models where relationships are represented by shared keys. Spatiotemporal data models are unique in that all of your keys are multidimensional intervals and relationships are traversed by evaluating key intersection rather than equality. Existing scalable architectures lack the ability to efficiently express this type of relationship. SpaceCurve is the first architecture based on algorithms that can efficiently represent and analyze spatial relationships at extreme scales.
,
Second, the workload profiles typical of modern spatial data applications are not supported by traditional data architectures. Machine-generated spatial data sources tend to be extremely high velocity and high volume, far beyond the design assumptions of most big data platforms, requiring data architectures that can continuously ingest millions of complex records per second and store them to disk. Furthermore, these data models tend to be operational, requiring that these real-time data sources be immediately analyzable concurrent with that ingestion. Stream processing systems do not work because the analytics are rarely summarizations. SpaceCurve developed a completely new massively parallel database design that was purpose-built for these workloads.
,
,
,
That is like asking us to pick our favorite child. They really run the spectrum from exploiting data in existing enterprises, that up to this point was not possible, or the creation of entirely new business that are only now ,possible. I tend to be drawn to the complexity of fusing numerous existing data sources, both internally generated and externally available, together to unlock the value of that data for our clients. They may have many silos of spatially attributed data that have previously been ignored, or have they sub-optimized their business processes around the limitations of their existing technology. I have seen cases where there is tremendous value to be provided by the real time fusing of these data sets and providing insights when they are most valuable, but their current business process takes 24 hours or longer to create a very limited version of what the end customers really desire. The value of these insights decays very rapidly. 
,
As far as the future, I am surprised daily by the level of innovation emerging in this new sensor based world. Right now the industry is very focused on human decisions and control of what I will call ?€?single actor?€? scenarios. A human is in the decision making loop and the sensor technology is generally single purpose or single platform. We are able to spatially fuse this data together at the speed it is generated, but the main consumer is still human. Very soon much of the consumption of this data will be by other machines (things in IoT parlance), and by a wide variety of machines. Think of cooperating distribution networks, with autonomous vehicles being completely aware of their physical surroundings and being able to negotiate the most advantaged outcomes. 
,
,
,
The past few years has seen great growth in the Location Analytics arena, moving it from primarily points of interest on map based media to a much deeper, richer and more current collection of intelligence about predefined locations. I see these defined locations as containers you fill with details.  But it is still primarily ,?€?placial?€? in nature, not spatial.  It is still relatively static information placed on a cartesian system that is very limited in what can be done beyond the ?€?container?€?. There is also a currency issue where even the most advanced offerings are updated in batch mode. We will soon see a very different approach to the problem, utilizing real time streams of information that are fused with historical data bases. This will enable not only immediate situational awareness but will also be able to look at patterns and abnormalities by instantaneously comparing what is happening in real time to history.  Future capabilities will also not be limited to what is in the container, but be truly spatial, even global in their capacity. 
,
,
,
  "
"
,
Booz Allen and Kaggle are giving data scientists the opportunity to use their power for global good with the launch of the first-ever 
,
, (NDSB). The NDSB is an online competition that will challenge the data science community to develop an algorithm to advance the study of marine biology and ocean health. Participants will have 90 days to create an algorithm and the winners will get a share of the $175k prize ?€? the largest ever purse on the community for a project focused on social good.
,
Traditionally, identifying marine life involved dragging a net through the ocean, and collecting (and killing) intact specimens so that they could be manually tagged. A new technology called the In Situ Ichthyoplankton Imaging System (ISIIS) instead acts as a Xerox system for the ocean by scanning organisms in their natural environment.
,
The only problem is, it takes so many pictures that reviewing them all would take years.
,
Through the NDSB, data scientists will be given 100,000 images of plankton from the , and asked to develop an algorithm that will enable real time analysis and classification of the plankton within these images.?? Think of it as facial recognition technology for plankton.
,
The algorithm will help researchers be able to make real-time assessments on ocean health or management, like being able to predict whether the fishing industry will have a good year or how a tropical storm impacted the marine ecosystem.
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Dec 16, 2014.
,
With the holiday shopping season upon us,
new KDnuggets cartoon takes a look at Data Science of shopping 
recommendations produced by machine learning algorithms.
,
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,
,??,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, Dec 16, 2014.
,
To mark the ending year, I looked at the top 14 most viewed and most shared stories of 2014.  We can see several repeated themes - Deep Learning, Data Scientist career, education and salary, resources for learning Data Science tools like R and Python, and polls on popular languages for data science and data mining.
,
The most viewed story had over 10,000 views and most shared had over 500 shares! 
,
,
,
,
,
,
,
,
"
"
,
Latest ,, (Dec 17, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
""The machine learning algorithm wants to know if we'd like a dozen wireless mice to feed the Python book we just bought"", ,.  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,??is a doctorate candidate at the University of California, Berkeley, where he is working with??,??and the??,??team. His research interests lie at the intersection of databases, distributed systems and programming languages: in particular, he wants to know how the lessons from the first may be incorporated into the third, and to what degree this mitigates the difficulties of the second. To this end, his team is designing a collection of declarative languages for specifying and implementing complex distributed systems in a data-centric manner.??Before joining the PhD program, Peter worked at Ask.com.

,

Recently, Peter delivered a keynote at RICON 2014 on , (,, ,).
,
Here is my interview with him:
,
,
,
,: In an ideal world, yes, ACID transactions should allow the programmer to treat the datastore as a black box.?? Constraining the programmer to interact with the underlying data in an abstract, declarative fashion has a variety of benefits, one of the most important of which is *data independence* -- the application code, which is likely to have a much longer lifetime than physical storage layout or even data store implementations, is decoupled and protected from low-level details and changes.?? In such an environment the programmer need not worry about making code robust to failure.?? Transactions expose a universal and relatively simple failure mode: some transactions may (atomically) abort.?? The programmer need not worry about efficiency, since (in principle) an optimizer can adapt the query plan to what is known about the data distribution and environment.

,
,
,

Now, as I indicated in my talk, at large scale this facade begins to weaken.?? I relied heavily on optimizer hints when I ran ad-hoc queries against our data warehouse at Ask.com.?? I often ?€?chopped?€? massive updates or deletions into multiple sub-transactions to improve performance, but this violated the atomicity of the updates.?? Any weakening of the transactional facade -- whether by apply tricks like these, or running at a lower isolation level, or by abandoning a relational database in favor of a NoSQL store with weak or nonexistent guarantees -- means that programmers are *forced* to peek inside the black box, so that they can anticipate and guard against system-specific semantics (e.g., concurrency and recovery in relational databases, asynchrony and partial failure in distributed NoSQL stores).
,
I have mostly talked about what programmers are forced to do in practice.?? As for what programmers ?€?should?€? do, I believe we don?€?t have the right tools yet to give a satisfactory answer to that question.?? I?€?ll discuss some of my thoughts on this later in the interview.

,

,

,

,The ACID acronym, while a fine mnemonic, is not rigorously defined -- the ?€?C?€? is perhaps the most ambiguous term. The usual definition is the following: a database is consistent if none of its constraints are violated. We then assume that each transaction, if run in isolation, takes the database from one consistent state to another (otherwise the constraint violations would cause it to abort).?? Serializable execution assures that interleaved execution of multiple transactions produces the same effects as a serial execution.?? It is often possible to directly encode application-level correctness properties as database constraints.?? Even if we cannot do so directly, the transaction abstraction goes a long way towards bridging application- and storage-level guarantees.
,
,
,
Consider my example from the talk: an application programmer wishes to ensure that a particular bank account balance is non-negative.?? This is an application-specific property, but it can be expressed via a check constraint.?? Then serializable transactions *automatically* guarantee that the application-level property is upheld despite failure and non-determinism in the schedule.?? Even if the database cannot single-handedly uphold the correctness property (perhaps due to expressivity issues in the constraint system), it nevertheless makes it trivial for the application programmer to do so: he or she needs merely to ensure that their transaction, if run in isolation, leaves the database in a consistent (application-level correctness properties hold) state -- and abort otherwise.?? 
,
In the absence of transactions, by contrast, the programmer must manually ensure that the invariant is upheld by anticipating and remediating the various failures and schedules that can occur.?? This not merely adds complexity to the application programmer?€?s task, but requires them to reason about the failure modes of the *particular* data store, which could change over time.
,
I am not arguing that application programmers should reason about consistency simultaneously at the levels of objects, flow and language!?? Rather, I argue that each of these various levels represents an improvement over the current state of the art -- storage-level weak consistency models -- by reducing the impedance mismatch between application-level correctness properties and system semantics.?? Enforcing consistency at a particular level requires navigating other tradeoffs: for example, language-level consistency is closest to the application, but requires systems to be rewritten in new languages; object-level consistency provides an incremental path to adoption, but composition remains a challenge (see my answer to question #6). Flow-level consistency -- which enables us to combine object- and language-level approaches -- presents an interesting middle ground.?? I discuss these various levels in detail in our SOCC vision paper (,).

,

,

,

,I have been interested in using query languages to do general-purpose programming for some time.?? When I joined the BOOM team at UC ,Berkeley, the Declarative Networking project begun by Boon Thau Loo (now at Penn) and carried on by senior graduate student Tyson Condie (now at UCLA) had already demonstrated that NDLog and Overlog (Datalog-based languages enhanced with communication primitives) could concisely express networking protocols.?? My colleagues and I then set out to implement a large-scale distributed application -- the Hadoop/HDFS stack -- in Overlog.

,

The project validated our hypothesis that data-centric languages were a good fit for programming implementing data-intensive systems (,), but Overlog presented significant semantic challenges when we began using it to implement complex protocols, including atomic commit and consensus (,).?? Overlog (like Datalog) is a set-oriented language: rules express relationships among records in the ?€?database,?€? but lack the ability to express fine-grained relationships *between states* in time -- relationships such as mutation, atomicity, sequentiality and mutual exclusion.?? Worse still, Overlog?€?s semantics failed to account for uncertainty in distributed executions arising from nondeterministic message ordering, delay and component failure.

,

What was missing, it seemed to me, was a notion of time.?? Dedalus extends Datalog ,with a notion of process-local logical time, and with a minimal set of temporal constructs that allow programmers to precisely express (when necessary) ??distributed systems details such as time-varying state and uncertain communication (,).?? Dedalus allows us to associate distributed executions with a clean, model-theoretic semantics that simplifies formal analysis (LDFI is an example).

,

,

,

,In my RICON talk, I contrasted ?€?bottom-up verification?€? approaches (e.g., using a model checker to verify individual components such as protocols) with ?€?top-down testing?€? approaches (e.g., using a combination of integration testing and fault injection, ??as do systems like Chaos Monkey (and other monkeys) at Netflix). ????In practice, the latter approach seems far more common, for a variety of reasons that I discuss in the talk.?? Top-down approaches can be very effective in finding bugs in complex, large-scale systems that might be intractable to verify formally.?? Unfortunately, while such approaches are trivially sound (they only report ?€?true?€? bugs, since they actually *run* the system rather than reason about its executions in the abstract) they cannot guarantee completeness (finding *all* of fault-tolerance violations, or (most desirably) certifying that no violations exist).

,

LDFI is a top-down approach that provides a valuable completeness guarantee: if some combination of faults can prevent a known good outcome, LDFI identifies that collection of faults and produces a representative execution in which the violation occurs.?? It does so by reasoning formally about the lineage (the ?€?why?€?) of correct outcomes, and using it to inject only those failures that it can prove might have prevented a known good outcome.
,
,
,
,
,  "
"
,
,
,
Accord, a machine learning and signal processing framework for .Net, is an extension of a previous project in the same vein, ,. A set of algorithms for vision processing are included; it operates on image streams (such as video) and can be used to implement such functions as the tracking of moving objects. Accord also includes libraries that provide a more conventional gamut of machine learning functions, from neural networks to decision-tree systems.
,
, ,
,
,
,
Yet another machine learning project designed for Hadoop, Oryx comes courtesy of the creators of the Cloudera Hadoop distribution. The name on the label isn?€?t the only detail that sets Oryx apart: Per Cloudera?€?s emphasis on analyzing live streaming data by way of the Spark project, Oryx is designed to allow machine learning models to be deployed on real-time streamed data, enabling projects like real-time spam filters or recommendation engines.
,
, ,
,
,
,
As the name implies, ConvNetJS provides neural network machine learning libraries for use in JavaScript, facilitating use of the browser as a data workbench. An NPM version is also available for those using Node.js.
,
, ,
,
,
,
By now most everyone knows how GPUs can crunch certain problems faster than CPUs. But applications don?€?t automatically take advantage of GPU acceleration; they have to be specifically written to do so. CUDA-Convnet is a machine learning library for neural-network applications, written in C++ to exploit the Nvidia?€?s CUDA GPU processing technology (CUDA boards of at least the Fermi generation are required).
,
,
,
Google?€?s Go language has been in the wild for only five years, but has started to enjoy wider use, due to a growing collection of libraries. GoLearn was created to address the lack of an all-in-one machine learning library for Go; the goal is ?€?simplicity paired with customizability,?€? according to developer Stephen Witworth.
,
, ,
,
,
,
0xdata?€?s H2O's algorithms are geared for business processes -- fraud or trend predictions, for instance -- rather than, say, image analysis. H2O can interact in a stand-alone fashion with HDFS stores, on top of YARN, in MapReduce, or directly in an Amazon EC2 instance.
,
, ,
,
,
,
,
The Mahout framework has long been tied to Hadoop, but many of the algorithms under its umbrella can also run as-is outside Hadoop. They're useful for stand-alone applications that might eventually be migrated into Hadoop or for Hadoop projects that could be spun off into their own stand-alone applications.
,
,
,
Apache?€?s own machine learning library for Spark and Hadoop, MLlib boasts a gamut of common algorithms and useful data types, designed to run at speed and scale. As you?€?d expect with any Hadoop project, Java is the primary language for working in MLlib, but Python users can connect MLlib with the NumPy library (also used in scikit-learn), and Scala users can write code against MLlib.
,
,
,
Python has become a go-to programming language for math, science, and statistics due to its ease of adoption and the breadth of libraries available for nearly any application. Scikit-learn leverages this breadth by building on top of several existing Python packages -- NumPy, SciPy, and matplotlib -- for math and science work. The resulting libraries can be used either for interactive ?€?workbench?€? applications or be embedded into other software and reused.
, 
, ,
,
,
,
Among the oldest, most venerable of machine learning libraries, Shogun was created in 1999 and written in C++, but isn?€?t limited to working in C++. Thanks to the SWIG library, Shogun can be used transparently in such languages and environments: as Java, Python, C#, Ruby, R, Lua, Octave, and Matlab.
,
, ,
,
,
,
Weka, a product of the University of Waikato, New Zealand, collects a set of Java machine learning algorithms engineered specifically for data mining. This GNU GPLv3-licensed collection has a package system to extend its functionality, with both official and unofficial packages available.
,
Original: ,
,
,
,
,  "
"
Most popular 
, tweets for Dec 15-16 were
,
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping 
, 
,
,
Review: #DataScience at the Command Line - great book, comes with a pre-built virtual machine 
, 
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping 
, 
,
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping 
, 
,
,
,  "
"
        ,  "
"
,
,
,??is a doctorate candidate at the University of California, Berkeley, where he is working with??,??and the??,??team. His research interests lie at the intersection of databases, distributed systems and programming languages: in particular, he wants to know how the lessons from the first may be incorporated into the third, and to what degree this mitigates the difficulties of the second. To this end, his team is designing a collection of declarative languages for specifying and implementing complex distributed systems in a data-centric manner.??Before joining the PhD program, Peter worked at Ask.com.
,
Recently, Peter delivered a keynote at RICON 2014 on , (,, ,).
,
,.
,
Here is second and last part of my interview with him:
,
,
,
,: Let?€?s look at a really simple example (in an arguably unrealistic environment).  Consider two processes A and B; A sends a stream of 10 messages to B, and B computes some function over that stream, producing some output.  If we assume reliable, ordered delivery, the programmer of the system needs only to consider the correctness of  the function with respect to a single execution.  Now assume that messages can be lost or reordered (with just two endpoints, we can easily prevent this in practice using TCP, but we want our approach to generalize to unconstrained topologies).  How many executions does the programmer need to consider to ensure that B produces the correct outcome?  A quick back-of-the-envelope calculation tells us that in the absence of semantic knowledge of the function, even if no messages are lost we need to consider 10! (10 factorial, or ~3.6M) delivery order schedules -- does B produce a correct outcome for all of them?  Of course, it is worse than that: we need to consider every possible failure pattern.  There are 210 = 1024 of them -- for each, we need to consider every delivery order!
,
,

,

Common sense tells us that for most programs, many of these executions are not ?€?interestingly?€? different.  For example, if our function is commutative, we do not need to consider the reorderings at all.  But how do we know if our program is commutative?  If it is not, what will it cost us to rule out the nondeterminism in delivery order (in this toy example, just TCP, but for arbitrary topologies, consensus)?  And even if we know the function is commutative, what about those 1024 possible executions under failure?  Does A always reliably retry when messages are lost?  Are these retries always safe -- that is, is the function applied at B idempotent?  These are very difficult questions that distributed programmer must answer over and over again.

,
As I argued in the talk, these questions are too difficult to answer on our own: we need tool support!  Blazes and LDFI attempt to answer these questions in a general way.
,
,

,
,At a high level, a key conceit of the talk was that the NoSQL community has, as Eric Brewer argued in his 2012 RICON keynote, rejected the ?€?top-down,?€? user-centric guarantees of transactional databases in favor of the ?€?bottom-up,?€? developer-centric ethos of operating systems (simple, reusable components).  That community has had impressive successes delivering reusable, distributed data management components.  But it has also sacrificed something immensely valuable: a contract with the application programmer to hide system-specific failure modes.  

,
,
,
What can we do to restore that contract?  Part of the solution will involve shifting our focus from that *state* of components to the *data in motion* through components: that is, how components change data, not how it changes them.
,
,

,
,At ,, we were maintaining a large-scale data warehouse on Oracle RAC,  and stretching the limits of the technology.  It was taking too ,long for website impression and click logs to flow from a massively partitioned set of logging servers through ETL, summarization and presentation.  I was working on two projects in parallel: a ?€?real-time?€? in-memory database that received multicast logs directly from the site middle tier and evaluated continuous queries against them, and a massively-parallel distributed query engine that distributed ad-hoc aggregation queries over the collection of logging servers, harnessing their computational power.  Both were painstakingly implemented in C.  

,
It began to dawn on me that the systems I was implementing (essentially a massively-parallel query framework and a streaming database) were both essentially engines that converted declarative queries into one-shot, ?€?bespoke?€? distributed systems.  In fact, most of the distributed systems that we implement, use and maintain are just queries -- they describe, at a high level, how data changes as it flows through a network over time.  Doesn?€?t a static webserver just implement a join between a table of pages and a stream of requests?  Doesn?€?t a dynamic webserver do the same thing, but also join a stream of parameters and perhaps call a user-defined function on the result?  And looking deeper, aren?€?t protocols queries too?  They describe the messages a process sends as a view over the stream of messages it has received.  

,
When I came to berkeley (see my answer to #3) I was delighted to find that Professor Hellerstein?€?s group was already working on this problem. 
,
,
,

,There is so much important work to be done.  Here are just a few things that I can?€?t wait to get started on.
,
,Current debuggers, like a majority of programming languages, still reflect a sequential, single-site model of computation.  Just as I explored disorderly programming as a way of raising the level of abstraction in the *implementation* of distributed systems, I?€?d like to dive into disorderly debugging.  I am not sure yet what the ideal disorderly debugger looks like, but I am sure it doesn?€?t look anything like GDB.  The lineage diagrams produced to, ?€?explain?€? bad outcomes in LDFI are a step in the right direction.
,

I spent the first few years of my PhD on language design and implementation, and I think the time is ripe for another round of that.  The right language for a particular domain involves hiding the details that don?€?t matter and bringing into focus the details that do.  The early generation of declarative networking languages postulated that the detail that mattered was *data*.  Dedalus showed that *time* matters, and programmers should think hard about it!  Blazes?€? analysis indicates that time only matters when computations are sensitive to *order*. 
,

Tools like Blazes and LDFI were created to make it easier for programmers to build large-scale data-intensive systems.  There are many more such systems to build: I can?€?t wait to get started on the next one.
,
,

,

,The future will hold lots of changes in the memory hierarchy for which we will need to adjust, but one thing seems pretty obvious: distances can always increase, while the speed of light remains constant.  It will keep getting faster to access data that is ?€?close?€? -- local data will be ?€?closer?€? than ever.  But data is likely to continue to be more distributed (due both to efforts to exploit parallelism and to ensure reliability), and distance matters.  ,Keeping up with this trend -- exploiting the close data, keeping the pipes and processors full -- will involve doing a better job not of avoiding communication (which will become increasingly necessary) but of avoiding situations in which *local* computation needs to wait for that communication.  Minimizing how much distributed systems wait must begin with understanding exactly *when* and *why* they wait.  Bloom and Blazes provide a framework for reasoning about coordination requirements using program analysis.    These sorts of ?€?coordination analyses?€? are compelling because they are invariant to scale, which could range from the distance between processor cache levels to the distance between planets.
,
,

,
, by Nabokov, and , by Bellow.  Alas, I don?€?t have as much time for pleasure reading as I once did.  In fact, I used to spend nearly all of my time reading: I got my Bachelor?€?s degree in English Literature -- hence all of the references to Joyce and Stoppard in my talk.  
,
When I am not working I am spending time with my wife Severine and my daughter Beatrice, who is starting kindergarten next year.
,
,
,  "
"
,
By Gregory Piatetsky,  
,, Dec 18, 2014.
,
LinkedIn has unique resources for answering a question 
,
,
,
They analyzed the skills and experience for over 300 million LinkedIn member profiles.
The result is the list of 
,
,
,
,
I was pleased to see that the top skill is ""Statistical Analysis and Data Mining"", although I was surprised they did not call it ,.
,
Related skills were also ranked highly:
,
,??,
Last year, the 
, was Social Media Marketing, with 
Statistical Analysis and Data Mining ranked at #5, and Perl/Python/Ruby at #4.
,
In 2011 I examined 
, including a LinkedIn list of top data mining professionals at the time.
,
This functionality does not seem to be present now (perhaps it is part of LinkedIn Premium?), but LinkedIn search for ""Data Mining"" finds 585,000 results, including members, companies, etc. 
,
The top 5 members I see are
,
,??,
They are all my 1st connections, so you may see different results.
,
The top locations for ""Data Mining"" are 
,
,??,
Top companies with ""data mining"" skills were
,
,??,
Of course, ""data mining"" is in English, and for completeness, one needs to search in different languages, especially in German, Chinese, and French, but it is gratifying to see ""Data Mining"" as the most-demanded job skill in 2014.
,
See also
,
,
,
 ,  "
"
Most popular 
, tweets for Dec 17-18 were
,
#NIPS2014 #MachineLearning Trends: Rapid progress in #DeepLearning; Large scale ML practical 
, 
,
,
,
Open Source Tools for Machine Learning 
,
,
Google shutting down Freebase, the crowdsourced knowledge base bought in 2010 w. Metaweb, export data to Wikidata 
,
,
Why #Amazon Ratings Might Mislead You: The Story of Herding Effects #BigData 
, 
,
,
,
,  "
"
,
,
, is in his 11th season with the 49ers and his 5th as Director of Football Administration & Analytics. Previously Hampton served as the team's Manager of Football Operations for two seasons and as an analyst before that. He originally joined the 49ers as a football operations intern in late 2003. 
,
In his current role, Hampton is responsible for contract negotiations, strategic planning & management of the club's player compensation budget and salary cap. Hampton also manages the football R&D efforts, providing frequent analysis on special projects for the owner, head coach, general manager & president.
,
Here is my interview with him:
,
,
,
,Analytics is not new to football but it has been developing quickly over the last decade.  Each NFL team has multiple Quality Control Coaches whose primary function is a mixture of breaking down plays and applying predictive analysis to determine likely sequences and cues that can be built into the game plan.  Most of these coaches wouldn?€?t consider themselves to be using analytics but really that is just because the approach has been used for so long that it just seems like part of coaching to them.
,
There is an anti-technology rule in place in the NFL which prevents anyone on the sideline or in the coaching booth from using a computer or anything of the sort.  Anything that is going to be calculated has to be done in advance and memorized or brought on paper.  Hence the application of analytics being primarily in the time leading up to the game and not during the game itself.  Every coaching staff has a card, often laminated, with charts and guides to aid in certain decisions.  For competitive reasons I won?€?t divulge any of the specific contents of the cards but in general they contain strategy decisions for a variety of game situations based on careful calculations using many years of game data.  If a decision that might be faced during a game is a complicated one and the coach will only have a few seconds to choose a course of action, the grunt work in making the decision has to be done well in advance.
,
,
,
,If you rank the major team sports by the amount they use analytics you will find the order to be exactly the opposite of the sports when ranked by the number of people involved in a typical play.  Baseball is 1 on 1 and is the most advanced.  To be fair there are some amazing advances in individual sports as well.  Basketball has 1 on 1 aspects and never more than 5 on 5 and ranks 2nd.  Hockey is similar to basketball with the addition of a goalie and the added wrinkle of teams playing with a man advantage.  Soccer and Football are both 11 on 11 but if soccer ?€?plays?€? are broken down into passing sequences then only a handful of people are in a specific play.  Football truly has an important role for all 22 players on each play with the success of each person highly dependent on the execution of assignments by others. 
,
,
,??,
The biggest challenge in football analytics is getting a sufficient amount of reliable data.  To do this properly requires evaluating each player on each play when you at best know the assignment of half of the players on the field and many of them have adjustments and options impacting their tactics.  This means our data is bound to have a lot of ?€?noise?€? in it.  We have to account for much more just to get to the starting point of analysis.
,
,
,
,The original draft trade value chart was built from observations, not value.  The result of negotiated trades that had been executed over many years were charted under the assumption that the value of the picks were equal in all historical trades.  A line of best fit was translated into numerical values and those values could be compared to evaluate future offers.  The teams using that original chart just wanted to make sure they were getting fair value or better based on historical trade values which in turn made them more confident in doing those deals, they traded more often and the chart proved itself accurate as more and more deals came in along the same lines.
,
When Bill Walsh used the chart but instinctively felt like he was getting better value by trading down than trading up, he questioned the methodology and commissioned an analytical project to create a new chart from scratch.  The 49ers chart was born.  A variety of factors (such as the rookie wage scale) change the relative values of picks so the chart we use continues to evolve.  This is one area where an analytical approach has gained the most acceptance because it is almost entirely about math and not about technical football details.  This opened a door to apply similar concepts to other areas of putting a team together.  
,
Now many teams use a form of similarity scores to compare prospects to pros and the data being collected is rapidly expanding.  Scouting is and will remain to be the most critical part of the process at both the beginning (identifying the pool of players) and end (marrying all of the quantitative and qualitative information about the player and his fit to the team) but analytical components are now contributing very heavily in between to inform the decisions.
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,, Dec 19, 2014.
,
Eric Siegel, a noted Data Scientist and the co-founder of 
, was recently on TV with great news: Computers (and AI) aren't going to kill you! 
,
A day prior, BBC showed Stephen Hawking say, 
,
,
Then Al-Jazeera America news asked Eric Siegel to comment. 
See a 1-minute clip with this discussion below 
,
and if you are human, vote in KDnuggets Poll: 
, (bots please refrain from voting).
,
,

,
,
,  "
"
, release,

We are proud to announce the release of ,, a project funded by the European Research Council (ERC) and headed by prof. , at the Sapienza University of Rome.,

BabelNet is a very large , and , created by means of the seamless integration of the largest multilingual Web encyclopedia - i.e., Wikipedia - with the most popular computational lexicon of English - i.e., WordNet, and other lexical resources such as Wiktionary, OmegaWiki, Wikidata, and the Open Multilingual WordNet. The integration is performed via an automatic linking algorithm and by filling in lexical gaps with the aid of Machine Translation. The result is an encyclopedic dictionary that provides Babel synsets, i.e., concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations.,

, comes with the following features:,
,
,
More statistics are available at: ,

Enjoy!,
The BabelNet team,
,

,
,
  "





















"
,
,
, is in his 11th season with the 49ers and his 5th as Director of Football Administration & Analytics. Previously Hampton served as the team's Manager of Football Operations for two seasons and as an analyst before that. He originally joined the 49ers as a football operations intern in late 2003. 
,
In his current role, Hampton is responsible for contract negotiations, strategic planning & management of the club's player compensation budget and salary cap. Hampton also manages the football R&D efforts, providing frequent analysis on special projects for the owner, head coach, general manager & president.
,
,.
,
Here is second and last part of my interview with him:
,
,
,
,The data limitations of 11 on 11 matchups are the most limiting factor.  Situational analysis is also complicated by the fact that many situations haven?€?t been faced very often and the game is evolving.  Plays from 10 years ago may not be useful today if the schemes and style of play are different.  There are many advances that are held back by a lack of data.  The anti-technology rule is also limiting to analytics but for the cause of increasing the chances that the winner of a game triumphs based on preparation, physicality and player performance and not better algorithms.

,

,
,
,Every other team in the league has already figured out all of the answers and has nothing to gain from further work.  We don?€?t tell other teams what we are doing and don?€?t expect them to tell us what they are doing so it is very hard to compare.  We strive to be the best in everything that we do but the prize we seek is the Lombardi trophy and that is what we judge ourselves on.
,
,
,
,Player tracking information is the next frontier.  Already quite a few teams have expanded their training staffs to make use of motion information and get the most out of each bit of energy on the practice field and in games.  Big data companies are making pitches left and right to try and improve team processes but I see the player monitoring information having a bigger impact over the next few years.  Game strategy and instruction remains largely up to the coaching staff but putting better athletes on the field is the arms race happening off screen.
,
,
, ?? ,
,
,
,?€?You don?€?t know what you don?€?t know.?€?  If you think you have all of the answers you are bound to be passed by.  If you keep inquiring, innovating, developing, challenging, etc then you will find ways to improve.  This is a mentality that I try to apply to my work.  Keep doing what works but find ways to make it better.  At the same time do more; to expand your role, your understanding, your potential.
,
,
,
,The collection of books I intend to read ,keeps growing relative to the books I have read.  Within my department we have started a process of pulling the key excerpts of books, journals or articles that can make us better and sharing them with each other.  I believe I have now read almost the entire book , by Daniel Kahneman because of this but I don?€?t actually even now what the cover looks like.
,
Before becoming a father a couple summers ago my main hobby was being a triathlete.  I managed to accomplish a life goal of completing an ironman triathlon (2.4 mile swim, 112 mile bike, 26.2 mile run all in a row) and then traded in the training hours for time spent being dad.  Taking the stroller and going running is probably my favorite activity these days.
,
,
,  "
"
By Roman A., Dec 2014.
,
Looking for a data scientist to help me analyze mined data of several companies in the 'share' economy vertical. We will be analyzing demand trends, as they relate to location and price. Our focus will be on predictive analysis based on past results. Willing to pay on a per hour or per project basis.
,
Please contact 
,
,  "
"
,
By Neil Biehn, Dec 2014
,
Organizations have spent billions of dollars on ERP and CRM systems, and today they?€?re sitting on petabytes of big data gold mines. The most astute CEOs are looking for new opportunities to use their data assets to extract predictive and prescriptive analytics that evaluate how their companies are performing. From sales, to pricing and profitability, and even customer attrition, companies are learning to use this data to better serve their customers, and drive value for their products and services. As 2015 unfolds, we?€?ll see changes in the world of big data as even more companies use predictive and prescriptive analytics as competitive differentiators:
,
,
,
In 2015, we?€?ll see the buzz term ?€?big data?€? significantly erode and begin to vanish. In contrast, we?€?ll see increased focus on the hidden assets found in data using predictive and prescriptive analytics. It?€?s these analytics that provide actual business value and help companies make easier, faster and smarter decisions about how to better engage with customers and drive top-line revenue. CEOs are looking not for more data, but how they can connect data with predictive and prescriptive insights to capture strategic business value resident in their systems.
,
,
,
Over the past few years,??we've??seen the term data science associated primarily with tools and software technology.?? In 2015, we expect a return to its scientific roots -- listening closely to the business in order to??hypothesize potential solutions, testing those hypotheses with data, observing their outcomes and finally, recommending a solution.??We see a big shift from buzz words and technology platforms into the fundamental principles of the Scientific Method.
,
,
,
The amount of data in corporate infrastructures grows every year both in volume and complexity. In 2015, if companies aren?€?t using prescriptive and predictive analytics to take advantage of the strategic assets resident in their vast array of systems, they?€?re losing their ability to compete and win. The art of doing business grows in complexity every year, and prescriptive and predictive analytics are now table stakes.
,
,
,
Data-driven Silicon Valley organizations like Yahoo and even the City of San Francisco have paved the way for the emerging CDO role. Beyond the Valley, you may not find many CDOs, but we?€?ll see a big increase in the need for more data governance in 2015 as data moves from an opportunity to an asset.
,
While we can?€?t know for sure if each of these things will come true, we do know that the world of big data is changing. It?€?s no longer about just having access to data and the ability to store it, but instead the ability to achieve actionable results with data through predictive and prescriptive analytics. Only time will tell how this evolves, but if you aren?€?t leveraging data to compete and win, it?€?s time to get on board.
,
, ,
,
,
,
,  "

"
,
,
Literally meaning ?€?Quickly done, well done?€?, I used to say that as a kid, when I didn?€?t want to spend too much time on a chore; ?€?Don?€?t worry mum, it was ?€?quickly done, [but] well done?€?!?€?
,
If a ?€?but?€? was implied with my mum, our most recent work on time series showed that classification can actually be performed better , faster.?? As part of a team of researchers spanning Australia, France and the United States I have just presented this work at the ,. Before getting to the content, I want to acknowledge their amazing work: Thank you Germain, Geoff, Ann, Yanping, and Eamonn!
,
,
,
Classification of time series is one of the hot topics in data mining. It is used to recognize gestures, detect abnormal ECGs, and even to find the species of a mosquito from its ?€?buzz?€?.
,
The data mining community has shown two somewhat surprising things:
,
,
,
This is not the place to discuss , this is true, but there is a strong consensus of the community that it is, supported by large scale reproducible experiments (see [a,b,c]).
,
,
,
The main issue is that using it to classify takes time. Even if recent advances have basically made the time complexity of DTW linear with the length of the series, NN cannot classify the query without looking at each time series of the database.
,
Whether you?€?re trying to decide if the insect that just flew past your sensor is a potential carrier of the Dengue virus, or recognize a gesture on your Xbox, there are many situations in which you don?€?t realistically have time to scan the whole database to obtain your classification.
,
,
We usually respond to this problem by using sampling: we satisfy our time constraints by comparing the sample to only a subset of the training database. The picture on the right shows the general behaviour of NN on our insect case study: the more examples you use, the better the classification is. Smart sampling techniques have also been studied for NN, for instance exploiting the fact that if an object of the training set is surrounded by objects of the same class, then this object might be ignored, because any query that falls within its neighborhood would be classified correctly even without it [d].
,
,
,
Another approach to remedying the shortcomings of NN-DTW is summarizing the database with , prototypes (as opposed to prototypes from the database itself). This is the idea exploited by one of NN?€?s variations called the , classifier (NC), which replaces the database with the centroids of a few clusters to which the queries are then compared against.
,
Why is that better than sampling the training set directly? Because the , very often captures the ?€?essence?€? of the set it summarizes much better than any object of the set can.
,
This idea that the , of a set of objects may be more representative than any , object from that set dates back at least a century to a famous observation of Francis Galton. Galton noted that the crowd at a county fair accurately guessed the weight of an ox when their individual guesses were averaged [e]. Galton realized that the , was closer to the ox's true weight than the estimates of most crowd members, and also much closer than any of the separate estimates made by cattle experts.
,
,
,
,
The issue for time series is that averaging might be , complicated. The reason is that defining the mean is actually an optimization problem for which the measure that induces the space is central. DTW induces a very complex space. Broadly speaking, this is because it optimally realigns the time series. This makes the definition of a DTW-consistent average time series challenging to say the least! Computational biologists working on the same problem on sequences even call it the ,[f]. In fact, everything seems to indicate that this problem is NP-complete [g], which means that we need a good approximation.
,
This is where we intervene with our averaging method: ,(DBA). In short, DBA is an expectation-maximization scheme that takes inspiration from the work of our colleagues in computational biology, but is specifically designed to tackle the particular needs of time series comparison and classification, such as autocorrelation (the likelihood to find an element similar to at ). We have demonstrated that DBA not only significantly outperforms competing methods, but is also very efficient (in fact, during my work for the French Space Agency, it was common for me to average millions of time series on my desktop computer).
,
,
,
Let?€?s put everything together: take each class of the training set, launch K-means (or your favourite clustering algorithm), construct an average time series per cluster, and use these centroids as the prototypes to compare each query against.
,
,
,
When we started this work, I was hoping to improve a bit on the ?€?smart?€? sampling techniques, i.e. that we would get faster without losing too much predictive power. In our experiments on 44 datasets, we did get a few that managed to do so (?€?Gun point?€? for instance ?€? bold lines are based on average time series). However, we often managed to achieve faster,better results, such as when classifying the insects. In fact, you can see there that even with one single prototype per class, we are able to improve on the full 1-NN classifier, with a 100x speedup.
,
,
,
Well, the short answer is Yes: we did demonstrate the statistical significance of the results I have intuited above.
,
The longer answer is that there is something even more exciting: we are starting to , models of time series. I?€?m saying this because in a way, NN does not really ,the data; its ,phase does not do anything but storing the raw data, without trying to make sense of their similarities and differences. To a data scientist, this is quite frustrating, because it is admitting that we don?€?t really understand how the classes are.
,
Our results however suggest that ,a model for time series can be useful. I know; computing a ,is a baby step toward what we call ,But having tried to make the most of time series for a few years, I definitely take it!
,
,
,
You?€?ll find more resources on my website ,, including the paper, the source code and the slides. Don?€?t hesitate to get in touch: ,, or on twitter at 
,.
,
,
,
[a] A. Bagnalland J. Lines, ?€?An experimental evaluation of nearest neighbour time series classification. Technical report #CMP-C14-01,?€? Department of Computing Sciences, University of East Anglia, Tech. Rep., 2014.
,
[b] X. Xi, E. Keogh, C. Shelton, L. Wei, and C.A. ??Ratanamahatana, ?€?Fast time series classification using numerosity reduction,?€? in ,, 2006, pp. 1033?€?1040.
,
[c] X. Wang, A. Mueen, H. Ding, G. Trajcevski, P. Scheuermann, E. Keogh: Experimental comparison of representation methods and distance measures for time series data. Data Min. Knowl. Discov. 26(2): 275-309 (2013)
,
[d] E. Pekalska, R. P. Duin, and P. Pacl??k, ?€?Prototype selection for dissimilarity-based classifiers,?€? Pattern Recognition, vol. 39, no. 2, pp. 189?€?208, 2006.
,
[e] F.??Galton, ?€?Vox populi,?€? ,, vol.??75, no. 1949, pp. 450?€?451, 1907.
,
[f] D. Gusfield, Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology, Cambridge University Press, January 1997, ch. 14 Multiple String Comparison ?€? The Holy Grail, pp. 332?€?367.
,
[g] L. Wang and T. Jiang, ?€?On the complexity of multiple sequence alignment,?€? Journal of Computational Biology, vol. 1, no. 4, pp. 337?€?348, 1994.
,
,
,
Francois Petitjean tries to develop useful solutions for big data. He completed PhD working for the French Space Agency in 2012, and since then was working in Geoff Webb?€?s team at Monash University?€?s Centre for Data Science.
,
,
,
,  "
"
By Ran Geva, Dec 2014.
,
,
So you need to obtain structured data from the world wide web. You could develop a proprietary crawling technology, modify an existing one, or you could look for a service that already provides a solution. I suggest the latter.
,
Data crawling services are divided into two main categories, the DIY (Do It Yourself) and the CAS (Crawlers As Service) solutions. In this post I will look into one solution from each category.
,
,
,
The crawlers behind?? , are already crawling hundreds of thousands public sources, downloading millions of posts a day. The output is unified and structured, where each post contains all the meta-data extracted, such as the actual post text, author, date and time, title, link, section details and more. You can consume the produced structured data, either via an API or a firehose. You can also request the addition of a new source and get it added to the crawling cycle at any time.
,
,
,
,
,
,
,
,
,
, lets you define and scrape data from individual websites into a structured format. It is perfect for gathering, aggregating and analysing data from websites without the need for coding skills. The tool allows people to create an API using their point and click interface. Just download the import.io browser, and from there it will guide you and asks you a few questions about the data you're trying to gather. Import.io is completely free for the time being and is available on Windows, OSX, and Linux
,
,
,
,
,
,
,
,
In short, both DIY and CAS approaches are valid, you just need to choose the right tool for the right job.
,
,
,  is responsible for the technological and business development of Buzzilla LTD (the company behind Webhose.io), as well as research and development of future products. Ran, one of the founders of Buzzilla LTD, has 20 years of experience in technology development and customer/server-related systems.
,
,
,
,  "
"
By Andrew Jennings (FICO), Dec 2014.,
,
, The hype around Big Data has finally crested. Big Data will always be with us, and it will keep getting bigger. But business users have come to the realization that analytics is the key to unlocking value from Big Data. In 2015, more companies will cultivate in-house analytic competencies and many will embrace analytics as an important part of their company cultures.,

,

, With cybercrime and cyber terrorism on the rise, the good guys are finally moving beyond their reactive approach to security. In 2015, police, military and intelligence agencies will employ analytics to predict when, where and how the next attack will occur so such attacks can be thwarted before damage is done.,

, The continued expansion of analytics is predicated on making the process as simple as possible and codifying the steps that many analytic experts take for granted. This is not without some danger, but I believe we will see significant investment in this area.,

, Every tweet, chat session, call center conversation, and customer support email is going to be analyzed to accelerate problem resolution, optimize scripts for sales people, enhance shopping experiences, make products less confusing, and increase compliance.,

, In most cases whether the underlying relationship is causal or correlative is irrelevant. In business circles, people want to make better decisions. They need to know what business levers to pull to get closer to the desired result; the why is secondary. Overall the important issue is to gather good data and trust it. As famed statistician Bradley Efron said, ?€?Those who ignore statistics are condemned to re-invent it.?€?,

, is FICO's chief analytics officer and head of FICO Labs. He has held a number of leadership positions at FICO since joining the company in 1994, including a stint as director of FICO's European operations.,

,
,

  "
"
,, March 17, 2015
,

While Chicago is known as the city of big shoulders, on March 17, 2015, this world-class business city will shoulder a must-attend, groundbreaking ,
,.   In the shadows of downtown?€?s biggest buildings, the University of Illinois at Chicago?€?s Center for Research in Information Management (CRIM) is hosting  the 2015 CRIM Symposium, ?€?Big Data Analytic Insights.?€? The Symposium will showcase how BIG Data is more than just the latest business buzz word.,

Come network with the BIG Data thought leaders as they discuss innovative ways of solving business challenges with BIG Data, Emerging Capabilities, Social Network Analytics and Visual Analytics. Don?€?t miss the keynote address by Bill Franks, Teradata?€?s Chief Analytics Officer and the CRIM Analytics Competition that promotes creative solutions to a defined problem in conjunction with a Chicago-area CRIM business partner.,

,

Click 
,
,
,
,
  "
"
Most popular 
, tweets for Dec 15-21 were
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping , 
,
,
,
Review: #DataScience at the Command Line - great book, comes with a pre-built virtual machine 
, 
,
,
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping 
, 
,
,
KDnuggets Cartoon: Unexpected #MachineLearning Recommendations for your holiday shopping 
, 
,
,
,  "
"
,
,Sydney, Australia, Aug 10-13, 2015.
,
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) is a premier conference that brings together researchers and practitioners from data mining, knowledge discovery, data analytics, and big data. 
,
Submissions to KDD 2015 are solicited on all aspects of knowledge discovery and data mining.
,
Key dates:
,
,??,
KDD is a dual track conference hosting both a Research track and a Practice track as outlined below. A more detailed CFP that includes reviewing criteria, formatting guidelines, the dual submission policy, etc. is available 
at ,. 
,
,
,
The Research Track invites submission of papers describing innovative research on all aspects of knowledge discovery and data mining, ranging from theoretical foundations to novel models and algorithms for data mining problems in science, business, medicine, and engineering. Visionary papers on new and emerging topics are also welcome, as are application-oriented papers that make innovative technical contributions to research. Authors are explicitly discouraged from submitting incremental results that do not provide significant advances over existing approaches.
,
Papers submitted to the Research Track are solicited in all areas of data mining, knowledge discovery, and large-scale data analytics, including, but not limited to:
,
,??,
,
,
The Practice Track invites submissions of papers describing research and implementations of data mining/data analytics/big data/data science solutions and systems for practical tasks and practical settings. The application domains of interest include, but are not limited to education, public policy, industry, government, healthcare, e-commerce, telecommunications, law, or non-profit settings. Our primary emphasis is on papers that advance the understanding of, and show how to deal with, practical issues related to deploying analytics technologies. This track also highlights new research challenges motivated by analytics and data mining applications in the real world.
,
Submitted papers will go through a competitive peer review process. The Practice Track (formerly known as the ""Industry and Government Track"") is distinct from the Research Track in that submissions solve real-world problems and focus on systems that are deployed or are in the process of being deployed. Submissions must clearly identify one of the following three areas they fall into: ""deployed"", ""discovery"", or ""emerging"".
,
For criteria for submissions and more information, visit ,
,. 
,
,
,
,
,
The KDD 2015 organizing committee solicits proposals for full-day and half-day workshops to be held in conjunction with the main conference. The purpose of a workshop is to provide an opportunity for participants from academia, industry, government and other related parties to present and discuss novel ideas on current and emerging topics relevant to knowledge discovery and data mining. Workshops are (tentatively) scheduled for August 10, 2015.
,
,
,
Each workshop should be organized under a well-defined theme focusing on emerging research areas, challenging problems and industrial/governmental applications. Organizers have free controls on the format, style as well as building blocks of the workshop. Possible contents of a workshop include but are not limited to invited talks, regular papers/posters, panels, and other pragmatic alternatives. In case workshop proposers need extra time to prepare their workshop, early decisions may be considered if justified.
,
The goal of the workshops is to provide an informal forum to discuss important research questions and practical challenges in data mining and related areas. Novel ideas, controversial issues, open problems and comparisons of competing approaches are strongly encouraged as workshop topics. In particular, we would like to encourage organizers to avoid a mini-conference format by (i) encouraging the submission of position papers and extended abstracts, (ii) allowing plenty of time for discussions and debates, and (iii) organizing workshop panels.
,
,
,
Possible workshop topics include all areas of data mining and knowledge discovery, machine learning, statistics, and data and information sciences, but are not limited to these. Interdisciplinary workshops with applications of data mining and data sciences to various disciplines (such as health, medicine, biology, sustainability, ecology, social sciences, humanities, or aerospace) are of high interest.
,
For more info and submission details, see
,
,
,
Workshop proposals should be emailed to , by March 6, 2015 at 11:59 PM Pacific Standard Time.  "
"
,
,
Wearables ?€? a technology which aims to make life easier. With wearable technology, learning more about yourself has not only become high tech but also real time. From devices and apps that help you track heart rate and food consumption details to gadget that monitor your mood and even surrounding air. According to an April 10 press release from International Data Corporation (IDC), a research company that analyses future trends, ?€?Wearables took a huge step forward over the past year and shipment volumes will exceed 19 million units in 2014, more than tripling last year?€?s sales. From there, the global market will swell to 111.9 million units in 2018.?€?
,
,
,
I believe there are major four ways wearables can help improve our life:-
,
,
Today the biggest market in wearable technology is health and fitness. Big companies are putting wearables to work to figure out how to use these kinds of gadget to improve their business. They are giving wearables to employees and customers to gather subtle data about how they move and act and then use that information to help them do their jobs better or improve their buying experience. However there is a big risk involved. People will naturally resist real world intrusion into their privacy, so businesses needs to be very careful about asking employees and customers to strap gadgets on their head, chest, wrists etc. This compels me to think that we need to truly evaluate the real need of wearable technology. Much of what is being done with wearable devices is happening simply because it can be done. However, several users still are not sure about wearables and whether they want to walk around with devices strapped to them all day. Is this the paradox of wearables?
,
,
,
Today, each one of us has so much personal digital data flowing out there that it is possible for someone to steal an entire online identity and cause real damage offline.
,
You already know that your personal information and references to your social media presence on Facebook, LinkedIn, Twitter, Instagram, Flickr etc. are all over Google. This is the data that you know about and it is just a fraction of what can be unearthed with a little drilling, what is really scary is the data that you don?€?t know is constantly being collected, like your location data. Multiple apps collect location data and track your movements 24x7. Apart from your mobile phone if you use a smart card to pay road-toll or access public transport, you can be tracked by that as well. Some companies are taking this to the next level by using location data to confirm that employees not in the office are actually where they claim they are.
,
On the financial front, every time you swipe a credit or debit card you release more digital information. A marketer may analyse credit card purchases and deduce likely interests. Online retail giants like Amazon, eBay uses such deduction algorithm when it offers hints like ""people who viewed this product also looked at the following products"".
,
Indian players like Myntra and Flipkart use similar analyses. Recommender systems can help people to find interesting things. Amazon?€?s recommendation system has helped the technology giant to reap billions in sales increase, Netflix is another such success story.
,
Put all these bits and pieces together with just a little online snooping and you could create a detailed composite of an individual?€?s identity. This may sound like a crime fiction but the basis for it is visible everywhere, if you know where to look. If this personal information falls into the wrong hands, it can lead to a wide spectrum of cyber-abuse like employment or housing discrimination, higher insurance rates, identity theft, or targeted advertising. Maybe it?€?s high time you give a thought to the question ?€? ?€?how public is your private life.?€?
,
Having said all this, I see a silver lining, with wearables every individual becomes a data generator and transmitter. We generate data that is continuously collected by various government agencies and private companies. This data can be monetised and can also be used to make life easier for the people, what we need to make sure is that the data do not get manipulated or misused.
,
Rohit Yadav is a Solution Manager ?€? Customer Intelligence at BRIDGEi2i Analytics Solutions Pvt. Ltd. in India.
,
,
,
,  "
"
Most popular 
, tweets for Dec 22-23 were
,
Great list: 20 new #data #viz tools of 2014: Visage, Landline, Yby, Mirador, Plotly, Visits 
, 
,
,
,
Great list: 20 new #data #viz tools of 2014: Visage, Landline, Yby, Mirador, Plotly, Visits 
, 
,
,
Great list: 20 new #data #viz tools of 2014: Visage, Landline, Yby, Mirador, Plotly, Visits 
, 
,
,
,
Review of #MOOC Learning from Data (#MachineLearning) - the class that changed everything 
, 
,
,??,
,??,
,
,  "
"
,
,
The countdown for 2015 has already begun. After making tremendous progress in 2014, ,Data Science is ready to witness new trends emerging and dominating in 2015. While some of these trends are widely acceptable, others are slightly debatable. Only time will tell the real winners. Until then, we all would have to decide for our self which 2015 trend to believe in.
,
,, a people-powered data enrichment platform, recently released an interesting infographic based on the response of their data scientists?€? expectation for 2015. Here are a few insights from their report:
,

, will take the hot seat, while the Chief Information Officers (CIOs) are pushed to the back seat, for driving the executive decisions in 2015. 
 ,
,
,
,?€? This one is no surprise as we have already seen a lot of articles in 2014 criticizing the over-use of ?€?Big Data?€? term, rendering it essentially meaningless. Like ?€?Innovation?€? or ?€?Disruption?€?, ?€?Big Data?€? is being used today for pretty much anything and everything related to Data Science, and this definitely needs to stop.
,
Following the dominance of Open Source in Big Data technology,, is expected to gain significance in 2015. People are increasingly realizing the value of unlocking data from the silo-ed data warehouses and making it freely, easily accessible.
 ,
In 2014, we saw Data Scientist positions being established in various departments across organizations. In 2015, we expect these , (within as well as across organizations) in order to capture greater value from data.
,
The progress of Big Data has so far been driven by corporate incentives for greater profit. But, now we are starting to see the value of ,. We expect more initiatives and conferences on this in 2015, than we saw in 2014. 
 ,
,
,
, ?€? The ability for anyone, and not just data scientists or geeks, to be able to analyze and use data can be a disruptive force in 2015. Several software releases in 2014 have focused on massive consumerization of analytics, and we are yet to see what users will do with this power (combined with Open Data initiatives).
 ,	
Further details and the complete infographic can be found here: ,
,
,
,
  "
"
,
,
, is currently serving as Vice President of SDN Cloud Services  and CTO, IBM Cloud Services Division and as a faculty member for the Cloud and Internet-of-Things Expos.
,
Mac has 25 years of experience with networking and virtualization. He became an IBM Master Inventor in 2006, an IBM Distinguished Engineer in 2008 and is a Member of the IBM Academy of Technology since 2009. 
,
He served 2 years as the Chief Architect for IBM System z Enterprise Software and 2 years as the CTO for IBM Cloud partnerships and client innovations within the IBM Corporate Strategy team.
 ,
Mac served 2 years as Director and CTO Cloud Portfolio within IBM Global Technology Services. He leads the technical due diligence for the SoftLayer acquisition which lead to the creation of IBM Cloud Services Division. He co-authored Springer?€?s Handbook of Cloud Computing used by colleges and universities
,
Here is my interview with him:
,
,
,
,The definition of a perfect storm in weather is a ""??rare combination of circumstances leading to an event of unusual magnitude"". That is what I see happening across the IT landscape. ,Cloud Computing technology is allowing customers to tap into almost ""infinite"" capacity. BigData is transitioning away from ""batch orientation"" to real-time, streaming analytics. Embedded sensors are getting smaller, smarter and cheaper. These dynamics all coming together will allow customers to reach new magnitudes for scale, speed and cost. In terms of when we will see this happening, I believe the future is now and in 2015 we will see significant intensifying of the Perfect Storm for IoT. ?? ?? ?? ?? ??
,
,
,
,The usual answer, it depends! It depends on the Industry, it depends on the ,geography, it depends on the business priorities, etc. If an enterprise customer is in a highly regulated industry or geography then a key question will be ""does the cloud platform and/or cloud provider give the needed data protection, data locality and data privacy needed to maintain data governance and compliance. Also in terms of business priorities, enterprise customers need to determine whether a Mobile First or Cloud First focus is most important. Refer: ,.
,
,
,
,IBM Bluemix distinguishes itself from its key competitors in several dimensions:
,
,
, ?? ,
,
,
,There are several use cases where we have seen Bluemix used by developers to build cool applications. Mobile applications where backend services are developed and run on Bluemix; New applications for Internet-of-things; Applications that use Watson APIs on Bluemix to build user interactions based on cognitive capabilities; and many many enterprise use cases that are building new ways of reaching users while securely integrating with existing enterprise services. All of these use cases have resulted in many new applications and services. 
,
,
Here are a couple of external links to some innovative apps which were developed with BlueMix:
,
,
,
,
,
,
,
,
,  "
"
,
,Friday, January 23, 2015
,Phoenix, AZ, USA
,
, Postponed
,
Why you should attend:
,
  "
"
By Rachel Delacour, CEO of BIME Analytics, Dec 2014.
,
,
As cloud services become the ubiquitous, invisible fabric of our private and professional lives, business intelligence (BI) and analytics emerge as the crucial components to make sense of data large and small. According to Rachel Delacour, CEO and co-founder of BIME Analytics, easy-to-use data discovery tools are the equivalent of navigation and mapping services for the data-driven world.
,
From the rise of social BI and data-driven public health to analytics as the backbone of a drone ecosystem ready to take flight, here are Delacour?€?s five data predictions for 2015.
,
,
,
Much like gamification was about a UI that effortlessly engages a user, future BI and analytics will be designed around real human beings. While the old model pushes or insights to users, the future belongs to live, bi-directional exploration of data -- similar to the give and take consumers love in social networks or personal apps. Pick data sets with a virtual circle of curious minds and visualize them collaboratively, bookmark a specific view and exchange live comments just like you would in a gamer or CRM landscape.
,
Social analytics also means service providers have to rethink their approach. Going forward, they have to help users right in the app or dashboard, offering live support 24/7. It gives them a 360-degree view of how people use analytics, where they excel and where they get stuck. This dynamic exchange will significantly shorten the time to push out enhancements and updates for all.
,
,
,
Whether or not you understand the R programming language, Apache Mahout or concepts such as Holt-Winters multiplicative exponential smoothing: data mining will be sprinkled into a lot of modern applications in clever ways, hiding all the complexities but keeping the magic. It?€?s something we have become accustomed to when Amazon recommends books or the Akinator app can guess the character we have in mind. Access to those advanced algorithms will become an integral and common part of the analytic tool set available to the average user.
,
What?€?s more, applications will find clever ways to make use of such advanced data mining algorithms in everyday situations. For instance, the recent update to Gmail now parses emails and automatically adds events mentioned in messages to the calendar, thanks to natural language processing. So all you need to bring to ring in the new year is curiosity and a modern browser on a tablet.
,
,
,
One of the many lessons of the Ebola outbreak is that the world need a far-flung and thorough early-warning system. Public health officials can't afford to be in the dark about new cases, existing patients and their extended networks, or promising candidate drugs for a cure.
,
Experts rightly lament that healthcare is ?€?data rich?€? but ?€?analysis poor,?€? and the consumer space demonstrate how to get out of this dilemma. Inexpensive trackers and apps for individuals lead the way for how to deal with health at large, from medical-grade apps to public health networks. We?€?ll witness the emergence of services and tools that accompany patients and doctors along their journey together, creating a data-driven connection much earlier than when traditional preventive systems usually kick in. Electronic health records are already being collected, but we?€?ll see progress in three specific areas: more compatibility between these different data stores, better integration with new analytics platforms specializing in health outcomes, and finally better integration between consumer-grade fitness apps and serious medical applications.
,
,
,
Leaving aside the headline-grabbing stories about drones making deliveries and being shot down by upset neighbors, the rise of unmanned or autonomous devices on land or in the air depends on one key thing: a reliable, bi-directional data feed to parse. Drones in the civilian world will do a lot of good in 2015, but they need to gather loads of data and feed them into analytics platforms that help humans -- whether it?€?s about environmental sensing, surveying natural disasters and relief efforts, or making sense of traffic patterns and other urban phenomena. Connecting to a smart node to offload data for analysis and receive instructions on what to do next will be a crucial factor for the success or failure of these systems, for experts as well as the general public.
,
As robots and UAVs become almost as affordable as a good vacuum, more and more services will pop up to help manage the novel chores of the data-driven world. Big Data, in short, will help define and build out the world of tomorrow?€?s robots.
,
,
,
Don?€?t expect the ?€?race to zero?€? in the cloud economy to let up in 2015. Quite the contrary. Big vendors such as Amazon, Google, and Microsoft will keep tweaking their services while dropping prices, and that?€?s a good thing for companies moving more and more of their data and services into the cloud. In 2015, the cloud will be everything, and everything will be in the cloud. As the plumbing of our connected world becomes cheaper and almost invisible, businesses can use this abundance of resources to quickly add high-value services to run on top. Owning, running and maintaining a proprietary data center will soon be a thing of the past, except perhaps for the very largest companies or those with the most sensitive data.
,
The rest of the enterprise community will count their beans in 2015 and break into a big smile. While the big guys figure out how to make a profit off a booming business with low margins, organizations can start thinking about how to benefit from vast resources at rapidly shrinking prices. They will be more inclined to experiment with tools to crunch terabytes of data. Big data, in short, will come with an ever smaller price tag.
,
Original: ,
,
,
,
,  "
"
Most popular 
, tweets for Dec 24-25 were
,
24 #DataScience, #Statistics, #MachineLearning Resources to Keep Your Finger on the Pulse 
, 
,
,
,
24 #DataScience, #Statistics, #MachineLearning Resources to Keep Your Finger on the Pulse 
, 
,
,
#Pig and #Python can't fly but can predict Airline delays: #DataScience with Apache #Hadoop 
, 
,
,
,
Pregnant women can intuit the sex of their children: in the study, 70% of women guessed right 
, 
,
,
,
,  "
"
Most popular 
, tweets for November
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
Latest ,, (Dec 29, 2014) ,:
,
,??,
Also
, |
, |
, |
, |
, |
, |
, |
, |
, |
,
,
,
All models are wrong, but some are useful. George E. P. Box, 1987  "
"
Most popular 
, tweets for Dec 22-28 were
,
,  "
"
,
,
, is currently serving as Vice President of SDN Cloud Services  and CTO, IBM Cloud Services Division and as a faculty member for the Cloud and Internet-of-Things Expos.
,
Mac has 25 years of experience with networking and virtualization. He became an IBM Master Inventor in 2006, an IBM Distinguished Engineer in 2008 and is a Member of the IBM Academy of Technology since 2009. 
,
He served 2 years as the Chief Architect for IBM System z Enterprise Software and 2 years as the CTO for IBM Cloud partnerships and client innovations within the IBM Corporate Strategy team.
,
Mac served 2 years as Director and CTO Cloud Portfolio within IBM Global Technology Services. He leads the technical due diligence for the SoftLayer acquisition which lead to the creation of IBM Cloud Services Division. He co-authored Springer?€?s Handbook of Cloud Computing used by colleges and universities
,
,.
,
Here is second and last part of my interview with him:
,
,
,
, Cloud Speed ,not only refers to the rate of technology advancement but also the way technology is delivered. We live in a continuous services delivery world.  Consumers expectations are also moving at ""Cloud Speed"" so business managers and developers need to develop skills which will allow them to optimize the quality of experience to the customer and continuously delivery value to them.   
, 
,
,
,Most of the unexpected events I have seen over the last 25 years have been related to the rate a pace of change. I remember being surprised as how quickly TCP/IP and Open Source matured within Enterprise IT. I was also surprised that Linux and Mobile did not happen more quickly (after a few false starts, they did finally both take off quickly within the enterprise).      
,
,
, 
,That is a difficult question as there are many interesting trends but I will call out a couple of my favorites. I am really starting to see a 5th ""V"" emerging for BigData. In addition to ""Volume"", ""Variety"", ""Velocity"" and ""Veracity"", enterprises are starting to focus on data ""Vibrancy"".  The definition of Vibrancy is ""pulsating with life and vigor"" and by providing Data-as-a-Service, enterprises can allow an vigorous ecosystem to extend the value of the data to bring new services to life. Another emerging IoT trend which is very interesting is how the IoT ecosystem is starting to leverage ""Block Chaining"" to provide secure and collaborative commerce.
,
,
,
,It is hard enough to predict what the Cloud Computing landscape will truly look like in 1 year ,much less 3-5 years. That said, I do believe that we will continue to see innovations which increase the scale and speed of business. I also believe that Data and Analytics will continue to be hot areas of innovation. In particular, I believe IoT ""Data Stories"" will become more common meaning that it will no longer take a Data Scientist with multiple PhDs to get value from the data generated by IoT use cases.
,
,
,
,It was from an IBM Vice President that I used to work for. He said that if everyone is happy/agreeing with what you are doing then you clearly are not ""pushing the edge of the innovation envelope"".    
,
,
,
,I find it difficult to find time to read too many books but I am enjoying reading through ,. I enjoy spending time with my wife and 2 sons away from work. I especially enjoyed this Thanksgiving as it was the first time all 4 of us have been together since my oldest son joined the US Army and my youngest Son went to The Kings College in New York City.  
,    

,
,  "
"
,
,
, launched , to transform the way businesses use big data analytics. Under Ben?€?s leadership, Platfora is now one of the hottest companies in Silicon Valley and a leader in the big data analytics space. Before founding Platfora, Ben was vice president of products for DataStax, where he shaped the company?€?s enterprise and Hadoop strategy, and was also head of product at big data analytics company Greenplum. 
,
Ben has a B.S. in Computer Science from Monash University and an M.S. in Computer Science from Stanford.
,
Here is my interview with him:
,
,
,
,: To get maximum value out of big data, it must be accessible and consumable by employees far beyond the walls of the IT department. And just as essential to data democratization is reducing analytic cycles from weeks to hours, and this is only possible with a self-service model. , 
,
,
,
,We recently conducted , of 120 executives who use or manage analytics software.  About half of respondents indicated that they have to break data into smaller pieces to work with it, and this is driven largely by in-memory limitations of older generations of analytics and because data is not yet centralized in massively parallel stores such as Hadoop. In addition, 53% said that linking existing ?€?small data?€? infrastructure to the new big data tools was challenging.
, ?? ,
, 
,
,
,
,Native and end-to-end means Platfora can analyze data at petabyte scale in seconds, and enable users to drill into data sets iteratively and interactively without having to fill out an IT work order every time you change the query.  This enables hundreds or thousands of employees to start working with data in security, marketing, finance, operations and the Internet of Things just to name a few popular use cases.
,

,
,
,Among many additions to the platform, we greatly improved the visualization capabilities in 4.0 so that more users in the organization could have a Tableau-like visualization experience while tapping into vast seas of Hadoop data.   Data that was once inaccessible without significant data preparation and technical support is now on tap to analyze by CXOs, non-technical employees and the data science team members just the same.
, ?? ,
,
,
,Platfora Big Data Analytics is still the only end-to-end big data analytics platform built natively on Hadoop.  We are sometimes confused with Datameer, but in several companies that use both of our solutions (Vivint, Opower) Datameer is for data preparation and ETL, and the full-stack analytics capabilities come from  Platfora. 
,
,
,
,
,  "
"
Most popular 
, tweets for October 2014 were
,
Air traffic data is being analyzed to predict Ebola's Spread , #EbolaOutbreak @hey_anmol 
,
,
,
A Great Collection of #MachineLearning Algorithms , #DataScience #RStats @hey_anmol 
,
,
,
Data mining classics: Classifying Shakespearean Drama with Sparse Feature Sets , 
,
,
,
Best Programming Language for Machine Learning: R #rstats, Python, MATLAB/Octave, Julia, Java/C - when to use what ,
,
,  "
"
,
,
, launched , to transform the way businesses use big data analytics. Under Ben?€?s leadership, Platfora is now one of the hottest companies in Silicon Valley and a leader in the big data analytics space. Before founding Platfora, Ben was vice president of products for DataStax, where he shaped the company?€?s enterprise and Hadoop strategy, and was also head of product at big data analytics company Greenplum. 
,
Ben has a B.S. in Computer Science from Monash University and an M.S. in Computer Science from Stanford.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: The biggest challenge we?€?ve noticed over the past two years is the decision to centralize data in Hadoop.  This isn?€?t easy at all for an organization, as it requires a fundamental shift in architecture and IT operations.  That said, it is surprisingly easy to convince Hadoop adopters to put a full-stack native analytics platform on top of their new platform.  They realize pretty quickly that building a data lake is not the end goal, it?€?s supercharged analytics that they are really after.
, ?? ,
,
, ?? ,
,
,
,Our customers are doing amazing things in the areas ,of security, IoT and marketing just to name a few.  Every month we are blown away by a new use case that?€?s made possible through our big data analytics platform.  As one example, , now collects and analyzes IoT data in Hadoop from almost a million homes.  Their team originally lacked any reasonable way to analyze IoT information, but now it is easily done in Platfora.  
,
,
,
,
,Today the vast majority of companies are data rich but insight poor. They are acquiring more and more data about customers, product experiences, behavior and interactions, and beyond. But simply visualizing and making sense of those patterns and behaviors (given the volume and variety involved) is itself ?€?rocket science?€? and requires state-of-the-art capabilities. 
,The bleeding edge of the next wave of capabilities -- incorporating newer AI and deep learning techniques -- will also be starting to show its potential. 
,
,
,
,I love the Peter Drucker quote that ?€?culture eats strategy for breakfast?€?. As the founder and CEO of a fast-growing company, the best way I have leverage today is to make ,sure we?€?re hiring incredible people -- holding the bar high on both ability and cultural fit -- and ensuring that everyone in the company knows what it means to work at Platfora and that we?€?re all ultimately focused on a common set of goals. I?€?m incredibly passionate about the market opportunity, product and technology, and how we make customers successful, but ensuring we have the right people and culture trumps any amount of smart executive strategizing. 
,
,
,
,There are too many data scientists that have a toolbox of statistical and machine learning techniques but ,don?€?t understand enough about their business to really know where or how to apply them. The great data scientists and analysts I know are curious and dogged about finding datasets that may be valuable, pulling them together to find meaning across them, and are proactive about engaging with their line-of-business colleagues to both share understanding of what is possible and learn/validate the problems that will have most leverage and impact for the business.
,
,

,
,For fiction, I?€?m a big fan of Charles Stross. His ?€?,?€? books (,) are secret-agent bureaucratic dark comedies inspired by Lovecraftian horror with a modern mathematics/science twist. 
,
For non-fiction, I just read Nick Bostrom?€?s ?€?,?€? which is a deep academic perspective on the state of AI and risks/dangers of an explosion in machine intelligence. A really intriguing perspective given the recent discussion on the topic by Elon Musk and others.
,

,
,  "




"
Most popular 
, tweets in September 2014 were
,
Any data scientist worth their salary will say you should start with a question, NOT the data, says , 
,
,
Watch: Statistical & Machine learning with R, great 15 hour online course, by Stanford profs Hastie & Tibshirani ,
,
Randomness: One pattern is random, the other is machine-generated. Can you guess which ?  , 
,
,
,
Randomness: One pattern is random, the other is machine-generated. Can you guess which ?  , 
,
,
,  "


"
,
,
,??,??
,
,
,??,??
,
,  "



















"
By Gregory Piatetsky,  
,, Jan 2015.
,
Several KDnuggets December stories were so popular they made it into all-year top list, so we had to update our previous 
, published Dec 16.
,
Here is a new list.
,
Top stories in 2014 had several themes - Deep Learning, Data Scientist career, education and salary, IBM Watson, resources for learning Data Science tools (especially R and Python), and KDnuggets annual data mining software poll.
,
The most viewed story had over 18,000 views and most shared had over 600 shares! 
,
,
,
,
,
,  "
"
,
By Gregory Piatetsky,  
,.
,
New KDnuggets cartoon takes a look at what might happen when 
Artificial Intelligence achieves Singularity, and how relevant human skills will be.
,
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,
,
,
,??,
,
,
 ,  "
"
By Manu Jeevan, Jan 2015.
,
In this post I will be discussing the 3 fundamental methods in data science. These methods are basis for extracting useful knowledge from data, and also serve as a foundation for many well known algorithms in data science. I won?€?t be getting into the mathematical details of these methods; rather I am going to focus on how these methods are used to solve data centric business problems.
,
So let?€?s get started,
,
,
,
,
Classification and class probability estimation attempts to predict, for each individual in a population, to which class does this individual belongs to. Generally the classes are independent of each other.  An example for a classification problem would be:
,
""Among all the customers of Dish, which are likely to respond to a new offer?""
,
In this example the two classes can be called  ""will respond"" and ""will not respond"". Your goal for classification task is given a new individual; determine which class that individual belongs to. A closely related concept is scoring or class probability estimation.
,
A Scoring model when applied to an individual produces a score representing the probability that the individual belongs to each class. In our customer response example, a scoring model can evaluate each individual customer and produce a score of how likely each customer is to respond to the offer.
,
,
,
Regression is the most commonly used method in forecasting. Regression tries to predict a real valued output (numerical value) of some variable for that individual.  An example regression problem would be: ?€?What will be the cost of a given house??€? The variable to be predicted here is housing price, and a model could be produced by looking at other, similar houses in the population and their historical prices.  A regression procedure produces a model that, given a house, estimates the price of the house.
,
Regression is related to classification, but the two are different.  In simple terms, classification forecasts whether something will happen, while regression forecasts how much something will happen.
,
By heart this concept: ?€?Scoring is a classification problem not a regression problem because the underlying target (value you are attempting to predict) is categorical?€?
,
,
,
,
Similarity matching tries to recognize similar individuals based on the information known about them.  If two entities (products, services, companies) are similar in some way they share other characteristics as well.
,
For example, Accenture will be interested in finding customers who are similar to their existing profitable customers, so that they can launch a well targeted marketing campaign. Accenture use similarity matching based on the characteristics that define their existing profitable customers (such as company turnover, industry, location.. etc) .
,
Similarity is the underlying principle for making product recommendations (identifying people who are alike in terms of the products they have purchased or have liked).  Online retailers such as Amazon and Flipkart use similarity to provide recommendations of similar products to you.  Whenever you see expressions like ?€?People who like A also like B?€? or ?€?people with your browsing history have also looked at ?€?..?€? The concept of similarity is being applied.
,
,
,
I talked about classification, regression and similarity matching in this post. I strongly believe the application of these fundamental methods to business problems is far more important than their algorithmic details.  Important things to keep in mind are:
,
,
Manu Jeevan is a self-taught data scientist and blogger at ,, where he writes about Data Science, Statistics, Python and Machine Learning, to help others learn data science.
,
,
,
,  "
"
,
,
Deep learning. Neural networks. Backpropagation. Over the past year or two, I've heard these buzz words being tossed around a lot, and it's something that has definitely seized my curiosity recently. Deep learning is an area of active research these days, and if you've kept up with the field of computer science, I'm sure you've come across at least some of these terms at least once.
,
Deep learning can be an intimidating concept, but it's becoming increasingly important these days. Google's already making huge strides in the space with the??,??and its recent acquisition of the London-based deep learning startup??,. Moreover, deep learning methods are beating out traditional machine learning approaches on virtually every single metric.
,
So what exactly is deep learning? How does it work? And most importantly, why should you even care?
,
,
,
If you're new to computer science, and you've followed me up till this point, please stick with me. Certain optional sections of this article may get a little math heavy (marked with a *), but I want to make this subject accessible to everyone, computer science major or not. In fact, if you are reading through this article and, at any point, you find yourself confused about the material, please email me. I will make whatever edits and clarifications that are necessary to make the article clearer.
,
,
,
Before we dive into deep learning, I want to take a step back and talk a little bit about the broader field of ""machine learning"" and what it means when we say that we're programming machines to??,.
,
Sometimes we encounter problems for which it's really hard to write a computer program to solve. For example, let's say we wanted to program a computer to recognize hand-written digits:
,
,
,
,
You could imagine trying to devise a set of rules to distinguish each individual digit. Zeros, for instance, are basically one closed loop. But what if the person didn't perfectly close the loop. Or what if the right top of the loop closes below where the left top of the loop starts?
,
,
,
,
In this case, we have difficulty differentiating zeros from sixes. We could establish some sort of cutoff, but how would you decide the cutoff in the first place? As you can see, it quickly becomes quite complicated to compile a list of heuristics (i.e., rules and guesses) that accurately classifies handwritten digits.
,
And there are so many more classes of problems that fall into this category. Recognizing objects, understanding concepts, comprehending speech. We don't know what program to write because we still don't know how it's done by our own brains. And even if we did have a good idea about how to do it, the program might be horrendously complicated.
,
So instead of trying to write a program, we try to develop an algorithm that a computer can use to look at hundreds or thousands of examples (and the correct answers), and then the computer uses that experience to solve the same problem in new situations. Essentially, our goal is to teach the computer to solve by example, very similar to how we might teach a young child to distinguish a cat from a dog.
,
Over the past few decades, computer scientists have developed a number of algorithms that try to allow computers to learn to solve problems through examples. Deep learning, which was first theorized in the early 80's (and perhaps even earlier), is one paradigm for performing machine learning. And because of a flurry of modern research, deep learning is again on the rise because it's been shown to be quite good at teaching computers to do what our brains can do naturally.
,
One of the big challenges with traditional machine learning models is a process called??,. Specifically, the programmer needs to tell the computer what kinds of things it should be looking for that will be informative in making a decision. Feeding the algorithm raw data rarely ever works, so feature extraction is a critical part of the traditional machine learning workflow. This places a huge burden on the programmer, and the algorithm's effectiveness relies heavily on how insightful the programmer is. For complex problems such as object recognition or handwriting recognition, this is a huge challenge.
,
Deep learning is one of the only methods by which we can circumvent the challenges of feature extraction. This is because deep learning models are capable of learning to focus on the right features by themselves, requiring little guidance from the programmer. This makes deep learning an extremely powerful tool for modern machine learning.
,
,
,
Deep learning is a form of machine learning that uses a model of computing that's very much inspired by the structure of the brain. Hence we call this model a??,. The basic foundational unit of a neural network is the??,, which is actually conceptually quite simple.
,
,
,
,
Each neuron has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs. A linear neuron takes a linear combination of the weighted inputs. A sigmoidal neuron does something a little more complicated:
,
,
,
,
It feeds the weighted sum of the inputs into the??,. The logistic function returns a value between 0 and 1. When the weighted sum is very negative, the return value is very close to 0. When the weighted sum is very large and positive, the return value is very close to 1. For the more mathematically inclined, the logistic function is a good choice because it has a nice looking derivative, which makes learning a simpler process. But technical details aside, whatever function the neuron uses, the value it computes is transmitted to other neurons as its output. In practice, sigmoidal neurons are used much more often than linear neurons because they enable much more versatile learning algorithms compared to linear neurons.
,
A neural network comes about when we start hooking up neurons to each other, to the input data, and to the ""outlets,"" which correspond to the network's answer to the learning problem. To make this structure easier to visualize, I've included a simple example of a neural net below. We let??w(k)i,j??be the weight of the link connecting the??, neuron in the??,??layer with the??,??neuron in the??,st??layer:
,
,
,
,
Similar to how neurons are generally organized in layers in the human brain, neurons in neural nets are often organized in layers as well, where neurons on the bottom layer receive signals from the inputs, where neurons in the top layers have their outlets connected to the ""answer,"" and where are usually no connections between neurons in the same layer (although this is an optional restriction, more complex connectivities require more involved mathematical analysis). We also note that in this example, there are no connections that lead from a neuron in a higher layer to a neuron in a lower layer (i.e., no directed cycles). These neural networks are called??,??neural networks as opposed to their counterparts, which are called??,neural networks (again these are much more complicated to analyze and train). For the sake of simplicity, we focus only on feed-forward networks throughout this discussion. Here's a set of some more important notes to keep in mind:
,
1) Although every layer has the same number of neurons in this example, this is not necessary.
,
2) It is not required that a neuron has its outlet connected to the inputs of every neuron in the next layer. In fact, selecting which neurons to connect to which other neurons in the next layer is an art that comes from experience. Allowing maximal connectivity will more often than not result in??,, a concept which we will discuss in more depth later.
,
3) The inputs and outputs are??,??representations. For example, you might imagine a neural network where the inputs are the individual pixel RGB values in an image represented as a vector. The last layer might have 2 neurons which correspond to the answer to our problem:??[0,1]??if the image contains a dog,??[1,0]??if the image contains a cat,??[0,0]??if it contains neither, and??[1,1]??if it contains both.
,
4) The layers of neurons that lie sandwiched between the first layer of neurons (input layer) and the last layer of neurons (output layer), are called??,. This is because this is where most of the magic is happening when the neural net tries to solve problems. Taking a look at the activities of hidden layers can tell you a lot about the features the network has learned to extract from the data.
,
,
,
Well okay, things are starting to get interesting, but we're still missing a big chunk of the picture. We know how a neural net can compute answers from inputs, but we've been assuming that we know what weights to use to begin with. Finding out what those weights should be is the hard part of the problem, and that's done through a process called??,. During training, we show the neural net a large number of training examples and iteratively modify the weights to minimize the errors we make on the training examples.
,
Let's start off with a toy example involving a single linear neuron to motivate the process. Every day you grab lunch in the dining hall where your meal consists completely of burgers, fries, and soda. You buy some number of servings of each item. You want to be able to predict how much your meal will cost you, but you don't know the prices of each individual item. The only thing the cashier will tell you is the total price of the meal.
,
How do we solve this problem? Well, we could begin by being smart about picking our training cases, right? For one meal we could buy only a single serving of burgers, for another we could only buy a single serving of fries, and then for our last meal we could buy a single serving of soda. In general, choosing smart training cases is a very good idea. There's lots of research that shows that by engineering a clever training set, you can make your neural net a lot more effective. The issue with this approach is that in real situations, this rarely ever gets you even 10% of the way to the solution. For example, what's the analog of this strategy in image recognition?
,
Let's try to motivate a solution that works in general. Take a look at the single neuron we want to train:
,
,
,
,
Let's say we have a bunch of training examples. Then we can calculate what the neural network will output on the??,??training example using the simple formula in the diagram. We want to train the neuron so that we pick the optimal weights possible - the weights that minimize the errors we make on the training examples. In this case, let's say we want to minimize the square error over all of the training examples that we encounter. More formally, if we know that??t(i)??is the true answer for the??,??training example and??y(i)??is the value computed by the neural network, we want to minimize the value of the error function??E:
,
,
,
Now at this point you might be thinking, wait up... Why do we need to bother ourselves with this error function nonsense when we have a bunch of variables (weights) and we have a set of equations (one for each training example)? Couldn't we just solve this problem by setting up a system of linear system of equations? That would automatically give us an error of zero assuming that we have a consistent set of training examples, right?
,
That's a smart observation, but the insight unfortunately doesn't generalize well. Remember that although we're using a linear neuron here, linear neurons aren't used very much in practice because they're constrained in what they can learn. And the moment you start using nonlinear neurons like the sigmoidal neurons we talked about, we can no longer set up a system of linear equations!
,
So maybe we can use an iterative approach instead that generalizes to nonlinear examples. Let's try to visualize how we might minimize the squared error over all of the training examples by simplifying the problem. Let's say we're dealing with a linear neuron with only two inputs (and thus only two weights,??w1??and??w2). Then we can imagine a 3-dimensional space where the horizontal dimensions correspond to the weights??w1??and??w2, and there is one vertical dimension that corresponds to the value of the error function??E. So in this space, points in the horizontal plane correspond to different settings of the weights, and the height at those points corresponds to the error that we're incurring, summed over all training cases. If we consider the errors we make over all possible weights, we get a surface in this 3-dimensional space, in particular a quadratic bowl:
,
,
,
,
We can also conveniently visualize this surface as a set of elliptical contours, where the minimum error is at the center of the ellipses:
,
,
,
,
So now let's say we find ourselves somewhere on the horizontal plane (by picking a random initialization for the weights). How would we get ourselves to the point on the horizontal plane with the smallest error value? One strategy is to always move perpendicularly to the contour lines. Take a look, for instance, at the path denoted by the red arrows. Quite clearly, you can see that following this strategy will eventually get us to the point of minimum error.
,
What's particularly interesting is that moving perpendicularly to the contour lines is equivalent to taking the path of steepest descent down the parabolic bowl. This is a pretty amazing result from calculus, and it gives us the name of this general strategy for training neural nets:??,.
,
,
,
In practice at each step of moving perpendicular to the contour, we need to determine how far we want to walk before recalculating our new direction. This distance needs to depend on the steepness of the surface. Why? The closer we are to the minimum, the shorter we want to step forward. We know we are close to the minimum, because the surface is a lot flatter, so we can use the steepness as an indicator of how close we are to the minimum. We multiply this measure of steepness with a pre-determined constant factor????, the??,. Picking the learning rate is a hard problem. If we pick a learning rate that's too small, we risk taking too long during the training process. If we pick a learning rate that's too big, we'll mostly likely start diverging away from the minimum (this pretty easy to visualize). Modern training algorithms adapt the learning rate to overcome this difficult challenge.
,
For those who are interested, putting all the pieces results in what is called the??,??for training the linear neuron. The delta rule states that given a learning rate????, we ought to change the weight??wk??at each iteration of training by????wk=???i??xk(t(i)???y(i)). Deriving this formula is left as an exercise for the experienced reader. For a hint, study our derivation for a sigmoidal neuron in the next section.
,
Unfortunately, just taking the path of steepest descent doesn't always do the trick when we have nonlinear neurons. The error surface can get complicated and there could be multiple local minimum. As a result, using this procedure could potentially get us to a bad local minimum that isn't the global minimum. As a result, in practice, training neural nets involves a modification of gradient descent called??,, that tries to use randomization and noise to find the global minimum with high probability on a complex error surface.
,
,
,
This section and the next will get a little heavy with the math, so just be forewarned. If you're not comfortable with multivariate calculus, feel free to skip them and move onto the remaining sections. Otherwise, let's just dive right into it!
,
Let's recall the mechanism by which logistic neurons compute their output value from their inputs:
,
,
,
The neuron computes the weighted sum of its inputs, the??,,??z. It then feeds??z??into the input function to compute??y, its final output. These functions have very nice derivatives, which makes learning easy! For learning, we want to compute the gradient of the error function with respect to the weights. To do so, we start by taking the derivative of the logit,??z, with respect to the inputs and the weights. By linearity of the logit:
,
,
,
Also, quite surprisingly, the derivative of the output with respect to the logit is quite simple if you express it in terms of the output. Verifying this is left as an exercise for the reader:
,
,
,
We then use the chain rule to get the derivative of the output with respect to each weight:
,
,
,
Putting all of this together, we can now compute the derivative of the error function with respect to each weight:
,
,
,
Thus, the final rule for modifying the weights becomes:
,
,
,
As you may notice, the new modification rule is just like the delta rule, except with extra multiplicative terms included to account for the logistic component of the sigmoidal neuron.
,
,
,
Now we're finally ready to tackle the problem of training multilayer neural networks (instead of just single neurons). So what's the idea behind backpropagation? We don't know what the hidden units ought to be doing, but what we can do is compute how fast the error changes as we change a hidden activity. Essentially we'll be trying to find the path of steepest descent!
,
Each hidden unit can affect many output units. Thus, we'll have to combine many separate effects on the error in an informative way. Our strategy will be one of dynamic programming. Once we have the error derivatives for one layer of hidden units, we'll use them to compute the error derivatives for the activities of the layer below. And once we find the error derivatives for the activities of the hidden units, it's quite easy to get the error derivatives for the weights leading into a hidden unit. We'll redefine some notation for ease of discussion and refer to the following diagram:
,
,
,
,
The subscript we use will refer to the layer of the neuron. The symbol??y??will refer to the activity of a neuron, as usual. Similarly the symbol??z will refer to the logit of a neuron. We start by taking a look at the base case of the dynamic programming problem, the error function derivatives at the output layer:
,
,
,
Now we tackle the inductive step. Let's presume we have the error derivatives for layer??j. We now aim to calculate the error derivatives for the layer below it, layer??i. To do so, we must accumulate information for how the output of a neuron in layer??i??affects the logits of every neuron in layer??j. This can be done as follows, using the fact that the partial derivative of the logit with respect to the incoming output data from the layer beneath is merely the weight of the connection??w,:
,
,
,
Now we can use the following to complete the inductive step:
,
,
,
Combining these two together, we can finally express the partial derivatives of layer??i??in terms of the partial derivatives of layer??j.
,
,
,
Then once we've gone through the whole dynamic programming routine, having filled up the table appropriately with all of our partial derivatives (of the error function with respect to the hidden unit activities), we can then determine how the error changes with respect to the weights. This gives us how to modify the weights after each training example:
,
,
,
In order to do backpropagation with batching of training examples, we merely sum up the partial derivatives over all the training examples in the batch. This gives us the following modification formula:
,
,
,
We have succeeded in deriving the backpropagation algorithm for a feed-forward neural net utilizing sigmoidal neurons!
,
,
,
Now let's say you decide you're very excited about deep learning and so you want to try to train a neural network of your own to identify objects in an image. You know this is a complicated problem so you use a huge neural network (let's say 20 layers) and you have 1,000 training examples. You train your neural network using the algorithm we describe, but something's clearly wrong. Your neural net performs virtually perfectly on your training examples, but when you put it in practice, it performs very poorly! What's going on here?
,
The problem we've encountered is called??,, and it happens when you have way too many parameters in your model and not enough training data. To visualize this, let's consider the figure below, where you want to fit a model to the data points:
,
,
,
,
Which curve would you trust? The line which gets almost no training example exactly? Or the complicated curve that hits every single point in the training set? Most likely you would trust the linear fit instead of the complicated curve because it seems less contrived. This situation is analogous to the neural network we trained. We have way too many parameters, over 100 trillion trillion (or 100 septillion) parameters. It's no wonder that we're overfitting!
,
So how do we prevent overfitting? The two simplest ways are:
,
1) Limit the connectivities of neurons in your model. Architecting a good neural network requires a lot of experience and intuition, and it boils down to giving your model freedom to discover relationships while also constraining it so it doesn't overfit.
,
2) Adding more training examples! Often times you can cleverly add amplify your existing training set (changing illumination, applying shifts and other transformations, etc.).
,
There are more sophisticated methods of training that try to directly solve overfitting such as including a dropout layers/neurons, but these methods are beyond the scope of this article
,
,
,
We've covered a lot of ground, but there's still a lot more that's going on in deep learning research. In future articles, I will probably talk more about different kinds of neural architectures (convolutional networks, soft max layers, etc.). I'll probably also write an article about how to train your own neural network using some of the awesome open source libraries out there, such as Caffe (which allows you to GPU accelerate the training of neural networks). Those of you who are interested in pursuing deep learning further, please get in touch! I love talking about new ideas and projects.
,
Author: , blog publishes a collection of adventures, thoughts, and random tidbits from an MIT computer science student.
,
Original: ,
,
,
,
"
"
,
,
, is a USTAR assistant professor in the School of Computing at the , and a faculty member in the ,. Her research focuses on the design of visualization systems for helping researchers make sense of complex data. She obtained her bachelors degree in astronomy and astrophysics at Penn State University, and earned a PhD in computer science from the University of Utah. Prior to joining the faculty at Utah Miriah was a postdoctoral research fellow at Harvard University and a visiting scientist at the Broad Institute of MIT and Harvard.
,
Miriah is the recipient of a NSF CAREER grant, a Microsoft Research Faculty Fellowship, and a NSF/CRA Computing Innovation Fellow award. She was named both a TED Fellow and a PopTech Science Fellow, as well as included on MIT Technology Review's TR35 list of the top young innovators and Fast Company's list of the 100 most creative people. She was also awarded an AAAS Mass Media Fellowship that landed her a stint as a science writer for the Chicago Tribune.
,
Here is my interview with her:
,
,
,
,: The idea came about when a co-author and I started sharing notes about conversations we were having with graphic designers. We both were surprised to learn that designers largely were creating infographics in Illustrator and drawing visualizations by hand. As computer scientists, we thought this was a terrible thing when we have so many tools for automating this sort of thing! We wanted to learn more about why designers would go through this manual process, and how it effects their designs. So, we developed some studies and conducted a series of interviews.
,?? , ?? ,
What we found was really interesting. First, manually creating visualizations isn't always a bad thing for designers -- often this process served as a way to get into the data and explore. Second, our observations showed some really interesting short-comings in current tools and provided some insights how to create new software systems that could support designers more effectively.

,


,
,
,Definitely. So many people now are aware that visual representations are great for summarizing complex data and helping us to see unexpected things. Unfortunately, though, there is still a big misunderstanding about what a visual ,representation can give you (and what it can't). Many people I talk to that are struggling to make sense of their data think that if they could *just* visualize their data in some new way then insight will magically happen. ,
,


,
,
,A whole subfield of visualization has popped up that focuses on telling stories with data. It is quite interesting -- how do you tell a linear story while giving the user tools to explore data in a nonlinear way? What are the important elements of an infographic that brings a story to life, and how can we automate that? Can we take manually created visualizations (such as this one: ,) and automatically produce them with different data sets? These are some of the interesting questions that people are now exploring.
, ?? ,

,
,
,There is a trend to look at how we can apply existing visualization techniques to real-world problems, basically taking visualization research into the wild. I expect this trend to continue, along with seeing more of a marriage between visualization techniques and statistical methods.

,
,
,
,One interesting challenge is how to quickly prototype with data. We know from design that creating rapid prototypes, or ""failing fast"", is really important for discovering novel and effective ideas. In visualization this is really challenging to do if you need to include real data in your prototypes.
,


,
,
,I'm probably not a good person to ask about this because I prefer to code rather than use a tool! For creating visualizations programmatically I'm a huge fan of Processing. Processing is a fantastic environment that abstracts away many of the annoyances of graphics programming. It also has been used extensively in the design community so there are loads of interesting projects out there.

,

,
,
,My first love has always been science, and I saw visualization as way to enable science while getting to build cool things. I have grown to love what I do because it lets me learn about many different fields from people at the fore-front of those fields. For someone with a short attention span, visualization is great!
,
,
,
,
,Learn to follow your gut. This is surprisingly hard!

,
,
,
,Become a data scientist that understands that there is a living, breathing human-being on the other side of a computer screen. People that can reason about data while also being empathetic to the needs of data consumers are a rare breed.
,
,

,
,
,I don't remember the last time I read a book as I have a nine-month old at home. But I did recently purchase , from David Mitchell because I loved reading Cloud Atlas. I hope to get to read it sometime soon. In my spare time, I'm working on keeping my crawling baby out of trouble.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for December 2014 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets
,
,??,
Here are previous company activities posts for 2014:
,  "
"
, on Coursera
,
,
,
,
,Jan 31 - Mar 24, 2015
,
Instructors
,
,??,
,
,
We introduce the student to modern distributed file systems and MapReduce, including what distinguishes good MapReduce algorithms from good algorithms in general.  The rest of the course is devoted to algorithms for extracting models and information from large datasets.  Students will learn how Google's PageRank algorithm models importance of Web pages and some of the many extensions that have been used for a variety of purposes.  
,
We'll cover locality-sensitive hashing, a bit of magic that allows you to find similar items in a set of items so large you cannot possibly compare each pair.  When data is stored as a very large, sparse matrix, dimensionality reduction is often a good way to model the data, but standard approaches do not scale well; we'll talk about efficient approaches.  Many other large-scale algorithms are covered as well, as outlined in the course syllabus.
,
,
,There is a free book ""Mining of Massive Datasets, by Leskovec, Rajaraman, and Ullman (who are the instructors for this course).  
You can download it at 
,  
,
Hardcopies can be purchased from Cambridge Univ. Press.
,
Enroll at ,  "
"
Most popular 
, tweets for Jan 05-11 were
,
New book: Data Driven: Creating a Data Culture, by top #DataScience experts @hmason, @dpatil , 
,
,
,
Great post: #DeepLearning in a Nutshell: what it is, how does it work, and why you should care , 
,
,
,
New book: Data Driven: Creating a Data Culture, by top #DataScience experts @hmason, @dpatil , 
,
,
Great post: #DeepLearning in a Nutshell: what it is, how does it work, and why you should care , 
,
,
,  "
"
,Upcoming INFORMS continuing education courses offer training
in Essential Practice Skills for Analytics Professionals, 
Data Exploration & Visualization,
Foundations of Modern Predictive Analytics,
and Monte Carlo and Discrete-Event Simulation.
,
,
  "
"
,
,
We are bringing together experienced analysts and providing the tools and support for them to run their own analytics business. We are calling it the Lityx Analytics Network or LAN for short and will view members of the LAN as extended members of the Lityx team. Lityx is an analytic software company with limited but highly experienced analytic consulting resources and so we are able to offer and support both software subscriptions as well as analytic services. 
,
As we grow we would like to push new analytics work to members of the LAN rather than hire more analytics staff. Our focus is software sales, but we recognize the value and need to support analytic services. Further we can provide the LAN with the same tools that Lityx employees have today including our cloud hosted LityxIQ predictive modeling and optimization platform, an Office365 license, a secure FTP server along with our thought leadership, sales collateral, access to sales staff and our consultants for assistance.
,
,
,
,
,
,
,??,
More Information
,
,  "
"
By Linda Burtch, Jan 2015.
,
,
When I wrote my , I wrote that thanks to Big Data, analytics has become inescapable, and looking now into 2015, I believe that statement more than ever.
,
It?€?s not just analytics in the public eye either ?€? data science has been thrust into the spotlight, and the hiring market for analytics professionals and data scientists has gone into overdrive. Despite , companies are still struggling to hire, and talented professionals are going on and off the market almost faster than you can snap your fingers. While it is still crucial to ,, there has never been a better time to be a Quant looking for a job.
,
So what does all of this excitement mean for next year? I have a few predictions:
,
,
,
, 
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
Last Halloween, Chris Wiggins, 
the New York Times Chief Data Scientist, invited me to come to his 
,.
,
Unfortunately, I could not attend at the time, but I saw his excellent 
, and
, of the talk, and we started a conversation.  As a regular reader of the NYTimes since my NYU grad school days, I loved the chance to learn more about the NYTimes digital journalism today, and the result is this interview.
,
New York Times (along with other publishers) is now going through a period of declining ad revenues but also a big, scary, and exciting transformation, and I hope that Data Science will help it to adapt and prosper in the new era.
,
,, 
,, is an associate professor of applied mathematics at Columbia University, 
the first Chief Data Scientist at The New York Times, 
and co-founder and co-organizer of hackNY  , .
,
At Columbia he is faculty in the Department of Applied Physics and
Applied Mathematics, a founding member of the Department of Systems
Biology, a founding member of the Data Science Institute 
, , affiliated faculty in the
Department of Statistics, and an instructor in the School of
Journalism. His research focuses on applications of machine learning
to real-world data, particularly biology.
,
Prior to joining the faculty at Columbia he was a Courant Instructor
at NYU and earned his PhD at Princeton University in theoretical
physics. In 2014 he was elected Fellow of the American Physical
Society and is a recipient of Columbia's Avanessians Diversity Award.
,
,
,
,
,
Absolutely! I think of data science teams as performing
principally either Data Product or Data Impact functions.  
We've done both, in that we're building a number of data products (so
far just for internal use at the NYT) as well as trying to
impact roadmaps of other groups.  Impact comes from learning
models which are not only predictive but also interpretable. 
,
For example, if we build a model which predicts individual
subscribers' likelihood of canceling, we're interested not only
in the machine learning part (does the model predict accurately
the at risk individuals), which is an engineering win, but also
in the data science win of revealing risky behaviors.
,
Identifying these suggests changes to product or marketing,
which then can be executed and tested.
,
To find out more about specific examples, I encourage you to
click here: ,
,
,
,
,
Key skills are being good listeners, good communicators,
sufficient creativity and familiarity with machine learning
methods to know how to reframe domain questions as machine
learning tasks, the kind of self-skepticism science demands, and
the desire to serve as a culture broker between the data and the
demands of our collaborators throughout the company.
,
Regarding tools: Everything we do is open source when it comes
to the machine learning, principally Python, sometimes R. The data
engineering uses a variety of tools depending on what data we're
querying, but typically map reduce, elastic map-reduce, or SQL as needed.
,
,
,
,
Absolutely! NYT has been building APIs since 2008, ancient history.  
See  , .
,
,
,
,??,
,
,
Sure. I often observe a ""cluster first, ask questions later""
approach when people face new data. This was particularly
pervasive in computational biology in the early days of
microarray data.  With unsupervised learning it's often
difficult to asses which of two clustering approaches is
""better"", meaning that I don't sleep well at night because I'm wondering if
I could have clustered differently and found something else.
,
,
Instead I prefer first to think through what I really want to
learn from the data, which usually means that there's some
particular covariate I want to predict from the others. 
So I try to hunt for supervised learning tasks rather than unsupervised learning
tasks. In supervised learning or ""predictive analytics"" I can
assess more clearly whether my model makes accurate predictions
on held out data, which gives me a way of saying whether my
model is ""wrong."" 
I also mean this as reference to the famous dismissal by Wolfgang Pauli: 
(cf. , )
,
,
,
,
Our group is on the engineering side rather than in the newsroom,
separate from the (awesome) graphics group. Certainly we benefit from
their expertise, especially since we use a lot of 
,, which was
developed by the NYT ""digital superstar"" (as the Innovation
Report called him) Mike Bostock.
,
,
,
,
Uh, no, I don't think we could infer political views or sexual
orientation. We have no training data on that! Moreover the
creepiness of the data we have is nothing - really, nothing - compared to that of Facebook, Google, or myriad other tech
companies that are far more integrated into your life.  
,
The only data we have access to are clicks, as opposed to all the text
you enter into most sites or apps or, for mobile apps, your location
(and phone calls, and photos....)
,
We're not a mail or messaging app. I get creeped out just thinking
about how much people share with those sites; when you think about how much data you give via your gmail and searches alone, we've got nothing creepy.
,
,
,
,: Every publisher now a start-up, says NYT Top Data Scientist ,: Newspaper ad revenue drops dramatically ,
,
,
, To clarify: the slide I pointed to was specifically to print
advertising revenue, not overall revenue. 
NYT responded to this already by creating paid digital products (collectively called ""The Paywall"") which now accounts for a large fraction of total revenue.
,
As to why I'm here: the New York Times is going through a really
exciting transition right now, where nearly every function of
the company is being transformed by data. This change is not
only reflective of a change in tool set (awesome hardware and
software) but in mind set -- more and more people are wondering
how data can help them make hard decisions and, most
importantly, better understand our audience.  We're trying
to define how to give data a seat at the table in making
hard decisions both on the business and newsroom side.
,
Also the mission is extremely important to me. 
I agree with Jefferson on the importance of defending a strong and free press for
the functioning of democracy, and there's simply no better place for journalism
in the country than the New York Times.
,
Everyone here is very forward looking about how technology is changing the way people demand and interact with quality journalism, including the role
of data science smoothing and speeding the digital transition.
,
In short you're right that news as a business is being
challenged, but the effect of that, along with the importance of
the Times' mission, has been to draw in more and more
super-talented and energetic people all the time, who want to
work somewhere were they can make a huge difference not only on
a company but on society.
,
,
,
,
,
,
,
Lots! I'd encourage you to read Chapter 1 of 
,. 
or 
,
at Harvard for more.
,
The most important advice is to apply to come work with us!
,
,
,
,  "
"
        ,
,
,  "
"
,
,
,
You've already mastered data collection and analysis; it's now time to take the next step. Find out how to challenge assumptions, spot patterns and reveal potential solutions to problems that otherwise would not be visible.
,
,
,
,
,
,??,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
,
,
Are you a passionate user or developer of open source data science projects? Is your company already benefiting from these projects? Then help us take open source data science to the next level by speaking at our upcoming conference.
, 
, 
,
, 
We?€?re heavily focused on applied data science featuring real world applications. Here is the list of suggested languages, tools and topics we?€?re looking to cover:
,
, 
,
We are also hosting a non-technical talk track aimed at decision makers and thus are looking for a range of non-technical but data-centric talks.
, 
The conference will take place at the Boston Convention Center on May 30 to 31, 2015 (Saturday and Sunday).
,
If you?€?re interested in covering any topics, please contact us at , and include the following: 
,
   A brief Bio of yourself,
   A Link to LinkedIn or similar if available,
   A Proposed talk working title and topic,
   A Brief talk abstract,
,
Open Data Science Conference Team,
Visit our , for more information about the conference.
,
,
,  "
"
Call For Papers
,
ACM Transactions on Knowledge Discovery from Data
,
, Philip S. Yu, University of Illinois at Chicago, USA
,
, welcomes papers on a full range of research in the knowledge discovery and analysis of diverse forms of data. 
,
Such subjects include, but not limited to: scalable and effective algorithms for data mining and big data analysis, mining brain networks, mining data streams, mining multi-media data, mining high-dimensional data, mining text, Web, and semi-structured data, mining spatial and temporal data, data mining for community generation, social network analysis, and graph structured data, security and privacy issues in data mining, visual, interactive and online data mining, pre-processing and post-processing for data mining, robust and scalable statistical methods, data mining languages, foundations of data mining, KDD framework and process, and novel applications and infrastructures exploiting data mining technology including massively parallel processing and cloud computing platforms. 
,
TKDD encourages papers that explore the above subjects in the context of large distributed networks of computers, parallel or multiprocessing computers, or new data devices. TKDD also encourages papers that describe emerging data mining applications that cannot be satisfied by the current data mining technology.
,
TKDD welcomes papers that both lay theoretical foundations for data mining, big data and those that provide new insights into the design and implementation of large-scale data mining systems and tools, data mining interface tools, and data mining tools that integrate with the overall information processing infrastructure. The emphasis on integration of theory and practice is an attempt to encourage authors of theory papers to consider applicability and/or implementability of the theoretical results, while encouraging authors of systems papers to reflect on the theoretical results that may have been used in building the systems and/or to offer suggestions on issues that may require theoretical treatment.
,
For further information and to submit you manuscript, please visit ,
,
,
Charu Aggarwal, IBM T. J. Watson Research, USA,
Diane J. Cook, Washington State University, USA,
Ian Davidson, University of California at Davis, USA,
Wei Fan, Huawei Noah?€?s Ark Lab, Hong Kong,
Johannes Gehrke, Cornell University, USA,
Aristides Gionis, Aalto University, Finland,
Rong Jin, Michigan State University, USA,
George Karypis, University of Minnesota, USA,
Irwin King, The Chinese University of Hong Kong, Hong Kong,
Ravi Kumar, Google, USA,
Tao Li, Florida International University, USA,
Srinivasan Parthasarathy, The Ohio State University, USA,
Jian Pei, Simon Fraser University, Canada,
Jimeng Sun, Georgia Institute of Technology, USA,
Vincent S. Tseng, National Cheng Kung University, Taiwan,
Ke Wang, Simon Fraser University, Canada,
Xindong Wu, University of Vermont, USA,
Mohammed J. Zaki, Rensselaer Polytechnic Institute, USA
,

,
,  "
"
By Isabelle Guyon.
,
,
,
Five rounds until June 2015,
First deadline February 14 (Tweakathon 0: submit results on sample datasets; AutoML 1: submit code to solve binary classification problems)
,

Design the perfect machine learning ?€?black box?€? capable of performing all model selection and hyper-parameter tuning without any human intervention.
,
,
,
We are progressively introducing 30 classification and regression tasks, with datasets pre-formatted in feature representations. There is a broad diversity of data types and distributions and the problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. 
,
There is a prize pool of $30,000 donated by Microsoft if you are willing to make your code publicly available. No condition on releasing code or algorithms to just enter the challenge.
,
,

,
,
,  "
"
,
,

,

,

,

,

,

,
,
,

,


,


,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,
,  "
"
,
Which executives, entrepreneurs and experts are shaping the Big Data market? Information Management tracks the names and personalities you need to know.
,
1. ,, World?€?s First Chief Data Officer
,
Cathryne Clay Doss is known as the world?€?s first chief data officer ?€? she was named to that post at Capital One in 2003. Gradually, more financial institutions added CDOs to their executive ranks, and the position gained stature in other verticals as well. She has since held data-related executive positions at Equifax and Fannie Mae, while also building her own business ?€? called Mother of Six LLC.
,
2. ,, CDO for Los Angeles County
,
Under Farahani?€?s leadership, Los Angeles County is developing a data management initiative that spans big data, analytics, and a push away from siloed data environments. He has held IT-related positions for Los Angeles County since 1989, rising to the CDO post in August 2012.
,
3. ,, Big Data Evangelist, Cloudera
,
Amy O?€?Connor is a technical evangelist for Big Data solutions at Cloudera ?€? one of the leading Apache Hadoop providers. Previously, she led the Big Data group at Nokia, and prior to that she was VP of services marketing at Sun Microsystems. Although Big Data is serious business, O?€?Connor keeps it real ?€? describing herself as ?€?a geek in high heels.?€?
,
4. ,, Chief Data Officer, Intercontinental Exchange Group/NYSE
,
Steve Hirsch co-founded the NYSE Big Data software team, which is focused on developing and commercializing a tool set (Pivotal Data Dispatch) built to run the exchange?€?s data environment. As part of the senior technical team, he now leads a team of 150+ people who drive Data Architecture, Database Development, Data Operations, Storage and Automation, supporting Data Products, Analytics, Data Warehousing and Transaction platforms.
,
5. ,, Big Data and Analytics Emerging Roles Leader, IBM
,
Cortnie Abercrombie is the Emerging Roles Leader for Big Data & Analytics at IBM. Her career has focused on inspiring and inciting innovation and action using data and analytics at a multitude of companies and industries including: Verizon, Citi, NTT and IBM. Now at IBM, she is an advocate and evangelist of Chief Data Officers, Chief Analytics Officers and Chief Data Scientists.
,
6. Rick Kochhar, Chief Data Officer, TD Bank
,
Rick Kochhar and his team are accountable for delivering a unified approach for TD Bank?€?s Business Segments and Corporate Functions. The overall goals are to leverage data as an enterprise asset; enable legendary customer, client, and employee experiences; and ensure continuation of a robust record of regulatory compliance.
,
7. ,, CDO, Raymond James Financial
,
Jennifer Ippoliti has more than 25 years of data management experience in financial services and capital markets, working with corporate and investment banks, brokerages, exchanges, hedge funds, ECNs, and data vendors. She was founder of Wipro's Global Data Management practice, and a founding member of Accenture?€?s Global Data Management practice. Now, she?€?s leveraging all of those skills as Chief Data Officer for Raymond James Financial.
,
8. ,, CEO, MapR
,
John Schroeder has served as MapR?€?s CEO and chairman since founding the company in 2009. Over the past five years, has has built MapR into one of the best-known providers of Apache Hadoop solutions. Previously, he was CEO of Calista Technologies (now Microsoft), CEO of Rainfinity (now EMC), and also held executive positions at Brio Technologies and Compuware.
,
9. ,, Chief Data Officer, AIG
,
What are the keys to success for a Big Data pilot program? ?€?It?€?s critical to be the queen (or king) of communication,?€? AIG Chief Data Officer Heather Wilson told McKinsey & Co. in 2014. ?€?You have to spend the time to make sure people know where you are and where you?€?re going with new data capabilities. To do this, you need to have a whole communication strategy. I talk to our executive sponsors monthly, and oftentimes every couple of weeks, to ensure they understand the pilot status. They have to understand what is in the pilot and how it affects their business unit. They are the owners of the pilots, so it is essential to communicate the entire life cycle of the project. Additionally, we created metrics with our executive sponsors to baseline our efforts.?€?
,
Wilson previously was Chief Data Officer at Citigroup as well as the Global Head of Decision Sciences and the Head of Big Data analytics.
,
10. ,, Founding CEO of Hortonworks
,
Eric Baldeschwieler exited Hortonworks in August 2013, but his impact on the Big Data market remains legendary. In mid-2005, Eric was at Yahoo ?€? where he ?€?assembled engineers to solve a really hard problem: store and process the data on the Internet in a simple, scalable and economically feasible way. Those were humble beginnings for an open source technology, namely Apache Hadoop.?€? Fast-forward to the present, and Hadoop is one of the most important technologies within the Big Data revolution. Plus, Hortonworks has gone public.
,
Original: ,
,
,
,
,  "
"
Most popular 
, tweets for Jan 12-13 were
,
Dilbert looks at #analytics of #dating and A/B testing  #funny #humor 
, 
,
,
,
Convolutional Neural Networks (LeNet) - #DeepLearning 0.1 documentation 
, 
,
,
,
#DataScience and #Statistics: Colleges Must Evolve to give students needed data engineering and #BigData skills 
,
,
Dilbert looks at #analytics of #dating and A/B testing  #funny #humor 
, 
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Seth Grimes, Jan 2015.
,

The , is the first, biggest, and best conference to tackle the business value of sentiment, mood, opinion, and emotion ?€? linked to the spectrum of big data sources and exploited for the range of consumer, business, research, media, and social applications.
,

,
The key to a great conference is great speakers. This year?€?s symposium ?€? running two full days, July 15-16, 2015 in New York ?€? will feature a presentations track and a workshop track. Whether you?€?re a business visionary, experienced user, technologist, or consultant, ,. Choose from among the suggested topics or surprise us. Help us build on our track record of bringing attendees ?€? over 175 total at the 2014 symposium ?€? useful, informative technical and business content. Please submit your proposal by January 23, 2015.
,

While the symposium is not a scientific conference, we welcome technical and research presentations so long as they are linked to practice. In case you?€?re not familiar with the symposium: ,.
,

As in past years, we?€?re inviting talks on customer experience, brand strategy, market research, media & publishing, social and interaction analytics, clinical medicine, and other topics. On the tech side, show off what you know about natural language processing, machine learning, speech and emotion analytics, and intent modeling. New this year, we?€?re planning a special segment on sentiment applications for financial markets.
,

Please help us create another great symposium! ,.
,
Thanks!

,
P.S. If you?€?re not up for speaking but would like to register, ,. And if you represent a solution provider or consultancy that would like to sponsor the symposium, it would be great to have your support, so just send me a note.
,

,
,  "
"
,
,

, is an educator, researcher, and entrepreneur. He is the LexisNexis Eminent Scholar and founder/executive director of the Ohio Center of Excellence in Knowledge-enabled Computing (,) at Wright State University. Kno.e.sis conducts research in social/sensor/ semantic data and Web 3.0 with real-world applications and multidisciplinary solutions for translational research, healthcare and life sciences, cognitive science, and others. 
,
He is among well cited authors in ,, ,, and ,. His research has led to several commercial products, many real-world applications, and three successful startups. One of these was Taalee/Voquette/ Semagix, which was likely the first company (founded in 1999) that developed Semantic Web enabled search, analysis and applications.
,
Here is my interview with him:
,
,
,
,: In my first use of the term , I described it as ?€?realizing productivity, efficiency, and effectiveness gains by using semantics to transform raw data into Smart Data.?€? More recently, in my ,, given the interest in Big Data, I describe it ,as what makes sense out of Big Data. That is, , ??Better and timely decisions are enabled by contextualized and personalized transformation of (raw, multimodal, big) data into situational awareness and actionable information using (background) knowledge.

,

,

,

,We are relatively early in our pursuit of Smart Data. There is too much attention on Big Data, a term that focuses on the problems or challenges of the Vs (volume, variety, velocity and veracity), rather than Smart Data, a term that focuses on ,value and implies solutions we (as humans, and applications serving humans) seek from all the data related challenges.?? Transforming data into actions, through contextual and personalized processing of data, applying both top brain (e.g., planning) and bottom brain (e.g., classification, interpretation, perception) processes is something we have started to appreciate. Further, all these need to be done with very diverse (multimodal, multisensory, a variety of resolutions, uncertainty, etc.) data, as human brains are capable of processing, but with much larger volume and velocity of data.

,

One of the milestone we have achieved relates to what we call semantic perception, which in essence?? converts diverse data within the context of a domain specific knowledge into higher levels of abstractions relevant to human actions and decision making (a , describing the idea; a , and , discussing Smart Data in more details using the example of one specific dHealth/mHealth application on Asthma control from the , project at Kno.e.sis).?? 

,

At the same time, there are many complementary and exciting developments in areas such as human-centered AI, including brain-inspired computing, which are relevant to the technologies I expect to enable Smart Data.

,

,

,

,For an increasing number of applications that can lead to improvement in quality of human life and experiences, we need to utilize data from the ,, such as observations recorded by devices/sensors/Internet of Things, the , (all the data/facts and knowledge residing on the Web), and the , (all relevant conversations, opinions, experiences shared over the social web).?? For example, in the case of asthma, we use data from the , obtained from sensors detecting nitric oxide, carbon monoxide, indoor temperature, dust, humidity, wheezing, outdoor pollen and smog, etc. From the , we utilize?? a conceptual model for how physicians classify levels of asthma control, historical information, information collected from health organizations (e.g., number of asthma related admissions and deviations from the norm), and from the ,we utilize data such as regional and local tweets on asthma and related symptom.

,
Some of the data arrives continuously (few observations every second or minute), while other arrives less frequently (such as a daily report from health facilities or wheezing information provided periodically by a patient). Notice that different patients will be susceptible to different conditions, and typically none of the data in isolation would make any sense to a patient or even doctor. What a human (patient, doctor, epidemiologist, or public health worker) really needs is actionable information?€? different in each cases. For example, a patient would like to know what he/she can do to avoid an asthma episode. When is risk high enough to take a preventive action (such as using inhaler), and how can this be done while also avoiding overuse? We capture these in terms as abstractions such as a risk score and vulnerability score; these score are examples of Smart Data, and result in patients seeking medical attention or carrying out physician prescribed procedures. This form of Smart Data and actionable information are artifacts of computing that lead to an improvement, in this case, of the quality of life and more generally, an improvement in overall human experience.

,

,

,

,When I was at University of Georgia (UGA), I had a very successful lab named Large Scale Distributed Information Systems lab. While at UGA, I also did two start-ups and engaged in several projects in multidisciplinary research.?? As a ,LexisNexis Ohio Eminent Scholar with an excellent commitment in terms of endowment, space, and overall charge, I knew that I could do something bigger and achieve a larger real-world impact through technology development and transfer, commercialization, as well as social and human development. I chose to take a highly multidisciplinary route.

,

Besides computer scientists (covering distributed systems/cloud computing, semantic/social/sensor Webs, AI/KR/ML/NLP, data mining and IR), we have an extensive amount of involvement with researchers and practitioners (both as faculty members and collaborators) in biomedicine, bioinformatics, clinical research and practice, cognitive science,?? and humanitarians.?? What we do is perform high quality computer science and interdisciplinary research while addressing ,:
,
,??,

One objective of our research remains constant: continue to achieve exceptional outcomes for our students.
,
,
,
,
,  "
"
,

,
, is an educator, researcher, and entrepreneur. He is the LexisNexis Eminent Scholar and founder/executive director of the Ohio Center of Excellence in Knowledge-enabled Computing (,) at Wright State University. Kno.e.sis conducts research in social/sensor/ semantic data and Web 3.0 with real-world applications and multidisciplinary solutions for translational research, healthcare and life sciences, cognitive science, and others.

,

He is among well cited authors in ,, ,, and ,. His research has led to several commercial products, many real-world applications, and three successful startups. One of these was Taalee/Voquette/ Semagix, which was likely the first company (founded in 1999) that developed Semantic Web enabled search, analysis and applications.
,
,.
,

Here is second part of my interview with him:


,
,

,

,: Social media monitoring and analysis is a very active, noisy area for both research and commercial activities. , probably has the most advanced technical capability to analyze social media along with relevant open (linked, structured) data along multitude of dimensions, including spatio-temporal-thematic, people-content-network, and sentiment-emotion-subjectivity. It does real-time, highly-scalable (it is hosted on our cloud with 864 cores, 17TB main memory, and 435TB of storage) semantic processing that utilizes extensive background knowledge (domain specific models and knowledge) for challenging disambiguation problems (Turkey as in bird or country?), personalization, and contextualization.

,

However, above all these features, it is the ability to quickly utilize components of Twitris to provide actionable information for situations such as major disasters or crises that make it all the more unique. It has been used during a variety of disasters (see ,). Its real-world use during Jammu-Kashmir Floods resulting in saving lives and reducing suffering is my most favorite. Hemant Purohit, one of my PhD students along with a couple of his Twitris team members quickly isolated Twitris component to develop a tool that allowed a number of international digital volunteers to quickly filter a stream of?? specific requests for rescue in addition to information related to evacuated zones and rescued people; it also enabled the volunteers to redirect rescue calls to?? the Indian Army?€?s coordination office, leading to actual rescues (,; tool ,).

,

,

,

,Your choice of the term ?€?actionable insight?€? is significant, as monitoring or analysis is much less valuable compared to the ability to take action and affect outcomes in crises and other situations demanding real-time processing. 
,
,

,

Additionally, with respect to some of the issues of disambiguation, when analyzing tweets on cannabis and synthetic cannabinoids, which routinely exceed provides over a million tweets a day, using the term ?€?spice?€? to filter for a synthetic drug, results containing or referring to a ?€?pumpkin spice latte?€? must be avoided. Also, with respect to issues of data sparsity and actionable information, when we analyzed two million tweets related to an Oklahoma tornado in search of help and offering help, only 1.2% and 0.02% of the tweets were found to belong to these categories, respectively.
,

,

,

,This is an important and exciting topic of research which has not received much attention. 
,
,

,

Semantics is key to dealing with variety, and earlier in systems such as , and, more recently, in our , project we have addressed issues such as heterogeneous sensor data fusion and real-time analysis. By adding our recent work on ,, we are now working on what I term ,.?? I will soon discuss my thoughts on semantic, cognitive, and perceptual computing in an article.

,

,

,

,Being an entrepreneur involves people, business and technical skills. For me, technical, marketing, financial issues were less of an issue, but as a CEO, you are also the company?€?s chief spokesperson and chief salesman. For me, sales was the toughest part, primarily because I did not like the process of going back to a potential customer. That said, I found quite a few ,: Taalee (before it merged to become Voquette, later Semagix, then acquired to become Fortent and now Actimize) grew to 30+ employees, spent 7 million in local payroll in Athens, GA which had no previous high-tech startup, all its employees were retained during the merger, whereas most of the employees of the other company were let go.

,

Even today, the Know Your Customer technology which we developed is deployed at some of the world?€?s largest banks. Taalee was awarded the first patent on semantic search, browsing, personalization, and advertising, and also developed first commercial products/applications on these topics.?? More recent use of a very similar ,, as I reviewed recently, makes me particularly proud as it validates what we pioneered nearly 15 years ago.
,
,
,
,
,  "
"
,
,
,
You know the feeling. You?€?re sat across the table from an interviewee and the conversation starts to run away from you. Your eyes start scanning the room as they?€?re reeling off another jargon-crammed answer. You notice the chip in the Channing Tatum mug you received from the office Secret Santa last Christmas. You then notice the scratch on your hand that your 4 year old gave you, in exchange for closing the laptop during that far too familiar One Direction song. The only thing you?€?re not noticing are the answers the other person in the room has been reeling off at you.
, 
If this typically happens to you most when you?€?re conducting technical interviews, it could be either that the person your interviewing is talking in a language not known to you, or you simply don?€?t have the right questions to ask that cut through the techno-babble and get to the point of why they're sat there in front of you.
,
,
The polar ice caps of the ?€?Data Scientist Age?€? have well and truly melted, so if you are one of those countless companies who are currently looking for one, there?€?s a chance you?€?re going to have one of these interviews.
,
The subject of this article probably won?€?t offer any value to those of you reading who know your way around a Random Forest, or a Neural Network, but it?€?ll hopefully give a few pointers to those of you who don?€?t.
, 
As I mentioned earlier, the age of the Data scientist is well and truly here. This means that are highly talented geniuses in our population who can change the landscape of an entire organisation, through the development of an algorithm and the implementation of some code.  This also unfortunately means that there are lots of people out there who want you to think that they are one of those geniuses too.  The question is, can you tell the difference?
, 
Having spent the last year interviewing a large number of Data Scientists, I?€?ve developed a simple set of questions that help me to understand , of what they do.
, 
,
, 
,At the very start of the interview, I normally like to get a general feel for who I?€?m speaking with. ?€?What projects are you most proud of??€? ?€?What contributions have you made to the businesses you?€?ve worked for??€? ?€?Can you describe in detail the responsibilities you?€?re looking for in your next position??€?
, 
When interviewing someone involved in post-doc/PhD projects, I?€?ll always look to get an understanding of the projects they?€?ve encountered there, however, it?€?s really important to understand the time constraints they?€?ve been under, as typically, someone solving problems in academia may be given far more time than you would encounter in the commercial world.
, 
,
, 
The best Data Scientists I?€?ve worked with have all had substantive expertise in a particular domain, or at the very least, will understand what the business impact is of the work they have doing.
, 
How can you solve the problems the business is facing, if you firstly don?€?t understand the business itself?  The same sometimes goes for domain too. However, it?€?s very important to note that a good Data Scientist will always be good at problem solving, no matter what domain they?€?re working in, so don?€?t get too hung up if the person you?€?re interviewing doesn?€?t work in your exact industry.
, 
I?€?ll typically ask questions like, ?€?What were the business outcomes of the projects you worked on??€? ?€?Give me an example of when you?€?ve thought about the businesses product??€? ?€?Tell me about a time when you?€?ve improved a business process??€?
, 
,
, 
,This is the part of the conversation where I?€?ll begin to understand whether or not the person I?€?m speaking with, is the person my client is looking for.
, 
It?€?s very important to note that there are lots of people with the title ?€?Data Scientist?€? who can?€?t write Machine Learning algorithms, can?€?t code?€?or can?€?t do either.
, 
What you may find, is some Data Scientists who use Machine Learning libraries that have been written by other people in the team, filled with algorithms they use like tracing paper. ?€?Can you give me an example of when you?€?ve written a unique algorithm??€? ?€?Can you give me an example of when you?€?ve developed an algorithm from a framework/research paper??€? ?€?Can you give me an example of when you?€?ve written/implemented your own code??€?, these are questions to help you understand if you?€?re speaking to someone who can create things from scratch, as these are typically the ?€?A Players?€? you?€?ll want to hire.
 
, 
It?€?s worth pointing out that if you find someone who has nailed all of the above questions and you have that gut feel that they may do wonders for your business, please don?€?t get too precious about culture, team fit, etc.  Don't get me wrong, these things are important, but people like this can be incredibly hard to find?€?sometimes harder than finding that missing piece of Mr Tatum.
,
,, is a Partner and Co-Founder at ,, a big thinking recruitment company focused on all things Big Data & Data Science. He is based in  Manchester, UK.
,
,
,
,
,
,
,
,
 ,  "















"
, is designed to strengthen your analytical and data management success. Join us this March to explore how major brands are harnessing the potential of ""big data"" to reshape their brands through information platforms. Dive deep into the dynamic 
,, and 
,
that will address a range of issues integral to marketing analytics, including:
,
,??,
You will interact with accomplished analytics leaders who have broad experiences and perspectives related to the role of 
,. 
Speakers include leaders from LinkedIn, IBM, Razorfish, Ford Motor Company, Northwestern University, Adobe, Blue Cross and Blue Shield of North Carolina, The Weather Company, Starwood Hotels, and many more. 
,
Join us and emerge ready to make stronger data-driven decisions that build competitive advantage and set you and your brand apart.
,
Visit 
,
to learn more and to register today! 
,
Use discount code 
,
and receive 15% off Full Conference and Main Conference packages!
,
Follow us , and 
,.  "

"
,

,
, is an educator, researcher, and entrepreneur. He is the LexisNexis Eminent Scholar and founder/executive director of the Ohio Center of Excellence in Knowledge-enabled Computing (,) at Wright State University. Kno.e.sis conducts research in social/sensor/ semantic data and Web 3.0 with real-world applications and multidisciplinary solutions for translational research, healthcare and life sciences, cognitive science, and others.
,
He is among well cited authors in ,, ,, and ,. His research has led to several commercial products, many real-world applications, and three successful startups. One of these was Taalee/Voquette/ Semagix, which was likely the first company (founded in 1999) that developed Semantic Web enabled search, analysis and applications.

,

,.

,
,.
,
Here is third and last part of my interview with him:
,

,
,
,:
,The certificate we now offer is called ?€?Big and Smart Data Sciences?€?, and we hope it will become a degree program in near future.?? There are now quite a few Big Data and Data Sciences related academic programs. We wanted to focus on a subset where we could provide a unique product, and this is reflected in the use of term Smart and in the combination of Big Data with Data Science.

,

My MS students who have done theses have had good success in getting high quality and high paying jobs. Through this certificate, working with other excellent faculty members in Kno.e.sis and our CSE department, we want to create better prepared MS students in an area with high demand. One way we can do this is by involving students in class projects that are aligned with our excellent research program in this area. We know that recruiters and companies do pay more attention to significant projects and internships on resume, rather than a list of courses and GPA.

,

,

,

,There has been a series of interesting, insightful, and provoking interviews and blogs by such stalwarts as Michael Jordan and Michael Stonebraker on Big Data. The little I could add are these points: (a) variety is a more difficult Big Data problem right now, compared to volume and velocity. The reason is that solving the ,variety problem typically requires human involvement (for such tasks as understanding data formats/structures/modality and their mapping to other related data), compared to other challenges where you can either throw computing power, improved algorithm or technology at the issue and arrive at a solution, and (b)?? both Big Data and Machine learning are at the top of the hype curve now; I think the focus will shift from data (e.g., what hidden insights in the forms of patterns can be extracted) to serving human needs that data can address in part (complemented by knowledge on the topic, etc.) but not exclusively, as is advocated by Smart Data, and Machine Learning will become an additional tool, albeit an important one, as people realize the need for complementing bottom brain type of processing (which Machine Learning supports) with top brain type of analysis.

,

The trend that will drive the growth will be increased and more effective human involvement in improving the processing of Big Data in comparison to current Machine Learning focused processing, and training part of the process will become more active and continuous.

,

,

,

,It may sound strange, but I do not recall receiving explicit advice related to my career. However, I do recall soliciting advice from Prof., on whether I could develop a world-class organization at a mid-tier university (he thought so, and I believe we have succeeded as exemplified with ,). My career path has taken me from Industry R&D to academia and startups. It received its inspiration from the similar paths that Ramesh and several others took.

,

,

,

,Recruiting good students was a serious business in the early days. Now-a-days, perhaps because the track , is rather clear, it is more a matter of selection from among those who contact me expressing specific and well-articulated reasons for why s/he would like to work with me. I subscribe to Malcolm Gladwell?€?s empirical study presented in the Outliers on the dominant role of desire to succeed and work ethics to go along.

,
Rather than GRE scores and GPA, I rely on extensive email exchanges and a video chat to understand why the student wants to do PhD, whys/he is aiming high, why s/he is willing to work to achieve the outcomes, and if I can see myself making a real difference in helping that student achieve her/his career goals. (My advisees have gone to academia, industry research labs, startups or have started their own startups soon after graduating, or industry engineering jobs).?? When possible, I also look for non-CS and eclectic or interdisciplinary backgrounds (former advisees who came from management, statistics, cognitive science, math and biomedical backgrounds have done fabulously well).
,
,

,

,
,: Surprising Insights into How You Think. I liked it, but the one in this general area that I liked more and read a couple of years ago was: ,: A Neuroscientist's Quest for What Makes Us Human.
,
,
,  "
"
Most popular 
, tweets for Jan 14-15 were
,
Computer needs 10 FB likes to predict personality better than a co-worker; 70 likes ~ a friend; 300 likes ~ a spouse 
,
,
A Deep Dive into Recurrent Neural Nets #DeepLearning #MachineLearning 
, 
,
,
,
A Deep Dive into Recurrent Neural Nets #DeepLearning #MachineLearning 
, 
,
,
A Deep Dive into Recurrent Neural Nets #DeepLearning #MachineLearning 
, 
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
In a previous article about ,, I analyzed the top articles about Big Data on the site. This was achieved by using Python to gather data directly from 
,. Since then, however, it?€?s been pointed out that there are some slideshares about big data that are clearly very important that were excluded.
,
For example, this slideshare, ,, by Bernard Marr was not included in the previous post despite having ten times the number of views of any of the slideshares included before! So clearly, using the API to retrieve the top slideshares was missing ,. But just how much do these results differ? Here are the top 15 big data presentations on slideshare, using a the Google search ?€?big data site:slideshare.net?€?:
,
,
,
Immediately, compared to the results returned by the SlideShare API, we see these presentations have far more views. In fact, the presentation returned from the SlideShare API with the most views/day has fewer views/day than the least popular presentation returned from the Google search! There is definitely something strange going on here.
,
,
,
To understand what?€?s causing this difference, we must first dive into the SlideShare API. The results from the previous post were reached using , (information on running them in the comments). What this does is contact the SlideShare API, request posts matching the provided query string by tag (in this case, ?€?big data?€?), and return the most viewed presentations. The Google results, on the other hand, are returned from Google?€?s site-specific search.
,
So if the API is searching by tag, if we search for ?€?big data?€? and the post has ?€?big data?€? as a tag, we should get it right? Maybe these posts simply weren?€?t tagged properly? Nope! If you use , and search for the details of, say, ?€?,?€? by , with ?€?detailed?€? enabled, you will find this presentation , have the tag ?€?big data?€?.
,
With this in mind, it seems the SlideShare search API doesn?€?t exactly return the results I?€?d expect. On top of this, if you choose to use the API to search by text, the results are very noisy, only being tangentially related to big data. This serves as a lesson: with the increasing availability of REST APIs to access data on the web, it?€?s becoming more convenient to gather data. But before you choose to use a site?€?s API (versus a more old-school crawler), make sure the API works as expected. Otherwise, you may miss out on data you?€?re interested in.
,
,
,
,  "
"
By Nicky Sarof.
,
,
My journey in data science began with an undergraduate course in data mining. It opened me up to a world of possibilities. The brilliance of it was that it could be used to solve a wide variety of problems such as identifying fraud, grouping products that were often bought together etc. and thus, better understand human behaviour and the only catch was that there needs to be data and patterns to find. When the opportunity to work at an analytics firm presented itself, I jumped at it!
,
After working for a few years, I decided that I needed to broaden my horizons and after much deliberation, I chose to attend the 
,. I chose the program because of:,
,

,
The curriculum itself is balanced and comprehensive that covers the current data science trends and an in depth coverage of business (telecom and utilities, finance, marketing etc.) and technical components including big data technologies such as spark, HDFS etc. There is a lot of emphasis on working in teams, development of business communication skills with a special focus on problem solving using real world cases.
, 
,
A typical day at IE includes 5-6 hours of classes, group work and individual work. The classes are conducted by eminent professors who are leaders and experts in their respective fields, and bring with them a wealth of experience in dealing with Real World Analytics Problems. As such we not only learn about the different techniques and algorithms, but also how and when those techniques and algorithms are used in solving real world business problems.
,
,
Groups generally have a broad mix of people from different places and diverse professional backgrounds which helps bring different perspectives and ideas when solving a problem. It is also easily one of the most important components of the program. Individual work involves a lot of self-study to back up the learning from the classes.
, 
,
As a part of the, we are exposed to a lot of challenging courses. One such course was Financial analytics. As a part of our group project for the class, a contest (with competition leader board) was held wherein 5 different teams competed to come up with the best model for detecting fraudulent transactions. 
,
Some of the key components of the exercise for coming up with the best model were-,
,
 
,
,
,
 ,
Different teams used different strategies. While some focused on using all the variables with a standard algorithm, some focused on using the right algorithm/technique in conjunction with right variable selection. On the leader board, it soon became apparent that having the right set of variables is as important as the model building itself.
, 
During the course of Model Development, we realized that different techniques are affected by data in different ways ?€? OLS Regression for instance is highly affected by outliers and non-linearity in the data. We also discovered that the right method of evaluation of the model is as important and why an over fitted model performs badly on out of time sample. By the end of the module, we had developed sufficient know how in tackling and building a model on a real world dataset
, 
,
,The biggest learning for us being this far into the program is that analytics sits on an interesting intersection of different fields and in order to be successful, it requires the ability to not only adapt quickly to change but also apply different problem solving approaches/algorithms to solve business problems. There is no one size fits all and finding the right combination of trade-off between accuracy and other measures is often very challenging.
, 

, is an analytics consultant with a background in computer science and the recipient of the Big Data Asia Scholarship, currently pursuing his Master in Business Analytics and Big data at IE (Madrid, Spain). 
,
,
,
,
,  "
"
,
,
This week on /r/MachineLearning, we there have been some interesting posts and videos, in particular.
,
,
,
,
,
This video lecture, part of MIT 6.034 Artificial Intelligence, offers a clear introduction to SVMs including the mathematics behind them without being too arcane for an advanced undergraduate viewer. In the comments, /u/Eagle-Eye-Smith notes ?€?The entire series is a great source of information for anyone wanting to learn about artificial intelligence, machine learning and pattern recognition,?€? so be sure to check out other videos from the course that pique your interest.
,
,
,
This post is interesting not just because it has some good datasets (,) but because it focuses on open-source datasets. Most of these datasets should offer some form of open-source license, though I would check the particular dataset you?€?re interested to be sure.
,
,
,
This blog post is a nice introduction to Bayesian decision theory at a relaxed pace, which works great for developing intuition if. If these sorts of tutorials are your cup of tea, definitely check it out.
,
,
,
,
,
This YouTube video shows the winning submission to the , challenge at work. If you?€?re interested in AI applications or games, this is definitely worth looking at, and if this seems like the sort of thing you?€?re interested in, the 2015 competition still has an ,.
,
,
,
,
,
This post has some very beautiful visualizations of different NN and deep learning based algorithms. My favorite is the ,. One warning, though: the webpage is quite heavyweight and may take some time to load.
,
,
,
,  "
"
,
,
,
Global warming is real. Winters come later and milder and summers come sooner and hotter. 
Many , and many , are being affected by the climate change. Scientists have made it clear that human activity is the main cause of this change and warn that damages caused so far (rise of sea levels, acidification of the oceans, melting of glaciers) will remain for centuries if governments don't take drastic measures to counteract global warming. 
,
The Intergovernmental Panel on Climate Change (IPCC) is the leading international body for the assessment of climate change. It was established by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO) in 1988 to provide the world with a clear scientific view on the current state of knowledge in climate change and its potential environmental and socio-economic impacts. These are some of their last forecasts (1):,
,
Is not necessary being a climatologist to confirm empirically global warming. It is enough to have a computer, a reliable data set of historical temperatures, and software like R to do some simple calculations. The 
, (ECA) site provides information on changes in weather and climate extremes, as well as the daily dataset needed to monitor and analyze these extremes. The ECA dataset contains series of daily observations at more than 2.500 meteorological stations throughout Europe. There are observations from the late 18th century to the present, although not all stations have records for the complete period.
,
It is easy to obtain a rough idea of the evolution of global temperature measuring the linear trend of the average temperature recorded by each station. This is the distribution of the meteorological stations according to this historical trend:
,
,
,
The distribution is significantly centered above zero: here we have the effect of global warming. Since ECA dataset provides geographical coordinates for every station, is also easy to locate in a map those with the highest trends. To do it I use the amazing googleVis R package:
,
,
,Click on the image above to open interactive data visualization
,
One of these stations is located in In Amenas (Algeria). These are the temperatures registered by this station from 1958 to 1998:
,
,
,
The rise, although very gradual, is palpable. 
,
Some days ago I read an interview with Bjorn Stevens, director at the 
Max-Planck-Institute for Meteorology where he leads the Atmosphere in the Earth System Department. He says, the big challenge is to require our politicians to learn from the best available knowledge: there is understandable information available to whoever wants to know.
,
Let?€?s hope they do. 
,
(1) ,
,
Here is
,.
,
Bio: Antonio S??nchez Chinch??n is mathematician and works as data scientist at Telef??nica. He is the creator of ,, an unclassifiable blog of mathematical experiments and R programming. You can follow him in ,
,
,
,
,
,
 ,  "
"
,
,
,
Data science and biology have never really mixed well. And in retrospect, it?€?s pretty understandable why. Biology and medicine have their own lingua franca, which makes for a pretty steep learning curve. People who thrive at this intersection not only have to be in tune with the fundamentals of biochemistry and genetics, but also need to be mathematically adept and strong algorithmic thinkers. 
,
For decades, we?€?ve gotten away with computer scientists sticking with computers and biologists sticking with genetics. But things are rapidly changing, and there?€?s a growing need for people who can bring a data-driven approach to medicine.  The advent of modern high-throughput biotechnology has brought upon a data deluge that has completely changed the field?€?s landscape. For example, a binary alignment file for a single human genome could easily amount to hundreds of gigabytes or terabytes of raw data.  Without data science, we risk missing out on valuable insights that could fundamentally change how we deliver medicine. 
,
Modern genetics is a clear example of where data science is already beginning to make huge impacts on our understanding of biology. Traditional biologists have nearly always approached biological systems from a highly simplified, focused perspective. We?€?ve tried to analyze single genes at a time, often isolated from the larger context in which they exist: protein A upregulates protein B which downregulates protein C. That?€?s all there was to it.
,

,

,
Caption: How biologists used to think about biochemical pathways.
,
But in reality, genetics is much more complicated than that. A single protein could have its expression be modulated by tens of upstream regulators (called transcription factors). And in turn, the same protein could affect the expression of hundreds of other proteins. In a sense, you can think about a cell?€?s genetics as a huge social network. The fact that protein A directly regulates protein B is analogous to person A following person B on Twitter.  So, quite surprisingly, the same techniques you might use to analyze a user?€?s Twitter network to get them to click an advertisement are also applicable to analyzing a cell?€?s regulatory network to diagnose disease and design new therapies.
,
But how exactly do we interrogate these relationships? How do we even know that protein A regulates protein B in a particular cell type? This is where high-throughput biotechnology comes in. Over the past couple of years, researchers have pioneered a technique called DNAse hypersensitivity ( ref: ,) which helps us infer these key relationships. In addition to having a region that directly codes for a protein, a gene also has a number of upstream sequences that are bound by regulatory proteins that control its expression. 
,
Essentially, the DNase hypersensitivity technique takes advantage of the fact that DNA, for the most part, is packaged very tightly except around these very specific regulatory sequences. As a result, when the DNA is exposed to a DNA digesting enzyme, it is mostly cut at these loosely-packed and exposed regions. The only exception is the small tract of nucleotides that are directly bound to a regulatory protein. These nucleotides are protected from digestion, resulting in a very clear ?€?transcription factor footprint.?€?
,
,
,
Caption: Histogram showing frequency of DNAse digestion at each location, with a characteristic hypersensitivity site (green) and corresponding transcription factor footprint (red)
,
We can then take the DNA sequences of the transcription factor footprints associated with each gene and predict the proteins bound to these regulatory regions using a database such as 
TRANSFAC (,). This procedure (,) enables us to reconstruct the genetic regulatory networks at play in every cell type in the body:
,
,
,
Caption: Algorithmically generating the network from footprint data. Figure borrowed from Neph et al. (,) 
,
This has a huge number of applications. For example, this data could be used to understand the foundational differences that differentiate difference cell types. Concretely, this could very significantly inform drug development by allowing researchers to predict how a drug for Alzheimer?€?s, for example, might have side-effects on the patient?€?s heart or kidney.
,
,
,
Caption: Comparing the regulatory networks in various cell types in the human body. Figure borrowed from Neph et al. (,) 
,
Moreover, my current research involves constructing these networks to compare humans to laboratory model organisms such as mice, rats, and chimpanzees. These comparative models could help us figure out why certain drugs work well in animal studies but fail miserable in clinical trials. Every single year, approximately 95% of drugs fail to obtain approval (,), and building these models could potentially save billions of dollars in wasted resources. 
,
With petabytes of data being produced every single year, biology and medicine need data science now more than ever before. Undoubtedly, data will shape the future in ways that we can only begin to imagine. 
,
Cross-posted from: ,
,
Bio: , is a computer science student at MIT with deep interests in machine learning and the biomedical sciences. He is a two time gold medalist at the International Biology Olympiad, a student researcher, and a ?€?hacker.?€? He was selected as a finalist in the 2012 International BioGENEius Challenge for his research on the pertussis vaccine. Nikhil also has a passion for education, regularly writing technical posts on his blog, teaching machine learning tutorials at hackathons, and in 2014, receiving the Young Innovator Award from the Gordon and Betty Moore Foundation for using augmented reality to re-envision the traditional chemistry set. In his free time, Nikhil loves pick-up basketball, improvising on the guitar, and teaching himself to play mainstream pop music on the piano.
,
,
,
,
,
 ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jan 20 and later.
,
See full schedule at , .
,
,  "
"
BOSTON, Mass. Jan. 20, 2015
,
,, 
the industry?€?s easiest-to-use Modern Analytics platform, today introduced , ?€? a program that provides free use of the commercial version of its platform to students, professors, researchers and other academics at non-profit educational institutions.
, 
,
said RapidMiner CEO Ingo Mierswa. 
, estimates a shortage in the U.S. of up to 1.7M professionals with analytics expertise. ?€?Giving back to the academic communities that helped ignite RapidMiner?€?s formation and success is at the very heart of this program,?€? Mierswa added.
, 
Dr. Katharina Morik, head of the Artificial Intelligence Unit at the University of Dortmund noted, ?€?Making RapidMiner available to academic communities puts a powerful modern analytics platform into the hands of professors to enrich their data analytics curriculum. Bringing RapidMiner into the student experience further prepares them for market entry as business analysts and data scientists of the future.?€?
,
,
,
,
, 
The RapidMiner user community in academia is strong with over 45,000 active users worldwide. 
Ranked by number of users: University of Nevada, Las Vegas; University of Arizona;
University of Illinois; Technical University of Dortmund; and the University of Pittsburgh top the list of more than 3,000 universities using the Modern Analytics platform.
, 
?€?In my lab, students love to analyze the big data from astrophysics with RapidMiner. They become data masters in astonishingly short time,?€? remarked Dr. Wolfgang Rhode, professor at the University of Dortmund.
, 
,
, 
RapidMiner understands the challenges of today?€?s academic leaders and the demands upon institutions to produce more skilled analytic professionals than ever before. The RapidMiner Academia program was created to serve students, professors and researchers on many levels.
,
,
,
Benefits for students and professors include:,
,
Additionally, students and professors will learn:,
,
,
,
The program also considers the need for university researchers looking to unlock higher value from data in advanced research initiatives. Both unfunded and funded researchers can take advantage of the latest RapidMiner technology. Benefits include:
,
,
For additional information on academic registration and eligibility for colleges and universities, visit ,.
,
,
RapidMiner enables anyone to make the most of all data in all environments, by providing a powerful code-free advantage and the wisdom of over 250,000 users around the world. For more information, visit ,.,
,
,
,  "
"
,
,Feb 10
,10 am PT, 1 pm ET
,
Linear regression plays a big part in the everyday life of a data analyst, but the results aren't always satisfactory. What if you could drastically improve prediction accuracy in your regression with a new model that handles missing values, interactions, AND nonlinearities in your data?
,
Instead of proceeding with a mediocre analysis, join us for this presentation, which will show you how MARS regression, TreeNet gradient boosting, and Random Forests can take your regression model to the next level with modern algorithms designed to expertly handle your modeling woes.
,
With these state-of-the-art techniques, you'll boost model performance without stumbling over confusing coefficients or problematic p-values!
,
,
,
45 minute presentation followed by Q&A with our experts   "
"
,
,.
,
,
,
It hasn?€?t always been well understood that correlation does not imply causation. In the late 1940s for example, public health experts recommended that people stop eating ice cream as part of an anti-polio diet. It turned out however that there was only a correlation between polio incidence and ice-cream consumption, because outbreaks were most common in the summer.
,
The distinction between correlation and causation is now much better understood.  Even if we accept that people who sleep with their shoes on often wake up with a headache, we?€?d more likely blame both incidents on their having too much to drink the night before than believe that the one is the cause of the other. But what would be the best way to proceed if we did want to determine whether the shoe wearing caused the headache? 
,
,
If you can influence A and observe how this modifies B, you might be able to make conclusions as to causation. For example, you can force yourself to go to bed with shoes. Observing that you don?€?t have more headaches than when you don?€?t wear shoes, you can tell that the shoes are not causing the headaches. 
,
Most of the time, however, you won't be able to use intervention because:,
,
,
,
So, can we deduce causation from pure observational data? If we?€?re given data about two variables X and Y, such as the one illustrated on the right, and we know that it?€?s either X???Y or Y???X (no common ancestor, etc.), can we pick the right direction for the arrow?
,
Researchers are divided on this question.Some argue that it is simply impossible ,, others, like J. Mooij, are more hopeful and argue that studying the distribution of noise can help us determine the cause and effect ,. 
,
Their intuition is that if X causes Y, then noise affecting X will also affect Y. 

,
,

Let me take the example of , vs ,, borrowed from ,. When the amount of traffic increases, it takes me more time to commute; when the traffic varies, my commuting time varies also. However, the opposite is less true. Sometimes, it takes me more time to commute than others: I might hit different red lights, be in more or less of a hurry, etc. 
,
More precisely, the core idea of these so-called , (ANM) is that if X???Y, then the variability observed in Y will be either explained by X, or by some noise that is independent of X. 
,
To test if ?€?traffic?€? affects ?€?commuting time?€?, they construct the following model: ,
,

They then use a regression to estimate , and noise (ie the residual). 
,
,
This is where it begins to get a tiny bit tricky:,
1. They look at the correlation between , and , (ie the variations of , that are not explained by the ,). ,
2. If , and , are independent, it means that the variations of , are independent of the ,. ,
3. This leads to being able to state that the noise affecting , does not affect ,. But, if X ??? Y is true, we have seen that the noise affecting X should also affect Y (here X is , and Y is ,). ,
4. It follows that , is unlikely to cause ,. ,
5. We can thus only choose the other direction: , ??? ,.,


,
In their paper ,, J. Mooij study the accuracy of these so-called , (ANM) over 88 datasets and conclude that ANM methods can put the right direction on the arrow with more than 65% accuracy. This opens new possibilities for inferring causation in circumstances where traditional methods make it unethical, impossible, or too expensive to do so. 
,
,
My name is Francois Petitjean (@LeDataMiner), and I try to develop useful solutions for big data. I completed my PhD working for the French Space Agency in 2012, and since then have been working in Geoff Webb?€?s team at Monash University?€?s Centre for Data Science.More at ,.
,
,
[1]    ,
[2]    ,
[3]  ,, QZ
,
,
,
,
,
 ,  "
"
,
,

,

,

,

,


,

,

,

,

,

,

,
,

,


,


,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,
,  "
"
Most popular 
, tweets for Jan 12-18 were
,
Dilbert looks at #analytics of #dating and A/B testing  #funny #humor , 
,
,
,
Great read: #Visualizing Representations: #DeepLearning and Human Beings #MachineLearning , 
,
,
,
Dilbert looks at #analytics of #dating and A/B testing  #funny #humor , 
,
,
A Deep Dive into Recurrent Neural Nets #DeepLearning #MachineLearning , 
,
,
,
,
  "
"
,
,
, is a Physicist & Hacker at ,. Prior to that, he was a founding Senior MTS at Skytree where he designed and implemented high-performance machine learning algorithms. He has over a decade of experience in high-performance computing and had access to the world?€?s largest supercomputers as a Staff Scientist at SLAC National Accelerator Laboratory where he participated in U.S. DOE scientific computing initiatives and collaborated with CERN. Arno has authored dozens of scientific papers and is a sought-after conference speaker. 
,
He holds a PhD and Masters summa cum laude in Physics from ETH Zurich. Arno was named , by Fortune Magazine.
,
Here is my interview with him:
,
,

,

,: Deep Learning methods use a composition of multiple non-linear transformations to model high-level abstractions in data. , are some of the oldest and yet most useful such techniques. We are now reaping the benefits of over , that began in the late 1950s when the term , was coined. Large parts of the growing success of Deep Learning in the past decade can be attributed to Moore?€?s law and the exponential speedup of computers, but there were also many algorithmic breakthroughs that enabled robust training of deep learners.

,

Compared to more interpretable Machine Learning techniques such as tree-based methods, conventional Deep Learning (using , and ,) is a rather ?€?brute-force?€? method that optimizes lots of coefficients (it is a , method) starting from random noise by continuously looking at examples from the training data. It follows the basic idea of ?€?(good) practice makes perfect?€? (similar to a real brain) without any strong guarantees on the quality of the model.

,

,
,
However, even some remarkably small Deep Learning models already outperform humans in many tasks, so the space of Artificial Intelligence is definitely getting more interesting.

,

,

,

,Deep Learning is really effective at , from the raw input features, unlike standard Machine Learning methods such as linear or tree-based methods. For example, if age and income are the two features used to predict spending, then a linear model would greatly benefit from , age and income ranges into distinct groups; while a tree-based model would learn to automatically dissect the two-dimensional space.

,

A Deep Learning model builds hierarchies of (hidden) derived non-linear features that get composed to approximate arbitrary functions such as sqrt((,^2+0.3*log(,-4) with much less effort than with other methods. Traditionally, data scientists , based on domain knowledge and experience, but , at coming up with those transformations, often outperforming standard Machine Learning models by a substantial margin.

,

Deep Learning is also very good at ,, such as in image or voice recognition problems, or in predicting the best item to recommend to a user. Another strength of Deep Learning is that it can also be used for unsupervised learning where it , learns the intrinsic structure of the data without making predictions (remember the ,?). This is useful in cases where there are no training labels, or for various other use cases such as ,.

,
,
,

,
,

,H2O is unique in that it?€?s the, on , (and we?€?re in the final phases of a more ,). It is built on top of a distributed key-value store that?€?s based on the world?€?s fastest non-blocking hash table, written by our CTO and co-founder ,, who is known for his contributions to the fast Java HotSpot compiler.

,
H2O is designed to process large datasets (e.g., from HDFS, S3 or NFS) at FORTRAN speeds using a , (that often beats gzip on disk). H2O doesn?€?t require Hadoop, but it can be , by MRv1, YARN or Mesos, for seamless data ingest from HDFS.

,

, tightly integrates the data pipelines in Apache Spark with H2O. In addition to native , and , APIs, H2O also provides a powerful REST API to connect from ,, ,, or , clients. It also powers our , for interactive exploration of H2O?€?s capabilities. There's also auto-generated Java code to , (e.g., with Storm), which many enterprise customers find useful.

,

H2O and its methods are also backed by , and some of the most knowledgeable experts in Machine Learning: Stanford professors ,, , and ,. Other independent mentors include Java API expert , and Founder of S and R-core member ,. We?€?ve literally spent days discussing algorithms, APIs and code together, which is a great honor and privilege. Of course, customers and users from the open source community are constantly validating our algorithms as well.

,
,
, ?? ,

For H2O Deep Learning, we put lots of little tricks together to make it a very powerful method right out of the box. For example, it features automatic adaptive weight initialization, automatic data standardization, expansion of categorical data, automatic handling of missing values, automatic adaptive learning rates, various regularization techniques, automatic performance tuning, load balancing, grid-search, N-fold cross-validation, checkpointing, and different distributed training modes on clusters for large datasets. And the best thing is that the user doesn?€?t need to know anything about Neural Networks, ,. It?€?s just as easy to train as a Random Forest and simply makes predictions for supervised regression or classification problems. For power users, there?€?s also quite a few (well-documented) options that enable fine-control of the learning process. By default, , and is , for maximum performance.

,

I share our CEO and co-founder,?€?s vision that a whole ecosystem of smart applications can emerge from these recent advances in machine intelligence and fundamentally enrich our lives.
,
,.
,
,
,  "
"
,
,
, is a Physicist & Hacker at ,. Prior to that, he was a founding Senior MTS at Skytree where he designed and implemented high-performance machine learning algorithms. He has over a decade of experience in high-performance computing and had access to the world?€?s largest supercomputers as a Staff Scientist at SLAC National Accelerator Laboratory where he participated in U.S. DOE scientific computing initiatives and collaborated with CERN. Arno has authored dozens of scientific papers and is a sought-after conference speaker.
,
He holds a PhD and Masters summa cum laude in Physics from ETH Zurich. Arno was named , by Fortune Magazine.
,
,.
,
Here is second part of my interview with him:
,
,
,
,: H2O is one of the fastest growing machine learning and data science communities with over 10,000 new installations last year. We sold out our first Community Conference, ,, ,. There were some fun ones we didn?€?t even know about. Being open source means that we typically hear from customers around the time they go into production.

,

For example, Cisco built a , using H2O. Paypal uses H2O for their , initiatives and H2O Deep Learning for ,. Ebay ,. ShareThis uses H2O for , to predict performance indicators such as CPA, CTR and RPM. MarketShare uses H2O to , for their customers. Vendavo is using it to build , and Trulia for ,. Some retailers and insurance companies are using it to do nationwide modeling and prediction of demand to manage just-in-time inventories and recommendations.

,

H2O Deep Learning is also being used for ,, , (following a recent ,), , based on the local weather history, and many more use cases in healthcare, financial markets (time series) and insurance verticals.

,

H2O really makes it easy to get great results with minimal effort due to its user-friendly web interface, it?€?s high performance and scalability, and the built-in automation and model tuning options. For example, we currently share the , with 0.83% test set error (for models without distortions, convolutions, or unsupervised learning) obtained with a ,. H2O also allows you to get fancy, and , that beat the various existing benchmarks. We recently hosted some of the , who shared some of their secrets on competitive data science.

,
,
,

,

,
,You can literally get started in less than a minute. For R-users, H2O is delivered as a simple R package in CRAN. Or you can , and start it with the ?€?java -jar h2o.jar ?€? command and point your web browser to ,. Simply run that same command on different machines simultaneously for multi-node operation (or specify a flat file with IP addresses and ports). A similarly simple single command line is used for launching H2O as a Hadoop job. The , has links to many resources, and I'd recommend ,, the , and ,, as well as the , (there?€?s one on ,). We also have great , of past events and ,. I would also encourage the community to ,. We typically have at least one meetup per week, and our brand-new office in Mountain View has ample space for meetups!

,

We also created an , to answer community questions. Our , is public and you can send questions, requests or comments to customer support(support@0xdata.com). Lastly, you can follow , or , on Twitter.

,

,

,

,The overall system scalability and power usage are key metrics for HPC systems. A decade ago, the , were basically tens of thousands of today's iPads clustered together via custom networks. Today, the fastest systems have the equivalent of 100,000 powerful workstations with top-notch GPUs connected through even faster interconnects.

,

The HPC community has had some time to figure out how to take advantage of massively distributed systems, but now it needs to adapt to new programming models that blend CPUs and GPUs (or accelerators) together. Hiding some of the system complexities such as complicated memory hierarchies or fault tolerance issues from the average application programmer will help this transition.
,

The key to high performance is to reduce data movement of any sort as much as possible: Hard drives, networks and even main memory kept speeding up at a much slower rate than the number-crunching processing units. Also, latencies have consistently lagged bandwidths across the board.,
,
Luckily, there have been some , towards reducing communication overhead for some standard linear algebra problems recently.

,

The next big frontier in HPC is called ,: 1 exaFLOPS (one trillion operations per ,second) - roughly the equivalent of the raw processing power of a human brain. With today?€?s technology, it would require a dedicated power plant for its ~200 megawatt power usage. We expect that such a system can become cost-effective enough to operate by the end of this decade.

,
Note that we already store more than 10,000 , (10 billion terabytes, or more than 1 terabyte per human) on the internet ,, and we?€?re going to have even more stored data per available processing unit a decade from now. It will be interesting to see what the term , stands for then...

,

,
,
,
,  "
"
,
,
Visit us at 
, in New York City on Tuesday, February 3 from 6:30 - 8pm and meet the instructors, students and 
, of our 
,.
,
Enjoy pizza and drinks on us as Data Science instructors Irmak Sirer and Bo Peng walk you through a sampling of what students learn throughout their 12 weeks of project-based Data Science work. 
,
,??,
,
Leah Nicolai, Metis Program Manager, and Lillian Landrum, Metis Talent Placement Manager, will be available to speak with you about the Bootcamp and career support experience. Metis co-founder Jason Moss will also be on hand to take your questions.
,
Plan on learning about the Bootcamp curriculum and outcomes and how it culminates with the Metis Career Day when hiring partners come to Metis for student portfolio reviews. 
,
,.  "
"
,
,
As a Big Data recruiter, the speed of change in the industry never ceases to amaze me. I can?€?t fail to get up in the morning and wonder what the next year (or day) might bring in this ever changing industry, and although I have made eight suggestions for 2015 trends, there seems to be one general theme: Big Data is going to become more accessible?€?.
, 
,
Big data talent with a Ph.D. or even a computer science degree is scarce and given the rapid growth of the industry, professionals from other backgrounds will be joining the teams. Increasingly automated platforms will simplify the collection and analysis of data, and the reliance on coding will be reduced, so the industry will be open to a wider skillset. A ?€?Data Scientist?€? with a BA degree? Not out of the question.
, 
,
In-memory analytics capabilities will provide immediate information to the business community. This will gradually replace batch reporting and for a fast moving business such as online retail, this analysis will enable swifter decision-making. In this example, demand planning and supply chain efficiencies will see huge improvements. Most businesses will benefit from this real time reporting.
, 
,
As the analytics software becomes easier to use, and companies educate their employees on some basic techniques, 2015 will be the year of accessible Big Data. It will no longer be some mysterious activity spitting out conclusions of questionable origin. People will understand where the data has come from, they will understand how to manipulate it and what it means for their particular part of the business. If they have the right tools, anyone will be able to draw their own insights.
, 
,
Data Scientists will be integrated more and more with the wider company, and may even start to sit alongside colleagues in other departments rather than in a stand-alone Big Data team. Big Data should be relevant to the business, and 2015 will be the year that it starts to become a true part of the business.
, 
,
With Big Data being at the core of the IoT, 2015 will see connected objects ?€? think home appliances, thermostats, wearable fitness monitors, robots, 3D printers ?€? become more and more prevalent, and useful. On even the coldest of days you will return home to a warm, cozy house.
, 
,
,
,
With improvements in analysis and software, also comes the inevitable consequence that the Data will start to mean more and more to business leaders. It will no longer just be impressive for its quantity, but 2015 is the year that the quality will dramatically increase, and therefore the impact on business. Rich data is more effective than big data.
, 
,
Big Data will be embraced by everyone in the business. People will start to understand it better, and management will be able to tap into its power. The speed of Big Data adoption will depend on layman advocates to translate it into tangible business results. Success will then breed success.
, 
,
Recruitment for Big Data professionals will be as competitive as ever. Companies will need to be clear about their requirements, engaging persuasively with the best talent, who will have their pick of employers. This is also the year of the ?€?open mind?€? in Big Data recruitment. There is not enough talent to go around, so HR will have to trust recruiters to partner with hiring managers to creatively find the best-fit solutions.
, 
Whatever Big Data brings this year will depend on you. Yes, you. Thanks for reading the article, thanks for showing an interest. Maybe it is worth understanding what Big Data can deliver for your company?
, 
 
,Bio: 
, is the Founder and Director at Big Cloud. Big Cloud is a talent search firm focusing on all things Big Data and helps innovative organizations across Europe, APAC and the US find the talent they need to grow.
,
,
,
,
,  "
"
Most popular 
, tweets for Jan 19-20 were
,
15 #programming languages you need to know in 2015 - #Java #PHP #C++ #Python #SQL #R 
, #rstats 
,
,
,
R Programming fun: writing a Twitter bot #rstats 
, 
,
,
,
With general #AI the most interesting ideas (recursive self-improvement) are also the most dangerous , 
,
,
Simple Pictures that State-of-the-Art #AI Can't Recognize (yet) #Vision #DeepLearning 
, 
,
,
,
,  "
"
        ,  "
"
,
,Austin, TX
,Apr 10-12, 2015
,
Global Big Data Conference is offering 3 day extensive bootcamp on Big Data on April 10-12 2015 at Austin Convention Center, TX, USA. This is a fast paced, vendor agnostic, technical overview of the Big Data landscape. No prior knowledge of databases or programming is assumed. Big Data Bootcamp is targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, Security, Operations, Spark, NoSQL (Cassandra, MongoDB, Neo4j), Data Science, R,  Python & Machine Learning.  Attendees will experience real Hadoop clusters and the latest Hadoop distributions.
,
,
,
,
,
,??,
,
,Engineers, Developers, Architects, Networking specialists, Managers, Executives, Students, Professional Services, Architects, Data Analyst, BI Developer/Architect, QA, Performance Engineers, Data Warehouse Professional, Sales, Pre Sales, Technical Marketing, PM, Teaching Staff,  Delivery Manager
,
Please register ASAP to take advantage of discount. 
Seats are limited (up to $200 discount )
,
Use Promotional code 
,
and register
,
Register : ,
,
Agenda: ,
,
Speakers: ,  "
"
,
,
Officially the 'hottest job of 2015,' data scientists are now in high demand. In 2014, major tech organizations filled this new position, but this year, we can expect the success of this role to filter down across all industries. 
,
It has also been predicted that this year Hadoop will transform organizations, moving beyond data storage, to providing valuable data insight. Hadoop technologies will also become more accessible, as cloud services have reduced cost barriers to entry.
,
On February 12-13, the leading Data Scientists, Big Data & Hadoop professionals will gather in San Diego to address the changes and challenges expected in the coming year. As partners of this event, all members can take advantage of an exclusive 20% off all two-day passes with the discount code ,.
,
The ,
will inspire data leaders to get the most out of Hadoop and associated platforms and use the technology to drive business applications. Join 25+ speakers from 
, & more ...
,
Check out the Hadoop Innovation Summit agenda here: 
,
,
The co-located ,
will bring together speakers from companies that
are leveraging the value of data science, including 
, & more. The widespread success of this new field is increasing the need for data scientists across multiple industries and this summit is a unique opportunity to hear from the best of the best. 
,
View the Data Science Innovation Summit schedule here: 
,
,
If you would like to attend, or for more information please contact 
Bola Williams at 
,
(+1 415 692 5378). Alternatively you can secure a pass via either summit website, with passes available that will grant you access to both events.
,
We hope to see you in San Diego!
,
Innovation Enterprise  "
"
,
,
, is a Physicist & Hacker at ,. Prior to that, he was a founding Senior MTS at Skytree where he designed and implemented high-performance machine learning algorithms. He has over a decade of experience in high-performance computing and had access to the world?€?s largest supercomputers as a Staff Scientist at SLAC National Accelerator Laboratory where he participated in U.S. DOE scientific computing initiatives and collaborated with CERN. Arno has authored dozens of scientific papers and is a sought-after conference speaker.
,
He holds a PhD and Masters summa cum laude in Physics from ETH Zurich. Arno was named , by Fortune Magazine.
,
,.
,
,.
,
Here is third and last part of my interview with him:
,
,
,
,:??I?€?m very fortunate that my parents and the , in Switzerland were very supportive of my early efforts. I?€?ve always liked computers and the idea of using them to simulate the real world with scientific algorithms. I remember programming an asteroid collision video game in BASIC as a teenager. And I?€?ve also always liked physics, especially remote-controlled airplanes and other fun ?€?experiments?€?.

,

One day during my studies of physics at ,, I discovered ,a graduate course on scientific programming techniques using C++ and , and got immediately hooked. After a conference presentation of my subsequent , using ,, , offered me a position as a Staff Scientist, where I got deeper into HPC and had access to millions of CPU hours on some of the largest supercomputers for , funded by the ,.

,

After 6 years at SLAC, and armed with a Green Card, I decided to try my luck in the startup world. Machine Learning is a perfect fit for me, given the similarity to scientific computing and its critical importance for the world. I feel that there?€?s still , to do!

,

,

,

,Yes, I?€?m more of a hacker these days. But if studying physics teaches you one thing, it's logical thinking - and that's quite useful when you?€?re trying to teach a computer what to do. I?€?m glad to see that my former peers in , as I used to collaborate with CERN on the LHC project. I?€?d love to introduce H2O to more scientific communities.

,

,

,
,Yes, it?€?s definitely not easy to find talented people with relevant experience in supercomputing, applied math and computer science, all at once. And that?€?s really what Machine Learning on Big Data boils down to, not to mention new infrastructure technologies such as Hadoop or Spark.

,

It seems that companies like Google, Twitter or Facebook have already gobbled up a large fraction of this talent in Silicon Valley over the past decade, but I fully expect outstanding new students of Big Data Machine Learning to emerge from universities all over the world given the enormous amount of interest shown by young people in computers and data science.

,

,

,

,The best advice I ever got was to think about what I want to achieve, in 2 years, in 5 years and in 10 years from now. I realized that I had never thought that far out until then.

,

,

,

,Try to do something you really enjoy doing. Being in the , makes a huge difference in your happiness and productivity.
,
,

,

,

,I really liked ""Jitterbug Perfume"" by Tom Robbins, and I like to keep up-to-date by reading ?€?The Week?€?, but I love spending time with my , and our adorable baby boy (with whom I?€?d like to play golf someday).
,
,
,  "
"
,
,
 (part of the CLEF 2015 QA track to take place in Toulouse, France, 8-11 September, 2015)
,
, ,
,
, ,
,
The BioASQ challenge consists of two different tasks (Task 3a and Task 3b).
,
If you are interested in any of the following areas: ,
,
,
then you may want to participate in BioASQ Task 3a (large-scale biomedical semantic indexing). ,
More information at ,.
,
If you are interested in any of the following areas:
,
 
,
then you may want to participate in BioASQ Task 3b (biomedical semantic QA). ,
More information at  ,.
,
Important dates:,
,

Detailed schedule at ,
,
Training data for both tasks available at ,

,
,
,  "
"
,
,.
,
,In early 2014, Mark Gibbs authored several fascinating blog posts around the topic of 
,. 
Dark Data as one may recall, is the ever present, relatively unknown, and certainly unmanaged volumes of data that exist in every corner of one?€?s business. This data may lie in spreadsheets and files, but is often a crucial component of running a business as it represents highly localized and valuable knowledge managed directly by users. Outside of the fears of possibly losing such crucial intellectual property, just imagine the potential if a business was able to catalogue and distribute such knowledge to others in the firm.
,
Gibbs outlined the many barriers to harnessing such content ?€? not the least of which involved the protectionism of its local owners, who fear disruption to the often elaborate stitching together of corporate and local assets to solve their unique needs.If corporate IT or any other organization wants to access, catalogue, and potentially re-distribute such intelligence, they will have to ensure at least three conditions are met: 
,
(1) any attempts to secure, catalogue, extract, or re-distribute information will not disrupt current usage patterns on the part of current owners; ,
(2) there will be no or very few new constraints placed on current owners who wish to continue to evolve their creation in pursuit of higher levels of value; and ,
(3) there will not be excess administrative burdens placed on local owners requiring them to better ?€?document?€? their creations.,

This is a tall order - gaining access, cataloguing, and extracting critical elements of an existing ?€?information infrastructure?€? without causing disruption or new barriers to the organic evolution of new forms of value. To fully capitalize on this opportunity, organizations are advised to evaluate a new breed of technology which non-invasively connects and extracts targeted elements of an information infrastructure. 
,
Such technology must thrive (read ?€?be highly accessible?€?) in the hands in both subject matter experts who solve real problems and business analysts who understand information management and systems. Their collective ability to access and use these pockets of dark, yet valuable, information and knowledge will directly drive reuse and creation of new composite value streams. In the best case, these users will be able to rapidly experiment with these new pockets of insight by seamlessly integrating not only data but all other assets (applications, data services, websites, etc.) within their ?€?digital?€? reach within a single interoperable fabric. This fabric promotes short cycle trial and error, low cost of success (and failure), yet retains the high degree of agility needed to smoothly swap in or out these newly discovered pockets of value.
,
At the end of the day, firms who create a lasting ability to rapidly innovate and bring new  ideas to market that capitalize on their collective strengths are those who win. ?€?Dark Data?€? represents far too much potential value to be left in the shadows, and firms with a strategy to full leverage these assets will be well positioned to compete.
,
Bio: , is the Chief Technology Officer of Pneuron Corporation. In this role, Fountain provides leadership of marketing, pre-sales and product management with focus on product strategy and positioning in the Enterprise market and working closely with customers to develop innovative solutions to their most pressing and complex business needs.
,
,
,
,
,
 ,  "
"
Most popular 
, tweets for Jan 21-22 were
,

Cepheus #AI program can beat almost any human in #poker, can even bluff #MachineLearning 
, 
,
,
,
,
,
Palantir #datamining used by NYC to crack down on AirBnB hosts, , looks at their VCs 
, 
,
,
,
Cepheus #AI program can beat almost any human in #poker, can even bluff #MachineLearning 
, 
,
,
Palantir #datamining used by NYC to crack down on AirBnB hosts, , looks at their VCs 
, 
,
,
,  "
"
,
,
, works on the , Big Data Solution Marketing team. His interest is on the benefits of Big Data in general with a focus on Hadoop in the SAP Big Data ecosystem along with the SAP HANA platform for Big Data. A graduate of McMaster University, he holds an MBA from the University of Windsor. He has worked in product marketing and product management in the high tech arena for a number of years, taught at a private college and has co-authored a number of published text books. He has a true love of technology and all that it has to offer the world.
,
Here is my interview with him:
,
,
,
,: Every CEO wants a unique Big Data ?€?model?€? to differentiate their company. Business managers and stakeholders are seeing the ,success Internet companies are having mining data, but they are struggling to translate that to their specific industry. ,
,
Big Data is an opportunity to re-imagine our world, to track new signals that were once impossible, to change the way we experience our communities, our places of work and our personal lives. However, its potential would be missed if we started by discussing data management technologies or data mining algorithms and not business needs and value. The most successful Big Data projects are collaborations between business and IT, but initiating those conversations can prove difficult. To that end, SAP offers Big Data Workshops focused on Big Data use cases specific to each customer?€?s business and industry.
,
,
,
,I feel that people are their own biggest obstacles. I believe that they see or expect implementing or bringing Big Data projects to life to be difficult and complex. And with this perspective either shy away or look for and create very complicated ways of consumerizing Big Data. People really need to see that they can just keep it simple. 
,
This is compounded by a few other factors as well. Often businesses do not really know how Big Data can benefit them so they really do not know what or how to consumerize it as a result. IT feels Big Data is a data management and data warehousing problem and the business managers?€? priority is to tie Big Data insights directly into their business processes, equip frontline staff, and act on insights in day-to-day operations. There is a need to involve both IT and business users in the discussion of how to lever and consumerize Big Data. It is often difficult to bring the two together to bridge that gap. And there is the issue of skills and abilities to implement Big Data solutions from Data Scientist on down. 
,
,
, ?? ,
Big Data is not and does not have to be complex and SAP can simplify things. Simplicity can be achieved through the capabilities of SAP HANA platform and other related technologies to unify and simplify data landscapes. And of course there is the SAP s-innovation initiative to drive simplicity across business to help drive instant value with simplicity removing barriers presented by lack of skills and custom built solutions. Things like SAP Sales Insight for Retail that lets business users get deep insights from Point of Sale data provide simple ways of consumerizing Big Data. SAP offers a complete solution set from acquisition, to storage to analysis and visualization to mobility to keep the consumerization simple.
,
,
,
,I believe there is talent shortage related to Big Data. From Data Science to those who really understand the various related technologies. This comes from not only what analysts are and have been saying, but also from customer's need and pain. 
,
There is and continues to be a skills shortage that impacts businesses trying to effectively leverage Big Data. This is why companies are often confused in how or if they can derive value from Big Data. That skills shortage will likely drive adoption of cloud where in-house skills are less key. 
,
This skills shortage is also one of the reasons that SAP has embarked on its s-innovation strategy and provides solutions like SAP Sales Insight for Retail. To help companies and business users without the need for extensive in-house skill sets.
,
,
,
,Start with a business focus and not a tactical IT infrastructure discussion. Identify what top business priorities can benefit from Big Data in the organization. ,Understand how they will engage people and processes to drive desired results. Once this is in place then look to see what technological platform innovation is needed to support the solution and what plan is needed to execute the project. This is SAP?€?s approach and through its Data Science team and Big Data workshops it is able to help companies do just this. Start with the business need and value and work toward the right technological solution from there.
,
,
,
,
,  "
"
        ,
,
,
,
,  "
"
By Gregory Piatetsky,  
,, Jan 24, 2015.
,
I was both surprised and not surprised to learn that 
,.  
,
Smaller stand-alone analytics vendors find it increasingly hard to compete,
and several have been acquired 
(,, 
,, ...), so Revolution Analytics acquisition is not a surprise. 
,
I was surprised that Microsoft was the buyer, since Revolution Analytics is an open-source company.  
,
However, David Smith (Revolution Analytics Chief Community Officer) explained in his 
,
,
,
Also, Microsoft has become a big user of R - for its Xbox gaming and 
especially 
Microsoft Azure Machine Learning platform. 
So perhaps this acquisition does make a lot of sense.
,
Congratulations to Revolution Analytics team which will 
continue to support and develop the Revolution R family of products - including non-Windows platforms like Mac and Linux. The free Revolution R Open project will continue to enhance open source R. 
,
There are only a few standalone analytics vendors left, so watch out for more acquisitions.   "

"
,
,
,
,
Document classification is an example of Machine Learning (ML) in the form of Natural Language Processing (NLP). By classifying text, we are aiming to assign one or more classes or categories to a document, making it easier to manage and sort. This is especially useful for publishers, news sites, blogs or anyone who deals with a lot of content.
,
Broadly speaking, there are two classes of ML techniques: supervised and unsupervised. In supervised methods, a model is created based on a training set. Categories are predefined and documents within the training dataset are manually tagged with one or more category labels. A classifier is then trained on the dataset which means it can predict a new document?€?s category from then on.
,
Depending on the classification algorithm or strategy used, the classifier might also provide a confidence measure to indicate how confident it is that the classification label is correct.
,
,
,
,
We can use the words within a document as ?€?features?€? to help us predict the classification of a document. For example, we could have three very short, trivial documents in our training set as shown below:
,
, 
To classify these documents, we would start by taking all of the words in the three documents in our training set and creating a table or vector from these words.
,
 (some,tigers,live,in,the,zoo,green,is,a,color,go,to,new,york,city) class 
,
Then for each of the training documents, we would create a vector by assigning a 1 if the word exists in the training document and a 0 if it doesn?€?t, tagging the document with the appropriate class as follows:
,

When a new untagged document arrives for classification and it contains the words ?€?Orange is a color?€? we would create a word vector for it by marking the words which exist in our classification vector.
,

If we compare this vector for the document of unknown class, to the vectors representing our three document classes, we can see that it most closely resembles the vector for class 2 documents, as shown below:
,
It is then possible to label the new document as a class 2 document with an adequate degree of confidence. This is a very simple but common example of a statistical Natural Language Processing method.
,

,
,
A real world classifier has three components to it, which we can look at in a bit more detail.
,
,
,
The quality of the tagged dataset is by far the most important component of a statistical NLP classifier. The dataset needs to be large enough to have an adequate number of documents in each class. For 500 possible document categories, you may require 100 documents per category so a total of 50,000 documents may be required.
,
The dataset also needs to be of a high enough quality in terms of how distinct the documents in the different categories are from each other to allow clear delineation between the categories.
,
,
In our simple examples, we have given equal importance to each and every word when creating document vectors. We could do some preprocessing and decide to give different weighting to words based on their importance to the document in question. A common methodology used to do this is TF-IDF (term frequency - inverse document frequency). The TF-IDF weighting for a word increases with the number of times the word appears in the document but decreases based on how frequently the word appears in the entire document set. 
,
,
,
In our example above, we classified the document by comparing the number of matching terms in the document vectors. In the real world numerous more complex algorithms exist for classification such as Support Vector Machines (SVMs), Naive Bayes and Decision Trees.
,
Additionally, we placed our document into just one category type but it?€?s possible to assign multiple category types and even multiple labels within a given category type. For example, using IPTC International Subject News Codes to assign labels, we may give a document, two labels simultaneously such as ?€?sports event - World Cup?€? and ?€?sport - soccer?€?. Where ?€?sports event?€? and ?€?sport?€? are the root category with ?€?World Cup?€? and ?€?soccer?€? being the child categories.
,
We may also have a hierarchical structure in our taxonomy and require the classifier to take that into account.
,
,
In supervised methods of document classification, a classifier is trained on a manually tagged dataset of documents. The classifier can then predict any new document?€?s category and can also provide a confidence indicator. The biggest factor affecting the quality of these predictions is the quality of the training data set. 
,
Bio: , is CEO and Founder of AYLIEN, a startup that focuses on creating simple and intelligent applications in the news and media space. AYLIEN Text Analysis API is designed to help developers, data scientists, business people and academics extract meaning from text. You can try out the API by signing up for an 
, or visiting  
,.
,
,
,
  "

















"
,
,
, works on the , Big Data Solution Marketing team. His interest is on the benefits of Big Data in general with a focus on Hadoop in the SAP Big Data ecosystem along with the SAP HANA platform for Big Data. A graduate of McMaster University, he holds an MBA from the University of Windsor. He has worked in product marketing and product management in the high tech arena for a number of years, taught at a private college and has co-authored a number of published text books. He has a true love of technology and all that it has to offer the world.
,
,.
,
Here is second part of my interview with him:
,
,
,
,: Simplicity. Starting with business needs/benefits discussion and not a tactical IT infrastructure discussion. Helping businesses get up to speed quickly and simply ,via the s-innovation and offerings like SAP Sales Insight for Retail. A focus on simplifying IT so business can define an improved strategy possible with a modern IT infrastructure where complexities are streamlined for real-time responsiveness so timely access to information can bring intelligence to users allowing informed action and unleashing innovation to transform the way business creates, performs, and connects. 
,
SAP HANA allows business to modernize IT with a streamlined real-time data platform ideally suited for Big Data and interaction with Hadoop. This along with SAP HANA analytics and predictive capabilities and leading BI tools and visualization tools like Lumira, lets business harness the full value of all their data to get meaningful, actionable insights to empower people with timely context-rich information anywhere, anytime, on any device. SAP?€?s cloud offering allows companies to tap into the rapid innovation that the Cloud enables, letting them transform and adapt quickly to ever changing market dynamics. 
,
From a technology point of view it?€?s the SAP HANA platform, s-innovations and cloud offerings all driving simplicity, our BI and visualization tools that bring insight in easy to digest actionable format to end users, and of course our ability to integrate with Hadoop and act as a single point of access to diverse data sources.
,
,
,
,: The SAP HANA platform with the speed and simplification it offers is our key differentiator. There is the SAP s-innovation initiative that brings simplicity and redefines how business software delivers value in a digital and networked world. Of course the SAP cloud offerings. And there are the relationships with, and open agnostic approach SAP has with Hadoop and NoSQL; SAP operates with the major Hadoop distribution. 
,
SAP has over four decades of experience over which it has gained breadth and depth of knowledge in 25 industries and 11 lines of business. This gives SAP the true ability to act as a trusted advisor to help guide and support organizations as they try to lever Big Data.  And of course there is the core team of SAP Data Scientists along with their industry expertise that can be offered to help businesses get up to speed.
,
,
,
,: Don?€?t rest on your laurels. Something a mentor said to me when I was much younger. Basically never feel that you are done. Always look for and keep setting new goals ?€? don?€?t become complacent. You may have achieved a lot, but your are only as good as your last effort. 
,
,
,
From my point of view, the crux of Big Data is Data Science. Simply having volumes of data does not mean insight or value from Big Data. In order to get business value from Big Data you need to be able to understand and analyze what you have. This is where the right people and the right tools come in. 
,
,Data Scientists can get an understanding of the business need and then work to understand the data with the proper algorithms, and tools to provide what is needed by the business user to get value from Big Data. ,
,
SAP has a team of PhD scientists and technology experts with global, industry-specific experience (e.g. Manufacturing, CPG, Retail, Energy, Transportation, Sports, etc.) who can leverage hundreds of use cases and differentiating intellectual property to help businesses achieve ROI quickly. 
,
,
,
,: Away from work ?€? who gets away from work?  But seriously, my family. I have 2 young grandchildren I am very lucky to live close to and thus am able to see and play with a good deal. And my wife and I love to travel. That and a couple of iPad app games.
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "

"
,
,Doha, Qatar, March 9-10, 2015,
,
,
Submission deadline: February 7, 2015
,(Position papers and extended abstracts)
,
,
,
MLDAS is a forum for machine learning and data analytics researchers and
practitioners from academia, industry, and government to share their
ideas, research results and experiences.
,
The symposium, organized by the Qatar Computing Research Institute
(QCRI) and Boeing, will include invited and contributed technical
presentations.  The contributed submissions are due by February 7, 2015.
See the website for submission details.
,
Limited financial support for travel is available for authors of
contributed papers (please specify if you need financial assistance in
the cover email).
,
INVITED SPEAKERS (More TBA)
,
,??,
,
,
,??,
,
,
,??,
,  "
"
        ,  "
"
,
,
This week on /r/MachineLearning, there are some great resources on NNs and genetic algorithms, plus a bit of humor.
,
,
,
This bit of humor points out the Hitchhiker?€?s Guide to the Galaxy reference in Max Kuhn?€?s ?€?Applied Predictive Modeling?€?.
,
,
,
This thoughtful article goes in depth about some of the assumptions that go into the k-means model. It?€?s a good read for those who aren?€?t sure what makes k-means feasible or infeasible. A good related read is the comments section of ,.
,
,
,
,
,
This deep article goes far into the topic of RNNs. It includes many helpful diagrams and examples to make a very technical topic easily digestible.
,
,
,
This Stack Exchange post contains genetic algorithm solutions to a coding challenge to make specimens learn how to get through a maze. It goes through how to address the challenge step-by-step in detail, providing some insight in how to implement genetic algorithm solutions. If you?€?re the sort that likes to have the code, the solutions are implemented in multiple languages at the end of the post.
,
,
,
This article is a bit different from most of what?€?s on /r/MachineLearning. Instead of discussing an algorithm or model, it instead addresses how to implement machine learning systems. This should be of interest to those actively implementing these types of systems in industry.
,
,
,
,  "

"
Most popular 
, tweets for Jan 19-25 were
,
#Facebook open sources its cutting-edge #DeepLearning tools , #MachineLearning #BigData #Torch 
,
,
,
Chart Type #CheatSheet: The Guide to Chart Design and Data #Visualization #dataviz , 
,
,
Optimizing optimization algorithms: MIT researchers show how get best simplified approximation , 
,
,
,
Very useful: Intro to #Python and #IPython for #DataMining , 
,
,
,
,  "
"
,
,

,

,

,
,

,

,
,

,

,

,

,

,

,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,

,



  "
"
By Gregory Piatetsky,  
,, Jan 27, 2015.
,
Update: As a media partner for Strata 2015 Conferences, KDnuggets has already given away to 
one of our subscribers a 
,
,
,, Making Data Work
,Feb 17-20, 2015. San Jose, CA, USA
,Here is a conference description from O'Reilly:
,
,
,
You can still get a 
,.
,
See below reports on trends and previous Strata Conferences.
,
,
,  "
"
,
,
, has been working with Big Data for over a decade now. He is passionate about databases and distributed systems. At Yahoo, he is currently building data applications that power digital advertising. He is also focused on advanced analytics that aim to improve user understanding at Yahoo. As a senior leader of Yahoo?€?s well regarded data team, he has built key pieces of Yahoo?€?s data processing platforms and tools through their several iterations. These include data repositories, data pipelines and reporting systems. In the past, he has contributed to open source projects, including Shark (part of the Apache Spark effort).
,
Nandu holds a Bachelor?€?s degree in Electronics Engineering from Bangalore University and a Master?€?s degree in Computer Science from Stanford University.
,
,
,
Here is second and last part of my interview with him:
,

,
,
,:  Yahoo is actively involved in several projects within the community, and continues to contribute to core, open-source projects like ,Hadoop, YARN, PIG, Hive, Oozie, etc. 
,
We are also making significant contributions to newer technologies like Druid and Spark. You should see interesting work from Yahoo added to the public domain in these areas, soon.
,
Beyond source code, the other major way in which several web companies, like Yahoo, contribute, is in openly discussing our often pioneering Big Data applications. These case studies often influence and grow the community more than code contributions can.
,
We constantly evaluate new technologies, and, once we adopt them, we strongly believe in giving back to these open-source technologies. You always see a steady stream of useful contributions from Yahoo.

,
,
,
,: ,
,
Big Data technologies skew heavily towards open-source. Vendors must offer solutions that are both better, and can evolve faster than open-source alternatives.
,
At this point in time, a focus on stability and ease of use might help their implementations more than adding features and competing directly with open-source choices.
,
,
,
,: I predict that Big Data will have a new name within the next five years. Our tool-chain will look completely different in the coming years., ,
,
,
,
,: This advice is targeted at individuals working towards building technical careers in Data Science. ,Become an expert at the fundamentals first. The theory of Databases, Distributed Systems, Machine Learning and Visualization are the basics that guide all our work on Big Data. 
,
The next step would be to jump in and participate directly in as many actual Big Data implementations as possible. The greater the variety the better. Finally, it is important to keep up with the latest work going on in this young, and quickly evolving field. New books, videos and conferences are key resources.
,
,
,
,: I wish this would happen more often to me! When it does, I tend to do one of two things. The first is to keep up with the latest going-ons, whether it is new projects at work or interesting developments in the industry or academia. Otherwise, I find myself trying to learn something new.
,
,
,
,: I haven?€?t finished reading the entire book, but I really like and am learning a lot from , by Stephen Few.

,
,
,  "
"
,
,
, is a principal data scientist at ,, where he designs machine learning and analytic solutions to solve problems for Ayasdi customers. Prior to Ayasdi he was a postdoc with Ayasdi co-founder Gunnar Carlsson in the Stanford University Mathematics Department. He's held academic positions at the Max-Planck Institute for Mathematics, Mount Holyoke College and the American Institute of Mathematics.  ,
His PhD is from the University of Pennsylvania on the connections between algebraic geometry and string theory. Along the way he co-founded a data analytics company working on political campaigns, worked on quantum circuitry research, and studied chaotic phenomena in sand boxes.  His friends say that his best idea was to found a College funded cooking club in order to eat food he couldn't afford otherwise.
,
Here is my interview with him:
,
,
,
,Topology is the study and description of shape. In Big Data problems, shape arises because you have a notion of similarity or distance between data points. This can be something like Euclidean distance, correlations, a weighted graph distance or even something more esoteric.  Shape is exploited in machine learning by bringing in some additional information such as ""my data has well defined clusters or classes""; ""this outcome is linear ""; or ""my signal is periodic"".  Then you would use specialized tools to apply models based on this information.
,
,
,
This seems like a trivial point, but can be key to solving complex problems with a high degree of accuracy.  A simple example of this comes from hospital predictive models. Hospitals want to measure how sick people are and collect a variety of clinical information (blood pressure, heart rate, temperature, breathing rate, oxygen levels etc.) or genetic information (gene expression levels). Typically, they fit a linear regression model that predicts how ""sick"" patients are.  The underlying assumption is that there is a near linear relationship between symptoms and ""sickness"". 
,
One of Ayasdi's academic collaborators took gene expression data for people at different stages of malaria.  ,When examined using TDA, he found patients all lying on a circle sitting inside of a high dimensional space (~1000 features).  While in retrospect the circle is obvious, your path from being healthy to sickness and back to healthy does not track up and down through the same set of symptoms, and yet nobody had thought to look for the circle.
,
Most real world data sets that I look at are larger and more complicated than this example and we find a variety of structures ?€? cluster, flares, loops and higher dimensional structures ?€? all appearing in a single data set. It is nearly impossible to guess or hypothesize the right structures ahead of time, and TDA is a tool to understand your data in an unbiased way, revealing its true complexity.
,
,

,Complexity means that it's too hard to hypothesize what the relationships and structures in your data are. Topological Summaries provide a way to understand and then exploit structure without having to first guess.
 ,
,
,
,There are lots of ways to answer this question but I'll just focus on one unique benefit: coordinate invariance.  The things that we care about do not depend on the coordinate system chosen to describe the problem.  For example, the boiling point of water does not depend on whether I describe it in Celsius or Fahrenheit.  In a similar way, your location can be described by an address, or with lat/long coordinates.  The essentials of where you are and your relationship to the rest of the planet do not depend on these coordinate choices.
,
And yet, for most statistical techniques the details of the coordinate systems matter. Even a rigid rotation of your data in some high dimensional Euclidean space can confound a statistical model.
,
TDA is more robust way to handle these kinds of choices because of its foundation in Topology.  Concretely, this means that there are more ways to get to the same answer, and your feature selection/engineering is less important than when using other methods.
,
,
,
,
,  "
"
,
,
, has been working with Big Data for over a decade now. He is passionate about databases and distributed systems. At Yahoo, he is currently building data applications that power digital advertising. He is also focused on advanced analytics that aim to improve user understanding at Yahoo. As a senior leader of Yahoo?€?s well regarded data team, he has built key pieces of Yahoo?€?s data processing platforms and tools through their several iterations. These include data repositories, data pipelines and reporting systems. In the past, he has contributed to open source projects, including Shark (part of the Apache Spark effort).
,
Nandu holds a Bachelor?€?s degree in Electronics Engineering from Bangalore University and a Master?€?s degree in Computer Science from Stanford University.
,
Here is my interview with him:
,

,
,
,: Yahoo is focused on making the world's daily habits inspiring and entertaining. This focus on the experience users have on our apps and websites drives us toward creating highly personalized and optimized experiences. Big Data is critical to implementing this for our hundreds of millions of users.
,
Yahoo?€?s digital advertising business, much like ad tech in general, is very data intensive. Big Data technologies allow us to serve billions of relevant, targeted ads every day.
,
,
,
,: Over the last decade, the industry?€?s ability to work with large amounts of data has advanced to the point where solutions like Hadoop and other cloud-based technologies have become commonplace. Using the right tools has allowed us to overcome the challenge that large scale imposes.
,
,
,
The tool ecosystem is not as broad or mature when operating out of the realm of RDBMS implementations. The equivalents of commercial ETL or Modeling tools do not compare well. Similarly the ability to interface BI or statistical implementations with good visualization is not as mature when we operate with large scale data.
,
Another problem in the Big Data world is the proliferation of open source choices that solve very similar problems. This healthy, evolving ecosystem can be an advantage; the challenge is simply to choose the right tool for the job at hand.
,
,
,

,: Trends include: 
,
, ?? ,
New NoSQL stores are showing up all the time, each targeting some specific subset of the data storage/query problem. Older RDBMS based tools are now trying to work well with Hadoop. Reporting solutions that are backed directly by data stored in Hadoop instead of an intermediate RDBMS are another trend.
,
,
,
,: As far as Yahoo is concerned, core Spark offers us a user-friendly, high-level language, and data model and, indeed, a clearly defined way of thinking about data manipulation.
,
Its performance, and potential for significant improvement, has also been a key reason for our adoption of Spark. We are excited about the maturing of the rest of the projects in the Spark ecosystem, and are eagerly awaiting better stability and reliability.
,
Shark brought SQL to Spark. Scala is great and Spark code is elegant, but SQL is accessible to many more programmers, and often results in clearer code. A SQL implementation also allows for ODBC/JDBC, and, hence, access to a large ecosystem of tooling built around those standards.
,
,
,
Most of the code associated with Shark is now part of Spark itself, and is available as Spark SQL. One of the original benefits of Shark was tight integration with the de facto standard for SQL on Hadoop - Hive. Spark SQL will soon match that integration, and go beyond with fantastic integration of SQL relations within the Spark data model. Spark code can now incorporate SQL, where appropriate, seamlessly.
,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jan 27 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
, 
, 
, , , , 
, 
, , 

, 

, , , , , 
, 

, 
, 
, , 
, 
, , , 
, 
, , , 
, 
, 
, 
, , , , , 
, 
,
,
,
 ,  "
"
By Erich Schubert.
,
The following timelines was generated using the event and trend detection tool 
, (published at , and covered 
,) on news articles collected for the year 2014.
,
The category of financial news was removed (which is otherwise overrepresented in the data source) and instead of the raw keywords and headlines, we manually described the trends detected.
These are the top 50 trends, with the top 10 trends detected highlighted in bold, everything is ordered chronologically.
,
,
2014-01-29: Obama's state of the union address
,
,
2014-02-07: Sochi Olympics gay rights protests,
2014-02-08: Sochi Olympics first results,
2014-02-19: Violence in Ukraine and Maidan in Kiev,
2014-02-20: Wall street reaction to Facebook buying WhatsApp,
2014-02-22: Yanukovich leaves Kiev,
2014-02-28: Crimea crisis begins
,

,
2014-03-01: Crimea crisis escalates further,
2014-03-02: NATO meeting on Crimea crisis,
2014-03-04: Obama presents U.S. fiscal budget 2015 plan,
,
2014-03-08: MH-370: many Chinese on board of missing airplane,
2014-03-15: Crimean status referendum (upcoming),
2014-03-18: Crimea now considered part of Russia by Putin,
2014-03-21: Russian stocks fall after U.S. sanctions.
,

,
2014-04-02: Chile quake and tsunami warning,
2014-04-09: False positive? experience + views,
2014-04-13: Pro-Russian rebels in Ukraine's Sloviansk,
,
2014-04-22: French deficit reduction plan pressure,
,

,
2014-05-14: MERS reports in Florida, U.S.,
2014-05-23: Russia feels sanctions impact,
2014-05-25: EU elections,

,
2014-06-06: World cup coverage,
2014-06-13: Islamic state (ISIS) Camp Speicher massacre in Iraq,
2014-06-14: Soccer world cup: Spain surprisingly destroyed by Netherlands,

,
2014-07-05: Soccer world cup quarter finals,
,
,
2014-07-19: Independent crash site investigation demanded,
,

,
2014-08-07: Russia bans food imports from EU and U.S.,
2014-08-08: Obama orders targeted air strikes in Iraq,
2014-08-20: ISIS murders journalist James Foley, air strikes continue,
,

,
2014-09-05: NATO summit with respect to IS and Ukraine conflict,
2014-09-11: Scottish referendum upcoming - poll results are close,
2014-09-23: U.N. on legality of U.S. air strikes in Syria against ISIS,
2014-09-26: Star manager Bill Gross leaves Allianz/PIMCO for Janus,

,
,
2014-10-26: EU banking review,

,
,
2014-11-12: Foreign exchange manipulation investigation results,
2014-11-17: Japan recession,

,
2014-12-11: CIA prisoner and U.S. torture centers revealed,
2014-12-15: Sydney cafe hostage siege,
,
2014-12-18: Putin criticizes NATO, U.S., Kiev,
2014-12-28: AirAsia flight QZ-8501 missing
,
Similar to the , it mentions many key geo-political events of 2014.,
There probably is one ""false positive"" there: 2014-04-09 has a lot of articles talking about ""experience"" and ""views"", but not all refer to the same topic (we did not do topic modeling yet).
,
There are also some events missing that we would have liked to appear; many of these barely did not make it into the top 50, but do appear in the top 100, such as the Sony cyber-attack (#51) and the Ferguson riots on November 11 (#66).,

,
,
In this model, trends are considered significant, if the number of articles is 3 standard deviations higher than the expected value - a very classic definition from statistics. To make this work on streaming data, we used exponentially weighted averages and standard deviations. To reduce spurious trends (in particular first occurrences of terms) we added a simple bias term akin to Laplacian correction that removes such background noise. The main challenge is to scale this up to every term - and term combination - in the data set: Facebook is mentioned every day, but the combination of Facebook and WhatsApp was rarely occurring until they bought it. But also single terms can be useful to track, as seen in below chart: Ukraine trended most when the Malaysia Airlines plane was shot down in July 2014 (bottom chart), although it had more coverage in March 2014.
,
,

To make this approach scale up to monitoring every word and word pair mentioned over time, we employ a classic hashing/sketching trick. We accept heavy-hitters style inaccuracy in rare terms, but with a high probability we won't miss any frequently mentioned trend by using multiple hash functions: in order to miss a trend, it needs to collide with a more frequent term in every hash function.,
Using a fixed amount of memory for the hashtable (we used a 256 MB hash table) we can this way track trends without specifying keywords in advance, even on large data sets such as Twitter. Using our algorithm, data sets such as this much smaller news data set can be processed on a single Raspberry Pi (Model B, 512 MB).
,
,
,
Essential to understanding the results is visualization. Showing absolute numbers, and the significance as interpreted by the algorithm is helpful; but there may be many words and word pairs trending at the same time. To visualize this, we also created a semantic word cloud, where the words are not randomly placed (as common with word clouds) but reflect the association of words with each other. In the following image, you can see July 20, when two major clusters trend: in the Israel-Gaza conflict many people were killed (green cluster on the left) but also fighting in eastern Ukraine with pro-Russian rebels causes many fatalities. Links in this figure indicate terms that trend together, colors indicate a cluster structure obtained from this data.
,
,
You can also , in a snapshot (sorry, we are currently not crawling news sites in real-time).
,
,
,
Details on the approach are , in the KDD 2014 conference proceedings, and you can even , (presented by Michael Weiler).
,
,
Bio: , is a research and teaching assistant at the Ludwig-Maximilians-Universit??t M??nchen, Germany. He finished his PhD in 2013 on ""Generalized and Efficient Outlier Detection for Spatial, Temporal, and High-Dimensional Data Mining"" and is one of the lead authors of the open-source , data mining toolkit. He is expanding his research into text-mining and big data analysis, and interested in post-doc and assistant professor opportunities in his research areas.

,
,
,  "
"
Most popular 
, tweets for Jan 26-27 were
,
,  "
"
, is happening in just a few weeks in San Jose, February 17-20--and if you don't act fast, you may miss it. 
,
There's a lot new for 2015:
,
,??,
Over 5,500 people gathered at the sold-out Strata + Hadoop World in NYC last October. 
,, and has become the 
, - changing how business, government, and even society make better decisions.
,
,.
,KDnuggets subscribers save 20% off passes with code ,.  "
"
,
,
, is a principal data scientist at ,, where he designs machine learning and analytic solutions to solve problems for Ayasdi customers. Prior to Ayasdi he was a postdoc with Ayasdi co-founder Gunnar Carlsson in the Stanford University Mathematics Department. He's held academic positions at the Max-Planck Institute for Mathematics, Mount Holyoke College and the American Institute of Mathematics.  ,
His PhD is from the University of Pennsylvania on the connections between algebraic geometry and string theory. Along the way he co-founded a data analytics company working on political campaigns, worked on quantum circuitry research, and studied chaotic phenomena in sand boxes.  His friends say that his best idea was to found a College funded cooking club in order to eat food he couldn't afford otherwise.
,
,
,
Here is second part of my interview with him:
,
,
,
,One of the first examples for the power of TDA was the well known NKI data set. This data set was built over a number of years and represents gene expression profiles and clinical traits for 272 breast cancer patients collected by the Netherlands Cancer Institute (NKI).  The data set had been used by countless researchers to develop gene expression signatures to predict good vs. poor prognosis for patients with stage I or stage II breast cancer. This was believed to be one of the most well understood data sets in cancer.
, ?? ,
Using TDA researchers discovered a previously unidentified group of cancer survivors in a sub-population that had very high overall mortality rates.  More importantly, TDA was able to identify ""why?€? these patients were surviving, with broad implications to understanding the disease and for improving mortality rates. While this breakthrough discovery was notable in itself, the fact that TDA was able to make this discovery within hours, while researchers spent over a decade with more traditional tools, speaks to the ability of TDA to discover hidden patterns and subtle signals.
, ?? ,
We see similar results with our customers all the time.  For many of our data science problems we work with teams of data scientists and machine learning PhD's. They work on relatively few problems for an extended period of time (think for example of credit card transaction fraud at a major credit card issuer). In almost all cases we are able to give them a new insight into the structure of their data, as well as concrete model improvement. Sometimes they can reproduce our results using traditional methods, sometimes not. The key point is that TDA is the tool that told them where to go look.
,
,
,
,Lenses are the tool we use to produce topological summaries of shapes.  As a metaphor, consider a Shakespeare play. There are many different summaries one could produce ?€? a plot summary, summary of characters, locations in the play, language used, timeline etc.  While none completely captures the full story, by looking at multiple summaries together you gain a rich picture of the play. If I want to summarize another play there are certain summaries of plot, character etc. that are universally useful, while others, eg. names of fishing boats, are only useful for specific plays.
,
In a similar manner certain lenses are universally informative. The summaries produced by a density estimator, or measure of centrality of a data point are informative almost across the board, while others, such as the third moment of feature values for a data point, have more specialized application. That said, at Ayasdi we have a variety of ways to help end users choose good lenses, either through previous experiences with similar problems or through automation. In other words, we can tell you which lenses are producing interesting and useful results leaving it up to the data scientist to interpret and ""understand"" what is being said.
,
.,
,
,
,
,TDA is well suited for automation for a variety of reasons. The first is that beyond a notion of distance or similarity, no other assumptions are needed in order to use TDA, so there's a simple and uniform way to specify problems for the system. On the other end of the pipeline, independent of the way the feature generation/selection method, metric or lenses are used to create the summaries, the output is always the same data type: a network or simplicial complex. This means that you need a single tool set to examine and score the output from a wide variety of data analytics and machine learning methods.
,
,
,
,
,
,I think we're going to start seeing payoff for the storage and processing pipeline investments that companies have made. This means that advanced analytics and machine learning at scale will become ubiquitous across the economy and become ""table stakes"" in order to survive. The focus will shift from gathering, storing and processing data to using and understanding it in ever more complex ways throughout business.
,
The ability to handle complex data through automation and smart tools will be critical for the success of these efforts. The data sets are too large, there are too many problems and there are too few machine learning experts to make headway without both automation and advanced analytics.
,
,
,
,
,  "

"
,
,
Conference Website: ,
,
,
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (,) is a
premier conference that brings together researchers and practitioners
from data mining, knowledge discovery, data analytics, and big data.
KDD 2015 will be held in Sydney, Australia during August 10-13, 2015.
It will be the first Australian edition of KDD and the second time held
in the Asia Pacific region. Submissions for Research track and Practice
track are solicited for KDD 2015 on all aspects of knowledge discovery
and data mining.
,
,
,
,: KDD is a dual track conference hosting both Research and Industry & Government tracks. Due to the large number of submissions, papers submitted to the Research track will not be considered for publication in the Industry & Government track and vice-versa. Authors are encouraged to carefully read the track descriptions on the conference website and choose an appropriate track for their submissions.
,

,
,
The KDD 2015 organizing committee solicits proposals for full-day and half-day workshops to be held in conjunction with the main conference. The purpose of a workshop is to provide an opportunity for participants from academia, industry, government and other related parties to present and discuss novel ideas on current and emerging topics relevant to knowledge discovery and data mining. Workshops are (tentatively) scheduled for August 10, 2015.
, 
,
,
This year, all workshops will have , for their paper submissions and notifications. In addition, all deadlines are at 11:59 PM Pacific Standard Time.
,
,

,
,

  "

"
        ,  "
"
By Gregory Piatetsky,  
,, Jan 27, 2015.
,
,, 
a leading provider of cloud business intelligence software, today announced major upgrades to its GoodData(??) open analytics platform.
,
GoodData wants to let organizations to move beyond BI, a become the first ""Insights-as-a-Service"" provider in the market. 
,
Gooddata made major enhancements to its Insights Engine, with the addition of Analytical Designer and Data Explorer for business user data discovery.
,
,
,Fig 1: GoodData Dashboard, sample project.
,
What makes this especially interesting is the Insights Network - the knowledge accumulated over seven years of deployments, services expertise and the activity within more than 50,000 projects in GoodData's cloud-based open analytics platform.
GoodData uses the Insights Network to offer recommendations within the new Analytical Designer that intuitively guide users through analysis based on data context and best practices while also helping develop their skills in data science.  
,
(Note: A similar feature is also present in RapidMiner, which gives data analysts recommendations based on what other analysts did in similar cases). 
,
,
,Fig 2: An example of GoodData Recommendations within Analytical Designer.
,
I had a chance to ask them a few questions about the new release.
,
,
,
,
When a user is choosing metrics that include a connected time attribute, the Analytical Designer suggests a Period-over-Period comparison. For example, when looking at Q4 from this year, it recommends comparing it to Q4 from last year.
,
When picking attributes like ""social channel,"" appropriate filters are automatically made available to users through the simple click of a button.
,
When picking product lines, the Analytical Designer will offer either time-over-time, granularity of time period (weeks, months, quarters, last 30 days, etc.) or a percentage of orders. 
,
,
,
, 
Recommendations are easily portable across domains when data sources or data types are the same.
,
,
,
, 
We are getting very close.  GoodData helps the user understand the data that is available to him or her through the Data Explorer. The Data Explorer will simplify adding user-defined data. With this announcement, the Analytical Designer offers users suggestions for best practices in data analysis.
,
,
,
, 
Half of GoodData's business is from Technology Vendors who embed GoodData analytics within their products.   These vendors serve the majority of end-customers hosted within the platform, currently about 40,000. Currently, the majority of the GoodData user-base is based in North America. However, with its upcoming European Data Center, the company is expanding business in the European Union as well.
,
See more on GoodData site:
, and 
,.  "
"
        ,
,  "















"
,
,
, is a principal data scientist at ,, where he designs machine learning and analytic solutions to solve problems for Ayasdi customers. Prior to Ayasdi he was a postdoc with Ayasdi co-founder Gunnar Carlsson in the Stanford University Mathematics Department. He's held academic positions at the Max-Planck Institute for Mathematics, Mount Holyoke College and the American Institute of Mathematics.  ,
His PhD is from the University of Pennsylvania on the connections between algebraic geometry and string theory. Along the way he co-founded a data analytics company working on political campaigns, worked on quantum circuitry research, and studied chaotic phenomena in sand boxes.  His friends say that his best idea was to found a College funded cooking club in order to eat food he couldn't afford otherwise.
,
,
,
,

,
Here is there and last part of my interview with him:
,
,
,
,TDA is ,not currently part of any standard data science curricula, but there are a growing number of schools that offer graduate level courses in TDA.  Penn, Duke, Stanford, University of Chicago, University of Ohio all come to mind, but there are doubtless others.  The perspectives and techniques are not exactly what we do at Ayasdi, but there are many shared ideas. A nice example syllabus with resources: ,
,
For self study I also recommend starting with ""Topology and Data"" by Gunnar Carlsson (,).  If you're interested in applications to Machine Learning or particular industrial problems, I recommend contacting Ayasdi and asking for the relevant white papers as there is relatively little published in the academic literature. For a more gentle introduction for a working data scientist I have a number of video lectures up on YouTube that I've given at various meetups, academic seminars and industry conferences (sorry for the self promotion).
 ,
,
,
,As a researcher I worked on connections between geometry and string theory, and while I found string theory satisfying in an abstract sense ,I wanted to work on projects that had more immediate real world impact. I started looking outside of academia, and data science was a draw for a variety of reasons. 
,
I was particularly drawn to the vibrant culture of creativity in data science; it pulls from whatever discipline it needs to without dogma.  Data scientists themselves are constantly seeking novel ways to quantify and explain the world around them, and while some people argue about the term ""Data Scientist"" and say they are really statisticians, analysts, or machine learners, I think that culturally we are distinct and embody the scientific approach to the world in ways that others do not.
,
,
,
,""Work it out for yourself. Start with a simple example.""
,
,
,
,We look for people with some quantitative background in mathematics or the natural sciences. At Ayasdi, Data Scientists work with customers to solve their problems, do product development, and more fundamental R&D. This is an extremely broad set of responsibilities and we generally expect people to be solid contributors in two of the three areas.
,
R&D is more important for us than for many data scientists embedded in industry. We work at the cutting edge of TDA and machine learning and can't necessarily rely on academic literature to find solutions for our problems.  Some of what we do at Ayasdi challenges machine learning and statistical orthodoxy, and we need people who can independently and creatively think through our methodology and distinguish a heuristic from a theorem.
,
One thing that we require is that all Data Scientists, even ,those with a more research oriented bent, work with customers. This keeps all of our efforts focused on solving problems as they appear in the real world and not in idealized settings where something can be proved. As a result, communication with both technical and non-technical counterparts at other companies is a requirement for a Data Science position at Ayasdi. We ask all interviewees to give a seminar type presentation on a data problem that they've work on. The presentation is evaluated for both content and clarity.
,
, PM me if you have a good way to measure this in an interview.
,
,
,
,I like to explore the Bay Area with my family.  While job opportunities in Tech is what brought me here, the cultural and culinary diversity set in stunning physical beauty is what keeps me here.
,
,
,  "

"
Copyright Clearance Center presents a complimentary webinar
,
,
,February 11, 2015
,1:00 - 2:00 PM EST 
,
Life science companies increasingly rely on text and data mining to glean important insights from vast amounts of published information. From drug discovery and clinical trial development to drug safety monitoring and competitive intelligence, text mining has many applications for life sciences.
,
Join Lars Juhl Jensen, professor at Novo Nordisk Foundation Center for Protein Research at the Panum Institute Copenhagen, as he discusses:
,
,??,
,  "
"
, has been launched. The , competition is organized as an attempt to advance the research field of eye movement biometrics, by giving an opportunity to scientists and researchers to use a large, high quality database of eye movements, recorded using different visual stimuli and with recordings conducted to allow testing for template aging effects.
,
You can visit competition webpage at: ,
,
Registrations are open and the development datasets are ready to download. You can , In the competition webpage you can find the description of the competition procedure and deadlines, and helpful resources regarding eye movement biometrics.
,
,: the winner of the competition will receive an eye tracker as a prize. Specific details about the model of the eye tracker and the vendor providing the prize will be announced at a later date.
,
The deadline for the submission of final results is: ,.
,
, is an official competition of the ,.
,
If you have any additional questions please contact us at: ,
,
,
,  "
"
Most popular 
, tweets for Jan 28-29 were
,
Useful list: Top 50 open source web #crawlers for #datamining: Heritrix, Nutch, Scrapy, WGET ...
, 
,
,
,
Useful list: Top 50 open source web #crawlers for #datamining: Heritrix, Nutch, Scrapy, WGET ... 
, 
,
,
Listen to , , guru of Data #Visualization, on how to see data (and things) better 
,
,
Useful list: Top 50 open source web #crawlers for #datamining: Heritrix, Nutch, Scrapy, WGET ... 
, 
,
,
,  "

"
,
By Gregory Piatetsky,  
,, Jan 30, 2015.
,
The excellent post by Zack Lipton 
, has examined the ""flaws"" found in deep learning algorithms, especially how one can generate adversarial examples that can fool the algorithms. 
Zack argued that all machine learning algorithms are susceptible to adversarial chosen examples, and we should not be surprised that deep learning have the same weakness as logistic regression.
,
This post generated 
,, including an interesting observation
from 
,, one of the leading experts on Machine Learning and Deep Learning. 
,
He wrote ""I agree with (Zack Lipton) analysis, and I am glad that you have put this discussion online.""
,
,Yoshua continued:
"
"
By Matt Reaney (BigCloud.io). 
,
We?€?re a pretty suspicious lot here in the UK.
,
Britain has the potential to be the world leader in Healthcare Big Data. We have (more or less) centralized NHS records dating back to the 1940s, and we have normalized personal data for each patient, which is far less fragmented than for the US, for example. We also have an innovative bioscience sector to work hand in hand with the findings.
,
,
The benefits of Big Data in Healthcare are considerable. The power to access and analyze enormous data sets can improve our ability to anticipate and treat illnesses. This data can help recognize individuals who are at risk for serious health problems. The ability to use big data to identify waste in the healthcare system can also lower the cost of healthcare across the board.
,
It all makes perfect sense, but, therefore, why did the government have to abort their first step in this direction just last year? It was about to create Care.data, a centralized collection of patient records that would truly transform UK healthcare. The reason that they cancelled it? Privacy concerns?€?.
,
The timing of the announcement was unfortunate. It coincided with the Edward Snowden revelations, and the consensus was that Healthcare Big Data could lead to a big brother situation where government, employers and anyone else can access your private records. In actual fact, everyone would still have the option for their data to be private; it was merely poorly communicated.
,
In my view, this suspicion is temporary. The benefits for society are just too great, and they won?€?t be ignored for long. There are a number of high profile initiatives already making a huge difference to our future. As the benefits become more tangible, the naysayers will change their tune.
,
Venture capitalists invested nearly $700 million into US digital health startups in just the first quarter of 2014?€?that?€?s 87% year-over-year growth versus the first quarter of 2013.
,
In another example, Intel recently announced that it is working with the Michael J Fox Foundation For Parkinson?€?s Research on a new pilot initiative that is aimed at using data mined from wearable devices to detect patterns in the progression of the disease.
,
There are many, many other examples.              
,
In my personal view, there is no reason to be so guarded. Society is becoming ultra-transparent, and soon there will be no hiding place. If someone wants to find out something about you, they will have the opportunity. In addition, with wearables fast becoming a part of our lives, we are in any case putting increasing amounts of information into the ?€?cloud?€? about our personal lives.
,
In terms of practicality and functionality, the healthcare revolution may not happen overnight, but a more transparent healthcare system, more affordable care, and?€?ultimately?€?a healthier nation are definitely worth waiting for.
,
I?€?m proud to have an impact on the big data industry, no matter how small. If just one of my candidates has the potential to make even the slightest difference in the area of healthcare, then there is no higher motivation for me.
,
If I can contribute to ensuring a longer and healthier life for myself and my daughter, then why not?
, 
Bio: Matt Reaney is the Founder and Director at ,, a talent search firm focusing on all things Big Data and helps innovative organisations across Europe, APAC and the US find the talent they need to grow.
,
,
,  "
"
,

,

,

,

,

,
,
,

,

,

,
,


,

,

,

,
,

,

,

,

,
,

,
,

,
,

,

,

,
,


,

,

,

,
,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,

,
  "
"
,
,
, is founder and CEO of ,, a Big Data analytics and apps firm. His global leadership experience of more than 16 years includes C-level positions across product management, sales and marketing, and corporate strategy and turnaround. Nuevora was recently ranked by CIO.com as one of the top 10 Big Data firms to watch out for.
,
,
,
Here is second and last part of my interview with him:
,
,
,
, 
There is a clear roadmap organizations should follow to maximize value from analytics. Organizations are moving from ?€?descriptive analytics?€? to ?€?predictive analytics,?€? which provides a level of certainty into knowing whether someone will buy a product from a company in the future and if so, by when. Having this predictive insight is great; however, not being able to take appropriate action based on this great insight would be tantamount to a wasted opportunity. 
,
The next stage is to enable prescriptive analytics, which uses a portfolio of analytical capabilities to recommend a particular decision path, one that has the power to influence the desired action from a consumer at a given point in time. However, due to the dynamic nature of every business and the ecosystems we live in, any recommended prediction and/or prescription has a limited shelf life. 
,
Thus, the third stage in this roadmap is to enable ?€?closed loop analytics,?€? which is an end-to-end analytics process that sets business goals, predicts outcomes, prescribes actions, monitors progress, and assesses impacts. It then realigns objectives, and optimizes predictions and prescriptions based on continual feeds of data that reflect the real-world changes in a business environment.
,
The challenge lies, however, in enabling this closed-loop process to operate in as real-time mode as possible. That is what brings us to the last stage in this roadmap, which involves powering real-time analytics that accelerate the previous analytics processes ?€? with real-time updates and insights needed by the organizational leaders at the point of decision making.
,
,
,
,Identifying the right data begins with the end objective in sight, and then going backwards in terms of identifying the data which will influence the end goal. These are the controllable variables, which can be acted upon within all the meaningful variables for a business.
,
Using unique hierarchical algorithmic modeling techniques and data discovery approaches, Nuevora identifies the right data from the ocean of big data based on the predicted influence they have on the end outcome. These preliminary analytics and data analyses, run on all the variables in the big data stream in relation to the dependent variable, identify the most influential and meaningful variables.
,
 
,
,
,Nuevora plays at the intersection of analytics, big data and cloud. Leveraging a powerful platform, Nuevora delivers scalable and repeatable analytics to organizations at rapid speeds. The platform, with its pre-built set of applications brings in 60 to 70 percent  automation for analytics delivery, with the last mile configuration and contextualization delivered by data scientists and business consultants. 
,
This approach for delivering analytics provides significant speed, scalability, and reliability, while enabling closed-loop recalibration capabilities as required by the changing business circumstances and data dynamics of a given business. Further, Nuevora?€?s unified algorithmic architecture enables marketing organizations to gain an integrated and predictive view into all stages of a customer lifecycle (retention, up-sell, cross-sell, profitability, LTV) in one glance. This is a big differentiator compared to a number of isolated solution providers.
 ,
,
,
,
,
, ?? ,
,
,
,Dream big! Stay glued to the vision; be practical; build and depend on a great team; be a great mentor yourself.
,
,
,
,
,
, ?? ,
,
,
,?€?,?€? an autobiography/biography of Rafael Nadal by Rafael Nadal with John Carlin. 
,
This is a great story of how a combination of intense focus on one?€?s goals, consistent hard work, human determination, intestinal fortitude and mental toughness can deliver wonders and world-class results.
,
,
,  "

"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "





















"
,
,
,
You've already mastered data collection and analysis; it's now time to take the next step. Find out how to challenge assumptions, spot patterns and reveal potential solutions to problems that otherwise would not be visible.
,
,
,
,
,
,??,
,  "

"
        ,
,  "
"
,
,
,
,??,
These classes are part of the ,.
,  "
"
,
By Gregory Piatetsky,  
,, Feb 11, 2015.
,
,
,
This poll is closed - here are the results:
,
,
,
Here are the results from past polls: 
,  "
"
,
By Gregory Piatetsky,  
,, Feb 7, 2015.
,
As a media partner for Strata 2015 Conferences, KDnuggets raffled a 
free, 2-day conference pass to
,
,, Making Data Work
,Feb 17-20, 2015. San Jose, CA, USA
,
To enter the raffle we asked people to 
specify 2-3 most interesting trends in Analytics and Big Data in 2015.
,
One person - Imelda Llanos de Luna - even submitted an essay
,, which we published in KDnuggets.
,
The winner, chosen by an unbiased random number generator, was 
Jeffrey Sukharev of ancestry.com. 
,
Others can get a 
,, unless the conference is sold out.
,
No single trend was dominant, but most common answers were Apache Spark, Deep Learning, Real-Time technology, and Internet of Things (IoT).
,
Here is a word cloud and selected interesting answers:
,
,
,
,??,
Here is a last year's report on top trends ahead of 2014 Strata Santa Clara Conference:
,
,.
,
,
 ,  "
"
,
MIT's Innovators under 35 contains many highly influential young innovators in various technical fields. Here we look at some of those who are innovating in the field of Big Data.
,
, is the CEO and founder of Snips. His work aims to make city life more frictionless. One example is his partnership with France's railway system, aimed at making travel by train less crowded.
,
MIT TR profile: ,
,
,, developed a method for much faster training of Deep Learning Neural networks.  His technology was used by Google and sparked a race at Facebook, Microsoft, and other companies to invest in deep-learning research.
,
MIT TR profile: ,
,
, is an interface designer, exploring innovative ways for people to interact with data. Jinha is currently on leave from his PhD at MIT to direct the Interactive Visualization lab at Samsung.
,
MIT TR profile: ,
,
, is the cofounder and CEO of Box. He seeks to change the way people work with data by making data access more convenient. Big clients like General Electric serve to validate his vision of the future.
,
MIT TR profile: ,
,
, has a vision for the future of data science - a vision of greater automation. His software Eureqa tackles the problem by generalizes laws based on physical observations.
,
MIT TR profile: ,
,
,
,
,  "
"
,
,
, ran one of the major search infrastructure teams at Google where GFS, BigTable and MapReduce were used extensively. He wanted to provide that powerful capability to everyone, and started , on his vision to build the next-generation platform for semi-structured big data. His strategy was to evolve Hadoop and bring simplicity of use, extreme speed and complete reliability to Hadoop users everywhere, and make it seamlessly easy for enterprises to use this powerful new way to get deep insights. That vision is shared by all at MapR. Srivas brings to MapR his experiences at Google, Spinnaker Networks, Transarc in building game-changing products that advance the state of the art.
,
Srivas was Chief Architect at Spinnaker Networks (now NTAP) which built the industry's fastest single-box NAS filer, as well as the industry's most scalable clustered filer. Previously, he managed the Andrew File System (AFS) engineering team at Transarc (now IBM). AFS is now standard classroom material in operating systems courses. While not writing code, Srivas enjoys playing tennis, badminton and volleyball. M.C. has an MS in Computer Science from University of Delaware, and a B.Tech. in electrical engineering from IIT Delhi.
,
Here is my interview with him:
,
,
,
,: I ran one of the major search infrastructure teams at Google where GFS, BigTable and MapReduce were extensively used. I wanted to provide that ,powerful capability to everyone, and started MapR on my vision to build the next-generation platform for semi-structured big data. The strategy was to evolve Hadoop and bring simplicity of use, extreme speed and complete reliability to Hadoop users everywhere, and make it seamlessly easy for enterprises to use this powerful new way to get deep insights.
,
,
,
,For a company that?€?s been out of stealth mode for about 3-1/2 years, we have done very well, with over 700 paying customers, many of whom are among the top companies in the world. One use case stands out: the Aadhaar project, an initiative of the Unique Identification Authority of India (UIDAI), runs entirely on the MapR Distribution including Hadoop. We donated our software for this noble cause, which has enabled hundreds of millions of people in India to access services like banking, pension payments, education, healthcare, fuel, etc., which were very difficult for them to access just a few years ago.
,

,
,
,MapR-DB benefits from lessons gleaned from traditional databases like Oracle and Sybase, and from NoSQL systems like BigTable and Amazon DynamoDB. We improved on these ideas while side-stepping weaknesses, and developed  a database system that is the best in the world. MapR-DB can easily handle over a million tables, each with trillion-plus rows, and each row with 1000s of columns in JSON format. It is unbreakable, with over 99.999% of uptime. We also recently announced breakthrough performance results, achieved by using only four nodes of a ten-node MapR-DB cluster, ingesting over 100 million data points per second.
,??,
By accelerating performance by 1,000 times on such a small cluster, MapR-DB makes it possible to cost effectively manage massive volumes of data and enable new applications such as Internet of Things (IoT) and other real-time data analysis applications, including industrial monitoring of manufacturing facilities, predictive maintenance of distributed hardware systems, and data center monitoring.
,
,
,
,Apache Drill enables users to run?€?for the first time?€?full ANSI SQL 2003 queries directly on Hadoop data. A user can now query semi-structured data with ambiguous schema, which was not possible before. For the first time, people can drill down into raw data, in place, without needing a DBA?€?s assistance, or without having to transform or migrate it first.
,
Apache Drill is breaking down the barriers that have existed in data analytics for as long as databases have existed. Apache Drill is fully self-service, letting you surf your data in-situ. It can handle schema changes as they happen, and process complex nested structures like JSON. Hive and Impala implement different, disjointed subsets of what Apache Drill is capable of. Hive and Impala implement HiveQL (Hive Query Language) which is not ANSI SQL, although Impala might be evolving slowly to include ANSI SQL 92. Shark has been discontinued, mainly due to serious quality problems.
 ,
,
,
,Apache Drill in the Spark context brings the power of ANSI SQL with JSON support to Spark. A Spark programmer now can treat any RDD as a SQL table in Drill, and have Drill seamlessly combine the Spark RDD with external data in HBase, MapR-DB, Mongo-DB, and other systems. Further, the output of Apache Drill can be treated as the input for a Spark map-reduce process. So the combination of Drill and Spark is unleashing some extraordinary and unprecedented data processing capabilities on the Hadoop platform.
 , ?? ,
,
,
,We recently had one customer say, ?€?We just think about the data; we don?€?t worry about limitations of underlying network and servers.?€? That comment tells me that our product architecture allows organizations to really focus on the data to improve their business. In one situation, we had a customer start with several use cases and improve their business top line to the tune of approximately USD $1 billion annually. Seeing customers use big data to impact the business as it happens is very gratifying, and we?€?re seeing that type of result over and over again.
,
,
,
,
,  "
"
Most popular 
, tweets for Feb 09-10 were
,
Should you teach #Python or R #rstats for #DataScience? Python for programmers, industry; R for academia 
,
,
Should you teach #Python or R #rstats for #DataScience? Python for programmers, industry; R for academia 
,
,
#WhiteHouse says problems with #BigData, differential pricing can be dealt by antidiscrimination, consumer prot laws 
,
,
#BigData on Divorce: Wedding with 200+ guests is 92% less likely to lead to divorce 
, 
,
,
,
,  "
"
        ,  "
"
,
,
July 26 - Aug 1, 2015
,Lipari Island, Italy 
,
Application Deadline:
,May 31, 2015
,
Social and urban systems have been the focus of social science theory and research for centuries, but only until recently have computational approaches enabled novel explorations of challenging and enduring research questions and the opening of new frontiers for investigation. What is the role of Computational Social Science in advancing the science of social and urban systems? Which advanced algorithms and data structures play a key role in these investigations? 
,
In 2015 our Lipari Summer School in CSS will address questions such as the role of GIS (geographic/geospatial information systems), social media, big social data, agent-based models, network models, and their integration in the study, design, and implementation of social and urban systems. Students are encouraged to apply early because enrollment is competitive and limited. 
,
For more information, visit 
,
,.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, ran one of the major search infrastructure teams at Google where GFS, BigTable and MapReduce were used extensively. He wanted to provide that powerful capability to everyone, and started , on his vision to build the next-generation platform for semi-structured big data. His strategy was to evolve Hadoop and bring simplicity of use, extreme speed and complete reliability to Hadoop users everywhere, and make it seamlessly easy for enterprises to use this powerful new way to get deep insights. That vision is shared by all at MapR. Srivas brings to MapR his experiences at Google, Spinnaker Networks, Transarc in building game-changing products that advance the state of the art.
,
Srivas was Chief Architect at Spinnaker Networks (now NTAP) which built the industry's fastest single-box NAS filer, as well as the industry's most scalable clustered filer. Previously, he managed the Andrew File System (AFS) engineering team at Transarc (now IBM). AFS is now standard classroom material in operating systems courses. While not writing code, Srivas enjoys playing tennis, badminton and volleyball. M.C. has an MS in Computer Science from University of Delaware, and a B.Tech. in electrical engineering from IIT Delhi.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,:
MapR is the only Hadoop distribution that brings the kind of integrity, reliability, performance, and manageability that enterprises have long desired, in all their systems. The other Hadoop distributions are all pretty similar to each other, and cannot deliver the sort of reliability and manageability that MapR delivers. For example, only the MapR Distribution including Hadoop can continuously self-heal and self-repair from any component failure automatically, whether it be a disk, a server, a rack, or the whole data center itself. MapR is the only distribution that automatically protects and versions your data, letting you roll back or roll forward in case of accidental corruptions or deletions. 
,
MapR is the only distribution for Hadoop, and in fact the only commercial data processing system, that can work across data centers worldwide, automatically move petabytes, and switch over from one to another and back, whether due to ?€?follow the sun?€? operations, or due to disaster recovery. The Aadhaar project in India moved to the MapR Distribution including Hadoop to take advantage of such advanced capabilities.
,
,
,
,The ?€?consumerization,?€? as you term it, has already taken place at Yahoo, Facebook, Twitter, LinkedIn, and at many, many other websites around the world. The challenge is always to balance privacy while delivering these services. People have started to get afraid of being tracked, and have started hiding themselves. , If they don?€?t do it themselves, it will get legislated and that can take a turn for the worse. For example, the European Union already has passed some bizarre laws about people who can ask web sites to not link to content that is already a matter of public record. Big Data is a powerful tool, and it must be used with care responsibly.
,
,
,
,Legacy databases and data warehouses are expensive because DBA resources are required to flatten, summarize and fully structure the data. Upfront DBA costs delay access to new data sources, and ,the rigid structure is very difficult to alter over time. The result is that legacy databases are not agile enough to meet the needs of most organizations today. Earlier big data projects focused on storing target data sources. Now, instead of focusing on how much data is being managed, organizations are moving their attention to measuring data agility. How does the ability to process and analyze data impact operations? How quickly can they adjust and respond to changes in customer preferences, market conditions, competitive actions, and the status of operations? These questions will direct the investment and scope of big data projects in the near term.
,
Additionally, data lakes and data hubs have represented a popular first deployment for Hadoop. A data lake or data hub is a scalable infrastructure that?€?s both economically attractive (reduced per-terabyte cost) and designed for flexibility, and it has the ability to store various forms of both structured and unstructured data. During the next few years, data lakes will evolve as organizations move from batch to real-time processing and integrate file-based Hadoop and database engines into their large-scale processing platforms. 
,
,
,
,What you do today will follow you for the rest of your life. Therefore, don?€?t ever compromise on pushing the ceiling. Quality is lasting. You want to be known as a person that demands and delivers the highest quality products, in every sense of the word.
,
,
,
,Start simple. Pick a small project that can succeed easily. The idea is to focus on learning how to implement and deploy the new technology first. Subsequent projects can be more complex, but it?€?s important to first get success on a small scale.
,
,
,
,Universities are doing a good job with big data, in many ways. Some tend to focus on the technology itself, while others look at combining various technologies to build new systems. Both are important. We have a pretty strong internship program every summer, where students in their senior or final years work at MapR for 3-6 months on various projects. We have been quite impressed by the quality of the students coming in. Many of these students have joined MapR after they?€?ve graduated.
,
,
,
,Attitude. Attitude and the ability to work as part of a team are the most important qualities. It goes without saying that the person has to have the relevant knowledge, intelligence, and ,good communication skills. But it?€?s also important to understand how they approach a problem and how they react when confronted with difficult technical issues. Do they ask for help? Do they share their issues? Do they look around to see what others have done? Do they keep trying, or do they give up? Do they get angry, or are they able to channel that frustration to try harder? Attitude is everything.
,
,
,
,I read ?€?,?€? most recently. It is an easy read compared to most business books, and as a founder/CTO of a company, I have run into the issues discussed in that book quite often. For relaxation, I like to watch movies, and I recently watched the top-rated TV series ?€?The Sopranos.?€? I also play a lot of badminton and chess?€?I follow former World Chess Champion Vishy Anand, whom I admire.
,
,
,  "
"
,
By Gregory Piatetsky,  
,.
,
Last October, , newspaper asked to me contribute to a feature called ""The Experts' Guide to the 21st Century"", where each expert would direct readers to 4-5 key books, films, or websites they should investigate in their field. My topic was Big Data and Predictive Analytics.  
,
I submitted my writeup in October and their editor told she liked it very much. However, perhaps other experts did not respond as quickly. The feature was supposed to run in November, then December, then January, and a few days ago I learned it was canceled.  Surprisingly, The Guardian even promised to send some payment for ""spiked"" contribution.
,
Since they will not publish, I am publishing my contribution here. Let me know what you think in comments below.
,
,
,
,
Big Data is both an over-hyped buzzword and a real trend, reflecting the rapidly growing digitization of our world, and the amazing, and sometimes scary implications. However, Big Data by itself is just numbers - what makes it so powerful is Predictive Analytics (also called Data Mining or Data Science) - the ability to model our world, predict events, and make data-driven decisions, with accuracy approaching and sometimes even exceeding our human abilities.
,
Here are a few books, movies, and websites related to Big Data, Data Science, and Data Mining, from more popular to more technical.
,
1. ,, by Victor Mayer-Schoenberger, Kenneth Cukier.
,
This is a very good, high-level, although sometimes overly enthusiastic explanation of the impact of Big Data on our world.
,
2. ,, by Nate Silver.
,
This book give a great explanation of how and where predictive analytics works well and why it is so easy to make bad predictions.  Nate Silver became famous after his near perfect predictions of US Presidential election results in 2008 and 2012, but not everything is so predictable - Nate had rather  
, results in Oscar predictions.
,
3. ,, by Eric Siegel.
,
Eric gives a more advanced look into the world of predictive analytics in action, including how one's location can be predicted, the infamous , before her father knew, Netflix prediction of movie ratings by viewers, and more. More advanced readers will benefit from a chapter on the important topic of Uplift or Persuasion modeling, very effectively used by Obama campaign in 2012.
,
,
4. ,
is the revolutionary machine learning method that has been achieving 
, levels of performance, especially in image recognition.
,
Here are some useful websites/links for Deep Learning for more advanced readers
,??,
,
,??,
,
,
5. The flip side of Big Data is the erosion of 
,. We leave so many digital trails, it is hard to remain private and anonymous.  Many of those issue were well raised in an excellent 2002 movie by Steven Spielberg: 
,.
,
Its uncanny vision of surveillance society with ""precogs"" who predict crime ahead of time and department stores that recognize people and push ads to them is actually already happening 
,.
,
6. Finally, just for fun, you can read  Charles Stross Sci-Fi novel 
,,  where data mining plays a key role in uncovering vampires in London. Here is 
,
,
,
 ,  "
"
,
By Gregory Piatetsky,  
,.
,
New KDnuggets cartoon imagines what would happen if a Big Data genie would grant 
a romantic Data Scientist 3 wishes for a Valentine's Day:
,
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,
,??,
,
,
 ,  "
"
,
,
,
Here we look back on the top posts in the month of January on ,. Some of these are included in previous weekly posts while a couple of these come from before the weekly posts began.
,
,
,
This post wasn?€?t included in any of our previous top lists because it comes from the beginning on January. It links to a fantastic new podcast that already has great episodes including Geoff Hinton and Kevin Murphy. Based on how well it has started out, I look forward to seeing how this develops!
,
,
,
This video is an excellent introduction to SVMs. No matter your experience level, it?€?s an approachable short lecture on the topic that?€?s worth a viewing.
,
,
,
This post links to a Stanford online course on statistical learning that started near the end of January. Registration is still open, so if you?€?re looking to take a course on statistical learning, this is a great opportunity.
,
,
,
As discussed previously, this thread initiated the recurring simple questions threads. There was a question thread each week for a couple of weeks, but it seems it has now died out.
,
,
,
When we first saw this thread post, one of the main attractions was that it was on GitHub, allowing public modifications to the list. Since its first post, it has been modified, so there should be some different data sets available on the list.
,
,
,
,  "
"
,
,June 1st - Aug 21st, 2015
,
,
,
NYC Data Science Academy offers the highest quality in data science training.?? It?€?s designed specifically around the skills employers are seeking, including R, Python, Hadoop, github, D3.js, raspberry pi and much more.?? The program delivers a combination of lectures and real-world data science challenges to its students.?? By the end of the program students have at least six completed projects to showcase to employers and will have had multiple opportunities to present.?? This prepares students to meet the requirements for premium data science positions.
,
Our teaching staff has years of experience building big data solutions for clients.?? We know what it means to be a data scientist, and we know what employers are looking for.?? We work closely with hiring partners and recruiting firms to make sure job placement is an integral part of the program.
,
Candidates with degrees in Science, Technology, Engineering, Math or equivalent experience in quantitative science or programming are preferred.?? We start preparing students the day they are admitted into the bootcamp by enrolling them in our bootcamp prep program and giving them free access to our on-going 
, (each course worth $800 to $3000).
,
,
,
Whether for fun or for business, our courses provide hands-on training for those looking to increase their skill level in data management, visualization, data mining, statistics, or critical thinking using mathematical tools.
,
,
,
,
Enroll online: 
,
,
Email:??,
,
Call:??1 (646).926.3881 (USA)
,Twitter: ,  "
"
Most popular 
, tweets for Feb 11-12 were
,
#DataMining finds corruption is correlated with low income, low development MIT , 
, 
,
,
,
My Brief Guide to Big Data and Predictive Analytics for non-experts 
,
,
Big move! Hitachi buys Pentaho to extend Its #BigData footprint, will set a new unit (Pitachi? Hitaho?) #BigDataCo 
,
,
Romantic #DataScientist , automates #Tinder with Eigenfaces, #SentimentAnalysis 
, 
,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
By Justin Long, ,
,
While my friends were getting sucked into ""swiping"" all day on their phones with Tinder, I eventually got fed up and designed a piece of software that ,.
,

,

Since Tinder?€?s rising popularity, including its use by Olympic athletes such as snowboarder Rebecca Torr, Tinder has achieved critical adoption as a launchpoint for singles meeting singles. Its rising popularity has encouraged a wave of ""Tinderbot"" inventions by nerds doing things such as ""swiping right"" for everyone near their location (and of course those pesky spammers). It wasn't my intention to ""one-up"" the competition, but using the facial recognition algorithm Eigenfaces I built a bot that learns when to swipe right (like a person) AND swipe left (dislike a person) AND start your conversations.,

Dubbed ""Tinderbox"", the first version only took 3 weeks to build. It uses an existing Tinder account and taps into Tinder APIs, which is nice so you don't have to create an entirely new account. Tinderbox recreates the Tinder app in your browser, including the inbox and discovery preferences. The workflow is simple:
,
,

,??,
,
,
, is a quick and easy algorithm to implement facial recognition without the use of complex software like OpenCV. Tinderbox first extracts faces using the Viola-Jones framework - specifically the , implementation - and converts them to greyscale. Only pictures with single, identifiable faces are used (to filter out false positives). Then after each image is normalized, its pixels are converted into a matrix where they are then appended to a list of models. The models are then averaged into a single face used for future comparison (as noted in the workflow above). The bot requires you to make 60 yes/no choices before it has enough data to choose on your behalf. Then its on full auto-pilot.,

If you're interested learning more about Eigenfaces, , has a relatively good overview of the math/matrix operations as well as a selection process using k-nearest neighbor. The paper is a description of another Eigenface facial recognition system.,

I also added a couple of features that are only in the paid version of Tinder. ""Undo"" buttons are there in the event a wrong swipe was made or the bot made a poor choice. Also, you can set your location to anywhere in the world for travel or curiosity purposes.,

The bot that runs in the background also has a messaging system that starts conversations. Using ,, the bot analyzes the sentiment of each chat response and classifies it as positive or negative. Using a ""message tree"" (see diagram below), the bot selects from pre-programmed chat messages as a response based on the sender's sentiment. This continues up to 3 replies until the user is notified that a chat is ready to enter. The advantage of this? It removes the time involved in filtering new Tinder matches since a lot of people tend to drop off and ""go dark"" early in the process. Extended conversation is a strong indicator of interest.,

,

What are the results so far? The bot is amazingly effective. I would estimate an accuracy of up to 70% in its selections - though there may be a hindsight bias. Using a brand new account, I did a quick test to see how quickly the bot could get results. In 48 hours, the bot registered 21 matches (starting all of those conversations), made 4 extended conversations, and the bot itself made over 300 moves. A ""move"" is any step the bot makes in either sending a message or making a swipe. And in that time I barely needed to touch the app. I also created a dashboard to give me an overview of my metrics (see screenshot below).,

,

What do girls think of the bot? I've gone on at least 10 dates with the help of the bot and I've shown my partners the bot in its entirety. One date literally didn't believe me and thought I was pulling her leg. Another person thought it was really cool and wanted the full tour. All were in agreement that it is , creepy, though some felt it was borderline. Kind of nice considering it's not something you'd come across everyday.,

Am I still using the bot? I've actually turned it off for now. Admittedly, it worked too well and started to conflict with work. Although in a couple cases I had follow-ups and I'm still seeing one person.,

Check out the code online: ,. Please feel free to contribute. I don't have current plans to pursue this commercially, though the license allows personal use and modification.
,
Original: ,
,
,
,
,
,
 ,  "
"
,
,
,
Here are some of the most interesting and regularly-updated blogs on Data Mining, Data Science, and related topics, given in alphabetical order.  If we missed some, please add in comments below.  See also 
,.
,
,
,
,??,
,
,
,
,
,
,
,
,
,
,
,
,
 ,  "



















"
On 
,, join 
,??
,- ,, author of ,, 
,- ,, Senior Marketing Analyst at L.L. Bean, 
,- ,, CEO of eContext, and 
,- ,, Director of Analytics at Visa 
,??,
for a free webinar addressing 
,
,
Sign up for free now: 
,
,
The webinar speakers will offer in-depth insight and discussion on:
,
,??,
,
,
Sign up here today:
,
,
Can't make the date? Sign up anyway to receive a recording of the webinar!
,
,Brian Parke
,Project Director
,Data Driven Business
,(201) 234-4755 | ,  "
"
Most popular 
, tweets for Feb 09-15 were
,
Why limit yourself to ""50 Shades of Grey?"" R has 102 shades of grey #rstats , 
,
,??,
,
Why limit yourself to ""50 Shades of Grey?"" R has 102 shades of grey #rstats , 
,
,
Why limit yourself to ""50 Shades of Grey?"" R has 102 shades of grey #rstats , 
,
,
Why Electric Cars Don't Have Better Batteries - a sad story of Envia - MIT @TechReview , 
,
,
,??,
,
,  "
"
,
,
, is ,'s Senior Technical Fellow in visualization and interactive techniques. He is pursuing new ways of using visualization for huge amounts of both geometric and non-geometric data.His work with geometric data made Dave a pioneer in interactive 3D computer graphics. He devoted his first 11 years at Boeing to research and development of computer-aided design software. These projects led to pioneering work in interactive 3D graphics, user interface management systems, and industrial use of non-uniform rational (NURBS) solids and surfaces. Dave is pioneering the use of visual analytics to help extract more information from complex non-geometric data. 
,
He earned his Masters in Computer Science from the University of Colorado in 1972 and a Bachelor?€?s in Quantitative Studies from the Johns Hopkins University in 1970. He participates in numerous professional organizations, including ACM (Association for Computing Machinery), which named Dave a Distinguished Scientist in 2007 and a Fellow in 2013; ACM SIGGRAPH, where he started the annual Pioneers student mentoring program and received the Outstanding Service Award in 2012.
,
Here is my interview with him:
,
,
,
,: Visualization has been part of the Boeing fabric since the company was founded in 1916.  ,Details of design geometry were visualized as engineering drawings.  Those drawings became the ?€?blueprint?€? for commercial and defense aerospace product manufacturing and support.  Engineering analyses summarized complex results in different types of graphs.
,
Computer-based visualization through computer graphics started in Boeing in 1960.  Verne Hudson coined the term, which Bill Fetter entered into computer jargon.
,
Recently, the importance of visualization has expanded beyond its traditional role in engineering.  The Boeing factory and customer support rely on an expanded set of new capabilities and insights for problem-solving and decision-making. 
,
My personal work involves computer-based visualization for both non-geometric data (through visual analytics) and geometric data (through 3D massive model visualization and model-based design).  The problems I address vary all over the map.  The dominant cross-project thread for me is spent working on interactive scalability and performance.   
,
,
,
,Visual analytics has a formal definition:  the science of analytical reasoning facilitated by interactive visual interfaces.  (Ref:  Illuminating the Path, ,)  
,
,In simpler terms, visual analytics allows a person to gain better business insight by interactively exploring pictures of geometric or non-geometric data.  That insight can range from providing easy access to different data sources to discovery of unexpected results.
,
Inevitably, the issue is to help people better cope with the immense amount of computer-based data already in existence.  Since data volume continues to grow exponentially, people need a wide path to the brain.  A person?€?s eyes are his or her most efficient sensory input device.  In quantitative terms, our visual system sends over 80% of all incoming data to the brain for processing. 
,
,
,
,What attracted me to computer graphics in the first place was the ability to tell stories visually.  
,
Fundamentally, this role of storytelling, understanding, and explaining is the same today.  The computer graphics tools we have are amazingly richer and handle much more data.  The fundamental human problem?€?how to tell a meaningful story?€?still exists.
, ,
,
,
,Boeing has an ever-increasing number of examples of effective use of analytics.  Analytics is helping reduce turn times for both military and commercial aircraft.  Sensor feeds are interpreted from in-flight data sensors to understand where a possible problem can be ,prevented by early part replacement.   Analytics is growing as part of the manufacturing process, too.  For example, we?€?re replacing out-of-tolerance tools before they fail.
,
One of our critical tasks is to insure safety both for the flying public and for our own workforce.  Analytics is in active use to help design better locations and increase crew responsiveness to bird strikes.  We?€?re helping industrial safety by finding assembly tasks that cause repetitive stress injuries so that the tasks can be redesigned.
,
,
,
,Personally, I love augmented reality.  Boeing (through Tom Caudell and David Mizell) coined the term in 1990.
,
In many ways, augmented reality has not penetrated the industrial workplace much as yet.  ,Getting AR into the mainstream requires integrating the technology with the overall production computing system.  The big technical hurdle is providing registration accuracy for totally mobile devices.  Today?€?s accurate systems require a fixed base.  In addition, dealing with visual scale, something we can do with notebooks and laptops is not quite there for lightweight, highly mobile devices.  I expect significant progress over the next five years.
,
The 30-year evolution is pretty normal for complex computing technology to make a real dent into Boeing?€?s complex business.
,
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Feb 17 and later.
,
See full schedule at , .
,
,
  "
"
        ,
The Chief Knowledge Officer at ,, 
Chief Software Architect at ,
and Principal Data Scientist at ,
are just a few of the 60+ speakers set to present at the upcoming 
,
,
The summit is taking place at the San Jose Convention Center on April 28 & 29, uniting 800+ data practitioners for a 70+ sessions, from keynotes and workshops, to panel sessions and countless networking opportunities.
,
Check out the schedule here: 
,
,
This year, the summit will bring a fresh look at Big Data, featuring 7 business and technical focused stages:
,
Across the stages, highlights include:
,
,
Wondering what to expect? Check out a presentation from the Chief Analytics Officer at the , from last year's summit: ,
,
,
If you would like to attend, or for more information please contact Jordan Dunne at ,  (+1 415 992 7918). 
,Alternatively you can secure a pass here: 
,
,
Early Bird passes are only available until next Friday, February 27, so book soon to get the best pass prices available.  "
"
,
,
,
This week on /r/MachineLearning there are some interesting posts ranging from applications to dating apps, an AMA announcement, and tutorials on practical ML.
,
,
,
This post explores using Eigenfaces in the context of Tinder, the dating app. 
(also published on KDnuggets: ,)
,In it, the author creates a system that automatically interacts with the Tinder app using k-nearest neighbors on an Eigenface of the user?€?s preferred match to decide who to match with. All of the code is open source, so feel free to peruse it ,.
,
,
,
This post actually links back to KDnuggets. This post details how lessons from statistics apply well to big data analysis, and if you missed it when it was posted last week, be sure to give it a read.
,
,
,
This is an announcement for the upcoming AMA (ask me anything) with Juergen Schmidhuber. He is the director of the Swiss AI Lab IDSIA and a professor of AI at the University of Lugano. If you have any questions for him, visit on March 4, and get a chance to engage with him!
,
,
,
This article goes through and explain PCA primarily through visual aids. If you find that this helps you learn, I?€?d recommend the article as a good introduction to the topic.
,
,
,
This tutorial introduces the topic of neural networks using numpy in Python. This can teach you the very basics of neural networks and connect those basics to some code. If you like to learn theoretical topics alongside the code, this is a good start.
,
,
,
,  "
"
,
,
,
, 
 Everything starts with you sending us your project idea. Your project can deal with ?€?the aggregation or usage, the crowdsourcing or production or the visualization of data. It should fit to one of the following categories: ,

,
,
,
, 

,
Everything is welcome ?€?as long as data is involved.
,
Projects can be submitted until March 31th 2015. Submission can be provided by either a single persons or a team. Beginning of April a top-class jury decides which 10 ideas will be preselected. The teams are invited for a one-day-workshop in Kronberg/Taunus which will take place end of April. After presenting your idea to the jury 3 projects will be selected for the development phase.
,
 At that stage project manager will be allocated together with experts needed to successfully execute the project during the following 12 weeks. The overall winner will join the exquisite list of inspiring TEDx speakers of that event and get the chance to present his project in a talk of his own.
,
 ,
,
,
,  "
"
,
,
,Can Data Science be automated? This billion dollar question is greatly interesting and greatly unsurprising at the same moment. The immense scope of advancement possible through automation of data science activities such as data model selection, optimization, inference, etc. has led to great interest in this idea. Meanwhile, the fact that we live in Big Data era implies that there is no way the manual approach to Data Science can keep pace with the rapid increase in volume, velocity and variety of data. So, automation is inevitable for sustainable progress. However, how soon we will reach there is still debatable.
,
There is no denying that a good amount of work in Data Science is mundane. Trying a variety of machine learning models and selecting the best combination of model and predictor variables is extremely time consuming and not remotely as interesting as it might sound in theory. If the underlying data was perfect, we could have been able to decide the best model from theory itself. But, in real life, data comes along with a lot of imperfections - errors, missing values, different scales/units, etc. With such varying data characteristics, one needs a lot of experimentation to determine which Machine Learning (ML) model would represent it the best. Today, we have a considerable number of ML models, each having a lot of inherent minor variations which are better suited to particular situations. Selecting a model goes hand-in-hand with selecting predictor variables, which is no less daunting considering the highly multi-dimensional nature of the data we are dealing with. So, choosing the right model is the same as looking for a needle in haystack. 
,
It's important to understand that the problem with manual approach is not just how many man-hours are required but also the mandatory skills required for the task. So, essentially there are two solutions (assuming that automation is yet not feasible): recruit more people with the mandatory skills (i.e. data scientists) or through use of innovative technology eradicate the need for those specialized skills, thus, enabling almost everyone in the organization to understand and perform those Data Science tasks. The latter is the focus of the , project led by , at the University of Cambridge.
,
The primary challenge here is not software development, but the advanced machine learning required in the areas of automated inference, model construction & comparison, and data visualization & interpretation. For any input dataset, the Automatic Statistician refers to an open-ended language of models to search (greedy approach) and evaluate the models that are potential candidates for selection. The selected model and other data inferences are then explained in a 10-15 page report that can be understood by non-experts.
,
One of the remarkable features of Automatic Statistician is the simplicity of its report which is targeted at non-experts. The conversion of insights from statistical representation to human-interpretable form plays a crucial role in widespread understanding and application of Analytics. Here is an excerpt from the report generated for airline passenger volume data set demonstrating non-stationary periodicity:
,
Yet another notable feature is the model criticism included in its report. This criticism answers key questions (for example, does the data match the assumptions of the model?) and includes the results of posterior predictive checks, dependence tests and residual tests.
,
The project is currently in early phase. Almost a year ago, Automatic Statistician project ,.
,
,
,
For successful transition to industry, the project will have to address various challenges such as reducing the computational complexity of searching through a large space of models and increasing the expressivity of language to describe sophisticated statistical inferences.
,
Attempts to automate Data Science are not limited to academia. Various startups such as , and , have made significant strides in this direction. There are also a few startups such as , offering services to turn numerical analytics results into readable reports.
,
Model selection and evaluation is not the only arena in Data Science where automation is keenly desired. ,In fact, for underlying data to reach this stage, it needs to first go through data cleaning (also known as ?€?data wrangling?€?, ?€?data munging?€? or ?€?data janitor work?€?). , suggested that data cleaning and preparation take up more than 60% of time on data mining projects. The goal of automatic data preparation is being chased by several startups including , and ,.
,
Besides saving our data scientists from mundane work and enabling non-experts to perform data science tasks, automation also offers the key benefit of performing these crucial tasks with speed, scale and accuracy. This is a core requirement for sustainable analytics. 
,
Will the increasing trend of automation adversely impact the data scientists and their job prospects? It depends. If you are a data scientist who treats his/her work as a typical 8am ?€? 5pm job with well-defined tasks that need to be performed repetitively, you must be scared of automation. On the other hand, if you are a data scientist who loves his/her job, but is bored of non-challenging, repetitive tasks, you would certainly embrace automation. 
,
Bottom-line: ,.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for January 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, but unlike the previous posts which listed the activity chronologically, the list below is sorted by decreasing number of engagements, with font size proportional to the log of engagements.
,
,??,
Here are previous months activities:
,  "



"
Applying analytics to big data requires a mechanism to rapidly get and share data.  For example: when deploying statistical or machine learning models as part of a data product, e.g. in a real-time recommender system, the consumer's initial choices, along with other information about similar consumers and similar purchase patterns, must be transmitted to the analytics algorithm.  Seconds later, the algorithm's output - the recommendation - must be transmitted to the web interface to show the consumer.
,
,
Learn about the standard way of sharing and communicating data among applications and services on the internet in Statistics.com:
,
Mar 27 - Apr 24:
,
,
,
stands for Representational State Transfer, and API for Application Programming Interface.  In this course, students will learn how to write Python code to ingest data from and communicate with RESTful APIs on the web.
,
Learn how to build these same APIs in Statistics.com second course:
,
Mar 27 - Apr 24:
,
,
The instructor, Allen Leis, is a software engineer with over 15 years of experience developing web applications in a variety of domains from the Federal Government to retail. He has deep experience in the production and consumption of RESTful APIs. These APIs power single-page javascript, iOS, and Android applications for the consumption or visualization of data, and are also an important part of the fabric of data ingestion and data quality efforts.
,
Participants can ask questions and exchange comments directly with Allen via a private discussion board throughout the period.
,
Registration and more details:
,
,??,
Why choose Statistics.com courses?
,
,??,
We specialize in personal attention and value pricing. There are no set hours when you must be online, and we estimate you will need about 15 hours per week. This is not a 'MOOC' (massive open online course) -- enrollment is limited, and your instructor will respond to each question that you ask.  "
"
, (,) .
,
R is an incredible tool to do Data Science. Currently, there are more than 6200 packages available in CRAN to be explored (one person would spend around 17 years testing one of these packages a day). R is almost infinite. One of the most exciting and entertaining packages I know is , package, which provides an interface to the Twitter web API. It allows you to download tweets from twitter. The searching function is very flexible: you can search tweet containing particular words, exact phrases, referencing persons, containing some hashtag, from some person, to some person, between two dates, around a place ?€? a detailed description of possible queries can be found ,.
,
Combining this powerful tool with text mining techniques and visualization tools can produce interesting outputs. In this experiment I search tweets talking about American states. The way I do it is looking for tweets containing the exact phrase ?€?[STATE NAME] is?€?. For example, I look for tweets containing ?€?Alabama is?€?. I do the same for all state names. Once I have the result of this searching, I clean and standardize tweets (removing punctuation characters and transforming words to lower case) and I convert the in a Corpus to use text mining functions containing in , package. One of these functions allows me to cross tweets with a list of opinion words obtained from ,. Once I have all the opinions, I only have to summarize them to obtain the two most common words for each state.
,
,
,
This is the time for visualizing results. The maps package allows to plot silhouettes of American states (except Hawaii and Alaska, I don?€?t know why). Combining it with , and , packages I arrange the states and place the 2 words from twitter analysis inside the state boundaries. Not all phrases make sense but all of them reflect a twitter feeling in a particular moment of time. Of course, this is just an experiment to show how to use and combine some interesting tools of R.  If you don't like what twitter says about your state, don?€?t take it too seriously.
,
Here is the table with top 2 words for each state
, 
,
, is mathematician and works as data scientist at Telef??nica. He is the creator of ,, an unclassifiable blog of mathematical experiments and R programming. You can follow him in ,
,
,
,  "
"
,
,
,
January was a really bum start to 2015.
, 
,
The massacre at the Charlie Hebdo offices in Paris last month was a stark reminder of how vulnerable we all are to acts of terrorism.  Whoever and wherever you are in the world, you should never go to work, and not get to go home afterwards.
 ,
As people from all around the world were glued to the events unravelling in Paris, Big Data was thrown under the spotlight.  After all the positive promise, the hype and the countless predictions made about Big Data?€?s role in 2015, we were instead having to debate privacy and specifically the unanswered question of whether or not internet service providers and social media companies should be allowed to encrypt their users data. 
, 
David Cameron (the British Prime Minister), fresh from marching (and jostling) in front of the cameras in Paris in support of freedom of speech, made the unusually contradictory move, upon returning to the UK, of threatening social apps such as WhatsApp and Snapchat by saying ?€?In our country, do we want to allow a means of communication between people which we cannot read??€?
 ,
Critics and ?€?privacy-pro?€? supporters quickly moved to slam Cameron?€?s comments, calling them ?€?ludicrous?€? and even accusing Cameron of ?€?living in cloud cuckoo land?€?. Besides the many breaches of basic human rights to privacy, decryption also leaves us wide open to hackers who want to meddle and do damaging things (ironically, handing even more power to terrorists).
, 
But if he has a point where does that leave companies like Google and Facebook when it comes to their Big Data?
, 

The US & UK government already have the right to make data requests from internet service providers and other social media companies, but if they had their way, Google and Facebook would agree to a ?€?revolving door?€? policy, when it comes to access to their users?€? data, including real time analytics on the conversations, status updates and posts happening across their sites.
, 
,
The Edward Snowden leaks revealed the enormity of wholesale surveillance by government agencies such as the NSA.  From their operational home in Utah (a complex sprawling a head-spinning 100,000 square feet, making it the largest data centre in the world), the NSA are receiving and processing the metadata of around 200 million text messages a day.  These messages are then fed through trained algorithms to detect anything from untoward sentiment to full scale terror plots.
, 
The NSA claim that they identified up to 100 terrorism related ?€?activities?€? since the beginning of their surveillance programme in 2006.  However, due to ?€?classified?€? status, little is known about the details, and only 4 ?€?declassified?€? arrests have been accredited to the NSA.
,
Critics call this the German secret police on steroids and argue that not enough prosecutions have been made to warrant the need for such wide-scaling spying programmes.  Government officials will argue that this is a necessity in the fight against homeland and global terrorism.
 , 
Despite the amount of pressure from government officials in Washington DC, surmounted onto the boys from Mountain View and Cupertino, into giving the NSA more access to their Big Data, there is strong resistance.  In November 2014, the ?€?Reform Government Surveillance?€? coalition was formed between Facebook, Google, Twitter, Apple and Microsoft, in an aid to stop the NSA from hovering mass email and Internet metadata.
 , 
In an open letter sent to the Senate, the coalition wrote ?€?The Senate has an opportunity this week to vote on the bipartisan USA Freedom Act,?€?.  They also added ?€?We urge you to pass the bill, which both protects national security and reaffirms America?€?s commitment to the freedoms we all cherish.?€?
, 
However, with the USA Freedom Act being scrutinized by a congress controlled by surveillance favouring Republicans, it looks increasingly likely that metadata transparency deals with the big Internet companies will become more and more clearer.
, 
,
If we are to see in increase in attacks happening across to the world, similar to the one that happened in Paris, the question companies like Google and Facebook need to answer, is how long can they fight the privacy war for?
, 
As Google puts it ?€?don?€?t be evil?€?, but when it comes to the subject of eavesdropping NSA officials versus plotting terrorists, to whom do they intend it the most?
,
Bio: , is a Partner and Co-Founder at ,, a big thinking recruitment company focused on all things Big Data & Data Science. He is based in Manchester, UK. 
,
,
,
,
,
 ,  "
"
By Gregory Piatetsky,  
,, Feb 17, 2015.
,
I came across this interesting infographic:
,
,, which presents Data Science 
as a union of 5 strands: 
,
,??,
, 
,
I was glad to see in the more recent section the 
,
I organized in 1989.
,
,
,
Through search, I eventually 
,
to a very good presentation
, 
 by Capgemini consultant Mamatha Upadhyaya, given at the Big Data and Analytics Summit 2014 in Hyderabad, India. It has many other good slides besides this infographic.
,
  "
"
Feb 17, 2015. ,, a Cambridge, MA Big Data startup,
introduced today at Strata + Hadoop World San Jose a new version of its scalable data unification platform.  The company also announced the addition of GE, Roche and Toyota as customers.  
,
Tamr uses machine learning and smart social engineering to dramatically improve the process of data preparation and data unification, usually the most time-consuming part of data warehousing and data mining.
,
The new , simplifies and speeds the process to make unified data available for analytics and downstream applications.  The key new features are 
,
,
,
,
,
,
, is a new tool for automatically cataloging in a central, platform-neutral place all metadata available to the enterprise.
The decision makers can now view an inventory of all of the company's metadata organized by the logical business entities, versus only 20% to 30% of metadata enumerated based on how they are stored.
,
,
,
Improvements in scalable data connection simplify and improve data unification, enabling it to be applied to more kinds of data and business problems. Improvements include easier matching of multiple entities by taking into account relationships between them; and enhanced support for Oracle, Hadoop, unstructured data, and user-defined functions via tools such as OpenRefine, Springbok and Data Wrangler. Data analysts and data curators ?€? both seasoned and less-experienced ?€? benefit.
,
,
,
Anyone can enrich spreadsheets or other applications with unified data from across the enterprise.  Via new APIs and a new spreadsheet plug-in, analysts can find, map and match external data with their internal data in Google Sheets or Excel.  Tamr handles the mechanics of data matching and enrichment, prompting users with suggestions and then auto-populating their spreadsheets with new and changed data based on their choices. By being able to manipulate external data as easily as if it were their own, business analysts can use the data to resolve ambiguities, fill in gaps, enrich their data with additional columns and fields, and more. 
,
,
,: 
,
,
provides a simple, scalable way to automatically convert, validate, and package clinical study data according to the latest CDISC standards. Today, organizing CDISC data submission is expensive, time-consuming and difficult to automate. Companies typically spend millions of dollars annually on specialty contractors, who manually convert the data from the proprietary-system files in which it's stored. Tamr understands the systems' input and output formats and controlled terminologies, and automatically converts clinical datasets using the proper definition files.  Future transformations become easier as Tamr learns, enabling businesses to build in-house conversion and integration expertise.
,
,
, enables a comprehensive analysis of procurement opportunities that leverages data across all enterprise systems. Today, procurement data is spread across siloed ERP and supply chain systems, making it hard to do a comprehensive analysis of savings opportunities. Tamr provides a simplified, unified view of supplier, part and site transaction data across the enterprise. It achieves this by (1) referencing each transaction and record across many data sources, (2) building correct supplier names, addresses, ID's, etc. for a variety of analytics, and (3) cataloging an organized inventory of sources, entities and attributes. Customers can now find all sourcing opportunities, including ""long-tail"" opportunities that can often add up to 75% or more of total savings. 
,
Patent-pending technology using machine learning algorithms performs most of the work, unifying up to 90% of different entities. When human intervention is necessary, Tamr generates questions for data experts, aggregates responses, and feeds them back into the system. This feedback enables Tamr to continuously improve its accuracy and speed. 
,
For more information, visit 
,.
,
,
,  "
"
,
,
,
,
If you pay attention to security news perhaps you have heard of the recent discovery of the so called JASBUG. 
,
See, for example
,, 
,, or 
,.
,
By using similarity search and large scale clustering, we discovered one of the worst security vulnerabilities of the past 15 years. In what follows, I explain the context of the issue and the general gist of the solution used to discover the issue. 
,
The Internet Corporation for Assigned Names and Numbers (,) engaged us to research potential technical issues relating to the rollout of new Generic Top Level Domains (New gTLDs) on the Internet. 
,
During the course of the research, we uncovered a vulnerability not directly related to ICANN?€?s New gTLD Program nor to new TLDs in general. Once we understood the seriousness of the vulnerability, we notified the affected vendor and withheld additional disclosure until the vendor addressed the vulnerability. Information from Microsoft relating to this issue is available , and ,.
,
Microsoft has classified this vulnerability , as , This is the most serious rating in Microsoft?€?s classification taxonomy.
,
The vulnerability impacts core components of the Microsoft Windows Operating System. All domain-joined Windows Clients and Servers (i.e. Members of a corporate Active Directory) may be at risk. The vulnerability is remotely exploitable and may grant the attacker administrator level privileges on the target machine/device. Roaming machines ?€? domain-joined Windows devices that connect to corporate networks via the public Internet (e.g. from hotels and coffee shops) ?€? are at heightened risk.
,
Unlike recent high-profile vulnerabilities like Heartbleed, Shellshock, Gotofail, and POODLE, this is a design problem, not an implementation problem, making this type of vulnerability unusual and much more difficult to fix. The fix required Microsoft to re-engineer core components of the operating system and to add several new features. Furthermore, the problem has been present for at least 15 years. 
,
How come this problem was not noticed before? 
,
The answer is simple: nobody was looking at the data that is collected by domain name servers by means of clustering and similarity. Every year, the Domain Name System Operations Analysis and Research Center (DNS-OARC) collects data from DNS nameservers through various means as the annual Day In the Life of the Internet (DITL) collection effort. The data set consists of billions of records that include various measurements including the Domain Name that was being queried, for example: 
,
,
,
Many of these queries will contain randomly generated numbers so for example a query would look like: 
,
,
,
With arbitrarily complex string generation algorithms, it is necessary to extract the gist of the string in the form of a regular expression or equivalent in order to explain a potentially large amount of strings that share similar elements.  In the case of the example above we would like to infer the regular expression: 
,
,
,
Our objective is then to discover overrepresented regular expressions out of the queries that are received by Domain Name Servers. The input is a very large set of strings and the output is an enumeration of regular expressions or a suitable visualization such as the Position Weight Matrices (PWM) that will explain over-represented patterns. Please review 
, for a description of a PWM. A visualization of a PWM looks like: 
,
,
,
Where the X axis (Position) represents a character position in the pattern and the Y axis is the frequency of each character. Each ball contains a character and by looking at characters of similar frequencies it is possible to identify sub-patterns that may not be clear from the regular expression. For example ?€?compatibility-at-addons?€? is a sub-pattern that does not emerge clearly from the original regular expression: 
,
,
,
The technique itself is inspired in Motif Discovery commonly used in Computational Biology to , Transcription Factor Binding Sites patterns. These techniques fundamentally employ sampling methodologies that heavily rely on randomness. The technique works well because they are executed in a constrained set of Genetic sequences. The DITL data-set is at least an order of magnitude larger than the human genome and since the analysis is applied on a wide scale, we must use alternatives procedures that will converge faster. 
,
Similarity search or Nearest Neighbor is the right choice for this problem. Traditionally, similarity search has been considered a very expensive operation specially for metric and pseudometric spaces. During the past 10 years, I have been working on a similarity index that I call ?€?Ramiel?€? that improves other techniques by an order of magnitude (some benchmarks ,). My company , commercializes the technology.  I used my index to cluster the entire data-set (about 2 billion records). The clustering algorithm will return many groups of similar strings. We use a clustering algorithm that discovers an arbitrary number of quality clusters where quality is a function of density and size. The number of clusters discovered is determined in real-time so you do not have to impose a number of clusters. The clustering algorithm will provide a ?€?center?€? for each group, this object is then compared against all the other elements in the cluster by using Smith Waterman Algorithm to align the center and each object of the cluster. This alignment process is then used to build a PWM and from the PWM it is trivial to infer a regular expression. 
,
The process generated many regular expressions, but one particular expression stood out as an anomaly. In fact, several patterns were extracted that contained a similar substring. This lead to the conclusion that there was a DNS query that could be configured hitting the root servers in a very consistent way.  The set of patterns led us to believe that there was a problem in the Active Directory service of Microsoft Windows. Effectively, it was possible to remotely control a client  by performing a man in the middle attack, so when a person that works remotely is trying to login into the company's network, there will be an improper use of DNS during the authentication process. It is here where the client can be attacked and arbitrary code may be executed in their machine. The improper use of DNS was being stored in the DITL data-set but nobody noticed in until we applied large scale clustering and similarity algorithms. 
,
The vulnerability was not noticed in 15 years because similarity search is a computationally expensive resource and it was necessary to first build fast similarity and clustering techniques that scaled to billions of records. Now we have the required performance and this allowed us to create the enumeration of regular expressions that helped us to find the bug. Similarity search has been the key to uncovering a security issue that is bigger than anything we have seen in the past 15 years. From my point of view, a distance function is the single most flexible tool that a data scientist can have at her / his disposal.  
,
, is the founder and CEO of ,, a company that applies metric similarity search to data problems to discover, to recommend and to predict. He was recognized as one of the top innovators under 35 years from his region by MIT Technology Review. 
,
,
,
,
,
 ,
  "
"
,
,
In the age of big data, many companies have accumulated massive, natural language text collections. These companies are responsible for organizing and recommending content to their readers, but they are having trouble keeping up with the rate at which it?€?s produced. The problem is that people don?€?t have time to manually read through all the content, and unstructured text doesn?€?t directly lend itself to analysis via automatic means.
,
At Prismatic, we crawl the web and index all the freshest, high-quality content in order to generate recommendations that people want to read. We have put a lot of thought into how to provide our users with the most relevant recommendations. To this end, we?€?ve engineered the Interest Graph -- a collection of tools for understanding our users?€? interests, content, and the connections between them. The Interest Graph can automatically augment unstructured text with a variety of meaningful annotations that are easily interpretable by both people and machines, enabling high-quality recommendations of products and content. We have published a subset of the Interest Graph API, so now everyone has access to our text analysis tools.
,
,
The first tool we?€?ve released from the Interest Graph can tag a piece of content with topics. Topics are single-phrase summaries of the thematic content of a piece of text; examples include Functional Programming, Celebrity Gossip, or Flowers. By surveying a variety of sources, we have produced a comprehensive list of topics that align with people?€?s interests.
,
Topic tags lend themselves to a variety of applications for organizing and recommending content based on semantic substance. They provide an organization scheme that readers can understand to find content interesting to them. Topics also provide a computationally-friendly structure for measuring similarity -- two pieces of content are similar if they have overlapping topics. You can use topics to automatically detect related advertisements, products, or articles.
,
To supplement the topic tagger, we?€?ve also released the Interest Graph?€?s similar topics tool, which finds the topics most related to a given query topic. Finding similar topics can be used to enhance the applicability of topics. For example, articles tagged with different topics might appear unrelated. However, by first expanding to include similar topics, connections between articles which might otherwise appear dissimilar can be detected.
,
Building a model of user preferences is yet another powerful application of topics. At Prismatic, we model our users?€? interests by analyzing how they interact with content. To get a sense of how content interactions reflect a user?€?s interests, take a look at the figure to see some of the topics we automatically extracted from the links shared on a few celebrities?€? Twitter accounts.
,
,
,
Although tagging text with topics can provide valuable insights, topics capture only a single dimension of content: what it is about. Text is complex and rich; there are many other useful aspects along which it can be classified. For example, these aspects can be structural (listicle, recipe), functional (product review, event description), or suppressive (spam, NSFW). The number of different aspects is broad and diverse -- the Interest Graph infrastructure can scale to accommodate them all.
,
With the initial release of the Interest Graph, you can now , to start tagging, organizing, and recommending your content. Stay tuned for future releases of other tools in the Interest Graph for automatically analyzing other aspects of a piece of content.
,
,
Bio: Dave Golland is a Research Engineer at ,. He got his PhD in Natural Language Processing from UC Berkeley.
,
,
,  "


















"
        By Gregory Piatetsky,  
,, Feb 18, 2015.
,
The 
,, Feb 17-20, San Jose conference is sold-out again, but good news - you can watch the keynotes live on 
Feb 19 and Feb 20, 2015.
,
,
,
,??,
,
,
,
,
,??,
,
,
If the links above do not work, check
,
,  "

"
Most popular 
, tweets for Feb 16-17 were
,
Most Popular Coding Languages of 2015: #Python 31%, Java 20%, C++ 9.8%,  C# 7.4%, Ruby 7.1% 
, 
,
,
,
History of #DataScience across 5 strands: CS, #Data, #Visualization, Math, & #Statistics 
, 
,
,
,
IBM Verse new messaging software will use #Watson to declutter your inbox MIT , 
, 
,
,
,
History of #DataScience across 5 strands: CS, #Data, #Visualization, Math, & #Statistics 
, 
,
,
,  "



"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
, is ,'s Senior Technical Fellow in visualization and interactive techniques. He is pursuing new ways of using visualization for huge amounts of both geometric and non-geometric data.His work with geometric data made Dave a pioneer in interactive 3D computer graphics. He devoted his first 11 years at Boeing to research and development of computer-aided design software. These projects led to pioneering work in interactive 3D graphics, user interface management systems, and industrial use of non-uniform rational (NURBS) solids and surfaces. Dave is pioneering the use of visual analytics to help extract more information from complex non-geometric data. 
,
He earned his Masters in Computer Science from the University of Colorado in 1972 and a Bachelor?€?s in Quantitative Studies from the Johns Hopkins University in 1970. He participates in numerous professional organizations, including ACM (Association for Computing Machinery), which named Dave a Distinguished Scientist in 2007 and a Fellow in 2013; ACM SIGGRAPH, where he started the annual Pioneers student mentoring program and received the Outstanding Service Award in 2012.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: In the world of Big Data, what?€?s interesting to me isn?€?t the exponential growth.  In reality, such growth has been around since computing started.  Computer usage and data storage just seem to exceed the limits of current capacity since I?€?ve been involved with computing.
,
Instead, the notion of Big Data has caused many Boeing staff members to realize that the data itself contains inherent value.  And those staff members rely on analytics tools and visualization, whether encapsulated as visual analytics to help explore the data or as information and scientific visualization to convey concepts.  The limiting factors for the tools themselves in a Boeing context are scalability and performance.  The data often presents a problem, too, because it contains errors, is incomplete, written in anguished English, and contains values only understood by experts.
,
,
,
,In my terminology, data analysis refers to hands-on data exploration and evaluation.
,
,Data analytics is a broader term and includes data analysis as necessary subcomponent.  Analytics defines the science behind the analysis.  The science means understanding the cognitive processes an analyst uses to understand problems and explore data in meaningful ways.  Analytics also include data extract, transform, and load; specific tools, techniques, and methods; and how to successfully communicate results.
,
,
,
,Prof. Bill Huggins first exposed me to computer animation as an undergraduate at Johns Hopkins.  He encapsulated the basic concept behind a film as iconic communication, which caught my interest.
,
Programming computers to generate images as movies taught me how difficult it is to design and build user interfaces.  As I moved from microfilm recorders to highly interactive devices, I learned it was essential to pay attention to both the images and the way people interacted with them.
,
Boeing taught me the essence of performance and scale, which remain challenges. 
,
,
,
,At this point, I?€?m most taken with the promise to move augmented reality into meaningful production.  As I mentioned, moving to a mobile platform that can auto-register an image with 3D data on a lightweight weight mobile device is still being developed.  I do expect to see such progress in the next 2-5 years.  
,
We?€?ll continue struggling with scale and effective interaction techniques.  Doing simple things like typing and selection of fine-grained graphical objects is still challenging on highly mobile devices.  Boeing?€?s factories are noisy enough that voice input still needs work to separate legitimate commands from their surroundings.
,
,
,
,It?€?s easier to seek forgiveness than permission.  This led me to seek implementing advanced new technology without an explicit mandate.
,
,
,
,I look for a couple of things beyond a basic programming ability.  First, I look for an ability to communicate effectively, especially in soliciting problem statements and understanding how to summarize analysis results effectively.  Second, I try to assess a person who is able to move from analysis job to analysis job quickly.  Most engagements are measured in days or weeks, not months or years.  Analysis staff has to adapt to in a fast-pace environment with highly variable assignments.
,
,
,
,I?€?m just finishing , and getting ready to tackle ,.   This Is Your Brain on Music is another good read.  Any Colum McCann novel is great fun.
,
Outside work, which is quite consuming, I?€?m a workout-a-holic and play handball.  My wife and I love going to concerts, baseball games, and traveling.
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,.
,
Gartner, the leading market and technology research firm, has published its 2015 Magic Quadrant for Advanced Analytics Platforms. The report evaluated 16 analytics and data science firms over 10 criteria and placed them in 4 quadrants, based on completeness of vision and ability to execute.
,
,
,
,
SAS, IBM, KNIME, RapidMiner (same as in 2014).
,
,
Dell, SAP .
,
,
Alteryx, Microsoft, Alpine Data Labs
,
,
FICO, Angoss, Predixion, Revolution Analytics, Salford Systems, Prognoz (a Russian company), Tibco Software.
,
The companies added were: Dell (which bought StatSoft), Predixion, Prognoz, Salford Systems, and Tibco Software.
,
The companies dropped were:
,
,??,
It is very interesting to compare 2015 MQ with the last year version - 
,.
,
I overlaid both MQs and added transparency, so you can see both in the chart below, with grayed circles for 2014 entries. 
The arrows connect 2014 and 2015 circles for the same firm, and   
are colored green if the firm position improved significantly (further away from origin), red if the position became weaker, and black if the change was not significant. Only the more important changes are shown.
,
,
,
,
The same 4 companies are in the lead, but SAS has improved its position relative to IBM, and KNIME relative to RapidMiner .
,
,
Dell bought StatSoft and took its position. 
SAP remained in the same place. Angoss, however, fell in its ability to execute and moved from Challengers to Niche Players.
,
,
Alteryx improved its position and almost entered leaders quadrant. 
Microsoft and Alpine Data Labs moved from Niche Players to Visionaries. Revolution Analytics dropped significantly from Visionaries to Niche Players (partly because Python is emerging as a serious alternative to R for Data Science) but has recently been acquired by Microsoft. 
,
Here is also an animated gif with both 2014 and 2015 
Gartner MQ for Advanced Analytics Platforms (made with gifmaker.me - a very nice tool).
,
,
,
You can download the Gartner 2015 MQ for Advanced Analytics Platforms from Gartner, if you are a client, or from 
,, or 
,.
,
,
,
,
,
 ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Feb 24 and later.
,
See full schedule at , .
,
,  "
"
        ,
,
,
,
,
,
,
,  "
"
Most popular 
, tweets for Feb 18-19 were
,
New Face Detection Algorithm to revolutionize search: finding faces no longer unique to humans 
, 
,
,
,
Well written: How to Transition from Excel to R: Intro to #rstats for Microsoft #Excel Users 
, 
,
,
,
New Face Detection Algorithm to revolutionize search: finding faces no longer unique to humans 
, 
,
,
Practical #DataScience in #Python #MachineLearning - nice introduction 
, 
,
,
,
,  "
"
By Gregory Piatetsky,  
,, Feb 20, 2015.
,
Google software engineer Felipe Hoffa recently posted a
, highlighting open datasets on Google BigQuery - once data is loaded there, you can make it public, let others analyze with SQL.
,
Here are some notable 
, (via reddit).
,
,
,
,??,
,
Here is a very good video made by Felipe Hoffa where  GDELT project leader Kalev Leetaru gives a very good example of analyzing events in Ukraine relative to the world, and Amanda Traud, a Data Scientist at L-3 Data Tactics, who uses R and Shiny to explore GDELT.
,
,
,
Other datasets include:
,
,
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,: ,
,
,
,
,
,
,  "
"
,
,Trevor Hastie and Robert Tibshirani, Stanford University
,
Sheraton Hotel,  Palo Alto, California - March 19-20, 2015
,
This 
, gives a detailed overview of statistical models
for data mining, inference and prediction.  With the rapid
developments in internet technology, genomics, financial risk
modeling, and other high-tech industries, we rely increasingly more on
data analysis and statistical models to exploit the vast amounts of
data at our fingertips.
,
This course is the third in a series, and follows our popular past
offerings ""Modern Regression and Classification"", and ""Statistical Learning and Data Mining"".
,
The two earlier courses are not a prerequisite for this new course.
,
In this course we emphasize the tools useful for tackling modern-day
data analysis problems. These include gradient boosting, SVMs and
kernel methods, random forests, lasso and LARS, ridge regression and
GAMs, supervised principal components, and cross-validation.  We also
present some interesting case studies in a variety of application
areas.
,
This course focuses on both ""tall"" data ( N>p where N=#cases,
p=#features) and ""wide"" data (p>N).  Typical examples of tall data are
credit risk and churn prediction, and email spam filtering.  Topics
include linear and ridge regression, lasso, and LARS, support vector
machines, random forests and boosting.  We give in-depth discussion of
validation, cross-validation and test set issues.
,
For wide data, typical examples are gene expression and protein mass
spectrometry data, and data from signals and images. Topics include
clustering and data visualization, false discovery rates and SAM,
regularized logistic regression and discriminant analysis, supervised
and unsupervised principal components, support vector machines and the
kernel trick, and the careful use of model selection strategies.
,
The material is based on recent papers by the authors and other
researchers, as well as the best selling book:
,
,, 
Hastie, Tibshirani & Friedman, Springer-Verlag, 2008
,
A copy of this book will be given to all attendees.
,
The lectures will consist of video-projected presentations and
discussion. Go to the site
,
, for more information and online registration.  "
"
Most popular 
, tweets for Feb 16-22 were
,
Most Popular Coding Languages of 2015: #Python 31%, Java 20%, C++ 9.8%,  C# 7.4%, Ruby 7.1% , 
,
,
,
History of #DataScience across 5 strands: CS, #Data, #Visualization, Math, & #Statistics , 
,
,
,
History of #DataScience across 5 strands: CS, #Data, #Visualization, Math, & #Statistics , 
,
,
History of #DataScience across 5 strands: CS, #Data, #Visualization, Math, & #Statistics , 
,
,
,  "
"
,
,
,
This week on /r/MachineLearning, we see a discussion of the current state of ML research, a new linear algebra course, a network analysis of Reddit comments, a hands-on introduction to deep learning, and a basic introduction to genetic algorithms.
,
,
This self-post (text-only post) decries the use of hyperparameter tweaking as a justification for publishing entire papers. The author sees these papers, which they claim are becoming more common, as being misleading and unnecessary. It seems that many in the /r/MachineLearning community agree, based on the fact that this is so highly-upvoted. There is some interesting discussion in the comments.
,
,
,
This new Coursera course covers basic linear algebra. More importantly, it does so through applications. So if you need to learn basic linear algebra and you find an application-based method best for you, give this one a shot.
,
,
,
This post analyzes the network of Reddit comments over a large dataset. The included visualization is well-made (and very complex), but I fear that complexity makes it fairly difficult to draw general conclusions about the network and Reddit itself. Regardless, the author has some interesting thoughts about apparent patterns in the data, and it?€?s worth looking at.
,
,
,
This step-by-step video explains why you may want to use deep learning, then dives into using deep learning using Python. It?€?s fairly in-depth for an introduction to the topic, so if you?€?ve been waiting for a good hands-on way to get into deep learning, this may be the way to do it.
,
,
,
This article is a great first-exposure to the topic of genetic algorithms. It goes through the structure and motivation for using genetic algorithms. It then details some pseudocode and explains various properties of genetic algorithms through some examples.
,
,
,
,  "
"
Most popular 
, tweets in January 2015 were
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
Sample #MachineLearning solutions with R on #Azure ML Marketplace #rstats , 
,
,
,
Good list of #MachineLearning Resources, #DeepLearning, Graphical Models, Ensemble Methods ... , 
,
,
,  "
"
        ,
,took place in San Francisco, Jan 29-30, 2015.
,
Over 40 experts discussed Neural Networks, Image Recognition, Language Processing, Advanced Deep Learning Algorithms, Artificial Intelligence, Machine Learning, Big Data, and Computing Systems.
,
Here are interesting videos and presentations from the summit:
,
,??,
,
,
More can be found on 
,  "
"
,
,
Plan now to attend Big Data TechCon, April 26-28 in Boston, to learn HOW-TO accommodate the terabytes and petabytes of data from your Web logs, social media interactions, scientific research, transactions, sensors and financial records. Learn how to index, search and summarize Big Data. Learn how to empower employees, inform managers, and reach out to customers.  In short - learn how to Master Big Data!
,
The tangible benefits of Big Data analytics are well known. You likely understand the ""why"" of Big Data. Big Data TechCon isn't a ""why"" conference. It's the HOW-TO conference for Big Data. Practical tutorials. Technical classes.
,
, is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Come up to speed on Hadoop, Spark, Yarn, HBase, R and Hive. Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or JavaScript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes. 
,
Mingle with fellow attendees. Be inspired by keynotes, including Matei Zaharia, assistant professor of computer science at MIT.  Be impressed by the hottest Big Data tools in the Expo Hall.  It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code 
, when prompted.  Register early for additional savings.  
,  "
"
        ,  "







"
By Raluca Crisan (,).
,
Enterprises value data highly;  its benefits include improved marketing, smoother
operations, and even selling this data to third parties.
, 
Data can also be a valuable asset to the consumer, but the barriers individuals face in extracting this value are high - data is spread across multiple platforms, devices and formats; there is no quick and easy way to centralize it, and platform/format data diffusion impedes both its usage, and its sale in a liquid marketplace.
, 
As such, there is a need for personal data analytics, and for data marketplaces that allow consumers to monetize their own data, as they can monetize other personal assets.
, 
,
 ,
While there are some excellent apps out there which provide analytics to users, they are primarily sector specific: e.g.  health, finance, social media, work productivity.
, 
This decentralized approach has the following consequences:
 ,
1) It can leave out patches of data that could provide useful analysis, such as the mundane browsing history or a user?€?s communication history. For instance, at Beagli, one of our use cases focuses on turning a user?€?s communication history into insight that will help users better manage personal and professional relationships.
, 
2) Value is left on the table by not merging different types of datasets (e.g. health and digital or health and finance). At Beagli, we want to offer a central repository for users?€? own personal data from multiple sources (online, social media, mobile, health), and the tools to enable them to use their data (initially BI, with a sprinkle of machine learning). Such tools would help users tease out revealing patterns about their behaviour and routines; this, in turn, should facilitate recognition and understanding of negative and positive habits, and how to break or strengthen them.
 ,
,
, 
,
 ,
In addition to deriving insight from data, another way to extract value is to monetize that data.
At Beagli, our goal was to design a data market-place that allows consumers control over selling their data. Such marketplaces have a few inherent challenges.
, 
,
, 
Privacy and trust are key considerations in this market. Our solution is to give customers choice on whether to sell their data, to which brands they sell it, and which parts of data they want to sell or store.
, 
Pricing is a challenging question as well. We wanted to give users the ability to set a reserve price. However, data needs to be sold in bulk, and reservation prices can be vastly different for different sellers and can pervert seller?€?s incentives. Our approach to this challenge was to constrain the reserve prices users can set to a range of values. This would ensure the market does not get distorted by a few outliers, but the consumer is still given choice.
 ,
Possibly the most important challenge is liquidity. Such a marketplace needs time (and volumes) to develop and personal analytics can act as an intermediate stage. ,  by researchers at Telefonica showed that consumers who understand more about their own data are more likely to value it more. Personal analytics can act as a catalyst and raise awareness in consumers of the value of their data and of what can be derived from it.
 ,
In summary, data is an asset, but consumers do not currently have the means to extract the full value out this asset. Personal analytics and data marketplaces are trying to fill this gap.
 ,
 
Bio: , is a co-founder of Beagli, a platform that enables users to mine and auction their personal data. Previously, she worked in data and analytics at Aimia and built up the loyalty analytics function for a mobile marketing company. Raluca holds a BA cum laude from Amherst College and an MA from University of York.
,
,
,
,
  "





"
,
,
,, MD is Chief Data Officer for , in Danville, PA.  He is a data scientist with expertise in predictive analytics, organizational data strategy, data-driven collaboration, and data science consulting.  He heads the Department of Data Science & Engineering in Geisinger?€?s Division of Applied Research and Clinical Informatics (DARCI) and co-directs the High Performance Computing Center in the organization?€?s Institute for Advanced Applications (IAA).  His specific academic interests include integration of heterogeneous data sources and application of advanced mathematical methods and modeling strategies to generate measurable value for organizations in the healthcare sector and beyond.
,
Dr. Marko is also a practicing neurosurgeon and serves as Geisinger Medical Center?€?s Director of Neurosurgical Oncology.  His clinical practice focuses on surgical management of patients with malignant brain and spine tumors. 
,
Here is my interview with him:
,
,
,
,: This is a broad question, and one that I could talk about for hours!  Let me give you a quick overview.  The , has had an electronic medical record in place since the late 90s, so we have about 20 years worth of comprehensive clinical data available for our very stable patient population of approximately 3 million patients.  Geisinger also owns an insurance provider, the Geisinger Health Plan, through which many of our patients are insured.  This creates a unique opportunity to study both clinical and claims / administrative data, and we are always looking for ways to capitalize on this opportunity to help improve the quality and efficiency of care delivery.  Add to that another 19 primary data sources (operational data, billing data, etc), and you have quite an array of information available for us to use to these ends.
,
In that context, Geisinger is always trying to improve the tool set that we apply to these analytic tasks.  We have had a dedicated EDW in place for about 8 years designed to make data available to clinicians, investigators, and operational personnel.  As more data sources have come online, as the variability of data types and structures has evolved, and as things like unstructured text data, genomics data, and imaging data have entered the mix we have continued to move towards a true ?€?big data?€? stack for data management and analytics.  Our current project in this domain is an aggressive effort to build out an EDW that uses the Apache big data stack (Hadoop, etc).  I anticipate that within one year the bulk of our analytics will be coming from that environment, although our relational database structure will still be very much in the mix.
,
Our analytics are varied and cover basically all parts of the spectrum.  Obviously we support a lot of routine business needs, including thousands of reporting and dashboarding functions that are executed regularly.  The more exciting part lies beyond that, where we are working with newer data types and more modern strategies to try to extract the most value from our data.  ,We have a comprehensive effort in imaging analytics, and our genomics analytic system is benefiting from a new agreement that is helping us generate sequence data on 100,000 patients over the next 2 years.  We are also using more sensor data / IoT type information, including RFID and patient tracking tools, and analyzing this data for predictive purposes (rather than just routine operations) is an active interest of ours.  I personally head our Data Science Department, where we are mixing research with application as we study how the structure and fundamental nature of various types of healthcare data affect downstream predictive analytics, and we keep a very practical component of this in play by trying to readily translate our most interesting results into immediate clinical application.  
,
We also have a dedicated high performance computing environment that we use for the genomics work, the imaging analytics, and some of the more advanced data science applications.  This supports a lot of our machine learning work as well as some of the more computationally intensive approaches to knowledge discovery (graph analytics / topological analysis, etc).  We are also working to hybridize this HPC environment with our evolving big data stack, because I believe that this type of hybrid computing environment will give us the best of both worlds.  So there is a lot going on with data and analytics here at Geisinger, and we've got our fingers in just about everything that?€?s out there at the moment trying to find the best ways to connect this with the world of patient care.
,
,
,
,Again, the question is broad and the position continues to evolve.  One big part of my mission is to get data in the hands of our end-users who need it to do their operational work and also to be innovative., ,
,
Another big part is working closely with our IT folks to help ensure that our infrastructure is the best that it can be from the data use perspective.  IT traditionally focuses a lot on the technical side of infrastructure, and they do that part very well.  But as we move toward a truly information-driven ecosystem it is also important to bring an end-user and patient-centric perspective to those discussions and decisions.  The right architecture gets data to the point of use quickly and efficiently, and we recognize that designing these architectures is not solely an IT function any more.
,
,The third major part of my role is enterprise data strategy.  I chair our EDS Steering committee and work with leadership across our organization to help set a course for how the system conceptualizes, works with, and manages its data.  , ,Geisinger has recognized the value and the importance of its data for a long time, and developing, executing, and maintaining a strong cross-institutional effort in data strategy is a critical part of that commitment.
,

,
,
,As in all institutions, this varies a lot with the individual executives and their preferences for understanding and consuming data.  My part of this equation is to make sure that the best quality, most comprehensive data is always readily available to institutional leaders who need to use this data to make decisions.  This has an architectural component as well as an information access and availability component.  Our efforts in data-driven executive decision-making are helped by the fact that data has been a part of our culture for nearly 20 years.  Our executive leadership is used to asking for the data and to making it an important part of their strategic process, in general.
,
,
,
,
,  "









"
,
,
,, MD is Chief Data Officer for , in Danville, PA.  He is a data scientist with expertise in predictive analytics, organizational data strategy, data-driven collaboration, and data science consulting.  He heads the Department of Data Science & Engineering in Geisinger?€?s Division of Applied Research and Clinical Informatics (DARCI) and co-directs the High Performance Computing Center in the organization?€?s Institute for Advanced Applications (IAA).  His specific academic interests include integration of heterogeneous data sources and application of advanced mathematical methods and modeling strategies to generate measurable value for organizations in the healthcare sector and beyond.
,
Dr. Marko is also a practicing neurosurgeon and serves as Geisinger Medical Center?€?s Director of Neurosurgical Oncology.  His clinical practice focuses on surgical management of patients with malignant brain and spine tumors. 
,
,
,
,
,
Here is third and last part of my interview with him:
,
,
,
,: Healthcare data is often unstructured, sparse, noisy, and heterogeneous.  It contains a lot of latent knowledge and frequently involves ,an important temporal dimension.  We almost never see a complete data table of numeric measurements that directly capture what they are trying to measure.  This can make analyses challenging, but it also makes it a great place to do data science research. ,
,
,
,
,I think the most forward-thinking healthcare analytics experts are starting to look at nontraditional data sources.  I like this, because 99% of a patient?€?s life happens outside the view of the electronic health record. By looking at data that is generated as a part of life outside of the hospital (and there is a lot of it!) we can better understand what makes our patient who they are.  This is key, because good health doesn?€?t happen in a vacuum.  It happens in the context of real life, and that is where we should be looking if we want to improve patient experience and outcomes.
,
I also like the evolution of tools that are particularly useful for analyzing unstructured data.  Graph analytics and other topological methods are particularly interesting, because they help us to see relationships that we might not have previously expected.  Knowledge discovery and data-driven hypothesis generation is critically important to our work, and it is exciting to see algorithms, tools, and systems evolving that can really support this for the first time.
,
,
,
,Innovate or die.  It?€?s that simple.  In this space there is no room for resting on previous accomplishments, no room for the comfort that stability brings, and no role for slowing down.  If you want to be a leader in the data and analytics world then you have to think fast, move fast, and decide fast.
,
,
,
,I will start by clarifying that when I say ?€?data science?€? I mean data science.  Not data architecture, not computer systems engineering, not information management, and not data engineering.  Those are all different jobs with different requirements that require different skill sets. , ,
,
I look for two general types of data scientists, but admittedly these types overlap (and overlap a lot in the best data scientists).  The first are programmers.  By this I mean real programmers ?€? they speak multiple programming languages, they would rather be on the command line than anywhere else, and they almost refuse to turn on a box that is running Windows or Mac OS.  They can move information from any language to any language, and they understand when and why it is appropriate to do so.  There is often a big distinction between a ?€?computer scientist?€? and a real ?€?programmer,?€? and I will take the latter every time.
,
The second type are mathematicians.  When I look for people to solve problems with data, build data models, answer questions, and really innovate in the algorithmic space then I want someone who knows mathematics inside and out.  I will go for either theoretical or applied math people, depending on how they ,conceptualize the work that they do.  Occasionally someone with an engineering or physics background can fill this role, but these are usually the kind of engineers or physicists who are really mathematicians who were scared that they couldn?€?t make a living from doing math.  Either way, these are the people who write more in greek symbols than they do in their native language.  They usually have a pile of books on their desk, a white board full of writing on the wall, and an inherent aversion to small talk.  They?€?re awesome people with awesome minds.
,
I generally avoid more applied disciplines, like computer science, operations research, or some of these new ?€?data science,?€? ?€?informatics,?€? or ?€?predictive analytics?€? type degree programs that have surfaced lately.  Not saying these are bad ideas, just that they usually aren?€?t a good fit for the kind of data science that my group does.
,
,
,
,I recently read the biography of Dirac (by Farmello) which I thoroughly enjoyed.  When I?€?m not working I read, I travel as much as I possibly can, and I do some photography.  I also like anything to do with tech gadgets.  And I?€?ve recently started scuba diving again, which is basically a vehicle for playing with tech gadgets under water.
,
,
,  "
"
,
Today we look at IBM's Big Data & Analytics Heroes. These are people that have lead the way in big data analytics. This program was established to promote leadership in the field, and the people included in the list come from various backgrounds. Here is the list in alphabetical order:
,
,
,
Chief Analytics Officer, Royal Bank of Scotland
,
,
,
Founder & CEO of Findability Sciences
,
,
,
Professor of Sports Management, Menio College, Senior Analytics Consultant and Data Engineer, Cleveland Cavaliers, Author
,
,
,
Chief Executive, Advanced Performance Institute
,
,
,
Director, Outcomes Analytics, UVN
,
,
,
Sr. Vice President, Marketing Technology and Data Insights, Zions Bancorporation
,
,
,
Founder, Retail Prophet
,
,
,
Director of Advanced Analytics, Liberty Mutual Insurance
,
,
,
Analytics/Data Mining Expert, Editor of KDnuggets
,
,
,
Founder and Executive Director of DataKind
,
,
,
Vice President, Benefits and HR Metrics and Analytics, Macy's
,
,
,
Chief Data Architect, Nielsen Company
,
,
,
PhD, Data Scientist, Top Big Data Influencer, Professor of Astrophysics & Computational Science, George Mason University
,
,
,
VP, Business Intelligence, Denihan Hospitality Group
,
,
,
Technical Leader of Predictive Analytics, Ford Motor Co.
,
,
,
CEO at DATASKILL
,
,
,
Chief Analytics Officer, UPMC Insurance Services Division
,
,
,
Information Delivery Team Lead, Canadian National Railway
,
,
,
Managing Partner, Information Management Executive Consulting Services LLC
,
,
,
Senior Scientist, Johns Hopkins University Applied Physics Laboratory
,
,
,
Corporate Technology Officer, Centerpoint Energy
,
,
,
CEO and Co-founder, nViso
,
,
,
Professor of Internet Governance and Regulation, University of Oxford
,
,
,
Partner at Ernst & Young
,
,
,
President, McKnight Consulting Group, Author of ""Information Management: Strategies for Gaining a Competitive Advantage with Data""
,
Original slides ,.
,
,
,
,  "
"
By Steve Mills (Booz Allen).
,
In conjunction with Data Science Innovation Day on January 22, 2015, the Booz Allen data science team decided to host its first-ever Twitter chat. In the span of an hour, more than 170 Twitter users from around the world started asking and answering questions or otherwise engaging and sharing their perspective on the industry and the work of the data science community.
, 
Topics included defining data science, making predictions about the future of the field, and addressing some of the challenges faced by data scientists. As both a participant and a ?€?listener?€? in the chat, I decided to share my 5 key takeaways;
,
Data science comes in all shapes and sizes. In talking about the impact of data science to-date, the range of answers we received demonstrated the diversity of how and where data science is applied. I shared a few simple, everyday applications, like the creation of a model for the seating arrangement at my wedding reception and a method to align the hardwood floors in a kitchen. Others talked about medical advancements that wouldn?€?t be possible without data science, such as algorithms for prioritizing vaccine aid and the ability to detect and predict seizures, and environmental solutions such as assessing forest fire damage and the impact on the field of oceanography. , even mentioned using data science to optimize the learning process for children. Absolutely amazing!
, 
We need more data. Compared to six months or six years ago, the amount of data that we have available at our fingertips is staggering. But, we can do even more if more data is made available to us. As , pointed out, it doesn?€?t just need to be government data. It can be any type of data, as long as it?€?s open, transparent and accessible. Perhaps the consumer will be the one giving us that data, if , vision for an opt-in culture for data collection and analytic transparency comes to life.
, 
Keeping up with the (data) Jones?€? is hard. One of the most exciting things about working in a field like data science is that there?€?s always something new happening ?€? a new tool launching, a new code being written or a new layer being added. While the industry pushes forward at an incredible rate, we are forever challenged to stay abreast of the latest developments. Interestingly, every single participant, from across fields and business sectors echoed this same sentiment. The acceleration of technology is happening so unbelievably fast, we need new processes to monitor, innovate, and keep pace.
,
Data scientists aren?€?t just computer nerds. Quite the opposite, actually. While many of us have taken more calculus classes than we can count, we?€?re also tapping into new ways of thinking via artists, musicians and creatives.  In fact, we would argue that being data scientist requires a mix of art and science, right brain and left brain. , refers to it as ?€?the three Cs?€?  -communications, curiosity and creativity. 
,
,
, 
Behind every Twitter chat is a great data visualization. Being data scientists, of course, the community had to gather the findings from the Twitter chat and develop a , (thanks ,). 
,
What it shows is that this relatively small community, while growing, is extremely connected. We may not have identified ourselves as data scientists five, ten, even fifteen years ago, but we were building the human foundation and network for what technology is allowing us to do today.
, 
Thanks to everyone who participated in the Twitter chat, including , from , who broke his 30-year vow of Twitter silence to take part.
, 
To check out the full conversation, search , on Twitter.
,
, is a senior associate at Booz Allen Hamilton with expertise in Operations Research, Modeling and Simulation, and Data Science. At Booz Allen, Mills leads a team of data scientists in the firm?€?s Strategic Innovation Group, working across the defense, health, and financial services markets to help clients solve their toughest challenges. 
,
,
,  "
"
Most popular 
, tweets for Feb 23-25 were
,
Microsoft is building fast, low-power neural networks for #DeepLearning with FPGAs, use 10% of GPU power #BigData 
,
,
Livestream Feb 25, 6:30 pm PT Stanford Prof Trevor Hastie on Sparse Linear Models #DataScience 
, 
,
,
,
10 Most-Funded #BigData #Startups, as of Jan 2015: ,, 
,, 
,, 
,, 
,  
,
,
5 lucrative tech careers in 2015: 1. Data Scientist ($150K), 2. Data Engineer ($148K) 
, 
,
,
,  "
"
,
,
,, MD is Chief Data Officer for , in Danville, PA.  He is a data scientist with expertise in predictive analytics, organizational data strategy, data-driven collaboration, and data science consulting.  He heads the Department of Data Science & Engineering in Geisinger?€?s Division of Applied Research and Clinical Informatics (DARCI) and co-directs the High Performance Computing Center in the organization?€?s Institute for Advanced Applications (IAA).  His specific academic interests include integration of heterogeneous data sources and application of advanced mathematical methods and modeling strategies to generate measurable value for organizations in the healthcare sector and beyond.
,
Dr. Marko is also a practicing neurosurgeon and serves as Geisinger Medical Center?€?s Director of Neurosurgical Oncology.  His clinical practice focuses on surgical management of patients with malignant brain and spine tumors. 
,
,
,
Here is second part of my interview with him:
,
,
,
,: This comes down to consistent messaging and to solid change management.  Of course there are some decision-makers who are used to relying on intuition and experience, and those practices aren?€?t going to change just because data is available.  Instead, I think it is a matter of making sure those people get a consistent message that the data is available to help them augment their decisions.  The process also involves learning how those people like to consume and to interact with data.  , , Through consistent messaging, easy availability, and tailored analytics we work to ensure that data is a big part of every key decision.
,
,
,
,Much the same as my answer to #4.  Make sure that the data is available and is easy to interpret.  Make sure that it is a part of every key discussion, and work to get a ,data advocate at the table whenever important decisions are being made.  It is also important to foster a culture that is constantly asking ?€?Why are we doing that??€? or ?€?How do we know that this is the best approach??€?  Data is there to answer questions, but if the corporate culture values blind assent over constant questioning and reassessment then there will never even be a space in which the use of data can grow. 
,
, 
,
,There are a variety of reasons.  One is that data and information is not the primary business of healthcare ?€? taking care of sick people is our primary focus.  Certainly information is an important part of how we do it, but when push comes to shove decisions about patients always get ,prioritized over decisions about data.  And that is a good thing, because it allows us to excel at what we are supposed to be doing.  However, it means that sometimes healthcare organizations don?€?t have as much time or resources as they would like to be focused on the data aspects of what we do.  This is particularly true in an environment of rising costs and declining reimbursements, because almost anything that doesn?€?t directly touch a patient can easily find itself on the chopping block.  We are addressing this by trying to focus on those parts of data and analytics that really help us meet our primary goal ?€? patient care ?€? in a more effective and efficient manner.
,
Another reason is the medical culture in general.  ,
,
There are a lot more reasons and we could talk for hours about this, but overall I?€?d say it?€?s getting better.  A newer generation of clinicians and business leaders are emerging, and these people have been raised in a data- and information-centric culture.  That is bound to spill over into the workplace ?€? even into an academic establishment with a tradition as long and proud as medicine!

,
,
,
,This tool is just one example of the many predictive modeling exercises that we use to try to improve the ways we take care of patients and keep the system running efficiently.  In this particular case I think we see how important information can be that is not traditionally stored ,in an electronic medical record.  Trying to predict what clinical volumes will be at any given time, who will show up and who will not, and what systems can be implemented to give patients better experiences with healthcare is largely dependent on how our patients behave under certain circumstances and how they perceive and interact with the world around them.  There is a lot more to that than what is recorded in a medical chart, and so we?€?ve really tried to work on incorporating ?€?ambient?€? data sources that are not traditionally part of healthcare analytics.  In the end it is about knowing our patients as well as we can so that we can make their interactions with the system efficient and positive.
,
,
,
,
,  "
"
        ,  "
"
,
,
Recently, The Insight Centre for Data Analytics (an Irish research initiative) announced its intentions to establish a ?€?,?€? that attempts to balance the privacy worries of the masses with the interests of big business. ?€?Existing ideas of ownership and privacy are not relevant in the data space,?€? Oliver Daniels, Insight CEO, said.
,
In an ?€?Always On Society,?€? where nearly everything we do is or soon will be tracked, establishing some ground rules is a good idea. We are all part of what I call the ?€?Internet of People?€? ?€? a network of entities participating in the digital environment and defined by our data and models which analyze that data. But there is no Bill of Rights or Magna Carta yet for how the data that defines us can be used.
,
The debate will largely hinge on one major issue: Who benefits from the use of the data? Is it used to improve society as whole? Or are we using it to criticize, penalize, and discriminate based on these new sources of information?
,
,
,
,
Take the scenario where cars become data transmitters. What if I speed and my car relays that information to the police? Would I have to give permission for that to happen? Maybe I trust the police ?€? but what about my insurance company? Do I trust it to not raise my premiums every time I go five miles over the speed limit?
,
Perhaps the data from my car could contribute to some social good. Perhaps it can be used to create more efficient traffic patterns, or to optimize crosswalks to increase safety for kids walking to school. As a driver, I have to weigh that potential good against the potential cost to me personally if my insurance premiums increase.
,
Today, individuals are making the choice about whether to transmit data on our driving habits to our insurers. But how long will that last before we decide this should be an ?€?opt-out?€? system, not an ?€?opt-in?€? system? If the risk of rising premiums makes us better drivers, why should we leave it up to the driver?€?s discretion?
,
,
,
This goes to the trade-off between the benefits of data sharing and the question of privacy. If it?€?s my data, don?€?t I own it? And shouldn?€?t I be the primary beneficiary of its use?
,
There are several personal information management (PIM) companies cropping up, such as It?€?s Really About Me, that would argue yes. But, maybe it?€?s not that black and white. If I?€?m driving on a public road, maintained by the state, then perhaps my clocked speed is not entirely my private data.
,
,
The privacy debate is even more meaningful in the case of something personal like medical data. Sharing it with one?€?s doctor is crucial to receiving proper medical care. It makes sense that we want to incentivize disclosure so that individuals can get earlier and less-expensive treatments and learn better habits sooner.
,
But many medical conditions cannot be helped with behavioral changes. We can?€?t change our DNA, or our family?€?s cancer history. The same data that help your physician return you to health could also prevent you from getting life insurance, or make it more expensive. Does sharing it with your doctor but withholding it from your insurer make you liable?
,
,
Perhaps the thorniest issue around the Internet of People involves predictive analytics. If Big Data is being used to predict behavior, are we punishing people for things that they haven?€?t yet done ?€? and may never do, or precluding them from the opportunity to do? This violates the fundamentals of the legal code ?€? and yet, from credit risk analytics to health risk analyses to predictive policing, the practice is growing.
,
,
,
What?€?s clear from this discussion is that the ground rules are still completely unclear. As a society, we have not decided how to balance the rights of individuals, businesses and society when it comes to data sharing. But as FICO has been thinking about this for a relatively long time ?€? nearly 60 years now ?€? let me offer what I believe are the four key practices for anyone storing, sharing or analyzing data:, 
,
,
Following these four principles will help the industry and its participants navigate the stormy debates to come over data use and the Internet of People.
, 
,: Dr. Andrew Jennings is FICO?€?s chief analytics officer and head of FICO Labs. He has held a number of leadership positions at FICO since joining the company in 1994, and was a lecturer in economics and econometrics at U. of Nottingham. He has a BA and Ph.D. in economics, and an MSc in Agricultural Economics. He blogs at ,.
,
,
,  "
"
WALTHAM, MA - Brandeis Graduate Professional Studies will host 
,
on April 8, 2015 at Brandeis University in Waltham, Mass. The all-day symposium will focus on promoting a discussion of the growing field of analytics and how organizations can leverage big data to make more strategic decisions. 
,
,
,
Panelists will engage in a conversation that places analytics in the context of big data, education, health, marketing and business. Sessions will cover a wide range of perspectives within the analytics field, from 
,, to 
,, to 
,.
,
,
Thursday, April 8, 8 a.m. to 4:30 p.m. 
,
,
,
Hassenfeld Conference Center, Brandeis University, Waltham, Mass.
,
,: Users can click 
,. Pricing is $265 for the general public and $135 for NERCOMP members. Members of the Brandeis University community should 
,
to learn more about special pricing.
,
,
Panelists include professors, leading executives, and researchers. The keynote speaker, Dr. Robert Carver, is an award-winning Professor of Business Administration at Stonehill College as well as Adjunct Professor at the International Business School at Brandeis University. Dr. Carver specializes in applied quantitative methods, big data, statistics education and business analytics. He will speak on the ethical dilemmas of big data in analytics. Other sessions include 
,, led by Leanne Bateman, Faculty Chair for Strategic Analytics at Brandeis University and Principal Consultant for Beacon Strategy Group, a Boston-based management firm specializing in project management services.
,
Interested participants can contact , or 781-736-8786 for more information. 
More information about Brandeis GPS is available at ,.
,
,
,Brandeis GPS offers eight fully online, part-time master's degrees, and is dedicated to developing innovative programs for working professionals. Brandeis University combines the breadth and scope of a world-class research university with the intimacy and accessibility of a small liberal arts college. Consistently ranked among the nation's best universities, Brandeis is widely recognized for the excellence of its teaching, the quality and diversity of its student body and the outstanding research of its faculty. As a leading research university and member of the prestigious Association of American Universities, Brandeis fosters self-motivated, curious students ready to engage new experiences and equipped for global endeavors.   "
"
,
,
,
,
,
,
,
The first wow was that Spark seems to have captured everyone??s
imagination. Mathei Zaharia of Databricks gave a detailed talk on Spark
Streaming. 
,
,
It surely sounds like Spark Streaming is production ready and
for the most part better than , 
or , based on my consensus
assessment.  Biggest limitation?  Its processing mini-batches results in
seconds of delay which I imagine is a non-issue for most real-time
analytics since there is no user explicitly waiting for the service.
Features include 100K+ records per node per second in throughput (much
better than Storm), exactly-once semantics, and code and developer
proficiency reuse from your batch processing.
,
,
Currently, Spark has the largest community of contributors of all other
Apache projects. One example of how Spark outperforming Hadoop I found
illuminating. On disk sort of 100TB it took Hadoop 72 min on 2K machines
while it only took 23 min for Spark on 200 machines.
,

Second, all the big guys seem to be standardizing on 
, as a universal
message bus.  Real-time analytics?  Kafka + Spark Streaming, especially
with their new API in Spark 1.3 (or Samza if you are a real LinkedIn fan).
,
Search, sending data to DBs, populating anything from news feeds to
monitoring dashboards, processing and re-processing data (yes, dataflow
graphs can have cycles!) ?? everything goes through Kafka.
 ,
Third, Netflix uses an S3 bucket in front of an HDFS as they do not believe
in being able to reliably pipe event data into HDFS directly.  This also
allows them to spin clusters up and down on demand or failure using Genie. 
,
Fourth, I really enjoyed , talk.  If your data dictionary (a
list of data fields) is long and most of your queries operate on a small
number of them as they often do, it pays to use a columnar data format and
get a field or two for all users rather than de-serializing and parsing
through loads of unneeded data.
, 
Fifth, some like their data like good sushi:  fresh, raw, and ready to
eat.  Disclaimer:  this talk was right before lunch, but no matter.
Should you pre-cook your data using complex ETLs and data models or do
work at query time?  The latter sounds simpler, that??s for sure.  And if
you sort, partition, and compress/serialize (yes, Parquet!) your data to
make common computations fast, you just might get the best of both worlds.
 Perhaps, it's an acquired taste.
,
Last, , is your data play tool.  
If you have bits and pieces of your
data in spreadsheets, DBs, HDFS and god knows where else, you can still
join all of this good stuff in an ad-hoc query right off the bat.  That
thing will even guess the data schemas on its own.  Natural intelligence?
,
The photo below shows the demand for Data Scientists - the jobs board was overflowing with ads.
,
,
,
Most keynote presentations concentrated on the opportunities in Big Data
space. The age of ""internet of things"" is upon us. Personalized medicine,
revolution in wearable devices, interesting applications of knowledge
graph are just some of the topics that were briefly touched by several
speakers including Lisa Hamill of Salesforce, Anil Gadre of MapR, 
Adam Kocoloski of IBM.
,
,
DJ Patil, US government first chief data scientist, talked about
establishing data-driven culture, described his vision for the role of CDO
(Chief Data Officer) in organizations.
He urged attendees to explore datasets released by the federal government
and build data-driven products using this data.
,
Prof. Poppy Crum from Stanford talked about her research at Dolby Labs in
the context of Big Data. Her work is focused on sensory ambiguity
resolution and has applications in immersive gaming.
,
Jeffrey Heer, professor at University of Washington and the co-founder of
Trifacta, gave an interesting talk on visualizing different variations of
data instead of focusing on a particular design decisions. He gave an
interesting example from a medical application how right visualization can
help important discoveries. He also talked about the need to migrate from specialized designer tools
to the tools that enable decision makers.
,
Bios: ,
, is a Sr Data Scientist at Ancestry.com working on Search Platform and other
data-driven projects since July 2013. Previously, he was a software engineer for more than a decade working on software development tools at Rational Software, IBM and on Content
Management Server at Interwoven Inc. Sukharev is currently a PhD Candidate
in CS at UC Davis, and his interests include data visualization and machine learning.
 ,
, is an entrepreneur, an executive, and a data scientist, working on next generation healthcare analytics.  He is a co-founder of GetGoing, a profitable Y Combinator travel technology
company, the only travel company to make Time's top 50 websites in 2013.
Ilya received his Ph.D. in statistics at Stanford.  He is an author of 17
top professional journal publications and an inventor on 21 patents.
,
,
,
,
,
 ,  "
"
,, The IEEE International Conference on Data Mining, will be held Nov 14-17, 2015 in Atlantic City, NJ, USA.
,
We invite proposals for organizers of the ICDM 2015 Data Mining Contest. The Data Mining Contest is an integral part of the IEEE ICDM conference and provides an opportunity for teams of scientists and domain experts to compete in order to develop data mining techniques for real-world applications.
,
Proposals should contain the following information.
,
,
,
,
,??,
,
,The organizers will also have an option to organize a special issue on the ICDM contest in the 
,.
,This option is tentative since it will depend on the number and the quality of the contest submissions. If the organizers plan to organize the special issue, it should  be clearly indicated in the contest proposal.
,
,
,
,??,
,
,
Takashi Washio (Osaka Univ.) and Chandan Reddy (Wayne State Univ.)  "

















"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "

"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Mar 10 and later.
,
See full schedule at , .
,
,  "
"
,
,
In March 2015, Paris ?€? the city of inspiration ?€? will become the central hub of NoSQL and Big Data community in Europe.  On 26-27 March 2015, NoSQL matters will welcome you to our , of great fun and severe tech nerdity. This time in Paris!
,
An informative training day will be held on , at the offices of our partner ,. An interactive conference will take place at the , on ,.
,
You will have an opportunity to network with leading NoSQL experts from all over the world, enjoy mind blowing talks or simply have loads of fun hacking away with other participants. Our national and international speakers cover a large amount of topics with different focus and various difficulty levels.
,
Get inspired by a keynote speech of ,. Followed by an enlightening presentations of Nati Shalom. 
Listen to , himself. Join , for his talk. Meet ,. ,
And enjoy so much more geekery!   ,
,
Find out the latest developments in the NoSQL field: zoom in on technical background and algorithmic innovations, from consistency models to geo-indices, and from CAP & BASE to Big Data. During the conference you will excessively upgrade your knowledge on current industry trends. You will get in touch with the leading NoSQL companies, and, of course, have a great time networking with the other fun-folks from the industry.
,
Good news ?€? the tickets sale is on!  ,
,
We have a , code for all members of our community: ,
,
The code is valid for one-day Regular conference (27-03-2015) tickets and for two-day Regular Combi (training day 26-03 + conference 27-03) tickets  ,
,
Or register using ,.
,
,
,  "
"
        ,  "
"
,
,

The Beatles sang ?€?All you need is love.?€?  If you are a data enthusiast you can sing a slightly different melody: ?€?all you need is knowledge.?€?  Open source is what paves the road to that knowledge and the 
,
, (Boston, May 30-31, 2015)
 is a stop along the way, but more about that later.
, 
Note that I choose the phrase ?€?data enthusiast?€? carefully since we range from full-fledged data scientists to software engineers to those who just dabble in data. So regardless of where you rank in that range, read on!
, 
Thanks to open source we are firmly in an era where it is not necessary to run costly software to work with data.  There are plenty of free open tools and languages that you can employ.  Think R and Python, which are stacked with rich data science packages and libraries.  Languages without strong libraries are like planes without engines: plenty of structure but no power. Now add to these languages tools like Hadoop, Spark, and Elastic Search to name a few and you are flying... data wise 
, 
What about data science topics?  That?€?s a part of the open revolution too. The days of black box models are long gone and most models and techniques are openly published.  There are so many available resources to teach you about predictive models, text analytics, machine learning, deep learning or whatever it is in the domain you are interested in.
, 
Another important tool in your data science tool kit is what?€?s right between your ears: your brain and the knowledge it holds. To expand that knowledge we want to build an inclusive, accessible conference.  Enter the Open Data Science conference.  It?€?s a two-day knowledge-dump packed with over 70 talks and workshops about the languages, tools, and topics of data and data science.
, 
We have the likes of, 
-  Wes McKinney who created Pandas for Python;,  
- Tal Galili, the data scientist who founded those wonderful R fountain-of-knowledge sites R-Bloggers and R-Statistics; , 
- Anthony Goldbloom, who founded and runs the open data science competition site Kaggle.  , 
, 
Speaking of Kaggle, we also have Owen Zhang, the #1 ranked Kaggle rank data scientist, a guy who, like all our speakers, is more than generous in sharing his knowledge.   For a full list of speakers and discounted registration please visit , .  Hope to see you there and?€? share the knowledge!
,
,
,  "
"
        ,  "
"
,

,

,One of the biggest challenges at big conferences such as , is that there is so much happening quickly and simultaneously that it is almost impossible to catch all the action.
,
We help you by summarizing the key insights from some of the best and most popular sessions at the conference. These concise, takeaway-oriented summaries are designed for both people who attended the conference but would like to re-visit the key sessions for a deeper understanding and people who could not attend the conference.

,
,
,
Highlights from Day 2:
,
,

,

, presented a new standard to protect all data including open data: "","" -> Data that self-contains properties that allows to secure its confidentiality and integrity. Data that comes with a built-in security contract between its producers and consumers.

,

, mentioned that as Big Data turns into a consumer product we should ensure that nobody should know more about our life than we do. He said ""When machines get intelligent, we might not even notice, because they'll be us and we'll be them.""

,

, announced Intel's collaboration with Databricks and AMPLab, UC Berkeley to accelerate spark analytics.

,

, talked about Project Cybersyn and Big Data lessons from our cybernetic past.

,

, mentioned that Spark 1.3 would include Spark Dataframes, which are automatically optimized; thus, making it faster than Python and Scala. He also stated that Spark is the most active project in Big Data space as well as overall at Apache Software Foundation.?? He also announced that Spark 1.4 will be released in June this year and would include R Interface - ""SparkR"".

,

, gave a fascinating talk on ?€?the Connected Cow?€? showing that data is the true power and every company is a data company. He said ""It all starts, not with data & analytics, but with an idea - a question""

,

, emphasized that visualization tools should show data variation, not design variation. Future of data visualization would rely on how we move from tools that work well for designer to tools that enable analysis and help better-decision making in industry and government.

,

, talked about what Netflix data platform team is up to and why. They are particularly focused on performance and ease of use. They?€?ve recently upgraded to Hadoop 2, partnered with the community developing Pig on Tez, adopted the Parquet file format, and fully integrated Presto into their stack. They are using Presto over other alternatives because it goes well with AWS and it is open source developed in Java providing real-world Big Data solution. Based on their experience good features about Spark are: cohesive environment, multiple language support, performance and a great community. However, they realized that it still lacks: maturity, multi-tenancy/concurrency, shuffling, etc.

,

They use Parquet file format since it is columnar, works well cross-project and has a great community with strong contributions. They are continuously adding to their big data open source suite, with their latest contribution being Inviso (which provides easy searching and visibility into Hadoop execution and performance). They are working on developing a cohesive framework for easy platform interaction (via their Big Data??API??and Big Data Portal).

,

, shared insights about tuning and debugging a production Spark deployment. Talking about Spark's execution model, he said that the key to tuning Spark apps is a sound grasp of Spark's internal mechanisms. In order to explain how a user program gets translated into units of physical execution such as jobs, stages and task, he started with explaining RDD API.

,

RDDs are a distributed collection of records. Transformations create new RDDs from existing ones and Actions materialize a value in the user program. Basically, Transformations build up a DAG and are materialized through a method runJob. He shared a stage graph in order to show how two stages performs their tasks and share data. He briefly explained units of physical execution:
,
,

Talking about performance, he mentioned that in general, avoiding shuffle will make program run faster. Regarding parallelism, he mentioned that if one is not using all slots in cluster then repartition can increase parallelism, otherwise not. Serialization is sometimes a bottleneck when shuffling and caching data. Using the Kryo serializer often makes the code run faster. Since Spark scales horizontally, so more hardware is better. He recommended to keep executor heap size to 64 GB or less.

,

,shared lessons learned from building anomaly detection systems for three operational systems. He mentioned that anomaly detection has three common cases: supervised, unsupervised and semi-supervised learning.

,

For supervised learning: We have two classes of data, one normal and one consisting of anomalies. This is a standard classification problem, perhaps with imbalanced classes and is a well understood problem. However for unsupervised learning we have no labeled data representing anomalies or normal data. And for semi-supervised we have some labeled data available for training, but not very much; perhaps just for normal data.

,

He mentioned that ""The core of most large scale systems that process anomalies is a ranking and packaging of candidate anomalies so that they may be carefully investigated by subject matter experts."" He then presented various use-cases. He concluded the talk giving three solutions when we have a lot of non-labeled data:

,

Approach 1:
,
,

Approach 2:
,
,

Approach 3:
,
, ??,
,
,  "
"
,
,
,, Ph.D. is Senior Vice President and Chief Analytics Officer at ,.   Dr. Akmaev leads innovation in Big Data analytics applied to fundamental patient management problems in healthcare, drug development, and diagnostics. During his tenure at Berg, Dr. Akmaev has developed and launched Berg Analytics Suite of data science applications that allow the life scientists and clinicians to harvest the power of Big Data and affect real world patient outcomes. He works closely with the drug and biomarker development teams and directs research informatics, healthcare analytics, and personalized medicine programs within Berg and its subsidiary companies. 
,
Dr. Akmaev continues to innovate in the application of Bayesian artificial intelligence in healthcare IT. Prior to joining Berg, he was Vice President of Scientific Affairs at a Big Data analytics company, Scientific Associate Director at Genzyme Genetics and a Bioinformatics lead at Genzyme. Dr. Akmaev holds a Ph.D. in Applied Mathematics from the University of Colorado at Boulder. 
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: We have completed numerous projects within Berg, and in collaboration with other organizations, and every project is remarkable in its own way. One of the more recent case studies that really excited us was the analysis of publically released CMS billing data. We looked at a high level data from 2011 of top 100 billing discharge codes across the largest providers in the U.S. This case study proved to us that hypothesis-free, data driven approaches have merit and are especially viable in Big Data. 
,
,
,
,As you pointed out in the question, I think the most challenging aspect of working with research molecular and clinical data is the transition from data to clinical utility. We as an industry have been living in the genomics era for more than 15 years. There are thousands of microarray data sets and more than a million gene expression microarray experiments on GEO. There are other publically available omics data linked to extensive clinical information. Again, the healthcare industry is still struggling to make the leap from data to actionable clinical utility. This is where Berg is. We hope to make a difference in creating socially impactful programs from Big Data in healthcare.
,
,
,
,The Healthcare Analytics industry is rapidly growing. I have seen announcements from dozens of start-ups in the area of healthcare analytics in the last several months - from nurse room dashboard software to complex machine learning and AI based approaches applied to longitudinal patient data. Large corporations are getting involved as well. GE Healthcare is investing heavily in healthcare analytics, IBM is training Watson in clinical oncology, and I am sure Google and Apple are up to something in mobile health and precision medicine. What will continue to drive the growth and sustainability is real world application and utility. Every day, my team at Berg Analytics looks beyond the volumes of data and the complexity of the analysis. Often times the most useful answers are not the most complex. The critical success factor for Healthcare Analytics is our ability to positively affect real world patient outcomes.
,
,
,
,The best advice I had in my career, and I am happy to give it to anyone working in healthcare, was to take the time to think about your work having an impact on the patient. Irrespective of the project type and area of the healthcare and pharma/biotech industry you work in, have the patient and the patient benefit come first.
,
,
,
,Outside of basic qualifications that are common in the analytics and bioinformatics space, I look for bright, self-motivated and driven individuals. We have positions open in Analytics for anyone fitting these criteria. Please apply on our website.  
,
,
,
,For better or for worse I mostly read scientific and medical literature. However, I can highlight a book that recently captured my attention, The Great Prostate Hoax, by Dr. Richard Ablin. I would recommend that book to all men in their 40?€?s and older. It is a compilation of a tremendous amount of information on the history and the present state of urology as it relates to prostate cancer. 
,
On a personal note, I try to find the right work/life balance in my life. When I am not working or not thinking about science and medicine, I spend time with my family.
,
,
,  "
"
By Gregory Piatetsky,  
,, Mar 10, 2015.
,
Digital Nova Scotia writes that 
,, August 14-17, 2017.
The award of KDD 2017 to Halifax is a significant international conference win for Nova Scotia in the Data Analytics space. 
,
,, The Knowledge Discovery and Data Mining conference is a premier event that brings together researchers and practitioners from data mining, knowledge discovery, data analytics, and big data. 
,
It is 
, for Data Mining and Data Science research.
,
The recent 
, held in New York set a record with over 2,200 attendees and was sponsored by top companies in Big Data and Analytics. 
,
, will be held in Sydney, Australia, Aug 10-13, 2015.
,
KDD-2016 is planned for San Francisco, CA, USA.
,
Here are the 
,.
,
,
,  "
"
The Machine Learning periodic table from ,, a machine learning newsletter, lists machine learning packages for languages like Python and Java and tasks like NLP and Computer Vision. Below, you can see the table itself.
,
,
,
Below are the elements in the table, listed by their ""group"".
,
Machine Learning Packages in Python (Light Blue):
,
,
Machine Learning Packages in Java (Green):
,
,
Machine Learning Packages for Big Data (Dark Blue):
,
,
Machine Learning Packages in Lua/JS/Clojure (Red):
,
,
Machine Learning Packages in Scala (Yellow):
,
,
Machine Learning Packages in C/C++ (Dark Orange):
,
,
Machine Learning Packages for Computer Vision and NLP (Orange):
,
,
Machine Learning Packages in R/Julia (Grey):
,
,
,
Original post: ,.
,
,
,
,  "
"
,

, is the leading platform for data science competitions, building on a long history that has its roots in the , and the ,, among others. If you?€?re a data scientist (or want to become one), participating in Kaggle competitions is a great way of honing your skills, building reputation, and potentially winning some cash. This post outlines ten steps to Kaggle success, drawing on my personal experience and the experience of other competitors.

While the focus of this post is on Kaggle competitions, it?€?s worth noting that most of the steps below apply to any well-defined predictive modelling problem with a closed dataset. However, in ?€?real life?€?, data scientists spend much of their time defining the problem together with stakeholders and chasing down the data required for its solution. Working on a Kaggle-like problem is often the more fun part of a data scientist?€?s job.

,
,
It?€?s surprising to see how many people miss out on important details, such as remembering the final date to make the first submission. It?€?s important to understand the competition timeline, be able to reproduce benchmarks, generate the correct submission format, and so on. Just like in real life, you should understand what?€?s going on before jumping into coding and model building.

,
A key part of doing well in a competition is understanding how the performance measure works. It?€?s often easy to significantly improve your score by using an optimisation approach that is suitable to the measure. A classic example is optimising the mean absolute error (MAE) versus the mean square error (MSE). It?€?s easy to show that given no other data for a set of numbers, the predictor that minimises the MAE is the median, while the predictor that minimises the MSE is the mean. Indeed, in the , we fell back to the median rather than the mean when there wasn?€?t enough data, and that ended up working quite well.

,
In Kaggle competitions, over-specialisation (without overfitting) is a good thing. This is unlike academic machine learning papers, where researchers often test their proposed method on many different datasets. This is also unlike more applied work, where you may care about data drifting and whether what you predict actually makes sense.

When competing, exploiting anomalies in the data can work in your favour. For example, in the aforementioned hackathon, we noticed that even though we had to produce hourly predictions for air pollutant levels, the measured levels didn?€?t change every hour (probably due to limitations in the measuring equipment). This led us to try a simple ?€?model?€? for the first few hours, where we predicted exactly the last measured value. This proved to be one of our most valuable insights. Obviously, this means that we were predicting what the measurement equipment would say rather than actual pollutant levels ?€? something you?€?d definitely want to avoid in a real-life situation!
  "
"
By Srinath Perera (,).
,
I was at , last week and certainly interest for realtime analytics was at it?€?s top.
,
Realtime analytics, or what people call Realtime Analytics, has two flavors.
,
,
In this post, I am focusing on Realtime Streaming Analytics. (Ad-hoc analytics uses a SQL like query language anyway.)
,
,
,
Still when thinking about Realtime Analytics, people think only counting use cases. However, that is the tip of the iceberg. Due to the time dimension of the data inherent in realtime use cases, there are lot more you can do. Lets us look at few common patterns. 
,
,
Why we need SQL like query language for Realtime Streaming  Analytics?
,
Each of above has come up in use cases, and we have implemented them using SQL like CEP query languages. Knowing the internal of implementing the CEP core concepts like sliding windows, temporal query patterns, I do not think every Streaming use case developer should rewrite those. Algorithms are not trivial, and those are very hard to get right! 
,
Instead, we need higher levels of abstractions. We should implement those once and for all, and reuse them. Best lesson we can learn from Hive and Hadoop, which does exactly that for batch analytics. I have explained Big Data with Hive many time, most gets it right away. Hive has become the major programming API most Big Data use cases.
,
Following is list of reasons for SQL like query language. 
,
,
Finally what are such languages? There are lot defined in world of Complex Event processing (e.g. WSO2 Siddhi, Esper, Tibco StreamBase, IBM Infosphere Streams etc. SQL stream has fully ANSI SQL comment version of it. Last week I did a talk on Strata discussing this problem in detail and how CEP could match the bill. Here are the slides,
,
, 
,
,
Bio: , is a scientist, software architect, and a programmer that works on distributed systems. 
,
Original: ,
,
,
,
  "
"
,, 
,Philadelphia, PA, USA
,Apr 30, 2015,
,
,
For more than 6 years, the Wharton Customer Analytics Initiative (WCAI) has helped shape the definition of 'Customer Analytics.' This year's annual conference is dedicated to real-world applications that exemplify a balance of high-level rigor and business know-how, as well as elevate the role of analytics in an organization's strategic decision-making.
,
,
,
Case studies include:
,  "
"
By Nikhil Buduma.
,
,

,

,

,

,

,

,

,
,
,
,  "
"
,
,
, is the Director of Analytic Services at , where his team builds data tools to support video game studios and embed analytics within the games they create. 
,
Prior to this his industry experience spanned diverse settings such as air pollution research, aerospace, retail loyalty programs and recommendation systems for grocers. 
,
Josh has an MS in Applied Mathematics from the University of Colorado at Boulder.
,
Here is my interview with him:
,
,
,
,. Millions of people connect to our game servers every day, and when they play they expect to find a match with other online players very quickly. Then, for any given match a game server has to manage the individual state and physics for 12 or more players at 60 frames per second of high-resolution graphics. So, there are many hard computational problems in delivering video games and analytical work has to be integrated into very large, complex code bases running on very large, complex operational systems that are running at massive scale.

,
There are a lot of areas analytics touch the game, everything from algorithms that determine which players around the world you play with to ones that determine the optimal light refraction to make a ,character?€?s eyeball look realistically wet. For the statistical applications that my team focuses on, one prominent example is detecting players from opposite teams who collude for their own benefit. We have built classifiers that are very accurate at picking out players who have abnormal patterns in their social, temporal and spatial data in a given match. Using this detection we can better enforce rules to keep the online community healthy and playing fair.

,

,

,

,I am not sure it is a unique challenge by today?€?s standards, but certainly the amount of data we deal with makes analytics life hard. For a single game like Call of Duty: ,, we have to process many TBs a day of binary event data that then gets persisted as many TBs a day of structured data. Even heavily down-sampled datasets can still be huge.

,

Beyond just data size, one challenge is its complexity. Many thousands of game events (e.g. where you are on a map, what weapon you are holding, who you are fighting) are generated for each player in a given match, each match having a dozen or more players. Each event can have hundreds of attributes. So weaving all of this social, temporal, and spatial data together to make inferences is especially challenging.

,

,

,

,Well, aesthetics really do matter. But I think it is easier and easier to generate graphics with beautiful colors that are increasingly interactive and displayed in a browser. Unfortunately, it is not getting any easier to make data visualization informative.
,

The types of visualizations I choose to publish are shaped by a couple of questions I always have in my head when doing this work:

,
,
,



The first question helps me think about the viewer and what supporting information they need in order to be comfortable with what a model is suggesting. The second question is one I imagine the viewer saying to me: what data issues, easy biases, confounders etc. are obvious to a domain expert but that I might be missing in my analysis and presentation of results? Recalling these questions helps me choose what data visualizations are the most useful in getting a model to affect a decision.

,

,

,
,Default settings can be just fine, but it is important to remind yourself that they are there and can sometimes vary widely between software tools. Thus, a viewer?€?s impression from a given data visualization could vary widely simply as an artifact of what software was chosen to create it.

,

What parameters are important could be quite different across settings. For example, almost all of the visualization work I do is with 2D graphics but I have friends working in scientific settings where nearly all visualizations are 3D. Some common defaults to be mindful of are:

,
,
,??,

,

,

,It is simple but crucial context for the viewer and yourself. Sometimes it becomes ,obvious that there is a huge imbalance between two groups you are comparing; that a p-value could be ?€?significant?€? largely because you have a massive sample; or that the sample size is tiny relative to the total population.?? People often have this context in their head, and simply reporting sample sizes on visualizations can really help viewers recall that context and assess the (in)validity of what they are seeing.

,

,

,

,A lot has been written about this topic and I am not an expert like William Cleveland or Edward Tufte. These style charts are easy to make, colorful, and compact so fit nicely in a single presentation slide. But as a viewer it becomes impossible to understand what I am seeing the more groups are stacked upon each other, only one of which has a consistent baseline.

,
Small multiples are often the better way to inform, but it will probably take more mouse clicking or more code to generate them. As professionals though, I think we owe our audience that extra effort.
,
,
,
,
,  "
"
,
,

, is the Director of Analytic Services at , where his team builds data tools to support video game studios and embed analytics within the games they create.

,

Prior to this his industry experience spanned diverse settings such as air pollution research, aerospace, retail loyalty programs and recommendation systems for grocers.
,


Josh has an MS in Applied Mathematics from the University of Colorado at Boulder.
,

,
,
Here is second and last part of my interview with him:

,

,

,

,:
,
,So you have to understand the data generating process, whether it?€?s by a machine or by nature, but you also have to understand how data is actually measured and persisted. In my work, it is the measurement and persisting steps that often introduce the most insidious biases.

,

,

,

,One example is the numerical conversions that happen when persisting data generated by a game into a relational database. A given variable instrumented in the game might take on only positive values and be written to a binary stream as 10-bits wide. But when this variable is persisted in a database it could accidentally be written as an 8-bit signed integer, leading to rollover. Application logic that assumes only positive values would either miss the negative-valued records or possibly break. We have thousands of such variables tracked in the game code so invariably bugs like this exist.

,

,

,

,This is advice I got years ago from , while taking a statistics course from him. It means that it is easy to be misled by a single statistic or a pretty plot. We need data visualizations to validate the inferences we draw from statistics and likewise, we need statistics to validate the natural inferences we make when looking at data visualizations. I recently wrote about this topic ,.

,

,

,

,No one has told me this advice directly, but I have learned it by watching how ,decisions get made across multiple organizations and settings:?? we are ultimately emotional animals and I try to remember that when confronted with too much pretense about being ?€?data driven?€? and ?€?objective?€?. It?€?s one thing to do analysis and present results, and quite another to get people in complex organizations to actually change what they are doing, or make some discrete decision differently than they would have without the analysis. And what is being analyzed in the first place connotes a lot about what an organization values and chooses to spend time and money on. ,

,

,

,

,
, At least in our current world that tolerance needs to be high because these games are so complex, take hundreds of people multiple years to develop, and operate at huge scale. Mapping a business goal to statistical code that has to be executed in game operations can be horribly frustrating at times and you have to have a personality that can be OK with that and continue to progress.

, ??,

,

,

,For statistical visualization my favorite is William Cleveland?€?s ,. He covers prescriptions for presenting data that are grounded in studies on how people perceive visual cues.

,

There is an old book titled , by J.D. Foley and A. Van Dam that I don?€?t turn to much lately but was important when I was first doing this type of work. It covers lots of, well, fundamentals about coordinate transformations, view clipping, rendering 3D objects, etc. I find it helpful that I have some familiarity with these topics when I run across odd behavior in some software tool or an obtuse API in a graphics library, because I have an intuition for what must be going on behind the scenes regardless of how much the particular tool abstracts away the complexities.

,

,I?€?d be remiss to not mention a couple of web resources. Kaiser Fung?€?s blog,, covers great examples of what , to do and often includes examples of better alternatives. I can stare at Mike Bostock?€?s , charts all day. And organizations like the New York Times and the Washington Post are doing amazing work layering in interactive data visualizations with news stories.

,

,

,

,I go through periods of frequent travel to our various studios, so when I am home I try to spend a lot of time with my wife and two daughters, especially playing in the great outdoors surrounding Boulder, Colorado. I like working with my hands, so fixing cars or building things for my kids are ways I decompress.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for Feb 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, sorted by decreasing number of engagements,
,
,??,
Here are previous month activities
,  "
"
        ,  "
"
Most popular 
, tweets for Mar 09-11 were
,
Comprehensive learning path from noob to Kaggler: #DataScience in #Python #DeepLearning 
, 
,
,
,
Comprehensive learning path from noob to Kaggler: #DataScience in #Python #DeepLearning 
, 
,
,
10 examples of #IoT & #BigData working well together: 
,, Barcelona city, 
,, John Deere, BC Hydro 
,
,
10 steps for success in , competitions, also for #predictive modeling of closed data 
, 
,
,
,
,  "
"
,
,

, has extensive Data Management experience in the Financial Industry. He has successfully led and delivered many strategic data initiatives at Fortune 500 Companies. Kenneth's primary management areas of focus include; Data Warehousing, Data Quality, Data Governance, Big Data, and Business Intelligence. He is driven to deliver powerful data management solutions in support of generating corporate revenue. 
,
In his current role at ,, Kenneth leads a team that is responsible for Enterprise Data Strategy, and transforming Big Data into valuable insights that are the catalyst for accelerating the delivery of solutions to real life problems. 
,
Here is my interview with him:
,
,
,
,: The Enterprise Data Strategy Team has three main responsibilities at Equifax.,
,
, ?? ,
,
,
,The motivation to building out the ?€?data lake?€? was enabling innovation within our Data & Analytics team.  This environment provides a hub with all readily available data and the proper tools to efficiently analyze the data.  It empowers our analysts to draw observations from the data, test hypotheses, and generate insights.
,
In any large scale implementation the requirements gathering phase presents a challenge.  You want to ensure all core requirements are accounted for in the build.  Additionally selecting the tools for the environment can be quite an undertaking.  We approached the tool selection process as a proof-of-concept opportunity; we provided prospective vendors with requirements and gave them the opportunity to demonstrate how their tool(s) were equipped to deliver on the requirements.
,
,
,
,Equifax Decision 360?? is our collaborative approach to help our customers address key business needs.  It starts with the ability to create deeper, connected insights that drive better business decisions, augmented through the ability to deliver those insights at the moment of opportunity so we can help our clients maximize customer value across their business and throughout the lifecycle.
,
,
,
,Creating an Insights Culture starts with establishing a collaborative work environment based on innovation.  Our team is staffed with inquisitive analysts that are challenged to find value and opportunities in the data.  We regularly have brainstorming/whiteboard sessions focused on generating new ideas and hypotheses, that are then tested and validated in the data.  The work is very interesting and the team has fun while remaining focused on supporting the delivery of insights.
, ?? ,
,
,
,Big Data projects present various challenges, but that?€?s what makes it interesting!
,
One major challenge in this space is providing transparency around the data that is available. The value of Big Data is being able to draw pertinent observations (from Big Data) that can be translated into insight. In a fast moving environment where new data assets are consistently boarded and enhancements are applied, our data consumers/customers need to be informed of these changes.
,
An additional challenge is information governance. By this I mean managing data access and usage. All data sources are not created equally, and each has its set of contractual/legal regulations that need to be applied. This effort seeks to provide a framework and foundation conducive to keeping the organization compliant.
,
,
,
,
,  "
"
By Gregory Piatetsky,  
,, Mar 13, 2015.
,
Here is HadoopSphere 
,, reflecting the people, products, organizations and portals that exercised the most influence on big data and ecosystem in 2014.
,
The methodology used included reach of the person/company, relevance, CTR, financial influence, and primary category.
,
,
,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
,
,
,??,
Comparing with 
,, only
Tony Baer, Merv Adrian, IBM, Gregory Piatetsky remained on the list in both years.
,Dropped in 2014 were: Analysts and Media: Matt Aslett, Derrick Harris, Alexandru Popescu; Products: Hadapt, Vivisimo, SAP HANA; Repositories: ASF, Github, Google Code; Social Media: DJ Patil, TweetChat.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
New KDnuggets cartoon looks at the most difficult challenge facing the first
,,
,.
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,  "
"
,
,
,
Forget about the meaning of words, forget about grammar, forget about syntax, forget even the very concept of a word. Now let the machine learn everything by itself. 
,
This is the amazing story that Xiang Zhang and Yann LeCun from NYU tell us in their recent paper ?€?,?€?. They posit that deep learning could make it possible to understand text, without having any knowledge about the language. 
,
,
The paper shows how to use deep learning to perform text classification, for instance to determine if a review given by a customer on a product is positive or negative. 
,
Deep learning has been very successful for big data in the last few years, in particular for temporally and spatially structured data such as images and videos. But it has been less successful in dealing with text: texts are usually treated as a sequence of words, and one big problem with that is that there are too many words in a language to directly use deep learning systems. 
,
English has more than a quarter of a million distinct words, so neural networks would have a quarter of a million neurons in the input layer. This high-dimensionality would prevent us from discovering the most interesting patterns with (deep) networks, and we would have to meticulously engineer the model to include knowledge about syntax and semantic structures. 
,
LeCun?€?s team has developed a solution to this problem: 

,

 Since there are only a few dozen possible characters, that would make it possible to directly use deep learning systems for text classification. 
,
Now, this is a very audacious idea, because what they?€?re basically saying here is ?€?Let?€?s forget everything we know about languages: meaning of words, the very concept of a word, grammar, syntax ?€? let?€?s make the machine learning system discover everything relevant to the task, by itself?€?. 
,
And the amazing thing? It works!
,
,
Convolutional Networks (ConvNets) [1,2] is the deep learning system used in the paper. Invented by LeCun in the late 80s, ConvNets work by trying to discover the convolutions that, applied to the data, would be useful to perform the recognition/classification task. The intuition behind convolutions is that they can be used to code many different mathematical transformations of the input. 
,
For images, a convolution is often used with a kernel to reveal certain properties of the image. For example, I depict below the application of different kernels (or matrix) to the same image (see also , for a great explanation of convolutions); the convolution uses the matrix the matrix is the function that is applied. 
,
,
ConvNets use this idea but instead of using fixed values for the kernel, they let the network infer them automatically so that the produced transformation of the image is specifically designed to perform the recognition task. 
,
,
They use the same idea in this paper: each character is transformed into a binary vector (?€?a?€? is [1,0,?€?,0], ?€?b?€? is [0,1,0,?€?,0] and ?€?z?€? is [0,?€?,0,1]).  Broadly speaking, a sequence of characters looks like an image, on which convolutions can be applied. 
,

The network is designed as a traditional deep ConvNet, with successions of convolutional and pooling layers, finishing with 2 fully connected layers. The succession of convolutions makes it possible to learn very diverse sets of features, by learning convolutions of convolutions of convolutions (as if we looked at, for example, the blurred embossed image of the edges of an image, but with the actual operation being learned by the system). 
,
,

,
As I mentioned at the start of the paper, the first amazing thing is that it works at all. Remember, they do not even tell the system how to construct words! But not only have they shown proof of concept, but the team?€?s experiments show that their system actually works very well on many different tasks, including:
,
,
My take
I really liked the work and the paper, particularly the authors?€? capacity to think out of the box ?€?it?€?s an audacious idea to classify text without any knowledge of words (would you have bet on it succeeding so well?).
,
,
,
I do however hope to see the team compare their method to stronger competitors in follow-up work, and keep working towards a deeper understanding of text than mere classification allows. 
,	
Deep learning has proved useful to bring lower bias classifier, essential characteristics to tackle classification for big data [3]. This paper enables deep learning to be applied to text data, a very useful addition to the data mining toolbox. I look forward to seeing what the authors will do about extending this approach from sequences (of characters) to time series. 
,
(Update: Here is 
,, code in Torch 7 for text classification from character-level using convolutional networks, 
used for this work, released by Xiang Zhang on github, April 2015)
,
,
, completed his PhD working for the French Space Agency in 2012, and is now a Research Fellow in Geoff Webb?€?s team at Monash University?€?s Centre for Data Science. He tweets at @LeDataMiner. 
,
,
[1]	LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278?€?2324, November1998.,
[2]	Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, ?€?Handwritten digit recognition with a back-propagation network,?€? in NIPS?€?89.,
[3]	S. Chen, A. Martinez, and G.I. Webb (2014). Highly Scalable Attribute Selection for AODE. In Proceedings of the 18th Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 86-97.

,





,
,
 ,  "






















"
,
,
Starts: ,
,
For more information and to register visit: ,
,
, 
, is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis. ,. Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically).
,

This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using a booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as ""data science in action"".
 ,
,
 , 
The Coursera course ?€?Process Mining: Data science in Action?€? explains the key analysis techniques in process mining. , joined in the first run where they learned various process discovery algorithms. These can be used to automatically learn process models from raw event data. Various other process analysis techniques that use event data were also presented. Moreover, the course provides ,, ,, and , in a variety of application domains. To give everyone who missed the first run a chance to follow this course, the course runs again as of Apr 1, 2015.
,
,
,  "

"
,, 
,5-7 May 2015
,
Forbes called Strata ""mind-blowing."" Attendees have called it ""an incredible community of people working on big data,"" ""an amazing event,"" and ""by far one of the most interesting and informative conferences I have ever attended.""
,
Strong words for a data conference, right? See for yourself.
,
At 
,, happening 5-7 May,
,
you'll have a unique opportunity to:
,
,??,
Have some fun. You'll be surrounded by like minds. You'll be
intrigued, invigorated, and excited. Because, admit it: data is fun.
,
You don't want to miss it. 
,
,.  "
"
By Karl Rexer (Rexer Analytics).
,
Data Analysts, Predictive Modelers, Data Scientists, Data Miners, and all other types of analytic professionals, students, and academics:  Please participate in the Rexer Analytics 2015 Data Miner Survey. 
,
Survey Link:  ,
Access Code:  KD7PE9
,
Survey results will be unveiled at the Fall-2015 Boston Predictive Analytics World event.
, 
, has been conducting the Data Miner Survey since 2007.  Each survey explores the analytic behaviors, views and preferences of data miners and analytic professionals.  Over 1200 people from around the globe participated in the 2013 survey.  Summary reports (45 page PDFs) from previous surveys are available FREE to everyone who requests them by emailing ,. 
,
,
,
Also, highlights of earlier Data Miner Surveys are available at ,, including best practices shared by respondents on analytic success measurement, overcoming data mining challenges, and other topics.  The FREE Summary Report for this 2015 Data Miner Survey will be available to everyone Fall-2015.
, 
Please tell other data analysis professionals about the survey.
, 
Rexer Analytics is a consulting firm focused on providing data mining and analytic CRM solutions.  Recent solutions include customer loyalty analyses, customer segmentation, predictive modeling to predict customer attrition and to target direct marketing, fraud detection, sales forecasting, market basket analyses, and complex survey research.  More information is available at , or by calling +1 617-233-8185.
,
,
,  "
"
By Nick Vasiloglou (MLconf Technical Chair).
, 
, is a community event for machine learning practitioners that come from the industry as well as theoreticians in the industry. 
, , 

Theoreticians? Yes because as , said ?€?There is nothing more practical than a good theory?€?! Speakers are free to express their progress and results with their respective work within their careers in machine learning. I?€?ve enjoyed observing the challenges and the innovation paths of ML, while attending presentations at the past 3 years of , events. First of all, it is really encouraging that after so many decades of research in machine learning and with so many algorithms in the literature, there are still new ones being used and developed. , present on their results and demonstrate the speed that these methods get absorbed in the production systems.
, , 

A great example was , factorization machines invented in 2012, which he presented last November in San Francisco. Many other presenters already mentioned the success they had in production, such as , from Netflix, who presented last year in Atlanta and many others. An upcoming talk in this vein is Retail Demand Forecasting with Machine Learning, which will be presented by  , on ,. Another interesting aspect of Machine Learning has been the introduction of machine learning practices inside business operations. , from LexisNexis and , from Think Big Analytics presented on their experiences with this last year in Atlanta.  This year, Dan Mallinger will return in , to present his views on that problem. As always distinguished scientists from big companies like ,, from Google, ,, from Facebook, and , from Yahoo, have focused on scaling simple algorithms like nearest neighbors and decision trees with smart algorithmic and engineering tricks.
, ,  
It is a common belief knowledge in the Machine Learning community that adding more data improves accuracy, so it makes sense to focus on scaling. Last Year, Xavier Amatriain from Netflix, now at Quora, gave a , (,) that shook the audience and destroyed the myth that the power is only in the data, explaining when this is true and when the power is in the algorithms. The talk had also a lot of other interesting points as well. 
,
, ,  
Of course it has been fascinating to watch the evolution of machine learning platforms like H2O, Graphlab (now Dato), Oracle, Ayasdi, Intel, Systap, Spark, and Ufora, which has been added this year and how they are placing their value proposition. MLconf has provided an x-ray of their time line and how they start reaching maturity. Most of them started developing fast and scalable algorithm implementations and started to gain massive adoption when they properly addressed the necessary engineering requirements.
, ,  
Deep learning has a strong presence too, with presentations  from companies such as Pandora and ,.  This year in New York, we are particularly interested in seeing the details of a large scale implementation with GPUs on Facebook. We started with the pleasant surprise of new algorithms making it production, and we want to close with the new paradigms that come up such as probabilistic programming as it was presented last year in San Francisco by ,.
, ,  
This year we are very excited to have MLconf hosted in 4 different cities, ,, ,, , and , with speakers from big, established companies and from emerging startups, bringing more ideas and experience into the game. Mention ?€?kdnuggets?€? when registering and save 15% on tickets!
, ,  

Bio:, 
, is a Technical Chair of MLconf- The Machine Learning Conference.  His PhD from Georgia Tech was focused on scalable machine learning over massive datasets. His work has resulted in patents and production systems.
, , , ,
,
,  "
"
,
,
, has extensive Data Management experience in the Financial Industry. He has successfully led and delivered many strategic data initiatives at Fortune 500 Companies. Kenneth's primary management areas of focus include; Data Warehousing, Data Quality, Data Governance, Big Data, and Business Intelligence. He is driven to deliver powerful data management solutions in support of generating corporate revenue. 
,
In his current role at ,, Kenneth leads a team that is responsible for Enterprise Data Strategy, and transforming Big Data into valuable insights that are the catalyst for accelerating the delivery of solutions to real life problems. 
,

,
,
Here is second and last part of my interview with him:
,
,
,
,: 
Before providing recommendations it?€?s important to level set on the topic.  I define Data Governance as the coordination of people, business processes, and technology to enable an organization to leverage data as a corporate asset. Successfully delivering Data Governance creates efficiencies, mitigates risk and supports the enablement of corporate revenue. 
,
The biggest misconception about Data Governance is that many people view it as red tape, unnecessary overhead, and even as a road block. In order for it to succeed, the various stakeholders need to be shown the value of Data Governance. What?€?s in it for them? Incent and influence the stakeholders to buy into the concept by showing them value. Without this approach it is very difficult to gain traction in Data Governance efforts.  

,

,
,
, Identifying the best advice in my career is difficult to isolate.  I?€?ve worked in a number of prestigious companies throughout my career, and I consider myself extremely lucky to have had some solid mentors along the way.
,
At one point early in my career I felt like I was stuck in a rut.  I was in a production role and just going through the motions to complete my day to day assignments.  I did not feel like I was learning new things nor did I feel challenged by the work. I had a talk with a mentor, and he told me that I was responsible for my career path.  He went on to say, if this role is not ideal for you then go find one that is of more interest but it?€?s up to you to do that.  Nobody is going to do that for you!  This message around ?€?owning my career?€? was very valuable.
,
,
,
,I?€?m very interested in The Internet of Things (IoT).  There is a large ecosystem of everyday objects that have network connectivity allowing them to send and receive data.  Things like sensors in your car, thermostats in your home, wearables, and sensors in smartphones are gathering data.  Further, this space will continue to grow as more IoT devices come to market and also the number of sensors in devices continues to increase.   
,
I?€?m genuinely interested in seeing how all of this machine data is put to use.  What problems will it solve?  Also I?€?m interested to see how far IoT goes as many believe some of the data collected in this space is crossing privacy boundaries and could be considered to be ?€?creepy?€?.
,

,
,
,In staffing roles I like to really have a good understanding of the candidate?€?s background and how they ended up in this interview.  I like them to walk me through their journey.  I also like them to tie experiences, skillsets, and knowledge to the role they are in interviewing for.  Additionally I often present a scenario in an interview, and ask candidates how they would approach the given situation?  It gives me a sense for the thought process and temperament of the candidate when challenged.  Specific traits I focus on are inquisitiveness, excitement/eagerness to learn, attention to detail, communication skills, and cultural fit.   
 ,
,
,
,The last book I read and enjoyed was ?€?,?€? by Michael Connelly.  This was a highly entertaining novel that I strongly recommend to others.
,
When I?€?m not working I like spending time with my family and friends.  I also really enjoy traveling, and just recently booked a cruise for my family that will take us to Honduras, Belize, and Mexico.  I?€?m very excited for this trip!
,
,
,  "
"
,
,
,In February, White House released a , discussing how companies are using big data to charge different prices to different customers - a practice known as , or differential pricing. 
,
,
,
In marketing, collecting and understanding the customer behavior information is a core principle and companies have developed a wide variety of tools to do so. The reason for that could be if sellers know exactly what a customer is willing to pay or what product they like by prediction analysis, they can set prices and expand the size of the market. 
,
Big data has lowered the costs of collecting customer-level information, and making it easier for sellers to identify new customer segments and to target those populations with customized marketing and pricing plans. Given sufficient data, sellers can try to predict how buyers will behave in response to different prices and pricing schemes. For example, a 2014 recent study by Benjamin Shiller estimates the increase in profits if Netflix were to use behavioral data for personalized pricing. He finds that differential pricing based on demographics (whereby Netflix would adjust prices based on a customer?€?s race, age, income, geographic location, and family size) could increase profit by 0.8 percent, while using 5,000 web browsing variables (such as the amount of time a user typically spends online or whether she has recently visited Wikipedia or IMDB) could increase profits by as much as 12.2 percent.
,
,
,
More precisely, the third-degree price discrimination occurs when sellers charge different prices to different demographic groups, as with discounts for senior citizens. 
,
Big data naturally raises concerns among groups that have historically been victims of discrimination. Given hundreds of variables to choose from, it is easy to imagine that statistical models could be used to hide more explicit forms of discrimination by generating customer segments that are closely correlated with race, gender, ethnicity, or religion. Moreover, the term ?€?price discrimination?€? may lead to concerns about economic injustice, even if the profit motive is different from, and in many cases fundamentally inconsistent with, the sort of prejudice that our anti-discrimination laws seek to prohibit.
,
Knowing that disparate impact occurs when some practice has an adverse impact on a protected group, even if the practice was not intended to be discriminatory. Big data helps to distinguish between disparate treatment and disparate impact. So it can further help to understand how big data to affect historically disadvantaged groups. The premise of big data is that when marketers have a wide variety of behavioral data to choose from, they will find imperfect proxies such as race or religion to be less useful. In other words, big data aims to reduce the rate of ?€?false positive?€? cases that potentially make disparate treatment a problem.
,
Big data also provides new tools for detecting problems, both before and perhaps after a discriminatory algorithm is used on real consumers. For example, it is often straightforward to conduct statistical tests for disparate impact by asking whether the prices generated by a particular algorithm are correlated with variables such as race, gender or ethnicity. Put differently, in markets where it is important to prevent disparate impact, big data can be used to enforce existing anti-discrimination laws more effectively, thereby obviating the need for broader restrictions on its use.
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
By Gregory Piatetsky,  
,.
,
New KDnuggets Poll is asking:
,
,
,??,
,

This poll is now closed.
,
Results will be published shortly.
,
KDnuggets has conducted a similar poll back in 2010:
,.
  "
"
As a media partner, KDnuggets is pleased to offer our readers a free pass (free registration) to 
,
,
,
To win a free pass (free registration), please email to ,
,
,with the subject: 
,
,
and please specify what 
,
,,
,
for example Deep Learning, Graph Databases, Hadoop, Hive, IoT Internet of Things, NoSQL Databases, Predictive Analytics, Privacy, Python, Security, Sentiment Analysis, SQL, Spark, R, Text Mining, Visualization, Uplift modeling,  or something else.
,
The winner will be chosen among all submissions (using a random number generator) and announced the first week of April 2015 along with a summary of the answers. 
,
Here is more information about the conference, and even if you don't win you can still get KDnuggets discount on registration - see below.
,
,
,
Big Data TechCon is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Come up to speed on Hadoop, Spark, Yarn, HBase, R and Hive. Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or JavaScript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes.
,
Mingle with fellow attendees. Be inspired by keynotes, including Matei Zaharia, assistant professor of computer science at MIT. Be impressed by the hottest Big Data tools in the Expo Hall. It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code 
,  when prompted. Register early for additional savings. 
,  "
"
,, 
,Nov 14-17, 2015 in Atlantic City, NJ, USA.
,
,
,
,
,
All deadlines are at 11:59PM Pacific Daylight Time
,
,??,
The IEEE International Conference on Data Mining series (ICDM) has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications. ICDM draws researchers and application developers from a wide range of data mining related areas such as statistics, machine learning, pattern recognition, databases and data warehousing, data visualization, knowledge-based systems, and high performance computing. By promoting novel, high quality research findings, and innovative solutions to challenging data mining problems, the conference seeks to continuously advance the state-of-the-art in data mining. Besides the technical program, the conference features workshops, tutorials, panels and, since 2007, the ICDM data mining contest.
,
,
,
Topics of interest include, but are not limited to:
,
,??,
For Submission Guidelines and more information, including 
,
,??,
visit ,
,
,
,
,??,
,
,
,??,
,
,  "
"
,

,
, is Chief Technology Officer at ,. Dave most recently served as SVP of engineering at Warner Music Group, where he led over 100 engineers building the company?€?s new Digital Services Platform, based on an open source enterprise platform as a service. His extensive experience in the cloud and virtualization industry included positions as a senior architect in Cloud Foundry while at VMware and as a cloud architect at Dell. 
,
Earlier in his career, he experienced successful exits for two companies he founded: Hyper9 (acquired by SolarWinds) and Surgient (acquired by Quest Software). Dave is well known for inventing the concept and coining the term ?€?Data Gravity,?€? which states that as data accumulates, there is a greater likelihood that additional services and applications will be attracted to this data and add to it.
,
,
,
Here is second last part of my interview with him:
,

,: Q6.	What do you mean by ""data gravity""? What role does it play in designing data management architecture for Enterprise IT?

,
,:  Data gravity describes the effect that as data accumulates, there is a greater likelihood that additional services and applications will be attracted to this data, essentially having the same effect gravity has on objects around a planet. As the mass and density increases, so does the strength of the gravitational pull and as things get closer to the mass, they accelerate towards it at increasing velocity. Although services and applications have their own gravity, data is the most massive and dense, meaning it has the most gravity. If data becomes large enough it can become virtually impossible to move. Usually as services and applications interact with data, they cause even more rapid growth of the data itself, creating a continuous cycle of data growth.
,
When designing data management architectures, it?€?s important to take data gravity into consideration. It?€?s easy to get data into your services and applications, however getting it out can be difficult and expensive. Whether you?€?re creating a single-user application or deploying a company-wide project, you need to consider the implications of data gravity. The stronger the data gravity involved, the more cautious you should be when choosing or designing your data storage solution and where you implement it (locally or in the cloud). 
,
,
,
,In Riak Enterprise 2.0, my favorite enhancement is the redesign of Riak Search integrated with Apache Solr. It powers integration with a wider variety of existing software through client query APIs. In Riak CS 1.5, I really like the improved Amazon S3 compatibility. Our expanded storage API compatibility with S3 includes multi-object delete, put object copy and cache control headers which provide more flexible integration with content delivery networks (CDNs).  
,?? ,
,
,
,Riak is easier to use and scales better than our competition. When companies are switching from an RDBMS to a distributed system or NoSQL database, they want a simple, scalable and easy to use solution. Riak offers extremely easy operations and great scalability.
,
A use case I find particularly interesting is Tapjoy, a mobile advertising and monetization platform. Tapjoy uses Riak to manage more than 250,000 operations per second without having to employ additional engineering staff.  Managing that amount of data with one of our competitors?€? solutions wouldn?€?t be possible with the staff Tapjoy has. Riak helps them keep costs down and reduce complexity while still guaranteeing performance and uptime. 
,
,
,
,The biggest challenge with distributed storage, and with all storage at this point, is dealing with the overwhelming amount of data that enterprises are generating while maintaining ease of operations along with performance and scalability. Also, networks are unreliable. Distributed systems architects have to always be ready to address and solve any networks problems quickly and gracefully. It can get pretty stressful, I don?€?t think many people understand unless they?€?ve experienced it themselves.
,
,
,
,Always hire people that are better/smarter than you are.
,

,
,
,I would focus on learning to code, statistics and probabilities, presentation skills and social media presence.
,

,

, by my friend Jonathan Murray (@Adamalthus)
,
Outside of work, I enjoy spending time with my family and travel. I also enjoy working on building a model for Data Gravity and studying Information Theory. I also enjoy Asian movies (subtitled), Anime, Racing Cars, and fine Whiskey and Bourbon.
,
,
,  "

"
,NYC Data Science Academy spring schedule includes 3 classes, 3 Meetups, 7 bootcamp events on Data Science, R, Python, Machine Learning, and related topics.
,
Upcoming weekend class,
,
,Date: Mar 28rd, April 11, 18, 25, May 2nd
,(We take a break on Easter Weekend on April 4th.)
,Time: 10am to 5pm
,
Paul Trowbridge is a adjunct faculty at NYU.  Paul has worked on projects in FMRI brain imaging studies, the analysis of international dispute data, experimental psychology,  micro-simulation methods in urban planning and urban economics applications, and the epidemiology of Chlymidia.
,
This intensive class will introduce you to the wonderful wold of R and provide you with an excellent understanding of the language that leaves you with a firm foundation to build upon. From the rudimentary building blocks of programming basics, to data manipulation and use of advanced drawing packages, the course will conclude with a demonstration of a project of your choice on Project Demo Day.
,
,
,Date: Mar 28rd, April 11, 18, 25, May 2nd
,(We take a break on Easter Weekend on April 4th.)
,Time: 1pm to 5pm
,
This five week course is an introduction to data analysis with the Python programming language, and is aimed at beginners. We introduce how to work with different data structure in Python. We covered the most popular modules including Numpy, Scipy, Pandas, matplotlib, Seaborn, ggplot and etc to do data analytics and visualization. We use iPython notebook to demonstrate the results of codes and change codes interactively during the class. Our past students include people have no programming experience and people have little exposure by taking Python class at NYU or General Assembly. Students told us our classes are very engaging, interactive, hands-on and have tons of content.
,
,
,Date: Mar 28rd, April 11, 18, 25, May 2nd
,(We take a break on Easter Weekend on April 4th.)
,Time: 1pm to 5pm
,
We do hands-on instruction over 30+ functions in scikit-learn library and cover topics as linear regression, multivariate linear regression, Naive Bayes Classifiers, KNN, logistic regression, LDA, Cross-validation, Bootstrap, feature selection, SVM, Decision Tree, PCA and clustering. We build solid foundation for you to advance your data scientist career into senior or manager role.
,
,
,
,
,Date: Mar 18, 7-90pm
,
Chaitanya Ekanadham leads a team of data scientists and analysts developing the core models that power Knewton adaptive learning products. Chaitanya will talk about the Knewton data science team's vision of representing learning experiences, and go into a few specific ways we do that, such as content graphing and measuring student ability.
,
,
,Date: Mar 24, 7-9pm
,
Data Science with R (Data Analysis level) 10th batch and (Data Mining level)(12th batch) are graduating. Their final projects will demonstrate:
,1. how to predict condo price in Jersey City using tax assessment data, building data  
,2. predict conversion rate based on Salesforce application  
,3. Predict view-ability score for each single impression using DSP log level data
,
,
,Date: Mar 30, 7-9pm
,
Scikit-learn is a machine learning library in Python, that has become a valuable tool for many data science practitioners. Andreas Mueller is an Assistant Research Scientist at the NYU Center for Data Science, building a group to work on open source software for data science. This talk will cover some of the more advanced aspects of Scikit-learn, such as building complex machine learning pipelines, model evaluation, parameter search, and out-of-core learning.
,
,
,
,
Application Deadline: May 1st, apply immediately!
,
Open House Meetup Events 
,
,??,
Learn how you can bring your data scientist career goal to life.
,
Meet our instructors, alumni, and staff. Learn what it means to be a Data Science student, and check out our alumni final projects. Get introductions to our upcoming course offerings or have a 1-on-1 chat with our admissions rep to find the right course for you. Learn about the NYC Open Data Group, NYC Data Science Academy and the benefits of becoming a member of the school and meetup!
,
,
,
Application Deadline: May 1st, apply immediately!
,
Online Information Session hosted on Google hangout
,
,??,
Learn how you can bring your data scientist career goal to life.
,
Meet our instructors, alumni, and staff. Learn what it means to be a Data Science student, and check out our alumni final projects. Get introductions to our upcoming course offerings or have a 1-on-1 chat with our admissions rep to find the right course for you. Learn about the NYC Open Data Group, NYC Data Science Academy and the benefits of becoming a member of the school and meetup!  "
"
,
,
,
,
,
,Enter  code PACE200 at the ,
,
,
,Enter  code PACE300 at the ,
,
, 
, Enter code ""EDU"" along with your academic organization, title and email when registering.
,
,
, ,
,858.534.8321
,
,
,
For  nearly 30 years, the San Diego Supercomputer Center (SDSC) has been at the  forefront of new trends and developments in high-performance and data-intensive  computing. SDSC's large-scale data systems include ,, a 5 PB parallel file system; and the ,, consisting of 3.5 PB of disk  (with replication). HPC resources include the data-intensive , system with 64 TB aggregate  memory, 300 TB of flash memory, and 341 TF of computing power; and ,, an all-new cluster with an  overall peak performance of 2 PF, total 253 TB of RAM, and 620 TB of flash  memory.  "

"
        ,  "
"
,
,
The , combines an
enterprise RDF triplestore (GraphDB???), text analytics services, Linked
Open Data sets and knowledge graphs in an on-demand, cloud-based
solution. In order to demonstrate the power and ease-of-use of S4,
Ontotext has announced the S4 Developer Challenge. The challenge will
award a cash prize to developers that write the most interesting demo,
application or show case utilizing the S4 capabilities for text
analytics, linked data and knowledge graphs. 
,
Submissions due Mar 31.
,
For complete details visit the 
,.
,
,
,  "
"
Most popular 
, tweets for Mar 12-15 were
,
#Cartoon: the most difficult challenge facing the 1st US Chief Data Scientist DJ , 
, 
,
,
,
In-depth intro to #MachineLearning, #Statistics, R: 15 hours of videos from top , profs Hastie & Tibshirani 
,
,
,
In-depth intro to #MachineLearning, #Statistics, R: 15 hours of videos from top , profs Hastie & Tibshirani 
,
,
,
#Cartoon: the most difficult challenge facing the 1st US Chief Data Scientist DJ , 
, 
,
,
,
,  "
"
,
,
,
Dear Yann,
,
In your recent IEEE Spectrum interview, you state:
,
,
,
However, for industries that have Small Data sets (less than a petabyte), a Specialized Deep Learning approach based on unsupervised learning is necessary.
,
Here?€?s how we see industries fall based on the amount of data they can collect and the type of Deep Learning method is appropriate for each. Ideally all of our efforts should gravitate to what we call the Holy Grail (Quadrant B), where Specialized Deep Learning converges with Big Data to give us amazing insights about our fields. Unfortunately that?€?s time dependent and we?€?ll have to work within our means until the data catches up.
,
,
Data Scientists in field without Big Data (Quadrant C) simply cannot wait to gather sufficient data to implement Deep Learning in the way that works in your industry (Quadrant A).
,
,
,
Deep Learning as a technique began in cognitive science and is increasingly gaining momentum in the fields of medicine (eg. radiology), physics and materials research. The reason for this is that Deep Learning can generate insights faster and lead to into product innovation or diagnosis rather rapidly. A lot of these researchers tend towards Deep Learning to mitigate the limitations presented by other machine learning techniques.
,
Since the datasets available in these fields are small, data scientists cannot apply pre-packaged Deep Learning algorithms, but have to artfully determine the features to train and engineer their networks with convolution/dense layers to learn these rather complex features. The data scientists that perform these tasks have to be machine learning engineers who walked in the shoes of a radiologists, internist or a chemical engineer. This understanding across domains enables them to delve into appropriate pre-processing and feature engineering involved prior to architecting and training their networks.
,
Let me explain with an example.
,
In the broad field of image recognition, a data scientist in search and a data scientist in MRI research will need to approach Deep Learning differently.
,
A data scientist working in search can identify an image of a cookie as a cookie when presented with several types of cookies because the features and classifiers are obvious. But to train an algorithm to diagnose a disease from an MRI image, the data scientist has to determine inconspicuous features to extract and hand craft their network to train these complex features. This data scientist does not have billions of images at their disposal to determine appropriate classifiers and yet are looking to train and coordinate a set of very complex features.
,
,
,
On a grander note, fields that require Specialized Deep Learning will soon gather more data. When Specialized Deep Learning is applied across Big Data (greater than 1 petabyte) I imagine we?€?d be able to glean insights previously unthinkable, such as generating psychological traits from a person?€?s genetic profile very precisely.
,  "

"
,
,
, is the Vice President Data Analytics and BI at ,, a leading interactive entertainment company for the mobile world and creator of games such as Candy Crush Saga. King listed on the NYSE in March 2014 (NYSE: KING). 
,
With more than 17 years?€? experience in data science, at King, Vince oversees a team working on all aspects of analytics and business intelligence, including downstream and upstream data pipelines, data warehousing, real-time and batch reporting, segmentation and real-time analytics ?€? information which is used across the business to improve the player experience. 
,
Vince holds a Master of Arts and PhD in Complex Systems from Harvard University and a Bachelor of Arts in Mathematics from the University of Cambridge. He is also a published author, having co-authored ?€?A NASDAQ market Simulation: Insights on a Major Market from the Science of Complex Adaptive Systems.?€?
,
,
,
Here is second and last part of my interview with him:
,
,
,
,:  We've obviously had a lot of success in the app stores, with many of our games reaching the top grossing charts and also staying in the top grossing charts over a long period of time. Key to this are games that can really engage each individual player for those months and years, adding new content and challenges on a regular basis (our Saga games offer new episodes every two weeks). 
,
Different games achieve that engagement in different ways ?€? through different combinations of challenge, competition, collaboration, novelty, progression. Most obvious in our games is getting the level progression right ?€? so the player develops the right combination of challenge, fun, mastery and success through hundreds and hundreds of levels. For many players there is also a strong social element ?€? a mixture of collaboration and competition with their friends, which is obviously very powerful. 
,
Perhaps slightly less obvious is that we believe a very important ingredient is the quality of the player?€?s experience ?€? for a game to genuinely delight a player for years, the quality must shine through in every way, so we focus a lot on that ?€? often in dozens of small but important ways ?€? for example the beautiful orchestral music in Candy Crush Soda Saga, which was recorded with the London Symphony Orchestra.
,
,
,
,Two key things for me: 
,
, ?? ,
,
,
,If I look back 3 years King hadn't at that time launched a Saga game on mobile! So it?€?s very hard to look ahead that far... But let me try.
,
As mobile devices in the western world gain progressively faster and always-on internet connections, I suspect we?€?ll see more games taking advantage of that ?€? whether through content, interactivity, social or something else?€?. And there?€?s a lot to learn from countries which already have ubiquitous high-speed connectivity. So, mobile will continue to remain very important in gaming.
,
,
,
,There are 5 qualities we look for ?€? and every candidate must at least demonstrate the aptitude to learn all 5, although they probably don?€?t have them all when they walk in the door. There are 3 technical areas ?€? (1) stats/machine-learning, (2) coding, (3) SQL/database skills. Then there are 2 somewhat less technical areas: (4) the ability to communicate complex ideas clearly and precisely, (5) being able to take a somewhat fuzzy, ill-defined business problem and formulate it sufficiently clearly and precisely as a technical problem. We believe all 5 of these are really important to success as a Data Scientist at King.
,
,
,
,Over Xmas I really enjoyed ?€?,?€? by David Foster Wallace ?€? years ago as a young scientist in Santa Fe, New Mexico I was fortunate enough to have dinner with the author. At that time I had no idea about his books, so we had a great talk about science (he had a lot of great questions about my research). Afterwards our host pointed me to his incredible novel ?€?Infinite Jest?€? which I read soon after, 300 pages of footnotes and all.
,
I've also read books on ultra-running by Scott Jurek and Kilian Jornet in the last couple of months, since in my spare time I like to run ultra-marathons ?€? last summer I ran two different 100 mile races in the south of England, and have a few more planned for the coming year. I've even managed to persuade 12 other crazy people at King to join me in a beautiful 100k race in July.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Mar 17 and later.
,
See full schedule at , .
,
,  "
"
,

,
, is Chief Technology Officer at ,. Dave most recently served as SVP of engineering at Warner Music Group, where he led over 100 engineers building the company?€?s new Digital Services Platform, based on an open source enterprise platform as a service. His extensive experience in the cloud and virtualization industry included positions as a senior architect in Cloud Foundry while at VMware and as a cloud architect at Dell. 
,
Earlier in his career, he experienced successful exits for two companies he founded: Hyper9 (acquired by SolarWinds) and Surgient (acquired by Quest Software). Dave is well known for inventing the concept and coining the term ?€?Data Gravity,?€? which states that as data accumulates, there is a greater likelihood that additional services and applications will be attracted to this data and add to it.

,

,: Q1.	In the near future, where do you think the next oductivity boost will come from, for Distributed Storage in the Enterprise?
,
,: ,
, We?€?ve seen a sort of system sprawl over the last few years that requires too many people to understand too wide a variety of solutions and technologies in order to manage and take advantage of the massive amounts of data being generated. Providing a single view into all of that, along with the ability to manage all of the core functions from a central point, will drive a productivity explosion, in terms of both development and operations.    
,
,
,
,What people are finding is that there is no single distributed storage solution that can solve all of their problems. However, they are now combining solutions to solve all sorts of complex data problems. Many of these problems were intractable just a few years ago. Companies are now able to process and understand massive amounts of data much faster than in the past. Because of this, we are seeing much larger data environments that are spread out to many geos and are scaled out inside of the data center.
,
Scaling out adds more nodes to a system versus scaling up, which adds resources to a single node in a system. While scaling up adds incremental costs, it doesn?€?t have a major impact on solution design, complexity, etc. On the other ,hand, scaling out not only adds incremental costs, but also complexity and sometimes even requires a redesign of the solution. Managing the trade-offs between the two can be difficult, however, the benefits of scaling out are often the better solution when it comes to large volumes of data and processing. The abundance of inexpensive hardware and the level of performance/concurrency achievable are higher when you scale out.
,
,
,
,Software design patterns in the Cloud are very different than ?€?traditional?€? software design. This is because of basic assumptions made in the past regarding underlying infrastructure. Previously, software design patterns focused on synchronous connections, reusability and modularity (which are still valid), except now on Cloud, you must move to supporting asynchronous connections and stateless approaches. Stateless meaning that the software persists everything externally to non-local storage.
,
Enterprise IT Managers should learn some simple concepts to ensure they know what they are getting into by moving to Cloud-y architectures and solutions. Some simple concepts being:
,
,??,
,
,
,Yes, I believe Microservices have a promising future because of how simple and powerful this way of design is. Each Microservice is tightly focused on doing a single thing well and this makes them more easily maintained. The downside is that you end up with very large numbers of Microservices and you must decide if you will evolve each or create derivative services based on need. Then there is integration between Microservices, which can become complex, which leads to my concern; operations and management of Microservices at volume will be a problem that needs to be addressed.
,
,
,
,In Enterprise IT, I believe that there will be a percentage (my guess is 10-15%) of solutions that still require a transactional-oriented RDMBS/SQL solution. These will remain into the ,future until this type of construct is built into other solutions (if that even makes sense). For all of the other SQL/Legacy systems, they will be in a transitional state where they are used by legacy applications. Making the move will take a decade or more, but the constraints on businesses stuck on Legacy SQL systems will force them to move. In hybrid environments and during these transitional states, synchronizing between SQL tables and a bucket looks to be a viable approach.
,
,
,
,
,  "




"
,
,
, is the Vice President Data Analytics and BI at ,, a leading interactive entertainment company for the mobile world and creator of games such as Candy Crush Saga. King listed on the NYSE in March 2014 (NYSE: KING). 
,
With more than 17 years?€? experience in data science, at King, Vince oversees a team working on all aspects of analytics and business intelligence, including downstream and upstream data pipelines, data warehousing, real-time and batch reporting, segmentation and real-time analytics ?€? information which is used across the business to improve the player experience. 
,
Vince holds a Master of Arts and PhD in Complex Systems from Harvard University and a Bachelor of Arts in Mathematics from the University of Cambridge. He is also a published author, having co-authored ?€?A NASDAQ market Simulation: Insights on a Major Market from the Science of Complex Adaptive Systems.?€?
,
Here is my interview with him:
,
,
,
,: Most of our data looks at the attempts players are making in our games?€? levels ?€?such as scores, stars, time-stamps, whether or not the players succeeded or failed in a level and also what aspect made players fail. We also track the help players give to their friends - for example sending lives or moves - and finally, any financial transactions.
,
One of the great features of our games is that you can play anytime, anywhere and on any device. From a data point of view, this can present a challenge in that we need to carefully merge together very varied data and identifiers to create a clear picture of the individual player. This can be hard because of the data volume (10-20 billion events/day), and because the data is (as all big data is!) noisy. 
,
But getting this right is super important ?€? there?€?s a big difference between someone who has bought a new device to play on (and therefore doesn't play much on the old device) vs someone who is losing interest and hardly playing on their only device. Secondly, we of course only observe behavior, but for insights to be usefully actionable we need to deduce something about the player?€?s motivation/state-of-mind, so that we can take the right action. That is hard!
,
,
,
,King really cares about the long-term perspective, so perhaps the most common case is about predicting or modeling the long-term customer-lifetime-value impact of a game/network feature-change on particular groups of players.
,
Another common, but very difficult, case is that of predicting that a player is going to stop playing. This is hard because it is only useful if we can predict this very early, for example while the player is still playing.
,
,
,
,We've had to adapt several times over the last 3-4 years. ,The first big change came when our games took off on Facebook?€?s platform and we put Hadoop into action to allow us to scale to millions of players. Hadoop has served us well through our explosive growth with the tremendous success of our Saga games to 100s of millions of players on mobile as a relatively cheap, effective ?€?supertanker?€? for (today) 2 petabytes of compressed data. I say ?€?supertanker?€? because Hadoop is big, solid, but not very nimble. 
,
So, over a year ago we added an Exasol system alongside Hadoop ?€? this is a very fast, in-memory analytics database ?€? and allows us to do our core processing (the ETL for example) much more quickly ?€? reducing nightly processing from 8-12 hours down to 1-2. There are a few key lessons here. 
,
,Firstly, at our scale (149 million daily active users as of Q4 2014) there is no one-size fits all solution ?€? hybrid solutions such as our Hadoop/Exasol combination are needed. Secondly, we recently let our Hadoop cluster fill up a bit too much, which made it surprisingly fragile. Best to leave a decent amount of headroom.
,
On the BI side we've used QlikView for our standardized reporting for several years and that has generally served us well. At our scale of many ,100s of millions of all-time players (of which some 356 million monthly active), building reports that allow us to quickly slice?€?n?€?dice by cohort, country, game, segment, acquisition channel, platform can be problematic. QlikView can sometimes feel a bit sluggish, and it isn't very self-aware (i.e. it surprisingly doesn't provide much support for monitoring the business user?€?s experience of report latency, load-time, response time, etc). But overall we?€?re comfortable with its pros/cons vs other things on the market right now.
,
All of our data infrastructure is in-house, although we keep ourselves aware of what?€?s going on in cloud services.
,
,
,
,Well I could speak for hours about A-B testing, so let?€?s focus on just a few core difficulties. First of all in our industry (free to play games), almost all metrics are very skewed/long-tailed ?€? this means small numbers of players can easily skew the analysis. Second we?€?d like our players to have a consistent experience across their multiple devices, so that complicates testing. Third, sometimes effects/impacts are subtle and so we might need to run an experiment for quite a long time to get a conclusive outcome. Fourth, we?€?d like what we learn in one game to be insightful enough that it informs decisions in other games, but its much easier to design AB-tests that tell you about a specific game than it is to design tests that tell you about the player.
,
Recommendations? Employ some good data scientists to deal with the statistical subtleties! Decide whether you need to know and precisely quantify ?€?the answer?€? or just be sufficiently confident you can make a sensible decision ?€? that will allow you to decide when to run an experiment for a short vs long time. Lastly focus on the player and their experience in the experiment design stage, rather than just ?€?does feature A or feature B give me better metrics??€?.
,
,
,
,
,  "









"
,
,
This week on /r/MachineLearning, we have articles on topics like hardware selection, word vectors, and CNN-based graphics engines.
,
,
,
,
This post is a nice article about vector representations of words in NLP. It begins with a more general introduction and moves on to practical examples using word2vec and gensim. This is a nice introduction to NLP with word vectors.
,
,
,
This presentation focuses on how machine learning can be applied to more general software development problems. It?€?s an interesting and well-delivered presentation that should be relevant to anyone working in implementing systems. Give it a watch.
,
,
,
It seems the intersection of GPUs and deep learning has become a very popular topic on /r/MachineLearning lately. This post adopts an analysis and review structure, detailing exactly what to look for in creating a consumer deep learning focused machine. If you want to build a system for processing large deep learning workloads, this is an excellent place to look. It even summarizes the suggestions at the end for easy digestion.
,
,
,
This self-post has a member of the community asking others if they would be interested in interviewing them to practice for a job interview. Surprisingly enough, someone offered their time to do so. Based on the responses to the post, it seems that there?€?s a demand out there for machine learning and data science interview practice and questions. The resource linked in the comments, ,, looks like a good resource, but it may be interesting to see a collaborative space where people can practice interviewing one another.
,
,
,
This article details a deep convolutional neural network approach to the problem of interpreting the structure of an object in an image. If you?€?re interested in this problem or CNNs, the paper and the code are both provided in the link.
,
,
,
,  "


"
,
,
, is the Director of Styling Algorithms at , in San Francisco.  His team uses data and algorithms to improve the selection of merchandise sent to clients. Prior to joining Stitch Fix Brad worked with data and predictive analytics at financial and technology companies. He studied applied mathematics at the University of Colorado at Boulder and earned his PhD in Statistics at Stanford University in 2012.
,
Here is my interview with him:
,
,
,

,: Stitch Fix is an online personal styling service for women. Clients sign-up online and tell us about their style and fit preferences. We then choose five items that we think she will love and send them to her in a shipment that we call a ?€?fix"". The key is that unlike traditional e-commerce where a client shops for themselves, we select all of the items for the client, acting as their personal stylist. After we ship the items to the client she tries them on at home and sends back whatever she doesn?€?t want to keep.  There is a $20 styling fee to receive a fix, but the cost is applied towards her purchase if she keeps at least one item. The client can send any or all of the merchandise back and we pay for shipping both ways -- therefore, it's critical we find things that will delight her.

,
Stitch Fix takes a unique approach to styling. We combine art and science by using both machine recommendations and expert human judgement to choose the five items that we send to clients. Data, algorithms and analytics are a key part of every aspect of our business from choosing the items to send to clients to deciding what inventory to hold in the first place. 
,
,

,

,As part of our signup process clients complete a style profile to help us understand their taste and preferences. This is very important since we offer a broad variety of inventory and the more we know about the client the better we can personalize their fixes. Clients can also share a Pinterest board with us with samples of their favorite styles or fashions. This is a great way for our stylist to get a visual sense of a client?€?s style.  

, ??,
In a larger sense the client profile is really just the beginning. One of the most interesting and important sources of data we use at Stitch Fix is client feedback. After a client receives a fix they leave thoughtful, detailed feedback on everything we?€?ve sent them. The feedback includes both structured data (optimized for machine consumption) and unstructured data (optimized for human consumption). The great thing about a client?€?s feedback is that it not only helps us when styling her next fix, but it also helps us learn about the items that we sent her and to improve the recommendations for all of our other clients.
, ,
,
,

,At Stitch Fix we combine machine predictions and human judgement to select the items that we send to clients. This lets us enjoy the best of both worlds. Our algorithms find patterns and structure in the data we collect and we use this to make recommendations for what to send in a fix. This is what machines are best at: distilling large amounts of data into simple predictions that we can evaluate empirically. Our stylists then curate these recommendations and make the final selection for the client. This allows our stylists to focus on the things humans are best at: processing unstructured data (such as requests from clients and the Pinterest boards mentioned above), developing a relationship with clients and integrating these disparate sources into a holistic view of the client.

,
The combination of human and machines works better than either on their own. But it also introduces challenges. Recommendations are generally made for the end consumer (such as when recommending music or movies) but at Stitch Fix our machine recommendations are made for our stylists. This gives us another type of feedback: a stylist can choose not to select something that we?€?ve recommended. It also introduces different types of selection bias to our data. For example, if stylists (sensibly) never sent heavy winter wear to clients in hot climates during the summer we would never observe the outcome empirically. It might be the right thing to do, but when it comes time to train a model you wouldn?€?t have any data about winter wear in hot climates. Addressing this requires considering not only what stylists send to clients but also the things they choose not to send.
,
,
,
,
,  "


"
,
,
,
This annual award was established by ACM SIGKDD in 2008 to recognize excellent research by doctoral candidates in the field of data mining, data science, and knowledge discovery.The KDD Doctoral Dissertation Award winner and up to two runners-up will be recognized at the KDD conference, and their dissertations will have the opportunity to be published on the KDD Web site (,). 
,
The award winner will receive a plaque, a check for $2,500. The award winner will also receive a free registration to attend the KDD conference. The runners-up will receive a plaque at the conference. The winner and runners-up will be invited to present his or her work in a special session at the KDD conference.
,
,
,
The final dissertation defense should take place at the nominee's host institution before the submission deadline. Furthermore, the final dissertation defense must not have taken place prior to ,. Nominations are limited to one doctoral dissertation per department or academic unit. Submissions must be received by the submission deadline.
,
 Each nominated dissertation must also have been successfully defended by the candidate, and the final version of each nominated dissertation must have been accepted by the candidate's academic unit. An English version of the dissertation must be submitted with the nomination. A dissertation can be nominated for both the SIGKDD Doctoral Dissertation Award and the ACM Doctoral Dissertation Award.
,
,

 Submission Deadline: April 30, 2015.,

 Notification of Awards: July 1, 2015.,

 Award Presentation at KDD 2015: August 10-13, Sydney, Australia.,

For submission procedure and additional details visit 
,
,
,
,
,  "

"
,
,
, is the Director of Styling Algorithms at , in San Francisco.  His team uses data and algorithms to improve the selection of merchandise sent to clients. Prior to joining Stitch Fix Brad worked with data and predictive analytics at financial and technology companies. He studied applied mathematics at the University of Colorado at Boulder and earned his PhD in Statistics at Stanford University in 2012.
,
,
,
Here is second and last part of my interview with him:
,
,
,

,: Fashion is complicated. To make our recommendations we use data about our clients, our inventory and the ,historical data from all past fixes. A lot of this is data that we create ourselves. For example, when our merchandising team buys new inventory they create structured data describing the items in great detail. This isn?€?t easy and requires the time of fashion experts. Getting the data right is very important, as is understanding the different factors that contribute to whether a client will love her fix. It?€?s not enough for clothes to fit - they also need to match a client?€?s style and price preferences and complement her current wardrobe. 
,
,

,
,The amazing effectiveness of combining humans and machines continues to excite me. The key to this approach is finding the strengths of each. Learning where the boundary should be and how to optimally use feedback is an extremely interesting problem - I am sure we will see much more of it in the coming years. 
,

Another important aspect of our business is how closely our clients are aligned with us. It may sound obvious but it?€?s actually quite important. Our clients want us to do a good job, and they want us to get better with feedback. This encourages them be effusive and thoughtful with their feedback - a critical source of data for all parts of our business.
,
,
,

,I?€?m most at home in Python, but am excited by the ever improving ecosystem of data science resources. Tools like the iPython notebook and pandas in Python and dplyr in R are making it easier and easier to manipulate and quickly visualize data. The trend toward easy reproducibility is important in industry, and certainly for academia as well.

,
On the statistical side, I enjoy following the continued development of tools for getting the most out of sparse data. I?€?ve also found myself very interested in some more classical statistical tools, such as random and mixed effects models.
,
,

,
,I studied applied math as an undergraduate and found that I was always the most interested in projects that involved data. This led me to study statistics in graduate school. I did several internships during the summers and found that I loved the practice of applying statistics and machine learning to solve business problems. I was lucky to graduate at a time when statisticians and data scientists were taking on an ever larger role in industry.
,
,
,

,For statisticians and data scientists especially I think the best advice is to stay close to the core business of the company they work for. This makes it more likely that you?€?ll be working on the projects that are important to the company - this is generally exciting and comes with more opportunities. This is also a good test when evaluating a prospective employer: do they have interesting data science problems that are central to their business? If not, you could find yourself far from the action.
,
,
,

,Two very important qualities are a maturity in framing problems and building models and a comfort with ambiguity. The first usually comes with experience working with data and models. We look for people who can take a business problem and identify approaches for solving it with data. The best candidates are often the ones who know when a simple solution is the right one and don?€?t overcomplicate their approaches. Comfort with ambiguity is similarly important. Many people can excel when a clear problem is presented to them, but the best are those who can find the problem in the first place. 
,

,

,
,The last book I read was ?€?,?€? by Brad Stone - a fascinating telling of the rise of Amazon. In my free time I enjoy hiking with my wife and dogs.
,
,
,  "
"
,
,
June 30 - July 3, Aalborg, Denmark.
,
The open source R language has become the tool of choice for data
scientists in the last few years. The main annual event of the R
community, the useR! conference will start June 30, 2015 in
Aalborg, Denmark.
,
The invited speakers include Thomas Lumley (R Core, survey), Adrian
Baddeley (spatstat), Di Cook (GGobi), Steffen Lauritzen (gRaphical
models), Romain Francois (Rcpp) and Susan Holmes (phyloseq).
,
Don't miss this opportunity to learn about the most widely used R
packages, hear the latest developments and connect with the best of
the best in the R community.
,
Register and submit abstracts here: ,
,
Best regards,
,
Torben Tvedebrink
,
the useR! 2015 Organizing Committee
,
,
,  "
"
Most popular 
, tweets for Mar 16-18 were
,
,
,  "
"
,
,
I had my first encounter with Big Data when working on a UN Population Fund project called ?€?Bachue?€? more than 30 years ago.
,
To finish the assignment, I had to automate linear equations, which I did that by using Artificial Intelligence (AI) basic theory. In that process, I realized that it I had a self-learning algorithm which would cover not only numerical variables but any type of variables. 
,
Pretty soon, it became evident that the parametric statistics was not a viable option for non-numeric variables. Therefore, the solution was in the probability space of non-parametric statistics. 
,
In the academia and science world, I developed the theory of self-learning in order develop automatic statistical software: which he applied with success.
,
The first self-learning practical application was in clustering and decision tree technology which I developed in mid 90?€?s. The software was able to use any type of variables.
,
The next software I developed, was an Automatic Decision-Tree technology which was using multiple interaction detection technology instead of a binary-split approach.
,
Understanding that ?€?real world?€? would be the most exciting place to apply self-learning, Artificial Intelligence statistical software, I moved from university to the private enterprise world.
,
In my first company, Generation5, I developed the first commercial, Automatic Statistical Software called MWM. 
MWM was proved to more than 80 Blue Chip Companies to be most the effective prediction ?€?Brain?€?.
,
,
In my second company ?€?Infer systems?€?, I developed an Automatic Statistician for Real-Time-Bidding. Both companies were bought out and are successfully still doing business today. 
,
In my third company, I developed a new ,, which is not only fast but also much more accurate than any human or manual method on the market today.
,
This success was achieved by introducing Automatic Multi-Modelling technology. The predictions are no longer constrained by the Best, ?€?One model?€? technology: and this has brought a never seen before increase in prediction accuracy.
,
In addition, by creating Artificial Intelligence which does not require any transformation of original variables-  it has eliminated the variable transformation cause of prediction error.
,
I was always guided by the principles of efficiency, simplicity, accuracy and reliability principles.
,
I developed Dr. Mo ?€? Automatic Statistician to be:
,
,
Beside common types of one variable predictions, Dr. Mo has been used for multiple variables predictions and, even more important, for longitudinal predictions.
,
Dr. Mo has been successfully been tested on more than 267 different databases with applications ranging from: finding the best customer to predicting gene sequencing for particular proteins to predicting healthcare needs by region, to consumer or political preferences etc..
,
With the ability to use Big data with great accuracy and speed like never before, I hope that Dr. Mo will be applied to help prevent or cure diseases, to preventing world poverty to help businesses thrive by making more accurate and best decisions!
,
Bio: , started his science and university career in former Yugoslavia at Belgrade University.  In his latest firm, ,, Dr. Krneta developed automatic prediction software, called Dr. Mo, Automatic Statistician. 
,
,
,  "
"
,

,

,

,

,
,
,
,

,

,

,


,

,

,
,
,
,

,


,

, 

,

,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,
,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
By Jeff Leek (,) 
,
,
,
,
,
,
,
,
,
,
,

 The reason is that inevitably the data are a mess and you have to clean them up, then you find out the data aren't quite what you wanted to answer the question, so you go find a new data set and clean it up, etc. After a ton of work like that, you have a nice set of data to which you fit simple statistical models and then it looks ,to someone who either doesn't know about the data collection and cleaning process or doesn't care.
,
,
,:  ,
,
, is a professor at Johns Hopkins, where he does statistical research, writes data analysis software, curates and creates data sets, writes a blog about statistics, and work with amazing students who go do awesome things. 
,
,
,
,
,
 ,  "
"
ComputerWorldUK has released their , of the top 10 UK Big Data Professionals.  We have listed them below and added their best URLs.
,
,
,
,
,
,  "
"
        ,  "
"
, is offering 2 day extensive Real-Time Data Developer Conference. This is a fast paced, vendor agnostic, technical overview of the Apache Spark??? landscape. Spark is fast general-purpose cluster computing framework. It is one of most active open-source software. Organizations are switching from Hadoop MapReduce to Spark. 
,
This 2-day Conference will include technical sessions, Use Cases and hands-on sessions. On day one, we will introduce Spark Core Concepts, Scala , SBT & Labs.  Day two will consists of Spark SQL, Spark Streaming, Machine Learning , Spark MLlib, Data Frames, Advanced Spark & hands-on sessions. Also, Technical talks on Storm,  Flink , Kafka & Lambda architecture will be covered.  You will practice the concepts that you learned on day one & day two. You will get the opportunity to do different types of data analysis using Scala, Spark and Spark libraries on your laptop.  
,
,
,Engineers, Developers, Architects,  Data Scientist, Networking specialists, Managers, Executives, Students, Professional Services, Data Analyst, BI Developer/Architect, QA, Performance Engineers, Data Warehouse Professional, Sales, Pre Sales, Technical Marketing, PM, Teaching Staff,  Delivery Manager   
,
,
,, 5001 Great America Parkway, Santa Clara, CA 95054
,
,
,
Use Promotional code KDNUGGETS and register
,
,  "
"
        ,  "
"
,
,
, is Executive Director for Data and Analytics at ,. She works across the GE businesses to drive analytics development leveraging big data technologies. During her time at GE, she has built an innovative team of big data engineers and data analysts, focused on delivering scalable business outcomes. She is passionate about data and analytics to aid cross functional teams to derive data insights, aid teams in articulating questions they did not know they had and help view data in more effective ways.
,
Beena has over 23 years?€? experience in the data arena with a number of international organizations including  British Telecom, E*trade, Thomson Reuters and Silicon Valley startups in engineering and management positions. She holds a Masters in Computer Science and MBA in Finance.
,
,
,
Here is second and last part of my interview with her:
,
,
,
,
Historically, you think about solving things through traditional data warehousing concepts and business intelligence, but with this push of IoT, or the industrial internet, there?€?s this whole thought process around the information coming off of machines. Being able to scale to support the amount of information that we were seeing coming from the OT space, or operational technology, the technology that relates to power plant, or a rail provider. That sort of backdrop has driven us to think about it in a way we haven?€?t had to before.
,
,
,
,I think one of the current big data trends that I am most excited about is security and governance for big data tools and technologies. There has been a lot of progress made in the past year but we are still so far from solving it.

,
,
,
,One of the biggest lessons I have learnt is that data scientists cannot operate in silos. Deep domain expertise is key in generation of robust algorithms. ?€?Know Thy Data?€?. Data Science is not just Science ?€? it is a combination of creativity, curiosity, story-telling and science ?€? being able to grasp the wider context to solve real business problems - the ability to understand the meaning of the data sets when applied to a machine operating in the real world and make the machine smarter.
,

,
,
,I have always been a data geek; fascinated with data and the outcomes we can drive leveraging analytics. I have worn multiple hats in the past 20+ years in the data arena. I like the fact that we can solve real problems and impact human lives using data and analytics like we are doing at GE. It?€?s so much more than making a social media experience better. 
,
Also, this is a constantly evolving area and there is so much more to learn. I don?€?t think there?€?s anything more satisfying than identifying hidden patterns in data, solving the puzzle and then using the solution to drive impactful outcomes.
,
,
,
,I read a lot. Blogs, newsletters, subscriptions, posts and feeds. The key is to be able to distinguish between the noise and the information that is out there. I also speak at technology, data and analytics events, where there is a gathering of like minds and are truly a great way to stay updated on trends and the latest advancements.
,

,
,
,Besides having the necessary technical skills and educational background, I think being curious is a very important quality ?€? being able to ask a lot of questions, finding correlations that are not obvious, being inquisitive and having a tenacious attitude. They should be passionate about data and be able to communicate with data.
,
,
,
,I just completed Rich Karlgaard's "","" ?€? it makes an excellent compelling case for businesses focusing more of their energies on the human, more creative aspects of business ?€? it elevates human capital to its proper place at the top of the corporate pyramid. As opposed to ?€?the hard edge?€? (speed, cost, supply chain, logistics, and capital efficiency) all of which are easy to measure, ?€?the soft edge?€? is more difficult to measure but so necessary for a company to thrive and be great.
,
When I am not working, you will most likely find me at the baseball field with my two boys. I also like to read, fly planes and I am a big foodie.
,
,
,  "















"
,
,
,
,
,
,
,
,
,
,
,
  "




"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,  "
"
The list below based on the , compiled by Pedro Martins, but we added the book authors and year, sorted alphabetically by title, fixed spelling, and removed the links that did not work. The descriptions are by Pedro.
,
,
,
Bio: , is one half of Data On Focus team, fascinated with all issues related to IT. He is in mid-twenties, from Portugal, has an informatics engineering background, and passion for data mining and data science.

,
,
,  "
"
By Gregory Piatetsky,  
,, Mar 25, 2015.
,
,, a database pioneer who turned many of his ideas into practice, won the 2014 
, Turing Award for fundamental contributions to the concepts and practices underlying modern database systems. 
,
The , says
,
,
Stonebraker was a Professor at Berkeley for 29 years, where he developed the influential Ingres and Postgres relational database systems. Currently he is an adjunct professor at MIT CSAIL where he has been involved in the development of the Aurora, C-Store, H-Store, Morpheus, and SciDB systems. He is also co-founder and co-director of the Intel Science and Technology Center for Big Data at MIT.
,
Stonebraker turned his research into practice and was a founder of many successful and influential companies, including Ingres, Illustra, Cohera, StreamBase Systems, Vertica, 
,, 
, and 
,.
,
The ,, widely considered the ""Nobel Prize in Computing,"" carries a $1 million prize with financial support provided by Google. 
,
Recently I attended several of Michael's talks and his presentations were usually the highlight of every meeting - always insightful, interesting, and brilliantly presented the core issues.  I am surprised it took ACM so long!
,
Here are some of his recent presentations :
,
,??,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,, DC, DACBSP, CSCS is managing director of sports medicine at , . He is a chiropractic physician and conservative evidence-based sports medicine clinician. In his current role, he oversees the organization?€?s sports medicine activities ?€? including providing medical care to U.S. athletes at the Olympic Games and flagship U.S. Olympic Training Center in Colorado Springs, Colorado ?€? and the USOC?€?s National Medical Network.  
,
He served as Team USA?€?s medical director at the London 2012 Olympic Games and the Sochi 2014 Olympic Winter Games, in which he was responsible for administering medical care to more than 500 athletes as provided by more than 100 medical providers. 
,
Dr. Moreau has more than 30 years of clinical expertise in both general practice and sports medicine. He serves on the editorial review board of the Journal of Chiropractic Medicine and has served for many years with the American Chiropractic Board of Sports Physicians.
,
,
,
Here is second part of my interview with him:
,
,

,: We are making steady headway using data to drive change. A great case study is our women?€?s wrestling program.  Since developing the Elite Athlete ,Health Profile (EAHP) program, which includes individual athlete risk profiling and development of injury prevention programming, there has been a 60% reduction in injuries that required surgical intervention for this team.  Interventions that were identified by the EAHP included programs to reduce shoulder dislocations/instability injuries and non-contact ACL injuries. To date these programs have been extremely effective over a four year term. We continue to track this specific analytic over time.
,
,
 ,
,Concussion can present in any athlete participating in any sporting activity. The best methods of management are to address the first step of injury prevention by establishing a better understanding of injury incidence across the many sport populations we work with.  Concussion best practices are evolving. Through the focused measurement and evaluative processes we now have a better grasp on concussion incidence in the sports we work with. 
,
USOC Sports Medicine is dedicated to researching concussion to gain a better understanding of the epidemiology of concussion across the many sports we serve as well as the natural progression of concussion symptoms and best practices for recovery in the elite athlete. 
,
The key to concussion management is to ensure that the athlete has recovered from their injury before they are returned to play. We foresee that we can use our sports medicine analytics to serve as a base of evidence to evaluate existing recovery strategies as well as investigate and provide new safe interventions to help athletes to safely return to sport from a concussion injury.  This return to play decision is a key and monumental task because of the individual variability in concussion presentation and recovery. We do not use time to dictate the recovery from concussion, rather each individual with concussion must be assessed and managed based upon their specific presentation with considerations to many modifiers and variables. 

,USOC Sports Medicine collaborates with International leaders in providing concussion research to best protect athletes and better understand best practices in concussion management and return to play decision processes. 
 ,
,
 ,
,Data mining from the electronic health records with forward looking and retrospective analysis will identify correlations between existing and newly available metrics regarding the correlation to health and performance outcomes. These findings will drive and further allow for new discoveries in health and human performance now and with an exponentially increasing return in the next few years. ,
,
,
The ultimate multivariable subject is humankind. Each of us has our own assets that both mitigate and drive risk of injury and illness. Over a course of time there will be identified commonalities for many injuries and illnesses whereby the variable that can be changed will be changed to lead to predictive preventive outcomes. My perspective is this is the single most important area that drives my thoughts around the application of data; to preserve an individuals?€? health.      
 ,
,A part of the first question may be answered through wearable technology. Anyone with a heartbeat can see the exponentially increased data available to sports medicine practitioners and scientists though wearable technology.  Many biomechanical and biomedical measurements that were only available in the laboratory setting are now readily available in field settings through reasonably priced clinical wearable packages. 
,
Taking technology to the field allows individuals, researchers and clinicians to record, measure and provide real time biofeedback with previously unavailable measurement technologies such as force pates, pressure sensors and 3-dimensional motion analysis and a vast array of other measurements.
,
,
,
,
,  "
"
,.
,
 ,

,

,
,
,
,
,
,
,
,
,

,
,
,
,

,  "
"
Most popular 
, tweets for Mar 19-22 were
,
Tensor methods for #MachineLearning: fast, accurate, scalable, need open-source libs , 
, 
,
,
,
Tensor methods for #MachineLearning: fast, accurate, scalable, need open-source libs , 
, 
,
,
Tibco survey #BigData top use cases: Customer & Experience Analytics, Risk/Threat Analysis 
, 
,
,
,
#DataScience and Reproducibility: Explaining when the experiment does not work 
, 
,
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Mar 24 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,

,
,
,
,
,
,??,??
,
,
,
,
,
,
,
,
,
,  "
"
,
,
, is Executive Director for Data and Analytics at ,. She works across the GE businesses to drive analytics development leveraging big data technologies. During her time at GE, she has built an innovative team of big data engineers and data analysts, focused on delivering scalable business outcomes. She is passionate about data and analytics to aid cross functional teams to derive data insights, aid teams in articulating questions they did not know they had and help view data in more effective ways.
,
Beena has over 23 years?€? experience in the data arena with a number of international organizations including  British Telecom, E*trade, Thomson Reuters and Silicon Valley startups in engineering and management positions. She holds a Masters in Computer Science and MBA in Finance.
,
Here is my interview with her:
,
,
,
,Through the connection of brilliant machines with people at work, or what we call the Industrial Internet, GE uses data analytics to find new ways to address major global challenges and improve healthcare, increase transportation and energy efficiency, and eliminate waste across every major industry. Analytics are one way that we drive new services offerings to help our customers optimize the availability and performance of their assets and operations.
,
A few examples:
,
The airline industry spends $200bn on fuel per year so a 2% saving is $4bn. GE provides software that enables airline pilots to manage fuel efficiency.
,
Another product, Movement Planner, is a cruise control system for train drivers. ,The technology assesses the terrain and the location of the train to calculate the optimal speed to run the locomotive for fuel economy.
,
Vegetation falling on power lines is the biggest cause of electricity outages. Software and analytics developed at GE is now being used by a Canadian electricity supplier to prune back trees cost-effectively along its electricity distribution lines.
,
In a wind farm the wind turbines at the front affect the turbines behind them. This may lead to vibrations causing a failure due to a stress fracture on the turbine's blades. We can adjust turbines based on the wind. We adjust the blade in real time to avoid vibration. As a result GE is able to deliver a 2 to 5% improvement on the efficiency of the wind farm.
,
As you can see, these are really impactful outcomes leveraging data science.
,

,
,
,When 1 billion people become connected two decades ago, the term ""Consumer Internet"" was coined. It speaks to the phenomenal transformation and disruption we have seen in the consumer segments, whether it be media, entertainment, marketing, advertising or retail. We believe the same thing is happening in the industrial world, as 50 billion machines get connected over the next 5-7 years. Everything is going to be connected, measured and managed.
,
,The Industrial Internet connects brilliant machines with people at work and data analytics to find new ways to address major global challenges and improve healthcare, increase transportation and energy efficiency, and eliminate waste across every major industry. The Industrial Internet will unleash a productivity revolution to build, power, move and cure the world.
,
The first frontier of the Internet gave consumers a voice, but the next frontier ?€? the Industrial Internet ?€? gives every machine a voice to predict and prevent problems and improve performance for a stronger, faster, cleaner and safer world. The global economic impact of building cleaner, safer, more productive railroads, airlines, hospitals and power plants will transform industry and help our customers be more efficient and productive. By eliminating downtime, waste and guesswork, the Industrial Internet will save hundreds of billions of dollars, unleashing a productivity revolution.
,
We believe that the industrial Internet could have a $15 trillion dollar impact on global GDP over the next 15 years.
,
I see two key differences between consumer internet and industrial internet. If you are not able to log in to your social media account, you get annoyed; but if your power goes down, you get angry, or fearful, or people in hospitals may die. Therefore, reliability is essential in the industrial world.  A second challenge is durability. People are buying machines that last 20 years. That means we have to create software with a life cycle that might seem absurd in the consumer world.
,


,
,
,Predix is GE?€?s software platform for the Industrial Internet. It provides software tools for developing and running applications that monitor machines and other devices, collect and analyze data about their performance, and deliver insights to users that help optimize industrial assets and operations.
,
Predix is built to address industrial-scale data and analytics and to provide a consistent standard way to connect machines, data and people. We look at four key capabilities that are the foundation for our solutions. Predix is optimized to connect machines, enabling them to become more intelligent and optimize from anywhere in the network. It is optimized for Industrial Big Data, real-time large scale analytics and asset management.

,Predix is architected for rapid design and deployment of collaborative apps to deliver the right information at the right time to drive outcomes. It employs leading edge security technologies and techniques to help protect industrial data and controls access to machines, networks, and systems. In 2011, we setup GE Software, a Center of Excellence, in the Silicon Valley focused on building the Predix platform.
,
,
,
,
,  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is a Scientist, Technologist, Author, Coach, Faculty, Speaker, and is a pioneer in the fields of data science, big data, analytics, and emerging technologies. He is currently chief data scientist at ,?€?s Land-mark PSL, leading expansion of integrated workflow capabilities and other data development initiatives.
,
Dr. Priyadarshy has appeared as speaker at several international conferences, and has written, co-authored, presented and published numerous research papers in peer-reviewed journals and magazines. He has held various leadership positions in AOL, Network Solutions, Acxiom Corporation prior to joining Halliburton. He was recently named to The Financial Times?€? list of potential board candidates with emerging technology and analytics expertise. Dr. Priyadarshy is an adjunct faculty at Georgetown University.  He is currently senior fellow at the International Cyber Security Center at George Mason University. He is advisory board member at multiple organizations including Big Data Summit, Virginia Tech?€?s MBA Board, etc.  

,
Dr. Priyadarshy holds a Ph.D. from Indian Institute of Technology in Bombay, and an MBA from Virginia Tech.
,
Here is my interview with him:
,
,
,
,: ,, founded in 1919, is one of the world?€?s largest providers of products and services to the energy industry. The company serves the upstream oil and gas industry throughout the lifecycle of the reservoir. The life cycle phases include locating hydrocarbons, managing geological data, drilling and formation evaluation, well construction, well completion, and optimizing production through the life of the oil field. 
,
For each of these phases during the lifecycle of oil field, data and analytics play a significant role. However, based on which phase were talking about, some leverage a significant amount of knowledge creation through scientific and 1st principle models, augmented with data analytics, while others rely on data analytics more. The product service line that I am in is called Landmark. Landmark is a leader in providing Analytics platform for the upstream oil and gas industry that is used internally by Halliburton as well as by majority of the oil and gas companies. 
,
,
,
,Yes, the deployment of IOTs, Sensors and M2M communication impacts the analytics in oil and gas in-dustry. For example, earlier it was fine to get the data from the remote sites and process in 24 hours or more, but as the new communication networks enable faster movement of data, the expectation for turnaround analytics based information reduces to near real-time. 
,
,
,
,The first challenge as I see is ?€?Data in Silos?€?. Silos by definition restrict innovation.  As we know the com-panies that have successfully created significant value from their Data, is mainly due to the ?€?Data democratization?€?.  The data democratization does not mean that one has let go data governance, privacy, and security requirements. Oil and gas industry is very slow in moving towards opening their silos for analytics purposes. One cannot find new patterns in the data if limited set of data is available to you. 
,
The second challenge as I see is the agility in adapting emerging technologies for creating value from the data that is available. For example, the world of scientific visualization and information visualization is fast moving into ?€?browser/client?€? with minimal round trips to servers, using new technologies, but Oil and Gas industry is still embedded with desktop version of the software, which has great degree of limitations. 
,
The third challenge as I see is that the unlike other industry verticals, Big Data in oil and gas needs to work with multivariate, multidimensional, multi-variant, stochastic and scientific data. Again, based on which phase of oil field lifecycle we are discussing the variability and value from the data is going to be different.    
,
,

,Traditionally, the Big Data has been described as Velocity, Volume and Variety (3Vs).  Later on Veracity was added by various organizations and that made it 4Vs.  However, I feel this confuses people, as it is incomplete way to describe Big Data. 
,
So, after few iterations, I have added three more V?€?s = Value, Virtual and Variable. Let me explain.  In the era of data explosion, we all generate data and consume the data ourselves. However, lot of data that we generate has no value for us, as in case of smart phones, but the same data has significant value for the phone service provider, the device manufacturer, and the app creators. Hence, Big Data must have Value.  
,
The next V, I added is Virtual, because the old thinking of moving data and putting in the silos is outdated. As long as we have the ability to connect through data pipelining the data one can leverage the data sets anywhere within the enterprise for big data analytics. 
,
Now, the last one is Variable - it refers to the fact that for different use cases there is variability in the combination of these 6 Vs - Volume, Velocity, Variety, Veracity, Value and Virtual.  If one compares the seismic, drilling phases of oil field lifecycle, the volume of data is much different in these two phases, to an extent quite of real-time analytics is needed in drilling phase compared to seismic so velocity is different in these two phases, but the value in seismic is large compared to drilling, because any predictability with seismic data leads to the beginning of the oil field. 
,
,
,
,
,  "
"
Most popular 
, tweets for Mar 23-25 were
,
Key #DataScience question: Do We Need More Training Data or More Complex Models? #BigData 
, 
,
,
,
24 free resources and online books on #DataMining, #DataScience, #MachineLearning, #Statistics 
, 
,
,
,
Key #DataScience question: Do We Need More Training Data or More Complex Models? #BigData 
, 
,
,
Very useful: New R Online Tool for Seasonal Adjustment of time series #DataScience #rstats 
, 
,
,
,
,  "




"
,
,
,
,
,
,
,
,
,
,
  "

"
,
,
,, DC, DACBSP, CSCS is managing director of sports medicine at , . He is a chiropractic physician and conservative evidence-based sports medicine clinician. In his current role, he oversees the organization?€?s sports medicine activities ?€? including providing medical care to U.S. athletes at the Olympic Games and flagship U.S. Olympic Training Center in Colorado Springs, Colorado ?€? and the USOC?€?s National Medical Network.  
,
He served as Team USA?€?s medical director at the London 2012 Olympic Games and the Sochi 2014 Olympic Winter Games, in which he was responsible for administering medical care to more than 500 athletes as provided by more than 100 medical providers. 
,
Dr. Moreau has more than 30 years of clinical expertise in both general practice and sports medicine. He serves on the editorial review board of the Journal of Chiropractic Medicine and has served for many years with the American Chiropractic Board of Sports Physicians.
,
Here is my interview with him:
,
,

,: The key decisions I make are related to the healthcare of the best athletes in the world. When my team and I can use evidence based medicine to drive decisions, it clearly makes a big difference in the outcomes of care. The USOC is a large sport organization that works with over 50 National Governing Bodies of sport.  The challenges of working with 50 different types of athletes is what makes work fun. 
,
The USOC Sports Medicine team identifies and analyzes the types and amounts of decision driving ,data collected from our electronic health record system, GE Centricity Practice Solution, to produce the best end result for our athletes. The 50 sports have many sub sport specifics and each of these disciplines varies in terms of the metrics measured, and the quality and frequency of collection. Therefore creating data platforms that allow our science and medical personnel to collect, analyze and act on data across sport disciplines is an exciting challenge that the USOC develops on a daily basis. 
,
As we improve our ability to incorporate data into our analyses and processes we have been able to create impactful reports for Team USA. Improved data analysis is a key focus of the USOC to drive medicine as well as high performance outcomes.  
,
,
,
,Improved performance outcomes can include maintaining health as well as sport specific metrics. ,
,
Today?€?s generation correctly expects immediate results from any applications of new technology.  Careful selection of meaningful data products and reports are only impactful when they are shared with end users. Early successes can help create momentum for new data science projects.
,??,
, When working with decision makers it is important to know that you research the right question, maintain your direction, review the results in regards to quality of the information and packaging of the information before advancing the materials outside the research team.  All parties also need to start the process by recognizing that data is a piece in the decision making matrix that will be considered alongside other decision analysis variables.  
,
,
,
,A healthy team competes best. Sports medicine analytics can improve athlete health and prevent injury and illness. The integration across sport science and medicine disciplines leads to helping all invested partners to better understand business practices, clinical care and performance trends which will ultimately create a foundation for measurable resource and clinical applications to better drive decisions directed at health and performance.  
,
For example, if the data science team can identify trends in a team?€?s blood work and report these finding back to the sport medicine, nutrition and physiology departments, an integrated action plan can be made to address the effect of nutrition and physiology on the athlete?€?s response to training. 
,
,
,
,The best predictor of future injury is previous injury.  This holds true across nearly all injury types and sport disciplines.  A well ,performed history and physical examination is the cornerstone to every patient interaction.  The USOC has developed methodologies whereby we can measure and extract from the USOC practice management electronic health record, GE Centricity Practice Solution, to create algorithms to predict indicators of future injury risk that are specific to sport discipline and gender.  As our data set grows the strength and ultimate value of these risk indicators continues to expand.
,
,
,
,
,  "







"
,
,
,, DC, DACBSP, CSCS is managing director of sports medicine at , . He is a chiropractic physician and conservative evidence-based sports medicine clinician. In his current role, he oversees the organization?€?s sports medicine activities ?€? including providing medical care to U.S. athletes at the Olympic Games and flagship U.S. Olympic Training Center in Colorado Springs, Colorado ?€? and the USOC?€?s National Medical Network.  
,
He served as Team USA?€?s medical director at the London 2012 Olympic Games and the Sochi 2014 Olympic Winter Games, in which he was responsible for administering medical care to more than 500 athletes as provided by more than 100 medical providers. 
,
Dr. Moreau has more than 30 years of clinical expertise in both general practice and sports medicine. He serves on the editorial review board of the Journal of Chiropractic Medicine and has served for many years with the American Chiropractic Board of Sports Physicians.
,
,
,
,

,
Here is third and last part of my interview with him:
,
,

,: Sports may have cultures that can be resistant to the implementation of data collection and analysis as well as changes in sport preparation.  Culture typically resists rapid change from what might be preserved as a successful platform to become more successful through new applications of technology driven change. The ?€?culture factor?€? can be a major barrier to implementing data driven science into sports medicine, sports science and coaching programs. 
,
Clearly culture is not an insurmountable obstacle because when the data driven change drives positive performance outcomes this makes a difference. Success can be achieved through careful communication and the appropriate introduction of data science into the sport. Positive results on the field of play rapidly change the culture.
,
,
,
,Young professionals should be encouraged to know their sport. One way to better know your sport is to acknowledge that you will learn more by routinely stepping out of your niche or area of specialty and engaging in conversation with anyone involved in the sport. Everyone has a story to tell and there are pearls of both success and lessons to be learned from defeats.  We all need to be open minded to new topics and new ideas when collaborating with other individuals within the same area of activity as well as those outside your specific area of knowledge. 
,
Take some time to understand the specific area of sport that really drives your inner heart.  Learn to avoid asking all questions, but instead to ask the right questions so you can provide the right answers. Remember the right question for you to solve might not be what you first think is important. Look from above down and see the entire field of play in regards to what the athlete, coach or other end user feels they need to achieve success.  Sometimes the question(s) may seem impossible, but by parsing the question into smaller blocks you can build to the final answer. 
 ,
,Sports Analytics is still very young and there are and will be many opportunities now and in the future.  At the USOC we encourage those interested in becoming an analyst to routinely step out and up to focus on the athlete. This is achieved by engaging in thoughtful conversation with all partners including sport scientists, coaches, sport technologists, and most importantly the athlete to learn better learn their needs. Always be open minded to new topics and ideas. Be a true partner with others within your department and across professional disciplines.  Ask the questions you want to answer and think of the athlete or coach being able to apply the end product to make a change.  
 ,
,
,
,I am currently reading and really enjoying , by Daniel Pink. It is a great read as demonstrated by these lines ?€?When facts become so widely available and instantly accessible, each one becomes less valuable. What begins to matter more is the ability to place these facts into context and to deliver them with emotional impact.?€? That is sage advice.
,
When I am not working I like to plant trees and take care of the woods where I live in Black Forest Colorado. It is always interesting what thoughts drift into my mind while I work in the trees. No electronics are allowed, well I guess I still carry my cell phone, but I do turn the ringer off! 
,
,
,  "








"
,
,
,??,??
,
,
,??,??
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Mar 31 and later.
,
See full schedule at , .
,
,  "
"
,
,
, is a Scientist, Technologist, Author, Coach, Faculty, Speaker, and is a pioneer in the fields of data science, big data, analytics, and emerging technologies. He is currently chief data scientist at ,?€?s Land-mark PSL, leading expansion of integrated workflow capabilities and other data development initiatives.
,
Dr. Priyadarshy has appeared as speaker at several international conferences, and has written, co-authored, presented and published numerous research papers in peer-reviewed journals and magazines. He has held various leadership positions in AOL, Network Solutions, Acxiom Corporation prior to joining Halliburton. He was recently named to The Financial Times?€? list of potential board candidates with emerging technology and analytics expertise. Dr. Priyadarshy is an adjunct faculty at Georgetown University.  He is currently senior fellow at the International Cyber Security Center at George Mason University. He is advisory board member at multiple organizations including Big Data Summit, Virginia Tech?€?s MBA Board, etc.  

,
Dr. Priyadarshy holds a Ph.D. from Indian Institute of Technology in Bombay, and an MBA from Virginia Tech.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: 
Predictive analytics is well matured in many aspect of oil and gas industry. Note that oil and gas industry is upstream, mid-stream and downstream. Predictive analytics more mature in downstream oil and gas compared to upstream, because upstream data has higher degree of complexity, as mentioned earlier. 
,
,
,
,In simple terms, I define KPIs (Key performance indicators) as essential to operate the business. It does not lead to any innovation or additional value creation for the enterprise.  With Big Data analytics, not only one gets KPIs, but is constantly looking for outlier events, and creating new patterns from the every growing and increasingly  connecting data sets. 
,
,

,The explosion of TAPS (Technologies, Applications, Products and Solutions) is one of the key driver for success in Big Data projects. For the data that one?€?s enterprise has, it is possible that various combinations of these agile TAPS will work to find new patterns and discoveries. One does not have to restrict to a fixed number of these TAPS to gain value.  
,
The reasons for failure are few:
,
, ?? ,
,
,
,Some of areas of interest from value creation point of view are:
,
,
From a total cost of ownership point of view, a much cheaper storage that supports data-model free stor-age of massive amounts of historical raw data for easy access would be welcome technology. As the raw data size grows, the cost of traditional Hadoop and other platforms will become significant and not feasible for many businesses. 
,
,
,
,The advice from the cosmic powers is that you should leverage your infinite potential that is bestowed upon you, and have faith in your parent?€?s upbringing to achieve the success you want. Unless you want to build a career for yourself, no one will build your career. 
,
,
,
,The qualities that I look for in a person - if the person is entrepreneurial in nature, it is contrary to large corporation recruitment, but only such people can leverage their computer science and analytics skill, because they need to look for new patterns from the ?€?data?€? on a regular basis. They have to find things that are not obvious.  This also requires that people have patience to deal with failures. As I say, finding gold in gold mine is a dirty and extremely high effort job, it takes typically 28 tons of raw ore to find 0.04 ounce of gold. Big Data analytics is about the same, high value for very small amount of gold. 
,
,
,
,The books that inspire me originate from the land of Bharat, like the , because it tells that a single entity can be successful in many areas, if they work sincerely for it. 
,
,
,  "
"
Most popular 
, tweets for Mar 26-29 were
,
The Basic Recipe for #MachineLearning explained by , in one slide 
, 
,
,
,
The Basic Recipe for #MachineLearning explained by , in one slide 
, 
,
,
The Basic Recipe for #MachineLearning explained by , in one slide 
, 
,
,
The Grammar of #DataScience - comparing #Python and R strengths and weaknesses #rstats 
, 
,
,
,
,  "
"
,
,
,
This week on /r/MachineLearning, we have posts ranging from computer vision security to new deep learning libraries.
,
,
,
This post details work done at Cornell and University of Wyoming investigating pathological cases where Deep Learning algorithms used for image recognition falsely identify common objects with high confidence. The researchers raise issues that this may cause in security moving forwards. It?€?s an interesting read if you like security and are interested in its intersection with computer vision and image recognition.
,
,
,
This page includes a nice chronological list of all DeepMind related publications. It also includes the most important publications at the top. If you?€?ve been interested in getting a comprehensive look at the development of DeepMind, this is a good launching point.
,
,
,
This guide, presented in a similar fashion to Python documentation, goes over how to conduct statistical analysis using Python. If you?€?ve been considering picking up Python for statistics work, this is an excellent guide.
,
,
,
This is a hacker?€?s guide to neural networks, meaning that it takes a very example and code oriented approach to teaching about neural networks. It develops the topic from the basics like implementing low-level circuits and functions as code. If you like this type of approach, give this guide a read.
,
,
,
This Github link leads to the repository for Keras, which is a minimalist and modular deep learning library. It?€?s based on Theano, but follows a similar mindset as Torch. If you like the idea of Torch, but want to work in the Theano/Python environment instead of Lua, this is a serious alternative to consider.
,
,
,
,

  "
"
By , (,).
,
The arguments over measuring inequality in America took a different turn recently, when New York Times op-ed contributor Thomas Edsall ignited a debate by , a controversial question: How poor are the poor?
,
Edsall argued that it?€?s reasonable to conclude that the actual poverty rate has fallen considerably over the past 50 years, citing economic measures and other statistics from a surprising , by Christopher Jencks, a liberal sociologist, in the current issue of the New York Review of Books. Opening that door prompted a firestorm of opinions in the blogosphere?€??€??€?the continuation of a battle rhetorically defined in the public consciousness in the 1980s, when former President Ronald Reagan declared that ?€?Some years ago, the federal government declared war on poverty, and poverty won.?€?
,
,
,Fig 1. Official Poverty Rate for US: 1959 - 2013.
,
If you want to stake out a position, you might want to , again at Edsall?€?s previous column in 2013 that explains the three ways we generally measure poverty in the U.S.: ?€?The official Census Bureau method, which uses , that vary by family size and composition; an experimental income-based method called the , that factors in government programs designed to help people with low incomes; and a consumption-based method that measures ,.?€?
,
Statisticians and sociologists can and do spend a lot of time debating how that data is applied and what the numbers mean, and in political arguments advocates sometimes cite only the results that support their views. The strong reactions we?€?ve just seen regarding how to best measure progress in addressing poverty clearly bear out the controversy that surrounds this question.
,
I?€?d like to take the debate in a different direction.
,
We can separate statistics from their ability to accurately track improvements in well-being over the past few decades and move on to what might be considered a constructive and forward-looking approach: How can Big Data improve the lives of the poor? For example, a decade-long project to collect real-time, on-boat fishing data on the east coast of India has the potential to curb illegal fishing trawlers and perhaps give greater scope to smaller, traditional fishermen.
,
Although it hasn?€?t drawn as much attention as the debate sparked by the Edsall and Jencks columns, the role of Big Data in allowing greater financial inclusion for the poor also is a trending Internet topic. But it?€?s mostly creating optimism and interest, rather than controversy and dissent.
,
The Consultive Group to Assist the Poor, a global , that supports financial inclusion, noted in a , this month that the expansion worldwide of innovative digital financial services has cleared a path for millions of poor customers to move from an unbanked economy to mainstream banking. More than 80 counties have launched such services through the use of mobile phones in recent years, the report said. A CGAP, explains how ?€?digital data offers huge potential for financial inclusion?€? for the poor. Providers can cross-reference huge amounts of data, from utility payments to housing conditions, to tailor mobile money services to consumers who previously had little or no access to them.
,
Building on that, the Center for Financial Services Innovation also released two research reports recently tracking current developments in Big Data analytics that also are helping poor consumers gain access to credit, financial management, and other services. , belief: Big Data has the potential to improve the financial health of consumers and the success of providers on a large scale.
,
Credit scores are one example. Consumers with poor credit histories often turn to expensive payday or car title lenders for loans. But using Big Data and overlaying vast data sets from public records, online habits, payment histories, and more can create alternative ways to lend and borrow. Some 30 to 35 million U.S. consumers are considered unscorable by credit bureaus due to thin or nonexistent credit file information, for example. But 10 million of them actually have scores over 600, when a wider range of data is used to compute their scores, the CFSI report noted.
,
The role of that data can be life-changing, given that some , adults in the U.S. are either unbanked or underbanked, meaning they have little or no connection to a traditional financial institution, and spend more for financial services outside the mainstream.
,
Using Big Data to improve financial services for the poor is not without risks. Privacy, data security and exploitation are among them. It?€?s worth looking more closely at how to tackle those concerns, as the role of Big Data in providing financial services for the poor continues to grow. No doubt we?€?ll still find plenty of statistical measures to hash out regarding the effectiveness of anti-poverty programs and the actual poverty rate. But when it comes to the politics of poverty, the promise of Big Data may create an entirely new conversation.
,
Bio: , is founder and president of The Institute for Statistics Education at ,.
,
Reposted with permission from ,.

,
,
,  "


"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Apr 7 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Apr 02-05 were
,
,
,  "
"
,, co-recipient of the Nobel Prize for Economics; ,, a guru in machine learning; and ,, a computational social scientist at Facebook, will be keynote speakers for 
,
,, 
,Nov 14-17, 2015 in Atlantic City, NJ, USA, the world?€?s premier research conference in data mining.
,
	Engle conducted much of his prizewinning work in the 1970s and 80s, when he developed improved mathematical techniques for risk evaluation and forecasting, which helped better understand stock market volatility. Engle and Clive W.J. Granger received the Nobel Prize for Economics in 2003 for their development of methods for analyzing time series data with time-varying volatility.
,
	Michael I. Jordan is the Pehong Chen Distinguished Professor in the Department of Electrical Engineering and Computer Science and the Department of Statistics at the University of California, Berkeley. Jordan is a member of the National Academy of Sciences, National Academy of Engineering and American Academy of Arts and Sciences. His research interests bridge the computational, statistical, cognitive, and biological sciences.
,
	Adamic leads the Product Science group within Facebook?€?s Data Science Team. She is also an adjunct associate professor at University of Michigan?€?s School of Information and Center for the Study of Complex Systems. Her projects have included identifying expertise in online question and answer forums, studying the dynamics of viral marketing, and characterizing the structural and communication patterns in online social media.
,
	The IEEE International Conference on Data Mining series (ICDM) provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications.
,
	ICDM draws researchers and application developers from a wide range of data mining related areas such as statistics, machine learning, pattern recognition, databases and data warehousing, data visualization, knowledge-based systems, and high-performance computing. By promoting novel, high quality research findings, and innovative solutions to challenging data mining problems, the conference seeks to continuously advance the state-of-the-art in data mining.
,
	Besides the technical program, the conference features workshops, tutorials, panels and, since 2007, the ICDM data mining contest. ICDM contest proposals are due March 29. The deadline for full paper submissions is June 3. For more information, visit 
,.
,
,
,  "
"
,, 
,Philadelphia, PA, USA
,Apr 30, 2015,
,
,
For more than 6 years, the Wharton Customer Analytics Initiative (WCAI) has helped shape the definition of 'Customer Analytics.' This year's annual conference is dedicated to real-world applications that exemplify a balance of high-level rigor and business know-how, as well as elevate the role of analytics in an organization's strategic decision-making.
,
,
and use ,
,
Case studies include:
,  "
"
By Peter Bruce.
,
,
When Apple CEO Tim Cook finally unveiled his company?€?s new Apple Watch in a widely-publicized , earlier this month, most of the press coverage centered on its cost ($349 to start) and whether it would be as popular among consumers as the iPod or iMac.
,
,
, saw things differently.
,
,
,
Indurkhya is in a perfect position to know. He teaches text mining and other online courses for , and the Institute for Statistics Education. And if you?€?ve ever wondered about the origins of a term we hear everywhere today?€?Big Data?€?the mystery is over. Indurkhya, along with Sholom Weiss, first coined ?€?Big Data?€? in a predictive data mining book in 1998. (I never anticipated Big Data becoming a buzzword,?€? he said. ?€?although we did expect the concept to take off.?€?)
,
The ResearchKit already has five apps that link users to studies on Parkinson?€?s disease, diabetes, asthma, breast cancer and heart disease. Cook has , other health benefits from Apple Watch, including its ability to tap users with a reminder to get up and move around if they have been sitting for a while. ?€?We?€?ve taken (the mobile operating system) iOS and extended it into your car, into your home, into your health. All of these are really critical parts of your life,?€? Cook , a Goldman Sachs technology and Internet conference recently.
, 
That helps explain the media fascination over another new Apple product. But it also tells us the importance of learning about Big Data. Having access to large amounts of raw numbers alone doesn?€?t necessarily change our lives. The transformation occurs when we master the skills needed to understand both the potential and the limitations of that information.
, 
The Apple Watch exemplifies this because the ResearchKit essentially recruits test subjects for research studies through iPhone apps and taps into Apple Watch data. The implications for privacy, consent, sharing of data, and other ethical issues, are ,. The Apple Watch likely won?€?t be the only device in the near future to prompt these kinds of concerns. It all leads to the realization that we need to be on a far more familiar basis with how data is collected and used than we?€?ve ever had to be in the past.
, 
?€?We are increasingly relying on decisions, often from ?€?smart?€? devices and apps that we accept and even demand, that arise from data-based analyses,?€? Indurkhya said. ?€? So we do need to know when to, for example, manually override them in particular instances.
, 
?€?Allowing our data to be pooled with others has benefits as well as risks. A person would need to understand these if they are to opt for a disclosure level that they are comfortable with. Otherwise the danger is that one would go to one or the other extreme, full or no participation, and have to deal with unexpected consequences.?€?   "



















"
,
,
, was held by , in San Diego on Feb 12-13, 2015. It provided a platform for leading executives to share interesting insights into the innovations that are driving success in the world's most successful organizations. Data scientists as well as decision makers from a number of companies came together to learn practical predictive analytics, data science and business intelligence from top companies like Google, Sony, Walmart, Facebook, Twitter,  etc. Industry leading experts shared case studies and examples to illustrate how they are using and improving predictive models to innovate in their organization.
,
Here are highlights from day 1:
,
, delivered a talk titled ""Needle in a Haystack - Hiring and retaining a big data analytics team"". He shared his experience  of hiring the right people, selecting key enabling technologies, building up business processes, and common-challenges and pitfalls. The Data Incubator is a six-week fellowship preparing PhDs for job profiles such as Data Scientist and Quants. Acceptance rate is very low (around 5%). There has been an immense growth in analytics and data science jobs. 
,
Data Scientist should have these three skills: data analysis, data engineering and machine learning. He mentioned that organizations should try to be data-driven in their quest for data scientists and shared his study on this topic. Even though about half of applicants claim that they know Python, only one-third are really able to write code and about quarter get to correct answer. People who use Python do really well as compared to people who mention R and Matlab in their resume. University is not a good predictor to select candidates. On how to go behind retaining data scientists, he said the bosses should be open to ideas and trust data. 
,
, started with mentioning that trillions of dollars have been spent in IT systems to automate and optimize key business processes. Now, with billions being invested in Big Data storage and access, and next generation analytics platforms, companies are beginning the analytic prosecution of the data stored in these centralized systems. Main technical challenges in getting tons of data pumped at you regularly are: Schema Mapping and Data duplication redundancy & incompleteness. A global equipment manufacturer with thousands of products across hundreds of databases from multiple suppliers wants to effortlessly identify the same part numbers across the supply chain. Also, Thomson Reuters took about 6 months on a single deduplication project of a subset of their data sources. Tamr provides the functionality of entity resolution. 
He shared some realities of data curation efforts as follow:
,
, ?? ,
Tamr provides object linkage model, machine learning approaches for handling scale and open-channel with humans in different capacities.
,
, talked about using data to drive business decisions in the home entertainment industry. He gave the following points proving the need of data:
,
, ?? ,
As a result of above three, consumer retail spending on a given title is growing more front-loaded, thereby, giving just one chance to get it right and this chance is exacerbated by market contraction. Over the time, the models have changed drastically as previous models failed to forecast a quarter of profit opportunity. 
,
, gave a talk titled ""Marketing Analytics & other uses in creating smart work places"". He mentioned that to him, analytics is the art of counting. In marketing analytics, typically they drill down on specific areas, perform attribution analysis, A/B test and run predictive models to identify opportunities. The ecosystem at Facebook is really rich. A/B testing is the best way to optimize the customer experience, select acquisition channels and allocate investments; however, customer segmentation, identification of risk and potential and close monitoring of testing performance are essential components of the strategy. He shared insights from ""lookback"" feature which Facebook had launched few months ago. 
,
,
,
,
,  "
"
,
,

, occupies a unique space at the intersection of data science and moral psychology.  He is the chief data scientist for ,, a consumer internet platform that collects millions of monthly consumer opinions, and the executive director of ,, a non-profit that uses technology to bridge the divide between practitioners and researchers in moral psychology.  He is an applied data science consultant for Zenzi Communications, NewReleaseNow, the Institute for the Bio-Cultural Study of Religion, and Siemer & Associates.   
,
He holds a PhD in Psychology from the University of Southern California and remains an active researcher, having published 20+ articles in leading peer-reviewed psychology journals over the past few years, most of which concern the empirical study of moral and political attitudes.  His research has been featured in the Wall Street Journal, Reason Magazine, Good Magazine, the New York Times, and at numerous industry and academic conferences including South by Southwest. He blogs regularly about his research at ,.
,
Here is my interview with him:
,
,
,
,: , is the world's largest crowdsourcing platform, with over 20 million unique visitors each month voting and ranking items across domains and questions.  Effectively, what Yelp does for questions about restaurants and TripAdvisor does for questions about hotels, we do for the remaining domains.  Our goal is to provide data driven answers as, just like the combined opinions of Yelp users often is more valuable than any individual opinion from a food blogger or journalist, so too are crowdsourced answers better than the individual opinions of the writers who often currently dominate discussions of ""what is the most anticipated movie of 2015?"", ""what is the most important life goal?"", or ""what is the best cure for a hangover?"".  If you believe in the math behind the wisdom of crowds, our answers are actually mathematically guaranteed to be of better quality, which is why our audience continues to grow.

,
,
,
,Crowdsourcing requires attention to two issues.  First, those who are being polled need to have some expertise in the question.  So we can't reliably ask people for the best ways to perform a root canal, since most people have no expertise.  Perfect expertise is not necessary, but some expertise is essential.  Second, diversity is essential as for crowdsourcing to work, the ""error"" associated with each judgment needs to cancel out in aggregate.  That won't happen if answers all tend to come from people with specific biases and characteristics, so effective crowdsourcing needs to take this into account.
 ,
,

,
,All analyses have bias, and so crowdsourcing is core to data science.  For example, cross-validation and techniques for weighting bootstrapped samples all take advantage of the wisdom of crowds, with ""crowds"" referring to groups of models specific.  A broader view of these techniques is that all models have correlated error, as do all methods of data collection and sampling.  Understanding how to aggregate intelligently across error sources is useful for answering any question, such that good data science knowledge correlates highly with being able to crowdsource answers effectively.

 ,
,
,
,Most bias involves how the data is collected and from who is it collected from.  For example, a lot of data involves positive signals (e.g. web visits, facebook likes, twitter mentions) without a corresponding negative signal.  Figuring out how to collect/create those negative signals is often key to correcting that bias.  The best way to correct for bias is to collect better data, which may involve sacrificing scale, but better data will almost always out-perform bigger data.  For example, a poll of 1000 diverse people as to who will win the presidential election will always out-perform a poll of a million people who all are relatively similar.
,
 
,

,My Ph.D. is in psychology and so I love collecting crowdsourced answers to questions that are generally not thought of as quantitative.  So understanding people's simple pleasures or life goals, and the differences between ages, genders, and clusters in those domains interests me.
,
,
,
,
,  "
"
,
,
,??,??
,  "
"
,
,

, occupies a unique space at the intersection of data science and moral psychology.  He is the chief data scientist for ,, a consumer internet platform that collects millions of monthly consumer opinions, and the executive director of ,, a non-profit that uses technology to bridge the divide between practitioners and researchers in moral psychology.  He is an applied data science consultant for Zenzi Communications, NewReleaseNow, the Institute for the Bio-Cultural Study of Religion, and Siemer & Associates.   
,
He holds a PhD in Psychology from the University of Southern California and remains an active researcher, having published 20+ articles in leading peer-reviewed psychology journals over the past few years, most of which concern the empirical study of moral and political attitudes.  His research has been featured in the Wall Street Journal, Reason Magazine, Good Magazine, the New York Times, and at numerous industry and academic conferences including South by Southwest. He blogs regularly about his research at ,.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: Dealing with agenda pushing and biased samples is always an issue.  Fortunately, human beings are really bad at faking organic behavior, so there is almost always a way to discover such anomalous patterns of data.  
,
,We use the same data processing tools that a lot of companies use to help scale analyses (MongoDB, Hadoop), but the most valuable tools are those that enable us to add new data to our system.  For example, we get data from Oracle/BlueKai that lets us examine patterns in different user groups (e.g. men vs. women) separately to test for the robustness of our rankings.  We are also increasingly merging our opinion graph data (e.g. people who think this movie is overrated, think these burgers are tasty) with graph data from Facebook in order to create even more robust and powerful insights by triangulating across datasets.  There are so many analysis tools that they are somewhat a commodity, and we are finding that it is the ability to add new data that differentiates the value that we can bring, not the different ways we can analyze that data.
,
,
,
,Our answers are purely quantitative.  So while someone on those platforms may give a good answer as to the most important life goals, you won't be able to rate the specific items within their answer.  By breaking things down into their constituent parts, we can provide far more granularity as to such answers.  It also allows us to collect great data as far as predicting what other items on a list will interest people, given their existing opinions.  Qualitative answers have a place, but there are very different things you can do with purely quantitative polling.
,
,

,Nobody ever succeeds doing something they don't love.  So I have constantly moved to answer questions that interest me intrinsically, sacrificing short-term gains for longer-term happiness.  Usually that leads to longer term career success as well.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,  "
"
,
,
, was held by , in San Diego on Feb 12-13, 2015. It provided a platform for leading executives to share interesting insights into the innovations that are driving success in the world's most successful organizations. Data scientists as well as decision makers from a number of companies came together to learn practical predictive analytics, data science and business intelligence from top companies like Google, Sony, Walmart, Facebook, Twitter,  etc. Industry leading experts shared case studies and examples to illustrate how they are using and improving predictive models to innovate in their organization.
,
,
,
Here are highlights from day 2:
,
, talked about how to lead a successful predictive analytics project. Analytics is a widening field in non-marketing areas. Predictive analytics in non-marketing areas tends to have less data, more scattered data sources, fuzzier objectives, shorter time expectancy to insights and team leaders have more executive exposure. Different environments call for different approaches to problems. 
He shared the following success factors:
,
, ,  "
"
By Gregory Piatetsky,  
,, Apr 9, 2015.
,
As a media partner, KDnuggets was pleased to 
, 2 free registrations to 
,
,
,
The winners were Sivakumar Rajendran and Janusz Cyganik.
,
As part of the raffle, we asked to specify what 
,
,
,
The most popular answers were
,
,??,
Other topics included 
Graph Databases, Hive, Internet of Things, 
NoSQL Databases, R,
Testing Analytics Models, and Text Mining.
,
Below is more information about Big Data TechCon.
,
,
,
Big Data TechCon is technology-agnostic. The tutorials and classes apply to Big Data in your data center or in the cloud, from hosted environments to your own servers. The sessions apply to relational databases, NoSQL databases, unstructured data, flat files and data feeds.
,
Come up to speed on Hadoop, Spark, Yarn, HBase, R and Hive. Learn from the smartest, hardest-working faculty in the Big Data universe in a way you never could by reading a book or watching a webinar. The faculty have real-world experience that you can tap into, whether you use Java, C++, .NET or JavaScript; whether you like MySQL, SQL Server, DB2 or Oracle; whether you love or hate Hadoop; and whether you are looking at dozens of terabytes or hundreds of petabytes.
,
Mingle with fellow attendees. Be inspired by keynotes, including Matei Zaharia, assistant professor of computer science at MIT. Be impressed by the hottest Big Data tools in the Expo Hall. It's all waiting for you. The show is produced by BZ Media - publisher of SD Times, the leading magazine for software development managers.
,
Receive a $200 discount off the prevailing fees of a 3-day pass by inserting the code 
,  when prompted. Register early for additional savings. 
,  "
"
,
,
The human sense of vision is unbelievably advanced. Within fractions of seconds, we can identify objects within our field of view, without thought or hesitation. But not only can we name objects we are looking at, we can also perceive their depth, perfectly distinguish their contours, and separate the objects from their backgrounds. Somehow our eyes take in raw voxels of color data, but our brain transforms that information into more meaningful primitives ?€? lines, curves, and shapes ?€? that might indicate, for example, that we?€?re looking at a can of Coke. 
,
,
Programming machines to replicate human vision has huge implications, from Facebook?€?s facial recognition algorithms to Google?€?s self-driving cars to futuristic biomedical diagnostics. But it turns out this is a pretty difficult problem. Why? 
,
Because where we automatically see lines, contours, and objects, computers just see large matrices of numbers.
,
,
To tackle this problem of learning more complex features out of raw pixel values, we?€?re going to use a special kind of neural network called a convolutional network. 
,
Convolutional neural networks, popularized by Yann LeCun and others in 1998 with , are behind many of the successes of Deep Learning that have been reported recently in image and speech recognition and are a very hot topic.
,
Convolutional neural networks use two major constructs as primitives: ,
1) ,(also called feature detectors) and ,
2) , maps
,
As we will see later, we can express these primitives as specialized groups of neurons, which enables us to build neural networks to represent them.
,
,
Let?€?s say we have an image (shown on the left in the diagram above), and our goal is to detect horizontal and vertical edges. To do so, we can create what?€?s called a filter (drawn in green). A filter is essentially a small matrix that represents a feature that we want to find in the original image. The filter on the top attempts to discover the parts of the original image with vertical lines, while the filter on the bottom tries to discover parts of the image with horizontal lines. 
,  "
"
,
,
, currently holds a principle scientist position at Biomedical and Health Informatics group within the , clinical development unit. Xia has long track record of successfully applying novel informatics solutions to support medicines development including predictive translational safety analyses, and leverage the real world evidence based analytics in clinical study design, epidemiology observational research, health economics and outcome, comparative effectiveness and marketing research. 
,
Prior to stepping into the clinical domain, Xia was with the AstraZeneca the innovational medicines unit, focused on the areas of informatics and computational modeling to accelerate candidate drug identification and optimization in the early discovery phases. Xia holds a Ph.D. in computational chemistry and has extensive training in broad areas of Informatics.
,
Here is my interview with her:
,
,
,
,: At AstraZeneca, I belong to a group known as the Advanced Analytics Center (AAC), which sits in our clinical research organization. The mission of AAC is to transform drug development decision making through applied data science. There are three skill groups in AAC including Informatics, Statistical Innovation and Scientific Computing Solutions. 
,
My role as a Principal Informatician always starts with understanding the key challenges in the life cycle of the development of a medicine. This involves focusing on identifying the most relevant data and implementing the most appropriate analytical methodologies, in order to reveal the insights and make the decisions that can meet those challenges.  
,
In my opinion, the most prominent use cases of Analytics here are those that show that new medicines are effective, safe and prescribed to the right patient. For example, the analytics of clinical trials data to obtain the approval of a new medicine, the comparative effectiveness analytics to compare benefit of existing treatment in clinical practice versus a newly approved medicine, and the analytics of patient populations to identify the common characteristics of a group of patients who can benefit the most from a certain medicine.
,
,
,
,At AstraZeneca, science and patients are at the heart of everything we do. Diabetes is one of the main therapeutic areas in our clinical research where we believe we can make the most difference. So from the scientific perspective we are very keen to gain in depth understanding about patient treatment journeys being observed in clinical practice, and how diabetic patients can benefit most from various treatment options. Such detailed information can help reveal the ultimate complexity and decision-making in treating diabetic patients.  However it is not usually captured in the structured data elements of healthcare databases currently available to Life Sciences companies.
,
We believe the newly available and de-identified digital patient charts which contain clinical documentation directly from physicians are promising to help address these challenges. This pilot focused to address two fundamental questions: 1) whether we can identify the events of interest along the diabetic patient treatment journey from the clinical note sources and 2) the evaluation of NLP technology in retrieving this meaningful information. 
,
,
,
,This pilot study evaluated two databases with clinical notes available. The tertiary care center database evaluated initially as part of this pilot contained a small percentage of new onset diabetic cases. Further, the ambulatory care database from thousands of separate outpatient physicians was also evaluated to supplement the overall picture of diabetes care and decision making in treatments. 
,
Ideally for this pilot we would like to have included the de-identified electronic health records (EHR) as a more structured data format that can be linked with the clinical documentation.  However, the longitudinal coverage that such structured data provides was less desirable at the time this study was conducted. 
,
,
,
,Through this pilot project, we were able to benchmark our understanding and validation of how NLP technology can be used to retrieve diabetic patient treatment journey from clinical documentation. We proved that the sequence of interested events and relative dates can be extracted efficiently into structured data tables with high accuracy, which were validated via chart review on a set of sample notes. 
,
At the same time, the limitations of the type and extent of information captured in clinical notes was observed.  So in the future the longitudinal coverage of patient life plus integration of structured data from clinical notes are essential to provide a comprehensive picture of treatment journey for diabetic patients.
,
,
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for March 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, sorted by decreasing number of engagements,
,
,??,
Here are previous month activities
,  "
"
,


,
Social networking sites have recently been buzzing with the following quote about 2015:
,
In other words, platforms have become the cornerstones for innovation. All these companies are in the business of providing a trust-worthy, convenient, and engaging platform to meet some of the very basic needs of customers.
,
This dominance of platforms in their respective ecosystems is nothing new in the technology world, however, what is new is the way it is being embraced and trusted by the community. The critical success factor for all platform providing companies is the widespread adoption by independent vendors, who use the platform to attract customers and deliver goods/service to them.
,
One of the milestone events in the history of platforms raising to glory was Apple?€?s App Store crossing the mark of 1 Million apps in 2013. A nice testimonial to their ad statement ?€?there?€?s an app for everything?€?.
,
,At KDnuggets, we cannot stop wondering when would this happen to the world of Data Science, Big Data or Machine Learning. Many companies are currently trying ideas in this space, but none has been able to ?€?cross the chasm?€? and get a lot of attention. Today, we would like to direct our spotlight to Algorithmia.
,??,
, is a live, crowd-sourced algorithm API. It aims to make applications smarter, by building a community around algorithm development, where state-of-the-art algorithms are always live and accessible to anyone. Algorithmia was co-founded in 2013 by Diego Oppenheimer and Kenny Daniel who currently serve as the CEO and CTO respectively. The startup has raised $2.4 million in seed money from a group of investors including Madrona Venture Group, Deep Fork Capital, Rakuten Ventures, Oren Etzioni, and Charles Fitzgerald.
,
Based in Seattle, Algorithmia is a cloud service for Algorithm developers and App developers to share their work, build on other algorithms, and then instantly make them available as a web service. Their platform was made available for public access about a month ago.
,??,
Currently, it has 600+ ,covering a wide variety of topics such as data preprocessing, topic analysis, ensemble learning, dimensionality reduction, apache opennlp, etc. Algorithmia also lets users request for an algorithm that does not exist yet on the platform. Such requests, referred to as ?€?bounties?€? are open to vote and donation pledges from all members. Currently, there are about 70 ,. The , is a good resource with a collection of articles on some algorithms and how to get started.
,
The authors of algorithms and apps can set up a price for every API call. The end price for a consumer is the algorithm cost per API call (set by author) and commute time (1 credit/second). Registration is super simple and free. All new users are given 10,000 credits to start using Algorithmia for free. After using up the free credits, users can purchase one out of 3 package options which are priced around 10,000 credits for $1 (or cheaper).
,??,
Through a large number of member contributed algorithms, Algorithmia Marketplace has grown and its current functionalities include summarizing and generating topics from unstructured text, extract structure and information from arbitrary websites, dozens of microservice building blocks, and computer vision tasks such as face detection and image similarity.
,
Summary: Algorithmia is on-track ?€? pursuing a real pain point in the technology community, enabling crowdsourcing and delivering a simple, yet effective solution. However, these were just the first steps. Now, it faces the big challenge of rapidly growing its community, boosting community contributions, and becoming a money earning machine for so many talented algorithm developers across the world.
,
Concluding thoughts: 
,
Crowdsourcing is like ocean shores next to deserts (such as California), you can clearly see the value in it (i.e. water) yet it requires a Herculean effort to derive the value in an efficient and cost-effective way (i.e. through desalination). I just hope that this situation changes soon, for Crowdsourcing as well as California (currently going through a major drought). Until then, I will eagerly follow the progress of innovative startups such as Algorithmia.
,
,
,
,
,

,  "
"
,
,
,
This week on /r/MachineLearning, we have some big AMA announcements, library changes, and datasets.
,
,
,
This week we have another big AMA announcement. It seems Andrew Ng will be doing an AMA along with Adam Coates, Director of Baidu Research?€?s Silicon Valley AI Lab together on April 14,. Before the AMA, there will be a thread posted to the subreddit, so be sure to check in if you?€?re interested in asking either of them a question.
,
,
,
The newest version of the Python library scikit-learn has been released featuring performance improvements in clustering and interesting additions like approximate NNs. As a user of the library for clustering, this is an exciting announcement. There are many more changes that can be read in the linked announcement.
,
,
,
This video is from Richard Socher?€?s course on deep learning for NLP. It?€?s part of a larger Stanford course whose webpage can be found ,. This is a great resource because it provides lecture notes, videos, problems sets, and more. So if you?€?ve been wanting to learn how to apply deep learning to NLP, give this a shot.
,
,
,
Criteo has announced their largest ever machine learning dataset. One caveat is that the columns don?€?t have meaningful labels, so for some types of work it might not be useful. But if you?€?re interested in testing the performance or scalability of your algorithm, this could be good to have.
,
,
,
DeepCL is an implementation of convnets using OpenCL. This is interesting because there are many CUDA-based implementations, but not as much in the OpenCL ecosystem. It?€?s good to see this change, because having more options when it comes time to implement can only be a good thing.
,
,
,
,  "
"
,
,
, currently holds a principle scientist position at Biomedical and Health Informatics group within the , clinical development unit. Xia has long track record of successfully applying novel informatics solutions to support medicines development including predictive translational safety analyses, and leverage the real world evidence based analytics in clinical study design, epidemiology observational research, health economics and outcome, comparative effectiveness and marketing research. 
,
Prior to stepping into the clinical domain, Xia was with the AstraZeneca the innovational medicines unit, focused on the areas of informatics and computational modeling to accelerate candidate drug identification and optimization in the early discovery phases. Xia holds a Ph.D. in computational chemistry and has extensive training in broad areas of Informatics.
,
,
,
Here is second and last part of my interview with her:
,
,
,
,: How text data are structured varies across data sources and its particular usage. Typically for clinical notes you can find the text within structured individual sections e.g. family history or medication history. Further under each section, there is generally free text recorded to reflect the physician?€?s specific observation or decisions. 
,
While NLP proves to be quite powerful in pulling well defined concepts and corresponding numerical or category data elements from the text, there are still huge challenges remaining, in order to apply NLP approaches to pull out deeper language structures like the details around decision-making. 
,
,
,
,I started my industry career as a computational modeler to implement computer aided drug design in the very early phase of Medicine Discovery.  I feel very fortunate in this industry that I was able to move further into research of translational safety, then into late phase of medicine development. 
,
,I think the best advice I have got was from my first mentor at AstraZeneca ?€? ?€?Be yourself, know your strength and make the best out of it!?€? These wise words always keep me focusing on what I am good at and what I can do to make a difference and have greater impacts. 
  "
"
Wikibon recently extended its Big Data forecast 
,
to year 2020.
,
Wikibon writes that Big Data market continued its maturation in 2014, experiencing both significant growth as measured by vendor revenue associated with the sale of Big Data products & services and increased adoption of Big Data tools and technologies by large enterprises across vertical markets.
,
For the calendar year 2014, the Big Data market - as measured by revenue associated with the sale of Big Data-related hardware, software and professional services - reached $27.36 billion, up from $19.6 billion in 2013. While growing significantly faster than other enterprise IT markets, the Big Data market's overall growth rate slowed year-over-year from 60% in 2013 to 40% in 2014. 
,
Wikibon expects the Big Data market to top $61 billion in 2020, a 26% compound annual growth rate for 2011-2020. After initial intense growth, Big Data market growth will slow considerably. This growth pattern is common to disruptive technology markets as they mature.
,
,
,Figure 1: Wikibon Big Data Market and Forecast, 2011-2020
,
There were a number of factors driving growth of the Big Data market in 2014. They include:
,
,??,
Clients can visit , to access full report.
,
,
,  "
"
        ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,  
,, Apr 12, 2015.
,
I got an email from 
, - an interesting site whose mission is to organize MOOC courses better. 
,
They put MOOCs in a course catalog format letting users compare courses and plan the order they might take courses.  
,
CourseBuffet covers
,, but those relevant to KDnuggets include
,
,??,
You can also search for topics and find courses for relevant keywords, eg
,  "


















"
By Gregory Piatetsky,  
,, Apr 13, 2015.
,
As a media partner for Strata 2015 conferences, KDnuggets is pleased to raffle to our readers a
,
,
,, 
,5-7 May 2015
,
,
To win this pass, please email to , 
,
with the subject: ,.
,
and please specify what, in your opinion, are
,
,
The winner will be chosen with a un-biased random number generator and a summary of the answers will be published in KDnuggets. 
,
Please enter only if you are actually able to attend the conference.
Note that the winner only gets free registration, and is responsible for their own travel, hotel, and other expenses. 
,
Here is more information about the conference.
,
At 
,, happening 5-7 May,
you'll have a unique opportunity to:
,
,??,
Have some fun. You'll be surrounded by like minds. You'll be
intrigued, invigorated, and excited. Because, admit it: data is fun.
,
You don't want to miss it. 
,.
,
,
,  "
"
,
NYC Data Science Academy upcoming schedule includes 7 bootcamp events  and 4 classes on Data Science, R, Python, and Machine Learning.
,
,
,
Program time: June 1 -  Aug 22, 2015
,Application Deadline: May 1, 2015
,Students work: ,
,Testimonial: ,
,
We offer job placement assistance! We work with hiring partners directly and fulfill their Data Scientist hiring need.Learn how you can bring your data scientist career goal to life. Meet our instructors, alumni, and staff. Learn what it means to be a Data Science student, and check out our alumni's final projects. Get introductions to our upcoming course offerings or have a 1-on-1 chat with our admissions rep to find the right course for you. Learn about the NYC Open Data Group, NYC Data Science Academy and the benefits of becoming a member of the school and meetup!
,
Students will present their final projects to the whole NYC Data Science Community. They will share their Kaggle competition and Port Authority project and etc.
,
,??,
Online Information Session hosted on Google hangout, Meet us online!
,
,??,
Open House Meetup Events, Meet us in person!
,
,??,
,
,
,
,Date: May 20 and 21
, Time: 9am to 5pm  
,
Presented by Vivian Zhang, Founder and CTO of SupStat Inc, Adjunct Prof at NYU and Stony Brook Univ. Vivian is a data scientist who was devoted to the analytics industry and data technologies for many years. Prior to taking entrepreneurial steps, she worked as a Senior Financial Analyst at Memorial Sloan-Kettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received Double Master Degrees in Computer Science and Statistics.
, This class is designed to provide a comprehensive introduction to R. We'll get you programming and analyzing data with R in no time. Participants will receive a copy of all slides, exercises, data sets, and R scripts used in the course. We will also emphasize how you can get work done easily with the RStudio IDE.
,
,
,
,
, Date: May 16, 30, Jun 6, 13, 20
, (We take a break on Memorial Weekend on May 23)
, Time: 10am to 5pm
,
Paul Trowbridge is a adjunct faculty at NYU.  Paul has worked on projects in FMRI brain imaging studies, the analysis of international dispute data, experimental psychology,  micro-simulation methods in urban planning and urban economics applications, and the epidemiology of Chlymidia.
, This intensive class will introduce you to the wonderful wold of R and provide you with an excellent understanding of the language that leaves you with a firm foundation to build upon.From the rudimentary building blocks of programming basics, to data manipulation and use of advanced drawing packages, the course will conclude with a demonstration of a project of your choice on Project Demo Day.
,
,
, Date: May 16, 30, Jun 6, 13, 20 
, (We take a break on Memorial Weekend on May 23)
, Time: 1pm to 5pm
,
This five week course is an introduction to data analysis with the Python programming language, and is aimed at beginners. We introduce how to work with different data structure in Python. We covered the most popular modules including Numpy, Scipy, Pandas, matplotlib, Seaborn, ggplot, etc to do data analytics and visualization. We use iPython notebook to demonstrate the results of codes and change codes interactively during the class. Our past students include people have no programming experience and people have little exposure by taking Python class at NYU or General Assembly. Students told us our classes are very engaging, interactive, hands-on and have tons of content.
,
,
, Date: May 17, 31, Jun 7, 14, 21 (break May 24, Memorial Day Weekend)
, Time: 1pm to 5pm
,
We do hands-on instruction over 30+ functions in Scikit-learn library and cover topics as linear regression, multivariate linear regression, Naive Bayes Classifiers, kNN, logistic regression, LDA, Cross-validation, Bootstrap, feature selection, SVM, Decision Tree, PCA and clustering. We build solid foundation for you to advance your data scientist career into experienced Data Scientists or even manager/director role.
,  "
"
By Gregory Piatetsky,  
,.
,
UMass Amherst has recently launched a new 
,
that will coordinate and significantly expand its capacity for research, education and industry collaboration in the rapidly growing field of Big Data and Data Science.
,
In conjunction with this new Center, the university also issued a
, which covers
the strength of UMass System in Data Science and Big Data research and applications across its 5 campuses: Amherst, Boston, Dartmouth, Lowell, and Worcester.
,
,
The UMass researchers have established partnerships with many Fortune 500 companies, including Google, Yahoo, Amazon, Oracle, Raytheon, BAE and MassMutual.  
,
Private investment in Big Data in Massachusetts since 2000 has totaled $2.5 billion. 
,
UMass system is the pipeline for Massachusetts data science workforce and awarded 17% of the 1,000 data science degrees granted statewide in 2012. The majority of graduates from comprehensive 
bachelor's, master's, and doctoral programs remain in their home state. 
,
The report projects that 120,000 Big Data related jobs expected by 2018, which I think is too high for ""hard"" Data Science jobs, but is quite plausible for a wider definition of ""soft"" Big Data-related jobs.
,
See also 
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Apr 14 and later.
,
See full schedule at , .
,
,  "

"
Most popular 
, tweets for Apr 06-13 were
,
Brilliant! Dilbert on Resume embellishing: if engineer, fire him; if marketer, promote him 
, 
,
,
,
5 most popular #similarity measures implementation in #python 
, #DataMining #rstats #KungFuPanda 
,
,
,
Languages have more ""happy"" words than unhappy; Spanish & Portuguese most happy 
, 
,
,
,
,
Languages have more ""happy"" words than unhappy; Spanish & Portuguese most happy 
, 
,
,
,  "
"
,
,
, is Senior Director, Enterprise Data Management for Time Warner Cable. He and his team are responsible for shared data warehousing assets and functions that benefit multiple Business Intelligence (BI) teams and their customers. This includes creation of enterprise data assets, BI architecture, quality assurance, and data quality management. In addition, Mike and his team are responsible for evaluation and adoption of Big Data technologies.
,
Prior to joining TWC, Mike held Product Management and Product Marketing positions with Amdocs, focused on decision automation, mobile content and personalization solutions. Mike?€?s prior experience includes senior roles at major analytical CRM & marketing services companies.
,
Here is my interview with him:
,
,
,
,: The Enterprise Data Management team is responsible for shared data warehousing assets, resources and functions that serve multiple constituencies and benefit multiple customer-facing Business Intelligence (BI) teams and their customers. This includes data integration, testing, data quality, reference data management, and architecture.
 ,
,
,
,We use multiple data warehouse appliances which serve both user-facing BI workloads and data integration workloads. We adopted Extract ?€? Load ?€? Transform (ELT) data integration methodology that leverages the power of the database to transform data at scale. ELT scripts are written in SQL and are orchestrated using Unix shell scripts and an enterprise job scheduling tool. We also use commercial ETL tools, mainly to bring data from the source systems into the data warehousing environment.
 ,
,
,
,Billing systems are the most popular data source but we have many others, including customer care systems, ERP, usage data, and Salesforce.com, to name a few. The total amount of storage across all data warehouse appliances is over 300TB. 
,
,
 ,
,

,I wouldn't say that our data warehousing system is not good enough. We could continue meeting TWC business needs by incrementally evolving our existing architecture. But with the emergence of Big Data technologies such as Hadoop and Spark, we found that we could deliver BI solutions more cost effectively by supplementing our existing architecture with Big Data platforms.
,
,
,
,First we looked at the typical Big Data use cases such as data integration, data lake, query-able archive and advanced analytics. We concluded that data integration and ELT offload represent the best opportunity for Big Data adoption at TWC. 
,
Since Hadoop and Spark run on generic hardware, cost was the main consideration for selecting a hardware vendor. For Hadoop distribution we considered openness, market momentum and support by the third party software ecosystem. We also brought in a specialty tool to increase developer productivity when building data integration jobs to run on Hadoop.
 ,
,

,I?€?m a big believer in ?€?buy when you can?€? approach. We have been using an off-the-shelf solution for web analytics for years and didn?€?t see the need to build a similar capability in house. On the other hand we accumulated several million lines of SQL ELT code that encapsulate TWC unique IP, and migrating that to the cost-effective Big Data platforms is where our ?€?build?€? efforts are focused.
,
,
,
,
,  "
"
Montr??al, Qu??bec (PRWEB) April 13, 2015. 
,Provalis Research announced
, the integration of 
,, the leading text analysis software, with 
,, the popular data analysis and statistical software. This new collaboration couples the cutting-edge numerical analysis of Stata with the unique text analytics functionality of Provalis Research. 
,
The combined technologies will enable business analysts and researchers to perform thorough statistical analysis and process unstructured data in a much faster and more accurate manner.
,
The business and academic communities have long needed statistical tools which enable both analysis of structured and
unstructured data, but the combination has so far been uncommon. Many disciplines performing mixed methods research, such as economics, sociology, psychology and political science, have been facing the same challenge. There is a need to analyze text data to identify topics while simultaneously determining similarities and relations to other data components. Some typical text mining tasks include text categorization, comparative analysis, topic modeling, entity relation analysis, and automatic document classification. In many ways, text mining addresses this challenge by enabling sophisticated indexing that ""turns text into numbers"", which can then be incorporated into other analyses.
,
In order to better serve their clients, Provalis Research and Stata have collaborated to build an integrated solution that leverages both Stata's statistical power and WordStat's text analysis tools. With Provalis Research's technology, Stata's clients are now able to import documents from various file formats and automatically extract numerical, categorical or date variables from structured documents for effective text analytics. 
,
,
,
With a capacity of up to 20 million words processed per minute, WordStat for Stata not only allows businesses and academics to handle large amounts of unstructured information in a faster manner, but it also enables them to identify temporal trends, assess relationships with quantitative data more easily, and ultimately make better-informed decisions.
,
To learn more about the new integration, go to the 
,
or order it at 
,  "
"
        ,
,  "
"
,
,
, is Senior Director, Enterprise Data Management for Time Warner Cable. He and his team are responsible for shared data warehousing assets and functions that benefit multiple Business Intelligence (BI) teams and their customers. This includes creation of enterprise data assets, BI architecture, quality assurance, and data quality management. In addition, Mike and his team are responsible for evaluation and adoption of Big Data technologies.
,
Prior to joining TWC, Mike held Product Management and Product Marketing positions with Amdocs, focused on decision automation, mobile content and personalization solutions. Mike?€?s prior experience includes senior roles at major analytical CRM & marketing services companies.
,
,
,
Here is second and last part of my interview with him:
,
,
,
,: One lesson I?€?d like to share is around the skill set and the organizational implications of adopting Hadoop. Companies that invented Hadoop, as well as early adopters, are full of developers accustomed to writing lots of code in low-level programming languages such as Java or Scala. But we are a BI shop and our developer skills are SQL and ETL tools, not Java. 
,
While Hadoop comes with higher-level tools such as Pig and Hive, we do not believe that converting several million lines of SQL code to a similar amount of Pig and Hive code would make sense for us. We decided to supplement built-in Hadoop tools with an ETL tool that presents a familiar GUI paradigm to developers while still leveraging Hadoop / MapReduce framework for job execution.
,
,
,
,It?€?s easier and harder. Easier because data quality tools are much better than they used to be, and harder because user expectations for data quality continue to rise.
,
,
,
,Don?€?t be afraid of change, think of it as an opportunity, not a threat. 
,
,
,
,Data scientists today are expected to be proficient not only in statistics and other quantitative disciplines but also in data processing and other IT disciplines. I expect this trend to continue and become even stronger. It?€?s no longer enough to build good models and score them offline in some data mining tool. To achieve real business benefit you need to operationalize analytics, drive it directly into the business process. This means you need to understand the entire data value chain, from source systems to core applications that are used to run the business.
,
,
,
,At TWC, data scientists work in business departments, not IT. I?€?m not directly involved in hiring them in my current role. I used to hire data scientists in the past in previous roles (we called them statisticians or ?€?quants?€?). A good data scientist combines three competencies: quantitative methods and algorithms, ability to understand business problems, and IT skills that I mentioned in the answer to previous question.
 ,
,
,
,?€?,?€? by Bill O?€?Reilly and Martin Dugard. I like traveling and try to find time for at least one good trip to some new and interesting location every year.
,
,
,  "
"
,
,
,
This week on /r/MachineLearning, Amazon announces its new ML service, we learn about conditional random fields, and we see a probabilistic theory of deep learning.
,
,
,
This is a big announcement ?€? Amazon is getting into the ML-as-a-service game with this offering. This service makes most of the machine learning process available through an online web app. It goes through choices like training/testing ratios for your input data to the choice of evaluation metric. If you?€?re looking for an end-to-end analytics solution, it will be interesting to see how this platform develops.
,
,
,
This article acts as an introduction to L-BFGS for numerical optimization. Considering the importance of numerical optimization for machine learning, this is a good topic to dive into. Give this a read if it matches your interests.
,
,
,
This New York Times pieces dives into extracting structured data with CRFs. In the post, they use their service NYT Cooking as the source of data to show how to use CRFs for extracting structured data. This is an interesting case study and is presented very well.
,
,
,
This video series adds a new way to learn about using scikit-learn for machine learning. It comes from Kaggle, and as you might expect, this gives it a very practical bend. If you?€?re a beginner and looking to start doing Kaggle challenges or any other sort of machine learning tasks from scratch, this is a good series.
,
,
,
This blog post goes into the probabilistic theory behind deep learning. This is an interesting post if you?€?re more well versed in probability theory and want to learn about how it relates to deep learning. The author links to an Arxiv PDF addressing these ideas that will appear to probability theorists out there.
,
,
,
,

  "
"
,
,
In this webinar session, you will learn how to make SharePoint more than a place where you put documents and start transforming your collected knowledge into your , knowledge.
,
, 
for the webinar session ""Implementing a better search experience"" focusing on best practices to
,
,
The webinar is complimentary, but you must 
,
to attend.
,
If you are unable to attend the live event, write us at 
, for on-demand access to the recording, or to request a demo.  "

"
,
,
,
,
,
,- Brian Parke, Data-Driven Business, April 2015
,
Data-Driven Business developed a free whitepaper from the perspectives of industry experts at , and ,
The whitepaper focuses on the culture, benefits, challenges, data and technology currently impacting the text analytics market today.
,
Download the whitepaper here: 
,
,
,
,
,??,
,
,
,??,
Download your copy today: 
,  "
"
        By , (Stitchfix).
,
,

,

,

,


,

,

,

,

,

,

,

,

,

,
,
,
,  "
"
        ,
,
Udemy Data Scientists sent me this  
,.
,
Some of it is addressed to a more beginner level, but here are the parts more interesting to KDnuggets readers and Data Scientists.
The data is California baby names from the US Social Security Baby Names Database.
,
The tutorial uses Excel 2013, so if you have a different version, may see slight differences as you go through the steps.
,
,
Download the state-specific data from ,. You?€?ll find a file named namesbystate.zip in your download folder. Extract the California file: CA.TXT. (In Windows, you can just drag the file out of the archive.)
,
Launch Microsoft Excel, and open CA.TXT. If you don?€?t see the file in your dialogue box, you may have to choose Show All Files in the dropdown box next to the file name box.
,
In the Text Import dialogue box, choose Delimited, then Next, then Comma, then Finish. This tells Excel to treat commas as column separators. Save your file as an Excel Workbook file called ,.
,
Select the first column, A, and delete it; all of your data in this file is from California, you don?€?t need to waste computer resources on that information. Insert a new row above Row 1, and type column headers: Sex, Year, Name and Births.
,
 

,
When doing data analysis, it?€?s essential to take a step back every now and then and ask, ?€?do these results make sense??€? This is especially important when you are changing the values of cells in an Excel Spreadsheet; if you make a mistake and change your data, it can be difficult to track down the error later.
,
But sanity checks can also be used to check the state of the data as it came to you. Use the filter to select only the name ?€?Jennifer?€?, and have a look at the results. The following things should stand out:
,
,

,
        ,
        ,
           ,
        ,
        ,
        ,
        ,
        ,

,
        ,
        ,
        ,
        ,
        ,
        ,
        ,
        ,
        ,

,
        ,
        ,
        ,  "
"
What is business intelligence? Last year 4.4 trillion gigabytes of new data was created by companies. Eighty percent of the data that was collected by those companies was wasted. What can a 10% increase of data do for your company? Data is on everybody's minds - from executives pushing their teams to take advantage of all the data the business collects, to consumers worrying about sharing too much of their personal lives.
,
A ,  by DOMO called 
,
details the growing importance of collecting, understanding, and applying data to make better decisions. The guide cuts through the buzzwords and the technical jargon, and gives you an overview of business intelligence - the tools, processes and skills that help us harness the data explosion. Here are some highlights of the BI guide; more information about it can be found at 
,.
,
,  "
"
,
,
, is Associate Fellow and Head Data Scientist at ,. She has over 20 years experience in using multichannel data sources to understand customer behavior providing key findings and actionable insights to business stakeholders, embedding those insights into business processes and deploying predictive analytics throughout the organization. 
,
She and her team covers variety of analytics for Marketing, Finance, Real Estate department, including predictive modeling, Big Data integration and unstructured data analytics. Ksenija is a regular speaker at Predictive and Big Data Analytics conferences in US and Internationally.
,
Here is my interview with her:
,
,
,
,: Predictive analytics is used throughout the Verizon business units as a way to utilize the latest technology to understand key business drivers, optimize customer service, address and anticipate customer needs beyond customer expectations and build trusting and lasting relationship with our customer base.
,
,
,
,To understand which products and services are beneficial for the specific customer groups to recognize and anticipate the immediate customer needs, organization's legacy data systems and new Big Data sources need to be integrated into the comprehensive unified view where pieces of the puzzle are assembled for the single version of the truth.
,
Just looking and analyzing limited bits and pieces of data, although presenting value within its own departmental areas, rarely gives a complete picture; for that you have to find the way to integrate different data sources and systems.
 ,
,
 ,
,Deciding what to model is often easy, usually those are the hot trending topics that everybody is interested in solving hence contributing time and resources. Once the first model is built, the model selection process is indeed where the math and science meet business reality. You can spend days and weeks deriving and optimizing model results, and although sometime few percent of accuracy can translate to significant additional ROI, lost time in doing that can deplete the estimated benefit.
, 
In addition each model has to go through a 'sanity' check. I tend to favor the models that confirm something that I do know, however, discovering new facts that gives 'aha' moment is also critical. These are the best candidates for story-telling, acceptance and implementation.
,
,
,
,Here are some pitfalls that in my opinion can lead to the failure of Predictive Analytics projects:
,
,??,
,
,
,
,  "
"
,

,

,

,

,

,
,
,

,

,

,

,
  "
"
,
,
, is Associate Fellow and Head Data Scientist at ,. She has over 20 years experience in using multichannel data sources to understand customer behavior providing key findings and actionable insights to business stakeholders, embedding those insights into business processes and deploying predictive analytics throughout the organization. 
,
She and her team covers variety of analytics for Marketing, Finance, Real Estate department, including predictive modeling, Big Data integration and unstructured data analytics. Ksenija is a regular speaker at Predictive and Big Data Analytics conferences in US and Internationally.
,
,
,
Here is second part of my interview with her:
,
,
,
,: I would start small by doing the first project that addresses the immediate and urgent business need then
,
,- Clearly define the scope of the project and deliverable
,- Assemble the team that will work together and champion the project within their own organization units
,- Think small but anticipate the few steps ahead, always keep in mind scalability
,- Include only data points that are stamped and verified
,- As many data points as possible, the more the better
,- Communicate the discovery progress and first results
,- Quickly show the results and achieved ROI
, ?? ,
,??,
,
,
,
 ,
,Unstructured data is messy, loosely formatted, they need to be integrated with structured data to realize their full potential. They can, on their own, uncover a few key pieces but you need to assemble the full puzzle to see the emerging trends and the complete picture. 
,
Big Data although coming in a big volume, velocity and variety  rarely cause Big Business changes on its own, they need to be well understood within the overall context to be acted upon appropriately. Their value is in bringing up-to-date time and granularity components that are important for getting high definition insights.
,
,
,
,After the initial start with scalability in mind you should be able to handle the increasing number of requests with no major issues. The key point is to bring all data sources together, to have well established processes that load and reconcile data.
,
Data scientists shouldn't be burdened with data processing step that needs to be only 20% of the work, the majority of their time should be spent on exploration and research tasks for achieving the best results. Once the unified and reconciled version of the data is available there are rarely any problems in handling new data analytics requests.
,
,
,
,
,
, ??,
,
,
,
,  "
"
,  "
"
By Gregory Piatetsky,  
,.
,
With more and more allergies and big trend towards gluten-free everything,
new KDnuggets cartoon envisions a possible solution for Data Scientist allergies.
Who, knows this may even help with the dreaded data preprocessing! 
,
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,  "
"
        ,By Gregory Piatetsky,  
,.
,
We update our 
, (Nov 2014).  
,
In this post we analyze membership growth trends.  Here is our post which examines
,.
,
We re-examined what are relevant groups and added 7 groups with at least 6,000 members
,
,??,
and dropped 2 smaller groups: ,, and , which had less than 4K members. 
,
We also extracted full data from each group statistics page which allowed us to go back to the start of LinkedIn groups in 2007.
,
The chart below shows the breakdown of top groups by quarter of creation.
,
,
,
,
We note 2 big clusters - most of the top groups created from 2007 Q3 to 2009 Q2, and many of these groups had Predictive Analytics or Data Mining in their (original) names. 
,
,
The second, smaller cluster has 7 groups, which were created from 2011 Q2 to 2012 Q1 - during the beginning of Big Data boom, and the largest of these groups have Big Data in their name.
Somehow, no top groups were created in 18 months between June 2009 and Apr 2011 (perhaps due to recession?).
,
(*) Group 54257, created as DecisionStats by Ajay Ohri in Feb 2008, was renamed and relaunched as KDnuggets in Dec 2010, so it can also be considered as a new(ish) group, but since it already had about 1,000 members in Dec 2010, I marked it as created in Feb 2010.
,
Next chart shows group membership growth, with nicknames of the top groups shown on the right. 
,
,
,
,
Next chart shows the average growth on logarithmic scale, which declined from Big Bang-like 16,000% in 2008 to a more reasonable 47% in 2014. Notice the increase in creation rate in 2011 which reflects the creation of the second cluster of groups then.
,
,
,
,
,
,
Next we look at the largest groups and their growth.  "
"
,
,
, 

,

,

,

,

,

,

,

,

,

,  "
"
,
,
, is Associate Fellow and Head Data Scientist at ,. She has over 20 years experience in using multichannel data sources to understand customer behavior providing key findings and actionable insights to business stakeholders, embedding those insights into business processes and deploying predictive analytics throughout the organization. 
,
She and her team covers variety of analytics for Marketing, Finance, Real Estate department, including predictive modeling, Big Data integration and unstructured data analytics. Ksenija is a regular speaker at Predictive and Big Data Analytics conferences in US and Internationally.
,
,
,
,
,
Here is third and last part of my interview with her:
,
,
,
,: 
There are few ones that frequently come to my mind:
,
, ?? , 
,
,
,When the data tells the story that nobody expected. At that moment when we look at each other around the room in one unified epiphany moment.
,
And then, over time, the story changes, the data captures new movements and different insights do bubble up. It is never the same, it is ever evolving, and a new story has to be compiled and told.
,
,
,
,Over time I came up with 5 skills that are in my opinion key for this role:
,
, ?? ,
And if I find 3 of 5, I consider that a good match (oh and a taste for dark chocolate is always a plus). What is more important is that you have the team that is covering all of these capabilities and members will share and get others up to speed while working on the joined tasks.
,
,
,
,The book that I recently finished and had hard time separating from was ?€?,?€? from Irvin Yalom. Yalom is one of my newly discovered writers, he is emeritus professor of psychiatry at Stanford University and I think there is an intersection between field of Analytics and complexity of human psychology, both looking into understanding the causes, either of unusual human behavior or interesting business trends. History is usually a good place to look. Recent or very distant, that is where science has the last word.
,
My passion is traveling, exploring remote lands, those last horizons where the modern technology hasn't touched and transformed yet like Amazon rain-forest, Tibetan monasteries, Cambodia villages hidden in the jungle. They put everything in perspective.
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "



















"
        ,  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
(,)
,
,
,
,
,
,
,
,
,
,
,If you tweet ?€?I am [username] on @algorithmia?€? we will add another free 5k credits to your account. ,

,, 
CEO and founder ,, ex-Program Manager , and , Business Intelligence., , mentor. player ,
,
Original: ,.
,
,
,
  "
"
,
,
,
Big data, and its wide range of innovative business applications will be discussed in 
, in Istanbul on 26-27 May 2015. 
,
An expert advisory board of 23 executives will lead the two day discussion with focus on big data in marketing, new technologies, digital and mobile worlds, optimization and IoT.
,
Being the most comprehensive big data conference in Europe, Middle East and Africa, smartcon 2015 is outshining with its first time held ""Big Data Big Ideas"" event. During the conference where the big data technology will be analysed in 360 degrees, IoT, new trends in optimization, outstanding ideas in digital, mobile and marketing, data science and analytics, and the new business models will be discussed during various sessions. 
,
Participants of the premium event will be from leading companies of various business sectors and highly valued academicians. 
,
,,
Co-creator at MIT Media Lab and Co-leader for Big Data Initiative at World Economic Forum, is an expert in data science will keynote smartcon 2015 discussing latest trends in big data, technology and IoT.
,
,, Chief Data Officer (CDO) and Group Managing Director at Barclays, will highlight big data and old data and how to tame the big data beast for value and insights.
,
,, Co-Founder and Chief Technology Officer (CTO) at Cloudera will be setting the scene for in-depth discussions leading us through his learnings as CTO at Cloudera.
,
,, previous Chief Scientist at Amazon, who has developed Amazon's recommendation engine revolutionizing e-commerce, will be talking about the changing role of the technology leaders in the new world. 
,
The full conference programme, with speakers' topics, is available on 
, website or by contacting the organisers on 
info@smartcon.com .
,  "

"
Paris - April 20, 2015 - , announces the launch of Linkurious Enterprise, the first data visualization platform for graph data. Linkurious Enterprise provides the ability to search, explore, and visualize the connections in graph data in a secure way.
,
,
,
,
Visualizing data as a network of nodes and relationships is called graph visualization. Until recently, it was an area of interest reserved to data scientists and intelligence agencies. However, as more and more organizations work with big data, they struggle to extract concrete insights from large and complex datasets. 
,
What is the impact of a server failure in a large IT network? Are seemingly normal bank customers indirectly connected to known criminals? Who is the most influential person in my organization? Graph visualization can help IT engineers, fraud analysts or decisions-makers find the answers to these questions by exploring the connections in their data. However, to date, graph visualization was reserved to organizations capable of building their own solutions or using complex tools designed for scientists.
,
Today, Linkurious Enterprise makes graph visualization accessible to all organizations through an easy-to-use interface. Compatible with Neo4j, the leading graph database, Linkurious Enterprise is an on-premise platform that allows users to interact with their data to investigate patterns, discover hidden insights and reveal opportunities through visual querying. Organizations can quickly unlock the value of their graph data with a platform that includes security and collaboration features. Linkurious Enterprise is used for example in fraud detection, network management or medical research.
,
Linkurious is already helping early customers like Ebay, Cisco, or the French Ministry of Finances to extract insights from their graph data. Recently the International Consortium for Investigative Journalism (ICIJ) used Linkurious Enterprise for the Swiss Leaks, a worldwide data-investigation of a giant tax evasion scheme allegedly operated with the knowledge of the British multinational bank HSBC via its Swiss subsidiary.
,
,
,
,??,
,
,The International Consortium of Investigative Journalists (ICIJ) obtained 60.000 files detailing bank account information of more than 100,000 clients at HSBC in Switzerland, which held 180.6 billion euros in 2006/2007. In a global investigation called Swiss Leaks, over 160 journalists in France, USA, Switzerland, and around 50 other countries worked with tools like Linkurious Enterprise to analyze the data and track the names of alleged tax fraudsters.
,
,
,Linkurious is a French graph visualization startup. For fraud detection, medical research, or network management, companies need to understand the connections within their data. With Linkurious, it is now possible to search, to explore, and to visualize graphs easily. Our technology helps some of the most innovative organizations understand the connections in their data and unlock the value of graphs.
,
,
,  "
"
,
,
, is Executive Director at ,. Michael has worked as a data scientist (Foursquare), quant (D.E. Shaw, J.P. Morgan), and a rocket scientist (NASA). He did his PhD at Princeton as a Hertz fellow and read Part III Maths at Cambridge as a Marshall scholar.
,
At Foursquare, Michael discovered that his favorite part of the job was teaching and mentoring smart people about data science. He decided to build a startup that lets him focus on what he really loves.
,
Here is my interview with him:
,
,
,
, is a 7 week fellowship to help masters students, PhDs, and postdocs transition from academia into industry data science roles.?? The program is free and the tuition is paid for by partner hiring companies (including EBay, Palantir, Pfizer, and the New York Times).?? For more information, visit: , or read about fellow experience on our blog ,.

,??,

The idea is born out of my frustration having been on both sides of the hiring table.?? While interviewing, I realized that companies (across a wide-range of data science industries) would often ask the same technical interview questions.?? As a hiring manager, I was surprised by how many people with strong resumes were unable to answer these basic questions.?? I figured it would be more efficient for someone to ask those questions to candidates just once. That way, we help aspiring data scientists identify and build their skills and help companies find top talent.?? We accept fellows who have the raw brainpower and give them a framework to analyze terabytes of data and save hiring managers time and resources in hiring.

,

,Additionally, there is a huge skills gap in data science.?? Nationally, 80% of the growth in STEM jobs will come from computing and mathematics (,) but they make up less than 3.5% of undergraduate STEM degrees (,).?? McKinsey is estimating that there?€?s a 140,000 - 190,000 shortage of data scientists.?? I?€?m really glad we?€?re able to help address this.

,

,

,

,Hiring, even for data scientists, is often not data-driven.?? Like with ,other forms of hiring, it relies on resume keyword scanning -- the corporate analogy of tea-leaf reading.?? A lot of smart, talented people who are not good at writing resumes are missed in this process and it still lets through too many people who shouldn?€?t make it.?? With challenge questions, in-depth interviews, and code reviews, we assess actual performance in advance of any subsequent interview.

,

The other major flaw of most hiring is that a lot of policies are based on small sample sizes.?? Because we are working with thousands of applications each cohort, we?€?re able to get a lot more visibility into these stats than many hiring companies.

,

,

,
,We look for people who have a solid foundation in mathematics (statistics, linear algebra, etc?€?) and computation.?? PhDs often get this from their research and coursework.?? The latter is mostly the ability to hack around, munge data, and get things done on a computer and is often a self-taught skill.?? Finally, we also look for people who can communicate complex, technical ideas to a general audience.

,
,
,
,The transition from academia is a challenge.?? They love how our curriculum provides a streamlined framework for analyzing big data problems.?? Our Fellows have found the immersion into industry culture and direct exposure to a variety of hiring partners, data scientists, and data science jobs valuable.?? You can also read some blog entries written by alumni here (,, ,, ,, ,, ,).
,
,
,
,
,  "
"
,  "
"
,
,
,
,
Before you can dive into any kind of advanced customer behavior analysis, you have to make sure you're counting each user once and only once. This is a surprisingly difficult problem, especially with customer touchpoints across devices.
,
To accurately measure your users, you have to tie identities across anonymous and logged-in sessions, account for when people change their email addresses, and plan for cross-platform interactions.
,
Join us April 28 to hear Erin Franz, Data Analyst at Looker, and Will Johnson, Success Lead at Segment, discuss: 
,
,??,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Apr 21 and later.
,
See full schedule at , .
,
,
,  "
"
,
,
Algorithmia is a new platform that acts as a sort of ?€?marketplace for algorithms?€?. We?€?ve looked at Algorithmia from a high level ,, so today we?€?re going to get hands-on and see how Algorithmia actually works from an applications perspective.
,
Our goal is to compare KDnuggets tags on articles from 2014 (a subset of ,) to the tags generated by an automatic tagging algorithm hosted on Algorithmia. We?€?ll first use Algorithmia as a client, sending scraped articles to the Algorithmia API and receiving generated tags. Then, we will use Algorithmia as a developer, writing an algorithm for frequent itemset generation to get multi-word tags. Then we will use this algorithm and see how the platform works end-to-end. Finally, we?€?ll look at the resulting tags and see how machine generated tags compare to our handmade tags.
,
,
,
,
,
,
Before we can generate tags, we need documents to tag. Using , and ,, this is a fairly straightforward task in Python. The gist for the code used to do this can be found 
,.
,
This code first scrapes the KDnuggets articles, and then passes the article to the , algorithm on Algorithmia using ,. The same tools you use to write API-aware applications in Python are the same things you?€?d use to target Algorithmia, making this whole process very easy if you have any experience writing these kinds of applications. Once these tags are generated, a TSV file is outputted with the links, hand-made tags, and machine-generated tags.
,
,
,
The resulting tags looked good, but there was one glaring problem ?€? they consisted only of single words. The ,algorithm depends on Mallet?€?s implementation of LDA, meaning that the output will take the form of the most representative word from each topic. What?€?s nice about Algorithmia is the emphasis on open source, which let me see that this is how the algorithm was performed.
,
,
,
Never fear, by using a frequent itemset algorithm, we can group together co-occurring terms to recover multi-word tags like ?€?Data Mining?€?. To do this task, I simply implemented a na??ve version of the Apriori algorithm in Python on Algorithmia. I used their online editor, which proved to be much more refined than most others that I?€?ve used previously. The way the algorithms act like , projects makes handling dependencies easy as well. You can see my resulting algorithm implemented on Algorithmia ,.
,
,
,
,
,
With our freshly minted algorithm written and published on Algorithmia, we now must access the algorithm to add the newer tags. This works in the same way as accessing the API provided by ,. The script for taking the output from our scraping, passing it into the Apriori algorithm, and getting the new itemset-based tags, can be found ,.  "
"
,
,
,
,
,
Over the past two to three years there has been a small explosion of companies offering cloud-based Machine Learning as a Service (MLaaS) and Predictive Analytics as a Service (PAaaS). IBM and Microsoft both have major freemium offerings in the form of Watson Analytics and Azure Machine Learning respectively, with companies like BigML, Ayasdi, LogicalGlue and ErsatzLabs occupying the smaller end of the spectrum.
,
,
These are services which allow a data owner to upload data and rapidly build predictive or descriptive models, on the cloud, with a minimum of data science expertise.
,
,
Yet as quickly as this has happened, there is already a step-change afoot.
As somebody working on enabling technologies in this area, I believe it is no overstatement to say that applied machine learning is undergoing a significant evolution right now - one which represents an inevitable step on the route to truly automatic general purpose predictive modeling. Examples of providers championing this new approach include Satalia, DataRobot, Codilime, and the company for whom I work, ForecastThis.
,
,
Unlike conventional MLaaS and PAaaS offerings (as much as any sector that has emerged within the last few years can be described as ""conventional""), the technology at the heart of these new services is not based upon any one algorithmic approach. Rather, these services draw upon a huge and diverse range of algorithms and parameters to identify those which , - often , algorithms in the process.
,
,
,
,
,
,Almost precisely a year ago, Dr. Mikio L. Braun of the Berlin Technical University , in which he details four reasons why automation is unlikely to transform predictive modeling any time soon:
,
,Remarkably, what Dr. Braun gives here are four reasons precisely why , happen!
,
,Let's break that down...
,
,To learn more about predictive modeling and big data, register to attend one of the upcoming analytics events in ,. ,
,
,
,
,  "
"
,
,
,Across all major surveys, R has clearly dominated as one of the top programming choices for data scientists. Thus, it is no wonder that knowing the important R packages can be a vital advantage in Kaggle competitions. Xavier Conort (currently Data Scientist at Data Robot) has compiled a list of 10 R packages that played a key role in getting a top 10 ranking in more than 15 Kaggle competitions (including winning a few of them). 
,
Since R is widely being used even outside the data science community (such as by statisticians, actuaries, etc.), this list of top 10 powerful R packages might help you in more ways than you might think.
,
Here are those 10 packages particularly powerful to build winning solutions:
,
, ?? ,

,??,
, Use your intuition to help the machine by doing the following:
,
,??,
The complete set of slides for this presentation by Xavier Conort: 

, ,

,
,
,
,  "
"
,
,
, is Executive Director at ,. Michael has worked as a data scientist (Foursquare), quant (D.E. Shaw, J.P. Morgan), and a rocket scientist (NASA). He did his PhD at Princeton as a Hertz fellow and read Part III Maths at Cambridge as a Marshall scholar.
,
At Foursquare, Michael discovered that his favorite part of the job was teaching and mentoring smart people about data science. He decided to build a startup that lets him focus on what he really loves.
,
,
,
Here is second and last part of my interview with him:
,
,
,
, Our hiring partners love that we?€?re presenting them with talented folks that have been pre-screened and evaluated technically and see us as complementing the traditional recruitment agencies they already work with.?? They also appreciate the opportunity to network with our Fellows in an informal setting before deciding whether or not to set up an interview.

,

,



,On the technical side, being a data scientist is about combining math and computer science.?? Having a strong background in mathematics and statistics is what allows you to interpret your findings from all this data.?? Having a strong background in computation is what will give you the tools necessary to manipulate all this data.

,

,

,

,The sparknotes version is to make sure you provide data scientists with adequate support in terms of technical infrastructure and continuing education, , over the work that they?€?re doing -- including interfacing with other groups within the company -- and visibility into their impact so your data scientists connect with your organization?€?s ,.?? You can check out this , I wrote to get the full story.
,

,

,

,Obviously, we focus a lot on machine-learning algorithms (NLP, random forests, SVM etc ?€?) and ?€?big data technologies?€? (MapReduce, Spark, Scalding) but I think one of our biggest value adds is helping them ask the right question.?? As any experienced data scientist knows, moving the needle is often not about applying the fanciest algorithm but applying a simple analysis to answer the right question.?? That?€?s a tough thing to get coming out of academia and is a lot of what we?€?re trying to get fellows to do.

,

,

,

,Industry is very ?€?output?€? driven -- it doesn?€?t matter how you do it, as long as you get the right answer.?? Academia is still very ?€?input?€? driven -- e.g. countless dissertations are granted for intellectually interesting but ultimately impractical machine-learning ideas that don?€?t do much better than linear regression.?? For people making that transition from academia to industry, one common hurdle is often: ?€?the simple idea does really well ?€? I clearly need to use something more sophisticated to impress my colleagues, even when there is little practical benefit from improving.?€?

,

,

,

,We see a lot of interest in unstructured data across many industries and migration away from slow batch-based analyses to real-time answers -- even for large datasets.?? We?€?re following this by emphasizing much more work with topics like natural language processing and online machine-learning algorithms.
,
,
,  "
"
Most popular 
, tweets for Apr 14-20 were
,
The 7 Most Unusual Applications of #BigData: Billboards, ResearchKit, Skiing 
,  
, 
,
,
,
great overview: Modern Methods for #Sentiment #Analysis #word2vec #NLP #python 
, 
,
,
,
Extensive, but a little confusing site: Understanding #Data #Visualisation #dataviz 
, 
,
,
,
Basics of #SQL and #RDBMS - must have skills for #datascience professionals 
, 
,
,
,
,  "
"
        ,
,
,

,

,

,
,

,

,

,
,
,??,??
,
,
,??,??
,  "
"
,
,Tue, April 28
,10 am PT, 1 pm ET
,(* If the time is inconvenient, please register and we will send you a recording.)
,
Registration Link: 
,
,
Alternative link: 
,
,
,
,
is a profitability measure that many companies use to quantify their efforts and make important business decisions. As an APPLIED example, we will look at ROI related to product sales and promotions at Walmart.  Using state-of-the-art data science techniques, and especially TreeNet gradient boosting, we will optimize product promotion options and maximize revenue and wider gain.  
,
Although the focus is ROI and Business, corresponding applications include: pricing optimization, risk analysis, scoring, segmentation, analytics-based decision making, scenario planning, population dynamics, suitability, etc.  
,
Who should attend?  Attend if:
,  "
"
,
By Gregory Piatetsky,  
,, Apr 21, 2015.
,
A friend has recently sent me a link to a 
,, created by 
,, 
author of SACS Toolkit for modeling complex social systems.
,
,
,
It shows the intellectual lineage of the complexity sciences, starting from 
,
,??,
,
,
However, the reason that my friend sent me the link is ... 
,
he found my name 
on the map next to Data Mining circle (lower right), not far from Douglas Hofstadter of the ""G??del, Escher, Bach"" fame, and Jeannette Wing (who just received ,).  
,
I deserve only a very small part of the credit for the growth of ""Data Mining"" field, but it is fun to see your name on the map.
,
,
,
,
,
,
 ,  "
"
,, May 29, 2015.
,
Location: Marriott Kingsgate at the University of Cincinnati.
,
This year the Summit will feature two internationally recognized leaders in the field of analytics.  
,
,
,
John Elder leads America's largest and most experienced data mining consultancy and his company has solved projects in a huge variety of areas by mining data in tables, text, and links. Dr. Elder co-authored 3 books, has created data mining tools, and was a discoverer of ensemble methods.
,
,
,
Stephen Few has over 20 years of experience as an innovator, consultant, and educator in the fields of business intelligence (a.k.a. data warehousing and decision support) and information design. He focuses on the effective analysis and presentation of quantitative business information. Stephen is recognized as a world leader in the field of data visualization.
,
In addition to the speakers there will be four afternoon breakout sessions focusing on descriptive analytics, prescriptive analytics, predictive analytics, and one other analytics related topic.
,
For more information visit
,
,
,
,.  "
"
,

,

, is the director and co-founder of , on Big Data and development, jointly created by the , (HHI), the , and the , (ODI). He is a Visiting Scholar at MIT Media Lab, a Fellow at HHI and a Senior Research Associate at ODI, as well as a PhD candidate (ABD) at UC Berkeley, writing his dissertation on Big Data and demographic research.

,

Emmanuel is the author of UN Global Pulse's White Paper ""Big Data for Development"" (2012), the lead author of the 2013 and 2014 OECD Fragile States reports and a regular contributor on Big Data and development.

,

He previously worked for UNDP in New York (2006-09) and in Hanoi for the French Ministry of Finance as a technical assistant in public finance and official statistics (2000-04). He holds a BA in Political Science and an MA in Economic Demography from Sciences Po Paris, and an MA in International Affairs from Columbia University, where he was a Fulbright fellow.
,
He is also a political cartoonist for various publications and media outlet including Medium and Rue89 in France, and a member of The Cartoon Movement.
,
Here is my interview with him:
,
,
,
,I had the idea of creating ?€?something?€? like Data-Pop Alliance since about late 2012, after I left Global Pulse where I worked and wrote the White Paper ?€?,?€? in 2010-11. That paper was my 1,??foray into what was then a tiny field, and it opened doors. I was back in UC Berkeley working on my PhD in 2012-13, and was increasingly involved in the field as it starting growing, talking at a few conferences, writing a few articles?€?and I wanted to build something lasting with a bit of a different feel and focus compared to what existed (Global Pulse, DataKind, for instance).
,
I wanted to create something more academic with a greater emphasis on capacity building, on politics, and work with partners in developing countries, which I had done in Vietnam for 4 years before moving to NYC in 2004. I attended many Hackathons and ?€?data dives?€? during those months but I didn?€?t think the ?€?techno-scientific?€? approach and the ?€?data-for-good?€? narrative they embodied would make much of a difference. I thought it overlooked many aspects of the problems the world faces, as I am sure we will discuss below. That was a key factor in my thought process.
,
Another important factor actually was meeting and talking to Kenn Cukier, the Data Editor of The Economist, and Eva Ho, who has been involved in data science for many years, at a conference in Los Angeles in the fall of 2013, who told me to go for it. That really unlocked the process in my head because it gave me the final impulse I needed to really try hard.
,
,Then the project and team grew gradually. I had previously met Patrick Vinck from the Harvard Humanitarian Initiative at a conference in DC, whose work I admired, and then at various events; we had several things in common?€?he is from Belgium, he did some cartooning in his youth for instance, which is one of my other activities, his spouse and collaborator is originally from Vietnam, etc. I floated the idea with him in the summer of 2013. He was on board right away, even as we didn?€?t know each other very well, and as the exact contours of the ?€?something?€? weren?€?t very clear.
,
The next person I talked to was Sandy Pentland, at MIT, whom had been in touch a few times by email since 2011; after a few emails about the idea I asked him one day at MIT Media Lab whether he?€?d agree to be involved in some capacity; I was pretty nervous about it because Sandy is an important figure in the field, and he just said ?€?sure?€?. I don?€?t really know why he agreed, but having Sandy on board was instrumental; with him we were credible. Having the institutional buy-in, a support of HHI, was also key?€?which Enzo Bolletino their Executive Director confirmed early on.
,
Then I talked to Emma Samman and Claire Melamed at ODI, whom I had known a bit for a couple of years and had several colleagues and friends in common at the UN, including Paul Ladd, who played an important role too. We wanted to build a multidisciplinary and multi-partner coalition from the beginning. Emma asked ODI?€?s Executive Director Kevin Watkins about it and called me 10 minutes later to say ?€?We?€?re in?€?. We held a first informal meeting with a few key people in the space in January 2014?€?like Bill Hoffman from the World Economic Forum USA, Nicolas de Cordes from Orange, and it really started from there. So it seemed relatively easy to get key people excited about and by the idea, but that was built on years of connections, and the implementation was and has been of course much harder!
,
The first question was the name. It?€?s Paul Ladd, who is a close friend of mine, who found the name ?€?Data-Pop?€? after agonizing naming sessions?€?we wanted something that had data, that talked about people, about an explosion, that was a bit fun. Later we realized that there was another Data Pop, an advertising company in LA, who asked us to change the name, and we settled for Data-Pop Alliance, which I think conveys better what we are about?€?a coalition. The other challenge obstacle was getting other people interested and involved?€?advisers, partners etc.?€?to solidify the whole project. That took a bit of time and legwork too. I traveled??,. In general the response was very positive. Only one institution said they were not interested in even discussing potential collaboration?€?oddly so my own??,??in France, Sciences Po. They said they didn?€?t see the need.
,
,The second to last piece was the initial funding. I had been approached in the fall of 2013 by the Rockefeller Foundation as part of their scoping of the Big Data space, and I then talked to them about the idea. After several months of discussions, they asked how much it would take to actually set it up, and asked for a proposal. We got $400,000 in core seed funding, which for me was beyond anything I would have hoped for a few months earlier.
,
The last important piece was finding a space in NYC where I live?€?even as I am affiliated with HHI and MIT Media Lab and finishing my PhD dissertation at Berkeley. I was invited on a panel organized at ThoughtWorks NYC in early 2014 and then asked them if they incubated non-profit start-ups. It took several meetings, but they eventually agreed in. Since then ThoughtWorks has been amazing to us. We ?€?officially?€? launched at ThoughtWorks NYC in November 2014, with the Rockefeller Foundation and most of the people who had been involved and supported us until then. But it?€?s only the beginning..  "
"
,,
, Washington D.C.
,Friday, May 15, 2015, 8:30 am - 3 pm
,
Cost: $99
,
Agenda and Registration Information: ,
,
Alternative Link: ,
,
,
,  "
"
,
,
,
This week on /r/MachineLearning, we have Andrew Ng?€?s AMA, an autoencoders paper, open source comparisons, deep learning textbook recommendations, and Airbnb?€?s use of machine learning.
,
,
,
This AMA (which has been scheduled for a few weeks) includes Andrew Ng and his Adam Coates from Baidu Research. In it, there is much discussion on topics like MOOCs and education, largely because of Dr. Ng?€?s experience with his very popular ,. Certainly give this a read if you?€?re interested in education or what Baidu Research is working on.
,
,
,
This post is a review of a paper on Autoencoders and probabilistic models. In it, the author details how to model a problem using denoising autoencoders. In particular, the author discusses the probabilistic understanding of these autoencoders. If you?€?re interested in deep learning or probability, give this a shot.
,
,
,
This blog post is a survey of sentiment analysis methods using open source tools on the ,. It details the relative performance of na??ve bayes using unigrams/bigrams, stopwords, and WordNet in the classifier. This is a nice practical introduction to making a sentiment analysis engine.
,
,
,
This self post seems to be very popular, considering the relative newness of the deep learning buzz. The most popular choice is Kevin Murphy?€?s ?€?Machine Learning: A Probabilistic Perspective?€? (which I personally like) with Bishop?€?s text also mentioned by a few. Another option to consider, if this is something you?€?re looking for, is the free online ,, which is a bit newer than the other two.
,
,
,
This blog post is a sort of case study into how Airbnb built their host preference prediction engine. In the post, the author details why they chose to build their own logistic regression model instead of adopting a collaborative filtering approach. If you?€?re interested in these types of consumer-facing models and how they are developed, this is an informative post.
,
,
,
,

  "
"
,
,
, was held in Austin during Apr 10-12, 2015. It provided a platform for leading experts to share interesting insights into the innovations that are driving success in the world's most successful organizations. Budding data scientists as well as decision makers from a number of companies came together to get technical overview of the Big Data landscape. The camp was targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, NoSQL & Machine Learning. Through practice on interesting examples, attendees got hands on experience of popular technologies in Big Data Space like Hadoop, Spark, HBase, MapReduce, etc.
,
,
,
Highlights from day 2:
,
, delivered an interesting talk titled ""Career in Analytics & Big Data - An Analytical Approach to ?€?tasting before eating?€? "". He started with sharing definition of analytics from different sources and briefed it as the method of logical analysis. He described Big Data as voluminous amount of data that becomes challenging to capture, store, search, analyse and visualize using traditional data management tools. He briefly explained 5 Vs of Big Data as: Volume, Value, Variety, Velocity, Veracity and Variety. 
He discussed how harnessing data is different from harvesting value in different dimensions such as data, process, technology, etc. He mentioned that analytical approach is cyclical, iterative, outcome-driven and involves continuous-improvement. He shared a very interesting slide (given below) which describes the process to check if a career in analytics is the right for you or not.
,??,

, delivered a keynote on Cassandra architecture and roadmap. He described that Cassandra is fully distributed and has no single point of failure. It also provides rapid read protection. He correlated HDFS architecture to container truck and Cassandra as the engine. Cassandra provides tunable consistency. Here we prefer availability by default and opt into linearizability (ACID) as needed.
,  
The core values in Cassandra are: massive scalability, high performance and reliability/availability. It provides performance with scale resulting in low latency. He explained along with examples how Cassandra also supports JSON, user-defined functions and UDF aggregation, local as well as global indexes. Cassandra Query Language (CQL) offers a model very close to SQL in the sense that data is put in tables containing rows of columns. 
,
, delivered an overview of the Python data analytics stack. He started his talk with an introduction of Continuum Analytics and walkthrough of the key features of Wakari platform that facilitates collaborative data analytics on the servers for their client. He mentioned that among the large number of programming languages, Python is unique because it is serving a wide spectrum of users and programmers (such as novice programmers, seasoned developers, quants, scientists, etc.). 
Python is one of the most often used tool for data analytics, only behind SQL and R. Recently, we have been seeing articles about Python displacing R as the programming language for Data Science. Next, he explained the SciPy ecosystem and the NumPy stack. Lastly, he provided an overview of a few libraries from the PyVis ecosystem to show how Python is being extensively used for data visualization.
 
,
, gave an interesting workshop on graph databases and Neo4J. He clarified at first that graph databases do not store graphs/charts and graphs are very different from charts. A graph databases is basically an online DBMS with CRUD methods that expose a graph data model. 
,
Two important components of graph databases are: Native graph storage engine and native graph processing. Graph databases have no rigid schema and they are organized collections of nodes and relationships. Graph databases are heavily used for fraud detection, recommendations, geo-routing and social data analysis. Cypher is graph query language which is human readable and very expressive. At the end, Srini shared some best practices for data modeling, storage, entity relationship & joins, transactions, and concurrency control. 
,??,
,
,
,
,  "
"
,
,.
,
The robots are taking over many jobs - will they take yours and mine?
,Will Data Scientists be replaced by software, and if so, when?
,
,
,
This poll is closed - here are the results
,
,.
,
Here are some of my thoughts on the topic.
,
Justin Washtell argues in 
, 
that the future of Predictive Modeling is automation. 
,
Clearly, there are not enough Ph.D. Data Scientists for all analytics tasks and 
""Democratization"" of analytics - making analytics easier to use by non-data scientists - is a big trend.
This trend requires automating many parts of the data mining process, like feature selection and (some) data cleaning/preprocessing. Taken to its logical conclusion, ""democratization"" will inevitably lead towards ""automation"" of analytics. 
,
,, DataRobot, Google Analytics (with its Intelligence alerts), IBM Watson Analytics, KXEN Modeling Factory (now part of SAP), and other companies and projects are already automating many predictive analytics tasks. 
,
On the other hand, data scientists like Mikio Braun warn that
, and ""you can't just give a few coders new tools and they will produce something which works. ""
,
I think there is no contradiction between these points of view: 
,
,??,
because there is a ""frontier"" of automation, as shown below.
How easy it is to automate Data Science tasks depends on how complex the tasks are, amount of domain knowledge required, etc.  
,
,
,
Some tasks - like online ads marketplace - are already automated, with gazillions 
of ad-related predictions and decisions done fully automatically every day.
,
Some tasks are partly automated, like fraud-detection, 
medical image analysis, and options trading 
(read about a twitter bot that ,). 
,
However, the frontier of automation is moving and getting closer to human level.
Analytics guru Tom Davenport , that marketing activities will also become increasingly automated and 
,
,
Data Scientists who want to keep their jobs in the future will also need to be very creative and have not just technical, but human-oriented communication and presentation skills.
,
Please vote in the KDnuggets Poll above.
,
,
,  "
"
,
,
, is the director and co-founder of , on Big Data and development, jointly created by the , (HHI), the , and the , (ODI). He is a Visiting Scholar at MIT Media Lab, a Fellow at HHI and a Senior Research Associate at ODI, as well as a PhD candidate (ABD) at UC Berkeley, writing his dissertation on Big Data and demographic research.
,
Emmanuel is the author of UN Global Pulse's White Paper ""Big Data for Development"" (2012), the lead author of the 2013 and 2014 OECD Fragile States reports and a regular contributor on Big Data and development.
,
He previously worked for UNDP in New York (2006-09) and in Hanoi for the French Ministry of Finance as a technical assistant in public finance and official statistics (2000-04). He holds a BA in Political Science and an MA in Economic Demography from Sciences Po Paris, and an MA in International Affairs from Columbia University, where he was a Fulbright fellow.

,

He is also a political cartoonist for various publications and media outlet including Medium and Rue89 in France, and a member of The Cartoon Movement.
,
,
,
Here is second part of my interview with him:
,


,
,
,I?€?ll limit my answer to the ?€?Big Data and development?€???ecosystem, which is the one I know best and is actually pretty vast and ,complex. First and foremost I?€?ll say it?€?s a fascinating and vibrant space, both in terms of the actors and topics that animate it. It?€?s a mixed bag of academics, UN people, think-tank folks, researchers and managers in big private companies especially telcos?€? Last year I wrote a series of articles for a ?€?Spotlight?€? report on Big Data and development published by??,??and??,??actually described the key actors in the space; I hope and think it?€?s still a useful resource.

,

Two characteristics that I would like to stress on are that it?€?s both a highly connected and highly competitive space. It?€?s highly connected in the sense that if you take 15 or 20 key people?€?maybe even a dozen?€?and their institutions and their networks, you will find a lot of overlap and common ties and it creates a web that covers a good 80% of the key initiatives going on. These are people who have known and worked with each other for 4 or 5 years, sometimes more. I would call these the ?€?usual suspects?€? of Big Data and development, who meet at the same events several times per month sometimes.
,
Recently at a conference at Leiden University, Hague the Director of the Peace ,Informatics Lab took and??,??with Bill Hoffman from the WEF, Robert Kirkpatrick for Global Pulse, Nicolas de Cordes from Orange and I, which read??,; I don?€?t know if we are ?€?pioneers?€??€?and if we are even remotely we are certainly not the only ones?€?and if I belong in that group, but the fact is that we have been in this space for longer than many others and know each other quite well now. There is value in this, because it has created trust, allowed and fostered partnerships and collaborations, and decreased duplications of efforts, that kind of things. It?€?s hard to do something without involving the others in some way at some point.
,
At the same time if you look at the photo in question, you see middle-aged white male?€?two Americans and two French, one of whom living in the US?€?and it?€?s actually not completely unrepresentative of the ecosystem; it?€?s quite male-dominated and US centric. One of connecting points of all of us is Sandy Pentland at MIT?€?our Academic Director, who also co-chairs the WEF?€?s Global Agenda Council on Data-Driven Development and about a dozen other initiatives, and talks to and knows everybody or almost everybody in the space. In and around the WEF Council you also find important figures?€?not just middle-aged white guys, too. I?€?d mention Danah Boyd, Kate Crawford, Juliana Rotich, for instance.
,
It?€? also a much wider ecosystem?€?with people working on different facets, including human rights and ethics, for example; I?€?d mention Lucy Berhnolz and Patrick Ball, notably. But you still find ties there too. Of course there are parts of the ecosystem that I am less or not familiar with?€?and what and whom I know is biased by my own position, being in the US, interacting with these same people. But overall I still think it?€?s a relatively small space with a smaller nucleus, let?€?s say.
,
I also said it was competitive; it?€?s competitive for both personal and institutional reasons?€?in which I include financial. As much as we know and often appreciate each other, we also often compete for the same funding sources, try to craft a sub-space for ourselves and our organizations; there are also people with pretty strong personalities and egos in the lot, typically smart and influential?€?so operating in it isn't a breeze, it can be tough, frustrating. It sometimes leads to ?€?sub-optimal?€? sharing of information?€?which is sort of ironic with all the talks about the ?€?data revolution?€? being about transparency, etc.
,
But overall it?€?s a really exciting ecosystem and times to be in I think.
  "
"
,

,

,

,

 ,.

 ,
 ,

,

,

 ,

 ,

,

 ,

 ,

,

,

,

, 

,

,
,
,  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,

,
,
,
,
,
,
,
,
,
,
,
,
,
, 
,
,
Reposted with permission from ,.
,
,
,
,
,
 ,  "
"
,
,
,.
,
Yuri Burger from Zoral Labs informed me that 
, has released free access to interesting data - history of Twitter sentiment changes & sentiment distribution around the world, by country, and over time.
,
,
,
,
The happiest (according to Happy - Grumpy sentiment levels) countries are
Hungary (+47%), Czech Republic (+40%), South Africa (+21%), India (+20%), and Myanmar (+19%).
,
The grumpiest countries are: 
Philippines (-23%), China (-8%), Denmark (-5%), Russia (-5%), and Bulgaria (-4.5%).
,
Denmark is unexpected in that list, since it usually is described as a happy country.
,
US has 13% Happy tweets, 3.5% Grumpy, 83% neutral, and is among happier countries.
,
Here is a chart with sentiment levels over the last 60 days.
,
,
,
,
The happiest (%Happy - %Grumpy) days were Feb 14 (+10.7%), Valentine's day, and Apr 5 (??).  The grumpiest days were April 15 (taxes due) and March 22. 
,(,).
,
This is an interesting data which is updated daily and is available in CSV format.
,
To download goto their homepage , and click download icon on ""HappyGrumpy World"" or ""HappyGrumpy Pulse of the World"" chart. 
,
You can also analyze individual twitter accounts for sentiments.
Here is KDnuggets analysis, which gives a summary of ""Happy"" with a score of 22.
,
,
,
,
 ,  "


"
,
,
, was held in Austin during Apr 10-12, 2015. It provided a platform for leading experts to share interesting insights into the innovations that are driving success in the world's most successful organizations. Budding data scientists as well as decision makers from a number of companies came together to get technical overview of the Big Data landscape. The camp was targeted towards both technical and non-technical people who want to understand the emerging world of Big Data, with a specific focus on Hadoop, NoSQL & Machine Learning. Through practice on interesting examples, attendees got hands on experience of popular technologies in Big Data Space like Hadoop, Spark, HBase, MapReduce, etc.
,
Highlights from day 1:
,
, kicked off the first day of the conference talking about distributed data computing platforms. Talking about data storage, he mentioned that as per CAP theorem it is not possible to get an acceptable latency when having all of following three properties at the same time: Consistency, Availability and Partition Tolerance. 
 ,
Relational databases provides consistency as well as availability. However, NoSQL databases either provide consistency and partition tolerance (in case of MongoDB, HBase, Redis) or availability and partition tolerance(in case of CouchDB, Cassandra, DynamoDB, Riak). He shared some interesting uses cases of data management techniques. 

,
After giving a brief overview of some popular NoSQL databases in market, he talked about in-memory data grids which store keys and values as objects. These do not enforce any constraints of rigid SQL schemas. Some use cases of in-memory data grids include trading systems, online gaming and session data caching. In-memory data grids currently available in market are: Hazel, MemSQL, SAP HANA, Terracotta's Big Memory and Oracle Coherence.
, 
He explained important concepts and technologies such as MapReduce, Hadoop, Resilient Distributed Dataset, Spark for large-scale processing. 
While explaining the Spark ecosystem, he stated that Apache Spark unifies batch, streaming and interactive computing. By supporting iterative, graph-parallel algorithms, Spark makes it easy to build sophisticated applications. Data Frames, newly released in Spark v1.3, are distributed collections of data organized into named columns, and are equivalent to a table in RDBMS or a data frame in R/Python. He also talked about Tachyon, a memory-centric distributed file system, and how it provides reliable file sharing at memory-speed across cluster frameworks.
,
Regarding streaming data, he mentioned that it is becoming the first class citizen for data driven companies. This data is continuously processed and transformed to derive new data feeds. He briefly described how Spark Streaming and Apache Kafka works. He concluded by talking about resource management tools YARN and Mesos. Srini also gave hands-on tutorial on Spark Core, Scala, Spark Streaming, and Spark GraphX.
,
, delivered a talk titled ""A Primer on NoSQL Scaling: Emphasis on MongoDB"". She started by describing term ""Scalability"" and methods to achieve it in various ways like scaling vertically or up and scaling horizontally or out. Scaling up costs far more than scaling out. 
,

NoSQL database technology growth is a significant chunk of the data explosion in recent past. NoSQL is rapidly growing because it is faster, provides more flexible development, and involves low software and deployment costs. She mentioned that MongoDB is the fastest growing #4 DB. It has built-in sharding via replica sets for scaling out. She described sharded clusters and how sharding provides horizontal scaling. She explained the process of sharding by discussing strategy for each step such as selection of best shard key, creating required sharding index, enabling sharding at DB level and sharding the collection. She also shared some DBA tips for managing at scale. 
,
,
,
,
,  "
































"
,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

Decision trees are similarly opaque. In practice, they tend to be weak learners and are effective only when hundreds or thousands of decision trees are ensembled together. In these cases, there is simultaneously the problem of interpreting the meta-classifiers and also each of the many decision trees. Even for a single large decision tree, a succinct verbal explanation of its behavior can be elusive. 

,

,

,

,

,

,


,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs. He will be working for Amazon this summer as a machine learning scientist.


,
,
,

,
,

,  "
"
,
,
,
This week on /r/MachineLearning, we see a new top post of all time, audio processing for goal detection, slides from a talk by Geoff Hinton, ML podcasts, and how matrix multiplication impacts deep learning performance.
,
,
,
This is certainly an unconventional, and unusually popular, post on the subreddit. It should be noted that the link is probably not appropriate if you?€?re at work. The app (which can be found on the Play Store ,) provides regions and confidences for nipple detection in pictures taken on or loaded on your phone. If you?€?re interested in what the author used specifically for this task, they state that they used ,. One funny consequence of the popularity of this post is that only a week after the Andrew Ng AMA (the top post on the subreddit at the time), there is a new top post.
,
,
,
This post details the creation of a system to detect NHL goals and perform a laser light show. It essentially uses signal processing and machine learning on the audio feed of the game to determine when a goal was performed, then interacts with the hardware to change the lighting in the room. This is a really interesting post because of how deep in the software stack it goes, from high-level dataset building down to interfacing between machine learning libraries and hardware.
,
,
,
This link leads to the slides for a talk on intelligence by Geoff Hinton. They go to a talk given last month at Stanford. Based on the slides, which are absolutely fascinating, I would love to see the actual talk, but it doesn?€?t seem to be available at this time.
,
,
,
This post lists a diverse selection of machine learning podcasts. If you want to look through these, keep in mind that they are each fairly different from the last, with some having very different tones and subjects, so listen around a bit before coming to conclusions about the whole list. Personally, I find the talking machines podcast to have good content and a fairly formal, but didactic, tone.
,
,
,
This post investigates performance properties of deep learning in practice. The author finds that most time done in common deep learning situations is spent in GEMM (generalized matrix multiplication). From there, the author looks at how GEMM is done in different contexts. If you?€?re interested in the under-the-hood performance of deep learning, read this post.
,
,
,
,  "
"
,
,
, currently works for Facebook as data scientist in the consumer marketing group; in this role he is responsible for improving the effectiveness of Facebook?€?s own consumer-facing campaigns. Key projects include ad-effectiveness measurement of Facebook?€?s brand marketing activities, and product campaigns for key product priorities. Prior roles included VP of business intelligence in digital textbook startup, people analytics manager at Google and eCommerce Sr manager at Symantec. 
,
He has over 18 years of progressive experience in data driven analytics with emphasis in database programming and predictive models creatively applied to eCommerce, advertising, customer acquisition/retention and marketing investment. Mario specializes in developing and applying leading edge business analytics to complex business problems using big data platforms, including Hadoop, columnar and traditional relational models.
,
Mario holds a Masters in engineering economics from Stanford University.
,
Here is my interview with him:
,
,
,
,We strive to understand people everywhere to inspire the development of indispensable products and high impact marketing programs. We foster a culture of continuous learning and accountability to help Facebook improve and grow.
,
We measure our success with a combination of product metrics and brand survey questions that track user satisfaction, trust and favorability.
,
,
,
,We use a combination of tools, technologies such as Presto and Hive (both open sourced), Experimentation Tools (internal), Survey deployment technologies (internal), Ads manager (available to advertisers), MS Excel, Tableau, Oracle and Vertica among others. The facebook data warehouse was built on Hive, however there are many needs for relational databases. We use Vertica for interactive analysis.
 
,
,
,
,Facebook has developed a sophisticated technology called Atlas that identifies users across devices; still, offline channels such as TV, radio, Out of Home, are difficult to measure precisely and we perform multi-channel attribution analysis. This attribution assigns probabilities of a user exposure to a campaign.
,
It is still part science and part art, and that is when a good analyst brings value to the table.
 
,
,
,
,We have accumulated experience and benchmarks by performing many campaigns that use advanced segmentation, such as lookalike models and other classification models; these campaigns are measured according to their objectives, in some cases are product metrics and in others are awareness and sentiment.

,
,
,
,We use sampling techniques and cohort analysis and extensive testing and apply statistical techniques to create confidence intervals; in other instances, we need to inter or extrapolate data to fill in gaps, and in some cases make educated guesses.
,

,
,
,Lookback was a huge success in terms of user satisfaction, fun, relationship enhancements and other key customer dimensions; after Lookback, we launched Say Thanks, Year in Review and other products that continue to help our mission to connect the world.
,
Lookback in particular, motivated friends to like and comment substantially more on this video than in any other piece of content.
,
We also learned a great deal about the technical difficulties of monitoring and measuring these products and we moved to network experimentation techniques for SayThanks.
,  "
"
,
,
, is the director and co-founder of , on Big Data and development, jointly created by the , (HHI), the , and the , (ODI). He is a Visiting Scholar at MIT Media Lab, a Fellow at HHI and a Senior Research Associate at ODI, as well as a PhD candidate (ABD) at UC Berkeley, writing his dissertation on Big Data and demographic research.
,
Emmanuel is the author of UN Global Pulse's White Paper ""Big Data for Development"" (2012), the lead author of the 2013 and 2014 OECD Fragile States reports and a regular contributor on Big Data and development.
,
He previously worked for UNDP in New York (2006-09) and in Hanoi for the French Ministry of Finance as a technical assistant in public finance and official statistics (2000-04). He holds a BA in Political Science and an MA in Economic Demography from Sciences Po Paris, and an MA in International Affairs from Columbia University, where he was a Fulbright fellow.
,


He is also a political cartoonist for various publications and media outlet including Medium and Rue89 in France, and a member of The Cartoon Movement.
,
,

,
,
,
,
,
Here is fourth and last part of my interview with him:
,

,
,
,I have mentioned quite a few individuals already; I think Sandy Pentland is really a pioneer in this space and has been leading and driving a lot of the discussions that are at the forefront today; I know that some of his arguments and examples?€?for instance those presented in his book ?€?Social Physics?€??€?may scare a few people, but he actually has a balanced and cautious perspective, in my opinion?€?when he talks about ?€?,?€? for example or giving people greater rights over their data.
,
Sandy?€?s student Yves-Alexandre de Montjoye is doing great work?€?and despite his exposure he has remained humble. A few of our Research Affiliates have been especially active in the space for several years and published really good research?€?Linnet Taylor, Simone Sala, Bruno Lepri and Emilio Zagheni for example.
,
,I think DataKind, Jake Porway and Drew Conway have also done great things; I?€?d also mention Nuria Oliver and her team at Telefonica, Rahul Bhargava and Cesar Hidalgo at MIT Media Lab, and the work of Kate Crawford on data ethics. Evgeny Morozov, whom I have never met, is a critical and important voice too in my opinion. On the UN side apart from Global Pulse I would also mention to work of OCHA.

,
,

,

,We have four main strands of work; one is a research and technical assistance program with Colombia?€?s National Statistical Office funded by the World Bank; another is a series of empirical research and white papers on various topics including Big Data and climate change, ethics, literacy, and methods papers; the third is a 2-year professional training program we will be launching in the summer, funded by a major philanthropic institution implemented by the MIT Media Lab; the last one is our series of events, with for instance the Cartagena Data festival on??April 20-22. We recently sent a??, that describes some of these projects.
,
I?€?d say three of our big priorities for 2015-17 are ethics and human rights on the one hand?€?with a series of papers, events, in partnership, with the AAAS; and training and capacity on the other hand; in partnership with several organizations like Paris21,??,. The needs for training are huge and it?€?s an area where I think we are well positioned. The third one is the development of Data Spaces?€?physical multi-partner and interdisciplinary collaborative spaces?€?in Bogot?? first in 2016 and Dakar next in 2016 or 17?€?where we and other partners would run a lot of our activities in these regions, from training to research via events, art fairs, etc.

,

,
,
,We are working out a strategy for people to be able to contribute in a more structured manner; we will be developing a blog post series to which we?€?d like people to be able to send submissions on key topics; we are also developing thematic working groups or ?€?Data Nodes?€? animated by our network or Research Affiliates in which external people and groups will be able to participate.
,
And we will offer positions as funding becomes available for specific projects. We have too much to do so really all help is welcome. We are also developing a process for people to become Research Affiliates, and a proper internship program. But all of this takes time if you want to do it well.
,
The nature of our activities are to be very multidisciplinary?€?or ?€?anti-disciplinary?€? as the MIT Media Lab characterizes itself. We need people with data science skills, econometrics skills, writing skills, and communication skills. Above all, we need and will seek people who are driven, motivated, creative, humble, who have a collaborative spirit and are not afraid of taking a bit of risks in their careers.
  "
"
,

,

,
 
,
 
,
 
,
 
,

,
 ,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
Successful analytics in the big data era does not start with data and software.  It starts with immersive hands-on training and goal-driven strategy.
,
The Modeling Agency's courseware spans all skill levels and analytic team roles.  Leadership and practitioners will collaborate more effectively to arrive at actionable results with measurable impact. Depending on your experience, role and objectives, you may choose to attend any specific independent course, or jump in at any point in the progression:
,
,
,
View The Modeling Agency's 
,
for upcoming productions:
, 
,
,
Attend an upcoming event to 
,
,
,??,
,
Now is the time to advance your analytic maturity and transform your data glut into actionable information assets.  Learn more about The Modeling Agency's courseware and register for an upcoming event.
,
,
,View The Modeling Agency's 
,
with links to the full course details for each production.  Also reference the 
,
to understand the focus and orientation of each independent course, and how they may be taken as a progression.  Class seating is limited.  Reserve your space today:,
,
,
,
,Not yet ready for public training, or you'd like a preview before registering?  Sign up for the next production of TMA's 
, - a free webinar presented by TMA's president and two senior consultants on how to get predictive modeling off the ground and into orbit.  Reference webinar details and sign up at: ,
,.  "
"
,
Text analytics or text mining is the natural extension of predictive analytics, and Statistics.com's text analytics program begin in May.  Text analytics is now ubiquitous and yields insight in:
,
,??,
Are you prepared?  You may already have the machine learning and python skills needed for text analytics; if not you can learn them in:
,
,, (May 29), 
and
,
,, (May 15th)
,
,
,
1:  ,
(June 12)
,
Learn to pilot, implement or analyze data mining methods aimed at data containing unstructured text (forms, surveys, etc.).
,
2: ,
(July 17)
,
Introduction to the algorithms, techniques and software used in natural language processing (NLP)
,
3: ,
(Sept 18)
,
Introduce natural language processing (NLP) processes into your projects and software applications.  NLTK provides cutting edge linguistic and machine learning tools that are on par with traditional NLP frameworks and allows you to quickly and easily analyze text data in larger applications.
,
4: ,
May 8-29 and Aug 21 - Sep 11.
,
Introduction to the algorithms, techniques and software used in sentiment analysis, illustrated by reference to existing applications, particularly product reviews and opinion mining.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Apr 21-27 were
,
Great discussion: Building #BigData systems in academia, industry - , and @mikiobraun
, 
,
,
,
#DeepLearning in a Nutshell - what it is, how it works, why care? 
, #MachineLearning #DeepMind 
,
,
,
#DeepLearning , predicting #crime in #SanFrancisco, #Chicago #MachineLearning 
, 
,
,
,
Great discussion: Building #BigData systems in academia, industry - ,  and @mikiobraun 
, 
,
,
,  "
"
,
,
,??,
,
,
,
,
,
,
,??,
,
,
,

,??,
,
,
,
,
,??,
,
,
,  "
"
,
Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching an 
, for 2015. Trail blaze your way to the top of the leader board to earn an opportunity at interviewing for a role as a software engineer, working on world class Machine Learning problems.
,
In this competition, you'll be chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.
,
Whether you are interested in Facebook or not, this should be a fun competition for any ML/DM person who wants to test her meddle against other practitioners.
,
The competition is at
,  "
"
,
,
, is Chief Data Scientist at , where his main responsibility is the development and refinement of the company's proprietary Velocity technology, which predicts and tracks the viral life-cycle of digital media content. 
,
Prior to joining Mashable, Haile led all research efforts for SocialFlow, one of the leading social media optimization platforms for brands and publishers. Haile specializes in statistical learning as applied to predictive analytics and has a background in theoretical physics, including a Ph.D from Rutgers University, a Masters of Science from King's College, University of London and a B.A. from Yale University. 
,
Here is my interview with him:
,
,
,
,Thank you! Our tenth anniversary is coming up this summer and it?€?s amazing to think that Mashable has been in existence for a decade. 
,
From a technology perspective, some great milestones in Mashable?€?s history have been creating Mashable?€?s site re-launch in late 2012 and making it fully responsive so our content looks beautiful on all devices and in all formats. 
,
Being able to license our Velocity technology for the first time last year was an accomplishment our product and sales teams are extremely proud of.  
,
Finally, over the last few years we've brought on such talented professionals across all departments, and I think each of those hires is a great milestone of growth for the company. 
,
,
,
,The development of our proprietary technology Velocity, which predicts and tracks the viral life cycle of digital media content, is one of the best examples of data driving Mashable. Velocity crawls the web looking for what content is going to go viral next, allowing our editors to see what content is resonating with audiences, keeping them and our brand partners ahead of the curve. 
,
,
,
,A few years back, Mashable?€?s Founder and CEO, Pete Cashmore, noticed the majority of reporters crawling Twitter, various RSS feeds and other sources looking for what stories were going viral. 
,
Pete and our Chief Technology Officer, Robyn Peterson, felt there had to be a better way. So our team was tasked with developing a technology that would help them scour the web and quantify what pieces of content were going to be shared on a large scale in the future. 
,
,
,
,Velocity uses an expansive crawler to identify new content. Thereafter, natural language processing and predictive algorithms we designed classify that content and predict how much sharing activity it?€?s likely to see on various social networks over its lifetime. It should be noted that the lifetime of a piece of content is itself something we have to predict -- some stories flame out very quickly, others grow steadily and others still reach a quasi-saturation and then experience a resurgence of activity well after initial publication. 
,

The technology has now evolved from being simply a tool for our editorial staff to software we can use with our brand partners and agencies. 

,
,
,
,We utilize a variety of different metrics to generate our predictions, including the total share count, rate of change in that share count for different types of content, as well as the topical nature of the pieces of content that Velocity identifies. 
,
,
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
Back in 1997, I was one of the founding editors of the
, journal, 
the first journal in the field of data mining, 
along with Usama Fayyad and Raghu Ramakrishnan.
,
Lovers of history, can check my report on the
,, 1997, or see
the Wayback machine 
,.
,
The journal, now published by Springer, has come a long way since, and for many years 
was very ably edited by 
,.
,
Recently, a new editor-in-chief was appointed - 
,, a professor at TU Darmstadt and leading researcher in Inductive Rule Learning, Preference Learning, Web Mining, and
Data Mining in Social Science.  
,
Academic publishing is going through a difficult financial transition, and I look forward to how DMKD will navigate these difficult waters.
,
In the meantime, I wish all the success to Johannes and the journal!
,
,
,
,
,
Has Impact Factor: 1.743 
,
,
,??,
,
,
,??,
You can choose to publish your article open access. 
,
Learn more at
,
,
,  "
"
,
,
,
,
,
,
,

,
,
,
,
,
,
,


,
,
,
,

,
,
,
,
,
 ,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Apr 28 and later.
,
See full schedule at , .
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
,
,

,

,
,
,

,

,
,
,
,
,
 ,  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
The kill chain is a purely reactive model that fails to resolve the complex threats facing organizations today. A complete security platform, powered by business intelligence, lets CISOs manage risks, not just react to intrusions. A proactive, preventive approach can ensure that many attacks never penetrate the network.
,
Download at
,
,  "

"
By Gregory Piatetsky,  
,, May 7, 2015.
,
Last week, many major publications from
, to
,
have covered the virally spreading 
and fun Microsoft site 
,
,
,
Like many, I also spent time with this site,
which is addictively easy to use.
,
As a Data Scientist, I could not resist doing some analysis,
but uploading a few of my photos produced disappointment, especially
with this picture taken in 1990 when I was a young 32-year-old (definitely NOT 67),
probably smiling about some early ideas I had then about KDD and Knowledge Discovery.
,
A few times the guess was under my age, but mostly it was over. 
On average over 8 photos, the guess was about 9 years over my actual age in the photo.
,
Of course, it gives infinite possibilities for people to have fun - here are 2 Doctor Who ages:
,
,
,
Eason Wang, Senior Program Manager on Bing who worked on this project, gives 
, how-old.net went viral.
,
,
Here is 
, that shows how it went viral
,
,
,
Perhaps more interesting for KDnuggets readers is 
, where he explains how how-old.net works, which is basically classic regression built on top of very advanced image recognition and face detection. The lack of advanced modeling method probably explains why the ages are so much off !  
,Eason Wang writes
,
,
,
,
,
,
,
,
,
Deep learning on Big Data have led to a breakthrough of image understanding, opening a door to more intelligent systems.
,
Check Eason Wang latest blog
, for more.
,
  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "

"
,
,
, is Vice President of Pricing & Analytics at ,. Alison specializes in managing analytics to solve business problems including Pricing, Customer Value Management, Risk Management, and Marketing. At ScoreBig, her group is responsible for the pricing engine that drives the Name Your Own Price format for ticket buying and associated analytics. The pricing engine adjusts for real time market pricing, partner requirements, predicted trends and purchaser characteristics.   
,
Prior to ScoreBig, she worked on the optimization of customer/company value exchange through both her own consulting company and several boutique Customer Value Management Agencies.   Alison has a PhD in statistics from McMaster University.
,
Here is my interview with her:
,
,
,
,: ScoreBig enables consumers to get great tickets for live sports, concert and theater events ?€? at savings up to 60 percent below box office price. ScoreBig customers pick their own price on seats from the floor to the rafters and never pay any fees. ScoreBig is the first and only opaque sales channel to move unsold ticket inventory in a way that protects the ticket owner?€?s brand and full-price sales. Headquartered in Los Angeles, Calif., ScoreBig was founded in 2009 and is backed by U.S. Venture Partners and Checketts Partners Investment Fund. ScoreBig was recently recognized by Forbes as one of America?€?s Most Promising Companies and by Billboard as one of the 10 Best Start-ups of 2012.
,
Analytics is the foundation of the ScoreBig offering ?€? since our customers name their own prices, the analytics team is responsible for setting an optimal reserve price at which we will accept, decline or counter offer.    We have current and historical data on ticket prices on hundreds of thousands of events across both the primary (e.g. TicketMaster) and secondary (re-seller) markets.   We mine that data to inform our own ticket pricing.
,
,
,
,We obtain tickets at a large discount to the market prices in order to attract the casual ticket buyer who is turned away by high venue and reseller prices.   Our goal is to allow that buyer to try out live events at a discount.   Our commitment to our partners (the performers, teams and properties who provide live entertainment) is to discover which of those buyers might be converted to a full price buyer in order to be able to choose their seat location and have a fixed price.
 ,
, 
 ,
,We are constantly benchmarking our pricing model against all available inventory data.   We strive to be below market but at a price point that optimizes yield for our partners (and revenue for ScoreBig).
 ,
,
,
,We expected to see that value conscious buyers would buy last minute and tend towards the cheaper seats.   From very early on it was clear that the majority of our buyers buy well in advance and actually tend towards the greater value seats ?€? seats in prime locations with big discounts but still a high absolute price.
 ,
,
 ,
,Customer lifetime value is measured for us with an actual return for each customer ?€? for every transaction we know revenue, coupon costs, cost of goods sold so we can measure lifetime value very accurately.  We look at quarterly trends since ticket buying is in general a purchase made only a few times a year vs more frequent purchase verticals like grocery, telecommunications, and restaurants.
 ,
,
 ,
,In an e-commerce world today the challenge is completely attention ?€? when we get our repeat customers to our site our conversion rate is high ?€? but keeping ourselves top of mind in today?€?s very cluttered e-commerce space is our challenge.  We use email to connect to our customers which raises the second challenge which is to keep an email communication stream relevant and fresh.
 ,
,
 ,
,In general it has opened up a huge range of possibilities for personalization which in many cases companies are just starting to be able to take advantage of.   However, with that range of possibilities comes the challenge of staffing to take advantage of it.    There still remains a shortage of skilled data scientists to manage and mine that data.
 ,
,
 ,
,I've always loved math and numbers, and while studying it in university found that the math area I liked the best was statistics.   The combination of elegant math and clear applications to the real world was very appealing.  Since graduating I've been lucky enough to work in a huge range of verticals from car manufacturing, to nuclear reactor engineering, finance, and retail.    It?€?s always new business problems and challenges which I really enjoy.   
 ,
,
 ,
,The best advice I got was early on in my career when I was consulting in the automotive industry.  The advice was to explain my results as if I was explaining them to my mother.  To be clear this is not the same as ?€?dumbing it down?€? - a phrase I hear a lot and really dislike. Instead it?€?s about telling a story and adding in technical detail where required to prove a point but without unnecessary complications. Often when explaining analytical results as analysts we can?€?t resist throwing in details to show how smart we are. But in general the story can be told much more effectively without that.       
 ,
,
 ,
,Good communications, and strong curiosity.   I look for someone who is technically strong but can also tell a story.   I generally ask them to describe a project they did to me as if I was their client.
 ,
,
 ,
,I love mysteries ?€? the last book I read and liked was Lynda La Plante?€?s Backlash. I hike, go to concerts, sports and theatre (a great work perk!), cook and read.  
,
,
,  "

"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for April 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, sorted by decreasing number of engagements,
,
,??,
Here are previous month activities
,  "
"
        ,
,
,
YouTube is a treasure trove of great videos on Big Data. It can be challenging, though, to separate the wheat from the chaff. Here we look at the top videos on big data by views, sorting through them to find those with great content. If you want an easy way to get all of these videos in one place, take a look at this ,. Views numbers as of May 9, 2015.
,
, (488,100 views)
,
This video is the first in a tutorial series on Big Data and Hadoop for beginners. It's a very long video, but it's also fairly engaging, so if you're interested in Hadoop from scratch, this could be a good place to start.
,
, (192,500 views)
,
This video from 2010 has then-CEO Mike Olsen talk about big data trends within and without Cloudera. It's a good video if you're interested in the success of Cloudera and big data trends, but considering its age, the trends are a bit dated.
,
, (81,204 views)
,
This Hortonworks video goes into the explosion in big data and how analytics is growing in importance. It brings up some interesting insights into what businesses can do to keep up and even get ahead of the curve.
,
, (74,565 views)
,
This is a great introduction not only to Hadoop, but the whole Hadoop ecosystem. If you are looking to understand how Hadoop, MapReduce, and the numerous associated programs work together, this is a good quick view of how all the cogs fit together.
,
, (65,533 views)
,
This panel on deep learning on big data held at Stanford has fascinating panelists discussing intelligence. If you're interested in deep learning, and deep learning as it relates to intelligence in particular, definitely give this a shot.
,
,
,  (62,430 views)
,
This is a high-level overview of what big data means and acts as a good general video. This might be a bit too broad for those more familiar with big data, but it is a good overview. It's a video I'd recommend to managers or business people looking to understand the potential benefits of big data from a high level.
,
, (55,543 views)
,
This is a TEDx talk by Kirk Borne on big data. The talk begins with a very approachable explanation of big data, then refines to more concrete examples. This would be a great way to introduce someone completely new to the subject.
,
, (54,333 views)
,
This is another video for those new to big data, but it's not necessarily for managers or for the general public. This video is aimed at those interested in the possibilities of Hadoop for manipulating data, and does a good job filling that role.
,
, (45,966 views)
,
This (very recent) TED talk on big data is a sort of case study on big data. It looks at how municipal big data can be applied to solving big problems, and gives a sense of the current state of city-level government applications of big data.
,
,
,
This Google Tech Talk looks at the evolution and trends in big data. What makes this interesting is the economic perspectives and analysis brought by the panel.
,
,
,
,  "
"
,  "

"
,
,
,??,??
,  "
"
,
,
The , was held by , in San Francisco on April 29 and 30, 2015.
,
Across many industries, large and small organizations are using analytics and data science to offer greater insight and customer service. The gaming industry is almost well placed in that - particularly with online and social gaming - the companies already keep a vast amount of data on gamers. The challenge remains to make use of this data in a way that offers true value for money whilst enhancing the experience of the customer.
,
Industry leading experts shared case studies and examples providing deep insight into how the gaming industry uses analytics and data science.
,
Here are the highlights from selected talks on Day 1:
,
, gave an insightful talk on ""Use of Telemetry Data at Warner Bros Games"". The mission of his team is to provide a fast, secure, and robust analytics solution that allows studios to generate near real-time analysis and recommendations. Telemetry data from games is huge. It can easily reach the orders of hundreds of terabytes per title per year. 
,
The growing business requirements mandate the analytics platform to offer various features such as scalability, elasticity, data security, customization and flexibility. Besides standard reporting, the analytics platform must also support ad-hoc queries, deep analytics and machine learning. It should be easy to analyze data across titles, studios, platform (console/PC/mobile), etc. The game data is used by the analytics team to answer questions on various aspects such as how many players can we market to, which players are going to churn, what is the lifetime value of our players, how can we increase engagement, which side missions are the most popular, etc. Thus, the analytics platform team plays a key role in enabling data-driven decision making throughout the organization.
,
He shared some usage statistics from Shadow of Mordor game, and explained how it provides insights on the player behavior. He explained the data architecture, which included Amazon web services, Apache Spark, Apache Kafka, SparkSQL, R and MLlib. For BI Services, Tableau, Vertica, and Redshift are used. The future plans include providing real time insights and visualizations using Spark and Spark Streaming.
,
In conclusion he shared a few lessons from his game analytics experience: plan for elasticity as the magnitude of event data is highly unpredictable in the beginning, do Upstream ETL to give analytics a way to transform their own data, and strive to increase the speed of processing leading to real-time insights.
,

, delivered an interesting talk on ""Principles of Funnel Analytics"". He defined funnel as a visualization tool focusing on the linear progression of a well-defined population. Funnel based visualization is 100% focused on simplicity and conveying a point to people who are not data-savvy. 
,
He emphasized that the variance generated from within group differences should be less than the variance on the funnel. Ideally, the population being represented through the funnel should be homogeneous within-product time and homogeneous outside of product. 
,
Humans are, in general, lazy. Thus, no matter what the subsequent steps are there will usually be some attrition throughout the linear progression. This can be termed as ""natural attrition"". While studying the funnel, it is important to distinguish the attrition due to friction points from this natural attrition, using the heuristic that the natural attrition rate is roughly proportional to the time that the user is in the app.  Since the friction points and user journey are so specific to the underlying app or game, it is very hard to compare the funnel across apps/games.
,
, explored the concept of customer life time value (LTV) in his talk ""Predicting the Value of a Social Game Install"". In simple terms, it is the dollar value of a customer relationship to a company. In other words, it is the upper bound of the amount that could be spent to acquire new customers. Specifically, for free-to-download games with in-app purchases, LTV is the total expected revenue from an install. Social/mobile game LTV varies a lot by demographic factors such as country, age, and gender. The traditional LTV model for apps is based on average revenue per daily active user (ARPDAU), retention, and virality. There are various challenges with the traditional model, such as, the model relies heavily on forecasting (for ARPDAU and retention values). Also, the model is very tightly-coupled to the app, and thus, varies a lot for different apps. 
,
Facebook's goal is to find a LTV model that will work for all monetizing games across the entire desktop gaming ecosystem. This model must be independent of the genre or friction point placement of the game. The model should deliver stable predictions, i.e. LTV should not change a lot week over week. His team computed cumulative (30/60/90/180 days) revenue curves for weekly install cohorts of the top grossing Facebook canvas games and explained their shapes and stability across various game genres. The revenue curves clearly showed regular cycles over weekly time period. Thus, it makes more sense to analyze revenue pattern weekly, and not daily. For most games, 90-day ARPI (average revenue per install) is sufficient to understand monetization behavior. 
,
Based on the shape of ARPI curve, linear or logarithmic regressions can be used to predict 90 or 180 day ARPI using data from the first 7 or 14 days. While this works well for puzzles and strategy games, it does not give much accurate results for casinos and table games. This insight can be used to understand which demographic cohorts yield the most profitable installs and determine targeting opportunities. ARPI is an important feature taken into account by game/app ranking algorithms, particularly for app and advertising platforms.
,
In conclusion, he asserted that 90/180 day ARPI is a more stable and accurate way of predicting an LTV-like metric across many apps and cohorts simultaneously. It is also important to note that even for games where overall app ARPI is predictable and stable, narrowly segmented cohorts often exhibit wild spend behaviors.
,
,
,
,
,  "

"
,
,
, is currently the Assistant Dean for Informatics, and Professor of Clinical Sciences at the ,, charged with developing a curriculum to support the needs of the next generation of Clinical and Research Informatics leaders.  He is also the Chief Medical Information Officer for the ,, overseeing the clinical aspects of the inpatient Epic implementation, ensuring it achieves its goals of supporting the clinical and academic missions of the institution.
,
Throughout his career, he has conducted research and applied techniques that help bridge the gap between Health Services Research, clinical and research operations and Medical Informatics.
,
Here is my interview with him:
,
,
,
,: Acquisition and availability of increasing amounts of healthcare data is only beginning to be fully utilized by pharma in the drug development cycle.  There is a great deal of focus on using data for study feasibility ?€? ensuring that a reasonable number of potentially eligible patients exist for recruitment in clinical trials.  While this is certainly valuable, a great deal more can be done with healthcare data for understanding where existing medications are working well, and where new medication development is truly needed.  
,
Given the benefits of blockbuster drugs from years past, it is harder to develop a new drug that can replace these and capture a similar market share.  The trick is finding the population that is not achieving the expected benefit, and develop new medicines that will work better for them.
,
,
,
,In the past, data available to the pharmaceutical industry has typically included administrative data sources that capture the billable markers of healthcare activity.  Increasingly, clinical data captured at the time of the patient encounter are available, enabling analyses that include more direct clinical parameters like vitals signs, smoking status, and family history.   To the extent the practices are collecting discrete data on patient status for registries or other purposes, it is sometimes possible to capture important, more direct patient parameters like functional status, and measures of disease severity.
,
, 
,
For example, if a patient has a pattern of routine labs once every year, but then has a series of 4 sets of routine labs in a 1 month period, the fact that there was a change in pattern to the labs is clinically relevant, and may impact expected outcomes, even if the laboratory results themselves were normal.
,
,I cannot discuss access to data without mentioning data privacy and HIPAA regulations.  Generally, data must be de-identified to be used outside the provider or payer organization that is the source of the information.  There can be no real medical record numbers, names, addresses, phone numbers, zip codes of more than 3 digits, or even exact dates.  Data can be de-identified at the source so that it contains random pseudo-identifiers in place of real MRNs (Medical Record Numbers) and dates are shifted or rounded down to the first day of the month.  
,
A good deal of research can be accomplished with de-identified data, but, by design, there is no way to follow up with patients.  It is also difficult to merge de-identified data on the same patient across different institutions, though there has been some work on matching one-way hashed transformations of the original identifiable data to facilitate cross-institutional matching while still maintaining patient privacy.
,
,
,
,The same holes in data that exist for clinical research, and the needs of the pharmaceutical industry also impact the information needs of clinical care.  While insurance data can capture the full spectrum of health utilization over time, the comprehensive, longitudinal availability of point-of-care clinical data is more elusive.   
,
, 
,
It is certainly feasible on a case by case basis to do this, but across the thousands of patients needed to do a good retrospective analysis, the problem becomes a lot more difficult.  Again, patient privacy issues must be respected when conducting cross institutional analyses.
,
,
,
,Providers and payers have more direct access to patient data, but the quality of analytics is restricted by the data they do NOT have.  Even the VA, which may consider to be a closed system, has difficulty accounting for  the full spectrum of healthcare activity of their patients given the volume of care that takes place in  non-VA facilities.
 ,
,
,
,Analytics can assist decision making by providing objective, quantitative evidence to support expert opinion that provides too coarse an estimate of disease prevalence, activity and responsiveness to existing treatments.  In design of clinical trials, analytics can do more than just tell you how many patients fit certain criteria, but help trial designers better predict outcomes within the heterogeneous sub-populations within the seemingly-homogeneous selection criteria.  
,
With better information and analytics available today, the ?€?discovery?€? at the time of an interim analysis of a low outcome rate in the control population should not occur.  The midstream changes to study numbers or enrollment criteria required by these findings are expensive, and impact the eventual analyses, and are much more avoidable with better data and preliminary analytics.
,
,
,
,
,  "
"
,
,
The , was held by , in San Francisco on April 29 and 30, 2015.
,
Across many industries, large and small organizations are using analytics and data science to offer greater insight and customer service. The gaming industry is almost well placed in that - particularly with online and social gaming - the companies already keep a vast amount of data on gamers. The challenge remains to make use of this data in a way that offers true value for money whilst enhancing the experience of the customer.
,
Industry leading experts shared case studies and examples providing deep insight into how the gaming industry uses analytics and data science.
,
,
,
Here are the highlights from selected talks on Day 2:
,
, shared his experience and best practices for Game Analytics in his talk ""Complex Event Processing & QoS"". In-game analytics allow developers to create clear pictures of gamer behavior - this is particularly important in multiplayer online games to aid with matchmaking and detect cheating. Spencer explained how Activision is using data science to create better games. 
,
He provided a quick overview of the Analytic Services team at Activision. The team's focus is on Complex Event Processing - advanced real-time analytics combining multiple streams of complex data. The Analytic Services team is working on a variety of projects including Quality of Service monitoring, identifying cheating (hacking detection, behavior monitoring), boosting detection, and recommender systems. The underlying data processing system is based on lambda architecture comprising of Kafka event queue, HDFS datalake, relational databases, batch and ad-hoc quering systems (greenplum), and real-time querying systems. 
,
The real-time Analytics stack is based on Storm. Events from the Kafka queue are cached in Redis and processed by Storm. He noted that Storm is very chatty and to resolve that problem, they are: 
,
,
Finally, the results are sent to something durable with fast writes, such as Cassandra or published to some other Kafka queue for further analysis.
,
The dashboards are created through a combination of Elastic-search, Logstash, and Kibana (ELK). Talking about quality control, he emphasized that the focus should not be on reducing the mean quality, but rather one should strive to reduce the variance.
,
, delivered an insightful talk on team-work in his talk titled ""Beyond the Math: Structuring Analytics Teams for Success "". Individual brilliance is great, but it?€?s only one part of the equation behind great analytics organizations. The best analysts work in teams that find ways to structure the way they work so as to increase exposure to the company while creating a safe space to explore data and hypotheses. Rudi shared the Analytics team structure at Riot, best practices and lessons learned.
,
Talking about team work, he explained a few attributes of the craft, relationships and experience. He emphasized the need to ensure personal and professional growth. For success, Analytics teams must have a healthy balance of business sense and analytic capabilities. He asserted the leadership for Analytics team must be divided across two tracks: Lead Analyst and Manager. The former should provide craft leadership and mentorship, and work on the toughest engagements. On the other hand, the latter should steward analyst career and work experience, while also working on building relationships and team brand. 
,
The above team structure offers several benefits to the analysts as well as business partners by promoting collaboration, creativity, and quality. He mentioned that the continual advancement of analytics is a big challenge and requires structuring teams so that new techniques can be regularly developed, disseminated, and applied.
,
, (formerly Sony Online Entertainment) gave an interesting talk on ""Holding Effective Data Meetings with Game Teams"". One of the challenges faced by game analysts is holding effective meetings with game development teams that lead to actionable results. In addition to providing automated reporting, one of the key functions of the data team at Daybreak is to sit down with development teams each week to review the reports, provide context for the metrics, share new analysis, and make recommendations to improve player lifecycles. His talk was structured around 3 key questions: What data to share with game teams?, When and how to share data?, and How to operationalize your data?
,
Some of the most common data analytics challenges are - 
,
, ,
At Daybreak, apart from a core Data Science team, there are data analysts embedded within game teams (such as H1Z1, PlanetSide 2, and DC Universe Online) to drive collaboration. The data pipeline (comprising of Vertica, Tableau server, and Excel) at Daybreak supports automated reports and a self-service portal.
,
In order to promote an interactive approach between game team and analytics team, he advocated that there should be a proper hand-off of the results, shared ownership of data model must be encouraged, and the ROI of results should be well communicated. In conclusion, he shared the following as the answers to the 3 key questions he had asked earlier: 
,
, ,
,
,  "

"
,  "
"
,
,
,
,
  "
"
,
,
,
Nearly a year ago, CRN released the Big Data 100, their list of the top Big Data companies in the industry. Of those, ,. These companies provide services like data reporting and management tools.
,
Today, we look at the updated list of , to see how the field has evolved over the past year.
,
,The word cloud below made from the company descriptions, shows the central role of 
Data, Analytics, and Intelligence in this field.
,
,
,
,
,
What you're probably asking how this list has changed since last year. Missing this year is 1010data, Chartio, Cirro, ClearStory Data, DataGravity, DataHero, DataSift, Digital Reasoning, Jaspersoft, Jut, Kognito, LucidWorks, Paradigm4, Pentaho, Revolution Analytics, Seeq, Sqrrl, and Via Science, for a total of 19 companies being dropped from the list. 
,
With the removal of these companies, DataRPM, H2O, InsightSquared, Interana, Knime, Logi Analytics, Looker, Luminoso, Predixion, RapidMiner, Salesforce, ThoughtSpot, and Visier are added. It's interesting to see H2O, Knime, and RapidMiner all added (indicated by ,), showing an increasing demand for flexible but powerful and tweakable platforms for business intelligence. Overall, many of the companies added to this list are fairly established entities, perhaps with the exception of newer companies like Interana (2013) and ThoughtSpot (2012).
,
,
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
        ,  "
"
        ,  "
"
,
,, ,.
,

,

,

,

,

,

,

,

,

,

,
,
,
,
,
,

,

,

,

,

,

,

,

,

,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 12 and later.
,
See full schedule at , .
,
,  "
"
By Karl Rexer (Rexer Analytics).
,
Data Analysts, Predictive Modelers, Data Scientists, Data Miners, and all other types of analytic professionals, students, and academics:  Please participate in the Rexer Analytics 2015 Data Miner Survey. 
,
Survey Link:  ,
Access Code:  KD7PE9
,
Survey results will be unveiled at the Fall-2015 Boston Predictive Analytics World event.
, 
, has been conducting the Data Miner Survey since 2007.  Each survey explores the analytic behaviors, views and preferences of data miners and analytic professionals.  Over 1200 people from around the globe participated in the 2013 survey.  Summary reports (45 page PDFs) from previous surveys are available FREE to everyone who requests them by emailing ,. 
,
,
,
Also, highlights of earlier Data Miner Surveys are available at ,, including best practices shared by respondents on analytic success measurement, overcoming data mining challenges, and other topics.  The FREE Summary Report for this 2015 Data Miner Survey will be available to everyone Fall-2015.
, 
Please tell other data analysis professionals about the survey.
, 
Rexer Analytics is a consulting firm focused on providing data mining and analytic CRM solutions.  Recent solutions include customer loyalty analyses, customer segmentation, predictive modeling to predict customer attrition and to target direct marketing, fraud detection, sales forecasting, market basket analyses, and complex survey research.  More information is available at , or by calling +1 617-233-8185.
,
,
,  "
"
        By Matt Sundquist, Plotly.
,
If you track events that change--temperature, stock prices, or sales--you need the most recent data.
Typical options are: refreshing data with jobs, queries, and code; pay a steep price for clunky desktop software; manually move data and update your graph. 
, new , solves these problems. Easily make, embed, and edit web-based graphs and dashboards that fetch your data. It's free, online, and collaborative. This feature works along with our 
,, free 
,, and 
,.
,
,
,
,
,
,
The graph below is an example of an updating plot that fetches data from ,. Plotly fetches the most up to date information when you load the page.
,
,
,
,
,
,
,
The graph is interactive. Click and drag to zoom, hover to see data, or press the legend items to turn traces on and off. Dates adjusts from years to nanoseconds as you zoom. See our , docs to learn more.
,
,
,
,
,
,
,
,
,
,
,
To build your graph, we'll start with a URL where your data lives (for other uses, ,). Plotly fetches the data from this URL whenever you load a graph. For example, the Quandl dataset we're looking at is here:
,
,
,
,
,
,
,
,
,
,
Next we'll add a description of your visualization using 
,. 
See our , to learn more. This includes , and , keys. 
For the plot we're building, the description is in 
,. Use special xsrc and ysrc keys to denote the columns that you'd like to plot. For example:
,
,
,
,
,
,
,
,
,
,
,
Our layout uses JSON that describes other parts of the graph. For example:
,
,
,
,
,
,
,
,
,
,
,
If you are a paying or enterprise user, you don't need to use this key.
,
,
,
,
,
,
,
,
,
We combine source, data, layout, and key to make 
, that you paste into your address bar. You can , in a dashboard, iframe, or other applications, edit the plot in a web app (as seen below), export an image, or ,.
,
,
,
,
,
,
,
You can use this feature along with our ,, our free web product, and our ,. For advanced use, you can use Plotly in ,, ,, and ,. The , contains this plot, and examples using a SQLite database, Yahoo finance data, Google Drive, GitHub, and Dropbox.
,
,
,
,
,
,
, 
,
,
,
  "
"
,
, (Case Study: Portuguese Bank's Direct Marketing Campaign)
,May 28
,10 am PT, 1 pm ET
,(* If the time is inconvenient, please register and we will send you a recording.)
,
Registration: 
,
,
,
,
,
,
This webinar will show you how to optimize your targeted marketing using techniques common in analytics, data science and machine learning.  We will demonstrate with real-world data from a Portuguese Bank's direct marketing campaign.  You will be given software, data, and step-by-step instructions so that you can replicate all steps after the webinar, both on provided data and even on your own data.   Note: No prior statistical, programming or data science background is required. 
,
,
,
,??,
,
,  "
"
        ,
,
In the wake of the , on how Qantas Airlines was at risk of losing valuable landing slots at London?€?s busy Heathrow airport, I became interested in examining flight data across the US to see if I could determine what airlines or airports had the most delays. 
,
Finding air-traffic data turns out to be easy. The Bureau of Transportation Statistics (BTS) provides the , of US airlines dating back to 1987. For the purposes of this analysis, I decided to focus only on recent performance, so I downloaded the files for the flights taking place in ,. 
,
You can follow along yourself by , and ,. 
,
I now want to get an overview of the on-line performance data and to re-structure it for my analysis. In this first blog post, I will walk through how I used Trifacta to prepare the data in the following way:
,
,
,
Upon registering the data within Trifacta, the system quickly examines the structure of the file and automatically infers how to split the data set into rows and columns and promote the header row of the file.
,
,
Following the initial transformation steps, I notice that there were a few columns that had quotes around the text. By selecting the quote character, Trifacta generates transform suggestions to clean up the quotes, including the one I want to use to remove quotations from all of the columns:
,
,
,
,
,
This flight performance dataset is incredibly wide with over 111 columns in the fill. Instead of dealing with all of these columns, I quickly remove the ones that are not relevant for my analysis. To do this, I use Trifacta column browser and column details views to explore the different columns, and then select columns that I want to remove. For example, the data contains a FlightDate column, making the year, month etc. column redundant:
,
,
Similarly, I remove other redundant and unrelated columns after inspecting them quickly using the column browser and the column details features. At the end of this process, I have reduced the number of columns to 32, making the whole data set more manageable.
,
,
,
Inspecting the OriginCityName columns shows that the state name is included in the city name. Below the city name, there is also a origin state column that I quickly open in the column details view.  
, 
,  "
"
,

,

, is Principal Data Scientist at ,. He has designed, simulated, patented, and built terrestrial and space robotic systems often stretching his optical, thermal, mechanical, chemical, and electrical engineering knowledge. He has chased his unreliable software across a field and into a street sign (1st and 2nd DARPA autonomous vehicle Grand Challenge competitor). He has also renovated and skippered a fiberglass sailboat halfway around the world with his wife. He can't wait to see what automation technology will make possible next year.

,

Here is my interview with him:
,
,
,
,: , contracted me to mine ERP (Enterprise Resource Planning) data and predict product return rates for consumer electronics products. The objective was to find ""business-actionable intelligence."" For one product line the data revealed savings of millions of dollars at the cost of a minor process change. This change will also significantly improve the customer experience and product quality and thus increase future revenue.

,

For a second project, SHARP needed predictions of commercial building daily power consumption profiles and peaks. We delivered a neural net and quantile filter with predictions that will enable fully autonomous operation of a system that dramatically reduces the energy bill for commercial buildings where it is deployed.

,
,

,

,
,

,

For example, on that project at SHARP that I mentioned, the non-technical sales team came up with a database query and statistical measure that was sufficiently accurate to monitor the effect of process improvements and forecast return rates well into the future. And it was in place, integrated into their process long before my slightly more accurate, precise, and complicated model was ready. And we continued developing and implementing ""value-add"" features such as natural language processing and interactive visualizations long after the lion's share of the value had been extracted from the data.

,

,

,

,I talk with people who understand the business area and technology I'm analyzing. I use data to put statistical weight behind their hunches, or, sometimes, steer misperceptions back onto sound scientific ground.

,
We brainstorm while visualizing and slicing data from various angles until a trend emerges. Only then do the executives have strong supporting evidence and team buy-in to support them as they begin the challenging task of redirecting a large, complex organization.

,

,

,

,I default to open source. Fortunately SHARP's Big Data ,project supported my preference for Python, Django, Postgres, numpy, pandas, d3.js, and bootstrap.js, on Linux. These are the most flexible and effective tools I know of for predictive data analytics and they've not let me down in the 4 years since I settled into that stack. On the devops side, I'm a big fan of??,??and??,. For presentations I use??,, sometimes with a??,??slide wired up to a??,??SMS number.

,

,

,

,Over 21 years of munging data I've seen, and generated, a wide variety of data errors, like CSVs with unquoted strings and delimiters, misspelled and multilingual categorical (ENUM) values, unenforced database rules, churning database schemas, mutating primary keys, impossible dates, insidiously misidentified units of measure, even easter-eggs encoded in data.
,
Fortunately python, Django, Postgres, pandas, and Python itself have features that make it straight-forward to identify outliers and impute or delete troublesome records.
,
,
,
,
,  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
, (Machine Learning) made a lot of noise , last month. Shortly afterwards, someone posted a link to , on HackerNews and it quickly became one of the most popular posts. Google?€?s product is quite similar to Amazon?€?s but it?€?s actually much older since it was introduced in 2011. Anyway, this gave me the idea of comparing the performance of Amazon?€?s new ML API with that of Google. For that, I used the ,. But I didn?€?t stop there: I also included startups who provide competing APIs in this comparison ?€? namely, , and ,. In this wave of new ML services, the giant tech companies are getting all the headlines, but bigger companies do not necessarily have better products. 
,
,
,
Here is a tweet-size summary: 
,
,

,
,
,

The ML problem in the Kaggle credit challenge is a binary classification one: you?€?re given a dataset of input-output pairs where each input corresponds to an individual who has applied for a credit and the output says whether he later defaulted or not. The idea is to use ML to predict whether a new individual applying for a credit will default.
,
ML has two phases: train and predict. The ?€?train?€? phase consists in using a set of input-output examples to create a model that maps inputs to outputs. The ?€?predict?€? phase consists in using the model on new inputs to get predictions of the associated outputs. Amazon ML, Google Prediction API, PredicSis and BigML all have similar API methods for each phase:
,
,

All 4 services offer free accounts which I used for this comparison (note: PredicSis is still in private beta but you can ,). In this post, I will only compare the performance of these two methods and I won't consider other aspects such as pricing, features, DX, UX, etc.
,
In order to evaluate the models produced by the APIs, we need to separate our dataset downloaded from Kaggle in two: a training set which we use to create a model, and an evaluation set. We apply the model to the inputs of the evaluation set and we get a prediction for each input. We can evaluate the accuracy of the model by comparing the predicted output with the true output (which was held out).
,
The dataset we start with contains 150,000 instances and weighs 7.2 MB. I randomly selected 90% of the dataset for training and in the remaining 10% I randomly selected 5,000 inputs for evaluation.
,

,
For each API, there are three things to measure: the time taken by each method and the accuracy of predictions made by the model. For accuracy, I used the same performance measure as that of the Kaggle challenge, which is called 
,. I won?€?t explain what it is here, but what you have to know about AUC is that a) performance values are between 0 and 1, b) a random classifier would have an AUC of around 0.5, c) a perfect classifier would have an AUC of 1. As a consequence, the higher the AUC, the better the model.
,
, 
,

Times for predictions correspond to 5,000 predictions. FYI, the top entry on the leaderboard had an AUC of 0.870. If you?€?d used these APIs in the Kaggle competition, here?€?s the approximate rank you could have had:  "
"
,
,
, is currently the Assistant Dean for Informatics, and Professor of Clinical Sciences at the ,, charged with developing a curriculum to support the needs of the next generation of Clinical and Research Informatics leaders.  He is also the Chief Medical Information Officer for the ,, overseeing the clinical aspects of the inpatient Epic implementation, ensuring it achieves its goals of supporting the clinical and academic missions of the institution.
,
Throughout his career, he has conducted research and applied techniques that help bridge the gap between Health Services Research, clinical and research operations and Medical Informatics.
,
,
,
Here is second part of my interview with him:
,
,
,
,: ,
,  I?€?d like to see efforts to simulate well-accepted clinical trials using observational data, at first focusing on patients very similar to those in the real clinical trial.  If minimal differences exist between the observational data analysis and the real clinical trial, then other observational analyses, anchored on the original clinical trial, but perhaps expanded to wider groups of people are more likely to be trusted.  If differences exits, we can study the causes and impact of the differences to better calibrate findings from the observational analyses.
 ,
,
,
,?€?Big data?€? enthusiasts often seem to view the acquisition and compilation of massive amounts of data as the major hurdle to overcome in understanding the relationship between disease outcomes and interventions.  While that is an important step, the analysis still needs to be guided, or at least informed by people with clinical domain expertise.  A big data analysis can find associations different between a positive and negative test, though with domain expertise, the analysis can include the clinical significance ordering the test in the first place, regardless of its result.
,
Even when large, longitudinal data sets are available, far too often, analyses are based on independent variables being set as the aggregate accumulation of clinical characteristics known at a point in time.   The different timing with which these characteristics appeared, and the full spectrum of clinical findings over time is often aggregated into the binary presence or absence of disease, or the highest, lowest or average laboratory parameter.  The pace and order of the availability of findings can often be as important as the results.
,
,
,
,A great deal of resources are expended to create and purchase analytical tools that are easy enough for anyone to use.  While these tools have a role in helping high level administrators become better consumers of data, the sophisticated analyses will still require engagement of people with the analytical, research and clinical domain expertise and the time to devote to the analysis. 
,
Furthermore, often, the features that make these tools easy to use, make them unsuited toward the more sophisticated analyses that are required.  ,
 ,
,
 ,
,While I have some reservations about ?€?big data?€? in general, I am intrigued by the possibility of merging high volume and velocity data collected through passive modalities to better understand potential contributors to or to make predictions about outcomes.  For years, patients have worn diagnostic Holter monitors to evaluate heart rhythm abnormalities, though patients needed to manually input what they were doing at various points over the day, and if they were feeling any symptoms.  , , With new sensors, we can automatically capture how active the patient was at the same time as we are capturing the heart rhythm, along with external factors like temperature, and external exposures.  The ability to align internal and external parameters, along with an understanding of how active a patient is over the course of a day fills in a lot of gaps in our understanding of how patients are responding to treatments.
,
,
,
,Taking risks to alter the direction of your career, within reason, can be very rewarding, both personally and professionally
,
,
,
,People who conduct data analytics often approach it mainly from the perspective of their initial training.  Computer scientists, mathematicians, statisticians, clinicians, and computational biologists all help to advance data science.  It is important to know the priorities, methods, assumptions and goals of the other players in the analytical space, and know how to engage them in analytics projects.
,
,
,
,In addition to supporting the creative arts pursuits of my wife and daughters. My current hobby is amateur robotics with the Raspberry Pi
,
,
,  "
"
Most popular 
, tweets for May 04-11 were
,
#DataScientist vs Data #Engineer: The difference in roles 
, #DataScience #rstats #DataViz 
,
,
,
Great #MachineLearning and #DeepLearning course at Oxford - slides, video, problems 
,
,
Courts docs show how Google Gmail slices users into ""millions of buckets"" 
, 
,
,
,
3 Things About #DataScience You Won't Find In Books: Evaluation, Feature Engineering, ... 
, 
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,  "
"
,

,

, is Principal Data Scientist at ,. He has designed, simulated, patented, and built terrestrial and space robotic systems often stretching his optical, thermal, mechanical, chemical, and electrical engineering knowledge. He has chased his unreliable software across a field and into a street sign (1st and 2nd DARPA autonomous vehicle Grand Challenge competitor). He has also renovated and skippered a fiberglass sailboat halfway around the world with his wife. He can't wait to see what automation technology will make possible next year.

,

,
,
Here is second part of my interview with him:
,
,
,
,: ,
,
,
,
,
,That depends. Most data science problems can be solved by a data or software engineer (not scientist) using off-the-shelf libraries and services combined with an undergraduate understanding of statistics. Machine learning technology is proliferating so quickly, and data is being generated at such a rapid pace, there is rarely a project where available data has been fully mined with existing tools. Rarely is new software or deep experience required. 
,
But when a company does need a data scientist, they'll want her to have software development skill and an understanding of algorithms so she can customize and test new algorithms to solve their difficult problems. A strong roster of these ""programming scientists"" makes sense in highly competitive industries where predictive analytics is your core business, like Finance, Information Security, Banking, Web Analytics, and Web Search.
,
,

,Fully autonomous deep learning services will rapidly catch up with the flood of data. These services will improve business efficiency, trim waste, supplant industry incumbents, reduce payrolls, and increase job satisfaction for those that survive the ""neck down"". Businesses that build or employ these services will thrive.
,
,
,
,A brilliant idea is worth little compared to diligent execution. Many will have the same excellent idea. Few will have the humility to share it with others or work diligently to build and employ it. Dreaming is easy. Building something valuable to others is hard work.
 ,
,
,
,Since I was a child I dreamt of machines that could make decisions better than I and help me see ""all the light we cannot see."" I've not been disappointed or bored by machines yet.
 ,
,
,
,Humility. If you're unwilling to have your theories disproven, you will have a hard time letting the data and your teammates speak the truth to you. Almost all theories are eventually disproved or refined. Humility helps you advance down that path more rapidly.

, ,
,
, by Anthony Doerr.
,
I like to rock climb, tinker with toy robots in the basement, and teach science.
,
,
,  "
"
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,,
,, 
is a Chief Data Scientist at The New York Times & Associate  Professor  of Applied  Mathematics at Columbia University. 
,
,
,
,??
,
,
,
,
,
,??
,??
,
,
,
,
,??
,??
,
,
,
,
,??
,??
,
,
,
,
,??
,??
,
,
,
,
,??
,??
, Here are these images accompanied by a lovely music:
,
,
,
Bio: , is a Ukrainian PhD Scientist interested in Marketing Data Research based on Data Mining & Machine Learning Techniques. 
He blogs at ,.
,
,
,  "
"
,
,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,


,

"
"
        ,.
,
I've noticed people frequently misusing data to find correlations between seemingly unrelated data sets and inferring a relationship. While they'll generally volunteer that they haven't proven causality, they frequently claim that there must be some underlying relationship for the p value to be so low.
,
I built a toy to try and show the error in this. Essentially, you can take almost any real life data and infer a relationship, especially if you perform multiple tests. Here I take a number of data sets from , and plot whichever have very low p-values.
,
Here are some examples (see live demo below).
,
,
,
,
,
,
,
The causes of these 'relationships' vary, but a few key factors that I think are generally worth checking. These don't invalidate the slope or intercept, but they may call the test statistics into question (e.g., p value).
,
,??,
,
is a student at Harvard Business School, last time studying biomedical engineering at Johns Hopkins University. He is interested in data science, investing, design, books, and really just about everything else.
,
,.
,
See below an embedded demo using shinyapps.io (may be pretty slow).
,
Wait for it to load, then hit the ""Another Relationship!"" button.
,
,  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
As with Big Data, Data Mining is a topic with many great videos on YouTube. However, it can be hard to filter through the mass of videos to find this solid content. Previously, we looked at the ,. This week, we see what YouTube has to offer by way of Data Mining videos. If you want to view all of the videos in one place, a playlist can be found ,. View counts as of May 17, 2015.
,
, (155,651 views)
,
This video covers the first day of a multi-day course produced by Google Tech Talks. There are actually twelve videos in this series that aren't afraid to get into the nitty-gritty of statistics in data mining. If you're looking for very detailed workshop-like videos, this is great for you.
,
, (152,825 views)
,
This is a brief (23 minute) introduction to using Weka for data mining. It's good if you're interested in a brief introduction into how to apply Weka to data mining from a beginner's perspective, this is a good resource.
,
, (117,173 views)
,
This video is the first in a series of data mining tool videos by StatSoft. This series goes over numerous interesting tools, starting with this first session on STATISTICA. Take a look at this series to see if any of the tools interest you.
,
, (103,078 views)
,
This introduction to R for data mining is produced by Revolution Analytics. It's a great, mostly-recent, introduction to the subject by a very knowledgeable group of people in the subject. Considering the popularity of R in the field, this is a very useful video for those looking to begin experimenting with hands-on applications of data mining.
,
,
,
, (89,698 views)
,
This video is part of a lecture series in a database systems class. It focuses on data mining and knowledge discovery and acts as a good introduction to the topic in an academic setting. If you prefer lecture-style videos, this is a good introduction to the subject.
,
, (85,758 views)
,
This video is the first in a three-part video series on data mining. It takes an application-driven approach and uses commonly-available tools in a business environment. I think this is a good video to show business users to show them how to apply data mining techniques to business cases.
,
, (31,236 views)
,
This brief video details ten top things to avoid in data mining. It's a fun short video that offers some solid tips. Anyone working in the field could benefit from being reminded of these tips from time to time.
,
, (23,993 views)
,
This brief video is somewhat more focused than the previous videos. It details how data mining can be applied to text for beginners, and more specifically, goes into how , attacks the problem. This is a good video for those interested in text mining, NLP, or Linguamatics.
,
, (17,608 views)
,
This is another R-focused data mining video. This one instead focuses on how to use Rattle for data mining. This is a great video because of how exciting of a prospect Rattle is for making data mining easier to perform. If you haven't had a chance to dive into Rattle yet, this is worth the watch.
,
, (12,666 views)
,
This is a much more applications-focused video than any of the others. This video goes into how data mining was applied to the analysis of basketball positions by Muthu Alagappan. If you're interested at all in applications of data mining to sports, this is an interesting watch.
,
,
,
,
  "
"
,

,

,

,


,

,

,

,

,

,

,


,

,

,

,


,


,

,

,

,

,

,

,

,



,
,

,
,


,
,

,

,




,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs. He will be working for Amazon this summer as a machine learning scientist.


,
,
,


,
,

,
  "
"
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,Chandramani Tiwary
,March 2015, Packt Publishing.
,
Acquire practical skills in Big Data Analytics and explore data science with Apache Mahout 
,
,??,
,
,
,??,
,
,
If you are a Java developer and want to use Mahout and machine learning to solve Big Data Analytics use cases then this book is for you. Familiarity with shell scripts is assumed but no prior experience is required.  "
"
,
,
,
,Ashish Gupta
,February 2015, Packt Publishing.
,
Build and personalize your own classifiers using Apache Mahout
,
,
,
,??,
,
,
If you are a data scientist who has some experience with the Hadoop ecosystem and machine learning methods and want to try out classification on large datasets using Mahout, this book is ideal for you. Knowledge of Java is essential.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
By Gregory Piatetsky,  
,.
,
, sent me a report on a 
,
group in Melbourne,
where they recently had a very good session with two speakers giving overviews of R and Python, and arguing why each one is better.
The videos are on Youtube and below, and the R one is particularly entertaining.
,
,
Related to that, here are the results of a KDnuggets Poll which asked: 
,
If used either R or Python for data analysis/data mining, did you switch, 
,
,
,
,
,
,R is a free, open-source language and interpreter that is designed for reading, handling, and analyzing data, and for reporting the results.  We'll introduce R and its motivating concepts, spend some time getting familiar with R's strengths, brush dismissively over its weaknesses, and make inflammatory comparisons with other languages.  We'll talk about tasks for which R is particularly well suited, and why, and demonstrate its use in situ.
,
Andrew Robinson has been messing around with applied statistics in various guises since the 1980's.  He started using S-plus (V3.3) in the late 90's and escaped to R in about 2002, to get away from the GUI disaster of S-plus 2000.  His analytical work has covered forestry, ecology, oncology, risk analysis, and biosecurity.  He is now Deputy Director for CEBRA, the Centre of Excellence for Biosecurity Risk Analysis, and Reader and Associate Professor in Applied Statistics at the University of Melbourne. 
,
,
,
,??,
,
,Python is super easy to learn. It's logical, well documented and well supported. It also has an amazing and approachable data stack which can be used for analytics, visualisation, machine learning and even building end-to-end production systems. This talk will give an intro to those things.
,
Chris Hausler is a Data Engineer at Zendesk. Previously he's held the titles of data scientist, student, consultant, programmer and before that student again. Throughout all these things he loved playing with data and he still does. 
,
,
,
,
,
,
,
,
 ,  "
"
,
,
, is currently VP of Data Products at ,, the leading online real estate auctioneers in the United States.
,
He has also held positions at two leading online games companies, Kabam and Playfirst, where he has built out Analytics and Big Data groups from inception. Earlier on in his career, Sheridan worked at Procter and Gamble in the areas of Decision Support and Executive Information Systems. He also worked as a Managing Consultant at Towers Perrin.
,
Sheridan holds a Bachelor of Arts degree in mathematics from Cambridge University and an MBA from the Haas School of Business at the University of California, Berkeley, both with honors.
,
Here is my interview with him:
,
,
,
,: I think the key advantage is typically for the marketing department; if you understand lifetime value, particularly across demographics and acquisition channels, you can understand what marketing investments are ROI positive and where you should invest or cut back.
,
,
,
,I find that the maturity levels differ across industry. In consumer focused businesses that are primarily online and do a lot of online acquisition, e.g. the online gaming industry, they can be pretty advanced. However if you?€?re in an offline industry, say steel production, they may not be very advanced, and quite frankly might not need to be. 
,
The biggest factor in the reliability of online models, is our ability to predict future revenue flows. In a traditional industry such as utilities where customers pay monthly may be much better able to predict than an industry undergoing rapid change, like online media.
,
,
,
,The model looks at future flows of gross margin; if we believe we need to make an ongoing investments then we can either factor those in, or we can consider the new investment in its own right and understand if the change in cash flows ends up being positive from a net present value perspective.
,
As for tools, I?€?m sure there are some sophisticated ones out there, and no doubt I?€?m going to subject myself to a bunch of vendor calls, but most CLV models seem to be done in Excel, and you know that?€?s often fine.
,
,
,
,I think the fundamental challenge is the future is hard to predict. Last time I checked, I wasn?€?t cruising around in my flying car. In strategy planning, it?€?s often useful to use more abstract analytical tools; I?€?m personally a big proponent of traditional scenario analysis. You might look at a variety of different scenarios around market growth, competitive pressures, and the regulatory environment that you may face. You can then get external data that helps understand and build these scenarios, and factor those into your models.
,
,
,
,Part of the problem of many of the efforts I see in this area is that they often start by asking question like ?€?What is my Customer Lifetime Value??€? or ?€?How should I attribute my marketing spend??€? These are somewhat abstract questions, and they don?€?t address the underlying decisions you?€?re trying to make. It would be better if you started with questions like ?€?Should ,I invest more money in online marketing, and if so where should I invest it??€? Those answers give me practical real world answers to decisions you need to make.
,
As for attribution models, they?€?re complex, and to understand how they may be subject to inaccuracies, you only need to consider yourself. If I buy a Gap Sweater, after I saw a billboard on my way to work, got a recommendation from a friend, and clicked on a keyword ad in google, I can?€?t tell you what percentage each of those three brand interactions contributed to my decision to purchase. Of course you can look at what effects different combinations have and there?€?s some very interesting work out there on this. One marketing tech company I came across, Abakus uses game theory to do it, which is absolutely fascinating. The bottom line is it?€?s a very complex problem and there?€?s no silver bullet.
,
,
,
,
,  "
"
,, May 29, 2015.
,
Location: Marriott Kingsgate at the University of Cincinnati.
,
This year the Summit will feature two internationally recognized leaders in the field of analytics.  
,
,
,
John Elder leads America's largest and most experienced data mining consultancy and his company has solved projects in a huge variety of areas by mining data in tables, text, and links. Dr. Elder co-authored 3 books, has created data mining tools, and was a discoverer of ensemble methods.
,
,
,
Stephen Few has over 20 years of experience as an innovator, consultant, and educator in the fields of business intelligence (a.k.a. data warehousing and decision support) and information design. He focuses on the effective analysis and presentation of quantitative business information. Stephen is recognized as a world leader in the field of data visualization.
,
In addition to the speakers there will be four afternoon breakout sessions focusing on descriptive analytics, prescriptive analytics, predictive analytics, and one other analytics related topic.
,
For more information visit
,
,
,
,.  "
"
        ,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,  "
"
,
,
,.
,
Following the viral success of 
, image recognition demo, which uses advanced image recognition and machine learning to determine the age and gender of a person, 
KDnuggets cartoon imagines a near-future where another demo determines, 
given a photo, how smart you are (and possibly your species).
,
,
,
,
,
This finally explains why cats ""rule"" the internet.
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,
,
,
 ,  "
"
,
,
, is Vice President, Data Sciences, at ,, one of the World's largest eCommerce sites. 
,
At Ticketmaster Antonio is leading key machine learning initiatives on recommendations, predictive user modeling, forecasting and large-scale distributed systems for real time optimization problems. Antonio worked on algorithmic content production and marketplace design at Demand Media. He acquired an extensive background in on-line advertising at Yahoo!/Yahoo! Research and Fox Audience Network/The Rubicon Project. Antonio conducted research on IP networks and network data mining while at Microsoft and Fujitsu Laboratories of America. 
,
He holds four patents and a Ph.D. from the University of Tokyo, Japan.
,
Here is my interview with him:
,
,
,
,: The Data Science Team at Ticketmaster is entrusted with the collection, management and mining of varied data sets. Our mandate is to use principled and quantitative approaches for knowledge discovery and decision-making. And use the information distilled from our data to design novel algorithms and data products that can transform the business.
,
In broader terms, our mission is also to contribute to the Machine Learning (ML) community, evangelize those principles and architectures that are key for the delivery of real world, enterprise-grade ML platforms. For instance, we organize regular meet-ups on lambda architectures, we sponsor internship programs, etc.
,
,
,
,The dimensions that typically characterize the data sets we routinely handle are those that one would expect from an eCommerce site that ranks among the top 5 world wide in terms of revenue. Volumes of data are quite large. Velocity at which data enters our data lakes is similarly high. Our catalog of live events and performing attractions spans all genres and multiple international markets.
,
In addition to these considerations, there are other aspects that are entirely peculiar to our business domain. Our data needs to be regarded as longitudinal in nature. We have records pertaining to user interests in live events that go back decades. The knowledge to be gained from mining this data is unique. Another singularity about our data is its bursty nature. When very popular events go on sale, it becomes a social phenomenon. Traffic spikes are of a far greater magnitude than regular traffic. These act like ?€?denial of service?€? attacks with a transient duration. Being able to devise algorithms that can manage such varied time scales and dynamics is a very exciting challenge.
,

,
,
,Anmol, as you stated there are several challenges we face daily. Like I mentioned before, on-sales are an intrinsic part of our business. On-sales embody the excitement of fans for the performers they love. On-sales can be so large as to acquire a social dimension. It is critical, therefore, for Ticketmaster to provide the best possible experience to fans during on-sales. Our platforms and algorithms need to be highly performant under these conditions of high system stress. Other retail sites typically are not exposed to such extreme situations.
,
You can easily conceive how label distribution for incoming traffic can be skewed and time varying. This poses a significant challenge in terms of the generalization power of models previously trained. We strive to adopt and engineer state of the art algorithms. On-line learning and streaming algorithms are heavily utilized in our various incarnations of lambda architectures. Naturally, we also adhere to sound and well-established engineering practices that guarantee scalability, consistency and resiliency.
,
,
,
,Lambda architectures present distinctive traits that make them particularly suitable for real-world, large-scale data products. They provide a viable answer to the need of gathering insights that are function of the entire data set. At the same time, lambda architectures are built with the capability to support data processes at different data and temporal granularity. Enterprise data pipelines, most of the time, comprise heterogeneous processing modalities: computing can be batch (e.g. Hadoop) vs. (near) real-time vs. streaming (e.g. Storm), data storage can he file-oriented (e.g. HDFS) vs. columnar (e.g. HBase). 
Additionally, given the distributed nature of lambda architectures, they are designed to be scalable and fault-tolerant.
,

,

,Because of the heterogeneity of our data and data sources, at Ticketmaster we have several instances of lambda architectures. Each one of them is customized to address the specific needs of each data consumer. We have been focusing on building new data products. As these products become successful and new data emerges, these separate lambda architectures will expand and integrate with each other organically.
,

,
,
,The adoption of lambda architectures has had a notable impact on our ability to move quickly, incrementally deploy and measure our progress. It is not uncommon to encounter significant obstacles to scale a prototype up in a predictable and reliable manner. Because of the traits that lambda architectures have built in from the get go, it was easy for us to scale as needed. This is critical for a business of the size of Ticketmaster. Additionally, having more timely data also made a huge difference for us in terms of algorithms (streaming/near real-time) to we could support and this has resulted in overall greater user engagement.
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
,
Apache Spark, which is a fast general engine for Big Data processing, is one the hottest Big Data technologies in 2015. 
It was created by Matei Zaharia, a brilliant young researcher, when he was a graduate student at UC Berkeley around 2009.  Since then , grew into
, with hundreds of contributors and deployments.
I recently had a chance to ask Matei Zaharia a few questions about Spark and Big Data - see interview below. 
,
, (,) was born in Romania, but his family later moved to Canada where he graduated from U. of Waterloo with a medal for highest academic standing.  He received numerous awards at programming contests including a gold medal in ACM ICPC in 2005. He started the Spark project while a graduate student at UC Berkeley, where he received his PhD in 2013. He is currently an assistant professor at MIT.
He is also CTO of 
, and serves as Spark's vice president at Apache. 
,
,
,
,:
I initially started working on big data systems through Hadoop, back in 2007. I worked with early Hadoop users (e.g. Facebook and Yahoo!) and saw that they all had common needs in terms of new workloads they wanted to run on their data that MapReduce wasn't suited for (e.g. interactive queries and machine learning). I first began Spark in 2009 to tackle the machine learning use case, because machine learning researchers in our lab at UC Berkeley were trying to use MapReduce for their algorithms and finding it very inefficient. After starting with this use case, we quickly realized that Spark could be useful beyond machine learning, and focused on designing a general computing engine with great libraries for building complete data pipelines.
,
,I think several things helped the project get to where it is today. First, we worked from very early on to foster a great community, including mentoring external contributors and accepting patches from them, publishing free training materials, etc. This led to many new contributors to the project from both inside and outside UC Berkeley. Second, Spark provided advantages in several dimensions (speed and ease of use) that were unmatched by alternatives. Third, we've managed to scale the development process and keep the project evolving quickly, so we continue to see exciting ideas added in Spark. Some recent ones include DataFrames, machine learning pipelines, R support, and a huge range of new algorithms that we're getting parallel implementations for in , (Apache Spark's scalable machine learning library).
,
,
,
,
Here are some lesser-known things:
,
,??,
,
,
,
,To me, the most exciting question is the applications, especially beyond traditional data processing. We're already seeing some exciting scientific applications built on big data systems like Spark, including in genomics, neuroscience and image processing. Some of these could enable new industrial applications, where this type of data crunching could be done on a regular basis to process data from industrial machines or sensors, analyze medical scans or sequencing data, etc. I think that by 2020 we'll see several such applications in common use.
,
I also think that the cloud will play a big role, which is why Databricks started with a cloud product. The cloud provides a very low-cost way to store and manage data, and lets organizations focus on just the processing they want to do instead of operations / IT. It's where a lot of data is ""born"". And it makes it very easy to deploy and run new applications at the same site where the data lives, which is important for data processing because a lot of it is exploratory. 
,
,
,
,
Databricks offers a cloud service that makes it easy to deploy Spark applications and work on data collaboratively in a team. The code you write is all Apache Spark code, and can therefore run on any Spark cluster. However, we provide tools to make it easy to run this code (e.g. schedule a production job and get an email if it doesn't run), and a UI for fast, productive data exploration (a Google-Docs-like notebook environment, publishable dashboards, etc).
,
,
,
,
In Databricks Cloud, we're building some pretty exciting new features that we plan to announce soon (stay tuned around 
,, June 15-17). One that I worked on closely was Jobs, our feature for deploying and monitoring Spark applications. Another area I'm involved is Project Tungsten, which is an effort to let Spark leverage modern hardware advances (e.g. solid-state memory, vectorized instructions in CPUs, maybe even GPUs) that presents lots of potential opportunities throughout the engine.
,
,
,
,
While it is true that Spark uses a micro-batch execution model, I don't think this is a problem in practice, because the batches are as short as 0.5 seconds. In most applications of streaming big data, the latency to get the data in is much higher (e.g. you have sensors that send in data every 10 seconds, or something like that), or the analysis you want to do is over a longer window (e.g. you want to track events over the past 10 minutes), so it doesn't matter that you take a short amount of time to process it. 
,
The benefit of Spark's micro-batch model is that you get ,. Flink and Storm don't provide this, requiring application developers to worry about missing data or to treat the streaming results as potentially incorrect. Again, that can be okay for some applications (e.g. just basic monitoring), but it makes it hard to write more complex applications and reason about their results.
,
,
,
,
In this day and age, you're never far away from Big Data. But when I am, I like to read books, walk around, and occasionally try to cook things. I really liked The Martian by Andy Weir.  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 19 and later.
,
See full schedule at , .
,
,  "
"
,
,
, is Vice President, Data Sciences, at ,, one of the World's largest eCommerce sites. 
,
At Ticketmaster Antonio is leading key machine learning initiatives on recommendations, predictive user modeling, forecasting and large-scale distributed systems for real time optimization problems. Antonio worked on algorithmic content production and marketplace design at Demand Media. He acquired an extensive background in on-line advertising at Yahoo!/Yahoo! Research and Fox Audience Network/The Rubicon Project. Antonio conducted research on IP networks and network data mining while at Microsoft and Fujitsu Laboratories of America. 
,
He holds four patents and a Ph.D. from the University of Tokyo, Japan.
,
,
,
Here is second part of my interview with him:
,
,
,
,: In my experience, being able to implement and deploy our recommendation platform in an incremental manner has been crucial. This has been a key feature that we have exploited about lambda architectures. ,This allowed us, firstly, to deliver the batch portion of it. This powers email marketing channels and provides personalized recommendations. Once the batch portion of the platform was in place, we decided to move forward and deploy the (near) real-time component of such an architecture to customize the user experience on the site. The ability to serve these two channels (email and web-site) that are so different in terms of SLA's and overall requirements is an example of the versatility and power of lambda architectures. Because this design enabled us to deliver incrementally, data mining and learning exploration benefited as we could start to gather data early on in the process.
,
,
,
,Recommender systems are one solution to a very common problem, that, in more general terms, ,can be cast into matching vertices in a bipartite graph in a manner optimal w.r.t. certain objective measures. Over the past decade, we have observed a greater appreciation for these types of algorithms and their efficacy. Several years ago, technologies required to build a full-fledged recommender system were not as mainstream as they are now. Currently, we have reached a tipping point and more and more companies understand how strategic it is to have full control on those technologies that have such a large impact on user experience, user retention and product discovery. I would expect this trend to become even more pronounced in the near term.
,

,
,
,I have been extremely fortunate to work with and be mentored by some exceptional individuals in the field of ML and computational modeling; individuals whom I consider pioneers in this field and whom I am lucky to call friends now.
,
One of the most valuable lessons has been to always be completely honest and transparent about data and what emerges from it. This requires commitment to communicate with integrity and evangelize what ML can do and, equally importantly, cannot do. This requires us to constantly strive to remove our own biases and expectations. In my view, this is critical in order to foster learning and discovery.
,
,
,
,Recently, deep learning has been enjoying a lot of excitement due to fundamental theoretical breakthroughs matched by improved practical results.  An increasing number of domains have benefited from this. I would expect this would be the case for recommendation systems as well. Similarly, I am quite excited to see recent work published about structured learning and contextual bandits and would expect ramifications in the field of personalization.
,
,
,
,The drive to ""get dirty"" with the data, the desire to handle data sets from the inception of the process and to learn from the data. In my experience, there is a really a lot of value in directly handling data ,yourself. It makes you aware of how, sometimes, this task can be challenging. This may inspire you on how to develop a better experimental setup where access to high quality data is more direct. If you have the desire to understand real-world data sets, other important, but somewhat ancillary, aspects take care of themselves. You may not have a lot of experience with Hadoop, Storm or other similar frameworks, however, if ""you are on a mission"" to understand and use data, you will have the right motivation to learn such technologies and be successful.
,
,
,
,I had bought , by R. Schapire and Y. Freund a while back but did not have a chance to read it until recently. It is a great book from authors who have made seminal contributions to this important learning framework. The book is a very complete presentation. In my free time, I still enjoy coding and working on fun projects.
,
,
,  "
"
        ,
By Gregory Piatetsky,  
,.
,
The 16th annual KDnuggets Software Poll continued to get huge attention from analytics and data mining community and vendors, attracting about 2,800 voters, who chose from a record number of 93 different tools. 
,
R is the most popular overall tool among data miners and data scientists, but Python usage growers faster and it is likely to catch up in 2-3 years. RapidMiner remains the most popular suite for data mining/data science, but it got fewer votes than last year. 
There was a notable increase in Hadoop/Big Data tool usage (29%, up from 17% in 2014), mainly driven by jump in Spark whose usage share grew over 3-fold. (see ,). Other tools with strong growth include H2O (0xdata), Actian, MLlib, and Alteryx.
,
This report has 5 sections
,
,??,
The participation by region was: US/Canada (41.5%), Europe (38.4%), 
Asia (8.2%), Latin America (6.3%), Australia/NZ (3.1%), Africa/MidEast (2.5%).
,??,
,
Here are the top 10 tools by share of usage: 
,
,
,
,
,
,
,??,
Compared to 
,, 
Tableau and Spark were newcomers to top 10, 
displacing Weka and Microsoft SQL Server.
,
The average number of tools jumped to 4.8, up from 3.7 in 2014 and 3.0 in 2013.
,
The distinction between commercial and free software is becoming harder to make, with many tools having both a free/community version and commercial/enterprise version. We classified each tool according to the primary type of the latest version, so we put RapidMiner in commercial category and KNIME in free software category.
,
Many vendors asked their users to vote in the poll and even tweet their vote, but we have not found any bot or illegal voting, and did not have to remove any votes.
,
This year, 91% of voters used commercial software and 73% used free software. About 27% used only commercial software, and only 9% used free-software. For the first time a majority of 64% used both free and commercial software, up from 49% in 2014. 
,
,
,
Among tools with at least 10 votes, the highest increase in 2014 was for 
,
,
Tools that showed at least 20% increases in their share for 2 years in the row are 
Alteryx, 
Hadoop, KNIME, Python, 
Qlikview, 
SAS Enterprise Miner, 
Tableau, 
and TIBCO Spotfire.
,
New analytics tools that received at least 20 votes in 2015 were
,
,??,
,
Among tools with at least 20 votes in 2014, the largest decline in 2015 was for these tools, which includes probably a combination of decline of popularity for free tools like Orange and lack of a voter drive for some of commercial tools this year.
,
,??,
,
,
,
Hadoop/Big Data tool usage jumped to 29% among voters, up from 17% in 2014, and 14% in 2013.
,
This is probably due to availability and low-cost of many cloud-based Big Data tools.  Very notable is the jump in Spark share to 11.3%.
,
However, most data analysis is still done on ""medium"" and small data.  
,
Top Hadoop/Big Data tools were 
,
,??,
,
,
New this year was a category of Deep Learning Tools, 
with most popular tools being:
,
,??,
,
However, this category is growing rapidly and above list is incomplete, since the largest count in this category was for 
,
,
See also
,
,??,
,
,
Python increased significantly in popularity.
Java is the second most commonly used language for analytics/data mining tasks.  Here is the 
,
,
,??,
,
,
The following table shows the poll results in detail.
, is the percent of tool voters used only that tool alone.  For example, only 3.6% of R users have used only R, while 13.7% of RapidMiner users indicated they used that tool alone.
,
,
,
Additional tools not included but mentioned in the comments include
,
,??,
Here are the results of past polls
,
,
,
 ,  "
"
,

,

,Since last few years, we have been occasionally seeing headlines about Big Data / Predictive Analytics success stories from Police Departments. At best, these are like twinkling stars in a dark night. Faced with declining budgets, increasing responsibilities, and limited human resources, law enforcement has typically had data analysis as either virtually non-existent or at most in its primitive form.
,


The rapid pace of technological development has created multitude of new data sources such as social media networks, CCTV video footage, etc. that could be of immense value for operational decision making and day-to-day policing. Thus, the police data systems desperately need an upgrade, otherwise more data will increasingly be a challenge rather than being an opportunity. After cascading for a long time, the problem has now become big enough to get attention from the highest office in the country.
,
Here is a brief summary of White House / POTUS events on Police Data, inter-leaved with my personal review:
,
,
,
Last December, President Obama signed an Executive Order to ,. The task force was asked to look into one of the most burning issues faced by our country ?€? lack of trust between the law enforcement agencies and the people they protect and serve ?€? as witnessed in multiple incidents across the country.

,

In March 2015, the task force submitted , with more than 60 recommendations to the President. The report clearly acknowledged the need for data:

,

,

,

However, when it comes to the recommendations they barely scratch the surface of the potential of data analytics towards enabling better policing. Reading recommendations such as the following one would rarely guess them to be from 2015 when the whole world is harnessing data as one of the key strategic assets:

,

,

,


Wouldn't you expect the above recommendation/action-item as something that must have already been in practice by now?

,

The absence of even basic data collection and reporting mechanism on critical aspects of policing is one of the biggest challenges towards any data-driven investigation of Ferguson, Staten Island, Cleveland, North Charleston (S.C.), and Baltimore incidents.

,

Even in cases, where such mechanism exist, they are rarely compatible and inter-operable, thus, delivering very limited usability.

,

,
,

It is important to note that data analytics is needed not just to better protect the citizens, but also to protect the officers who are often forced to make sub-second decisions in high stress situations. A majority of officer injuries and deaths result not from interaction with criminals, but rather from poor physical health due to poor nutrition, lack of exercise, sleep deprivation, and substance abuse. While there is extensive data about vehicle maintenance, weapon service, etc. there is virtually no data about the physical, mental, and behavioral health of law enforcement officers.

,

,
,


Even though the 35 year old Bureau of Justice Statistics (BJS) publishes numerous reports every year, it clearly needs to do much more to deliver operational insights to law enforcement agencies nationwide, besides delivering macro-level aggregated statistics to policymakers.

,
,
,
On Monday May 18,, President Obama announced the , in Camden, NJ. This initiative, a byproduct of the president?€?s Task Force on 21, Century Policing, will be implemented in 21 leading jurisdictions across the country. The goal of this initiative is to establish a community of practice that will allow for knowledge sharing, community-sourced problem solving, and the establishment of documented best practices that can serve as examples for police departments nationwide. The effort includes two main streams of work:
,
, ,

Police departments participating in the White House Police Data Initiative:
,

,

Many organizations, including Code for America and the Police Foundation, are participating in this initiative to ,.

,

In order to boost internal accountability and effective data analysis, 12 police departments have committed to sharing data on police/citizen encounters with data scientists for in-depth analysis, leading to better insights for police to intervene early and more effectively.

,
,
,

On Wednesday May 20, Dr. DJ Patil, the U.S. Chief Data Scientist, hosted a Twitter chat to discuss the Police Data Initiative, and how data can empower community policing. As soon as the chat started, people shared concerns about the quality of Police Data:
,
,
,
Some people were also concerned that data collection could lead to greater chances of information leakage which can compromise investigations, and also that bad data could lead to bad decisions. One of them referred to the Pew research stating that ,. Dr. Patil clarified that privacy and security must be central to all data projects. Besides, data collection is often prohibited for ongoing cases.
,
While the need for standards was evident, Dr. Patil remarked that the standards should not become a hurdle:
,
,
,
,
The chat ended with a call for participation, highlighting the importance of team work in Data Science:

,


,
,
,
,

So, the Police Data Initiative has got the conversation started and is making some steady progress. Amid today?€?s turbulent times, the expectations are very high and patience to achieve results is very little. Thus, the initiative would have to demonstrate early success for long-term interest across the nation.

,

This is no doubt the toughest challenge in the career of U.S. Chief Data Scientist and let?€?s hope he succeeds here as brilliantly as he had in his past roles.
,
,
,  "
"
Most popular 
, tweets for May 12-18 were
,
#Hadoop demand falls, 54% of enterprises have no plans for it, as other #bigdata tech rises 
, 
,
,
,
., #MachineLearning @Coursera class - complete & excellent course notes 
, 
,
,
,
., Learning path and resources - for Business Analyst, Data Scientist 
, 
,
,
,
#Hadoop demand falls, 54% of enterprises have no plans for it, as other #bigdata tech rises 
, 
,
,
,  "
"
,
,
, is the Chief Data Officer of the Consumer Financial Protection Bureau.  Previously, she was the CDO at the Office of Financial Research within the U.S. Treasury Department and Chief of the Economic Data Management and Analysis section in the Research Division at the Board of Governors Federal Reserve System.  
,
She has a BA in Economics from Rutgers University and an MS in quantitative Finance from George Washington University.  She has over 25 years of experience in the finance industry including at the Federal Reserve Bank of NY, Board of Governors, OFR, and CFPB. 
,
Here is my interview with her:
,
,
,
,: The Chief Data Officer role oversees the CFPB's governance, acquisition, documentation, storage, analysis, and distribution of data.  There are several advantages to having the life cycle of data centrally managed.  The first is the increased ability to ensure strong internal controls and adherence to best practices.  There are also economies of scale related to data management.  Therefore, having data management centralized creates efficiencies and helps to ensure consistency across the Bureau. An advantage for this role at a new agency is that we don?€?t have legacy systems or processes that we need to accommodate.
,
,
,
,The volume of data in the finance industry has grown exponentially in the last decade.  In 2005 a 500 gig dataset was considered big.  By 2011 the industry was working with multiple terabyte files.  When working with large and complex datasets the need for good data management practices increases.   This increased volume should further encourage standards and best practices across the industry.
,
,
,
,An ontology is a dictionary where the definition is derived in part by relationships.  I like to use the analogy that an ontology is like a forest of family trees.  In a family tree Linda Powell can be defined as a woman, a mother, a wife, a daughter.  Although I don?€?t change, how I?€?m defined depends on the relationship.  If you add multiple family trees the relationships grow to include aunt, sister in-law, daughter in-law, co-worker.
,
,
,
,Because reported data can be revised, the data can have different values depending on when you do your analysis. For example, GDP is released and may be revised as new information is received. Vintage data means that you can look at what was the value on any specific date. This is important in evaluating the quality of policies.  In order to evaluate the quality of decisions you need to know what information was available at the time you had to make the decision.
,
,

,The financial crisis highlighted the need for data standards in the financial sector. The finance industry is now starting to embrace data standards.  This is evidenced by initiatives such as the Legal Entity Identifier.  "
"
        ,.
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,
,
,
,
,
,
,
,

,
,

,
,

,
,

,
,

,
,
,

,
,

,
,
,

,
,

,
,

,
,
,
,

,
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for May 19-25 were
,
KDnuggets Poll: R leads RapidMiner, #Python catches up, #BigData tools grow, #Spark ignites 
, 
,
,
,
Handy Guide: Choosing a Learning Algorithm in , ML  #MachineLearning #BigData 
, 
,
,
,
#MachineLearning predicts that a fair race between Mo Farah and Usain Bolt is ~ 492m 
, 
,
,
,
R vs #Python, why each is better #rstats #DataScience 
, 
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
,, a site which provides training for professionals, analyzed thousands of Twitter accounts to find the most influential people in Big Data & Hadoop,
and compiled a list of top 150, based on their 
, score and their Social Authority score (Moz).
,
The word cloud from all 150 descriptions shows, not surprisingly, Big Data and Hadoop, 
as the most popular terms.  Other important terms include Analytics, Apache, Engineer, Cloud - all very much associated with Big Data and Hadoop.
,
,
,
Here are the top 20 and their short Twitter bio (Description).
,
1. Merv Adrian ,, Score: 128
,Description: Gartner analyst - Microsoft Lead, Hadoop, Big Data, NoSQL,       DBMSs. Guitarist, husband, dad, dog lover.
,
2. Alistair Croll ,, Score: 127
,Description: Writer, speaker, startup accelerant. Big Data, tech and business. Strata, Startupfest, Bitnorth. Wrote ,. 
Working on ,.
,
3. Ben Lorica ,, Score: 125
,Description: Chief Data Scientist ,. 
Director of Content Strategy ,. Aspirant Parisien. Every Sunday is a Hack Day.
,
4. Paul Zikopoulos  ,, Score: 121
,Description: Award winning speaker & author (19 books, 350+ articles). IBM VP. Trying to be best dad I can be (no manual provided at birth). Opinions boisterous but mine.
,
5. Mathias Herberts   ,, Score: 117
,Description: Disruptive Engineer, Chief Technology Officer, Co-Founder ,
,
6. Gregory Piatetsky   ,, Score: 117
,Description: KDnuggets President, Analytics/Big Data/Data Mining/Data Science expert, KDD & SIGKDD co-founder, was Chief Scientist at 2 startups, part-time philosopher.
,
7. Gil Press    ,, Score: 117
,Description: I launched the #BigData conversation; writing, research, marketing services; 
, 
,
,
,
8. Tony Baer     ,, Score: 114
,Description: IT analyst with Ovum covering Big Data & data management with some systems engineering thrown in. Views stated here are my own.
,
9. John Akred     ,, Score: 113
Description: Data Scientist, Musician, Engineer, Analog Audio and Vacuum Tube lover. These thoughts are my own, but I love being CTO at Silicon Valley Data Science.
,
10. Jeff Kelly    ,, Score: 113
,Description: Industry analyst covering the impact of Big Data on the enterprise at ,.
,
11. Michael E. Driscoll     ,, Score: 112
,Description: Founder + CEO ,. Investor @DCVC. I heart data, analytics, & visualization.
,
12. Ted Dunning   ,, Score: 111
,Description: Committer on Apache Mahout, Apache Drill, PMC on Mahout, Zookeeper and Drill, Product Architect at MapR, ex-Chief Scientist at Veoh, Musicmatch.
,
13. Krish Krishnan    ,, Score: 110
,Description: Big Data Evangelist, Data Scientist,DW SME. Author, Strategy Consultant, Speaker, TDWI Faculty, CTO
,
14. Dain Hansen   ,, Score: 110
,Description: Marketing Evangelist for Oracle. Passion for all things #Digital | #Cloud | #BigData | #PaaS | Foodie + Traveling Family Man. Views my own.
,
15. Patrick McFadin    ,, Score: 107
,Description: Chief Evangelist for Cassandra. Data Modeling. Operations. Architecture. DataStax. I travel the world educating and advocating.
,
16. Milind Bhandarkar   ,, Score: 106
,Description: Data, Analytics, Parallel & Distributed Systems Practitioner. Opinions are my own, of course! RT != concurrence.
,
17. Beau Cronin    ,, Score: 106
,Description: I want to understand and build intelligence. Probabilist, computational neuroscientist, AI realist, wide-eyed VR schemer.
,
18. Michael Hiskey     ,, Total Score: 105
,Description: Coalescing the vapor of human existence into a palatable whole; outspoken purveyor of disruptive Tech: #BigData #Cloud #Analytics #BI. #BBBT Alum, #DataWrangler
,
19. Michael Cavaretta   ,, Score: 105
,Description: #DataScience Leader - Ford Motor Co. Opinions are mine. Top Big Data Influencer/Speaker - Leader of Big Data Drive
,
20. Arun C Murthy    ,, Score: 105
,Description: Co-Founder, ,. Moving Apache Hadoop forward since day one, since 2006. Older soul on sports than tech.
,
Here is a full list of  
,.
,
,
You can also get top users by social authority from
,.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
Miner3D, a leader in 3D data visualization systems, releases version 8 of its flagship product.
,
The new software is available now and features a redesigned user interface, making it a perfect couple with Excel or other popular Windows data storage or analysis programs. Further improvements can be seen in the graphics visualization engine, which is now faster and smoother.
,
,
,
Miner3D is reasonably priced and is also available as a free 30-day trial.
,
Miner3D includes Enterprise, Professional, and Basic versions.
,
,
,
,
,
Miner3D ONE??? uses a collection of new user interface controls and actions designed for easy, intuitive and efficient data analysis on touch screen and mobile devices. There are no menus, no dialogs, no annoying texts, and no useless window borders.
,
The whole control is at the tip of your finger, exactly where you would expect it.
,
Got no touch screen? No problem!
,
Miner3D ONE??? is equally easy to use with mouse and is working perfectly also on Windows 7, Vista and XP.   "
"
,
, is the world's largest conference focused on Apache Spark. 
,
This year, the premier west coast event takes place in San Francisco from June 15-17, 2015. Join us to learn how organizations such as the CIA, NASA and Andreessen Horowitz along with companies like Databricks, Hortonworks, Toyota, Cloudera, IBM, Intel, MapR, Baidu, Amazon are utilizing Spark to run their businesses more effectively.
,
The Summit contains 2-days of presentations with an optional 3rd day of Spark Training provided by Databricks. We have a rich line up of presentations ranging in topic from 
,
,??,
Visit our site to see the full 
, of over 70 presentations and use the code 
, for 10% off at 
,.  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
INFORMS, the world's largest organization for advanced analytics professionals, provides data analytics and operations research information, training and best practices to its members and the industry at large. 
,
Combining analytics training with baseball, the number 
one sport on the use of analytics, is a natural fit. Learn how to better apply analytics over two days and enjoy a Cubs game to see analytics in practice while networking with your colleagues.
,
,
,: Register today for 
, and we'll take you to Wrigley Field for a night at the ballpark to see the Chicago Cubs. 
,Register early as we only have a limited number of tickets available.
,
,
,
The essential tools for solving real-world problems in businesses and other organizations are critical. Drawing on best practices from the field, this course helps analytics professionals add value from beginning to end: listening to clients, framing the central problem, scoping a project, defining metrics for success, creating a work plan, assembling data and expert sources, selecting modeling approaches, validating and verifying analytical results, communicating and presenting results to clients, driving organizational change, and assessing impact. 
,.
,
Don't miss these intensive hands-on courses that provide attendees with real take-away value. For detailed course information and to register visit , or contact Thedra White (,; +1-443-757-3570).  "

"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for May 26 and later.
,
See full schedule at , .
,
,  "
"
,
,June 10, 2015, 10am PT, 1 pm ET
,
Everywhere you look, companies are using external-facing analytics to maximize the value derived from their data assets, by moving customers up the value chain, increasing stickiness, and offering a more competitive product on the marketplace.
,
Join to learn about how to bring an external-facing data product to market by embedding BI software, and what that can add to your offering.??Colin (Chief Analytics Officer) and Zach (Data Analytics and Product Marketing) will discuss:
,
,??,
,
, June 10. 10am PT, 12pm CT, 1pm ET.
,
,
,
,??,, Chief Analytics Officer
,
,??,, Product Marketing Manager
,
,.  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
The First Virtual Expo for the Big Data Community
,
The BIG DATA Expo is a unique platform that provides a B2B experience in the form of 
,  (mass multiplayer online role playing game) featuring live presentations, virtual exhibition halls with virtual booths and enhanced networking capabilities.
,
,
,
,
, The event is dedicated to host the BIG DATA industry key players, among them: 
Engineers, Data Scientists, Developers, Architects, Managers, Executives, Students, Professional Services, Architects,Networking specialists, Data Analyst, BI Developer/Architect, QA, Data Warehouse Professionals, and others.
,
,
,Our unique and innovative platform will allow you to participate in a virtual worldwide event and do business matching as if you were there in person Network with people from around the world easily with no traveling and very low costs
,
,:
, The industry is characterized as an industry of prominent ongoing developments, trends, and innovations. Working in this industry requires frequent traveling to exhibitions around the world every few weeks. We believe that ""The Big Data EMeetup"" is the optimal solution for many key industry players to explore new way to network and do business without the need travel so much and have endless budgets for exhibitions. 
,
,
will be open to everyone, and will offer free access to the presentations and to the virtual exhibition halls, where companies will showcase their products via virtual booths and have their representative chat with the attendees via a video conference, the virtual expo will function just like a regular MMORPG (Mass Multiplayer Online Role Playing Game) and will offer a mix of business and fun.
,Entering the event is as simple as opening a browser and joining online. There are no traveling costs involved for the attendees and no booth design costs for the exhibitors. Today, in the digital age, networking and doing business is just a click away.
,
We are horned to invite the industry to join us at ""The Big Data EMeetup"" and to experience this unique opportunity to explore, mingle, and network with the Data  community - right from your own computer!
,
The first event will be held on July 7
,
For further information please visit , 
or contact us at , .
  "
"
        ,
,
,
  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is currently a Senior Data Scientist working on Discovery & Personalization algorithms and data processing at ,. ,Before Netflix, he studied computational biology at The Johns Hopkins University School of Medicine, where his PhD research in the Department of Neuroscience employed machine learning models to predict adverse side-effects of drugs. ,He also previously worked at Chicago-based Accretive Health as a Data Scientist, focusing on data related to patient billing and referrals.
,
Here is first part of my interview with him:
,
,
,
,: Optimizing user discovery of content on Netflix, like many machine learning problems, has two major elements: trying to understand the patterns in the customer?€?s historical activities, and generalizing those patterns to future behavior. 
,
The first part is where ?€?Big Data?€? processing, aggregation, and analysis are involved. By logging what content customers play, browse, and search for, we construct a profile of their interests, and perform exploratory analyses to examine which signals are more predictive than others for engagement (e.g., choosing to stream a program). We use promising signals from this analysis to prototype models offline and compare their performance against our current systems in an effort to identify potential hypotheses for improvement.
,
The second step involves experimentation: taking these hypotheses and testing if they successfully generalize in a head-to-head comparison with our existing algorithms by randomly assigning a subset of users to receive these alternative recommendations. Once we?€?ve evaluated whether a statistically significant difference exists between the performance of the old and new models (after controlling for potential confounding variables and looking for multiple points of evidence to support our conclusions), the cycle begins anew in a continuous process of innovation.
,
,
,	
,While the diversity of platforms affords our customers great flexibility in how they enjoy our product, it presents special challenges for our personalization efforts. For example, the website features an ?€?instant search?€? feature that generates results in real-time as users type queries, while Smart TV and mobile devices require you to explicitly input a search term; our algorithms need to accommodate both scenarios. 
,
Similarly, our predictive models and the UIs that utilize them need to work on a mobile phone that can display only a few titles in a screen, as well as the large-screen TV in your living room. On the back end, this involves a lot of careful work to design flexible data schema in our warehouse that can accommodate inputs from multiple client platforms without obscuring important differences between them in user behavior.
,
,
,	
,A major benefit of our system logging is that our investment in cloud infrastructure makes it possible to scalably incorporate new devices, platforms and geographic regions as we continue our global expansion. Because this data is usually supplied in the form of Javascript Object Notation (JSON), it is flexible for recording many kinds of data, and for changing what we are logging in an agile way. 
,
This flexibility can also be a challenge, as the first step in most of our algorithmic data preparation is taking this unstructured data and transforming it into a more conventional row-column dataframe used for predictive modeling, which with nested data structures can require a lot of custom logic during Extract, Transform, and Load (ETL) jobs.
,
,
,
,I think we?€?re frequently surprised by the differences between data derived from user feedback (such as ratings or taste preference selections) and what interests are revealed by their actual behavior on the service. Many of our users claim to enjoy foreign films and documentaries; practical experience tells us that, come movie night, they might watch Family Guy instead. 
,
,
,
,
,  "

"
,
,
,

,

,

,

,
,

,

,

,

,

,

,

,
,


,

,

,




,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.




,
,
,

,
,

,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 16 and later.
,
See full schedule at , .
,
,  "
"
,
,
, is currently a Senior Data Scientist working on Discovery & Personalization algorithms and data processing at ,. ,Before Netflix, he studied computational biology at The Johns Hopkins University School of Medicine, where his PhD research in the Department of Neuroscience employed machine learning models to predict adverse side-effects of drugs. ,He also previously worked at Chicago-based Accretive Health as a Data Scientist, focusing on data related to patient billing and referrals.
,
,
,
Here is second part of my interview with him:
,
,
,	
,: In a sense content acquisition is both a science and an art. While it is difficult to exactly pin down what makes a show a hit, we make extensive use of analytics in examining what our users are watching, and thus where we might want to prioritize our licensing in the future. Examples include building predictive models for popularity of upcoming titles, using expert tagging to assign metadata labels to content, and qualitative interview sessions conducted by our Consumer Sciences branch to link these empirical findings with feedback from our customers. 
,
We also extensively track the on-site performance of our Original programming in conjunction with our marketing efforts to constantly refine our delivery of Netflix-produced content. 
,
,
,	
,Our Big Data infrastructure is primarily built on top of Amazon Web Services (AWS), specifically the Simple Storage Service (S3) file system that backs our Hadoop cluster. Production ETL jobs are largely written in Pig, with Hive used for adhoc analyses and Presto for interactive analytics. We also maintain a Teradata cloud instance, which is the backend for many of our reporting tools such as Microstrategy and Tableau. 
,
Hadoop job submission is managed through a service developed in-house called Genie, which abstracts the details of cluster configuration when users submit jobs (it resembles submission of a POST command to a web server). Along with Genie, you can find many of our other Big Data tools on Github, such as Inviso (a central dashboard to visualize cluster load and debug job performance), S3mper (an S3 consistency monitor), Lipstick (a visual interface for Pig which we utilize extensively for sharing and analyzing data about these jobs), PigPen (a Clojure wrapper for Pig), and Surus (a collection of User Defined Functions (UDFs) developed for data science tasks such as model scoring and anomaly detection). 
,
,
,
,I think the pros of this approach are the ability to develop deep expertise in particular data types, whether it be external marketing feeds, server side impression data, search clicks, or playback logs. Each has its own nuances, and understanding these often becomes important in developing signals in our algorithms. 
,
As with any sort of specialization, the potential downside is lack of exposure to alternative areas that might provide unexpected inspiration, but I think our regular in-company seminars, all-hands meetings, and cross-team initiatives are effective means of socializing such learning across verticals.
,
,
,
,The motivation is one of scale and self-enablement through automation. In the case of Genie, abstracting the details of job submission from a particular cluster configuration allows us to flexibly scale the resources brokering job submissions behind the scenes as demand for computational resources changes, and also helps tie together the increasingly heterogeneous tasks we are running (Hive, Pig, Spark, Presto, etc.) with a common architecture. 
,
In the case of Lipstick and Quinto, these tools make auditing and debugging ETL jobs much more ?€?self-service?€?, whereas in the past we might have to dig through Pig log files or write adhoc data quality checks. Automated and user-friendly interfaces like these tools increase our productivity to tackle ever-harder problems in processing and understanding our data.
,
,
,
,
,  "
"
Most popular 
, tweets for Jun 09-15 were
,
Which #BigData, #DataMining and #DataScience Tools go together? 
, 
,
,
,
Good Comparison of ML classifiers: Decision Trees, Regression, SVM,  #NeuralNets #DeepLearning 
, 
,
,
,
In #machinelearning, what is better more #data or better algorithms? Need both , 
, 
,
,
,
Good Comparison of ML classifiers: Decision Trees, Regression, SVM,  #NeuralNets #DeepLearning 
, 
,
,
,  "
"
Introducing New Titles on Machine Learning and Data Science
,
From Chapman & Hall/CRC Press
,
,
,
Trevor Hastie, Robert Tibshirani, Martin Wainwright
,
A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data...
,
,.
,
,
,
,
,
,
Masashi Sugiyama
,
Supplying an up-to-date and accessible introduction to the field, Statistical Reinforcement Learning: Modern Machine Learning Approaches presents fundamental concepts and practical algorithms of statistical reinforcement learning from the modern machine learning viewpoint. It covers various types of RL approaches, including model-based and model-free approaches, policy iteration, and policy search methods.
,
,
,
,
,
,
,
,
Chandan K. Reddy, Charu C. Aggarwal
,
At the intersection of computer science and healthcare, data analytics has emerged as a promising tool for solving problems across many healthcare-related disciplines. Supplying a comprehensive overview of recent healthcare analytics research, Healthcare Data Analytics provides a clear understanding of the analytical techniques currently available to solve healthcare problems.
,
,
,
,
off your purchase today!  "
"
,
By Gregory Piatetsky,  
,.
,
I received email from 
,,
an interesting site which provides rankings for online courses in over 100 subjects, from Accounting and Aerospace Engineering to Women's Studies and Writing, with options for learner skill and how much you are willing to pay.
,
They also have have a recent post
,, 
which ranks MA or MBA in Data Science using following 3 criteria
,
,
,??,
Those criteria seem very fuzzy and they do not explain how much a school gets in each dimension. Further, there is a problem with basic arithmetic, since 3x0.3=0.9, so where is the remaining 0.1 of the ranking?
,
However, apart from questionable ordering (Deakin University and La Salle as top 2 choices - really ??), this list provides somewhat useful information.  You still have to search for the direct page for the program, since they only provide the university homepage.
,
See top entries below. See also KDnuggets pages for 
,
,??,
Here are the top 7 entries according to OnlineCourseReport.  Note - they also include programs in Health informatics and Business Intelligence
,
1. ,, Australia
,Offered fully online through the Faculty of Law and Business, students can earn a Master of Business Analytics in just 1.5 years, attending online classes as a full-time or part-time student. The degree is composed of eight core classes and a business project elective. To be considered, applicants must have a bachelor's degree or three years of work experience in business. Overall tuition cost is $24,150.
,
2. ,, USA
,For those wanting a Masters in Analytics, the College of Professional & Continuing Studies at La Salle University allows students to earn a degree with just 10 courses, including data mining, data visualization and modeling and simulation of data analytics. Tuition is $850 per credit.
,
3. ,, USA
,The School of Professional Studies at Northwestern University currently offers a Master of Science in Information Systems with a Concentration in Business and Analytics. This online program is composed of 10 courses and offers seven areas of specialization, including database and Internet technologies, information system security, project management, information systems management, medical informatics, information systems, and analytics and business intelligence. Students also have the unique opportunity to utilize IBM resources during the tenure as graduate student. Overall cost for the degree is $49,962.
,
4. ,, USA
,The School of Management at Union Graduate College offers a Master of Science in Healthcare Data Analytics. Students can complete the degree in just a year. The coursework is composed of 12 courses, covering healthcare operations research, data analytics and business intelligence.  Tuition is $2,995 per course.
,
5. ,, USA
,The School of Library and Information Management at Emporia State College offers a Master of Science in Informations. Students can earn their degree over the course of two years taking as little as two courses per semester. The program is made up of 36 sequenced credits. Tuition is $227 per credit hour.
,
6. ,, USA
,For those wanting a Masters of Science in Analytics, Dakota State University offer a robust degree, with three different tracks to choose from, including general studies, information systems, or healthcare analytics. The degree is made up of 24 credit hours. Tuition is $431.25 per credit hour.
,
7. ,, USA
,The Kanbar College of Design, Engineering, and Commerce offers a Masters of Science in Modeling, Simulation and Data Analytics. Over the course of 30 credits, students will focus on systems modeling and simulation, as well as data visualization and decision support. Graduate students must present a capstone project as their final course. Tuition is $1,050 per credit.
,
Other schools mentioned are
,
,
Which schools/options would you add and how would you rank them?

,
,
 ,  "

"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is currently a Senior Data Scientist working on Discovery & Personalization algorithms and data processing at ,. ,Before Netflix, he studied computational biology at The Johns Hopkins University School of Medicine, where his PhD research in the Department of Neuroscience employed machine learning models to predict adverse side-effects of drugs. ,He also previously worked at Chicago-based Accretive Health as a Data Scientist, focusing on data related to patient billing and referrals.
,
,
,
,
,
Here is third part of my interview with him:
,
,
,	
,: To me it?€?s a combination of statistical measurement and business acumen. In some sense, ?€?good enough?€? is always whatever data we might need to sufficiently understand user preference such that we could improve engagement with our service. On a technical level, we often have to temper the desire for perfect information with the practical effect noise might have on a model?€?s accuracy, and whether a corner case is frequent enough to change an algorithm?€?s behavior. 
,
,
,	
,Indeed, I think this is where model generalization comes into play: we want to personalize to not just what you already enjoy, but what you are likely to enjoy but don?€?t know about yet. It?€?s something we keep in mind with most evaluation metrics for our algorithms, and in the research phase of designing new models. 
,
,
,
,My dissertation advisor in graduate school once explained a business trip as less about getting a particular task accomplished and more about building relationships; if you only visit people when you need something from them, it cheapens the interaction. I?€?ve found this to be broadly applicable, that assistance for coworkers or professional colleagues is best to ?€?pay-forward?€?. It increases the likelihood of receiving genuine mutual help in the future, and I think just makes a more pleasant work environment in general.
,
,
,	
,I am fascinated by the growth of Massive Open Online Courses (MOOCs) as a method of disseminating knowledge about statistics, machine learning, and technology to an increasingly wider audience. This is in some ways paralleled by the push for open-access journals in academia, and the combination of easily accessible educational materials and tools, dynamic documentation of work through Github, and discourse through online forums.
,
I imagine that this will stimulate a growing body of informal research on all kinds of data, be it consumer web logs or scientific experiments. Also, while we may frequently hear that there is a shortage of Big Data talent in the marketplace, I think the growth of such resources may lessen this gap by making it easier than ever for anyone with Internet access to discover and cultivate a passion for analytics.
,
,
,	
,Curiosity and courage: more than any tool or methodology, the ability to self-educate and critically question is a key success factor, especially in a rapidly moving organization like ours. Likewise, having a voice and being unafraid to back up (with evidence) potentially unpopular opinions is an important aspect of how we make data-driven decisions.
,
,
,	
,Margaret Atwood?€?s ,, the conclusion of an innovative dystopian sci-fi trilogy whose other volumes (Oryx & Crake, Year of the Flood) I also recommend. Atwood always expertly blends linguistic precision with social commentary and a dark sense of humor, and this piece was no exception.
,	
Outside work, I have a long-standing interest in men?€?s fashion that often finds me browsing the local department stores on weekends. I also am a dedicated reader of current events and arts magazines (The New Yorker, The Atlantic, Vanity Fair), and am also enjoying exploring San Francisco?€?s varied restaurant scene with my wife. Also, watching Netflix!
,
,
,  "
"
,
,

, is currently one of the hottest technologies in data science space. ,, a top quality conference focused on Apache Spark being held in San Francisco is bringing together the Apache Spark Community from various parts of the world. The three-day event (July 15-17, 2015) is still on.

,

Leading production users of Spark, SparkSQL, Spark Streaming and other relevant technologies are discussing project development and use of Spark Stack in variety of verticals and applications.?? On day1 and 2, there were 2 keynote sessions followed by three tracks that ran in parallel: Developer, Data Science and Applications.?? On day 3, Databricks is hosting three different parallel day-long Spark training sessions: Introductory Apache Spark Training, Advanced DevOps Spark Training and Data Science with Spark.

,

Here are highlights from keynotes on day 1:

,

, opened the summit offering some impressive statistics on the current state of Apache Spark Project. Some important points he mentioned:

,
,
,
,??,
, recapped features of Apache Spark 1.4. Apache Spark 1.5 would have following more features: Project Tungsten, expansion of SparkR to include ML APIs and more streaming features.

,

, announced that Databricks is now generally available (GA). New features available in Databricks deployments are:
,
,

, gave a quick demonstration of Databricks cloud describing its features.

,

, shared few good use cases and success stories of some companies.

,

, made following big announcements:
,
,

,asserted that Spark and Hadoop work perfect together. Hortonworks has enhanced Spark to be enterprise ready by enabling Spark on YARN and applying enterprise governance, security and operations services for Spark applications. They have integrated Spark as part of HDP 2.2 release. He also talked about Apache Zeppelin - a web-based notebook that enables interactive data analytics. One can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more.

,

,emphasized that companies should not hire data scientists and put them to work as ?€?SQL monkeys?€?. Instead the data scientists should implement self-service BI for business users.

,

,talked about complex challenging data problems and how they?€?re using Spark to solve them.

,

,delivered a keynote on ?€?Software Above the Level of a Single Device: The Implications?€?. He emphasized that we all should work on things that matter.

,

,

,
,
,  "
"
,
,
,
,
This is what recognized technology authority Phil Simon studies. He helps organizations navigate an increasingly complex business world by consulting on strategy, management, platforms, communication, big data and technology. Simon is the award-winning author of seven management books, including his most recent, ,. His book , covers how companies can use a new generation of tools and visualization technology, as well as foster a mindset of data exploration and discovery to gain a competitive edge.
,
,
,
,
,
,
,??,
On-demand webcasts featuring additional thought leaders are also available.
,
,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
There has been a lot of recent interest from scientific journals and from other folks in creating checklists for data science and data analysis.??The idea is that the checklist will help prevent results that won't reproduce or replicate from the literature.??One analogy that I'm frequently hearing is the analogy with checklists for surgeons that??,.
,
,The one major difference between checklists for surgeons and checklists I'm seeing for research purposes is the ,. 
,
You would never let me do surgery on you. I have no medical training at all. But I'm frequently asked to review papers that include complicated and technical data analyses, but have no trained data analysts or statisticians. The most common approach is that a postdoc or graduate student in the group is assigned to do the analysis, even if they don't have much formal training. Whenever this happens red flags are up all over the place. Just like I wouldn't trust someone without years of training and a medical license to do surgery on me, I wouldn't let someone without years of training and credentials in data analysis??make major conclusions from??complex data analysis.
,
You might argue that the consequences for surgery and for complex data analysis are on completely different scales. I'd agree with you, but not in the direction that you might think.??I would argue that high pressure and complex data analysis can have much larger consequences than surgery. In surgery there is usually only one person that can be hurt. But if you do a bad data analysis, say claiming say that??,,??that can have massive consequences for hundreds or even thousands of people. So complex data analysis, especially for important results, should be treated with at least as much care as surgery.
,
,
The reason why I don't think checklists alone will solve the problem is that they are likely to be used by people without formal training. One obvious (and recent) example that I think makes this really clear is the??,??data we are about to start seeing. A ton of people signed up for studies on their iPhones and it has been all over the news. The checklist will (almost certainly) say to have a big sample size. HealthKit studies??will certainly pass the checklist, but they are going to get??,??big time if they aren't careful about biased sampling.
,
If??I walked into an operating room and said I'm going to start dabbling in surgery I would be immediately thrown out. But people do that with statistics and data analysis all the time. What they really need is to require careful training and expertise in data analysis on each paper that analyzes data. Until we treat it as a first class component of the scientific process we'll continue to see retractions, falsifications, and irreproducible results flourish.
,
, is an assistant professor in the , of the Johns Hopkins Bloomberg School of Public Health. His work focuses on statistical methods for high-dimensional data and genomics.
,
,. Reposted with permission.
,
,
,
,
,
 ,  "
"
,, 
,Sept 29-Oct 1, 2015
,New York, NY
,
Strata + Hadoop World in New York is where big data's most influential
business decision makers, strategists, architects, developers, and
analysts gather to shape the future of their businesses and
technologies. If you want to tap into the opportunity that big data
presents, you want to be there.
,
,??,
,
,
,
Over 5,500 people gathered at the sold-out Strata + Hadoop World in NYC last October. 
,, and has become the 
, - changing how business, government, and even society make better decisions.
,
,.
,
,KDnuggets subscribers save 20% off passes with code ,.
,  "
"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "

"
,
, is revolutionizing cybersecurity with their major new release to their cyber analytics platform. Stay ahead of threats by actively recalibrating your security posture. 
,
,.  "
"
Sydney, 17 June 2015. How do you cope when you've got 20 petabytes of data coming in every year, what can sentiment analytics teach you about your customers, and how will machine learning and the burgeoning Internet of Things (IoT) impact the analytics industry?
,
These burning questions and more will be answered at Advancing Analytics 2015, the third annual conference organised by The Institute of Analytics Professionals of Australia (IAPA). The event takes place Tuesday, 4 August 2015 at the Sydney Hilton Hotel from 9.00am-5.15pm. 
,
,
,
,
,
The seven global analytics leaders who will present at this year's event:
,
,??,
,
,
The day also features a panel discussion on how to build the ultimate analytics team. Dr Leif Eversen, General Manager, Business Performance Analytics, Westpac; Julie Batch, Chief Analytics Officer, IAG and Liz Moore, Head of Research, Insight and Analytics, Telstra, will share their views on how to build and retain high quality talent for an analytics team and what skills and know-how are in greatest demand.
,
To learn more or to purchase tickets to the conference, 
visit ,
and join the conversation on Twitter 
,
,
,
,The Institute of Analytics Professionals of Australia (IAPA) is the professional organisation for the analytics industry in Australia, incorporating business analytics and data mining across multiple disciplines and sectors. Its mission is to unite, inform, support and promote analytics professionals in Australia. The 3,500 plus IAPA members represent a diverse range of industry verticals and brands including financial services, government, utilities, retail/FMCG and telecommunications as well as leading analytical consultants and software providers. The IAPA community holds regular meet ups in six cities. IAPA entered into a formal alliance with the Association for Data-driven Marketing and Advertising in 2014.
,
Media contact:
,Kim Carter/ADMA PR
,(02) 9277 5412 (Mon-Wed)
,0407 771 698 (Thurs)
,  "
"
,
,

, is currently one of the hottest technologies in data science space. ,, a top quality conference focused on Apache Spark being held in San Francisco brought together the Apache Spark Community from various parts of the world. The conference was three-day long (July 15-17, 2015).
,
Leading production users of Spark, SparkSQL, Spark Streaming and other relevant technologies discussed project development and use of Spark Stack in variety of verticals and applications.?? On day1 and 2, there were 2 keynote sessions followed by three tracks that ran in parallel: Developer, Data Science and Applications.?? On day 3, Databricks hosted three different parallel day-long Spark training sessions: Introductory Apache Spark Training, Advanced DevOps Spark Training and Data Science with Spark.
,
,
,
Here are highlights from keynotes on day 2:

,
,explained why they?€?re focusing on compute instead of just IO. He talked about Project Tungsten, which is aiming to make Spark faster. ??He presented roadmap of Project Tungsten. Apache Spark 1.4 configuration has already enabled experimental Project Tungsten. The key concept here is to spend less time creating objects and collecting garbage. Xin presented a review of key hardware trends and project?€?s impressive performance gains to date. Key quote: ,
,
,started the talk with mentioning the large number of attendees and taking a selfie along with attendees in background. He showcased amazing success of Spark in a financial company and a consumer company. He explained how perfectly and where exactly Spark fits in the Hadoop ecosystem. Spark will be dominant general-purpose processing engine in Hadoop and it extends the Hadoop ecosystem with new analytic and processing capabilities. He mentioned ?€?Apache Spark would be the processing engine for Big Data?€?.
,

,described the big data journey of Toyota through Customer 360 Project, which is currently in production.?? Toyota now leverages Spark since it combines compute, streaming and machine learning in a single framework. Recoding a batch job on Spark they reduced the runtime from 160 hours to just 4 hours. Toyota uses social media monitoring, analysis of tweets and having an interesting social ML pipeline in place.?? Spark is enabling kaizen because of its speed.

,

, mentioned that the 3Vs are no longer constraints but productivity is still a challenge in Big Data processes. He showcased few customers such as Washington Post, Gumgum, etc. using Spark in production with Amazon EMR. He also announced availability of a new Spark on EMR service from AWS.
,
,delivered an impressive talk giving an overview of CIA?€?s key IT requirements and approach to satisfy them. He mentioned that too much red tape slows down innovation in typical large organizations. Wolfe said one shouldn't build a product without understanding the market for it. CIA recently adopted C2S cloud (AWS) and it is a major change to the way they did business. C2S provides them a compute fabric on demand. CIA challenge requires a marketplace of continuous ideas and innovation derived from all sources.

,

,at first introduced Baidu as China?€?s leading search engine, with about 73% of search market share. Briefly giving a review of Baidu?€?s Big Data Infrastructure he mentioned that Baidu hadoop cluster currently has more than 13,000 nodes. Baidu started working with Spark 0.8, then developed a 200 node cluster in Spark 1.0. Baidu has put SparkSQL 1.2 in production and is now running more than 1,000 nodes. Baidu?€?s Interactive Query Engine runs over Spark/Tachyon reducing query run time drastically to 4 seconds as compared to 30 seconds with Hive. When compared to MapReduce they observed more than 50X performance improvement.

,

,announced performance portal for Spark and briefly talked about Spark?€?s speedup record for a number of companies. In addition, he announced the release of Streaming SQL for Apache Spark, part of a project to develop a complete open source framework for streaming analytics and making these capabilities pervasive.

,

,
,  "

"
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,??,
Please become a 
,. Membership is a great way to stay connected and contribute back. Encourage students too, and help support their participation at KDD. Student travel awards are available. At $25 ($15 for students), just the discount (over $100 USD) on this year's KDD conference is worth it!The conference is yet to start but the conversations are ready to go on 
, , 
, or  ,.
,
The 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining will be held in Sydney, Australia on August 10-13, 2015.  This is the first time for SIGKDD to be held in the southern hemisphere.
,
,KDD-2015 features an exciting program of , by leading authorities: Susan Athey, Hugh Durrant-Whyte, Ronny Kohavy, and Daphne Koller.
, from recognized thought-leaders.
,
,
,
The conference will feature 160 Research Track papers and 68 Industrial and Government Track papers. In addition to oral presentations, all papers will be showcased during evening poster sessions. Full details available at: 
,
,
,
,
14 exciting workshops featured at KDD-2015 are:
,
,
,??,
,
,
,
12 in-depth tutorial sessions to be conducted at KDD-2015 are:
,
,
,??,
,
KDD 2015 Early Bird Registration is now open. 
,
,
,
We look forward to hosting you at KDD 2015 in Sydney, Australia!
,
Ankur Teredesai
,Information Director
,  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
How do we make sure that data mining projects are successful??? 
KDnuggets readers are invited to participate in a research project that investigates
if top management support is a contributing factor in making data mining projects
successful.?? We already know that top management support is a critical success
factor when it comes to other enterprise-wide information systems projects, but
we don't yet know if there's a relationship between this support and data mining
projects in particular.

,

While there are many possible factors that can contribute to a project's
success, this study focuses on specific behaviors that top managers may exhibit
in supporting data mining projects.

,??,

Your participation is completely voluntary, and your candid responses will be of
great assistance. All responses and data from the survey are kept anonymous.
The survey takes approximately 5-10 minutes.

,??,

If you have questions about the survey, contact Rich.Huebner@yahoo.com
,
,.  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
By Gregory Piatetsky,  
,.
,
While Data Science platforms are getting easier to use, data scientists frequently need to go beyond them and actually code in a lower-level language for their tasks.
,
,
Recent , showed that R and Python are the top 2 such analytics languages, and while R is more popular, Python grows faster. 
,
R and Python are also very popular topics among KDnuggets readers.
,
Our previous poll examined 
,, and we also found that R and Python are the most popular among
,.
,
New KDnuggets Poll looks the current state of R vs Python fight.
,
I apologize in advance to users of many other languages, but R and Python are the most popular two, and the flow diagram becomes too complicated if another language is added.
,
,  "
"
,

Today thousands of scientists and engineers are applying machine learning to an extraordinarily broad range of domains. Equip yourself with machine learning skills in an all new way by reading this ,, ,.
,
 
,
,
,

This book is rather unusual for a machine learning text book in that we do not review dozens of different algorithms. Instead we introduce all of the key ideas through a series of case studies involving real-world applications. ,Case studies play a central role because it is only in the context of applications that it makes sense to discuss modelling assumptions. Each chapter therefore introduces one case study which is drawn from a real-world application that has been solved using a model-based approach. ,

,
,
, ,
Model-based machine learning can be applied to pretty much any problem, and its general-purpose approach means you don?€?t need to learn a huge number of machine learning algorithms and techniques.
,
,
,
, ,

Let?€?s look more closely at the relationship between models and algorithms., We can think of a standard machine learning algorithm as a monolithic box which takes in data and produces results. The algorithm must necessarily make assumptions since it is these assumptions that distinguish a particular algorithm from the thousands of others out there. However, in an algorithm those assumptions are implicit and opaque.
,
Now consider the model-based view. The model comprises the set of assumptions we are making about the problem domain. To get from the model to a set of predictions we need to take the data and compute those variables whose values we wish to know. This computational process we shall call inference. There are several techniques available for doing inference which will be discussed in the book. 
,
, ,
All of the models in this book were created using Infer.NET (a software framework at Microsoft Research), and the corresponding model source code is available online.
,
Get the ebook at ,

,
,
,
,



  "

"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 23 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,   "
"
,
,
, is the SVP of Product Management at ,. Prior to MapR, Anil was the EVP of Product Management at Silver Spring Networks, responsible for product strategy, planning and marketing of networking and software products focused on the Smart Grid for the energy industry. 
,
Before that, Anil was with Sun Microsystems, a Fortune 200 technology leader, serving as EVP of The Application Platform Software organization and had previously been the Chief Marketing Officer leading global branding, demand creation and an extensive developer ecosystem program.
,
He has a BSEE from Stanford University, and an MM degree from the Kellogg School at Northwestern University.
,
Here is my interview with him:
,
,: Q1. Analytics impacting the business ""as-it-happens"" definitely sounds tempting, but only a few companies have been able to do it successfully. What factors separate winners from losers in the quest for a real-time data-to-action cycle?
,
,: First of all, this is a continuum and not a specific end state. What that means is that everyone will move towards a more ?€?as-it-happens?€? or real-time process in their companies. The technology will certainly enable it, but the much bigger issue is the related business processes and decision making that has to go faster now that there is more insight in real time. Finally, different businesses have very different needs for what ?€?as-it-happens?€? means. What might be very slow for some might be just fine in other businesses.
,
,
,
,There is no one single answer to this issue because it is very dependent on the processes inside a business for acquiring, ingesting, processing, and using the data. It is also possible to significantly speed up the data-to-action cycle without needing to have a single integrated cluster. Having said that, we are increasingly seeing customers who are pushing the envelope, planning to do both analytics and operational work in the same cluster because of the time savings resulting from not having to copy and move data around. The data is just ?€?there?€? for both analytics and operations. The milestones for such an initiative should be based and driven by the business needs and desired outcomes, and not by the technology.
,
,
,
,Actually, there is less major architectural change than people may be worried about. This is one reason why our customers are able to get value quickly and not be stuck in long-term projects.  Making sure the transition goes smoothly comes down to the well-known basics of project planning: clear scope, enough detail in the tasks, and of course, a great project manager!
,
,
 ,
,MapR is to Hadoop what Teradata is to data warehousing:  best in class in terms of performance, reliability, and production success.  Therefore, we share a lot of the same customers with Teradata across major industries who wanted to have better integration from their Hadoop and data warehousing environments.  Teradata calls this the ?€?Unified Data Architecture?€? and technologies like QueryGrid give analysts fast SQL access to any data ?€? whether it is stored in Hadoop, Teradata Aster, or the Teradata data warehouse ?€? on the fly, at run-time, so it?€?s really fast. 
, 
In a way, the MapR integration with Teradata using QueryGrid enables multi-tiered ?€?Lambda Architecture?€? in terms of supporting streaming, interactive, and batch within a hybrid and well-orchestrated environment which reduces data deduplication and optimizes performance for various workloads. For example, we share a joint customer in financial services who supports a real-time contextual call center application. Real-time information from the website streams through Apache Storm for transformation within the MapR Distribution including Hadoop, and then it?€?s integrated with customer information (e.g., most recent transactions, customer lifetime value, propensity to churn scores) coming from Teradata and served up in a customer 360 application for the call service agent.
,
,
,
, is a Software Development Intern at Salesforce. He is a data science researcher with extensive interest in innovative applications of analytics, machine learning, and information retrieval. He is former MDP Fellow and graduate mentor at UCI-Calit2. His novel analytics solution for online education was the runner-up at UCLA Developers' Contest 2014. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "

"
        ,
,
,
YouTube contains a great many videos on the topic of Machine Learning, but it can be hard to figure out what's worth watching with your limited time. Here the top videos by views were sorted through to provide some of the greatest video content on the subject. 
,
If you want an easy way to get all of these videos in one place, take a look at this ,. View numbers as of June 21, 2015. 
,
, (1,514,045 views)
,
This fun demo video shows how machine learning can be applied to video games. This is achieved using neural networks and genetic algorithms. What's really great about this video is how the author concisely describes how the neural network acts to achieve the given performance. Having played Super Mario World myself, I can say the gameplay is extremely impressive, learned or not!
,
,
,
, (761,843 views)
,
This is the first video in the great series of Stanford machine learning lectures given by Andrew Ng. This would make a good starting point for self-learning the essentials of machine learning. If the material in this video appeals to you, his , may also appeal to you.
,
, (401,740 views)
,
This Google Tech Talk by Geoffrey Hinton covers the next generation of neural networks. This acts as a good introduction to deep learning. It's a few years old, but still acts as a great technical introduction to the topic.
,
, (233,875 views)
,
This video also comes from Andrew Ng. In this video, instead of focusing on machine learning in particular, Andrew Ng dives into applications of AI for robotics. If you're interested in these topics, give this video a watch.
,
, (233,703 views)
,
This video is also the start to a lecture series on machine learning. With this series, it is given by Professor Yaser Abu-Mostafa of Caltech. This series also has an ,, which also has a great companion textbook. This is another great way to learn the foundations of machine learning.
,
, (104,808 views)
,
This Google Tech Talk is also delivered by Geoffrey Hinton. This talk also goes in depth on the topic of neural networks. Where this talk differs is how it discusses the way this reacts with biological neurons and how that is represented in the algorithms.
,
, (103,166 views)
,
This demo video shows a system that uses machine learning to detect NHL goals based on the audio feed of the game. When a goal is detected, the system communicates with the hardware in the living room to create a light show. This is a great integration of learning into living spaces.
,
, (92,820 views)
,
This video demonstrates machine learning applications in video games as well. In this case, the applications are some what more ""malicious"". By observing many previous games of Hearthstone, an electronic trading card game, the system developed by the author learns to predict the hands of opponents. This is fascinating for anyone interested in competitive applications of machine learning.
,
, (89,518 views)
,
This Google Tech Talk, delivered by Andrew Ng, covers the use of feature learning to automatically learn features from unlabeled data. This is great way to save time over the conventional methods of feature selection. If you're interested in the theoretical basis of this method, this video provides a wonderful explanation.
,
, (89,506 views)
,
This panel, hosted by the Stanford Graduate School of Business, has many industry professionals discussing the impact of deep learning. This is interesting because it gives many viewpoints and different ways of understanding the same concept. This is a good video to watch to get a diversity of opinions.
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is the SVP of Product Management at ,. Prior to MapR, Anil was the EVP of Product Management at Silver Spring Networks, responsible for product strategy, planning and marketing of networking and software products focused on the Smart Grid for the energy industry. 
,
Before that, Anil was with Sun Microsystems, a Fortune 200 technology leader, serving as EVP of The Application Platform Software organization and had previously been the Chief Marketing Officer leading global branding, demand creation and an extensive developer ecosystem program.
,
He has a BSEE from Stanford University, and an MM degree from the Kellogg School at Northwestern University.
,
,
,
Here is second part of my interview with him:
,
,: Q5. When and how was the idea conceived for Project Myriad? What is the current status? Future plans?
,
,: This is an exciting story!  One of our engineers, Santosh Marella, went to a hackathon event in Silicon Valley. The event was focused on trying to bridge the world of Apache Mesos and YARN/Hadoop.  He came up with a very clever answer, which was plugin-based, rather than requiring a major change to the core code.  Suddenly the two worlds of Mesos and Hadoop could be bridged and he won the competition.  That led to creating the Apache open source Project Myriad with the support of community members. 
,
,
,
,: We see our customers paying appropriate attention to security. One reason for this is that we have customers in financial services, telecom, and healthcare, among many other industries, who are used to living in regulated environments with high compliance requirements. There are two aspects that customers should look at. One is the core platform's security, and the second is the broader view of data governance.  
,
,
,
,: The Quick Start Solutions were designed to speed the time to value for a customer and radically lower the risk of doing a big data project. We took our experience from hundreds of customers, found the most common use cases, and focused on making it an easy and affordable decision for a customer.  The three initial solutions are data warehouse optimization and analytics, improving a website's recommendation engine, and security log analytics.  Nearly every customer has at least one of these needs as they start on big data projects, and we have made it really easy to get going with little risk.
,
,
,
,: It is important to make sure that there is buy-in and sponsorship from the line of business for any big data project in addition to the IT group who has to make it happen.  Secondly, they should pay attention to the people and process side just as much as to the use cases selected, because the real value will come from people using the insights.  
,
As for questions to ask Hadoop vendors, we think customers need to make sure that the platform is ready for their future needs for reliability, datacenter-grade security, and scalability.  Once an organization builds confidence in the value of big data, the use cases multiply and the need for mission-critical reliability becomes very important.  Customers need to make sure they are ready for this eventuality.
,
,
,
,: While all three players offer essentially similar community software tools, we believe MapR offers more comprehensive support of the Spark environment.  Once you get to the foundation however, the difference is that MapR uniquely provides a number of innovations that are highly differentiated both on reliability and real-time capability.  MapR has won three ""top ranked"" analyst awards while the competitors have none (Top-Ranked Hadoop Distribution from Forrester, Top-Ranked NoSQL database from Forrester, Top SQL-on-Hadoop solution from Gigaom).  MapR is widely seen as the leading Hadoop provider in high-scale production environments that demand mission-critical reliability.  
,
,
,
,: The problem is that the data itself is changing. That means that the Data Scientist must figure out how to keep up with that change. This is one reason we are a champion of the Apache Drill project, which brings self-service data exploration to Hadoop, and discovers the data's schema on-the-fly.  It is like taking an x-ray of your data to find out what the structure is. This is a radical advance in the ability to explore big data to figure out which use cases might be most valuable.
,
,
,
,: Like what you do because you will wind up doing a better job on whatever you work on!
,
,
,
,: With the data changing as rapidly as it is, we tend to prefer high agility people. What matters is how quickly they can learn new things. 
,
, is a software development intern at Salesforce. He is former MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,

The , features 6778 active packages. Which of these should you know? Here is an analysis. See also link to the raw data at the bottom of the post.,

,

Most of these R packages are favorites of Kagglers, endorsed by many authors, rated based on one package's dependency on other packages. They are also rated & reviewed by users as a crowdsourced solution by ,. However, these user ratings are too few to be based on for analysis.
,
Let us explore how many machine learning packages are being downloaded from Jan to May by analysing CRAN daily downloads.
,
,

It is interesting to note that some open source R tools are gaining popularity such as ,, a GUI for data mining using R (35539 downloads), and ,, fast hierarchical clustering routines for R and Python (14214 downloads). 
,
Did we miss your favorites? Light up this space and contribute to the community by letting us know which R packages you use!!
,
For completeness, here is ,.
,
,
,

,
,

,
,
 ,

 
   "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,

The , features 6778 active packages. Which of these should you know? Here is an analysis of the daily download logs of the CRAN mirror from Jan-May 2015.  See a link to full data at the bottom of the post.,

,

Most of these R packages are favorites of Kagglers, endorsed by many authors, rated based on one package's dependency on other packages, some of them gained mentions on Quora and on various R blogs. 
,
They are also rated & reviewed by users as a crowdsourced solution by ,.
We also present the CRANtastic rating of few packages here only to represent that it is gaining popularity. However, some of these packages have user ratings that are too few to be based on for analysis and hence, for these we omitted the ratings.
,
Let us explore the list based on the number of downloads! ,
,

All the top 20 (Jan-May 2015) above are covered in ,. ,
For completeness, here is ,.

 ,
Did we miss your favorites? Light up this space and contribute to the R community by letting us know which R packages you use!!


,
,
,


,
,

,
,
 ,

 
   "
"
,(edited for  KDnuggets by Bhavya Geethika).
 ,
,
,
Model ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article the author shares his ensembling approaches for Kaggle Competitions. 

,

We explore Model Ensembling in three parts : ,
,

,
,
The most basic and convenient way to ensemble is by ensembling Kaggle submission CSV files. You only need the predictions on the test set for these methods ?€? no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up.
,

,

Consider a simple majority vote ensemble. Let?€?s see why model ensembling reduces error rate and why it works better to ensemble low-correlated model predictions.
,
,

,
,
,

,
,
Suppose we have a test set of 10 samples. 
,
The ground truth is all positive (?€?1?€?): 1111111111 
,
We furthermore have 3 binary classifiers (A,B,C) with a 70% accuracy. 
,You can view these classifiers for now as pseudo-random number generators which output a ?€?1?€? 70% of the time and a ?€?0?€? 30% of the time.
,
We will now show how these pseudo-classifiers are able to obtain 78% accuracy through a voting ensemble.
,
,
,
This majority vote ensemble will be correct an average of ~78% (0.3429 + 0.4409 = 0.7838).
,
,
,
Like repetition codes increase in their error-correcting capability when more codes are repeated, so do ensembles usually improve when adding more ensemble members.
,
,
Uncorrelated submissions clearly do better when ensembled than correlated submissions.
,
, 
  "
"
,(edited for  KDnuggets by Bhavya Geethika).
 ,
,
,
Averaging prediction files is nice and easy, but it?€?s not the only method that the top Kagglers ,   are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.,
,
, 
,

Netflix organized and popularized the first data science competitions. Competitors in the movie recommendation challenge really pushed the state of the art on ensemble creation, perhaps so much so that Netflix decided not to implement the winning solution in production. That one was simply too complex.
,
Nevertheless, a number of papers and novel methods resulted from this challenge:
,
,
All are interesting, accessible and relevant reads when you want to improve your Kaggle game.
,

,
,
Stacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper ?€?Bagging Predictors?€?. Wolpert is famous for another very popular machine learning theorem: ,
,
,
The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.
,

Let?€?s say you want to do 2-fold stacking:,
,

,

,
,
Blending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use ?€?stacked ensembling?€? and ?€?blending?€? interchangeably.
,
With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.
,
Blending has a few benefits:,
,
,?? ,
However, The cons are:,
,
,??,
As for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. The author prefers stacking.,
If you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage which we will explore next.
,
,

,



,How are you planning to implement what you learned? Share your thoughts! 
,
Original: , by Henk van Veen.
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,4 PM ET / 1 PM PT
,[ >> , << ]
,
,
,11 AM ET / 8 AM PT
,[ >> , << ]
,
,
,
,
,
,The vast majority of BI professionals are excited about the prospects of data mining, but are fully mystified about where to begin or even how to prepare. Of those who did initiate a modeling initiative, a recent data mining industry survey of predictive modeling practitioners reports that 51% of data mining projects either never left the ground, did not realize value or the ultimate results were not measurable.
,
In most cases, those who attempted an implementation ended up building excellent predictive models that answer the wrong questions. This is precisely like placing a perfectly good rocket upside down on the launch pad.
,
So, how does one approach an intangible, cryptic and seemingly immeasurable technology? Beyond the inherent up-front risks of engaging in what is essentially a discovery process, just identifying a starting point can be intimidating and mystifying.
,
Attend this free webinar to learn how to get started with data mining and overcome both strategic and tactical limitations that cause data mining projects to fall short of their potential.
,
,
,This webinar is intended for stakeholders, functional managers and business practitioners in business, industry, government and academia, who have made substantial investments in data collection, storage, retrieval, visualization and basic analysis but may not have the technical or strategic experience necessary to chart an effective roadmap to uncover the valuable predictive insights hidden within their existing data. No prior knowledge is required. Participants will learn:
,
,??,
,
,
,
,
,
,If you want to ,, dive straight into The Modeling Agency's Predictive Analytics & Data Mining course series.   
,
>> 
, << for full course details and special incentives for KDnuggets subscribers.
,
,  "
"
By Gregory Piatetsky,  
,.
,
While feats of Deep Learning has been gathering much attention, 
there were also breakthroughs in a related technology of Recurrent Neural Networks (RNN). RNNs hold great promise for learning general sequences, and have applications for text analysis, handwriting recognition and even  machine translation.
,
,
,
See a fantastic post by Andrej Karpathy,
, where he uses RNNs 
to do amazing stuff like paint house numbers in this image, or generate text in the style of Paul Graham, 
Shakespeare, and even Latex.
,
See below an excellent tutorial 
,
,
,
by Alec Radford, Indico Head of Research, who led a workshop on general sequence learning using recurrent neural networks at 
, in San Francisco, Feb 2015.
,
Alec introduces RNNs and sketches how to implement them and cover the tricks necessary to make them work well. Then he investigates using RNNs as general text classification and regression models, examining where they succeed and where they fail compared to more traditional text analysis models.
,
Finally, he presents simple Python and Theano library for training RNNs with a scikit-learn style interface and you'll see how to use it through several hands-on tutorials on real world text datasets.
,
,
,
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,(edited for  KDnuggets by Bhavya Geethika).
 ,
,
,
,

,
,
Stacking with logistic regression is one of the more basic and traditional ways of stacking. You can create predictions for the test set in one go, or take an average of the out-of-fold predictors. Either works well.
,
Though taking the average is a clean and accurate way to do this, you might want to consider one go as that slightly lowers both model and coding complexity.
,

,
,
The author uses , to compete in this classification competition.
By stacking 8 base models (diverse ET?€?s, RF?€?s and GBM?€?s) with Logistic Regression he is able to score 0.99409 accuracy, good for first place.
,
,
,
Here the author again used , to improve a model. The model before stacking scored ~0.605 AUC, and with stacking this improved to ~0.625.
,

,
,
Popular non-linear algorithms for stacking are GBM, KNN, NN, RF and ET. You can note a couple of interesting points here:
,
,
,
,
,
The ,  can be treated as a multi-class multi-label classification challenge. For every label a separate ensemble model was trained. The key point is that stacking the predicted class probabilities with an extremely randomized trees model improved the scores. The author stacked generalization with standard models and was able to reduce the error by around 30%.
,

,
,
Feature-weighted linear stacking stacks engineered meta-features together with model predictions. Linear algorithms are used to keep the resulting model fast and simple to inspect.
,

,
,
The author framed the name ?€? Quadratic linear stacking of models. It works similar to feature-weighted linear stacking, but creates combinations of model predictions. This technique improved the author's score in many competitions, most noticeably on the ,   on DrivenData.
,

,
,
By stacking you can use classifiers for regression problems and vice versa. Even though regression is usually not the best classifier. But it is a bit tricky.
,
,
A good stacker must be able to take information from the predictions.,
,
,
You can also stack with unsupervised learning techniques as well. A sensible popular technique is the K-Means Clustering. An interested recent addition is to use 
,: 
,
,
,
,
,
A good example of online (or semi-) stacking is with ad click prediction. Models trained on recent data perform better here.
,
,
,
,
,
When doing stacking/blending/meta-modeling, think of every action as a hyper-parameter for the stacker model.
,
So this makes the below simply extra parameters to be tuned to improve the ensemble performance.
,
,
,
,
You can further optimize scores by combining multiple ensembled models.
,
,
,

,
,
Adding many base models along with multiple stacked ensembles can only get you so far in a competition.
For the rest, you might consider the below for automating:
,
,
,

,
,
Using the automated stacker in this competition, the author got to top 10% score without any tuning or manual model selection. Here's his approach:
,
,
,

,
You may wonder why this exercise in futility: stacking and combining 1000s of models and computational hours is insanity right? Well?€? yes. But these monster ensembles still have their uses:
,
,
,
See also
,
,
,  of this article. 
,

Original post: , by Henk van Veen.
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
, is the Founder, CTO at , Before co-founding Cloudera in 2008, Amr (,) was an Entrepreneur-in-Residence at Accel Partners. Prior to joining Accel he served as Vice President of Product Intelligence Engineering at Yahoo!, and ran one of the very first organizations to use Hadoop for data analysis and business intelligence. Amr joined Yahoo after they acquired his first startup, VivaSmart, in July of 2000. 
,
Amr holds a Bachelor?€?s and Master?€?s degrees in Electrical Engineering from Cairo University, Egypt, and a Doctorate in Electrical Engineering from Stanford University.
,
Here is my interview with him:
,
,
,
,The most significant achievement is taking a technology (Apache Hadoop) which was only relevant to a couple of web companies (like Yahoo and Facebook) and then growing it into a whole industry as we see today. It is very satisfying to see such an evolution happen in a matter of 7 years.
,
,
,
,In our first iteration for the company we were planning on launching Hadoop as a cloud service. We truly believed that the era of the cloud is upon us, hence the name Cloudera. Six months after starting the company we shifted from being a cloud service to on premise enterprise software since that is what our customers wanted back then. We kept our name Cloudera as it was already well known, we thought it was cool :), and we always knew we will be coming back to the cloud (our software now runs in a number of Cloud environments via a product we have called Cloudera Director).
,
,
,
,I have an article on quora in which I describe what a CTO does, you can find it here: , . The summary is to balance the outside world (customers, partners, developers) with the inside world (product, technology, culture) without missing any major trends. 
,
In my role I also do a lot of evangelism, which involves giving talks at conferences all around the world. I cannot share my current top priority as that is confidential :) but in general it is to make sure that Cloudera continues to build technology that sets us up for success in the long term.
,
,
,
,Over the last couple of years we proved that this platform is very scalable, very economical, and very flexible (any data type, any workload -- not just SQL). The most critical factors for success right now are to focus ,on making this platform bulletproof in terms of security, and to make sure it is extremely stable and reliable. That is easier said than done since the platform still continues to evolve very quickly with new project additions like Apache Spark or Apache Kafka. All these new additions require very extensive testing to catch all the corner cases and race conditions that might lead to stability issues in such a large scale complex system. At Cloudera we spend almost half of our engineering resources focused on these issues. 
,
In terms of performance vs flexibility, they are both important but sometimes hard to achieve both at same time. If you are running the same dashboard a thousand times every day then you care more about performance. If you are exploring and trying to ask new questions that you haven't thought about before then you care more about flexibility. The beauty of our platform is that it allows you to pick which operating mode you prefer. We have schema-on-read formats like Avro that are extremely flexible, and we have highly optimized schema-on-write formats like Parquet which are very high performance. You pick which is best for the task at hand.
,
,
,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
,
,
Analysts believe that it has taken 15 years or so for companies to harness about 50% of the productivity potential of the Internet, and the next 50% of productivity gains likely require connecting things. The Internet of Things is causing an interconnected world where ""smart"" devices allow things and people to be connected from anyplace, anytime, with anything and anyone. However, the IoT data loses its value if it is not detected and acted upon immediately. That is where real- time streaming analytics can help!
,
Enterprises and IoT applications can benefit immensely from real-time streaming analytics by visualizing the business in real-time, cutting preventable losses, detecting urgent situations and automating immediate actions.
,
This
,
will help you learn how real-time streaming analytics can add value to business.  
, it now to find out:
,
,??,
Many industry experts also believe that there is a need of streaming analytics platforms that provide development tools and processing capability. 
,
, with us if you are looking for one such product that can help you build real-time streaming applications in minutes!  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jun 30 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Most popular 
, tweets for Jun 22-29 were
,
Free , #MachineLearning Tutorial in R - learn how to compete in #DataScience #rstats 
, 
,
,
,
Free , #MachineLearning Tutorial in R - learn how to compete in #DataScience #rstats 
, 
,
,
UNC #Teacher Quality Dashboard, built with ,, lets people analyze #education data 
, 
,
,
,
Free , #MachineLearning Tutorial in R - learn how to compete in #DataScience #rstats 
, 
,
,
,  "
"
,
,
, is the Founder, CTO at , Before co-founding Cloudera in 2008, Amr (,) was an Entrepreneur-in-Residence at Accel Partners. Prior to joining Accel he served as Vice President of Product Intelligence Engineering at Yahoo!, and ran one of the very first organizations to use Hadoop for data analysis and business intelligence. Amr joined Yahoo after they acquired his first startup, VivaSmart, in July of 2000. 
,
Amr holds a Bachelor?€?s and Master?€?s degrees in Electrical Engineering from Cairo University, Egypt, and a Doctorate in Electrical Engineering from Stanford University.
,
,
,
Here is second part of my interview with him:
,
,
,
,Self-Service Analytics is about enabling the end users to ask new questions without having to wait months and months for the IT team to bring in the data that they need to ask these new questions. Cloudera's Enterprise Data Hub (EDH) goes at the core of solving that problem. It allows you to consolidate all of your data regardless of type (structured, semi-structured, or unstructured) then later on the business users can extract the sub-set of the schema that they care about dynamically without having to block on the IT team to re-architect the ETL pipeline. 
,
The Xplain.io product allows you to analyse the query logs from a traditional RDBMS then it infers which set of queries are ideal candidates to be moved to the EDH, makes suggestions on how should the data be laid out schema/partition wise, and rewrites the queries to be more efficient in the EDH. It is magic.
,
,
,
,You can think of Cask as a middleware abstraction layer on top of the EDH that simplifies a lot of the common development tasks (similar to IBM WebSphere on top of Java for example). For Cloudera customers that are building complex Big Data applications (as opposed to just using SQL or Search) they are able to implement such applications much quicker using the primitives available in the Cask development framework.
,
,
,
,The most common concern is skillset. They are afraid that this is a new technology and they don't have the knowledge inside to deal with it ,so they stand still. That paralysis is very dangerous since it means they will deadlock while their competition leapfrogs them technologically. While Hadoop was hard to use 7 years ago, today there is a very rich ecosystem around this platform that makes it much easier to adopt (at Cloudera we have more than 300 software partners with tools and applications that simplify the adoption curve for this platform). 
,
Cloudera also offers administrator training courses, which coupled with Cloudera Manager, allows a Linux sysadmin or DBA to learn how to operate this environment in a matter of weeks. We also offer training for the end users of the system, starting with straightforward data analytics to more advanced data science and machine learning concepts. Check out , for more details.
 ,
,
,
,Our product offering is more mature and more complete, especially when it comes to security and stability. First, on the security front we are the only solution with native encryption and private key management due to our Gazzang acquisition last year. Our security capabilities were also significantly enhanced when Intel partnered with Cloudera and merged their Hadoop distribution with ours. 
,
Second, as I mentioned above, we have a lot of sophistication at Cloudera around the testing of the software to ensure the highest reliability and stability. That sophistication is further strengthened by Cloudera Manager which collects telematics from our installed customer base about how they are operating their clusters. We store all these telematics in our own EDH cluster which allows us to quickly analyse the customer operational data to support them when they have any issues. This data also allows us to do predictive maintenance for our customers, where we predict failure before it happens and reach out to our customers to make corrective changes to their environments. 
,
Cloudera is much older than the other competitors and we have a much large customer base that allowed us to create the largest most-diversified operational telematics database in the industry. 
,
,
,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,
  "









"
,
,
, is the Founder, CTO at , Before co-founding Cloudera in 2008, Amr (,) was an Entrepreneur-in-Residence at Accel Partners. Prior to joining Accel he served as Vice President of Product Intelligence Engineering at Yahoo!, and ran one of the very first organizations to use Hadoop for data analysis and business intelligence. Amr joined Yahoo after they acquired his first startup, VivaSmart, in July of 2000. 
,
Amr holds a Bachelor?€?s and Master?€?s degrees in Electrical Engineering from Cairo University, Egypt, and a Doctorate in Electrical Engineering from Stanford University.
,
,
,
,
,
Here is third and last part of my interview with him:
,
,
,
,Community, community, community. The key reason for why open source projects succeed is fostering a strong community of developers around them, they obviously also need to be solving a real customer problem (as opposed to reinventing the wheel, or building something cool that nobody needs at the first place). 
,
I personally think open source disruption will come to ERP systems, it is a question of when it will happen. In fact, if you look at the current ERP-as-a-service vendors (ala Netsuite and Workday), I wouldn't be surprised if much of their backend is already being built on top of open source solutions.
,
,
,
,I am biased, but my personal favorites are all the ones in the Cloudera Distribution :) One of our key values for our customers is curation of the ,Cloudera distribution by picking the best of breed projects to be part of it. So far our track record have been flawless in terms of making the right choices. For example, about a year and a half ago we added Apache Spark to our distribution, now everybody else is doing the same. I have no favorites among the projects in our distribution, I love them all equally. It is the combined power of these projects, the platform, where the true power lies.
,
,
,
,For this question I will refer your readers to this video interview which I did recently at Draper University (http://livestream.com/draperuniversity/amrawadallah). In it I cover the history of formation of Cloudera and what are some of the key entrepreneurial lessons that I learned along that journey. 
,
,If I was to pick one lesson it would be: team, team, team. The success of your company will depend entirely on that, make sure you spend your time to hire great people, and make sure to correct that when it isn't working out. If there is one thing I would do differently it would be to correct for hiring mistakes faster, much faster.
 ,
,
,
,Don't waste time on regrets, learn from your mistakes and evolve quickly. This is advice that I formed for myself as I experienced life.
,
,
,
,Our Director of Data Science (Josh Wills) has a great line he uses to describe what is a data scientist: , So the key qualities are to have good statistics background, good software skills, but most importantly have the investigative eye and business acumen to wrangle the data into an insight that makes sense for the business (that last part is what is hard to find).
,??,
,
,
,My main hobby is video games, over the last year the games I really liked are: Destiny, Call of Duty AW, Battlefield 4, Last of Us, and Grand Theft Auto 5. I can't wait for the new Uncharted 4 and Halo 5 to come out. As far as books, the last one I read is ,, I had read it a long time ago, but wanted to refresh my memory after seeing ""The Theory of Everything"" movie. A recent movie that I really liked is ""The Judge"" by  Downey Jr. and Duvall, amazing acting, I like movies that make me tear up.
,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
,
,
,
This month on /r/MachineLearning, we see images generated by Google Research's neural nets, 16 great free books on data science, a machine learning system that can play Super Mario World, a tutorial to implement neural networks in Python, and a video stream that visualizes terms provided by the audience made it to the top.
,
,
,
This post links to an image generated from a trained neural network. It's really fascinating to look at a convolutional neural network in this way. The blog post explaining how it was made is ,.
,
,
,
Here we have 16 great free data science books, all compiled in one place. These books cover topics ranging from Python and R to Deep Learning. Each category has at least one book that I would consider very good in its respective topic, and the list as a whole is high quality.
,
,
,
This video demonstrates a machine learning system made to play Super Mario. It's impressive how quickly it proceeds through the level, especially considering it is an automated system. This video was also included in my list of ,.
,
,
,
This tutorial builds up an implementation of neural networks from the ground up using Python and Numpy. It's great because of how it works from the basic pieces into a whole neural network library. This provides a great way to learn about neural networks in a hands-on way.
,
,
,
This post is related to the number 1 post. Here, instead of generating images, the neural networks generate a video stream that creates video based on the words the viewers suggest. It can be very entrancing.
,
,
,  "
"
By Gregory Piatetsky,  
,.
,
Here are upcoming July-December 2015 meetings and conferences. 
,
For full list see ,  page.
,
,
,
Here are the upcoming meetings.
,
Top locations are:
,
,??,
Color code: Business-Oriented meetings in Blue, ,
,
,
,??,
,
,??,
,
,??,
,
,??,
,
,
,??,
,
,  "
"
,
,
,
,
,??,
,
,  "
"
By Gregory Piatetsky,  
,, Jul 3, 2015.
,
We read recently that a 
,. 
Computers have already defeated human champions in Chess and Jeopardy.
,
Ahead of US July 4th holiday, when hot dogs (grilled sausages) are a popular food, KDnuggets cartoon by 
, finds an area where humans still have an advantage.
,
,
,
,
,
,
Here are other 
,
,
and KDnuggets posts tagged 
,.
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,

Bio: , has over 40 years experience in research and industrial practice in databases, distributed systems, integration, artificial intelligence, and multi-disciplinary problem solving. He is concerned with the Big Picture aspects of information ecosystems including business, economic, social, application, and technical. Dr. Brodie is currently a Research Scientist at CSAIL at MIT; advisor to several startups; serves on Advisory Boards of national and international research organizations; and is an adjunct professor at the National University of Ireland, Galway and at the University of Technology, Sydney.
,
,
,
,
,
,
 ,  "
"
        ,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
Strata + Hadoop World in New York sold out last year with more than 5,500
attendees. Have you seen what's new for 2015?
,
,??,
Strata + Hadoop World sells out every time. 
,KDnuggets subscribers 
,
,
,  "
"
,

,



,

,

,

,

,
,

,

,

,

,

,
To begin, theoretical guarantees usually assure
that a hypothesis is close to the best hypothesis in some given class.
This in no way guarantees that there exists a hypothesis in the given class
capable of performing satisfactorily.

,

,

,
,


,
,

,


,


,
,

,



,
,

,
,

,





,





,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,

,  "
"
,
The Big Data Innovation Summit returns to Boston this September 9 & 10, taking place at the Westin Waterfront Hotel.
,
Check out 
,.
,
Exclusive to KDNuggets, you can use discount code 
,
to save 
,
off any two-day passes.
,
So what can you expect from this year's summit?
Presentation topics include:
,
,??,
,
Uncovering some of the key trends within Big Data, this event will gather over 800 senior level executives taking part in interactive panel sessions and fantastic networking opportunities.
,
As Big Data is part of almost everything we do, we take a closer look at how Big Data is used in real life. 
,
Take a look at the article here: 
,
,
If you would like to take advantage of this exclusive discount, please contact Hayley Law at 
,
(+1 415 692 5378). 
,
Alternatively you can register here: 
,
,Don't forget to enter in , to save ,!  "
"
,
,
, is Global Product Manager for ,. Reiner has over 20 years of computer software industry experience focusing on encryption and security for big data environments. 
,
His background ranges from device management in the telecommunications sector to GIS and database systems. He holds a Diploma from the FH Regensburg, Germany in computer science.
,
Here is my interview with him:
,
,
,
,We provide data-centric security and stateless key management solutions that help organizations combat new security threats and address compliance by protecting structured and unstructured data as it is used across data centers, public and private clouds and mobile devices. 
,
We were founded in 2002 with the vision of commercializing identity-based encryption (IBE), and that?€?s how HP SecureMail (then called Voltage SecureMail) came about.  Email was, and of course still is, a highly exploitable source of sensitive data, and SecureMail grew to be one of the world?€?s most popular email encryption solutions. 
,
Since then, we?€?ve listened to customers and ?€?followed the data?€? if you will, across technical platforms like the cloud and Hadoop, and across vertical applications like mobile and online payments. Healthcare has also been a key area because personal health information (PHI) is increasingly being shared over various platforms. Sensitive data is everywhere and we have a responsibility to protect it. 
,

,
,
,HP Attala and HP Security Voltage now combine to protect the world?€?s most sensitive data. We drive leadership in data-centric security and protect the world?€?s largest brands. The HP Atalla/HP Security Voltage difference is we have the widest variety of use cases, are well-supported and widely proven. We are Standards-based (ANSI, NIST, IEEE, KMIP) and offer a full breadth of platforms
,
HP Attala and HP Security Voltage provide a full umbrella of data protection use cases; PCI compliance/ scope reduction, data de-identification and privacy, and collaboration security.
,

,
,
,The standards organizations you listed above have focused for a long time on regular encryption problems. NIST in particular is working Format Preserving Encryption (FPE) into those standards. This is important because they are recognizing the need for adding the next generation of encryption as a standard so that companies can operate with a more granular set of encryption that wasn?€?t available before.
,
The good news is, the standards have been recognized widely and today almost all companies utilize some form of the existing standards available to them (i.e. SSL).
,
,However as the threat profile has changed over the last decade companies are turning to the newer forms such as FPE that are being recognized and help them achieve a broader sense of security that does not interfere with their regular business operations while avoiding the overhead of traditional encryption.
,
, reduce the risks associated with theft of sensitive and private information, support privacy guidelines including PCI DSS, HITECH, U.S. Data Breach Disclosure laws and European Data Privacy directives.
,
,
,
,Data exists in three basic ways - at rest, in use, and in motion. Our data-centric approach is in contrast to traditional network-based approaches to security, which haven?€?t responded directly to the emerging need for data-centric security that neutralizes the effects of a data breach through protection of sensitive data at the field-level.
,
With data-centric security sensitive field-level data elements are replaced with usable, but de-identified, equivalents that retain their format, behavior and meaning. This means you modify only the sensitive data elements so they are no longer real values, and thus are no longer sensitive, but they still look like legitimate data. 
,
,The format-preserving approach can be used with both structured and semi-structured data.  This is also called ?€?end-to-end data protection?€? and provides an enterprise-wide solution for data protection that extends into Hadoop and beyond the Hadoop environment. This protected form of the data can then be used in subsequent applications, analytic engines, data transfers and data stores.  
,
A major benefit is that a majority of analytics can be performed on de-identified data protected with data-centric techniques?€?data scientists do not need access to live payment card, protected health or personally identifiable information in order to achieve the needed business insights.
,
,
,
, is a software development intern at Salesforce. He is a MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
        ,
,
,
,
,
,: Datasets now have their own contextual menu that enables users to easily manipulate & transform data as needed. ??No need to look all over for the tools you need?€? they?€?re now right at your fingertips. In addition, our redesigned Search tool provides access to recently-used items and contextually-relevant objects.

,

,
,
,

DSS 2.0 now supports advanced visual data preparation for use in machine learning models. To facilitate this, we created a new unified module called ,. Specifically optimized for fast iteration cycles, the new Analysis Module enables users to transform their data and immediately view the results of their predictive models?€? these drag & drop visualizations, in turn, allow for the rapid evaluation of model quality. Key features include:
,
,
,

,

DSS 2.0 has taken a whole new approach to machine learning via a complete redesign coupled with significant functional enhancements. The new machine learning interface empowers you to create both supervised and unsupervised models with ease. Features include:
,
,
,
,

,

,

,

Facilitate fast data preparation without the need to write SQL code. Our five new visual recipes provide an easy way to initiate data manipulations, such as cleansing and aggregation. DSS 2.0 now features:

,

,
,

,Enough of our rambling?€? why not give DSS 2.0 a try yourself? We offer a ,, great support, and a new user-friendly environment designed to engage with all of your team members.  "
"
,
,
,
Did you know that over 87% of your peers said their organization struggles with tying their data and analytics to real-world business goals? And that 63% said they specifically struggle with using strategies like text analytics to impact and improve marketing campaigns?  Text Analytics is an expanding industry, but there are still a lot of challenges to overcome.
,
The Incite Group's recent white paper takes an in-depth look at the current state of text analytics and unstructured data to give you the tips and tools you need.  
,Get your FREE copy here: 
,
,
Featuring interviews with senior analysts and chock full of statistics, the comprehensive paper looks at the influence analytics can have on customer engagement and retention, and includes:
,
,??,
Download your copy now: 
,
,
It's an invaluable resource for anyone working in text or marketing analytics.  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,. Reposted by permission.
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
Data Scientist, Business Analytics professionals, IoT enthusiast, Executives, Managers & Big Data Architects are attending conferences between July and September 2015 hosted by Global Big Data Conference held in ,.
,
This is a great opportunity for
data analytics experts to meet their global colleagues and exchange cutting edge developments in this area.
,
,
,??,
,
,
,??,
,
,
,
,??,
Big Data Bootcamp, covering hands on workshop, Use Cases & Classes on Spark, Hadoop, Cassandra, MongoDB, HBase, R, Python, NiFi, Flink, Predictive Analytics, Neo4J & Industry vertical uses cases of Big Data. 
, & 
,
,
,
,
,??,
,Attendees can register for either one event or multiple events. Please contact 
,
for group discounts.
,KDNUGGETS readers receive up to $200 off when they sign up with promotional code ,
,If you are interested in speaking or sponsoring at any of our upcoming events, Please send email to 
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
,
Using data to find and keep your best customers is not a new concept. However, the power to predict their behavior is based on the ability to successfully leverage not only historical information but incoming data from multiple channels. Whether your focus is introducing new products or services, positioning, branding, advertising, segmentation or promotion, how do you determine the best ways to meet, or even shape, customer needs?
,
,
,
,
,??,
,  "
"
,
,
Registration: ,
,
*Alternative link: ,
,
July 21, 10AM - 11AM PDT
,
If the time is inconvenient, please register and we will send you a recording.
,
,: Customer segmentation is the process of dividing a client database into distinct groups of individuals who share common characteristics. Knowing how different groups of customers act is key to a marketing strategy and can be implemented using modern data mining and machine learning techniques. 
,
In this webinar, we will demonstrate customer segmentation of a German banking database. Attributes such as credit history, employment, age, and gender will be used to identify segments likely or unlikely to be good credit risks. 
,
CART (Classification and Regression Trees) will be used as a way to cluster similar records and extract the conditions under which these customers are classified. Other algorithms, such as gradient boosting, will be used in conjunction with the insights drawn from CART.
,
,
,
,??,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
        ,
,

,
,
,
,

,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,

,
,
,
,
,

,
,

,
,
,
,

,
,
,
,

,
,
,
,
,
,
,

,
,

,
,

,
,
,

,
,

,
,
,
,
,

,
,

,
,

,
,
,
,
,
,
,
 ,  "
"
,
,
, is Global Product Manager for ,. Reiner has over 20 years of computer software industry experience focusing on encryption and security for big data environments. 
,
His background ranges from device management in the telecommunications sector to GIS and database systems. He holds a Diploma from the FH Regensburg, Germany in computer science.
,
,
,
,
,
Here is third and last part of my interview with him:
,
,
,
,
,Many may still think that data at rest encryption solves data privacy problems. The bottom line is that it solves only a fraction of the risk and compliance problem, leaving gaping holes in security that malware will exploit. The majority of today?€?s data breaches use malware to attack the gaps between storage protection and actual use or transit of the data. There are many instances of major data breaches where data at rest encryption has been in place and the data at rest solutions did nothing to protect the data. 
,
Data at rest encryption protects from theft of data at a server level ?€? at rest, when powered off ?€? not at the application or in transmission, or in use. So if a cloud or enterprise application is executing with data protected at rest, then that same data is typically decrypted at the disk or database layer before it?€?s presented to the network, up to and into the application running in the cloud or enterprise or Hadoop.
,
Newer, secure, proven, innovative approaches are being adopted by leading banks, telcos, credit card processors and issuers, healthcare entities, government agencies and even industry regulators: they are all embracing data-centric security. Data-centric security protects the data across its lifecycle ?€? from capture, in motion, at rest, and even in use.
,
,
,
,Market solutions for Hadoop security are beginning to emerge, delivering data masking features that make it possible to obscure sensitive data. But whether you leverage a commercial solution or create a homegrown approach, I suggest the following five steps to identify what needs protecting and apply the right techniques to protect it?€?before you put Hadoop into production.
,
,
,
,
,The best advice I ever got was to never give up. Often times there are challenges in what we do and people frequently come back with ?€?this cannot be done?€?. One has to find a way to help others understand that there is a different approach possible.
,
,
,
,The growth of data is not stopping; in fact as new technologies such as Internet of Things continue to grow, we can expect the huge explosion of data to continue. And, along with that is the growth in sensitive data ?€? data that must be protected by enterprises while they continue to execute their business processes. All of this data presents opportunity for analytics and so we expect the continued accelerated use of Hadoop and Big Data technologies. At the same time, data thieves will continue to grow more sophisticated as the value of sensitive data on the black market increases. 
,
Additionally the movement of applications into the cloud is creating additional challenges for companies as they must give their data to other companies and hope it is correctly protected. The ability to still control the real data vs. the de-identified data will drive additional changes in organizations and how they view data security.
,
,
,
,Product Management is a role where one carries the responsibilities for many activities in the organization but has no control over the delivery itself. So the ability to influence others is important. For good product management to be successful it is good to have worked in a similar product environment, yet less important to have the same product experience.
,
,
,
,For me, it is enjoying the great outdoors, where we usually hike and kayak as much as possible. Viewing and enjoying the scenery from mountains, dessert and ocean is something my family enjoys as much as possible, driving in our RV with our dog.
,
,
,  "
"
,
,
,
,One of the first things I did when I moved to Montreal was installing??,. For those of you not familiar with the online meat market, Tinder is a dating app showing nearby users you can like or dislike by their profile pictures. After using the app for a while, I discovered that although I consistently disliked girls with a lot of piercings and tattoos (no offense, just not my type), the app kept showing me these profiles. It surprised me that Tinder did not use my history of swipes to learn what type of girls I like.
,
This observation made me think: can a computer learn to which girls I'm attracted? I have tried by labeling over 9K profile pictures on Tinder and using??,, the latest revolution in artificial intelligence. In this blog post, I will provide a high-level view how I used these techniques to predict my Tinder swipes. You can find the technical details in??,, which was accepted for the??,.
,
,
,
,
,
Computers have been around for many decades, but there still exists a lot of problems that the most powerful computers struggle to solve, yet we (as human beings) can do without effort. You can think of detecting faces in pictures, recognizing speech and translating text. The problem is that although we perform such tasks easily,??,. That means, we can not come up with a set of rules that can be transformed into a computer program.
,
Is predicting attractiveness from a profile picture such a task? Yes, it is simply too difficult to specify a set of rules to who you're attracted. For some of you this might sound counter-intuitive because you can specify a clear list of traits e.g. blue eyes, blond hair, etc. However, even if you could do that, you have to believe me that it is too complex to construct a program by hand that detects such characteristics in images.
,
On the other hand, if I show you a picture of a girl/boy you can decide in a split of a second whether you like her/him. This suggests a better strategy: we let the computer program itself by showing it profile pictures of girls you do or do not find attractive. We call this??,??because the program learns a set of attractiveness rules from these profile pictures.
,
,
,
,
,
,??is a particular class of learning algorithms that has recently lead to a breakthrough in??,??(and many other artificial intelligence problems). If you want to see yourself how well these techniques work nowadays, I encourage you to upload a picture to
,
,??,
In deep learning, the structure of the program is an artificial neural network which is inspired by a biological neural network i.e. the brain. Neural networks are called??,??when they consists of several layers of connected neurons as shown in the figure above. Each neuron is connected to a set of neurons in the previous layer. The strength of those connections (shown in red) determine to what image pattern the neuron responds. You can think of it as an adjustable filter, only responding when it sees a particular pattern.
,
The connection strengths of the network are randomly initialized, meaning that all neurons are just random filters. Therefore, when we feed an image to the network (by inputting the intensity of its pixels), only a few neurons in the first layer will respond, which in turn let some random neurons in the next layer to be active (and so forth till the output). As a result, the output of the network will be random like or dislike.
,
Therefore, in order to make good predictions, we need to??,??the network. We do so by inputting a profile picture and propagating it through the network to get the output. If the network's prediction is different from the desired output (like or dislike), we adjust its filters (i.e. its connection strengths) to make the prediction more correct. How we exactly adjust the filters is rather difficult and outside the scope of this blog post, all you need to know is that we can do it. The training phase is nothing more than repeating this process for many examples, and then hope that the network predicts well for profile pictures it has not seen yet.
,
After training, we often observe that the neurons in the first layer respond to edges and color blobs, while the next layer uses these observations to detect more complex patterns such as eyes and ears. The last layer might detect complete faces or piercings and tattoos (which is important to predict my preferences).
,See a continuation: 
,
, is a PhD student supervised by Aaron Courville and Roland Memisevic, and a member of the Montreal Institute for Learning Algorithms headed by Yoshua Bengio. His main interests are in the theory and application of deep learning. 
, by Harm de Vries.
,
,
,
,
,
 ,  "
"
,
,
,??,??
,
,
,??,??
,
,  "
"
Most popular 
, tweets for Jun 30 - Jul 06 were
,
,
16 Free #DataScience Books #Python R #Statistics #MachineLearning #Visualization , 
, 
,
,
16 Free #DataScience Books #Python R #Statistics #MachineLearning #Visualization , 
, 
,
,
Clever Post: How Screenshot Click Testing Proved that Beards Are Still A Thing 
, 
,
,
,
Clever Post: How Screenshot Click Testing Proved that Beards Are Still A Thing 
, 
,
,
,  "
"
,
,
,
,
This inaugural global data competition is focused on Climate Change and split into 22 regions around the world. The first phase is a predictive benchmarking competition.
,
The competition will be held in two phases. Each of which will have a scoring system that will allow solutions to be compared within the region it is submitted in. Competitors can compete on an individual, group (2-10 people), or organization (10+) level. 
,
The first phase is a predictive benchmarking competition. We're trying to provide options here to compete on. 
The first is: ,
,
Code will be provided to help you get started. Another option directly related to Climate Change should be available soon. 
,
There will be a link to submit your solutions to a Kaggle style/Real Time grading system. Submissions in this phase must include your full code. Multiple Submissions will be allowed. Your best submission will be benchmarked and your performance will award you points on a sliding scale.
,
The closer your team's prediction is to 100%, the more points you can receive for your team in your region. The higher your points, the higher your ranking title. (e.g. ""Master"" or ""Participant""). 
,
The second phase will begin later in the Summer. Phase two will still be related to Climate Change. You will be tested on many aspects of Data Science. This phase encourages collaboration, as there are many skills within Big Data and Data Science and being the best at all of them... well, you get the point. You will be able to build on your overall ranking and title with points awarded in the second phase.
,
Details about the Global Data Competition are at: 
,  "
"
Most popular 
, tweets for Jun 16-22 were
,
,
#DeepLearning resources from , to help you get started: books, blogs, podcasts 
, 
,
,
Free , #MachineLearning Tutorial in R - learn how to compete in #DataScience #rstats 
, 
,
,
,
10 Common #NLP Terms Explained for the #TextMining Novice 
, 
,
,
,
#DeepLearning resources from , to help you get started: books, blogs, podcasts 
, 
,
,
,  "
"
,
,
,
,
,
,
,
Neural networks work especially well when we provide them a lot of labeled examples. So, all I needed to do was intensively use Tinder for some time.
,
But how do you save the liked and disliked pictures from the Tinder app? The application is communicating with a server to get new profile pictures, and by inspecting how the app did this communication, I could build a program on my laptop that mimics the tinder app (but saves the pictures). You can see the program at work in the screenshot. I have to say that the real Tinder app was a bit more convenient because it can be used anywhere and anytime. Now I really had to sit down and make time to swipe profiles. Nevertheless, I managed to label 9364 profile pictures in less than two months.
,
,Almost 10K profile pictures sounds a lot, but in practice it was not enough to successfully train a deep neural network. What happened was that the network worked very well for the pictures it was trained on, but did not generalize to new profile pictures. Looking at the images, I could see that there was a lot of variation in the profile pictures (compared to the clean datasets we work with in academia). One way to account for those variations is to collect more data, but I didn't want to wait half a year before I had labeled enough profile pictures (or spent my whole day using Tinder..).
,
Instead, I focused on another way to overcome this issue: we can use another neural network that is successfully trained for a related task (for which there was enough data). The lower-layer filters learned by that network are recognizing patterns that might be useful for our task. We simply copy the trained network, and then only adjust the last layers to predict my attractiveness preferences.
,
I thought it would be a good idea to use a network to predict gender from profile pictures for two reasons. First, it is easy to collect data for this task. I scraped over 400 000 (!) male and female profile pictures from??,??in a couple of weeks. Second, the filters extracted by this network could be useful to predict my preferences, since I am probably more attracted to girls that do not look like men.
,
,
,
Now that I had collected the data, I had to face the facts about my own Tinder behavior:
,
,??,
,
,
I'll spare you the details of processing all this data, and simply summarize the results:
,
,
,
,
Almost 70% does not sound that impressive, right? But how well could someone else predict to which girls I am attracted?
,
My co-author??,??tried it. He 'trained' by first looking at all 50 dislike and 50 like pictures side by side, scrolling through them all a few times. Then the training set was shuffled, pictures were displayed one a time, and he produced a label prediction after each photo. The correct label was shown after each picture so that he could learn from his mistakes. See his study program in the screenshot above. He achieved 86%, 82%, 88% and 88% on 4 training rounds.
,
He then predicted the labels for 100 unseen profile pictures (the same ones as in my consistency test). He made 24 errors, and thus achieved??,??accuracy. Given that's also hard for other people to predict my preferences, the??,??is not that bad!
,
,
,
The techniques I have used are not tailored to my preferences, and could be used for other people whenever they provide their labeled profiles pictures. In fact, deep learning is known to be working much better with bigger datasets, so I am pretty confident that this approach would be even more successful when I train a neural network on data gathered from many people (e.g. when I have access to all data from Tinder). As far as I know, the most popular dating websites (like??,) still use questionaires to suggest potential partners. I think the success of Tinder has shown that people care about somebody's appearance, and with the rise of deep learning we now have a tool to actually incorporate this kind of information in matching algorithms.
,
The time that you were predetermined to marry one of the 50 girls/boys of your town is over. New technologies will continue to change the way how we find our romantic partners. Although randomness sometimes determines love, we should embrace the great opportunities in machine learning to help anyone looking for a soulmate!
,
, is a PhD student supervised by Aaron Courville and Roland Memisevic, and a member of the Montreal Institute for Learning Algorithms headed by Yoshua Bengio. His main interests are in the theory and application of deep learning. 
,.
,
,
,  "
"
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
Here is a good reddit find: 
, prepared and maintained by Donne Martin.
,
,
,
Topics include:
,
,??,
Github: ,
,
or Bitly: ,  "
"
,
,
,
,
,
,

,
,
,

,
,
,??(,) is a must-have! It is a fantastic source for all things having to do with Analytics, Data Science, Big Data etc.?? I would encourage everyone to subscribe to the daily newsletter.
,
,(,) is a good educational platform.?? Their bite-sized short videos on YouTube (,??) can be great to brush up on Analytics topics ?€? Statistics, Probability, Linear Algebra, Calculus, Optimization etc.
,
,??(,) is an excellent way to get educated on pretty much any topic!
,(Note: BrightTalk videos & webinars can also be a helpful educational source of Analytics in the business context.??
,,??
,??)
,
,??(,??) is great to way to stay on top of the latest happenings in Big Data Analytics!?? I suggest you sign up for their free newsletter.
,
, (,??) is an informative portal for Predictive Analytics.
,
,
,
,
,?? A few good ones to call out here ?€?
,
,
,
,
,
,
,. Reposted by permission.
,
,
,
,
,
 ,  "
"
        ,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,
,
,
,

,  "
"
        ,
  "
"
,
,

,
There are thousands of packages and hundreds of functions out there in the Data science world! An aspiring data enthusiast need not know all. Here are the most important ones that have been brainstormed and captured in a compact few pages. 

,
Mastering Data science involves understanding of statistics, Mathematics, Programming knowledge especially in R, Python & SQL and then deploying a combination of all these to derive insights using the business understanding & a human instinct?€?that drives decisions. 
,

Here are the cheatsheets by category:,

,
Python is a popular choice for beginners, yet still powerful enough to back some of the world?€?s most popular products and applications. It's design makes the programming experience feel almost as natural as writing in English. Python basics or Python Debugger cheatsheets for beginners covers important syntax to get started. Community-provided libraries such as numpy, scipy, sci-kit and pandas are highly relied on and the NumPy/SciPy/Pandas Cheat Sheet provides a quick refresher to these.
,
,

,
,
The R's ecosystem has been expanding so much that a lot of referencing is needed. The R Reference Card covers most of the R world in few pages.The Rstudio has also published a series of cheatsheets to make it easier for the R community. The data visualization with ggplot2 seems to be a favorite as it helps when you are working on creating graphs of your results. ,
,
,
,

For a data scientist basics of SQL are as important as any other language as well. Both PIG and Hive Query Language are closely associated with SQL- the original Structured Query Language. SQL cheatsheets provide a 5 minute quick guide to learning it and then you may explore Hive & MySQL! 
,
,
,

Apache Spark is an engine for large-scale data processing. For certain applications, such as iterative machine learning, Spark can be up to 100x faster than Hadoop (using MapReduce). The essentials of Apache Spark cheatsheet explains its place in the big data ecosystem, walks through setup and creation of a basic Spark application, and explains commonly used actions and operations.,
,
,
,
Hadoop emerged as an untraditional tool to solve what was thought to be unsolvable by providing an open source software framework for the parallel processing of massive amounts of data. Explore the Hadoop cheatsheets to find out Useful commands when using Hadoop on the command line.
A combination of SQL & Hive functions is another one to check out.
,
,
,
We often find ourselves spending time thinking which algorithm is best? And then go back to our big books for reference! These cheat sheets gives an idea about both the nature of your data and the problem you're working to address, and then suggests an algorithm for you to try.

,
,
,
Django is a free and open source web application framework, written in Python. If you are new to Django, you can go over these cheatsheets and brainstorm quick concepts and dive in each one to a deeper level. 
,
,
,
,
,
,


,
,
 ,
  "
"
,
,
As Director of Analytics at ,, , is responsible for helping the Leadership & Stakeholders with actionable insights derived from Analytics. The business questions he works on span the whole spectrum across the Product, Marketing, Sales and Relationship. His team leverages any of the various options, i.e., Strategic analysis, Advanced Analytics, Text Analytics or Mining depending on the problem being solved. 
,
Here is my interview with him:
,
,
,
,Analytics at Visa is responsible for generating actionable insights from data and helping with decision making for various stakeholders across Product, Marketing, Sales, Relationship Management & Solution Delivery. Our operational philosophy is ?€?Success of our Customer is our success?€?.
,
,
,
,The overarching need is to ensure that data is reliably and efficiently captured, processed, managed, analyzed and used in ,decision making. It is also critical that users are able to understand the end product easily, intuitively and when and where they need it.  
,
An Outcome Focused Strategy, where the Analytics/Data roadmap is driven from the high level Business goals will be the best way to ensure that right prioritization happens and key things are taken care of first and best way possible. It encompasses everything from right Instrumentation, Data management & Governance, Reporting, Analytics, A/B Testing, Research and most importantly Knowledge Management & Education.
,
,
,
,
,
,Although the Customers typically use one word ?€?actionability?€? to describe all the gaps, it typically encompasses everything from Turn-around time, Relevance of Analytics and fit of answers to big picture, access of the data/insights whenever and wherever required, ease of understanding of the insights and finally the trust around the numbers.  
,
,
,
,Some of the above mentioned gaps will be addressed with the Technology getting better (speed, delivery and access) but others require proactive leadership from the Analytics world (relevance, fit and ease of understanding). We need to hire right skills     (UX and Storytellers); train analysts to think backwards from the end goal/handle ambiguous questions and apply best Project Management principles (Iterative learning, Feedback from End Users, Analytics on ?€?Analytics Practice?€?). But to make it all work, we also need to educate our Customers & manage expectations (set and exceed). 
,
,
,
,Depending on the problem being solved, the methodology varies (Data, Business or Advanced) and so do the data (API logs, Clickstream, Transactions, etc.)  - which means I use any or all of the tool kits (Microstrategy, Tableau, MS Office, Google Analytics, SAS, SQL, Hadoop Hive, Power BI, Optimizely, etc.) depending on the need.
,
,
The views/opinions/ideas in the interview is purely on personal basis and not representing VISA in any form or matter. It is based on learnings from work across industries and firms. Care has been taken to ensure no proprietary or work related info of any firm is used in any material.
,
,
,
, is a software development intern at Salesforce. He is a former MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for July 14, 2015 and later.
,
See full schedule at , .
,
,  "
"
Most popular 
, tweets for Jul 07-13 were
,
#DeepLearning and the Triumph of Empiricism , #MachineLearning 
, 
,
,
,
#DeepLearning and the Triumph of Empiricism , #MachineLearning 
, 
,
,
#DeepLearning and the Triumph of Empiricism , #MachineLearning 
, 
,
,
#DeepLearning and the Triumph of Empiricism , #MachineLearning 
, 
,
,
,  "
"
,
,
,
,
,
,
,
,
,
,
  "
"
,
By Gregory Piatetsky,  
, .
,
,
,
The results of latest KDnuggets Poll:
, show a surprisingly high level of stability among users of R and Python, with interesting 2nd-level flows which we analyze below.
,
The , indicated that data scientists use on average 4.8 tools, with R and Python being among most popular tools.
,
We also looked at 
,
and found that R and Python are used together 41% more frequently than indicated by chance. 
,
In this poll: 
,, 
we wanted to find out which language was primary and whether there were significant changes
,
,
,.
,Circle sizes correspond to percentage of voters who chose that language as primary in 2014. Among those who chose None in 2014, 55% switched to R, 15% to Python, 7.5% to Other, and 22.5% stayed with none, but the last 3 arrows were omitted from the graph for comprehensibility.
,
Compared to 
,, 
the 2015 results show much higher stability - about 88% of R users in 2014 stayed with R and 91% stayed with Python. Percentage of primary R and primary Python users have grown, while percentage of users who chose Other or None have declined.
,
We do observe that Python has a stronger inflow than outflow from both R and Other language users.
,
regional participation was
,
,??,
,
,.
,. Green arrows indicate significant increase, black arrows significant decline.
,
Looking by region, we note significant regional differences.
,
,??,
,
Here is the table with votes.
,
,
,
,
,
,
,
,
 ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
As Director of Analytics at ,, , is responsible for helping the Leadership & Stakeholders with actionable insights derived from Analytics. The business questions he works on span the whole spectrum across the Product, Marketing, Sales and Relationship. His team leverages any of the various options, i.e., Strategic analysis, Advanced Analytics, Text Analytics or Mining depending on the problem being solved. 
,
,
,
Here is second and last part of my interview with him:
,
,

,
,The advice that has defined my career was from my first manager at Modelytics - Girish Subramanian. He said ?€?Never ever make your customer work ?€? your output should be exactly what the customer needs and can use directly. For that you need to think from Customer?€?s perspective, analyze from the end goal and deliver in the format that they need?€?. 
,
Yes, it took me time, learning and practice to be able to fully deliver on that but today more than ever I realize the value of those words and how the ?€?Customer Service?€? mindset helps one become a better person and professional.
,
,
,
,Over the past few years there has been a ?€?genuine?€? revolution in data and analytics space not only in the awareness of the value add from data and insights but also in the way what is captured, processed, analyzed but also in the way it is consumed.
,
,??,
The biggest disruptor will be the ?€?Internet of Things?€? world with its own nuanced challenges (from Data Instrumentation to analyses to actions), but the learnings from today will help us evolve far quicker in that world. 
,
Investments are being made, Innovations are being done to make them scalable/fast/efficient and integrations done to make all this enterprise ready.

,
,
,
,With the operating philosophy of ?€?Customer Success?€?, the fundamental need is for the person to be passionate about solving Customer?€?s problems. Going the extra mile ?€?is not desired, but a mandatory requirement?€?. 
,
We require the person to understand the end goal vs. the request from the Customer, suggest efficient and effective solution by choosing among the various data points and/or analytical methods available and deliver in a format that the Customer will understand and can act on right away. Also we would love the person to help us become a better team by constantly innovating and reinventing him/herself and the processes/products/technology/culture. 
,
,
,
,The last book I read and loved was ?€?,?€? by George Anders. ,It is a wonderful read on how crucial ?€?right?€? hiring is (any level, any organization). Reading it helped me understand the hiring is not just an HR task and it cannot be just ?€?Job Description?€? based, i.e., years of experience, education, skillsets and level based. It is much more than that ?€? integration with the culture, attitude and vision of the company. 
,
The key message that stuck with me was that we have to make ?€?talent find process?€? iterative, cost efficient and quick ?€? we have to constant adapt with changing times and needs. This becomes especially important when hiring for ?€?new?€? needs where we don?€?t have history to learn from.
,
,
The views/opinions/ideas in the interview is purely on personal basis and not representing VISA in any form or matter. It is based on learnings from work across industries and firms. Care has been taken to ensure no proprietary or work related info of any firm is used in any material.
,
, is a software development intern at Salesforce. He is a former MDP Fellow and graduate mentor at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
        ,
,.
,
,
,
Until recently, nearly any input could fool an object recognition model.
We were more surprised when object recognition worked than when it didn't.
Today, object recognition algorithms have reached human performance as
measured by some test set benchmarks, and we are surprised that they
fail to perform as well on unnatural inputs.
,
are synthetic examples constructed
by modifying real examples slightly in order to make a classifier believe
they belong to the wrong class with high confidence.
, examples
(such as ,) are pathological
examples that the model assigns to some class with
high confidence even though they should not belong to any class.
,
,
,
,The modification is performed on 32-bit floating point values used as input to the network, and is so small that it does not change the 8-bit representation of the image used for publication. See , for details.
,
These mistakes have captured the public imagination. In the excitement,
some misconceptions about adversarial examples have become widespread.
In this blog post, I address some of these misconceptions.
,
,
,
,
In conclusion, adversarial examples are a recalcitrant problem,
and studying how to overcome them could help us to avoid potential
security problems and to give our machine learning algorithms a
more accurate understanding of the tasks they solve.
,
Bio: , is a Research Scientist at Google. 
He received Ph.D. in machine learning in 2014 from U. Montreal where he was in Yoshua Bengio group, and BS/MS from Stanford.
,
,
 ,  "
"
,Sydney, 14 July 2015 - International and local analytics professionals will be at the ready to answer challenging career questions when the Institute for Analytics Professionals of Australia (IAPA) holds their inaugural 
,
event for young analysts and data scientists looking for advice on managing their careers.
,
The two-hour event is part of the 
,
and ADMA Advancing Analytics event on Monday 4 August at Sydney's Hilton Hotel. Challenge the Chief runs from 1.45pm-3.45pm and is designed for those studying analytics and analysts with one to three years' experience under their belt.
,
, will feature seven international and local analytics professionals who can answer the tough questions and offer career guidance. At the table:
,
,??,
,
,
The format is one chief per table with seven guests at each table. Like speed dating, participants get 15 minutes to challenge their particular chief with career questions before moving to the next chief's table. 
,
Tickets cost $40 and participants also get a free pass to attend the final keynote sessions of the IAPA National Conference and networking drinks. 
,.
,
To learn more or to purchase tickets to the IAPA National conference, visit 
,
and join the conversation on Twitter 
,
,
,
,The Institute of Analytics Professionals of Australia (IAPA) is the professional organisation for the analytics industry in Australia, incorporating business analytics and data mining across multiple disciplines and sectors. Its mission is to unite, inform, support and promote analytics professionals in Australia. The 3,500 plus IAPA members represent a diverse range of industry verticals and brands including financial services, government, utilities, retail/FMCG and telecommunications as well as leading analytical consultants and software providers. The IAPA community holds regular meet ups in six cities. IAPA entered into a formal alliance with the Association for Data-driven Marketing and Advertising (ADMA) in 2014.
,
Media contact:
,Kim Carter/ADMA PR
,(02) 9277 5412 (Mon-Wed), 0407 771 698 (Thurs)  "
"
Here are upcoming webcasts on Analytics, Big Data, Data Science and Data Mining for Jul 21 and later.
,
See full schedule at , .
,
,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
My monthly summary of the company, startup, and acquisition activity for Jun 2015 from 
,.
See the latest under hashtag
,.
,
Here are KDnuggets tweets, sorted by decreasing number of engagements,
,
,??,
Here are previous month activities
,
  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
By Gregory Piatetsky,  
,.
,
KDnuggets posts are assigned to one category, like 
,, 
,,
,, 
,, etc. 
,
I was recently struggling to assign a recent post to the right category.  It was not quite an opinion, and not quite about a publication, and not quite a software post.
,
,
Then I realized that there is a perfect category for this post.  In fact it was already the most popular category on KDnuggets - only it needed to be created: 
,.
,
I hope that having this category will also lead to more 
submissions of interesting tutorials, overviews, and how-tos, to share with our audience of analytics, Big Data, data mining, and data science professionals, students, and researchers, over 150K unique monthly visitors and over 70,000 subscribers/followers.   Email to 
editor1@thissite.com  (replace thissite with kdnuggets). 
,
I reclassified June posts that had the best fit for that category - here they are
,  "
"
Most popular 
, tweets for Jul 14-20 were
,
R remains primary #Analytics language, but #Python grows faster #DataScience #DataMining 
, 
,
,
,
50+ #DataScience and #MachineLearning #CheatSheets: R #rstats #Python #SQL #Hadoop #Spark 
, 
,
,
,
4 Companies Using #BigData Successfully: CapitalOne, T-Mobile, Free People, Starbucks 
, 
,
,
,
The Master #Algorithm: How the Quest for the Ultimate #Learning #Machine Will Remake Our World 
, 
,
,
,
,  "
"
,
is pleased to announce that Jian Pei is the winner of its 
,
for his significant technical contributions to the principles, practice and application of data mining and for his outstanding services to society and the data mining community.
,
ACM SIGKDD Service Award is the highest service award in the field of knowledge discovery and data mining (KDD). It is conferred on one individual or one group for their outstanding professional services and contributions to the field of knowledge discovery and data mining.
,
,  has a long history of contributing to the frontier of data mining research and serving the data mining community. As one of the most cited authors in data mining, Pei is one of the key organizers of many KDD and data mining conferences and events, such as ACM KDD, IEEE ICDM, SIAM Data Mining, and ACM CIKM, in various roles, such as general co-chair, program committee co-chair, tutorial co-chair, workshop co-chair, and senior program committee member.  
,
He is the current editor-in-chief of the IEEE Transactions on Knowledge and Data Engineering, and is serving or served as an associate editor of several premier academic journals in the general fields of data mining, data science, and data analytics, such as the ACM Transactions on Knowledge Discovery from Data, Knowledge and Information Systems, and Data Mining and Knowledge Discovery.  He is currently serving as a director of ACM SIGKDD. Carrying his vision and experience from his productive research, Pei sticks to the highest professional standard in his services to the society and the community, and always advocates new ideas and emerging directions.
,
Pei is dedicated to promoting data mining and data science.  He contributed many tutorials, lectures, and short courses to both the data mining community and the broader communities interested in data mining. He has trained over 40 graduate students and other highly qualified personnel.  Many of his students are active in industry and academia.  Close to 50% of his students are female.
,
Pei's outstanding professional services have been concretely built on his prolific research.  He is responsible for a series of fundamental data mining methods, such as FP-growth and PrefixSpan, which have been covered by popular data mining textbooks, taught in undergraduate and graduate data mining courses, adopted by industry, and implemented in many popular open source data mining toolkits. Since 2000, Pei has published over 200 technical contributions, which have been cited more than 50,000 times.
,
Pei received a B.Eng degree and a M.Eng degree from Shanghai Jiao Tong University, China, and a Ph.D. degree in Computing Science from Simon Fraser University, Canada.  He is currently a Canada Research Chair in Big Data Science, a Professor in the School of Computing Science and an associate member of the Department of Statistics and Actuarial Sciences at Simon Fraser University.  He also holds the courtesy appointments as a Chao Kuang-Piu Chair Professor at Zhejiang University and a Distinguished Adjunct Professor at King Abdulaziz University.  He is a Fellow of IEEE and a Senior Member of ACM.  He received several prestigious awards, including the 2014 IEEE ICDM Research Contributions Award.
,
The thirteen previous SIGKDD Service Award winners have been: Gregory Piatetsky-Shapiro, Ramasamy Uthurusamy, Usama Fayyad, Xindong Wu, The Weka team, Won Kim, Robert Grossman, Sunita Sarawagi, Osmar R. Zaiane, Bharat Rao, Ying Li, Gabor Melli, and Ted Senator.
,
The award includes a plaque and a check for $2,500 and will be presented during the Opening Plenary Session of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2015), on Monday August 10th in Sydney, Australia.
,
, (in alphabetical order):
,
,
,
,  "
"
, is pleased to announce that Hans-Peter Kriegel is the winner of its 
,.  He is recognized for his influential research and scientific contributions to data mining in clustering, outlier detection and high-dimensional data analysis, including density-based approaches.
,
The ACM SIGKDD Innovation Award is the highest award for technical excellence in the field of Knowledge Discovery and Data Mining (KDD). It is conferred on one individual or one group of collaborators whose outstanding technical innovations in the KDD field have had a lasting impact in advancing the theory and practice of the field. The contributions must have significantly influenced the direction of research and development of the field or transferred to practice in significant and innovative ways and/or enabled the development of commercial systems.
,
,
has been a Professor of Informatics at Ludwig-Maximilians-Universitaet Munich, Germany for over since 1991. He has published over a wide range of data mining topics including clustering, outlier detection and high-dimensional data analysis. In 2009 the Association for Computing Machinery (ACM) elected Professor Kriegel an ACM Fellow for his contributions to knowledge discovery and data mining, similarity search, spatial data management, and access-methods for high-dimensional data. He received the 
,
for his influential contributions to the field of data mining. So far, his more than 450 publications have been cited more than 35,000 times according to Google Scholar. Microsoft Academic Search currently ranks him Number 5 for field rating in the field of data mining.
,
Professor Kriegel's ground-breaking contribution to the data mining field was his paper at the 1996 KDD Conference titled 
,
(DBSCAN) with co-authors Martin Ester, Joerg Sander and Xiaowei Xu, with more than 7,000 citations in Google Scholar. This paper received the 2014 SIGKDD Test of Time Award. It also led to other density-based approaches such as OPTICS (Ordering Points To Identify the Clustering Structure, SIGMOD 1999) and LOF (Identifying density-based Local Outliers, SIGMOD 2000), both with more than 2,000 citations. His more recent work on clustering as well as on outlier detection in high-dimensional data was presented in numerous tutorials at IEEE ICDM and ACM SIGKDD, among other venues. This research has been done together with his team members Peer Kroeger, Erich Schubert and Arthur Zimek.
,
More recently, Professor Kriegel has been applying some of his work on clustering and outlier detection in interdisciplinary cooperations with other sciences such as archeology, biology, engineering and medical science as well as in industrial collaborations.
,
The previous SIGKDD Innovation Award winners have been: Rakesh Agrawal, Jerome Friedman, Heikki Mannila, Jiawei Han, Leo Breiman, Ramakrishnan Srikant, Usama M. Fayyad, Raghu Ramakrishnan, Padhraic Smyth, Christos Faloutsos, J. Ross Quinlan, Vipin Kumar, Jon Kleinberg, and Pedro Domingos.
,
The award includes a plaque and a check for $2,500 and will be presented at the Opening Plenary Session of the 
,, on Monday August 10th in Sydney, Australia. Prof. Kriegel will present the Innovation Award Lecture immediately after the awards presentations.
,
, (in alphabetical order):
,
,
,
,  "
"
,
,Chi Wang, Microsoft Research
,Jiawei Han, University of Illinois at Urbana-Champaign
,
Paperback ISBN: 9781627056601, $60.00
,eBook ISBN: 9781627056618
,March 2015, 159 pages
,
,
,
,:
,
The ""big data"" era is characterized by an explosion of information in the form of digital data collections, ranging from scientific knowledge, to social media, news, and everyone's daily life. Examples of such collections include scientific publications, enterprise logs, news articles, social media, and general web pages. Valuable knowledge about multi-typed entities is often hidden in the unstructured or loosely structured, interconnected data. Mining latent structures around entities uncovers hidden knowledge such as implicit topics, phrases, entity roles and relationships. 
,
In this monograph, we investigate the principles and methodologies of mining latent entity structures from massive unstructured and interconnected data. We propose a text-rich information network model for modeling data in many different domains.
,
This leads to a series of new principles and powerful methodologies for mining latent structures, including 
,(1) latent topical hierarchy, 
,(2) quality topical phrases, 
,(3) entity roles in hierarchical topical communities, and 
,(4) entity relations. 
,
This book also introduces applications enabled by the mined structures and points out some promising research directions.
,
Table of Contents: 
,
,??,
,.
  "
"
        ,
,
,
,
,

,
,
,
,
,  "
"
,

,

,

,

,

,

,

,

,
,

,

,
,

,

,

,
,

,

,


,


,
, is a PhD student in the Computer Science Engineering department at the University of California, San Diego. Funded by the ,, he is interested in both theoretical foundations and applications of machine learning. In addition to his work at UCSD, he has interned at Microsoft Research Labs.

,
,
,

,
,
,
  "
"
by Seth Grimes, 
,.
,
This year's 
, is a dream event for data scientists and analysts working in text, social, and emotion analytics.
,
The 2015 symposium, July 15-16 in New York, features parallel Presentation and Workshop tracks. Workshops include a half-day tutorial on Mining Opinion, Sentiment, and Emotion, taught by leading researcher Prof. Bing Liu, and workshops on Google's Word2Vec for Sentiment Analysis, Cognitive Computing, Natural Language Generation, Social Media Metrics and Measurement, and Motivational Insights and Marketing Strategy.
,
Visit , for the full agenda. Register for either day or both and save 10% via the KDNUGGETS code. Government/academic attendees save 50%, and a low student rate is available.
,
SAS15 presentations cover:
,
,??,
Instagram engineer Thomas Dimson will speak on the semantics of emoji (""Emojineering @ Instagram"") and Bottlenose's Bethany Bengtson will cover online sentiment in the fashion industry. These and other presentations will cover leading-edge application of NLP, machine learning, and analytical techniques.
,
The program was curated by ,, the leading industry analyst covering text analytics and sentiment analysis.
,
,
A half-day workshop segment on Sentiment Analysis for Financial Markets with speakers who include Vika Abrecht from Bloomberg's machine learning R&D team and Eiji Hirasawa from Japan's NIKKEI Group on Financial News Analysis.
,
The 2015 Sentiment Analysis Symposium takes place July 15-16 at the New York Academy of Sciences. 
,
Don't miss it: ,. 
,
Don't forget to use ,!  "
"
,
,
,
,
,

,
,
,
,
,
,
,
,

,
,
,
,
,
,
,

,
,
,
,
,
,
,
,

,
,
,
,
,

,  "
"
, on Coursera
,
,
,
,
,Sep 12 - Oct 31, 2015
,
Instructors
,
,??,
,
,
We introduce the participant to modern distributed file systems and MapReduce, including what distinguishes good MapReduce algorithms from good algorithms in general.  The rest of the course is devoted to algorithms for extracting models and information from large datasets.  Participants will learn how Google's PageRank algorithm models importance of Web pages and some of the many extensions that have been used for a variety of purposes.  
,
We'll cover locality-sensitive hashing, a bit of magic that allows you to find similar items in a set of items so large you cannot possibly compare each pair.  When data is stored as a very large, sparse matrix, dimensionality reduction is often a good way to model the data, but standard approaches do not scale well; we'll talk about efficient approaches.  Many other large-scale algorithms are covered as well, as outlined in the course syllabus.
,
,
,There is a free book ""Mining of Massive Datasets, by Leskovec, Rajaraman, and Ullman (who are the instructors for this course).  
You can download it at 
,  
,
Hardcopies can be purchased from Cambridge Univ. Press.
,
Enroll at ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,

,  "
"
,
,
Behind the immense excitement around Cloud, Big Data and IoT is a dark fear of decreasing Digital Trust. With the rapid pace of technological innovations, an unprecedented amount of customer information is being collected, stored, and processed for various needs. While there is no doubt that convenient access to this plethora of information is the breeding ground for innovative ideas ranging across all aspects of life, this integrated data is a sweet spot for hackers to make the most from their malicious activities.
,
,recently released a report ?€?,?€? summarizing results from the 2015 Accenture Digital Consumer Survey on consumer aspirations towards the security of their personal data over Internet. The report states: ?€?As the Internet of Things (IoT) unleashes an exponential jump in the data businesses have on consumers, digital trust is the gateway to monetizing its value. Companies that can establish and maintain trust will have defensible differentiation that allows them to convert data into growth instead of massive risk.?€? Accenture identifies four areas as keys to digital trust: security, privacy, benefit/value and accountability.
,
The survey results show that by 2020, nearly half of consumers will own a connected IoT device. The demand is expected to be the strongest for home cameras and security, smartwatches, and fitness devices, followed by in-vehicle entertainment, personal health monitors and 3-D printers.

,

We have been witnessing security breaches over a long time, however, it is simply frightening to see how the scale and impact of these breaches has grown manifold over the recent years. It is particularly worrisome that some of the top brands and large corporations have also not been immune to data breaches.
,
Extrapolating the current rate of data breaches, the report estimates that during 2014-2015, around 900 million consumers will have been impacted by data breaches, with some consumers affected in multiple instances. This number is more than double of what was observed for previous two years, 2012-2013 ?€? 393 million consumers impacted.

,

More than half of the consumers do not trust the security of their personal data on the Internet. Across geographies, the lowest confidence in the security of their personal data over Internet was shown by consumers in Western Europe. Across age groups, generally the older groups showed lesser confidence on the security of personal data over Internet.
,
The report also found staggering differences with regards to benefit/value (offering value in exchange for some personal information) across age groups. Teens are twice as likely to exchange personal data for some non-monetary incentives, such as free service or loyalty points, compared to older consumers.
,
In the absence of good, reliable information on whom they can trust, the consumers are generally inclining towards trusting major brands and companies specializing in a specific type of device (such as Fitbit). Also, consumers have a greater trust on their current mobile phone manufacturers (such as Apple, Samsung, etc.) compared to the telecom providers.
,
The report adds. ?€?In the connected ecosystem, companies no longer have to worry just about the security of their own data but also the risk associated with data breaches in companies with whom they are linked. Perhaps most importantly, they will be unable to reap the benefits if consumers don?€?t trust the entire value chain in which they operate.?€?

,

The increasing security awareness among consumer has led to significant changes in the attitude towards adopting newer, better security measures. More than three-quarters of consumers are interested in trying alternatives to username/password based security mechanisms. Around 60% of consumers find usernames and passwords as too cumbersome to use, and thus, are willing to try biometrics based authentication for convenient and secure access to their personal data.
,
IoT proliferation will unveil datasets that are far bigger and far more personal. Thus, trust is at the very core of IoT?€?s anticipated success. The report concludes with the following three action items towards building Digital Trust:
,
Survey vital facts: The survey was conducted online between October and November 2014 with 24,000 consumers (age range: 14 ?€? 55+) in 24 countries across six continents.
,
For details, refer:
,
,
,
,
,
, is a software development intern at Salesforce. He is a MDP Fellow and a graduate mentor for IoT-SURF 2015 at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,

,  "
"
,

,

, is an enterprise architect, technologist and innovator with over 15 years of progressive experience specializing in building large, highly scalable software systems. At ,, Thanigai is the lead architect responsible for defining and driving the technology roadmap initiatives for building the next generation technology vision and platform for the company. Thanigai?€?s interests and specialties include Hadoop/Big data, NoSQL, Distributed Systems, Enterprise Architecture, Scalability, etc. Prior to joining Art.com, Thanigai has worked in engineering roles at Sanmina and Flextronics.
,
Here is my interview with him:
,
,

,

, is a leading online retailer for wall art. We have the world?€?s largest selection of hand-curated art images for wall-art providing over 3 million art images from different publishers. We have a portfolio of 5 brands that are global and have a strong international presence.

,

Our mission is , ?€? and we rely on a data-driven culture for understanding our users and all aspects of our business. So, data science and analytics play a vital role in key decisions and how we shape our business in fulfilling that mission.

,
,

,

,At Art.com, we have a heterogeneous technology stack seamlessly integrated thanks to the adoption of a Service Oriented Architecture. In addition, we are also constantly working on evolving and upgrading the stack to leverage latest technologies. As a result, ,we have to deal with multiple technologies across the stack. In the data-tier, we use different database technologies to support different workloads and use cases. We use SQL Server for transactional applications and we use MongoDB for document-oriented storage where we need to store hierarchical objects. We also use HBase to store search related metadata and other application. Also, our ERP runs on Oracle. These are some of the major components of our polyglot data architecture.

,

,

,

,We had couple of use cases that drove us towards Hadoop. The first use case was to implement a next generation search/discovery engine that had to be updated in near real time and powered by ML algorithms. The second use case was to be able to build a large-scale ?€?clickstream analytics?€? platform for our websites. These two use cases demanded the need for a distributed computing platform (like Hadoop) to handle and process large volumes of data.

,

We went with the Cloudera?€?s distribution as they had a proven track record and also offered support for components like the Lily HBase Indexer and Search (SOLR). In addition, Cloudera?€?s EDH provides data governance capabilities such as allowing complete metadata management, audit logging and reporting.

,

,

,

,Conventional discovery experiences on ecommerce sites are typically taxonomy based and at art.com we have a state of the art catalog search engine that supports poly-hierarchy and targeted ranking algorithms. However, we wanted to build a visually engaging experience ,that is contextually and semantically relevant and not tied to a typical parametric search experience. So, the goal was to build semantically related clusters for all searches and categories.

,

Another important aspect of the engine requirements was that it should be ?€?demand-based?€? and it needs to weigh the different facets of search metadata based on search demand. In addition, we also wanted to make sure that the search indexes are updated in near ?€?real-time?€? as we collect new metadata on searches. So, we analyzed these key aspects of the engine and logically divided the engine into the following sub-components:
,
, ?? ,

,
,
,Avro provides a data format designed to support data-intensive applications and is widely supported throughout the Hadoop ,ecosystem. Also, with Avro we can read and write data in a variety of languages like Java, C, Python, etc. which is shipped by default. However, we wanted a format that works well with .NET and Node.js and we found good library support for Avro. A key reason for choosing Avro is the ability to do seamless schema evolution. The Avro format is self-describing as the schema is stored with the data, which allows for schema evolution in a scalable manner and this was very important for us. Avro compresses really well and can lead to significant storage savings for data such as clickstream or traffic data.
,
Regarding cons, Avro is a row based storage format ?€? this can be inefficient when you need to retrieve only specific columns or fields ?€? in those cases, a column based storage format (like Parquet) is a better choice.
,
,
,
, is a software development intern at Salesforce. He is a former MDP Fellow and a graduate mentor for IoT-SURF at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
,

,

As sensors and telematics systems become more common, information that can be collected about vehicle usage and driver behavior is becoming increasingly available and accessible. Everyone in the automotive value chain is interested in gathering information and understanding driver behavior but there is still a lot of uncertainty about how to use the data responsibly.
,

Ford Motor Company engineers recently completed a real-world driving experiment with HP, discovering which commuting commonalities could provide future breakthroughs for better managing fleets, personalized services and recommendations for individual drivers.
,
,
The??,one of 25 Ford Smart Mobility experiments, is designed to better understand how driving behavior is changing. The experiment tracked 100 HP fleet vehicles and drivers via a device plugged into the car and leveraged ,and the HP Haven Big Data platform to gather and analyze data to determine possibilities for lowering operating costs and optimizing underutilized vehicles for fleets as well as personal driving.
,
Using the??,??analytics engine, part of the HP Haven platform, Ford data scientists and IT leaders were able to explore patterns and multiple dimensions of fleet driver activity. The data was sent to the cloud where location details, vehicle status, and trip specifics were used to create mobility profiles and analyze fleet optimization. The goal? To improve the efficiency and provide new services for both the company fleet manager and the drivers.
,
Efficiency optimization is critical for fleet operators. The project found that managers could swap vehicle types for task-based needs or use parked company vehicles, instead of renting a car at airports, when the driver is out of town. Additional observations during the experiment included:
,
,
Drivers may have even more to gain: quicker coffee runs, cheaper gas, and faster routes. Resulting in a more efficient day and less of a headache. The study created mobility profiles using four questions:
,
In the future, the patterns observed could allow vehicle systems to notify friends, family, and co-workers of a driver?€?s location and estimated time of arrival. It could also suggest restaurants, fun activities that passengers might enjoy and assisting in time-planning by linking private and business calendars with expected traffic and weather.
,
The trips fell into four groups:
,
,
HP and Ford share a common vision around bringing together data, mobility, and analytics to explore new ways to deliver better customer experiences, new revenue streams, and lower fuel and maintenance costs in the automotive industry. The results of this experiment can help unleash improvements for business operations for fleet management and personal driving experiences. As the project continues throughout 2015, the conclusions are likely to shed more light on how fleet solutions can benefit companies and individuals.
,
The Big Data Discovery Experience is not only assisting in improving HP?€?s fleet efficiency for the future, but also will help give employees a quick kick of caffeine to improve their bumper-to-bumper blues.

,
, is Senior Vice President and General Manager of the Analytics & Data Management Practice (A&DM) organization within HP?€?s Enterprise Services business. This organization delivers solutions that address the most challenging problems companies face in managing information and data assets. A&DM provides services to handle both structured and unstructured data, leveraging ?€?big data?€? to drive business outcomes for clients.

,
,
,  "









"
        ,  "
"
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
  "
"
,
,
, is an enterprise architect, technologist and innovator with over 15 years of progressive experience specializing in building large, highly scalable software systems. At ,, Thanigai is the lead architect responsible for defining and driving the technology roadmap initiatives for building the next generation technology vision and platform for the company. Thanigai?€?s interests and specialties include Hadoop/Big data, NoSQL, Distributed Systems, Enterprise Architecture, Scalability, etc. Prior to joining Art.com, Thanigai has worked in engineering roles at Sanmina and Flextronics.

,
,
,
Here is second part of my interview with him:

,

,
,
,When working with polyglot architectures, we are constantly evolving and optimizing the stack in not only ensuring that we use the right tool (framework, platform, language, etc.) for the right application but also in making sure that they integrate well with each other. It is very important that every component in the stack is integrated into your software development lifecycle processes ?€? everything from staffing, interoperability, devops, continuous integration, etc.

,

In addition, choosing the right data format and API interfaces can make the integration easier. Another key factor is to always make a balanced trade-off between using the best tool versus making sure that it aligns with your long-term technology roadmap.

,

,

,

,I think one of most underrated challenges of working with Big Data is ?€?Governance for Data Quality?€?. With Big Data technologies, we are increasingly processing different types of data (unstructured, structured, media, etc.) using ELT patterns based on co-located data. In most cases, there is no fixed schema on the data as the schema is applied dynamically during reads and writes. Thus, it becomes very important to apply proper governance around the processing algorithms that exposes these views of the data, considering that both data and the processing algorithms are constantly evolving.
,
,
,

,

,

,Data is one of the key assets for any business. Big Data technologies allow the organizations to untap, collect and process this data at scale that helps in unearthing powerful insights that can transform the business. I would provide the below recommendations to make ?€?Big Data?€? a success within an organization:
,
, ?? ,

,

,

,We are seeing a gradual trend towards convergence of real-time and batch oriented systems. Patterns like the lambda-architecture are being increasingly adopted to build systems that are fault tolerant and serve a wide range of use cases and workloads. In addition, I think we will see a big push towards more memory-centric distributed computing technologies in the coming years. Recent developments in Tachyon and Apache Spark seem really promising.

,
,

,

,Actually, the best career advice that I got was from my mom! She said ?€?Hard work and persistence pays off ?€? eventually?€? and I think that is very true and was certainly applicable in my case.

,

,

,

,The main skills that I look for when interviewing candidates (for data engineering) is their understanding on fundamentals related to data collection, processing, analysis and transformation. I think learning a particular technology/stack is much easier when your fundamentals are strong. In addition, I also look for analytical and problem-solving skills that can be applied to any domain.

,

,

,

,I recently read ?€?Thinking, Fast and Slow?€? by Daniel Kahneman ?€? a must read for anyone interested in understanding human behaviors and how certain biases affect the decisions we make in our daily lives.

,
, is a software development intern at Salesforce. He is a former MDP Fellow and a graduate mentor for IoT-SURF at UCI-Calit2. He has presented his research work at various conferences including IEEE Big Data 2013. He is currently a graduate student (MS, Computer Science) at UC, Irvine.
,
,
,  "
"
        ,  "
