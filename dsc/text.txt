text
Data scientists are in notoriously high demand, so when your company is ready to make the leap into big data, it pays to understand how to tell if you?€?re getting a good one.,??,Because of the vast amounts of money at stake with some big data projects, every data scientist wants you to believe that he or she is the kind of genius that can tease industry-changing information from a set of numbers and some code. And some can. ??But some can?€?t.,??,If you?€?re ready to hire a data scientist for your project or organization, there are some important questions to ask to make sure you get the right person for the job:,??,If you are a data scientist or have hired one for your company, what other traits would you add to the list? What differentiates a good data scientist from a mediocre one? I?€?d love to hear your thoughts in the comments.,??, , , , , ,??
??,??,??,??,??,??,??,??,??, ??
 1,Real Versus Fake Data Science 2,The Data Scientist 9,Data Science Applications in 13 Real-World Scenarios 13,Data Science History, Pioneers, and Modern Trends 30,Summary 39, 41,Two Big Data Issues 41,Examples of Big Data Techniques 51,What MapReduce Can?€?t Do 60,Communication Issues 63,Data Science: The End of Statistics? 65,The Big Data Ecosystem 70,Summary 71, 73,Key Features of Data Scientists 73,Types of Data Scientists 78,Data Scientist Demographics 82,Training for Data Science 82,Data Scientist Career Paths 89,Summary 107, 109,New Types of Metrics 110,Choosing Proper Analytics Tools 113,Visualization 118,Statistical Modeling Without Models 122,Three Classes of Metrics: Centrality, Volatility, Bumpiness 125,Statistical Clustering for Big Data 129,Correlation and R-Squared for Big Data 130,Computational Complexity 137,Structured Coefficient 140,Identifying the Number of Clusters 141,Internet Topology Mapping 143,Securing Communications: Data Encoding 147,Summary 149, 151,Data Dictionary 152,Hidden Decision Trees 153,Model-Free Confidence Intervals 158,Random Numbers 161,Four Ways to Solve a Problem 163,Causation Versus Correlation 165,How Do You Detect Causes? 166,Life Cycle of Data Science Projects 168,Predictive Modeling Mistakes 171,Logistic-Related Regressions 172,Experimental Design 176,Analytics as a Service and APIs 178,Miscellaneous Topics 183,New Synthetic Variance for Hadoop and Big Data 187,Summary 193, 195,Stock Market 195,Encryption 209,Fraud Detection 216,Digital Analytics 230,Miscellaneous 245,Summary 253, 255,Job Interview Questions 255,Testing Your Own Visual and Analytic Thinking 263,From Statistician to Data Scientist 268,Taxonomy of a Data Scientist 273,400 Data Scientist Job Titles 279,Salary Surveys 281,Summary 285, 287,Professional Resources 287,Career-Building Resources 295,Summary 298, 299
This reference was first posted ,??by??,, and several authors contributed.,Here we provide a summary:??,??on DSC.
Big Data, Data Sciences, and Predictive Analytics are the talk of the town and it doesn?€?t matter which town you are referring to, it?€?s everywhere, from the , as the first chief data scientist to the , to forecast bombings on schools. There are dozens of Startups springing out every month stretching human imagination of how the underlying technologies can be used to improve our lives and everything we do. Data science is in demand and its growth is on steroids. According to Linkedin, ?€?Statistical Analysis?€? and ?€?Data Mining?€? are two top-most skills to get hired this year. Gartner says there are 4.4 million jobs for data scientists (and related titles) worldwide in 2015, 1.9 million in the US alone.?? One data science job creates another three non-IT jobs, so we are talking about some 13 million jobs altogether. The question is what YOU can do to secure a job and make your dreams come true, and how YOU can become someone that would qualify for these 4.4 million jobs worldwide.,There are at least 50 data science degree programs by universities worldwide offering diplomas in this discipline, it costs from 50,000 to 270,000 US$ and takes 1 to 4 years of your life. It might be a good option if you are looking to join college soon, and it has its own benefits over other programs in similar or not-to-so similar disciplines. I find these programs very expensive for the people from developing countries or working professionals to commit , years of their lives.,Then there are few very good summer programs, fellowships and boot camps that promise you to make a data scientists in very short span of time, some of them are free but almost impossible to get in, while other requires a PhD or advanced degree, and some would cost between 15,000 to 25,000 US$ for 2 months or so. While these are very good options for recent Ph.D. graduates to gain some real industry experience, we have yet to see their quality and performance against a veteran industry analyst. Few of the ones that I really like are ,, ,, ??,, , and the famous ,.,Let me also mention few paid resources that I am a fan of before I tell you how to do all that for free. First one is the , program by Booz Allen, it costs 1,250 $ but worth a single penny. Second one is recorded lectures by Tim Chartier on DVD, called ,, it costs 80 bucks and worth your investment. The next in the list are two courses by MIT, ,, that costs 500$ and provides you a very solid theoretical foundation on big data, and ,, that costs only 100 bucks and gives a superb introduction on how the analytics can be used to solve day-to-day business problems. If you can spare few hours a day then Udacity offers a perfect , that costs 200$/month can be completed in 6 months or so, they offer this in partnership with Facebook, Zipfian Academy, and MongoDB. , has a wonderful program for 500$/month to connect you live with a mentor to guide you to become a data scientist.,Ok, so what one can do to become a data scientist if he/she cannot afford or get selected in the aforementioned competitive and expensive programs. What someone from a developing country can do to improve his/her chances of getting hired in this very important field or even try to use these advanced skills to improve their own surroundings, communities and countries.,Here is my cheat sheet of becoming a Data Scientist for Free:,??,??,The whole list will take 3 to 12 months to complete and will cost you absolutely nothing, and I can guarantee you that with this skills set you really have to try very hard to remain jobless. Even if you complete half of it, send me a note and I will have something ready for you.,Ball is in your court, it doesn?€?t matter where you are and how much you can afford, if you want to make at least four times higher the average income of your countrymen, this is the way to do it, at least for next 10 years (where we will be generating 20 TBs of data per year per person versus 1 TB of data per year per person in the last 10 years.),??,I will write separate articles on Data Science Books (I?€?ve read 127 of those in last six months) and MOOCs (I am celebrating my 25, MOOC certification today).,For everyone else data sciences is an opportunity, for me it?€?s a passion,I tweet at 
This infographic was originally published by Intel and can be found ,?? It dates back to 2014 but still provides a very comprehensive view of this fast expanding field.
Have you noticed how many people are suddenly calling themselves data scientists? Your neighbour, that gal you met at a cocktail party ?€? even your accountant has had his business cards changed!,??,There are so many people out there that suddenly call themselves ?€?data scientists?€? because it is the latest fad. The , even called it the sexiest job of the 21st century! But in fact, many calling themselves data scientists are lacking the full skill set I would expect were I in charge of hiring a data scientist.,What I see is many business analysts that haven?€?t even got any understanding of big data technology or programming languages call themselves data scientists. Then there are programmers from the IT function who understand programming but lack the business skills, analytics skills or creativity needed to be a true data scientist.,Part of the problem here is simple supply and demand economics: There simply aren?€?t enough true data scientists out there to fill the need, and so less qualified (or not qualified at all!) candidates make it into the ranks.,Second is that the role of a data scientist is often ill-defined within the field and even within a single company. ??People throw the term around to mean everything from a data engineer (the person responsible for creating the software ?€?plumbing?€? that collects and stores the data) to statisticians who merely crunch the numbers.,A true data scientist is so much more. In my experience, a data scientist is:,If you can find a candidate with all of these traits ?€? or most of them with the ability and desire to grow ?€? then you?€?ve found someone who can deliver incredible value to your company, your systems, and your field.,But skimp on any of these traits, and you run the risk of hiring an imposter, someone just hoping to ride the data sciences bubble until it bursts.,What would you add to this list? I?€?d love to hear your thoughts in the comments below., , , , , ,??
Hi all,,I am a novice in this field.??,I met few novices like me missing clarity on career prospects in the wide Analytics domain. I can see a lot of companies employing those with skills in using UI-based tools. But I also heard from them that their job is too monotonous and less interesting in comparison to those who code. Am I simply forming my opinions baseless?,How do you differentiate (in terms of career prospects) between professional who use tools for their analytical tasks, such as SAS, Orange, or RapidMiner and those who depend on R, Python's Scikit-Learn or Pandas? Same is the question when it comes to visualization - people who use tableau for example versus those who use matplotlib for producing visualizations. Which domain has long and steady future?,I am not able to explain why but I can see that my mind is more inclined toward becoming a professional with skills in coding rather than tools with their GUI and drag-drop options. Please help me see the difference. Thanks in advance for your time.
We?€?ve had software as a service, platform as a service and data as a service. Now, by mixing them all together and massively upscaling the amount of data involved, we?€?ve arrived at Big Data as a Service (BDaaS).,It might not be a term you?€?re familiar with yet ?€? but it suitably describes a fast-growing new market. In the last few years many businesses have sprung up offering cloud based Big Data services to help other companies and organizations solve their data dilemmas.,Some estimate that business IT spending on cloud-based, x-as-a-service activity will increase from about 15% today to 35% by 2021. Given that it is estimated that the global Big Data market will be worth $88 billion by that point, we can see that the forecast value of the BDaaS market could be $30 billion.,So, here I will attempt to give a brief(ish) overview of the concept, as well as examples of how it is being put into practice in real life businesses and organizations around the world.,Big Data refers to the ever-growing amount of information we are creating and storing, and the analysis and use of this data. In a business sense, it particularly refers to applying insights gleaned from this analysis in order to drive business growth.,At the moment, BDaaS it is a somewhat nebulous term, which is often used to describe a wide variety of outsourcing of various Big Data functions to the cloud.,This can range from the supply of data, to the supply of analytical tools with which to interrogate the data (often through a web dashboard or control panel) to carrying out the actual analysis and providing reports. Some BDaaS providers also include consulting and advisory services within their BDaaS packages.,So, in many ways, BDaaS encompasses elements of what has become known as software as a service, platform as a service, data as a service, and so on ?€? and applies them to solving Big Data problems.,There are several advantages to outsourcing or virtualizing your analytics activities involving large datasets.,The popularity of Hadoop has to some extent democratized Big Data ?€? anyone can use cheap off-the-shelf hardware and open source software to analyze data, if they invest time learning how. But most commercial Big Data initiatives will still involve money being spent up front on components and infrastructure. When a large company launches a major initiative, this is likely to be substantial.,On top of upfront costs, storing and managing large quantities of information requires an ongoing investment of time and resources. When you use BDaaS, all of the techy ?€?nuts and bolts?€? are, in theory, out of sight and out of mind, leaving you free to concentrate on business issues.,BDaaS providers generally take this on for the customer ?€? they have everything set up and ready to go ?€? and you simply rent the use of their cloud-based storage and analytics engines and pay either for the time you use them or the amount of data crunched.,Additionally BDaaS providers often take on the cost of compliance and data protection. When the data is stored on their servers, they are (generally) responsible for it.,A good example is IBM?€?s??,??service, which provides businesses with access to data and analytics on Twitter?€?s 500 million tweets per day and 280 million monthly active users.,As well as the ?€?firehose?€? of tweets it provides analytics tools and applications for making sense of that messy, unstructured data and has trained 4,000 consultants to help businesses put plans into action to profit from them.,Another is agricultural manufacturers John Deere, which fits all of its tractors with sensors that stream data about the machinery as well as soil and crop conditions to the??,??and Farmsight services. Farmers can subscribe to access analytical intelligence on everything from when to order spare parts to where to plant crops.,The arrival of Apple?€?s Watch ?€? perhaps the device that will bring consumer wearables into the mainstream ?€? will doubtlessly bring with it a tsunami of new BDaaS apps. They will soak up the data from the presumed millions of people who will soon be using it for everything from monitoring their heart rate to arranging their social calendar to remote controlling their home entertainment. Then they will find innovative ways to package it and sell it back to us. Apple and IBM have just announced their collaboration on a??,.,In sales and marketing, BDaaS is increasingly playing its part, too. Many companies now offer customer profiling services, including??,???€? the world?€?s biggest seller of direct marketing data. By applying analytics to the massive amount of personal data they collect, they can more effectively profile us as consumers and hand their own customers potential leads.,Amazon?€?s AWS as well as Google?€?s AdSense and AdWords are better known services that would also fall under the banner. They are all used by thousands of small to medium-sized businesses to host data infrastructure, and target their marketing at relevant niches where potential customers could be lurking.,The term may be rather unwieldy and inelegant (I?€?ve written before that I?€?m not even particularly a fan of the term Big Data, so BDaaS is a further step into the ridiculous) but the concept is rock solid.,As more and more companies realize the worth of implementing Big Data strategies, more services will emerge to support them. Data analysis can and generally does bring positive change to any organization that takes it seriously, and this includes smaller scale operations which won?€?t have the expertise (or budget to develop that expertise) to do it themselves.,With the growth in popularity of software as a service, we are increasingly used to working in a virtualized environment via a web interface, and integrating analytics into this process is a natural next step. We can already see that it is making Big Data projects viable for many businesses that previously would have considered them out of reach ?€? and I think it is something we will see and hear a lot more about in the near future.
I recently read a Harvard Business Review (HBR) article [1] ?€?You need an algorithm, not a data scientist?€?. The author (from an analytics vendor) argues that:,Other articles present similar arguments [2] [3]. These arguments are off the mark for several reasons. I want to present an alternative perspective. What you actually need is??,.,First of all, I want to address each of the arguments made in the HBR post.,The HBR post?€?s argument assumes that algorithms and data scientist are mutually exclusive rather than complementary. Actually, both work together in a lab/factory analogy [4]. With a good organisational structure, ideas are ?€?experimented?€? with using Data ?€?Science?€? in the ?€?lab?€?. When proven measurably useful, then investment is made in productionising the associated Data Science in the ?€?factory?€?. The Data Science will have yielded everything from insights about data quality and data profiles through to the most appropriate visualizations and??,??for the business problem. In addition, good Data Science should have provided expected performance metrics for the factory product. This helps build a business case for the product and ground expectations.,The HBR post?€?s argument assumes that organisations are trying to scale with data scientists rather than productionised data products. This is patently not the case and any organisations taking this approach need to reconsider their Data Science strategy. The point of Data Science is to be a service.?? This service can quickly do agile experiments to quantify and investigate business hypotheses about data and help inform the roll out of products. Doing Data Science therefore informs the investment decision in software development, software purchase, software tuning, etc.,The HBR post claims that certain patterns cannot be perceived by humans doing manual analysis but are detectable by an algorithm. This is partially true. Algorithms can certainly work day and night, quickly processing refreshed and streaming data better than any human could ever hope to. However, if the system being analysed is not well understood then appropriate analyses cannot be chosen and tuned before ?€?switching on the fire hose?€?. It is this understanding, modelling, analysing and tuning that is the job of the Data Scientist in collaboration with the domain expert. The Data Scientist does this in part using statistical and machine learning??,.,The HBR post claims that modern tools require limited intervention, tuning, and integration. This claim needs to be taken with a pinch of salt. It is common knowledge that the vast majority of time on a data project is spent understanding and cleaning the data. We should be very sceptical of claims that software can simply be ?€?turned on?€? without the necessary understanding of the data and the problem domain. There are just too many variations in data to make such a confident claim about the capabilities of data software.,Data Scientists and automation (data products, algorithms, production code, whatever) are complementary functions. Good Data Science supports automation. It quickly adds value by investigating, testing, and quantifying hypotheses about existing data and potential new data.,Simply switching on software ignores the reality of working with data, regardless of the claims of that software. Data is full of nuances, errors and unknown relationships that are best discovered and tested by an expert Data Scientist. This takes time and does not scale but it does not have to scale. It is the necessary prudent investment that you make before spending months in product development and automation of the wrong algorithm on the wrong or broken data.,Data Science done well tells you:,You can read more about how to do agile Data Science that is transferable to the ?€?production factory?€? in my book,??and get the latest news at
Starred articles are candidates for the picture of the week. A comprehensive list of all past resources ,. We are in the process of automatically categorizing them using ,.,Check out??,.
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??|??,??|??,??|??,??|??,??|??,??|??
??,Everyone in every profession and aspect of life should strive to become better each day. As the saying goes "the practice makes the master". Here I outline 7 skills/attitudes we can have to become belter data scientists.,??, Intellectual curiosity is an important pre-disposition for a data scientist. We cultivate and sharpen it by "always wanting to know a bit more about data" when analyzing it.,??,??, A solid understanding about the business we work on is necessary for a better data analysis. It is a good idea to absorb as much as possible through reading and training.,??,??, Data science is all about communicating our analysis, findings and predictions to other people. The ability to write well and succinctly is of great importance in order to make good reports.,??, It is??very handy to know more than one programming language. If we know R, which is a great starting language, it may be good to move on to Python, SAS,??Julia or any other. Besides adding skills to our curriculum, it will improve our logic and enable us to work cross platform.,??,??, SQL has been for years one of the most used programming languages for data analysis. Today most of the data still lies in relational databases. As data scientists often we will have to create new datasets using data from different tables. To know how to query in SQL will become very handy in these moments.,??,??, Participating in competitions is a good way to push ourselves to learn something new, to collaborate with different projects and to deal with data other than those we are used to analyze in our day jobs. To have a "pet project" may be useful as well.,??, There are tons of information out there and having just a little bit of daily contact with some piece of new information is very helpful to stay tuned. A good thing is to sign up for newsfeed or to follow data scientists, blogs and data related profiles through social media.,??,These are simple ideas about actions that can help us improve in our role as data analysts/scientists. Of course there are more skills that were left out of the list. Feel free to add them in your comments., ---, Flavio Bossolan is a data analyst and Machine Learning enthusiast. He has experience in data analysis using SQL, SAS, R, Octave and spreadsheets. He has worked with default financial analysis for Basel II requirements and financial loss predictive analysis for one of Brazil??s major private banks. He currently works with data analysis related to clients?? experience at Telef??nica. He can be reached at flavio.bossolan@gmail.com or at 
Today?€?s marketers are becoming technically savvier. They understand the need to improve customer experiences or implement digital marketing strategies to engage consumers across channels. Customer retention and acquisition, Big Data, social media marketing, and content marketing are just a few of the goals and strategies in today?€?s marketing toolbox.,However, perhaps not so widely discussed are some important fundamentals ?€? high quality marketing data.,As marketers we understand that without data, there are no insights. But managing the quality of the data and applying analytics are key to successfully implementing all these other great marketing goals.,When data goes bad, even the best laid out strategies are doomed to fail. After all, ?€?Garbage In, Garbage Out?€?, right?,So let?€?s take a step back from the world of Big Data, Digital Marketing, and Customer Experience Management to focus on the basics ?€? ,.,Let?€?s first check out some of these great stats by ,:,The implementation of a data quality initiative can lead to reductions of:,And increases of:,??,Unfortunately, many organizations take a reactive approach to data management, only taking action when something negative occurs. For example, multiple messages sent to the wrong person damages customer relationships, revealing a need to improve customer information.??,However, choosing NOT to fix dirty data can be extremely damaging. Consider these data quality horror stories:,While these examples are extreme, the negative impacts of dirty data on your business are very real.,As part of any new data initiative, a business needs analysis should be performed to understand what is required of data moving forward. A business needs analysis focuses on understanding business objectives, strategic goals and business drivers.,For example, what information is required to meet these objectives and how accessible is it to end users? Are data gaps occurring, limiting the availability of required information to support decision-making? What data issues may be impacting revenue, increasing costs, or causing inefficiencies in operations?,Documenting business objectives helps determine what data should be captured, how the data is related, and how data should be structured to create value.,Once a data management project is approved, data must be properly cleansed and integrated to ensure information is of the highest quality to drive smarter decision-making across departments.?? ??,Begin by evaluating the quality of your data with a data assessment. Many vendors offer a , to help identify areas where data quality can be improved, what types of data may be missing, and other problems that may be affecting optimal data performance.,Data must also be integrated and placed into a central repository for a complete, 360-degree view of the customer or other business area. , automates integration processes and improves data quality by performing the following functions:,Business processes should also be established to ensure data manually entered into systems is of the highest quality possible. As we learned previously in our example of the pregnant men, many organizations experience data errors when information is manually entered, at a ,.?? Even one wrong number entered incorrectly can cause a payment to fail, a wrong part number to be shipped, or apparently a man to become pregnant.,Data validation controls can be integrated into on-line forms, using rules to check the validity of data sets. For example, an on-line website form may require a visitor to enter data in specified formats. Or an IRS form may utilize controls to check that positive numbers are being entered into fields.?? Training employees to be more aware of the importance of data quality is also a crucial step to achieve a company-wide awareness of maintaining high quality information.,With an integrated and clean database in place, customer analytics can be applied to target customers with the most relevant offers. Begin by creating customer segments. Customer segmentation refers to dividing customers into groups who share similar characteristics, such as age, gender, lifestyle, and so on.,Any number of segments can be created as long as each segment is:,?? The segment should not be too narrow, making it cost-prohibitive to reach these customers. The segment should be worthy of the marketing efforts.,:?? The consumers must be able to be reached through channels already established, such as a website or store, or by new channels that can easily be created.,When creating customer segments, a company must consider a wide range of customer characteristics, such as:,Based on any combination of these characteristics, companies can develop key customer segments and develop marketing strategies designed to generate the most profit from each unique customer group. A company may want to enhance loyalty, increase customer value, or provide products and services to a particular geographic area.,Consider the following examples:,??,??,Similar to segmentation, predictive modeling allows marketers to develop very precise, targeted campaigns. Both techniques examine the characteristics of customers and prospects, however modeling takes this one step further by also predicting future behaviors. Modeling is the practice of forecasting consumer behaviors and assigning a score based on the likelihood of completing a desired action, such as purchasing a product. For example, which customers are most likely to spend the most across a 6-month cycle?,Check out the following examples of how a predictive model may be used:,Data quality and , are critical for success in today?€?s economy. More companies are increasingly investing in data management and business intelligence solutions to maintain high quality data and target multi-channel consumers. And with better data insights, marketers are better able to focus on today?€?s data-driven, technically-savvy marketing strategies.
Here is the complete view
In the past couple of months we have seen several major cyber attacks ranging from the New York Stock Exchange (NYSE) to American Airlines. In addition these well known breaches ??more than 150 other major data breaches have occurred. This has resulted in the theft of more than 250 million confidential records. Naturally this concerns many large organizations and is leading to massive security infrastructure over halls amongst many fortune 500 organizations.,Since the beginning of computing, security issues have always been present. Despite numerous advances in electronic security technology there has always been a sort of arms race between security systems and hackers. A recent piece on the "," outlines many of the largest hacks and outlined their impact which has let to millions if not billions of lost revenue.??However many of the breaches that occurred came through cloud infrastructure. Although the cloud tends to be quite safe there has been numerous issues with it's security. Despite concerns the clouds champion Apple insists that it is safe. This was to calm Apple Pay users.,Although you might envision that the main target of these threats is directly financial most of the threat is aimed at data. Passwords, usernames and emails are the main target. The ability to access individual accounts is more difficult to trace and can have a much larger impact. Typically advanced persistent threats are aimed to large business due to the vast amounts of data. ??Another subset of targets is big data. many companies are beginning to utilize Business Intelligence by utilizing vast amounts of information such as on hadoop. However how secure is this data? The reality is that anything can be hacked given the time and expertise.??,??,??
 , , , , , , , , , , , , 
 
At????Machinalis we work daily on projects that fall within the area known today as ,. Here are 6 tips and learned lessons for people who want to provide a sustainable data science service and don?€?t want to avoid the mistakes that we made.,To summarize: there?€?s nothing wrong with loving doing Data Science, and being passionate about it can get you a long way, but keep the objectives in focus and direct most of your effort towards it.,Thanks to Agustin Barto for his cooperation to write this article.
Over the past several years, companies have moved from simply asking where to get the data from to support their decisions, to asking how they are going to leverage it to create actionable insights.,??use statistical or machine-learning techniques to analyze current and historical facts, and find relationships and patterns that can be used to predict future events. A basic example of its application is in retail banking. Banks can use predictive models to look at customers?€? spending patterns. They can then use this information to predict their financial and life events, and send them more relevant offers, make a decision as to whether to give them a loan, and so forth.,By using predictive analytics, it is possible for a company to make better and faster decisions, and at a lower cost - either by using them to supplement human decision making processes, or replacing them entirely.,In decision making, the more that you know about the likely outcome, the more confident you can be that you decision is the right one. Predictive analytics allows you to get an idea of every possible eventuality, so you can weigh up the risks and the potential return on investment (ROI). It enables you to see every factor that could play a role in the outcome, including some that you may not have thought relevant.,Another advantage of predictive analytics is that it helps to remove politics from the decision making process. When making a big decision that has a company-wide impact, there are normally a number of departments with vested interests involved, often relying on different experiences and knowledge bases that may run contrary to those of others. Using predictive analytics, it is possible to gain a single version of the truth that can override such ulterior motives.,Awareness of predictive analytics is growing, and companies that have adopted the methods are reporting success and seeing increased ROIs. However, according to Wayne Eckerson?€?s 2014 study, Making Predictive Analytics Pervasive, the number of organizations that reported successfully implementing them actually dropped from 21% to 18% in 2014. One aid that could see this change is the new IBM Industry Analytics Solutions, which is designed to provide interactive and role-specific dashboards that business users can share predictive insights on. These are visible across teams and organizations, increasing the speed at which decisions are made.,Another useful tool in optimizing predictive analytics to produce better choices is decision-modeling. Decision-Modelling is the structure of the decision-making involved in a business scenario that has to be made repeatedly, such as pricing deals. It breaks the process down and identifies what information and knowledge was required and where. It is also possible to link decision requirements models to KPIs and metrics, making it apparent which metrics will improve if the decision making does. By doing this, it?€?s possible to see how predictive analytics can add value, and how ROI can be measured later.
DFS, ''the next big thing'' is taking North America by storm and slowly knocking on Europe's doors. The way it works is simple: sports lovers select a team of real world athletes who then score fantasy points according to set scoring rules.,Presently, in most US states, fantasy sports (including DFS) is generally considered a game of skill and therefore not legally considered as gambling (where there's an element of both luck and chance). However, there are a few states that have either a more restrictive set of laws that outline what is considered a game of skill or have specific laws outlawing paid fantasy sports. These states are currently Arizona, Montana, Louisiana, Iowa and Washington.,At a US federal level, fantasy sports is defined and exempted by the Unlawful Internet Gambling Enforcement Act of 2006 (UIGEA). The act, signed into law by ,, included an explicit provision noting that the law would not apply to fantasy sports games, educational games, or any online contest that "has an outcome that reflects the relative knowledge of the participants, or their skill at physical reaction or physical manipulation (but not chance), and, in the case of a fantasy or simulation sports game, has an outcome that is determined predominantly by accumulated statistical results of sporting events, including any single score, point-spread, team performance, or individual performance in such sporting events...", Talking about the legal issues of DFS in Europe is not easy as each country has a different vision of what iGaming/moneytainment is, as well as what a skill and luck game is. Most countries allowing fantasy sports have already included fantasy sports in their traditional iGaming licences. The most obvious example is the UK, where DFS falls in the same category as sports betting and horse racing. Therefore, the operators need to acquire an operating ''pool betting'' licence from the Gambling Commission.,Some countries are not aware of DFS yet, and their legislator has therefore no opinion on that subject for the moment.,In many countries, for instance in Germany, the regulation of gaming is based on whether the predominance for the outcome of the game lies in skill or chance. While this distinction is simple for pure games of chance and pure games of skill, it is a complex and yet unsettled question. Is relative skill or the chance dominating the outcome of a game? Is DFS more similar to chess, which is legally classified as a game of skill ?€? i.e. not gambling, or to roulette, which is regarded as a pure game of chance?,According to ,, a researcher at the University of Hamburg who has a deep understanding of gambling, DFS falls into the category of mixed games. ''Whether it is considered a game of skill or chance depends on the question at hand. For example, there is a distinction between a game being recognized as a game of skill/chance in the legal/regulatory sense and other senses, e.g. for income tax purposes. From a legal perspective, it is essential if the majority of the players are playing it as a game of skill or as a game of chance. In this regard, I think of fantasy sports as a game of chance.'' For other purposes, e.g. if a professional fantasy sports bettor is due to income tax, the game is to be deemed a skill game, because this individual player plays it as a game of skill.,The reason for this distinction is the repetitions: the more often a mixed game is repeated, the higher the share of skill that is involved. Fiedler says that some fantasy sports bettors do not play it often enough to play a game of dominant skill. ''However, a few heavy highly involved players (like the professional bettors), play it often enough to reach the so called Critical Repetition Frequency (CRF), which is the threshold of repetitions a mixed game has to be played until skill dominates the outcome.'' At the same point he stresses that empirical evidence is yet to be presented on where the CRF lies for fantasy sports as it was originally applied to poker.,For ,, CEO of the most advanced European fantasy football/soccer game ,, it is very important to differentiate a skill game from a luck game because the whole point of DFS is to prove to your friends that you know sports better than them: ''The results have to be as close as possible to reality. If you have an encyclopaedic knowledge of football, you should reasonably expect to beat your friends who only have a social interest in football. Therefore, if the result of the game is random, your friends might crush you, which is purely unacceptable.'' In other words, what's the point of playing the game if luck determinates the winner?,So, what are the skills that are needed to play DFS? As written on the website of American ,, DFS players ''must take into account a myriad of statistics, facts and game theory in order to be competitive''.,Bollier believes that Oulala has everything it takes for a client to become a skilled player ?€? thanks to Big Data: ''The cornerstone of our game is a sophisticated mathematical matrix allowing our game's results to be as close as possible to reality. Our scoring system includes 70 soccer statistics, and each criteria is weighted based on the player's position (keeper, defender, midfielder, and striker), to stick even more to the reality of a soccer game.'', In concrete terms, when a soccer player performs one of the actions on the field, all the virtual teams having selected him will be automatically given points, positive or negative ?€? according to the real on-field performance. Extra bonuses don't exist, since that increases the luck aspect of the game. The effects of events that are hard to predict, such as injuries and substitutions, can be eliminated by a live-coaching feature. All collected data, broken down to different stats, are then available for further analysis, which offers an excellent overview of a player's current form. And the numbers obviously don't lie: FC Barcelona star , was statistically the best player of the 2014/2015 season with 2330 points, ahead of Real Madrid rival , (1897pts) and top Chelsea boy , (1435pts). The best goalkeeper was Lyon's , (1101pts, 7th overall), followed by Montpellier's , as the best defender (1066pts, 9th overall). Gone are the days when we talked only about players' goals.,The best client of their last season's game, who took part in 41 daily competitions, finished among top three ''fantasy managers'' on five occasions. According to a mathematical calculation, the chances of finishing five times on Oulala's podium in a given year if the game was based on luck were 560,000,000 against 1.,Bollier adds that they are currently working with the Maltese government to help create the first skill game licence purely dedicated to Fantasy Sports: ''We believe this is a critical issue in Europe (and a very different situation than in the USA), as some iGaming operators might want to pretend, for marketing reasons, that they offer a game of skill when it is actually a game of luck.'',So far there haven't been any empirical investigations as to whether DFS is really a skill game. Despite the fact that skill in games has to be measured relatively, that is, relative to the skill of other players, in some DFS games ?€? as I have explained in the case of Oulala ?€? luck is certainly present to a certain extent. However, skill plays a greater role in determining the outcome, and this is evident especially at the top of the leaderboard where one can usually find ''fantasy managers'' who do their homework.
Clayton Christensen in his brilliant book titled ?€?The Innovator's Dilemma?€? spoke about disruptive innovation using the following framework:,While established companies in any sector focus on existing customer needs and sustained innovation at the top of the market, they might leave the space open for new competitors to use simple and disruptive innovation??that identify unmet customer needs. These start at the bottom of the market and then relentlessly move up. - See more at: ,It?€?s a trend we have seen??across every sector today with technology often leading the disruption.,Now imagine if we apply this paradigm to the world of Competitive Intelligence (CI); a discipline that is supposed to monitor the changes in the market and the competitive threats for its business? Is CI also monitoring disruptive innovation that is creating in a ripple in its own waters?,Let?€?s look at 3 companies and 3 specific??technology-led ideas??by which they could potentially disrupt CI: a) Crowdsourcing, b) Temporal analysis, c) Artificial Intelligence,:,: Owler.com,: Crowdsourcing of Competitive Intelligence,: The company launched its API with data on over 10 million companies. Once you subscribe and identify your competitive set,??it gives you??information on Company Background, History, Financials, Location, Leadership, Industry information and provides reports such as business round-ups, company profiles as well as breaking news alerts, directly to the subscribers?€? mobile.,: So the company lets you monitor your competitor set and sends you automated alerts and company profiles. And it has ambitious plans to have profiles for every single company, public or private? So what you ask? Aren?€?t they just making information search an automated and mobile process without providing real intelligence?,The real differentiation comes??from the feature??called ?€?Poll Builder?€? - this allows polls to be conducted for the competitors. These polls could be around revenue, employee strength etc. Now imagine researching a private company where information is at best extremely difficult to access, forget analyse. These polls, even though not completely accurate, could give you an indication of the sentiment about that company., A simple bottom of the curve innovation could change the way CI analysts access and look at private company data, in particular.,: Data quality issues could be a killer for this model.,: Recordedfuture.com, Temporal Analysis Engine to understand Real Time Threat Intelligence, The firm deploys what it terms as a Temporal Analysis Engine to understand history and progression of events of interest, correlate past predictions with actual events and show the results real-time through a visualization layer. It claims to track over 650,000 web data sources. For example, if you were to search for ?€?Driverless Cars?€? and try to look ahead??and see??who?€?s going to dominate the future, the API will help you reduce the noise: signal ratio you get by typing the same text in google search. By visualizing futuristic or predictive statements from companies on this theme, it will show companies other than the usual suspects such as Google who are quietly investing and preparing for this innovation.,: It is just another text analysis+social listening+visualization tool that gives you more information with no actionable insight? Well, what differentiates the firm is the ability??to use its??temporal analysis engine to time-stamp events and analyse forward looking statements and future events such as plans to launch product, protest declarations etc., Can its API help CI analysts bring predictive power to analysing news signals? If that happens, it could be powerful., In the recent past, the company has been speaking about many use cases from monitoring cyber attacks to terrorist attacks. The impact from the different use cases remains to be seen.,: Narrativescience.com,: Transform data into narratives with Artificial Intelligence,: The firm starts off saying "while advances in data science and visualization are helpful, they don?€?t take you to the last mile". And the last mile, according to them, is being able to craft the right story and communicate the insights that people can act on. The firm?€?s Artificial Intelligence (AI) powered platform first identifies the key facts that are foundational for the narrative, uses natural language software to automatically write the content and then delivers 1:1 written communication in a brand-consistent voice., ,: Is it just a bunch of charts and data? But the firm goes further. It looks at crafting narratives for the usual reports various teams are requested for starting from Fund Portfolio Commentary for Wealth Management and Investment Management??firms??to marketing and sales performance reports. These reports??earlier took weeks to write in the right executive -ready language.,: From showing the data to telling the story, can the firm actually??craft the right narrative for the senior leadership? That could not just crunch time but reduce a lot of report writing manual effort.,: Despite the spate of AI driven innovations, the heart of the story you tell, I feel, will still remain with the mind and the pen of the writer. While Narrative Science could conjure up simple, repeatable narratives, it would be hard for them to provide original narratives or commentary.,Crowdsourcing, Temporal Analysis, Artificial Intelligence. 3 disruptive ideas. Will they re-shape the way companies are currently looking at Competitive Intelligence? Going back to Clayton Christensen, that depends on the balance between sustaining innovations and disruptive innovations as compared to the pace of technological progress.,As George Bernard Shaw said: ?€?Some look at things that are, and ask why. I dream of things that never were and ask why not??€?,Innovators' Dilemma indeed. Are you seeing such innovative companies disrupting the traditional way of doing Competitive Intelligence? The 3 companies mentioned here are just samples for a technology-led innovation explosion we are seeing around us. They may or may not be the real disruptors. Would love to hear your thoughts on the disruptors you are seeing in your business.
As a data science professional how would you respond to the question, ?€?How familiar are you with PMML??€?,Your choices are:,-?????????????? ?€?I have no idea what PMML is?€?,-?????????????? ?€?I have heard and read about PMML?€?,-?????????????? ?€?I have played around with PMML?€?,-?????????????? ?€?I have used PMML in my projects?€?,I have seen many responses to the question and the most popular one was ?€?I have no idea what PMML is?€?.,Is this surprising? I guess, if we look at the distribution of people who responded this way, most of them were somewhat removed from the task of model deployment. One might say that the response is normal since they are only ones responsible for the model-building process. However, it is quite common for a model development team to also assume the task of model deployment into production. Even if they are not directly involved in the deployment, they will most likely have to guide the IT team. Herein lies the unfortunate aspect. Not knowing what PMML is will lead the deployment team into an awkward, inefficient and difficult production process. It is time-consuming and usually costly. This is the reason why people in the data science field should be aware of what PMML is and how it can be useful.,PMML stands for Predictive Model Markup Language. It was developed by the ,, an independent, vendor-led committee. PMML provides an open standard for representing data mining models. In this way, models can easily be shared between different applications by avoiding proprietary issues and incompatibilities. Currently, all major commercial and open-source data mining tools already support PMML. Check out R?€?s ,??package.??,PMML is an XML-based language which follows a very intuitive structure to describe data pre- and post-processing as well as predictive algorithms. Not only does PMML represent a wide range of statistical techniques, but it can also be used to represent input data as well as the data transformations necessary to turn raw data into meaningful features.,Consider now that if there were an application that can process a PMML model and execute it in any production environment?€?common these days are Hadoop, Spark, Storm, Teradata, Greenplum, Oracle, IBM Pure Data, AWS, Azure, etc.?€?we would have a very clean and simple model deployment environment that ?€?connects?€? many different modeling tools to different production systems. I think this is one way to get every component of the analytics ecosystem to work together without obstruction. This is the reason why I titled this blog ?€?PMML ?€? A Good Citizen of the Analytics Ecosystem.?€?,Actually, this process doesn?€?t need to be restricted to just model deployment into production. It applies equally well to model deployment into test environment. For example, one could use open-source R to develop a model on a reasonable sample size and apply the model in its PMML form against very large data/Big Data in the test system.,The simplicity of the PMML approach can mean making your analytical model available to your business users in a day or less, versus in months.,What was your answer to the question?
 
The insurance industry is all about assessing risk and managing the same successfully. Life insurance industry operates intrinsically by balancing risk assessment and risk management. Compiled with a large volume of data the insurance industry operates with, arriving at meaningful information can be a challenging task. Also, the insurance industry is growing competitive with each passing year and numbers of insurance service providers are constantly on the rise. In such a scenario only those companies that can increase their top and bottom growth line can stay competitive and profitable in the long run.,Insurance companies use a large chunk of data available with them and develop new tools to understand and analyze the data for their profitability. This process of using large chunks of data to understand the market dynamic and risk assessment is known as insurance analytics. Insurance analytics can go a long way in helping insurance companies develop their business model and various insurance products which allow the companies to stay profitable while attracting new clients and retaining existing ones.,??Analytics can help insurance companies who plan to enter new market segments. For example, with FDI in the Indian insurance sector now increased to 49%, more and more insurance companies are likely to enter the Indian markets. Before entering a new market, they need to offer products that have been developed for the local population, making them attractive for Indians as a whole. The same should apply to North America, EU, Asia, etc...,??On the other hand, the companies also have to take note that they remain profitable and do not end up losing money while offering protective services to the clients. Insurance analytics data helps companies to balance such a dilemma allowing them to grow and sustain business in the long run.,The three most important aspects of a well defined insurance analytics data is to acquire new customers, retain the existing ones and analyzing risk management to stay profitable in the long term.,Since market uncertainty and the rising costs mean that finding profitable customers remains a challenge for the industry overall, balancing both new customer acquisitions while retaining the older ones is an important element of success., This is a re-blog of an article by the same name??with permission??from??
??A data scientist is someone who analyzes an organization's big data to discover actionable trends that lead to business results. Data scientists look at what questions business people need to ask to remain competitive. They work directly with C-level executives, advising them on how to drive maximum value from big data and integrate new information. In many ways, a??,??serves as a change agent in today's workforce, pushing organizational collaboration and information integration.,Successful data scientists can handle information complexity, influence both the business and IT organizations, and pursue their own research. They provide the most value when they are learning what the data is telling rather than spending time working on the functional aspects of data management. Therefore, deep knowledge on how to mine the data or use predictive analytics is less important than experience in applying the right questions with the technology tools. Companies will miss out on excellent candidates if they use a traditional checklist focusing on data skills.,People interested in the role should have some background in math, modeling and analysis, with possible professional work in machine learning, data mining or predictive analytics. Candidates should be able to work across the organization and influence various disciplines. And once in the role, the data scientist must continue professional education, pursuing things like IBM's free boot camps on improving data management and strategy skills.,??A Google search will find over 10,000 postings for data scientists. That's an impressive number for a nascent field. Businesses are starting to understand the need for data-driven insights. I find myself frequently explaining the position to prospective and current clients. Specifically, I discuss how the role works, the appropriate academic and professional background of a good candidate, and how to integrate the position into your existing organization.,Another indicator of the demand for data scientists is the number of conferences focused on this role. In May 2011, the first annual Data Scientist Summit was held, and in August 2011, the Joint Statistical Meeting was held, with significant attendance by practitioners and companies hoping to find good candidates. Stanford University has estimated that 1,200 exabytes of data were created in 2010. That's a tenfold increase in five years, and it shows that there is a real demand for this role.,Read full interview at??
Below (in italic) is Revolution Analytics point of view. Mine is that is not just science, but a mix of??craftsmanship, intuition, art, business acumen, magic and science. I would rather call it??,, and I call myself a??,.,??,(full article at??,),??
How??,??your data is depends on the quantity of information that it contains (measured using entropy metrics), rather than the number of terabytes. Huge data that is sparse or shallow is indeed not huge - and can be compressed very efficiently. What do you think?,Here's Stfan Carandy's viewpoint (founder of Bayesia Networks):,If I may cross-post the following from our blog at??,, which speaks to the same point:,"It has long been understood that even when confronted with a ten-gigabyte file containing data to be statistically analyzed, the actual information-theoretic amount of information in the file might be much less, per haps merely a few hundred megabytes. This insight is currently most commonly used by data analysts to take high-dimensional real-valued datasets and reduce their dimensionality using principal components analysis, with little loss of meaningful information. This can turn an apparently intractably large data mining problem into an easy problem." [1],As an alternative to dimension reduction, we can exploit existing regularities in the data to create a more compact and thus more tractable representation with Bayesian networks. "In context of Bayesian network learning, we describe the data using DAGs [Directed Acyclic Graphs] that represent dependencies between attributes. A Bayesian network with the least MDL [Minimum Description Length] score (highly compressed) is said to model the underlying distribution in the best possible way. Thus the problem of learning Bayesian networks using MDL score becomes an optimization problem." [2] Consequently, learning Bayesian networks is inherently a form data compression.,References:,[1] ??Davies, S., and A. Moore. ?€?Bayesian networks for lossless dataset compression.?€? In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, 391, 1999.,[2] Hamine, Vikas. ?€?Learning Optimal Augmented Bayes Networks?€? (n.d.).,.
Microsoft's first acquisition was in 1987 and it has purchased an average of six companies a year.,Microsoft has made eight acquisitions worth over one billion dollars: Skype (2011), aQuantive (2007), Fast Search & Transfer (2008), Navision (2002), Visio Corporation (2000), Yammer (2012), Nokia (2013) and Mojang (2014).,Check out??,Here is the complete details of the??
Source:??,.,I have been working on this (mostly) annotated collection of tools and articles that I believe would be of help to both the data dabbler and professional. If you are a data scientist, data analyst or data dummy, chances are there is something in here for you.??Included is a list of tools, such as programming languages and web-based utilities, data mining resources, some prominent organizations in the field, repositories where you can play with data, events you may want to attend and important articles you should take a look at.,??,??,The second segment of the list includes a number of art and design resources the infographic designers might like including color palette generators and image searches. There are also some invisible web resources (if you?€?re looking for something on Google and not finding it) and metadata resources so you can appropriately curate your data.,This is in no way a complete list so please??,??with any suggestions!,??,Read much more on this subject at??
PLEASANTON, Calif.- February 25th, 2011 - The largest benchmark research ever done on business analytics establishes for the first time that analytics has become the new engine for business competitiveness and profitability. This landmark research from Ventana Research, which involved input from more than 2,850 organizations, makes clear for the first time the little-understood role of business analytics in the success and failure of businesses.,The research found that more than half of organizations still spend the majority of their time in unproductive data preparation and quality assurance processes rather than in applying analytics to gain the most value from their metrics. It also shows that the application of analytics widely remains a spreadsheet-based activity; spreadsheets play a universal or regular analytics-related role in 88 percent of organizations. Yet these same organizations report issues with data accuracy and timeliness of analytics, and find themselves at a competitive disadvantage against the most mature organizations that are using predictive analytics to help determine future outcomes and mobile technologies to simplify access to analytics and metrics. Applying the Ventana Research Maturity Index, the research finds that most organizations have room to mature in their use of business analytics, with only 15 percent ranking at the highest Innovative of the four maturity levels.,Yet despite the obvious value of analytics and the ready availability of analytics tools, businesses largely are not acting to eliminate the disadvantage at which they find themselves. The research indicates that that only a third of organizations are planning to change the way they generate and apply analytics in the next 12 to 18 months.,This in-depth benchmark research on Business Analytics, the culmination of a year of work from the leading business technology research firm, analyzed business and IT organizations around the world to assess the maturity and direction of their efforts. Ventana Research undertook this research to acquire real-world information about maturity, trends and best practices in how organizations use analytics and key indicators. It explores how they do this now, how teams view the current processes and tools, plans they have to change or improve them, and benefits they hope to gain by doing so.,The findings offer new insights on companies' attitudes toward business analytics. They confirm that business and IT organizations use a variety of analytics, but they have not yet automated many of the underlying data integration and analytics operations needed to generate metrics. The research indicates that the most desired analytics capability is to be able to search for relevant analytics and metrics that answer business questions; businesses also want to be able to drill down into the causes of situations by performing root cause analysis and to be able to collaborate in reviewing analytics. These are capabilities not readily available to organizations that rely heavily on spreadsheets as analytics tools or s data sources. Not surprisingly, the research finds that a quarter of those organizations are not happy with their existing technology.,The research also finds a shifting deployment landscape as more than a third of organizations wants to be able to rent software for business analytics, while the majority still prefer on-premises deployment of business analytics.,"The advent of business analytics is clearly a game-changing opportunity for organizations that want to be more efficient in their business processes and still increase the likelihood of achieving needed levels of bottom-line business performance," noted Robert Kugel, SVP of Research at Ventana Research. "Without a strong business analytics foundation, organizations will not be able to understand where to optimize performance, let alone plan to achieve the goals required for guiding an organization's performance. The organizations that invest in business analytics and collaborate across line-of-business areas and IT are most likely to be innovating in their use of analytics and to achieve increased operational efficiencies.,"This groundbreaking research provides the breadth and depth required to help the industry advance the science of analytics. I believe this research not only will help guide a better understanding of best and worst practices, but also will be a wake-up call for technology suppliers about what matters most to business organizations no matter what their size, industry, level or area of focus.",Ventana Research will release and detail the findings of this benchmark research in a live interactive webinar on March 10th, 2011 at 10:00 am Pacific time that will discuss the research findings and offer recommendations for improvement. Key research findings to be discussed will include:,?€? top ten recommendations for business and IT organizations.,?€? the current state of use of analytics and metrics across line of business and IT.,?€? top patterns in the adoption of new methods of business analytics across industries.,?€? the types of metrics and indicators that are most important.,?€? the current state, future direction of and potential investments in business analytics.,?€? the competencies required for business analytics.,The research was sponsored by EMC and IBM. The Business Analytics benchmark research executive report will be made available following the webinar. Those interested in learning more about the benchmark research can find additional information at??,; information about our research into line-of-business- and IT-specific analytics can be found at,. As a part of your registration you will receive insights and education from Ventana Research on using technology effectively in business. In the coming six months specific vertical industry benchmarks on analytics will be brought to market.,Ventana Research is the leading benchmark research and advisory services firm. We provide expert guidance to help organizations manage and optimize performance - to become not only more efficient but more effective. Our unparalleled insights and best practices guidance are based on our rigorous research-based benchmarking indexes of people, processes, information and technology across business and IT functions worldwide. The combination we offer of benchmark research, rigorous market coverage and in-depth knowledge of hundreds of technology providers means we can deliver business and technology education and expertise to our clients where and when needed. The Ventana Research Indexes - the Value Index and the Benchmark Index family - have redefined the research industry by providing accessible, easy-to-use research-based business and technology guidance to businesses.,Ventana Research provides the most comprehensive analyst and research coverage in the industry; the many business and IT professionals worldwide who are members of our community benefit from Ventana Research's insights, as do highly regarded media and association partners around the globe. ??Ventana Research was ranked the #1 analyst firm you can trust in enterprise software for 2009 for its relevance to the industry. We focus on business and technology trends and best practices that maximize organizations' potential to perform while reducing the time, cost and risk and still achieve optimal outcomes.,Our views and analyses are distributed daily through blogs and social media channels including??,,??,,??,??and Business Week?€?s??,Media: Copies of benchmark research report and interviews are available upon request.,Media Contact:,Marisela Reynoso,(925)??242-2579,??
"
"
The figure titled "Data Pipeline" is from an article by Jeffrey T. Leek & Roger D. Peng titled, "Statistics: P values are just the tip of the iceberg.??These are both well known scientists in the field of statistics and data science, and for them, there is no need to debate the importance of data integrity; it is a fundamental concept. Current terminology uses the term "tidy data", a phrase coined by Hadley Wickham from an article by the same name. Whatever you call it, as scientist, they understand the consequences of bad data. Business decisions today are frequently driven by results from data analysis, and, as such, this requires today's executives to also understand these same consequencese. Bad data leads to bad decisions.??,Ok, case closed. There is nothing more to discuss, or debate. Right? On the surface, this is an obvious conclusion, and you would think there would be no need to discuss it any further. I have been accused of having a stranglehold on the obvious on more than one occasion. However, if this is so obvious, why, in my 20+ years of working as an information architect and data engineer, do I continue to see bad data? When I am engaged to help a company with their data, the first thing I should be handed is documentation that defines the company's data management strategy. However, this has seldom happened (of course, they probably wouldn't need me if they handed me their data management strategy documentation).,Typically, the first thing I do is obtain access to "at least one" of their major databases and reverse engineer it using a tool like Erwin to see how they are managing their most important data. Invariably, I see very nicely arranged rows, and columns of 100's of tables without a single relationship, or primary key assigned. In addition, you hardly ever find a data dictionary. If you have a question about the data, generally you are required to schedule an appointment with a very busy individual in the company that is the keeper of this data and considered the local subject matter expert.,I quoted "at least one" in the paragraph above to highlight that there are usually several major databases, and numerous lesser databases. Just the fact that there are numerous databases siloed throughout the company is a good indicator that there is a lot of work to do. There are large companies that literally have thousands of data stores of duplicated data, and a massive ETL team that is busy moving data from one database to another.,This is a "Big Data" forum, so what does this discussion have to do with big data? If your company is anything like described above, then you are not ready to manage a big data project. Organizations that successfully implement a big data strategy have a documented corporate data management strategy, and big data is simply a part of the overall strategy to properly manage this valuable asset. We have all heard of the failed Big Data projects, and there are numerous reasons for that. The lack of a corporate data management startegy, and just a general lack of understanding of data management can explain most failed projects., , When data is received from a third party, as was discussed in my previous post,,, there needs to be a process in place for managing this data upstream. A huge mistake, in my opinion, is to put data into a Data Lake, or any other type of data store without putting it through the learning and cleansing process. It is far too easy to rationalize shortcuts, and it is far too difficult to justify revisiting the same work. Clean the data upstream before it is allowed in your data stores, and then the manipulation and analysis of that data will always serve its purpose.,This gets into the subject of Data Governance, and Data Quality management (a component of a Data Management Strategy). We will leave this for another forum discussion, but I didn't want to discuss the data pipeline, and data integrity without at least mentioning the key component of governance.,Using the example from my last post regarding the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database, if NOAA had properly maintained their data, it would be of far more value to the consumer than it is in its current state. Let me restate that I am not picking on NOAA. These examples are everywhere and in many cases much worse than NOAA's storm database. Since my last post, I have worked on this data very little, and in this short time I found more errors than were discussed last month. Not only were there 2013 duplicates in the FATALITIES data, and 28 FATALITIES records without a storm, but there are 28,332 LOCATION records that refer to a storm event that does not exist in the DETAILS table.,As you will recall from the previous post, the problems identified in NOAA's storm database included:,The last problem area identified in the NOAA data is exactly why you need referential integrity on a normalized data set with properly defined constraints. All of these controls protects your data from sloppy data management practices. The duplicates discovered in the NOAA data were probably caused during the data cleansing process they went through in 2014. The file dates where the duplicates occurred were all from 1997 through 2014, and was probably a process where human error introduced the duplicates during a batch update. This happens to the best of us, and it is exactly why we need stringent controls on our data -- to protect us from ourselves.,The errors discussed in this data set are not complete. I addressed some of the major issues, and the quality of the data is greatly enhanced at this point. However, as I started developing exploratory plots to demonstrate the errors, it was discovered that there are 248,982 records in the LOCATIONS table that contain no values for the LATITUDE and LONGITUDE, and of these, 187,367 records contain no value for location (most of those with a LOCATION are very general, like CountyWide, yet there is no County name provided). I guess my question here is, why create a LOCATION record with no LOCATION?,The example in the previous paragraph highlights the importance of understanding your data. Someone could mistakenly think that there are valid locations for 1,001,608 storms, when in reality, it is closer to 724,294. Regarding the LOCATION data, violation of normalization rules is that there is LAT/LONG data in the DETAILS table, as well as the LOCATIONS table. Which one is correct? Do the LAT/LONG provided in the DETAILS table match those in the LOCATIONS table? I will leave that for someone like NOAA to fix. Lastly, the existing LAT/LONG are not all in the correct format, nor are their values within a valid range. All longitudes East of the Prime Meridian should be negative numbers, yet the longitudes are positive for LON2 variables in LOCATIONS table. The range of valid latitude and longitudes values for the 48 contiguous United States are:,In the NOAA LOCATIONS table, the range of values for the beginning latitude of the storms are:,As you can see, these are far outside of the ranges for the United States. Let's quickly take a look a the NOAA DETAILS range of values for the beginning and ending latitudes and longitudes:,The values are quite different once the data has been coerced into being the correct data type. Just showing the BEGIN_LAT ranges, instead of a range from??"" to "REDLANDS", you get a range of values from -14.4 to 70.5. Still not all exactly in the United States. Values less than 0 for latitude would be somewhere South of the Equator, but we can assume that the 70th parallel is somewhere up in Alaska (you can Google the 70th parallel and it goes through the Arctic, and the Northern tips of Alaska. Positive values for longitude would be East of the Prime Meridian (somewhere in Europe). ??,Nevertheless, this once again would be a very simple thing to control with a numeric data type field, and a domain constraint on the acceptable range of values.??,Over the years I have had many debates with colleagues over the value of a rigidly controlled database versus a loosely controlled database. I think what some fail to understand is that well defined standards and structures enable flexibility, extensibility, reuse, and resilience of a database. I like to think of it in terms of plug-and-play. The hardware standards that evolved in the 90's revolutionized the computer industry, and it was all because of very well defined standards. Who recalls trying to find a sound card that would work with your Compaq computer?,The same applies to data management best practices and standards; they enable flexibility. Relational data maintained at the ATOMIC level, can easily present data in multiple views of the same data, dependent on the requirements. Aggregate, dimensional, and fact tables can be created as views, and then easily modified to accommodate changing requirements. The same applies to Big Data analysis.??,In my opinion, there are very few legitimate reasons for removing constraints, indexes, and denormalization. Complexity and performance are the two arguments I hear most frequently, and neither hold water in my opinion. Yes, there are use cases where some exceptions are made, but they are few and far between. I also believe that,Big Data is simply a lot of small data. That said, there are legitimate use cases for a Hadoop platform (e.g., sensor data, clickstream analysis, realtime predictive analysis, AKA Amazon, and Netflix), but my personal opinion is that integrated platforms with a reputable RDBMS like Oracle, is the way to go. My next post will be on the topic of using Oracle R Advanced Analytics for Hadoop on Oracle's Big Data platform.,To wrap up this series of posts on data integrity, I hope you have taken something positive away from the discussion, and if nothing else have a greater appreciation for the importance of data integrity, as well as a better understanding of what is involved in maintaining clean data. And please, if you disagree, or find errors, or whatever, please feel free to leave a comment. I would love to hear your opinion on the subject. I will provide a link to the code, exploratory plots, and maps on RPubs later this week.??
See below an example of an Analyticbridge email campaign that was monitored over a period of about??,. It clearly shows that 20% of all clicks originate after day #5. Yet most advertisers and publishers ignore clicks occurring after day #3. Not only 20% of all clicks occurred after day #3, but??,??(in terms of conversions) occurred several weeks after the email blast. Also, note an organic spike occurring on day #23 in the chart below - this could be due to our vendor (iContact) featuring the newsletter in question on their website, without charging us an extra fee.,This brings interesting points:,Thus, there's a systemic methodology flaw and bias when measuring half-life of your campaign (unless you use ad-hoc statistical methodology): the data is right-censored. How can you be sure that 97% of all clicks occur in the first 3 days? Maybe as many clicks will arrive between day 11 and day 300.?? But since your window of observation is only 3 days (or at best 14 days), you just can't answer the question. You can compute good estimates for half-life though, if you use a statistical model based on (say) exponential distributions, together with statistical inference, and integrate the fact that the data is right-censored, in your statistical model.,Below is a chart showing that even 60 days worth of historical data covers only 95% of your campaign in terms of clicks - and indeed much less in terms of revenue:,??,Here's an example where the data and conclusions are biased and wrong due to ignorance of the "right censorship"??principle??that applies to time series:??,: You don't need to track your email campaign for 600 days to measure ROI, you can monitor your campaign for 28 days, and then make interpolation using statistical models. Of course, if you started your campaign just before a great event (like Christmas shopping), then you need to take into account seasonality. That's where a statistician can help you. The above chart represents a campaign generating about 6,000 clicks.??
What would happen if you replaced , ???,??,Determine the average, minimum, maximum number of days between visits and the count of the total number of visits by gender and frequent buyer level for the months of June and July 2008. Marketing would like this information sorted by gender and frequent buyer level.,??,??,??,Detect customers that purchase the same category of items in three baskets in a row with a total value of those items greater than $150 just before Halloween (Use the month of October 2008).,Display the customer name, category name and three month basket value in the output.,??, 
In this blog, I will be discussing some distinct types of data involved in feedback. The types that I will be covering are as follows: 1) structural; 2) event; 3) quantitative; 4) contextual; and 5) systemic. In 2014, I recall reading a number of blogs about three types of data: prescriptive, descriptive, and predictive. There was a data scientist apparently on tour lecturing extensively about these three types. I don't recall the individual's name. Well, prescription, description, and prediction are actually "applications" of data rather than data types. In my own blogs focused on prescriptive and descriptive data, I made some effort to describe the nature of the data - particularly its organizational role - since I recognized that the conversation was mostly about role or application. However, in this blog, I will be discussing actual types of data: e.g. boolean, byte, int, double, long, char, and String are types of data. Perhaps some readers that program are comfortable describing objects as data. Certainly the five types that I will be describing should be considered objects in relation to the example below.,I will be using a single example throughout: a technician, Rose Reliable, has submitted a stack of completed work orders to administration; unfortunately, the server that normally handles the record-keeping went up in flames. Jerome Pensive is responsible for constructing a new data system from scratch primarily to help the company make better decisions but also to determine proper compensation levels.,#1 Structural - Pensive normally receives stacks of completed work orders from all sorts of staff in many different departments. Pensive decides that - apart from handling the submitted records - he has to start maintaining a record of the source of the records. For many companies, the source might be the department and name of the person submitting the documents. Pensive has something more elaborate in mind. He wants to keep track of the setting that brought about the data - details pertaining to the "environment." He wants to know the locations that Reliable worked at, handling which types of appliances, and during what hours. Since Reliable is one of the few female technicians working for the company, Pensive is interested in what work conditions seem to contribute to the following: high levels of production; superior quality; and excellent customer satisfaction. He reasoned that some male technicians might exhibit less teamwork when working with Reliable; this can adversely affect her stats. Although Reliable might be affected by gender bias and harassment, Pensive reasoned that all workers probably have settings, situations, and conditions where they operate best. Consequently, Pensive would like to start keeping "structural data" about the entire staff.,There isn't necessarily a single way to form structural data. Consider the relational-neural structure that I discussed in an earlier blog as shown below, which clearly can hold a lot and all sorts of data.,#2 Event - In terms of the actual work orders, there is a need to extract production metrics. If only the documentation itself said something like, "Hey, this is a production metric." Alas, specific criteria need to be established in order to establish what is or is not something. I am describing an ontological exercise. Pensive notes that Reliable, like most of the other technicians, has preferences in terms of what work gets done, under which circumstances, and using parts from which suppliers. There are certain "combinations" that the company frequently promotes. The company is aware that certain combinations seem to work poorly, resulting in complaints. Pensive decides that the new system has to maintain a record of combinations in terms of the parts used and services provided for which appliances. In effect, Pensive wants to work towards a system that evaluates the judgment and level of skill of the technician - along with his or her willingness to take directives. He therefore intends to collect "event data" pertaining to each completed job. Event sequences can be used to record combinations.,In my own projects, event data is simply made up of fieldless and non-relational strings as shown below. Again, this is not necessarily the only way to form the data. An argument can be made that event data seems similar to structural - differing only in application - and maybe the use of "[ ]" rather than "/" parsers. Such an observation would be fundamentally incorrect. Structural data contains information about setting while event data is concerned about the existence of conditions. The data types do not "contain" the same kind of information: the setting is unlikely to represent criteria leading to scorable events - although I suppose sometimes it might.,#3 Quantitative - So far, none of the data that Pensive will be collecting is quantitative in nature. The attachment of quantity to something does not mean that the quantity fully or even slightly represents both structural and ontological event data. It is a bit like attaching a price to stock that, in the trading days to follow, might fluctuate dramatically even if the company itself remains essentially the same. Nonetheless, quantities can be attached to structures and events for example in relation to a formal process of performance evaluation. After using criteria to set the defining events, a method of value attachment becomes possible. Pensive decides to use an elaborate scoring table so each type of event or sequence of events results in a particular score. In this way, he gains access to a running total of important operational metrics or "quantitative data." While quantitative data might be the most familiar to most people particularly data scientists, it represents a small fraction of the data from the three types mentioned up to this point.,A table can be used for the quantitative form. A table externally defines the meaning of data - making it seem that only the numbers make up the data. But internally defined quantitative data might be expressed more like the contents of an .ini file. (The use of brackets is my own practice in order to reliably perform rapid tests: e.g. if(line.indexOf("[quoted_cost]") != -1) preventing a false-positive if the line contains something like quoted_cost_today.),#4 Contextual - Assume that Pensive has successfully implemented the new data management system. He has large amounts of data pertaining to each worker, the work he or she has performed, and in what workplace settings and circumstances. One morning he receives a call from Reliable's manager: it seems that Reliable has been absent on a number of occasions during the busy season. The manager would like some stats to get a better sense of Reliable's overall performance. Pensive has an enormous amount of data. Although absenteeism is at issue, it is a fair bet that the manager wants something more elaborate than attendance data. What data makes the most sense to send? It is illogical to send everything. However, it would be unfair and non-constructive to provide data about Reliable in isolation - for example, not comparing her to coworkers. Consequently, to some extent, Pensive is responsible for setting the parameters in which decisions will be made. I call the fourth types of data "contextual data," which is the data that is sent to address "in context" a particular request. This might not strongly resemble the data that is gathered, forming the underlying basis of the response. For example, I consider data returned in plain language a type of contextual data since language itself can serve to set the context of interpretation. Context is something that is attached to the base ontology and environment similar to the attachment of a quantity. Contextual data addresses the question, "What is this supposed to mean to me?",The contextual data form shown below might not make sense without some preamble. Assume that a [325C] event is thrown after a job is successfully completed. With the [325C] criteria satisfied, there are a number of comments possible in different contexts. Contextual data is a tangible concern rather than something I just invented for this blog. The data in a computer is destined for two entities: humans and computers. Within these two groups, there are major partitions that prevent the direct conveyance of meaning. It would be a mistake for instance to directly transfer data from an HRMS to a GIS; from a GIS to people in accounting; from logistics to the board room. The English translation is as important as the original German manuscripts, one might say.,#5 Systemic - I call the fifth form of data "systemic data." Pensive maintains data resources regardless of whether or not he expects to receive calls from managers. In any large organization, there are many potential data recipients or clients. Thinking of the organization as a system, the signals that are sent to different parts can trigger particular responses. Data that is sent to the right places and posed in the proper context might lead to more desirable outcomes than data sent to the wrong places in bizarre contexts. A system is a living thing. Organizations are alive. People are moving around. Their roles change. Departments are constantly adapting along with functions and resources. The marketplace is dynamic with competitors springing up constantly. What data to send, where, and in which contexts are questions that can never have the same answers for long. I would compare the established connections within organizations as cultural niches of migrant populations. The question of data relevancy really depends on this perpetual process of internal reconstruction. Systemic data is organizational data in a literal sense although contextualized in informational terms.,I consider it difficult to paste the contents of anything resembling a system here. But I offer a few lines of how data formed by the handler [hr_type] might be distributed to HR, accounting, and Reliable's manager.\,[hr_type]=[Reliable][field][operations][current][month7][day4][year2015],[hr_type]/hr/recruitment/Thomas Grady/performance,[hr_type]/accounting/auditing/Judy Lancaster,[hr_type]/manager/performance,It is common in relation to "systems" to emphasize the following basic pattern: inputs, processes, and outputs. Businesses talk about production systems where raw materials are processed into finished products. In environmental studies - reflecting my background - a system also involves feedback. The processes of an organization might change according to feedback. In an example a professor once shared with me, exposing water to heat can cause the water to boil; the water responds to energy by becoming more complex. (Consider this more of an analogy than conceptual explanation.) In society, social structures may have emerged in response to hostile natural and man-made environmental conditions. In business, restructuring is common in the face of changing market conditions. Transformations also occur within the workplace - for example, after the introduction of computers. For individuals given performance feedback, there is often an expectation of behavioural restructuring.,I suggest that data types for feedback can affect both the message being conveyed and the dimensions of any response. Consider a scenario where the data structures cannot convey meaning adequately to ensure an effective response. For example, perhaps the company is incapable of expressing seasonal fluctuations in labour demand. In order to address over- and under-capacity, it is necessary for specific individuals in operations to know where labour is needed, when, doing what specific jobs, to achieve what particular outcomes. What if the company only kept sales data? Sales data provides little guidance on labour requirements except for the broadest trends. There are opportunities in feedback to support structural initiatives for organizations assuming there is adequate sophistication in the types of data used.,I believe that in the rush to incorporate changes to workplaces, it has been tempting to use computers to handle processes in the same old ways. Traditional data forms reinforce patterns of behaviour and existing conventions. I am referring primarily to externally-defined data, often quantitative - for example, the sort that might be found on a relational database. This type of data is not necessarily part of a coherent indigenous "system." The use is merely customary or prescribed. Production might be unaffected by such data. However, feedback would likely be impaired. If an organization has to adapt to challenging conditions - if it is already losing its war - efficiently performing the same behaviours can lead to certain defeat. The cornerstone of organizational adaptation is its data: not just what it collects but what it "can" collect - what the data is capable of conveying - through different structures.
Big Data is powering Big Business these days, but could also spell big legal troubles if not handled properly.,We invite you to a two-day workshop on Legal Issues and Big Data that Gary Rinkerman and I are conducting at New York University on July 30-31 in NYC. This two-day seminar covers the legal issues affecting big data, data analytics, and data-driven business models. Specifically, we will review the intellectual property, contractual, and regulatory legal environments for data-driven businesses. For example, we discuss under what circumstances intellectual property rights apply in the creation of data-driven businesses; what contractual models are appropriate; and what governmental regulations currently in play and on the horizon to constrain, limit, or support these businesses. If these are issues you are dealing with, we hope you will join us.,You can get more details and sign up at the NYU website: ,Dr.?? Andres Fortino
Here I put together a number of new data science techniques to solve a real life problem: identifying good articles to write and publish (or to harvest and re-post) on a website, and??re-tweet them with the optimum frequency, given a specific audience. The focus is on scoring articles based on selected features (keywords in subject line, author, channel and many more), feature selection, data generation and harvesting to solve the problem, automatically categorizing and tagging articles using ,, and??predicting the lifetime value and total page views of an article. We also discuss ,, and use both , and , for scoring articles.,The methodology applies to many contexts, not just digital publishing. It applies to situations in which a lot of unstructured text data needs to be processed (categorized and scored using natural language processing methods). In the context of digital publishing, our entire system described here can be viewed as IoT (Internet of Things) for the media industry: automated selection and distribution of content via Hootsuite, to optimize pre-specified goals. This involves automated machine-to-machine communications via API's (Google Analytics, Twitter, Hootsuite) to deliver optimum content at the right velocity. Journalists and editors are being replaced by software.,For the data scientist reader, this article is putting together many of the techniques that I recently developed, and represents an interesting case study. It is my final contribution to my upcoming book ,, though I will write one more book later, ,.??It is also a follow up to my article ,. Reading this article will help you understand the context.,This article is divided into multiple parts. Some are completed already, some will be written in the coming weeks. From a technical viewpoint, we used Python, R, Excel, web crawling, and API's, as well as modern machine learning.,The data available consists of pageview counts for thousands of active, live URL's on our websites. The data is accessible from Google Analytics. By crawling our websites using a Python script, we were able to add a ,??as well as , to each URL. Note that a URL represents a web page: an article, book, event, announcement, or generally speaking, any piece of content on our websites. One web page can have multiple URL's (a mobile and desktop version, or a version with a specific query string attached to it, to identify the traffic source). Deduping URL's, to get one URL per page, is an easy process. From these basic, raw metrics, tons of compound, useful metrics are derived: see Part 3, 4, and 5. The basic methodology for data harvesting (with source code) ,. A recent version of the base data set??,??(CSV file),We are considering using an alternate source of data, to increase accuracy. Instead of using Google Analytics, we could use the ,??to extract, for each article: the URL, title, creation date (approximately equal to the associated weekly digest time stamp). And by crawling each URL, we could also??retrieve??the pageview count, as this metric is published on each webpage, on the website itself (it's a public metric).,Crawling is performed once a month, incrementally: each month, we look at URL's found in Google Analytics, but not yet found in our database of previously crawled URL's. We add them to our database, with the relevant metrics for each new URL. A full, comprehensive crawling can be performed slowly in the background or every six months, to eventually detect URL's that no longer exist, and to improve accuracy of both pageview counts and article lifetime values.,The purpose is to categorize articles using a very simple and scalable technique known as??, or automated tagging, to attach a pre-specified category or , (Python, R, Big Data, Hadoop, Excel, NoSQL, Visualization, Clustering, Machine Learning, etc.) to each article. The category is a , metric: it is derived from the raw metrics, essentially, from the keywords found in the title of each article, or some tokens or keywords found in the URL. Thus the first step consists of producing a listing of all keywords found in all URL's and titles, filter out keywords that don't make sense or with low frequency (ex: "mining data") and keeping the popular ones (ex: "data mining"), and rank them by popularity. This exercise is also important to identify the seed keywords.,An alternative to the indexation algorithm is to use the Google index: ,??(illustrated - when you click on the link - for the keyword Machine Learning)??for pre-specified keywords. These pre-specified keywords are manually-produced ,??derived from the big keyword list, as described in the previous paragraph,. Check out (automatically, with a Python script) ??which articles are returned for each keyword and in which order, and assign the keyword (category) in question to the article in question, somehow taking order into account.??,This category metric is expected to have significant value to identify the best articles, and to predict lifetime value of an article. Note that the category is a better metric than the tags entered by authors, for each article. User-generated tags can be erroneous or missing: it's unstructured data. The category generated by our algorithm is a well structured piece of information, by design.,: To provide an analogy with chemistry, a , is similar to a , or ,. A raw metric is similar to an atom. A molecule is made of several, usually different atoms, bonded in various ways. It can be simple like water, or far more complicated like plastics. Likewise, a compound metric is a function / combination of raw metrics found in the raw dataset. It can be a simple ratio of two raw metrics (pageviews per day) or something more complicated (category assigned to an article).??,The purpose here is to produce normalized values for pageview counts. An old article obviously has more pageviews than one published yesterday, regardless of quality. Just like in Part 2, the normalized pageview count is a compound metric, this time derived from the observed pageview count and the creation date, for each article. Survival models can be used in this context, but we found a simple solution described in details in our article on page decay:??,??of the article in question. In that article, the normalized pageview count is called ,. The metric is indeed used to score articles. This problem is considered solved, and the solution (at least a decent approximation) is,Score( article ) = pageviews / (time elapsed since creation - some offset).,In short, we've found that popular articles don't experience observable decay, because the traffic growth over time compensates for the natural decay. So decay is hidden by general traffic growth. For all matters, it is as if there is no decay. Should a decay become noticeable in the future (or to the contrary, if growth more than compensates for the decay), ,??can be adjusted accordingly.,We try to predict two things: the popularity or score of an article (lifetime value, decay, pageviews), as described in Part 3, and which articles / with what frequency, to re-tweet over time. The latter is discussed in details ,. Modeling is further discussed in Part 5 and Part 6.,Here we focus on the ingredients (the metrics) rather than the recipe, to predict popularity of an article. Metric selection is a key feature of any data science project. Doing it wrong leads to bad results. In this particular case, many of the useful metrics are compound metrics. Domain knowledge is useful to create a great list of metrics, to eventually ,.,One critical part is to detect and filter out time-sensitive articles. An event taking place in 2012 should not be re-tweeted in 2016. This is easily achieved based on the URL (containing keywords such as ,, , or ,). So here we assume that these time-sensitive articles have been eliminated from our database.,The first step consists in creating a,, in this case:,with frequency count and source (title of article, URL path, URL domain, body of article) for each keyword, after eliminating stop words ("the", "what", "when",??etc.) Look at all 1-token, 2-token, 3-token keywords; eliminate those with low frequency. ,??sheds some light on how to handle keywords, an NLP (natural language processing) technique. A list of top keywords will be published shortly.??,Then, we need to identify candidate metrics:,Not all metrics have been gathered in the actual implementation of this project. These metrics are listed for illustration purposes.??,We use an approximate but robust regression technique known as ,,??as the auxiliary scoring system to be blended with pseudo decision trees (see??Part 6) to predict the popularity score for each article. Given the fact that scores have a ,, we will instead predict the logarithm of the score: we will apply a log transformation to the raw score defined in Part 3, before performing the regression. ??,Note: for each article, the click rate - the metric that we indirectly try to predict here - is determined mostly by the subject line, and by metrics derived from the subject line (see Part 4); shares/likes are determined by the actual content (the body of the article).,The final part consists of blending the Jackknife regression with a simple, scalable, robust ,??to produces popularity scores for all articles, based on the metrics identified in the previous steps. Once this part is completed, we will also discuss ,, how to smooth regression parameters across multiple regressions performed on multiple buckets, as well as data bucket aggregation.,Finally, fine-tuning the algorithm, to provide more accurate predictive scores, is performed via cross-validation. Performance can be measured using robust, outlier-insensitive ??metrics such as ,??or ,??between predicted and observed values. Confidence intervals for scores ,, even though no actual statistical model has been used.??,As you can see in Figure 1, starting around early March 2015, when we deployed our system in full force, the growth started to suddenly and significantly accelerate. It is as if you have a regression line for data prior to March 2015, and another one with a steeper slope, thereafter. In statistical terminology, the date when the change occurred is called a ,. Note that the accelerated growth is also due to external factors, such as publishing more, high quality articles, but these external factors are somewhat connected with this data science project, and are a by-product of the insights derived. Indeed, our competitors did not experience the same accelerated growth. The numbers in Figure 1 are for one of our channels, and do not represent our entire traffic.,To summarize, this data science project is a success story that helps us, on a monthly basis, identify the new articles worth publishing, and the old ones worth re-tweeting, automatically via @Analyticbridge (100,000 followers), and with the right frequency. Accurately measuring the yield (that is, ROI) is not easy, but clearly the results are spectacular, and can be attributed to this project, as evidenced when we look at traffic statistics - the number of additional, high-quality users coming from this traffic source, and measured by Google Analytics and elsewhere in various ways (pageviews from Twitter or Hootsuite, sustainable spike in monthly new members, pageviews per article, etc.),Still we continue to also write less popular / more specialized articles, of interest only to a specific audience, or dealing with a peculiar technique or industry: content variance, and originality, is a key component to success. We could perform some A/B tests to find out the optimum balance between core, popular content, and specific, niche articles.
Doing periodic reporting always feels like it's going to be easy, just make another copy of everything, change some dates, and run it again! But stakeholders have a way of looking at your hard work, thanking you for (if you're lucky) and asking for exactly the new view that was the hardest to implement. How do you build out a report that won't become tangled mess when all of these requests pile up?,The first thing to understand is that you can't predict the future. You, an analyst, should understand this better than almost anyone in the organization. You spend all day every day trying to help decision makers predict the future, and the impact of their decisions will have on it. You know how many factors can throw off carefully laid plan, and need to have the humility to incorporate flexibility into your own project. The most important thing you can do is go into planning and understanding that things will change. So do it by hand first time, isn't inefficiency -- it's a rational response to the exploratory nature of your first round of reporting. Think of it another way, don't make a huge investment in building out an optimized data pipeline to a data lake until you've manually collected and tested some of its water.,My next piece of advice, after "do it by hand first time" is even more controversial -- do it by hand the SECOND time as well! And third, if you can get away with it. The reason is that your stakeholders don't know exactly what they want until they see it, and it's only when they are regularly taking your results without additional requests that it makes sense to start setting the pipeline in stone.??,During these first two rounds by hand will be painful, since at least half of the work will probably be exactly the same.??A famous advertising executive once said "half of my money is wasted, I just don't know which half". Your situation is a little bit different: Half of your effort is wasted, and you can't PREDICT which half. Fortunately, once you have a few examples under your belt, you will have a much better understanding of what the stakeholder will need each reporting cycle. That is how your manual slog through the first few iterations will pay off in spades, when you build your automated pipeline right the first time.,??,Read the original version on my blog 
Let us analyze who wins the popularity contest by comparing the number of searches for each of the terms.,: Each of the keywords Tea, Coffee, Beer and Wine were used with Google Trends to extract the weekly data and then plotted as below. Year 2015 is excluded due to partial year.,A simple group bar chart trended over years should give a clear picture for the winners and losers.,As can be seen by the super imposed trend lines, Tea and Beer have been gaining increasing popularity whereas Coffee has been on the decline since 2012. The popularity of wine has been on the decline since 2005 and has found a constant level since 2011-2012,Let us flip the dimensions and view them as a stacked area chart,There was a dip in the overall popularity of the four drinks as seen by the trough between years 2007 through 2011.,Now let us aggregate the searches by Quarter and see if there are any cyclical patterns,Interestingly all the peaks on the above chart occur during the 4th quarter of every year.,Let us zoom in and see it by the month.,Interestingly, we see there is a peak for roughly every 1 or 2 months interval.,And finally the searches by each week, 
Marc Zuckerberg from Facebook is not thinking in terms of what new features to add. Marc thinks about??,. More engagement does not only lead to more advertising revenues; it leads to more engagement resulting in more data.,To get this wheeling spinning enormous amounts of data are needed, coming from different sources. This data needs to be processed in real-time and ?€?interpreted?€? by emerging technologies such as Artificial Intelligence.,The Facebook Timeline once started as a simple log of activities from your friends. It is now moving towards new frontiers taking into account a wide variety of data, including:,Another example is the recent??,. It uses machine-learning models (software) to interpret and predict pricing.,These algorithms are fed by data, such as seasonality (for example the date of popular events such as the SxSW conference in Austin), unique features of a listing (such as price, review sentiment, and rating trends), and the performance of other listings in the neighborhood. Without this data, there is no foundation to make dynamic pricing work.,Also Uber is famous for its??,??(aka surge pricing) to keep demand and supply in balance. Whether it is fair or not that the same ride has a varying price tag, the fact is that such models are impossible without data.,To maximize the number of Uber rides, the algorithms are??tweaked in such a way that when demand is at its peak, the price is??automatically adjusted, thereby maximizing conversions and revenues.
Please see the following list of videos that I have created to document some of Asters 120+ Analytic Functions:
 
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??
Yup. We scraped 25,000 tweets about gay marriage in the few days surrounding the ruling and found that about 80% of them were positive. That's a staggering metric, far exceeding what we see about nearly any other topic we've run sentiment around. In other words, that one uncle from Cleveland is part of an ever-shrinking (and yes, fairly vocal) minority.,So what did the 20% have a problem with? Which states were the most positive? And were men or women more supportive overall? Those answers, below.,Buckle up: here come the graphs.,Let's start with that first question: why did Twitter users disapprove of the gay marriage verdict anyway? Keep in mind, again, that only 20% of ALL tweets were negative, so the following graph applies to those and those alone.,To wit: negative sentiment skewed towards the religious. Some of those tweets aren't really fit to print on a family blog, but read the this one and you'll get the gist:,Biblical and religious gripes were followed by grousing about a loss of "American values", a complaint that was sometimes appended to a idle and heavily ironic threat to move to Canada. We say idle because people love throwing up their collective hands and threatening to move north with our continental neighbors; we say ironic because gay marriage has been legal in Canada for a decade. Googling things can be hard.??,When we broke down the tweets by gender, we found that while men were nearly three times more likely to express positivity about the ruling, women were more than??,??more likely to do the same.??,Men also had a higher tendency to simply tweet out news links about the case (which is what the majority of all "neutral" tweets were: simple links to the NY Times or similar outlets announcing the verdict free of weighted opinion).??,As we mentioned above, the first thing that struck us when we looked at the data was the overwhelming support for the ruling. That bore out across not only men and women, but across each and every state in the union.??,Alabama lead negative sentiment, which shouldn't be altogether surprising. This is a state whose House voted 79-12 to forbid same-sex marriages in the late 90s, then actually became more strident in 2006, with an 85-7 vote. Voters there agreed with the ban to the tune of 80%.??,, with all that considered, only 30% of Alabaman tweets were negative. Maybe the folks who hate the ruling were quiet last weekend or maybe they were registering their disdain on some other medium, but low negative numbers in a state that roundly rejected gay marriage less than a decade ago is, well, pretty promising.,We went into this exercise bracing for a lot of negativity. We bet wrong. In the days after the Supreme Court (narrowly) granted equal marriage rights to same-sex couples, America resoundingly applauded. And when you consider where we were as a country in the early 90s, that's nothing short of amazing.,First, we scraped Twitter for tweets using various search terms and hashtags, avoiding weighted ones like #LoveWins, where essentially every message was roundly positive. (For the record: 90% of our tweets had the terms "supreme court", "#SCOTUS", or "gay marriage" in them.),Next, we ran a preliminary job to remove tweets from brands and news agencies, as well as the irrelevant tweets that always pop up during popular discussions. We made sure tweets were in English, from real live people, and were about gay marriage or the Obergefell v. Hodges ruling. After that, we ran a sentiment analysis job where we had our contributors note whether a tweet was positive, negative, or neutral, and, if negative, choose a reason (we only ignored positive reasons because they were roundly identical). Lastly, we ran a pair of demographic jobs to determine user's locations and gender.,Have questions? Email me: leena.kamath@crowdflower.com,Thanks for reading!
At the heart of good marketing is data. And when data drives your marketing, the average return on investment (ROI) is a whopping 224%! (,),VB Insight surveyed over 3,000 marketers and looked at tools used on over 3 million websites. Even small improvements have the potential to drive higher ROI. According to the research, if your business has a 4-phase customer acquisition funnel, and you improve each phase of the customer acquisition funnel by as little as 5%, the overall improvement is 22% at the bottom of the funnel.??,Customer data has clearly become a valuable currency for marketers of all sizes and across all industries. However, if you feel as if you are downing in a sea of data, here are some key ways to drive more revenue with better marketing and better data.,You may have loads of data, but if it?€?s sitting in different systems and in various formats, your data is doing you no good. For example, your billing records may be stored in one system with certain customer details, and your customer service logs may be stored in another database with an entirely different set of customer information.,Each of these sources contain important customer details ?€? basic information such as contact details, as well as more detailed information, such as behavior information (frequency of purchase, transaction value, email click-throughs, and so on.),A data management solution should be implemented to integrate multiple data sources and automate data quality processes. Data management software will perform functions such as:,By merging these systems of data into a single repository, customer details come together into a 360-degree view. With this singular customer view, you can apply analytics to accurately understand what your best customers look like and find answers to key questions such as:,??,Your customers ultimately want to be understood ?€? they expect a seamless experience with your company, want personalized offers and are more receptive to messages sent through the channels they prefer., how they felt about their data being collected by companies to establish this comprehensive customer view. The overall feeling was that if their marketing data allowed companies to personalize offers, provide better customer service, or prevent situations in which they have to give the same information to multiple people over and over, then it was a good thing. One respondent said, ?€?I don?€?t want to have to repeat my information to 5 different people when I call their support line,?€? and according to another, ?€?It dramatically increases the likelihood that I would continue to give a company my business.?€?,Research shows that 45% of consumers are more likely to shop on a site that offers personalized recommendations, and 56% are more likely to return to a site that recommends products. (,),When deciding which types of personalization to utilize, consider the following statistics from ,. Consumers purchase more from brands who:,??,??,While there is obviously huge uplift when personalizing consumer interactions, many marketers are unaware of the negative impacts of sending impersonal messaging. A , revealed that 67% of consumers have unsubscribed from an email list when sent irrelevant information. An additional 43% ignored future communications from the company, and 32% stopped visiting the company?€?s website or mobile app.,At the heart of good marketing is data. Your data can tell you who your customers are, what will keep them loyal, what offers they will respond to, and ultimately, how to drive higher revenue.
In my??,t, I covered setting up the basic tools to start doing machine learning (Python, NumPy, Matplotlib and Scikit-Learn). ??Now, you are probably wondering how??to do this on a very large scale, involving terabytes (may be even petabytes) of data and across several??server nodes. ??,The best answer is Apache??,??! ??Spark is an??in-memory??analytics engine which??runs on top of HDFS and also unifies many other data sources e.g. NoSQL databases??like MongoDB or even CSV files. Spark is also a much faster and simpler replacement of Hadoop's original processing model - MapReduce. ??IBM has announced plans to include??Spark in all its analytics platforms and has committed ??3,500+??developers to Spark-related projects.,The picture below shows how Spark plays across applications, data sources and environments.??,A ton of material is already available to tell you the benefits of Spark. ??I will keep it??short and simple. ??Spark is great because it is free, it is Open Source, ??General Purpose, ??scales up massively (I mean up to??,??nodes and Petabytes of data) and has amazing speed (~100x??faster than traditional Map Reduce. details??,!) and comes with??a delightfully elegant programming model! ??BTW, it is the programming model with deep roots in Functional programming that??won me over. Details??,!,Spark is also the hottest project in Apache.,??,How do we get started? See this ,??taught by Professor Anthony Joseph of Berkeley. ?? The lectures were??broken in small bite-size videos (3 to 4 minutes maximum) which are simple and very nicely explained. ??Some are followed??by quiz questions which helps to validate??the knowledge immediately. ??The entire course environment??is provided in a Virtual machine (you need VirtualBox, Vagrant, and the image) which is runnable on your??laptop. ??The best part of the course were the 4 labs. ??The labs came as iPython notebooks with sample exercises. ??Each exercise was followed by tests which gave a pass/fail result immediately. ??,The first lab was meant to count the most frequent words in ALL????of Shakespeare's plays. ??The second lab provided WebServer Logs from NASA and asked students to parse the Apache Common Log Format, create Spark RDDs (that is Resilient Distributed Datasets), and analyze how many valid requests/responses (200X), how many failed, which resources failed and when!,A screenshot of a section of this lab to visualize 404 responses by hour of day is shown below.,The third??lab provided??product listings from Google and Amazon and the objective was to use a Bag-of-words technique to break up product descriptions into tokens and compute similarity between products. ??A TF-IDF ??(Term Frequency and Inverted Document Frequency) technique was used to compute similarity between documents of product descriptions. ??Learning such powerful text analysis techniques to do entity resolution would??be a real asset to solve live problems.,The fourth lab was the icing on the cake. ??It analyzed ??movies from IMDB and came up with historical ratings of movies. ??A dataset of??500K ratings??came along with the VM. ????,A screenshot of a section of this lab to retrieve highest ever rated movies??shown below.,It is a lot of fun to see the highest rated movies and be able to run your own queries on the RDD :-),The lab used Spark's MLlib (Machine Learning Library) to use Collaborative Filtering. ?? This is a method to make??,. ??The basic assumption is if a person A has the same opinion as B on one item??x, then they are more likely to have same opinion on another item??y compared to a random person. CF was combined with Alternating Least Squared techniques to make predictions of movie ratings. ??Finally, the lab??asked the user to rate a small sample of movies to make personalized movie recommendations. ??You will love this lab!,Additionally, the course discussion group was??full of??questions and very supportive responses from the staff and other students.,I found this to be an excellent course, at the right level of difficulty and helpful in?? de-mystifying Spark for a beginner and putting it to actual use. ????,I am looking forward to the next in the series - ",".,Best wishes and best regards,,Somnath.??
Given all the buzz happening in the market around IoT, We looked at related projects in the crowd funding website Kickstarter.com to see how are IoT projects doing with respect to all the other ones.,We chose projects which have either ?€?IoT?€? or ?€?Internet of Things?€? either in their title or description and here are our findings.,The success rate of projects at Kickstarter is around 37.5%, for Technology projects it is 21% which is a lot less than the average success rate of projects. In Spite of this our analysis shows that the success rate of IoT projects is 44%, which is pretty good news. People are realizing the importance of IoT and are willing to fund the related projects.,??,The projects locations are almost concentrated in US and Europe with a few scattered in Asia and Australia,??,Because the projects are spread all over the world the goals of money to be raised were also in different currencies so to be able to analyse the monetary part we normalized all the numbers to US dollars.,The total sought out money for all the IoT related projects ( ongoing, successful and failed ) in Kickstarter is around $4.7 million and the actual pledged amount for the projects is around $1.5 million.,If you only consider the projects which have made it the total sought out and pledged amount is approximately $1.2 million. So only 2% of the pledged amount went to the unsuccessful projects which is usually the case with most of the projects on Kickstarter.,The average requested funding for all projects is around $60 thousand while the average funding requested by the successfully funded projects is around $44 thousand. For the failed projects it is $3500.,The top 10 successfully funded projects along with their links are given below
R has become a massively popular language for data mining and predictive model building with over two million users worldwide.?? The wide adoption of R has to do with the fact that it is available as open source, runs on most technology platforms and is commonly taught in academic institutions in courses with significant components of data science, machine learning and statistics. ??A recent study found that R is now cited in academic papers more often then SAS and SPSS, a change from previous years.,R fans feel the language mirrors how they think about problems. In addition, the way R works provides an abstraction layer from the data. This ability to analyze and manipulate data whether it is 1 dataset or a 1,000 datasets is critical as R is often used for the design and development of predictive models to eventually be deployed in Big Data production environments.,??,Because R is open source, anyone can extend it without asking permission and due to the rapid growth in adoption since the 1990s there are numerous packages that extend R beyond the original core functionality at no additional cost. The active and large open source community also provides a great deal of support.,??,R is often used to design, develop, train and test predictive models in a laptop / desktop development environment. While all of the above is true, R has some very important and real limitations. To quote the folks at Datacamp ?€? R was designed to make data analysis and statistics easier to do, not make life easier for your computer.?€?,??,??,??,??,??,The good news is the limitations of R described above can be easily solved. R can be exported to the Predictive Modeling Markup Language (PMML). Zementis provides software that consumes PMML and executes it against batch or real-time data in against many different computing platforms.?? What this means is that R users can design, develop, test and train as they always have without having to concern themselves or their colleagues in IT with the complications of model deployment.,??,Zementis software scores both batch and real-time data in a streaming way reducing memory requirements for scoring models. In addition, the software has been built from the ground up to optimize and scale PMML code for MPP computing environments. This means the user gets advantages of massively parallel processing without needing to do the complicated and time consuming programming typically associated with working in MPP environments.??,??,By using Zementis, a focal point for the administration, deployment and management of models is created allowing for efficiencies in the information technology environment and processes.?? And it eliminates the security risks associated with enabling CRAN in a Big Data production environment.,??,It is common for enterprises that are using R today in design and development of Predictive Analytics to have the analytics and data science team work with IT to re-code models in Java or C or another programming language for use in production environments. This process often takes many months and has several costs associated with it 1) loss of ROI due to delayed deployment;?? 2) data science team members spending significant amounts of time managing IT projects when they are a scarce resource; 3) ??there is an opportunity cost associated with IT resources being dedicated to re-coding the predictive models rather then focusing on issue that are higher priority for over-burdened and under-resourced IT organizations.,??,Outside of the pure financial cost issue there is a reputational risk i.e. cost to both business and IT executives who sponsor significant investments in Big Data and Analytics projects. Once a project is funded and underway it is critical that demonstrable results can be shown as quickly as possible to maintain an organization?€?s commitment and inertia for the project. By being able to immediately deploy predictive analytic models as soon as they are built, positive and quantifiable results of the project can be made apparent to all stakeholders. Enabling not only continuation of the original Big Data and Analytics project but expansion and initiation of additional projects as well.,??,By leveraging PMML and Zementis solutions, model deployment is immediate, ensuring that the associated ROI is realized. These solutions also enable models to be easily kept up to date as new data sources become available, the market changes, or better algorithms are developed.?? By leveraging Zementis a model developed in R can be run against the traditional enterprise data warehouse, a Hadoop cluster or real-time streaming data without custom coding. As these environments change over time, the future iteration of the model will persist in an abstract, logical package that can be run against whatever the new environment is. It is important to note that the benefit of using Zementis solutions is not just in new model deployment but in working with models that are being modified or retrained on a periodic basis as well.,??,If you are interested in taking R to the next level please contact me at mark.rabkin@zementis.com.,??,??
"
,Please watch this video and learn how we take multiple channels of data:?? Bank systems, IVR, Call Center Notes, Clickstream, and others to perform behavioral churn prediction.?? We do this at data scale and not using samples."

From Text and Data Mining to Market Research and Social Media Consulting, few are more influential than today?€?s guests. In advance of the??,??(Nov. 10-11, San Jose), Text Analytics News caught up with four analytics leaders who are helping connect and educate text analytics professionals on the Web.,The first marketing research firm to leverage modern text analytics, and currently in development of patent pending OdinText, Anderson Analytics has been a long time supporter of the Text Analytics Summit. CEO, Tom Anderson is the thought leader and proponent of social media analytics as well as advanced techniques in the field of marketing research. He founded and manages the largest and most engaged group of marketing researchers on the web, Next Gen Market Research (NGMR), as well as one of the first text mining groups on LinkedIn, Data & Text Analytics Professionals (DTAP).,Cliff Figallo has a long history of helping groups start online communities that will be both useful and lasting, and provides marketing analysis for the use of social media,Social Media Today is an independent, online community for professionals in PR, marketing, advertising, or any other discipline where a thorough understanding of social media is mission-critical. The site provides insight and hosts lively debate about the tools, platforms, companies and personalities that are revolutionizing the way we consume information. Content is contributed by members and curated by their editorial staff.,??,Most recently, he successfully launched??,??and,, the largest social network for analytic professionals. Thanks to their network of talented Statistical Consultants, Data Shaping Solutions also offers a wide array of expertise in design of experiments, time series, predictive modeling, survey analysis, customer profiling, pattern recognition, statistical testing, and data mining across several industries.,??,Data Mining and Dr. Gregory Piatetsky-Shapiro are inextricably linked. Before staring??,??he led data mining and consulting groups at GTE Laboratories, Knowledge Stream Partners, and Xchange.,He serves as the current Chair of??,, the leading professional organization for Knowledge Discovery and Data Mining. He is also the founder of Knowledge Discovery in Database (KDD) conferences, having organized and chaired the first three Knowledge Discovery in Databases workshops. KDNuggets remains one of the Must Go To sites for the Data Mining Community.,??,: I started publishing KDnuggets email Newsletter back in 1993, before the term social media existed, as a way to connect people who attended KDD-93, Knowledge Discovery in Data in workshop.,From the beginning it was designed to have social content - people would contribute and my role would be as a moderator - select most relevant content and keep the spam away.??,I added a website in 1994 and moved to current website??,??in 1997.????,In the last couple of years KDnuggets also added other social media channels (twitter, FB, LinkedIn), because this is where a lot of conversation in analytics space is happening.?? I find twitter.com/kdnuggets especially useful for broadcasting real-time or "smaller" items.,??,: For much the same reason that I started Anderson Analytics in 2005. Coming from the Marketing Research/Consumer Insights industry I was frustrated by how slow my industry was in adopting new techniques especially in the area of data and text mining.,I founded Next Gen Market Research (NGMR) in 2007 for like minded marketing researchers, though the membership of about 13,000 professionals now include those in several other fields from Competitive and Business Intelligence to CRM and Web Analytics. Analytics is the common ground.,: ??The idea started after checking large social networks set up by recruiters on Ning.com, back in 2007. I had a fairly large network already at that time, I decided that it would be useful to create one big network for all analytic professionals, rather than multiple independent smaller communities (data miners, operations research, statisticians, quant, econometrics, biostatisticians etc.),??,: ??I've been working in social media for 25 years as the technical environments have evolved. That's my profession, but the companies I've worked for have had various reasons for starting social media groups. In the current case, with Social Media Today, the founders recognized that there was value in providing central sites where writers on a range of specialties could share their ideas and talents with their natural professional communities.,I started a second group, Data & Text Analytics Professionals (DTAP) just a few days later for those of us who were more involved in the specifics of text analytics, that group now has well over 4,000 members.,??,??,: We see analytic professionals from government and from all industries (especially Finance, Health Care), as well as a good share of University students. Proportionally, consultants and startup executives are over-represented, while data miners from big corporations such as Google, Microsoft or IBM are under-represented. Job titles include web analyst, data analyst, analytic recruiter, database marketer, statistical programmer, military officer, head of analytics, scientist, VP of analytics, software architect, risk analyst, University professor, SEO or SEM expert, etc. According to Quantcast, our US demographics is as follows: 5 times more Asian than an average web site, 1.4 more in the 35-49 age range, 1.4 more with income above $100K, and of course 2.3 more with a graduate degree.,: NGMR is still heavy marketing research. In our last survey we had almost an 20/80 Client to Supplier ratio which is far higher than the other groups. We were also the heaviest US based research group initially, but we have much more global representation now.,Our visitors come for the engaging discussion. There?€?s no other place like it, where you can ask a question on almost any analytics topic and expect to get several great answers in a few minutes. Many members also share Information via their blogs (,??) or on Twitter, and the group now runs various competitions and is giving out our second annual innovation in research awards this fall.,: I have done a number of surveys of KDnuggets visitors and about 2/3 of them are technical people who analyze data, and about 10% analytics managers/directors.?? The rest are academic researchers and students.,: In the case of the Social Media Today site we attract practitioners in social media marketing, online community management, enterprise-level directors in marketing and PR, social applications development and business leaders looking for best practices in use of social media channels.,??,??,??,: We realize the need for more sophisticated text analytics to better understand what attracts readers to our republished content. Our audience is looking for answers and out of hundreds of articles addressing "best practices for Facebook" (for example), we need to be able to drill down deeper than categories and tags can take us.,: I use web analytics to understand the behaviour of visitors to KDnuggets.,I have experimented with text analytics and word clouds many times, but found that the results were rather predictable with most important words being Data, Mining, Analytics, Jobs, Technology, etc .???? So, I am still looking for an effective way to use text analytics.,: We have a special group dedicated just to text mining, see,.?? It features references, conferences, books and posting from members, including from myself. But many other text mining discussions are spread throughout the network, including in forums and groups such as?? Collective Intelligence and Semantic Web, Social Network Analytics, Web Analytics. Google??analyticbridge+text+mining to find more" to find more. Also, many Analyticbridge members??have included text mining or NLP in their domains of expertise, on their profile.,??,: Text Analytics is often discussed more generally in NGMR where market researchers are trying to make sense of what social media monitoring tools to use/not use, and understand what role if any text analytics should play in their specific research area.,??,The DTAP group tends to get a lot more technical, though there are also a lot more text analytics suppliers who are competitors (including my own firm) in that group, so the conversation there tends to be a bit more academic relating to text analytics.,??,??,: Text analytics is a key component of understanding social media, but it should also be integrated with social network analysis and data analytics.??,: Better detection of spam, commercial or irrelevant posts. Also by clustering members or groups using text mining techniques, one could create segments which can then be used for highly targeting advertising.,??,Other areas of interests: crime and infringement detection based on analyzing and classifying posts, analyzing corporate image (what people think about your product or company), and leverage postings from social networks by blending this data with internal company data to create richer data sets. This means creating a strong structure on otherwise loosely structured data, using text mining technologies such as NLP, text clustering, and taxonomy creation..,??,: Text analysis can help organizations better understand their communities of customers, fans, advocates and colleagues by surfacing commonly-used phrases and memes. Revealing the juxtaposition of key terms across hundreds or thousands of posts and conversations would reveal deeper levels of shared experience and sentiment. It would also bring more understanding of disagreement and conflict within communities, telling organizations how to better address and serve people with varied attitudes toward an organizations products and services.,??,: You really can?€?t separate data and text mining, and both have a critical role in helping to leverage social media for insights. We?€?re going to see more real time use of text analytics based models in the near future.,??,My problem is rarely convincing people that text analytics is critical for social media, but more often getting them to take a step back to realize where else they should be using it.,??,??,: Start with a LinkedIn profile, join analytic groups on LinkedIn, and see what other members are postings before contributing to the discussions.,You can search LinkedIn groups by keywords: some of these groups have more than 20,000 members, some but not all are vendor-neutral, some are very specialized, and some are very good at filtering out spam. Then visit or sign-up with popular analytic networks such as KDNuggets, AnalyticBridge, SmartDataCollective, Quora. Check what your professional association offers in terms of networking.,??,: Participate regularly and deeply on social media platforms - immerse yourself in them so that you understand them. Put yourself in the role of a marketing or public relations person and ask the questions that they would have about mining conversational content.,Try to understand the difference between text as "content" and text as "conversation.",??,: Contribute - where you know the material and topics.,Learn from others - see what they do right.?? It is a constantly shifting landscape.??,Have a sense of humor,??,: Just do it!,Don?€?t be afraid to ask questions.??,Try to contribute, people really appreciate it.,Realize just like traditional networking it?€?s a give and take, you need to be ready to return favors as well.
Check the three charts below: only one shows no pattern and is truly random. Which one?,Chart #1,??,Chart #2,??,Chart #3,??,It is very clear that chart #3 exhibits a strong clustering pattern, unless you define your problem as points randomly distributed in an unknown domain whose boundary has to be estimated. So, the big question is: between chart #1 and #2, which one represents randomness? Look at these charts very closely for 60 seconds, then make a guess, then read on. Note that all three charts contain the same number of points - so there's no scaling issue involved here.,Let's assume that we are dealing with a spatial distrubtion of points over the entire 2-dimentional space, and that observations are seen through a small square window. For instance, points (observations) could be stars as seen on a picture taken from a telescope.??,The??,??is the fact that the data is censored: if you look at the distribution of nearest neighbor distances to draw conclusions, you must take into accont the fact that points near the boundary have fewer neighbors because some neighbors are outside the boundary. You can eliminate the bias by??,: you need to use better visualization tools to see the patterns. The fact that I use a + rather than a dot symbol to represents the points, helps: some points are so close to each other that if you represent points with dots, you won't visually see the double points (in our example, double points could correspond to double star systems - and these very small-scale point interactions are part of what makes the distribution non-random in two of our charts). But you can do much better: you could measure a number of metric (averages, standard deviations, correlation between x and y, number of points in each sub-square, density estimates, etc.) and identify metrics proving that we are not dealing with pure randomness.,In these 3 charts, the standard deviation for either x or y - in case of pure randomness - should be 0.290 plus or minus 0.005. Only one of the 3 charts succeeds with this randomness test.,: even if multiple statistical tests suggests that the data is truly random, it does not mean it really is. For instance, all three charts show zero correlation between x and y, and have mean x and y close to 0.50 (a requirement to qualify as random distribution in this case). However only one chart exhibits randomness.,: we need a mathematical framework to define and check randomness. True randomness is the realization of a Poisson stochastic process, and we need to use metrics that uniquely characterizes a Poisson process to check whether a point distribution is truly random or not. Such metrics could be e.g.??,: some of the great metrics (distances between kth-neighbors) might not have a simple mathematical formula. But we can use Monte-Carlo simulations to address this issue: simulate a random process, compute the distribution of distances (with confidence intervals) based on thousands of simulations, and compare with distances computed on your data. If distance distribution computed on the data set matches results from simulations, we are good, it means our data is probably random. However, we would have to make sure that distance distribution uniquely characterizes a Poisson process, and that no??,??processes could yield the same distance distribution. This exercise is known as goodness-of-fit testing: you try to see if your data support a specific hypothesis of randomness.,: if you have a million points (and in high dimensions, you need much more than a million points due to the curse of dimension), then you have a trillion distances to compute. No computer, not even in the cloud, will be able to make all these computations in less than a thousand year. So you need to pick up 10,000 points randomly, compute distances, and compare with equivalent computations based on simulated data. You need to make 1,000 simulations to get confidence intervals, but this is feasible.,:??,:??,??
Read my full interview for AMSTATat??,. You will also find my list of recommended books. Here is a copy of the interview, in case the original article (posted on AMSTAT News) disappear.,(Dr. Granville's Interview for AMSTAT),Web and business analytics are two areas that are becoming increasingly popular. While these areas have benefited from significant computer science advances such as cloud computing, programmable APIs, SaaS, and modern programming languages (Python) and architectures (Map/Reduce), the true revolution has yet to come.,We will reach limits in terms of hardware and architecture scalability. Also, cloud can only be implemented for problems that can be partitioned easily, such as search (web crawling). Soon, a new type of statistician will be critical to optimize ?€?big data?€? business applications. They might be called data mining statisticians, statistical engineers, business analytics statisticians, data or modeling scientists, but, essentially, they will have a strong background in the following:,An example of a web analytics application that will benefit from statistical technology is estimating the value (CPC, or cost-per-click) and volume of a search keyword depending on market, position, and match type?€?a critical problem for Google and Bing advertisers, as well as publishers. Currently, if you use the Google API to get CPC estimates, Google will return no value more than 50% of the time. This is a classic example of a problem that was addressed by smart engineers and computer scientists, but truly lacks a statistical component?€?even as simple as na??ve Bayes?€?to provide a CPC estimate for any keyword, even those that are brand new. Statisticians with experience in imputation methods should solve this problem easily and help their companies sell CPC and volume estimates (with confidence intervals, which Google does not offer) for all keywords.,Another example is spam detection in social networks. The most profitable networks will be those in which content?€?be it messages posted by users or commercial ads?€?will be highly relevant to users, without invading privacy. Those familiar with Facebook know how much progress still needs to be made. Improvements will rely on better statistical models.,Spam detection is still largely addressed using na??ve Bayes techniques, which are notoriously flawed due to their inability to take into account rule interactions. It is like running a regression model in which all independent variables are highly dependent on each other.,Finally, in the context of online advertising ROI optimization, one big challenge is assigning attribution. If you buy a product two months after seeing a television ad twice, one month after checking organic search results on Google for the product in question, one week after clicking on a Google paid ad, and three days after clicking on a Bing paid ad, how do you determine the cause of your purchase?,It could be 25% due to the television ad, 20% due to the Bing ad, etc. This is a rather complicated advertising mix optimization problem, and being able to accurately track users over several months helps solve the statistical challenge. Yet, with more user tracking regulations preventing usage of IP addresses in databases for targeting purposes, the problem will become more complicated and more advanced statistics will be required. Companies working with the best statisticians will be able to provide great targeting and high ROI without ?€?stalking?€? users in corporate databases and data warehouses.
One of the most valuable tools that I've used, when performing exploratory analysis, is building a data dictionary. It offers the following advantages:,A data dictionary is a table with 3 or 4 columns. The first column represents a label: that is, the name of a variable, or a combination of multiple (up to 3) variables. The second column is the value attached to the label: the first and second columns actually constitute a name-value pair. The third column is a frequency count: it measures how many times the value (attached to the label in question) is found in the data set. You can add a 4-th column, that tells the dimension of the label (1 if it represents one variable, 2 if it represents a pair of two variables etc.),Typically, you include all labels of dimension 1 and 2 with count > threshold (e.g. threshold = 5), but no or only very few values (the ones with high count) for labels of dimension 3. Labels of dimension 3 should be explored after having built the dictionary for dim 1 and 2, by drilling down on label/value of dim 2, that have a high count.,category~keyword travel~Tokyo 756 2,In this example, the entry corresponds to a label of dimension 2 (as indicated in column 4), and the simultaneous combination of the two values (travel, Tokyo) is found 756 times in the data set.,The first thing you want to do with a dictionary is to sort it using the following 3-dim index: column 4, then column 1, then column 3. Then look at the data and find patterns.,Browse your data set sequentially. For each observation, store all label/value of dim 1 and dim 2 as hash table keys, and increment count by 1 for each of these label/value. In Perl, it can be performed with code such as $hash{"$label\t$value"}++.??,If the hash table grows very large, stop, save the hash table on file then delete it in memory, and resume where you paused, with a new hash table. At the end, merge hash tables after ignoring hash entries where count is too small.
When you see google ads on Google search result pages or elsewhere, the ads that are displayed in front of you eyes (should) have been highly selected in order to maximize the chance that you convert and generate ad revenue for Google. Same on Facebook, Yahoo, Bing, LinkedIn and on all ad networks.,If you think that you see irrelevant ads, either they are priced very cheaply, or Google's ad relevancy algorithm is not working well.,Ad scoring algorithms used to be very simple, the score being a function of the max bid paid by the advertiser, and the conversion rate (referred to as CTR). This led to abuses: an advertiser could generate bogus impressions to dilute competitor CTR, or clicks on its own ads to boost its own CTR, or a combination of both, typically using proxies or botnets to hide its scheme, and thus gaining unfair competitive advantage on Google.,Recently, in addition to CTR and max bid, ad networks have added ad relevancy in their ad scoring mix (that is, in the algorithm used to determine which ads you will see, and in which order). In short, ad networks don't want to display ads that will make the user frustrated - it's all about improving user experience and reducing churn to boost long term profits.,Here's our solution. There are three components in the mix:??,First create three taxonomies:,The two important taxonomies are B and C, unless the ad is displayed on a very generic web page, in which case A is more important than B. So let's ignore taxonomy A for now.??,. Taxonomies might or might not have the same categories, so in general it will be a fuzzy match, where for instance, the page hosting the ad is attached to categories??,??in Taxonomy B, while the ad is attached to categories??,??in Taxonomy C. So you need to have a system in place, to measure distances between categories belonging to two different taxonomies.,How do I build a taxonomy?,There are a lot of vendors and open source solutions available on the market, but if you really want to build your own taxonomies from scratch, here's one way to do it:,Let's say that (X, Y) is such a pair. Compute n1 = # of occurences of X in your table; n2 = # of occurrences of Y in your table, and n12 = # of occurences where X and Y are associated (e.g. found on a same web page). A metric that tells you how close X and Y are to each other would be R = n12 / SQRT(n1 * n2). With this dissimilarity metric (used e.g. at,) you can cluster keywords via hierarchical clustering and eventually build a taxonomy - which is nothing else than an unsupervised clustering of the keyword universe, with labels manually assigned to (say) top 20 clusters - each representing a category.
In this post I show how the accuracy of the classifier is influenced by the bag of words.??,The test has been done on a naive classifier but it returns good information about the data set.,The above image is an example of the bag of words used in the experiment (using the Closeness Centrality function): the red features depict higher CC values, the violet depict lower CC values
??,??,??,??,??,??,??,??,??
There are at least two definitions for Big Data: a broad sense definition and a strict sense definition. For the broad sense definition, Big Data includes all the possible available data on earth.?? For the strict sense definition, Big Data is a term for large datasets that traditional data processing applications cannot handle. We are going to follow this last approach.,For Davenport [2014] ?€?Big Data refers to data that is too big to fit in a single server, too unstructured to fit into a row-and-column database, or too continuously flowing to fit into a static data warehouse.?€? This definition matches the 3V definition: Volume: too big; update Velocity: too continuous flowing; Variety of formats: too unstructured.,In the definition of multimedia [Chapman and Chapman 2000] distinguish between dynamic (video, animation and audio) and static (photography, graphic and text) media.,Regarding text the dichotomy between structured and non-structured is usual. The structured text includes the traditional SQL databases, data warehouse and XML databases, as well as the more recent NoSQL databases.?? The unstructured text includes the plain text found in e-mails, forums, blogs and wikis. A growing form of text that is not tagged or specially formatted is the event logs.,Given the data taxonomy, in Figure 1, Big Data should include the three data sub-types: video, NoSQL and event logs.,Figure 1 ?€? Data taxonomy and Big Data,The term Big Data presented as a broad term is actually the union of three different issues. The aggregation of different problems creates further difficulty in finding possible answers. This issue should no longer be treated as a whole entity, but be divided into three sub-types of Big data: (i) too unstructured or complex: video; (ii) too big: NoSQL databases of web companies; (iii) too many updates: log events.,We believe SQL databases and Data Warehouses cannot be classified as Big Data, since they support traditional applications. Regarding the plain text analysis for web pages, e-mails, forums, blogs and wikis, considerable progress has been achieved in Information Retrieval, so we do not include it in the Big Data challenge. However, some authors mix the traditional and the new applications creating an even greater Big Data challenge.,The high-technologic based idea of massive parallel processing is not a solution, since we have three different problems for each data sub-type. The right approach to the Big Data challenge implies dividing the problem into three sub-problems.,With the advent of web 2.0 (a web of people) associated with mobile devices and the Internet of Things (surveillance cameras, video security systems, health care) the production of video increases exponentially.,While to search text is quite an easy task in the computing area, image processing involves algorithms with high complexity and is consequently very time consuming. When searching images, the human-machine interface is not direct. The dense and complex format of an image creates obstacles in the way the user asks the system questions. In video search with dozens of frames (images) per second the intricacy expands substantially.,Despite these difficulties, video analysis and in particular facial recognition of humans is becoming a major development.,The NoSQL technology appears in web companies like Google, Amazon and Facebook, to face the limitations of the traditional 30-year old relational databases technology.,The NoSQL solutions are divided into several groups:,On the one hand, NoSQL presents features to deal with large volumes of data using algorithms with better scaling.?? It also supports high flexibility in updating the schema during the runtime. On the other hand, SQL technology presents a large number of consultants, which allows rapid implementations and a unique ability in the integration of applications.??,To unify these approaches, NewSQL is a new class of relational databases oriented to high-performance applications on Web-based architectures.,The information regarding the execution of a computer program that is recorded in files is called the event logs. The increasing use of sensors in surveillance, industry and logistic magnifies the need of event logs analysis.,The Internet of Things (IoT) is a network of physical objects embedded with software and electronics. Each thing is uniquely identifiable by an IP address and connected to the Internet. In IoT the networks of sensors allow the digital meets physical (or more humorously the bit meets the atom). The increasing use of sensors in surveillance, industry, logistic and IoT magnifies the need of event logs analysis.,There are many approaches to deal with event logs. In order to detach one of them, we can refer Process Mining. Process Mining is a Data Mining technique which used event logs to find data sequences in the organizations and of the more relevant approach in the Business Process Management??technology.??,After this reflection we can find a more pedagogic definition for Big Data, as a dataset that is too difficult to search, because it is:,Notice that the disjunction ?€?or?€? is used purposely instead of any conjunction that aggregates the three conditions. For each specific case the more suitable algorithms should be chosen, instead of mixing all the concepts. Big Data is not one, but three data issues.,Chapman, N. and Chapman, J. (2000), Digital Multimedia, John Wiley & Sons.,Davenport T.H. (2014), Big Data at Work: Dispelling the Myths, Uncovering the Opportunities, Harvard Business Review Press.
Companies are fighting tooth and nail to stay ahead of the competition. Besides deploying aggressive market campaigns, they are focusing on increasing their dependency on research in order to understand market competition and trends. The changing market dynamics entails closer look at various aspects connected to the data.,The market research aims at giving business organizations information about customers or markets for making informed business decisions. This effort to collect information is done to analyse market size and needs along with the nature of market competition. This can help in making business strategies to get an edge over competitors. Companies either do research themselves or hire the services of agencies that offer professional market research services.,This is where specialised fields such as quantitative analysis and qualitative analysis come handy and gives companies the desired results. Essentially, qualitative data collection and analysis is exploratory research. It is done in order to have a better understanding of opinions and reasons. This gives deep understanding into a problem or aids in developing ideas for potential quantitative research. It also uncovers trends in opinions and thoughts by delving deeper into the problem. On the other hand, Quantitative data analysis generates numerical data which can be changed into statistics of utility. Basically, analysis of quantitative data helps in quantifying attitudes, opinions and behaviours along with other variables for generating results from a big set of population. This uncovers patters through research by using the measurable data.,Hence, while ,??uses unstructured or semi-structured and the sample size of people involved is typically small, methods involved in analysis of quantitative data collection methods are much more structured than Qualitative data collection methods. The qualitative data collection methods include surveys such as paper surveys, telephonic interviews, online polls and systematic observations.,Companies today need a mix of quantitative and qualitative data analysis to give them details of trends, patterns and consumer mindset. Hence, many small research firms have started offering these specialised data analysis to their clients who are willing to spend a fortune in order to launch strategies and create products that give them a foothold in the market which is witnessing more competition with each passing day. Choosing a competitive research firm can be a task and one which understands your requirement by giving you the desired output is not an easy task. Impetus research is one such firm offering both these services. A list of reputed clientele who have taken their services indicates the robust analysis that the company offers.,Qualitative data analysis and collection involves ethnography studies, telephonic interviews, focus group recruitment and discussions amongst others. In addition to it, Impetus research also provides stand-alone services such as transcription, custom reporting in word format as well as development of discussion guides. However, in quantitative research, it undertakes development of questionnaire, data processing amongst other services.,Overall, an informed decision to opt for a reliable firm offering both qualitative and quantitative data analysis can go a long way in helping businesses in achieving their goals.
On the other hand there's the array of semi-structured, unstructured, and inconsistent data types like server log files, sensor data, social-network comments, and other forms of text-centric information. For that world the Hadoop open-source project has emerged as the leading platform for making such information computable. (Hadoop also handles highly structured data, but mostly as a high-capacity, low-cost data store.)


A while back, we found an interesting dataset online. The URL,??,, is fairly self-explanatory. It's a community-sourced list of all "police-involved fatalities", started in May of 2013, but the data itself was a bit jumbled and messy. Race and gender identifiers were in the same column, dates were inconsistent, and though most entries had a news story, we felt there was more information we wanted to know. We put the dataset through our platform to get more information and are presenting it below, as a joint project with??,, who helped us visualize the enriched data.??,(,: you can download the entire dataset??,, or visit our??,Library),It's impossible to know if this database contains each and every incident, but starting May 2013 and through May 2015, 2355 people have been killed by the police in America. That's over 3 people daily across the country.??,In the span of two years, 30 people were killed in both Houston and L.A. 5 cities in Texas are among the top 20. That noted, California has by far the most police-involved fatalities, at at least 398 in the past two years alone. Texas, Florida, and Arizona had over 100 each.??,Police-involved fatalities skew heavily towards men. A staggering 93% of deceased were men (2207 of 2355 total).??,It's important to note that most police-involved fatalities involved armed citizens. It's important to note that from some of the source articles, it was unclear whether the deceased was armed, but in 1485 of the 2355 cases, there was explicit evidence that the deceased was armed with some sort of weapon.??,This was a harder question to answer, since not all news stories contained information about the deceased's priors (or lack thereof). We were able to confirm that 435??,??have a criminal record, which is where that 19% comes from.??,The map below is interactive and contains the number of police-involved fatalities and underlying data. The range goes from a single death (North Dakota) to 398 (California).??,While California had far more police-involved fatalities than any other state, red states like Oklahoma consistently ranked highly based on deaths per capita. The chart below is for 2015 and Silk has broken this data out by year,??,.,We also learned that many counties and localities automatically place police officers involved in a shooting on administrative leave while they investigate the incident in question. Again, though many articles were unclear about whether an officer was placed on leave, over one fourth of involved officers were as confirmed by news stories.??,To enrich this dataset, we first scraped??,, then built a series of jobs. The first was a simple categorization job, where we asked our contributors to clean some of the messier rows, remove bad characters, separate race and gender (which were listed in the same column), etc. After that, we ran a pair of jobs that asked our Level 3 contributors to read source articles and answer questions like "In what city did the incident take place?" and "Was the deceased armed?"??,Our??,??is available for download in our??,??library. It contains all the information above, as well as the source articles for each, names and photos of the deceased, specific dates, and more. If you choose to visualize or analyze the data, we'd love to hear from you.??
This may lead to a practical, feasible solution, , (with thousands of digits), making many security systems vulnerable to new types of attacks: factoring encryption keys, en masse.,Let's start with the pictures. The patterns (as well as how to leverage them) are explained below.,The fact that products of two large primes exhibit such strong patterns, make you think that there must be a simple algorithm to extract the two factors, and thus compromising the encryption keys attached to it. Here we give some hints to find a solution to this (supposedly) very difficult problem. Chances are that such algorithms have been found before and are classified, used by the military exclusively. Here we want to publish solutions for the general public.,We've been interested in such problems for a long time, see for instance ,, or another one on ,??(some exhibit patterns very similar to the two above charts), or ??an article about ,??- digits of SQRT(2)??- which is also a main ingredient in all encryption systems, as well as reverse-engineering / hacking of such systems.,There's a lot of literature on the subject given the fact that it is related to national security, and widely presented as a very hard problem. Here we explore a specific strategy, hoping that it can lead to interesting results, or at least beautiful mathematics.,Let's say that , is a product of two large primes. The purpose is to try to find numbers m, r, s, such that ,, with m very is close to n, and one of the factors r or s (say r), is close to either p or q (say p), where "being close" needs to be defined. Rather than a formal definition, let's provide two examples based on the two above charts:,The general idea is that it is far more easier to find , (or ,) than , (or ,) - because , can easily be partially factored, unlike , - ??and then , can help us find , once we have found ,.,The general algorithm is as follows:,Note that the amount of computation, except maybe for the factors recombination, barely increases as , increases. The numbers , and , are pretty much the same for both??,'s investigated here, and indeed smaller than the value suggested. I suggested , = 1,000 and , = 1,500 (both are a function of ,) to be conservative. If we choose a too small value, we may fail to discover the right m leading to the factorization.,Finally, our tests involve only two numbers ,, but these numbers were picked out randomly: they might be either easier to handle, or to the contrary more challenging, than your average product of two large primes. ??More tests need to be carried out. Note that the largest , =??651,313,413,679 =??501,077 *??1,299,827 that we analyzed, was based on a prime factor ,??=??1,299,827, which is the largest prime number in our list of ,??used to produce this article. In short, 1,299,827 is??the 100,000-th prime number.,The first chart represents Modulo(37,200,034,517 |,45,869-,??on the y-axis. for various values of , between 1 and about 500 on the x-axis, showing that ,=16 provides a strong minimum (zero) ??at 45,869 - 16 = 45,853 which is one of the two primes dividing??37,200,034,517. For the second chart, it's the same data, for the larger , =??651,313,413,679.
The first computer program that I encountered mimicking or emulating human interaction through language was called ",." The version that I knew ran on the ,. It communicated in English. Eliza made comments that made some sense but which indicated lack of understanding of the conversation. If a person mentions "mother," Eliza might pick up on this by asking for some elaboration; but then it would probably say something entirely disassociated from the conversation. One of my earliest projects after I learned how to program in a contemporary computer language was to create a type of Eliza that exhibited some basic structural awareness. I do not mean "self-awareness." I was thinking more on the lines of the ,??that made use of language to carry out worthwhile functions. The conversation doesn't have to be particularly animated - or the personality of the AI all that lively. In my version of Eliza, which I called Rebecca, I devised a method to extract certain basic relationships between elements of data; the computer program was designed provide responses in the context of these relationships. Rebecca made use of a structural object that I called a "pentaneuron" meant to contain a 5-sided relationship. Here is a funny story. One day as I was working on the program, it responded in a manner that I considered reasonable but rather peculiar in syntax - much as young child would try to say something in English. I found the experience spooky because the response, although grammatically deficient, was so human - and the program seemed to be responding directly to what I was saying. I decided to shut the project down. I dismembered the code - enough to prevent me from reviving it. I replaced Rebecca with something less ethically problematic. This blog discusses some of the data structures connected with Rebecca's replacement.,About 15 years ago, I decided to write my own programming language: it made use of "is" and "has" relationships between elements of data. I called it Neura. I wrote it in Java. I still have the program today as a reference study or prototype. I gave Neura some basic programming features such as getting user input, printing output, handling loops and conditionals. On Windows and indeed most operating systems, it is common to hold data in files and folders. Neura did not interpret its resources as files and folders but rather as "is" (using files) and "has" (using folders) relationships. Perhaps the "is-has" and "file-folder" distinction is largely conceptual more than functional. Rebecca by the way also made use of "is" and "has" relationships, but I believe it also used specific variations in order to retain details of actions. A "is-has" action is quite different from containment of data elements: e.g. to have a walk (action) is quite unlike having a wallet (containment). Neura was designed to "understand" only aspects of logical containment. This ability gave the program characteristics similar to any operating system. In response to a question such as "Where is my photo album?" it wouldn't be difficult to make my notepad computer respond, "Your photo album is in the desktop folder." "Where is the demon?" "There is a demon in a jungle in a novel about children on an island." The computer operations are similar.,Depending on one's occupation - or perhaps preoccupation - data might be a straightforward topic or issue. Data might present itself in a highly processed form and rather importantly only as an "is." For example, "X = 2" is an "is" type of assertion. If I were to point out that "cat=Felix" this does not negate the possibility that "Toronto has house" "house has family" "family has cat" and finally that "cat is Felix." Expressed as files and folders, opening the file cat by accessing the path Toronto/house/family/cat might indicate "Felix." It adds to clarity to say Toronto/house/family/pet/cat/name where name is "Felix." In this assertion, a computer could suggest that Felix is something that can occupy a house. Without a more detailed understanding of facts, family/pet could mean that Felix occupies the pet or that the pet ate Felix. Rebecca distinguished between actions and logical containment - but not Neura.,In English, there are many aspects of "is" and "has." Suffice it to say that relational details can be complicated. Certain fields of study such as statistics make great use of data that is disassociated from any structural meaning. This disassociation is not necessarily problematic - e.g. if the relationships don't matter or if there is great preoccupation with uniformity. I use the term "preoccupation" because really in the immensity or enormity of existence where shades of meaning exist in structural details, it takes quite a one-sided single-mindedness to focus on a specific characteristic or property above all else. It's a bit like a person - after being invited to a buffet - piling up on donuts. It seems a little unusual to develop a narrow fetish amid such an abundance of data. The idea that statistics would have a relevant place in relation to big data is rather ironic because one would expect almost the exact opposite.,I am not the first or only person to raise "is" and "has" relationships in structural terms. As I noted earlier, perhaps many or all operating system invoke these structures through the use of files and folders. (I certainly claim no ownership either as a theorist or developer.) Once broken up in relational terms, a computer might be able to deal with a command such as, "Find a cat named Felix having a red hat." The relationships would have to be in place: Toronto/house/cat/name=Felix; cat/hat/colour=red. The computer might not know the meaning of colour or whether the cat that has a hat is the same cat that is associated with the name of Felix. Nonetheless, here we have some reasonably good building blocks to construct something elaborate and sophisticated.,Most computer users should know that folders can contain both files and folders. This means that it is possible to deposit long structural paths into other paths (folders into folders); moreover, different shades of "is" can be added to a folder. For example, cat/name=Felix; cat/weight=17 pounds; cat/temperament=poor; and cat/age=7 years. In English, it is possible to say that a cat "is" 7 years (old). In French, it is more customary to say that a cat "has" 7 years. I therefore make the rather confusing suggestion that an "is" can be a specific type of "has" relationship: Toronto/house/cat/name/value="Felix." Alternatively, rather than Felix, the value could be a symbolic link to another relational path, which is probably closer to the truth for the operating system. Meaning can be added to both "is" and "has" using standard notation: cat/weight.yesterday=17 pounds; cat/weight.desirable=13 pounds; and cat/weight.loss=4 pounds. The exact notation is perhaps mostly a matter of creativity and need.,"Has" asserts a hierarchical distribution of elements. It is possible to have a purely associative distribution or a mix of hierarchy and association: cat/happiness ([food] [mouse] [sofa] [catnip] [milk]) where events of no particular order might be associated with happiness for the cat. It might be difficult to use ordinary files and folders at this point. Perhaps it is necessary to make use of more advanced neural objects. Imagine a dendritic expansion from a folder into something like a reflex hub. Or, the individual associated events might be incorporated into an "is" file along with symbolic links to other structural paths: [food@1257] [mouse@1352] [sofa@7879] [catnip@2111] [milk@1011]. Where should the CPU go or stop going? For the sake of retaining logical cohesion, it is necessary for the processor to recognize resistance if it travels far from the main thread or line. However, for greater depth of meaning, the symbolic links that might be important should be brought closer to the main line.,I find that people routinely talk about extracting data from plain language. While the general idea is sound, it is important to acknowledge the non-quantitative nature of the data that would be extracted. The expedition or journey might not lead to anything statistically meaningful. Interpreting natural language is at least conceptually - if one is focused on meaning rather than word or syntactical events - quite different from any form of quantitative analysis. Why? Well, the meaning of the data in natural language is embedded within the structure of the language and content of conversation - intrinsic to the instrument of conveyance and the items being conveyed. In contrast, the meaning of quantitative data is usually externally defined and contextually constrained. The data in the latter is free of structure because its relevance is actually regulated or imposed. If I study 10 years of sales data, I have myself 10 years of sales data. The data will not reveal itself as anything but sales data. It carries no meaning beyond the parameters set during and perhaps prior to collection. If I examine the relational structures associated with sales, I actually have no idea what to expect. I do not define , what I will extract.,There is a fundamental difference in the predefinition of meaning in quantitative analysis and its undefined conveyance in language. If we ask 100 retail shoppers why they have chosen to purchase a particular DVD title, they might all have different reasons. This is perfectly fine although perhaps difficult to quantify. On a quantitative basis, sales might be the only common thread available for analysis; of course, sales might have nothing to do with "why" people decided to purchase. I have many hundreds of DVDs. I don't recall ever basing my purchase on sales history. "Over 1 million copies were sold last month - I'm buying!" is not something I would ever say. The "is" and "has" details make it possible to gain substantive and materially relevant facts explaining the human reality surrounding the purchase of the DVD title. Quantitative data might just give the sales. The structural relationship between the elements of data represents an entirely different sphere of study. I think that most people would agree that structural data is not something that can be extracted and retained by accident - e.g. using a system intended to deal with quantitative data. The software has to be deliberately designed to articulate the meaning of phenomena within data. For me, one of the major challenges in the use of artificial intelligence in business involves the extraction of relational structures from common language. I consider quantitative methods useful for accounting, logistics, and budgeting - managing resources rather than expanding our understanding of the underlying business.
There are leading and lagging indicators in business. It is important that managers understand the difference between them and ensure they have both types of metrics to get an accurate picture of performance. However, there are also some important issues to watch out for.,All performance measures are, by definition, backward looking. They tell us what has already happened. Current trends are on merging the data and information from backward looking metrics or performance indicators with the forward-looking insights gleaned from predictive analytics.,However, this focus on the future is simply a natural step forward from the work that has been done in understanding the relationships between leading and lagging indicators.,The use of the terms ?€?leading?€? and ?€?lagging?€? indicators has become a standard part of the performance measurement and management approaches in many companies. The term ?€?leading indicator?€? has been used in the field of economics since the early years of the 20th century. In economics a leading indicator is defined as a measurable economic factor that changes before the economy starts to follow a particular pattern or trend.??Leading indicators??are used to predict changes in the economy, but are not always accurate. (we shall come back to this).??,Switching to the world of organizational performance, a leading indicator is one that is focused on the factors that predict likely future results/outcomes, whereas a lagging indicator is focused on the result/outcome itself and whether the expected result was achieved.??,The popularity of management tools like the Balanced Scorecard largely stemmed from the argument that financial performance measures (such as profit, etc) were lagging measures (so provided information on what had already happened) and no longer provided a complete enough picture of performance in what was then (the early 1990s) the beginnings of the knowledge era (they provide an even less complete picture in 2015!).??,What was required for a broader understanding of performance, was the inclusion of leading indicators, such as the new product development pipeline (a leading indicator of future financial results) and other indicators that looked more at the development of new organizational capabilities or employee skills. As simple examples, customer satisfaction might be a leading indicator of customer loyalty and employee satisfaction might be leading indicators of employee retention.,Although of course useful for providing a broad performance overview and for understanding causality between strategic objectives, both lagging and leading indicators do have their shortcomings, which provide watch-outs for executives. Let?€?s begin with lagging indicators.,Because they tell us what has already happened, lagging indicators might provide insights too late for management to do anything about it ?€? irreparable damage might already have been done. For example, simply finding out that most of your key customers have defected to the competition will not likely keep you in business. It?€?s too late! Even if not too late, lagging indicators may not offer useful or insightful information about what is causing poor results and where to focus the improvement interventions required to improve performance. What exactly is causing the downward trend in customer retention and what can we do about it???,Another issue with lagging measures is that there is a tendency (prevalent in the government sector) to focus on outputs (a numbers-based measure to what has happened) rather than outcomes (what did we really want to achieve). As a an illustration from the health service, a lag measure of number of patients treated is often used: this is relatively easy to hit by simply reducing the time spent with patients, which will negatively impact the more important outcome lag measure of correct diagnosis and treatment plan (much harder to measure).??,The value of leading indicators is that they provide data and information on what is likely to result in the future (as captured in lagging measures). This is why we require leading indicators??,However, leading indicators are not without their issues. Note from the earlier definition from economics, ?€?Leading indicators??are used to predict changes in the economy, but are not always accurate.?€? They provide insights into what??,happen and not necessarily what??,happen. For instance, we now know that a high satisfaction with a product might not, in these fast-moving times, always be a good predictor of likely repurchase (many people were very happy with their Nokia mobile phones, but that didn?€?t stop then defecting to Apple when the iPhone was released). Typically, organizations need to understand and analyze the contributions of a number of leading indicators to get a firmer view of likely lag performance.,Moreover, leading indicators are much harder to identify (are often company specific and therefore the data might not be available to built the indicator) than their lag counterparts (which are more often industry wide and therefore easy to build, measure, and indeed benchmark).,This often means companies overvalue the lagging measures and undervalue leading indicators, even if the latter are in reality much more useful. Leaders should beware such performance sapping cul-de-sacs and keep in mind that the purpose of measurement (lag or lead) is to identify and act on improvement opportunities. It is about having a data-driven conversation about performance and improvements, and not a conversation about data!
Big data is growing ?€? in fact, the sector is growing so fast and we are producing data so voraciously, that no one can afford to ignore it as a ?€?fad?€? any more.,And, it?€?s going to affect all companies, large and small, across all segments of the market ?€? from healthcare to public safety, and retail to wholesale.,Big data is changing the world as we know it. If you don?€?t believe me, check out some of these (frankly staggering) statistics:,Studies also show that companies (especially small and medium sized companies) put cost and personnel problems as the top reasons they??,??implemented big data projects yet, but these projects need to become priority fast, or those companies risk being left behind.??,Do you have questions or comments about big data? I?€?d love to hear them in the comments below.,Follow us on Twitter:??,??|??
Originally posted in??,. I've never heard about many of these programs (see top 10 below), and I have questions regarding the methodology used for rankings:,Other organizations such as IEEE rank programs according to ROI, which is in my opinion a better metric, but more difficult to measure. Anyway, here's the list:
Since betting on sporting events has been increasingly evident since the turn of the century and has become a global professional sports sector in itself, it makes sense to explore how it can merge together with FSB. Are they complementary products or is FSB a dangerous substitution product for sports betting?,The answer can be found in the impact that big data will have, in the mid to long term, on these two industries (and more specifically on sports betting). In the short term, the developmentof big data seems to be beneficial for sports betting as clients' demand for more statistics is growing quickly, since they rightfully think that it increases their odds of making money. However, there is no doubt that clients will eventually realize that big data is actually helping bookmakers to optimize the offered odds. Simply put: if big data allows bookmakers to predict sports events' outcomes, clients will start losing interest.,If you think that this scenario would never happen, please have a look at the statistics' competition that took place during the last football (soccer) World Cup in June 2014, between ,. The results were astonishing: Google lost the contest by correctly predicting the outcomes of the last 14 matches out of...15. Meanwhile, both Baidu and Microsoft had a perfect 100% accuracy score. For these big data giants, there is no doubt that the future will be read through statistics.,One can therefore predict that, in the long run, sports betting clients will massively become FS players. To reduce this risk, it seems logical that sports betting operators start offering the FS experience to their clients as soon as possible, to anticipate this potential shift. They need to rapidly have access to this new market since, as always, when a new product arrives, first movers are often the winners.,In countries like USA, Canada, and, to a lesser extent, the UK, FSB has been a cultural activity for decades. ,??and ,??have mastered the industry, allowing them to control the vast majority of the daily fantasy sports action. Converting a free player into a paying one can be done through traditional marketing efforts, such as advertising and affiliation.,In Europe, where the most popular sport is soccer, a different effort has to be made to evangelize clients who are not familiar with the concept of FS. One of the solutions to this is to make strategic partnerships with high profile professional soccer clubs and sports media. Both have large and faithful communities, and they can therefore be the perfect evangelists. One of the latest European fantasy sports firms called ,. is doing exactly that - most recently, they established a partnership with ,. After the launch of the monetized Business to Customer (BtoC) version of their game in August, two months later Oulala will be launching a monetized white-label product for Business to Business (BtoB) clients, specifically aimed at sports betting operators.,?€?At the same time, big data has boosted the growth of FS: a true skill game, entirely based on the soccer players' live statistics. Adopting the peer-to-peer format (clients playing against each other), the operator does not care about the matches' outcomes. Indeed, in FS (and on the contrary to sports betting), clients are therefore competing on a level playing field. The games are powered by highly sophisticated algorithms to assess the crucial aspects of a soccer player's performance that contribute to an overall result. While most fantasy soccer sites limit themselves to 10 to 20 criteria, the guys at Oulala have developed an ,??that includes a total of 70 different criteria dependent on a player's position resulting in a total of 275 ways to gain or lose points. The platform is so fast that for most fans the delay time between action on the pitch being reflected on their smartphone, laptop or tablet is just 20 seconds. The main advantage of following real-time scoring on a second-screen is that it gives a whole new dimension to watching live soccer. Moreover, the live-coaching feature is another exciting aspect of fantasy soccer. It gives clients the possibility to make live substitutions and formation changes to their team, which creates the closest experience to soccer reality in addition to optimising their chances of winning.,Big data is clearly revolutionising the industry and this is just the beginning. While it may seem that European Fantasy Sports hasn't really picked up yet, since Europe is known as a historically late adopter, the future looks promising for soccer lovers who seek richer experiences than what is being offered at the moment by well established betting operators.
:,:,For more resources on Python, R, Julia, or visualization, try the ,.
Usually I tend to criticize this type of articles, but in this case I agree pretty much agree with ,, the author of this article, even though the article is more than 6 months old. Note that BurtchWorks is a recruiting firm that recently posted interesting ,.??,Below is the skills list they recommend:??,??,??,Doing a quick search for ,??will provide tons of additional valuable information.
??,??,??,??,??
With digital at the top of the agenda for every company, both the CMO and CIO need to work together in close collaboration to be successful. It will indeed take equal parts of understanding the market dynamics of digital savvy consumers and practical know-how of the digital backbone to make their digital goals a reality.
"AsterR is a Teradata produced package installed within the R client application.?? This package is distinct from, but complements, the installation of R within Aster.?? Together the AsterR package and the R installation into Aster create a rich environment that provides the R user with the normal look and feel of R while maintaining the power and speed of Aster.?? There is a great deal of new functionality in AsterR that duplicates standard R functions while carrying out the operations and data storage within Aster.?? All the Aster analytic functions may be executed from R using SQL, but many of the functions such as nPath, cFilter, Minhash, and Random Forest have been ""translated"" into a pure R look and feel.?? In addition, AsterR provides pathways called the ""R Runners"" to push R code into Aster for execution.,At its most trivial level, AsterR provides R with database integration.?? Simple DB integration is something that R users are trying to achieve in a variety of ways because it begins to address some of R's most important weaknesses.?? Typically data is passed into and out of R via flat files, ODBC integration is awkward, its file system is not open, and it even struggles with Excel files.?? R also suffers from important limitations in the size of the data that it can manage and simple DB integration provides workarounds.,AsterR establishes a connection to Aster that is based on both ODBC for small data exchanges and mule copy for large data exchanges.?? Using Aster as a simple database provides enterprise quality security for accessing and subsequently landing data and Aster is a component of the larger Teradata Unified Data Architecture so data can easily be sourced from Hadoop, an EDW, or other enterprise systems.,Create Connection to Aster,Watch the Video:,
"
This post was originally published by my partner, Aureus Analytics, a specialist in Insurance Analytics. It is a very interesting illustration of how one would choose a statistical model. I thought you may find it just as interesting and it may help those who are new to analytics.??
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??
Over the past few weeks, we?€?ve had several conversations in our data lab regarding data engineering problems and day to day problems we face with unsupervised data scientists who find it difficult to deploy their code into production.,A lot of the data scientists actually think of themselves as mathematicians, trying to formulate business problems into math/statistics problems and then trying to solve them in the data science projects.,However, the popular misconception arise sometimes out of the big-data hype articles churned out by big data vendors, including some evangelists ?€? who equate data scientists with superpowers across a multitude of disciplines.,The developer?€?s views arise due to their unique perspectives on the complexities of data wrangling and fragmentation around tools, technologies and languages.,The reality, as always, is quite different from the hype. There are actually probably just a handful of the ?€?unicorn?€? data scientists on the planet, who have superpowers in maths/stats,AI/machine learning, a variety of programming languages, an even wider variety of tools and techniques, and of course are great in understanding business problems and articulating complex models and maths in business-speak. For the lesser mortals, and less fortunate businesses, we have to do with multiple individuals to combine these skillsets together into a team or data science squad.,In terms of hiring, building a data science team becomes much easier, once we get around the idea that the ?€?unicorn?€? data scientists are not really available. The recruitment team and hiring manager can then focus on the individual skills that are required on the team and try to hire for profiles with strengths in these skills. Once hired, the manager?€?s role switches to building the team in terms of setting expectations and facilitating collaborative team dynamics to evolve self-governing teams, which can then focus on achieving the objectives in a collaborative manner, instead of having to be superheroes.,So what roles would a data science team have? Depending upon the organizations?€? objectives, the team could either focus on service-oriented consulting gigs, or focus on building reusable assets or data products.,Irrespective of whether the data science teams focus on consulting services in one-off projects or build data products which are reused, in both cases, the team would still require a??,??to build on ?€? in terms of processes or??,, and??,??to perform the actual work. You'll need to ensure??the??,??requirements for such tools and platforms??are in place, so your team is set up for success.,For more insights on data science engineering and analytics, subscribe to??my??,??and follow me on??,.
For those that haven't heard of??,??before, Kaggle is a??,??of people that provide the functionality and support to host Data Mining contests. Here is how it works : Suppose that you are working for a Telco and wish to implement a new Churn prediction model. Rather than running this project in-house, you submit your data to Kaggle. What happens next is that -hopefully- many statisticians globally will each analyze your dataset, produce a model and then submit their prediction model(s) to Kaggle. The best model (and hence its creator) gets the prize which is given by the Telco company. Here is the interview with Kaggle CEO, Anthony GoldBloom :
Of all the ills that impede development around the world, persistent conflict may be the most pernicious and the most widespread. As the World Bank noted in its April 2011 report, insecurity ?€?has become a primary development challenge of our time. One-and-a-half billion people live in areas affected by fragility, conflict, or large-scale, organized criminal violence, and no low-income fragile or conflict-affected country has yet achieved a single United Nations Millennium Development Goal.?€?,Much of this conflict takes the form of ?€?small wars?€? that involve nonstate armed groups such as insurgents, terrorists, pirates, gangs, narcotics and weapons traffickers, and people smugglers. The United States Agency for International Development (,) now spends almost 60 percent of its foreign assistance in 50 countries that are experiencing, recovering from, or attempting to prevent armed conflict.,??Civil war also takes its toll. Some estimates of the social and economic cost of a civil war to a low-income country run as high as $54 billion.,??But the costs aren?€?t just financial. Violence also poses huge challenges for public and private entities that work in these dangerous and unstable environments. And many of the traditional approaches to development simply will not work in conflict-ridden zones. From lack of mobility for aid workers stuck ?€?behind the wire?€? to the corruption unleashed by the cash that flows from large-scale programs to the difficulty of finding local partners able to stay the course amid instability, development projects in war-torn regions present unique challenges.,We believe that work in these difficult regions requires a new approach, which we call Designing for Development. The approach combines several elements. First, to create a deep understanding of the issue to be addressed, it calls for quantitative, remote observation and analysis, using new tools, such as big data, crowd-sourced reporting, and interactive visualization. To build a deep contextual understanding, it also requires on-the-ground observation and research, preferably carried out and directed by well-trained members of the local community. Finally, the big data and local insight must be integrated and used to shape a solution with the help of design thinking.,Let?€?s look at the data component first. New analytical methods, powered by advances in computing power and software, enable the use of data in important new ways. The ability to manipulate big data, visualize dynamics, and recognize patterns and signatures for conflict creates new opportunities for humanitarian and development assistance in the most complex and dangerous environments. New tools also enable remote assessment in places that are simply too risky for traditional on-the-spot evaluation. Analysts can use signatures?€?patterns of population movement, price fluctuations, market activity, or Internet usage, for example?€?to make informed judgments on the stability of a community over time.,Acquiring the data to be analyzed has also become more feasible. The World Bank,,, and the United Nations, for example, have all taken steps to improve transparency and accountability through data sharing.??,??offers the Foreign Assistance Dashboard, a data-driven interface that allows users to examine, research, and track aid investments. Interested users can trace??,??investments in programs to support peace and security programming worldwide. At the World Bank, users can call up the Data Visualizer, which tracks civil war, homicides, terrorism, and trafficking in countries around the world, as well as socioeconomic, demographic, and political data to put it into context.,Organizations also have more options for generating data themselves.??,, an open-source tool developed by a Caerus associate, helps civil society and local governments collaborate, while??,, from Ushahidi, allows users to crowdsource information and display it on a map and timeline.??,, created by Stamen Design, is a tool for citizen-generated mapping that enables real-time exchange of data. Caerus is now helping to adapt it for use in conflict zones.,But remote observation and computation, alone, do a poor job of developing usable insight. Data can be corrupt or patchy, analyses open to multiple interpretations, and causation difficult to identify. Hence, the best new approaches combine quantitative remote observation with on-the-spot qualitative field research by teams of local, primarily indigenous analysts and designers.,Obviously, fieldwork in conflict environments requires close attention to risk management and security, as well as a tight partnership with local communities. And this underlines a lesson that development professionals have learned and relearned in conflicts over decades: the local population truly is the principal actor. Far from being a passive recipient or ?€?beneficiary?€? of international or governmental expertise and largesse, in real-world environments the locals rapidly end up calling the shots. Acting on this understanding?€?genuinely putting local people, their perceptions and ideas, their priorities and knowledge at the heart of a design-driven development approach?€?is hard but important.,The two perspectives?€?remote observation and deep-dive fieldwork?€?must next be fused into an integrated picture of conditions on the ground. We can then begin to design for development in a more conscious and deliberate way, even under conflict conditions. The disciplines that make up the field of design?€?graphic, product, structural, process, and business model design?€?share a focus on creative problem solving. In recent years, evidence-based disciplines such as management consulting have taken notice of design. Roger Martin, dean of the Rotman School of Management, in Toronto, describes design as the ability to create??,??things as opposed to traditional analytic thinking, which he says is about??,??things that already exist. Like design, development seeks to create new experiences for communities: justice, equitable access to resources, political legitimacy, economic stability, and peace. Thus, design and development share a commitment to transforming the status quo, in creative ways that may not be readily apparent at the outset.,Human communities are the engines of social transition, and thus meaningful development programs must put communities at the center of the design process, but doing so in a high-threat environment presents extra challenges. Designers must be present in the community long enough to build credibility and trust, and to help align interests within and across communities. But they must also be acutely aware of the risk their presence may create for themselves and the communities, and must actively work to mitigate that risk.,Designing for development can help manage many of these challenges. Following a process that combines remote observation with deep field research can facilitate deep understanding of both the community and its issues. A willingness to experiment, synthesize feedback, and engage in real-time learning and rapid prototyping can help to understand and avoid unintended consequences before scaling up. Where poor freedom of movement threatens accountability, crowd-sourced reporting technologies can augment community-led monitoring and evaluation.,Our experience suggests that restrained, nonintrusive, minimally disruptive interventions, guided by local knowledge, are the key. Practitioners must be trained, equipped, and prepared to accept a level of physical risk that allows them to engage communities directly, without the massive security presence or logistical footprint that typifies traditional approaches. Most important, they must be willing to support the community?€?s emerging ideas and resist the temptation to impose their own goals, timelines, and objectives.,Problem solvers working amid the overlapping challenges posed by conflict?€?resource inequity, poverty, fragile governance, environmental damage, and disease?€?need imagination and integrative thinking skills. Development professionals, designers, and communities need to be the joint architects of social, economic, and political change. A design approach to development emphasizes communities?€? experience and unique perspectives, informed by rigorous analysis.,Designing for development in this way, truly putting local people at the center of a design process that seeks to optimize their experience, we believe, is the best path to overcoming the threat that conflict poses to social, political, and economic development.
 , , , , ,??cloud storage, , , 
When I returned to university to do a graduate degree, I was interested to discover how certain terms are subject to "intellectual interpretation." A word that I was asked to explain during one of my earliest classes was "ontology." Since this term was absent from my dictionary, I originally confused it with "oncology." I faintly recall that oncology involves the study of tumors. After consulting a few sources, I said that ontology is the study of how things come to exist or into being. I came across another perspective although I don't recall the source: ontology is the study of how things gain relevance or become recognized, indicating that existence can be regarded as a matter of recognition. Perhaps there is no monopoly on the exact meaning. However, I would say of ontology in relation to data science, it explains how meaning is attached to data and therefore how that data gains and retains meaning. For example, if I were asked to count the number of trucks in a parking lot, it isn't obvious what should be included: small pick-ups, tow-trucks, commercial hauling vehicles, dump trucks, and maybe heavy construction trucks. Consequently, if I have databases containing running counts of trucks found in various parking lots over a period of time, the comparability of the data is worth questioning. It might be necessary to ask some fairly philosophical questions: under what circumstances, how, and why is something a truck? We find ourselves with a fairly esoteric topic colliding with the concrete demands of a data-oriented world.,In feminist literature, which I rather enjoy reading when I have the opportunity, one might encounter the question of what makes a person a man or woman? I know that at first glance and without deliberation, it might be tempting to pose genitalia as the distinguishing feature. We rarely go about our affairs in the world with our genitalia exposed. Some women are able to pass as men. Some men might be mistaken to be women. Certain men will be regarded as more masculine than others. I remember a comedy where one of the main characters said he had been raped in jail by a man. He was asked if the rapist was a black man - the explanation being that a scrawny white man probably seemed fairly feminine compared to a hulking black man. No. He was then asked if the rapist was Hispanic. No. The questioner went down a list of different types of men. In the end, the victim admitted that he had been raped by a Filipino. Perhaps Filipino men don't seem like imposing male specimens. I was born in the Philippines. I found the film rather funny. Well, my point here is that masculinity and femininity are more complicated than genitalia. On matters of ontology, I would say that a great many things are more complicated than they seem on the surface. When can a sapling be regarded as a tree? What makes a person Italian rather than French? What does it mean to be a Canadian citizen? In our time, we find ourselves trying to determine what makes a person a terrorist - the behaviours that seem suspicious. For some, ethnicity and religious inclinations are reasonable determinants. Then we try to give estimates and projections as if we understand everything.,I personally cannot help but laugh - albeit with the greatest level of respect - at the idea of using a computer to interpret language in order to determine meaning. I laugh because I have difficulty myself trying to ascertain meaning - or, more specifically, confining meaning. Being in a room full of people speaking the same language doesn't mean that communication will go smoothly. Meaning and shared understanding might be dependent on ontology. I will give an example. I was studying customer service issues relating to people with disabilities brought forward to various human rights tribunals. I developed a model called ACTOR that I found aligned with tribunal decisions. ACTOR is an acronym for Attraction, Conduct, Tenacity, Organization, and Recognition. I noticed that when a tribunal adjudicator made ACTOR-related comments - either on the specific case or the character of a service provider - I could almost guess which side had won. I also found a connection between ACTOR comments and the damages awarded. None of this research has been published by the way - quashed prematurely by my departure from academic life.,In order to carry out my analysis, I went through the case documentation to locate ACTOR comments. Then I had to ask myself questions like the following: is that point about Conduct or Tenacity? What is Conduct and Tenancy, really? What makes Tenacity different from Organization? In the context of a case, it is difficult to make clear delineations bringing about the existence of ACTOR elements. Below is an image of a completed ACTOR survey sheet - just one of several dozen that I used to do my research paper. It is a joy surveying case documents rather than people. There is no need to submit the questionnaire for approval. Moreover, it isn't necessary to compensate participants. On the downside, it was necessary for me to assume the persona of the adjudicator - noted on the document as the "subject." I had to constantly ask myself, "What did I (the adjudicator) mean when I made that comment?" Since I was writing an academic paper, I couldn't just go by my opinion. I wrote down page numbers and specifics for each ACTOR-oriented comment that I encountered. Upon close examination of the sheet, it should be apparent that ACTOR is focused on the adjudicator's perspective of the service provider's mental processes as supported by the case documentation. It was a considerable challenge associating the "evidence" (the comments) with the appropriate ACTOR definitions. I was responsible for determining when the sentences created or became evidence in the context of ACTOR.,When dealing with a database, routinely the ontological basis of the data is external. By this I mean that the designer of the database has set aside specific columns for data. Meaning is not intrinsic to the data but defined by the structure of the database. For example, one column might contain "quantity sold" and another "price per unit." One would not expect to find temperature under quantity sold. I know some might consider my point ludicrous and extreme. Well, weight and volume and indications of quantity. It might be possible to sell something based on BTUs. My point more generally is that the database defines the meaning of the data placed within it. If there are several databases, due to the externally defined nature of the data, one cannot immediately assume comparability. Somebody with an extremely good understanding of the data might apply adjustments to allow for greater levels of comparison. This means that the data would change - further emphasizing the fact that the ontological control is external.,If we refer back to my ACTOR example, it should be apparent that subjectivity is an important consideration. The assertions are qualitative although I have assigned quantitative characteristics to the data. The meaning of the data doesn't have to abide by my assessment. The meaning exists irrespective of me or any database that I create. The ontological basis is intrinsic. I can create a database to try to fit the data, but this does not mean that the database defines the data; it merely provides structure for my particular application. For example, if there is a number under "quantity sold," this is the literal meaning. But if there is a number under "Tenacity," that number doesn't mean tenacity. In fact, it is quite difficult to provide a clear definition for tenacity. I attach meaning to something not entirely understood. The meaning exists beyond my contextual application. Suffice it to say that my ability to attach meaning through measurement does not mean that I understand everything about the object being measured - just the aspect of it that conforms to my contextual requirements.,Similar to ontology, I have encountered various interpretations for the fallacy of nominalism. I don't recall the exact reasons leading up to the conversation, but an undergraduate professor explained to me, "This is the idea that a person can, just by having a name for something, somehow have a total understanding." I have read other perspectives on the term. From an ontological standpoint, this might mean that things could gain relevance merely through identification and labeling; moreover, labeling or naming forms the totality of that relevance. These days in relation to data science, I believe that there is a similar line of reasoning. Rather than naming, we sometimes imply the totality of things through authoritative quantitative and statistical representation. Even if an object exists in a multiplicity of states and contexts, epistemological authority might be commandeered through the assertion of one context in particular. In this way, the data extracted from phenomena might become disassociated from reality, reflecting more its contextual application than the underlying truths of its existence. I have referred to this contextual domination as "instrumentalism" (although this term might already exist in other applications).,At one time, the separation between humans and machines was clear. But as factory environments emerged, people started to wear out and be replaced like machines. They started moving repetitively and incessantly like machines. The lines between the factory and a person became blurred. Moreover, the reach of the work environment has started to extend well beyond the workplace - influencing the way people structure their lives and their interactions with society. We might say, therefore, that the contextual dominance of the workplace has over a period of time dissolved the multiplicities once more accessible to people. The metrics of the workplace have served to define the value of people and their placement in society. I therefore find it difficult to separate ontology - these days involving the recognition of phenomena in data - from the power dynamics that have consumed society for many centuries. While we might not be dealing with names and labels per se, in many respects we continue the drive towards greater levels of nominalism through our quantitative expressions and assertions of phenomena.,I have so far identified three ways in which the relevance or existence of things can be externally defined and conveyed through data: 1) through labeling and naming; 2) through authoritative quantitative expression; and 3) by design using the structures of a database. I will now discuss definition in the conceptualization of process, which I admit is rather distant from generally recognized ideas relating to ontology. In our world today, technology and processes making use of it have become extremely complex. It seems that we tend to address this complexity by "black-boxing" processes. The expression "out of sight, out of mind" comes to mind. Once inside that black box, significant effort and highly specialized skills might be needed to rectify even minor problems. While the ontology of the past could sometimes be attributed on idealism, in the future I suggest that it will more frequently take shape as a consequence of technological developments. Many technologies are meant to persist over a period of time. Although the world is evolving and changing, the tools that we have to enable human existence within it are prescribed and predefined at design time. While the obsolescence of something tangible is apparent by its physical presence - e.g. a bag of horseshoes in a world of automobiles - the emerging constructs of technology are far less so.,Consider what it is like to require a wheelchair for mobility in a world that by design is fairly inaccessible. The roads and buildings are designed to last for decades. Architects, urban planners, and perhaps many employers might fail to take into account those with particular needs: elderly people, pregnant women, those with injuries, paramedics, firefighters, and as I mentioned earlier individuals making use of mobility assistance. The same way that the material world can be made inhospitable through the actions and decisions of designers, the same is true in terms of the disabling nature of technology and processes. There could be a bus stop at a corner . . . although buses no longer stop there . . . due to decisions to cut back on service. But not everyone has a car, the option to car-pool, or alternate bus routes. So if the transit service fails to take my needs into account - fails to consider my existence or the value of my existence in the scheme of things - my involvement in society might greatly diminish. Perhaps I am interested in settling down and building a home near that bus stop. Maybe if the bus actually stopped there, some people would get off to eat at a local restaurant. I don't pose myself as a public transit activist (although I believe in promotion accessibility and mobility). I'm just saying that sometimes, the attachment of value and relevance occurs not near or by those affected but perhaps extremely far away by individuals insulated from the consequences.,I will now describe a rather deep shift in the manifestation of ontology in society that I call centrality. In a modern production facility, automation is everywhere. Robots and machines operate within their design parameters. They sometimes provide signals: for instance, when a printer is out of paper, it might beep or blink to alert the user. Despite the possible use of sensors and data-loggers, machines often provide little or no feedback. A company can successfully produce products that are defective and that nobody wants to buy since the machines are not going to care or complain. In fact, even if the factory is flooded or on fire, its machines that are still able to operate would probably continue to do so regardless. Machines are extensions of designers - the individuals exercising power to set priorities and define what counts. In a highly automated production setting, the recognition of reality is centralized. Recognition and the attachment of relevance occur at design - just like in my bus stop example. In comparison, in a factory that contains a lot of people, these individuals exercise some autonomy and personal discretion not just in their behaviours but also in terms of their recognition of relevance and in their decision-making.,Many years ago, I instructed my bank to take monthly amounts from my bank account to purchase mutual funds. I enjoyed seeing my investments grow as a result of my purchases. After a number of months, I noticed that my bank account balance was not declining although my investment account showed regular investments. Since I was dealing with a major bank, I considered it unlikely that the discrepancy would last for long. As the months continued and I became richer perhaps due to glitch in the processing environment, I decided to contact the bank to set things straight. It seems that the discrepancy was "invisible" - that is to say, the internal systems of the bank lacked the ability to detect the discrepancy. I thought to myself, those responsible for setting up the system perhaps failed to take certain types of problems into account. Of course, these days, it is often difficult to reach humans to complain about service. Companies might be losing money every day without even being aware of the loss. This desensitization to reality is connected to the centrality of ontology. If humans were more - prevalent, abundant, present, I'm unsure which term to use - let's just stick with more humans; if there were more humans - allowed to exhibit natural human tendencies - at least there might be some chance of problem identification outside the design parameters. People can take into account things the designer did not.,I recently tried to sign up for a first-aid course. I went through all of the registration pages online; at the end of the process, I was advised that there was an error processing my request. I was directed to phone a particular number. I phoned that number. I listened to quite a lot of options. Finally, I was told to leave a "general message" in the organization's inbox. To me, this seemed to imply that "specific messages" were not for the inbox; thus, I actually had no inbox to leave my specific type of message. I was left wondering what the IVR system designer meant by "general message." I concluded that my message isn't of a general nature. However, the system designer only anticipated or built the system to deal with general messages. Consequently, directing me to call a phone number without a human arbiter was entirely illogical. Taking into account the possibility that there might not actually be a process to deal with registration errors, I decided to send an email. I noticed that a competing organization offering first-aid courses offers "live chat" to provide assistance - for those that require more than what the system is designed to give. So the latter company might be losing all sorts of money without having the foggiest idea. Their "institutional response" is premised on the effectiveness of ontological centrality. The former company assumes that the process might fail; and humans have been assigned to compensate for any technological-procedural deficiencies.,My pick-up truck is part of that huge Takata airbag recall announced recently. I'm unsure about the exact nature of recall, but I remember the world "shrapnel" mentioned on a few occasions. I made the observation in an earlier blog that quality control is not necessarily about controlling quality but ensuring conformance to design specifications. As such, it is possible for a company to produce and install defective, poorly constructed, or shoddy devices. Indeed, some devices can be tested and approved at laboratories by people likewise ensuring conformance to prescribed testing parameters. The detection of something "bad" (recognition of something bad) might occur at a much earlier point in production than the checker. For the checker does not define what is good or bad; he or she merely ascertains conformance or lack of it. If the need to save money is quite strong, there might not even be a human checker. After all, the work would likely be mundane. There might simply be machines ensuring the proper installation of specified components. Not only can problems be rendered "invisible" to the organization, but opportunities for improvement might likewise be centralized - i.e. limited to the designers. Matters of ontology are really relevant in business. It wouldn't be enough to say that the data seems to indicate that the parts conform to quality standards; or that the process conforms to design specifications. To be sensitive to reality post-design, people should be involved (as opposed to merely present) in operations.,In the digital age, the difference between centralization and decentralization is actually illogical; distance is irrelevant using computers. People in Cuba and Guam can communicate and work with people in Greenland likely over the internet. Decentralization is relevant to the extent that the operational conditions can be "recognized" by those presumably closer to the activity. Decentralization for me is actually a call for more human involvement. People can offer the "lived experiences" and reflections that would likely evade computers. Humans are not just thinkers but feelers endowed with all sorts of motivation to change the world. As part of that involvement, there has to be reduced centrality in terms of ontology. Decentralization probably wouldn't work well or make much sense without reduced centrality. Yet the direction of technology development, I suggest as indicated by the increasing absence of humans in production, is towards greater centrality. The commandeering of ontology will be reflected in the data that is collected - its narrow contextual focus - its non-adaptability to changing needs - its market alienation - giving rise to problems that seem almost negligent and most certainly bad for business. Data scientists should ensure that phenomena can be portrayed in data beyond their immediate analytical requirements. It might be necessary to extract or mine for data of apparent value. However, it is also important to respect the complex multiplicities inherent in phenomena. There should be more willingness to consider the internal ontology of data.
We had the chance to use the NFL play by play dataset all the way from 2002 through 2013 and the best part is the analysis was carried within Hadoop using Cloudera Impala.,For the analysis we wanted to be at the individual game level but the data contained mixed grain including the play by play data. So what we ended up doing was apply some SQL filters to restrict it to the first row of each play by play dataset.,Here are some interesting insights,This was basically grouping the winners, summarizing the count of wins and sorting it in descending order.,Click to see larger image,You can see the winning team going down on the y axis and the losing team horizontal on the x axis. Each intersection represents the number of times the winning team has won against that particular losing team from 2002 through 2013,Check out the detailed??
"Your website?€?s search capabilities may be a potential customer?€?s first (or only) interaction with your website. ??Customers who can?€?t find relevant products based how they search are likely to?? abandon and go to competitor websites. ??For many retailers, 30% - 40% of search queries are under-performing. ??Underperforming search queries are costing you sales and customers.,Please view the following Video for more Details:,
"

In this post, I will cover in-depth a Big Data use case: monitoring and forecasting air pollution.,A typical Big Data use case in the modern Enterprise includes the collection and storage of sensor data, executing data analytics at scale, generating forecasts, creating visualization portals, and automatically raising alerts in the case of abnormal deviations or threshold breaches.,This article will focus on an implemented use case: monitoring and analyzing air quality sensor data using ,??and R Language.,Hourly readings of several key air quality metrics are being generated by over 2,000 monitoring sensor stations located in over 300 cities across the United States, the historical and streaming data is retrieved and stored in ATSD.,The data is provided by??,, which is a??,??program that protects public health by providing forecast and real-time air quality information.,The two main collected metrics are??,??and??,.,PM2.5 is particles less than 2.5 micrometers in diameter, often called ?€?fine?€? particles. These particles are so small they can be detected only with an electron microscope. Sources of fine particles include all types of combustion, including motor vehicles, power plants, residential wood burning, forest fires, agricultural burning, and industrial processes.,o3 (Ozone) occurs naturally in the Earth?€?s upper atmosphere, 6 to 30 miles above the Earth?€?s surface where it forms a protective layer that shields us from the sun?€?s harmful ultraviolet rays. Man-made chemicals are known to destroy this beneficial ozone.,Other collected metrics are: pm10 (particulate matter up to 10 micrometers in size), co (Carbon Monoxide), no2 (nitrogen dioxide) and so2 (sulfur dioxide).,A total of 5 years of historical data has been collected, stored, analyzed and accurately forecast. In order for the forecasts to have maximum accuracy, account for trends and for seasonal cycles, at least 3 to 5 years of detailed historical data is recommended.,An issue with the accuracy of the data was immediately determined. The data was becoming available with a fluctuating time delay of 1 to 3 hours. An analysis was conducted by collecting all values for each metric and entity, resulting in several data points being recorded for the same metric, entity and time. This led us to believe that there was both a time delay and stabilization period. Below are the results:,Once available, the data then took another 3 to 12 hours to stabilize, meaning that the values were fluctuating during that time frame for most data points.,As a result of this analysis, it was decided, that all data will be collected with a 12 hour delay in order to increase the accuracy of the data and forecasts.,??was used to collect the data from monitoring sensor stations and stream into ,.,In Axibase Collector a job was setup to collect data from the air monitoring sensor stations in Fresno, California. For this particular example, Fresno was selected because it is considered one of the most polluted cities in the United States, with??,.,The File Job sets up a cron task that runs at a specified interval to collect the data and batch upload it into ATSD.,The File Forwarding Configuration is a parser configuration for data incoming from an external source. The path to the external data source is specified, a default entity is assigned to the Fresno monitoring sensor station, start time and end time determine the time frame for retrieving new data (,??is used).,Once these two configurations are saved, the collector starts streaming fresh data into ATSD.,The whole data-set currently has over 87,000,000 records for each metric, all stored in ATSD.,The next step was to analyze the data and generate accurate forecasts. Built-in Holt-Winters and Arima algorithms were used in ATSD and custom R language data forecasting algorithms were used for comparison.,To analyze the data in R, the??,??was used to retrieve the data and then save the custom forecasts back into ATSD.,Forecasts were built for all metrics for the period?? of May, 11 until June, 1.,The steps taken to forecast the pm2.5 metric will be highlighted.,The Rssa package was used to generate the forecast. This package implements Singular Spectrum Analysis (SSA) method.,Recommendations from the following sources were used to choose parameters for SSA forecasting:,pm2.5 series was retrieved from ATSD using the query() function. 72 days of data were loaded., SSA decomposition was built with a window of 24 days and 100 eigen triples:,dec <- ssa(values, L = 24 * 24, neig = 100), eigen values, eigen vectors, pairs of sequential eigen vectors and w-correlation matrix of the decomposition were graphed:, plot(dec, type = "values"),plot(dec, type = "vectors", idx = 1:20),plot(dec,type = "paired", idx = 1:20),plot(wcor(dec), idx = 1:100),A group of eigen triples was then selected to use when forecasting. The plots suggest several options.,Three different options were tested: 1, 1:23, and 1:35, because groups 1, 2:23 and 24:35 are separated from other eigen vectors, as judged from the w-correlation matrix.,The rforecast() function was used to build the forecast:,rforecast(x = dec, groups = 1:35, len = 21 * 24, base = "original"), Tests were run with vforecast(), and bforecast() using different parameters, but rforecast() was determined to be the best option in this case.,Graph of the original series and three resulting forecasts:,Forecast with eigen triples 1:35 was selected as the most accurate and saved into ATSD.,The next step was to create a competing forecast in ATSD using the built-in forecasting features. Majority of the settings were left in automatic mode, so the system itself determines the best parameters (based on the historical data) when generating the forecast.,To visualize the data and forecasts, a portal was created using the??,.,Thresholds have been set for each metric, in order to alert the user when either the forecast or actual data are reaching unhealthy levels of air pollution.,When comparing the R forecasts and ATSD forecasts to the actual data, the ATSD forecasts turned out to be significantly more accurate in most cases, learning and recognizing the patterns and trends with more certainty. Until this point in time, as the actual data is coming in, it is following the ATSD forecast very closely, any deviations are minimal and fall within the confidence interval.,It is clear that the built-in forecasting of ATSD often produces more accurate results than even one of the most advanced R language forecasting algorithms that was used as part of this use case. It is absolutely possible to rely on ATSD to forecast air pollution for few days/weeks into the future.,You can keep track of how these forecasts perform in comparison to the actual data in??,.,A smart alert notification was setup in the??,??to notify the user by email if the pollution levels breach the set threshold or deviate from the ATSD forecast.,Analytical rules set in Rule Engine for pm2.5 metric ?€? alerts will be raised if the streaming data satisfies one of the rules:,At this point the use case is fully implemented and will function autonomously; ATSD automatically streams the sensor data, generates a new forecast every 24 hours for 3 weeks into the future and raises alerts if the pollution levels rise above the threshold or if a negative trend is discovered.,The results of this use case are??,, for whom it is important to have an??,??that they may face during their visits or for expats moving to work in a new city or country.??,??that long-term exposure to high levels of pm2.5 can lead to serious health issues.,This research and environmental forecasting is especially valuable in regions like??,, where air pollution is seriously affecting the local population and visitors. In cities like??,,??,??and??,,??pm2.5 levels are constantly fluctuating from unhealthy to critical levels and yet accurate forecasting is limited. Pm2.5 forecasting is critical for travelers and tourists who need to plan their trips during periods of lower pollution levels due to potential health risks associated with exposure to this sort of pollution.,Government agencies can also take advantage of pollution monitoring to plan and issue early warnings to travelers and locals, so that precautions can be taken to prevent exposure to unhealthy levels of pm2.5 pollution. Detecting a trend and raising an alert prior to pm2.5 levels breaching the unhealthy threshold is critical for public safety and health. Having good air quality data and performing data analytics can allow people to adapt and make informed decisions.,Big Data Analytics is an empowerment tool that can put valuable information in the hands of corporations, governments and individuals, and that knowledge can help motivate or give people tools to stimulate change. Air pollution is currently affecting the lives of over a billion people across the globe and with current trends the situation will only get worse. Often the exact source of the air pollution, how it?€?s interacting in the air and how it?€?s dispersing cannot be determined, the lack of such information makes it a difficult problem to tackle. With advances in modern technologies and new Big Data solutions, it is becoming possible to combine sensor data with meteorological satellite data to perform extensive data analytics and forecasting. Through Big Data analytics it will be possible to pinpoint the pollution source and dispersion trends days in advanced.,I sincerely believe that Big Data has a large role to play in tackling air pollution and that in the coming years advanced data analytics will be a key tool influencing government decisions and regulation change.,You can learn more about Big Data analytics, forecasting and visualization at ,.
1. Identify your target audience.??,2. Customize the data visualization.??,3. Give the data visualization a clear label or title.??,4. Link the data visualization to your strategy.??,5. Choose your graphics wisely.??,6. Use headings to make the important points stand out.??,7. Add a short narrative where appropriate.????,. For other articles on visualization science and art, as well as best practices,??,.,Follow us on Twitter:??,??|??
 through an academic-industry collaborative partnership with SAS, has developed a long term research initiative connected with analytics and management innovation.,Last month I posted a blog pointing to the findings from our latest data and analytics global executive study and report, "The Talent Dividend," highlighting the role of analytics talent in creating competitive advantage at data-oriented companies.??,Following publication we conducted a free, live webinar to share our findings, featuring as speakers the authors of the report: ??Sam Ransbotham??(associate professor in information systems at Boston College and guest editor at??,David Kiron??(executive editor at??,),and??Pamela Kirk Prentice??(chief research officer at SAS Institute Inc.).,Intended to provide deeper insights into the report's findings, the discussion revolved around the following topics:,If you missed the webinar live, the recorded version is??,. ??,Our premise is as follows:,?? ?? ???€?It?€?s not just high-tech. It?€?s going to be every kind of business.?€? ??-- Hal Varian, chief economist, Google,??,?? ?? ???€?The shift from looking backwards versus prescriptive and predictive analytics is also a shift in people's ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? thinking.?€? ??-- Mathew Chacko, director of enterprise architecture, The Coca-Cola Company,?? ?? ???€?A new space and a new way of looking at things ?€? it?€?s drawing people to come together. The silos of ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? the past aren?€?t holding us up.?€? ??-- Vince Campisi, CIO, GE Software,If this is a topic that interests you, I invite you to watch all or part of the webinar, download the handouts and if so inclined, let us know if you have any additional insights to share. ??Enjoy!
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The ,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??
On the face of it, Analytics and Law are manifestly divergent fields of practice. One need only consider the nature of Algorithms that require numerical attributes for their calculations and the textual rigidity of substantive law to realize this. The very first obstacle one will encounter in applying Analytics to Law is the absence of calculable numerical variables in raw legal data. No judicial precedent, statute or common law principle has ever been reduced to a mathematically sound numerical expression; raw legal data is simply not Analytics-receptive., There are however some methods of mining raw legal data, like powerful Text Analytics that make it possible to build reasonably accurate classification, sentiment analysis and many other models. There are also methods like Discretization (e.g. Nominal to Numerical) in Neural Networks for example that try and facilitate this kind of machine learning. There are a few more techniques that are available but in my humble opinion not worth mentioning, precisely because the results that they yield in their application to raw legal data are catastrophic.,Law is incredibly nuanced and has with it intricacies peculiar to it alone, you need to be able to factor in those intricacies as mathematically adept numericals to assist accurate machine learning. To simply apply text analytics alone or simplistic variations of machine learning is overly facile.,We have seen the use of purely aesthetic numerals as legal data sets for Algorithmic processes. By aesthetic I mean a completely superficial value, say 78, to denote something like contractual breach in a predictive model. The results were a statistical calamity to say the least; it had solved the numerical anomaly at a surface level, but not at a completely authentic one. With this sort of weighting, something like contractual breach is simply expressed as ?€?x?€?, however ?€?x?€? has to have a legitimate value in that it has to be calculable mathematically and you have to be able to solve for it. Therefore other variables have to be factored into a calculation that ultimately has breach or ?€?x?€? as a result. This sort of superficial weighting does not do that.This kind of representative weighting has an adverse effect on the statistical integrity of the algorithm and\or machine learner. Legal data architecture necessitates a collaborative approach between this sort of representative weighting and actual math-based weighting.,Before even purporting to mine legal data competently the numerical anomaly has to be reconciled, legal analytics is simply inconceivable without this reconciliation. This is extremely hard, which is probably why weighting systems in legal technology are non-existent. We spent a year developing a math-based metrics system for law, the system facilitates the conversion of raw legal data (for example a section in an act) into numerical values for the purposes of algorithm-based machine learning. Amongst other things a system such as this one required calculable and inter-dependent variables, stratified weighting schemes, proportionality and most importantly numerical values that are mathematically apportioned according to their peculiar legal implications. What has resulted is a metric system that can numerically quantify legal permutations not only at a purely epidermal level, but at a systemic one as well. For the first time a common law principle like the duty to disclose material facts in insurance contracts, can be mathematically quantified to a value of say ?€?3.22?€?for the purposes of an algorithmic process. Once this has been done, the values can be used for data pre-processing, architecture and analysis.,At first we intended to use the metrics system for internal purposes only, however we have since resolved to provide these metric conversion systems to other Legal Technology firms and Analytics firms in general. We feel that the data science field needs alternative means of analyzing raw legal data in the most statistically robust manner. Without a capable metrics system in legal analytics it is impossible to unearth very subtle insights, any legal data mining without a system such as this one is unfortunately very finite. Decision science and diagnostics, advanced predictive modeling, algorithmic trial simulations, pattern recognition and automated policy enforcement are some of the advanced areas of our practice that simply would not be possible without a math-based metric system for legal data. Many Data science ventures into law fail before they even begin because there does not exist a numeric conversion system for their legal data. For lawyers, Data Science is a numbers game they almost lost, fortunately though they are now beginning to win it.
While everyone talk about unusual extreme weather events (heat waves, cold spells, floods, droughts), very few, including scientists, have been able to make sound predictions for extreme events, be it weather or stock market extreme behavior, or any bubble. Here you will learn how to produce simple model-free confidence intervals for extreme events in Excel, how to generate (correlated) simulated stock market data and (uncorrelated) natural data such as air pollution index, understand why extreme events are predictable in one case but not in the other one, predict them, and learn about how to simulate ad-hoc data. Extreme events are a big issue for insurance companies and anti-terrorist agencies: predicting expected loss is critical to set up premiums correctly.,We propose a solution to predict extreme events. It is based on simulations, and very different from ,??(a statistical technique), which so far has proved useless. This article was first proposed as a ,, but we believe it is important enough to make it a featured article and tutorial of its own.,Here, ,??(click on the link to download), we perform simulations to predict extreme values, with model-free confidence intervals, using ,??that does not involve any statistical knowledge or statistical models. We generated two types of data:,In case #1, predicting extreme values is impossible (there is lack of convergence), while in case #2, it is easy, as illustrated in the charts below. The simulations in case #2 try to reproduce a phenomenon where each new iteration can generate a value larger than all previously observed values. The record computed over the past k observations ,. For each of the two examples, we produced 10 simulations (,) each with 10,240 data points, then computed the confidence interval for the highest value R(k) observed over the last k iterations (for various values of k), based on these 10 time series.,To generate the stock market data, we used basic random walks, see Excel spreadsheet. It produces values that are NOT (statistically speaking) independent. For the air pollution index A(k) at time k, we first generate a random deviate U on [0, 1], then set A(k) to A(k) = - ln(U). The A(k) values are independent. The function used here, in this case -ln(U), must be chosen to provide ,??with actual, observed data. ??,Even though our simulations clearly show that stock market highs are unpredictable, it is still possible ,??to make money in the stock market (insider trading is your best bet, more on this later, especially on how to use big data to beat insider traders).??,In the stock market example, we are unable to make predictions. We could start with a positive gain, then stay in negative territory forever, or in positive territory for ever, or oscillate forever.,In the case of predicting air pollution, extreme values are rather rare, rarer than for a Gaussian distribution if we use A(k) = -ln(U), and this makes predicting extreme events easy.
 ,A frequently cited study by McKinsey predicts that by 2018, the United States could face a shortage of 140,000 to 190,000 "people with deep analytic skills" as well as 1.5 million "managers and analysts with the know-how to use the analysis of big data to make effective decisions.",The field is so hot right now that Roy Lowrance, the managing director of New York University's new Center for Data Science program says "Anything that gets hot like this can only cool off." Regardless of this, the current school year won't be over for another five months and 50% to 75% of its students already have firm job offers!, ,To say the least, data science involves some art because it requires creative experimentation. This balance of creative experimentation and detailed analysis is not attainable by everyone. Thus the demand-supply gap, as forecasted by multiple MNC?€?s, for the Data Scientist talent will never be completely filled!,However, if you are comfortable with numbers and know how to code, even at the basic level, a career in Data Science is something you must explore. Here, we list down some of the best courses you can undertake to get the certification of being a Data Scientist:, This course, primarily, examines learning from data in order to gain useful predictions and insights. The people who take this course are expected to have prior programming experiences and a basic understanding of statistics. The main focus of this course is to teach students to deal with data (collect and prepare it), analyse the collected data and make useful predictions.,The 5 focus areas of this course are:, The UC Berkeley School of Information offers the only professional Master of Information and Data Science (MIDS) delivered fully online. Through this program, you can achieve the Online Master?€?s Degree in Data Analytics [M. S.]. This can help you make sense of real-world phenomena and everyday activities by synthesizing and mining big data with the intention of uncovering patterns, relationships and trends.,This course introduces students to concepts of information systems and the role of information systems within an organization. Topics covered will include:, This program is a collaboration between Stanford?€?s Department of Statistics and Institute for Computational and Mathematical Engineering. The core curriculum is, as you might imagine, heavy on mathematics and computer programming. This Data Science course is carved to attract, both, engineering or science students as well as mathematically oriented students. These students are interested in better understanding of the mathematical and statistical underpinnings of data science. They are looking to gain expertise in data science and its applications.,With the MISM program, the students will be trained in business process analysis and optimization, and will be educated on data warehousing, data mining, predictive modelling, GIS mapping, analytical reporting, segmentation analysis, and data visualization. This program has an important component of experiential learning. The students are trained to acquire the skills for analytic technology practices with applied business methods.,Carnegie Mellon?€?s MISM focuses on three core areas:,The Harvard Business Review recently identified this Institute among the best. It was identified as one of only a few sources of talent with proven strengths in data science alongside Stanford, Berkeley, Harvard, and Carnegie Mellon. The curriculum is a carefully calibrated and a mix of applied mathematics, statistics, computer science, and business disciplines.,The point to remember is that it always makes sense to get certified by trusted institutes and platforms when it comes to a niche job profile.,Which course are you taking, Mr. Data Scientist?,Sudhanshu Ahuja is the founder and CEO of a , ?€? a platform to provide companies access to top data-scientists. Ideatory counts some of the largest companies in the world as it clients.
 , ,??, , ,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??, ??, ,??,??,??,??,??,??, ,??,??,??, , , , ,??,??, ??,??,??,??,??
This blog post was originally published as part of an ongoing series, "Popular??Algorithms Explained in Simple English" on the ,.,Commonly used in Machine Learning, Naive Bayes is a collection of classification algorithms based on ,. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. So for example, a fruit may be considered to be an apple if it is red, round, and about 3" in diameter. A Naive Bayes classifier considers each of these ?€?features?€? (red, round, 3?€? in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren?€?t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it?€?s labeled ?€?naive?€?.,Although it?€?s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification.,In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features).,A simple example best explains the application of Naive Bayes for classification. When writing this blog I came across many examples of Naive Bayes in action. Some were too complicated, some dealt with more than Naive Bayes and used other related algorithms, but we found a really simple example on StackOverflow which we?€?ll run through in this blog. It explains the concept really well and runs through the simple maths behind it without getting too technical.,So, let's say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it?€?s long or not, sweet or not and yellow or not, as displayed in the table below:,So from the table what do we already know?,Based on our training set we can also say the following:,Which should provide enough evidence to predict the class of another fruit as it?€?s introduced.,So let?€?s say we?€?re given the features of a piece of fruit and we need to predict the class. If we?€?re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it?€?s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.,In this case, based on the higher score , we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.,Now that we?€?ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g. ?€?spam?€? and ?€?ham?€? in case of spam detection) as classes we can start to classify documents and text automatically. You can read more about Text Classification in our??,??or use our Text Analysis API for free ,.,There you have it, a simple explanation of Naive Bayes along with an example. We hope this helps you get your head around this simple but common classifying method.
I was deep into a presentation at a major retailer. In the darkened room, a lone hand shot up. ?€?John, we spend 80% of our time on data load and prep. Only 20% is used to produce analytics. We don?€?t like that ratio.?€?,The speaker was right. About 80% of the analytics process is spent on data preparation and loading. Numerous examples come to mind. I remember a project for an auto insurance company using telematics and driver behavior. The one-off code to prepare the data took three days to write. That?€?s typical.,Time spent in data load and preparation and writing lines of code do nothing for the bottom line. Reducing these aspects of analytics saves enterprises money and assets. Wall Street doesn?€?t measure value by the number of lines of code an organization produces. That?€?s no way to work.,The three most expensive letters with regard to data are E-T-L (extract, transform, load). Traditional data movement approaches can choke data sources and the network. We must do better. This requires tools that speed up data movement and transformation and techniques like change data capture. Data should be moved through a parallel network or via network data integration. It should be highly available and in a connected environment that is always on and always on time.,Another issue is code surface area. If your analytic platform is based on low level languages like Java or C++, your analytic agility is reduced significantly. The best way to reduce complexity is to reduce code surface area. ??The cost to develop, change, test, and support an analytic is directly related to the lines of code to construct it. Fewer lines of code, or better, a set of simple commands, enable you to be analytically nimble.,Some define big data as data of such size that I cannot afford to land and store that data. I define big data as data of such size and complexity that you are unable to find analytic value from it in an expedient way to positively impact business. When analytics is slow and complex, it is treated as a tangent and kept in a lab off to the side; that?€?s not successful. When use of analytics is fast, easy and responsive, it can be leveraged widely. Further, analytics results can impact business operations and have impact on things that matter.,Applying analytics is much harder than creating them. To ensure an analytic is successful takes changes to people and processes that are far more difficult than technology changes. Changing people and process enables an organization to apply analytics to real world business problems. Having more time to make these changes is paramount to making analytic output valuable.,So how do we change the 80/20 rule? We use tools to increase the effectiveness and the efficiency of data preparation. That gets us part of the way there, reducing the 80% by making the job less onerous and manual.,We need to work both sides of the equation, to increase the efficiency of data prep and expand the number of people using data. We must democratize analytics by lowering the barriers to advanced analytical topics. With greater efficiency and better tools for more people, we shift the conversation from one that focuses 80% on esoteric technology to one that invests 80% into making things happen.,We are integrating more data sources and analytics techniques than ever before, with a Wild West of tools and techniques: one for text analytics, another for cluster analysis, another for affinity analysis, and another for pathing. With different departments using different tools, you wind up with governance problems that even the best sheriff can?€?t solve. Companies need a central strategy to flip the 80/20 rule. Think of it as a race with the competition; the first one to flip it wins. And you won?€?t flip it without centralizing your strategy and rationalizing data across the organization.,Flipping the rule will mean more data-driven decisions. Today effort is invested in creating analytics that support high-value decisions. Reducing the grunt work in data preparation and increasing the use of analytics by more stakeholders help make that $5 million dollar decision the right one, and also help make 10,000 $500 dollar decisions better.
When we talk about Big Data, many of the examples and use cases we share center around how Big Data is changing the way businesses must operate. But Big Data is changing the world on some exciting global levels as well. So let?€?s take a step back from the business impact of bigger data and check out some of the exciting ways Big Data is changing our lives.,Environmentalists and policy makers can monitor, almost in real-time, the status of forests around the globe with the help of satellite imagery. Tools like the Global Forest Watch, which was launched in 2014, uses high-resolution NASA satellite imagery processed through Google, to analyze over 700,000 satellite images. High resolution maps show annual forest cover change since 200 to assist environmentalists and government organizations to monitor deforestation in ?€?near-real time.?€?,Scientists leverage Big Data technologies to predict weather to prevent disasters, save assets, and take precautionary measures. Research organizations and weather companies such as Basho?€?s Riak NoSQL, use 13 data centers to capture 2.2 million weather data points from all over the globe four times per hour.??,The Weather Company, which already monitors over 20 terabytes of data per day to create some of the most accurate forecasts, is integrating this new technology to bring weather predictions to a whole new level.,By 2050, the world?€?s population is expected to reach 9.2 billion people, 34% higher than it is today. To enhance crop production, farmers are using precision technology, a concept that collects real-time data on weather, soil and air quality, crop maturity, and equipment and labor costs, to develop predictive analytics used to make smarter decisions.,One study found that farmers using only one type of precision technologies increased their yield by 16% and cut down water use by 50%. Another example given by IBM is the use of sensors placed throughout the fields to measure temperature and humidity, which when coupled with predictive weather modeling, helps farmers make proactive decisions on when to water or fertilize.,Big Data is used in the health industry for numerous applications ?€? from monitoring wearable data to detect early signs of Parkinson?€?s disease, to fighting the Ebola crisis, or predicting outbreaks of infectious disease.,IBM recently released , showing how Big Data can help contain global outbreaks of dengue fever and malaria. With dengue fever having spread to over 100 counties and malaria causing 1 million deaths annually, this Big Data application is saving lives.,Researchers can also predict infectious disease outbreaks such as the flu by using modeling developed from data gathered through mobile, social media, and other public web data. , analyzed page view histories of disease-related Wikipedia pages in 7 languages to forecast the number of people who may become sick in 4 weeks.,Every 26 seconds, a student in America drops out of high school, which is 7,000 a day. To combat the failing education system, education technology companies are chomping at the bit to leverage their Big Data technologies to make learning more effective. ??, is an adaptive platform that collects real-time data based on students' learning habits. They recently received $105 million in funding to enable smarter learning that's tailored to the ways in which students learn best. The company also recently partnered with Mifflin Harcourt and Gutenberg Technology to create smarter digital textbooks that adapt to individual student in real time. The technology assesses students as they solve problems and can adjust the difficulty of the remaining problems as needed.,Big Data is changing the world in new and exciting ways. 
I got this in my mailbox this morning:, , , ,[dangerous Link deleted], , , , ,This message screams "spam / scam" to all of us, yet you'd be surprised to see how many people clicked on it, and their geographic distribution. Because the scammer hided itself behind a bit.ly redirect URL, all of us can track ,:,As you see, there is a lot of clicks, and almost from US exclusively. You would wonder: are these guys using data science to deliver their spam? Companies that I am familiar with would typically pay above $20,000 to get that kind of email performance. Those who do a better job at hiding the bad links and fake sender, would probably get even more traffic. Assuming their click rate is 0.1%, they must have sent their email to 1,000,000 people, with 88% of the recipients in US. Based on the stats below, I suspect Chinese hackers might be involved. It is very unusual to see China as the top non-Western country.,Geographic distribution of those who clicked (and most likely, got infected):,Here's the click distribution over time:,And bit.ly flagged this link as suspicious, last time I checked:
Data exists in a dangerous state of near-non existence. Few businesses would risk not having backups in place. With cloud computing becoming commonplace in enterprise, we?€?ve come to accept that our data will be replicated and stored in duplicate.,Even data that is intentionally deleted can often be recovered. When Yahoo! purchased Geocities, nobody dreamed that it would go ahead and delete the entire archive ?€? more than 600 gigabytes of internet history. Despite this, enthusiasts were able to quickly archive the collective work of 35 avid Geocities webmasters ?€? an important milestone in our ability to breathe new life into data that someone else does not want.,Deleting data is not just a catastrophe for the user, or the business, or the system itself. Deletion of data also has a cost attached. We?€?ve all deleted files, essays, reports or emails by accident, and we?€?ve been forced to spend hours recreating what we lost. ??Other consequences also make data loss costly: loss of custom, loss of reputation, or damage to a brand.,Often, it is easier to harvest massive amounts of data, and create massive backups, than to be selective and economical. This means we have huge data silos just waiting to be used. That?€?s if they?€?re useful at all.,If data is immortal, surely we can just save it, back it up and move on? Why worry about data if it can take such good care of itself? Surely all we need to do is back it up on a regular basis?,Unfortunately, it?€?s not that simple. Data may be able to replicate itself and survive certain catastrophes, and we might be capable of creating lots of copies for a relatively cheap price.,But data cannot check its own validity or keep itself error-free. And the longer you keep a piece of data, the less useful it becomes.,The rate of data decay is estimated at about 2 percent per month. That doesn?€?t mean a lot on paper. In real terms, industry experts suggest that almost a quarter of??,.,Our own figures suggest that 42 per cent of failed CRM projects came off the rails because of the state of the data. It?€?s not that businesses don?€?t use the systems, or fail to adopt them enthusiastically. They just expect the data to live on, untouched, without any further maintenance.,Just because data is safe, and immortal, and secure, that does not necessarily mean that it is worth preserving. The world?€?s data centres are packed with old data that could be inaccessible, corrupted, duplicated or out of date ?€? or all of the above.,Clean data does not occur by accident. We get clean data because we invest time in making it so. That means:,In order to purify the database, some kind of management or intervention is required,There are three main ways to obtain clean data from a dirty database.,Whichever route you chose, you must do something about your data. Simply saving it, copying it and archiving it is not going to ensure its quality and longevity.,Data storage technology has come on leaps and bounds in the last five years. From stone tablets and vellum, we?€?ve transformed the way we record the things we think, say and do. We can store hundreds of books on a tiny memory card, smaller than a postage stamp. And archives have saved the early days of the web from obliteration, ensuring that our first attempts at web design ?€? MIDI files and all ?€? are given their rightful place in history.,It?€?s possible to manually undelete data, or delete it and start again. If data goes bad, we can trash the whole lot and purchase a list from a third party. But for the best ROI, the best conversions and the best relationships with our customers, we should aim to keep our data clean, and ensure it?€?s relevant and useful for years to come.
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,How big is ?€?Big Data??€? Business analytics spending will grow to $89.6 billion by 2018. Banking, healthcare, energy and business all rely on accurate, insightful analysis. Prepare for this tremendous opportunity now with a PhD Management & Decision Sciences. At Capitol Technology University, you can start your doctorate with live courses, delivered online. Visit??captech.edu/gocap??to ,.,??,??|??,??|??,??|??,??|??,??|??,??|??
We're looking for highly motivated individuals to help us extract meaning from Twitter's massive dataset. As a Data Scientist for the Analytics team, you'll use statistical analysis and data mining techniques to help us better understand how users engage with Twitter, determine whether new and experimental features should be launched, and measure Twitter's success across the entire organization. You should be passionate about finding insights in data and using quantitative analysis to answer complex questions.,:??,Please mention??,??when applying. Thank you.
Congratulations to our Data Science Central partner, Vincent Granville of Analytic Bridge! In a recent Forbes.com post Vincent shows the power of community in the wildy growing universe of Big Data. From the post:,Take a look at the entire list 
According to the San Jose Mercury News writer Chris O'Brien, not only does Silicon Valley have the official theme of Big Data, but there is a data-driven revolution coming in healthcare. From the article:
??,??



We have re-designed our online, accelerated data science apprenticeship: it is now available to anyone, at no cost, with no restrictions, and does not require any application nor deadlines. Data sets, a cheat sheet to get you started, ,, sample code, and tons of resources, ,, and are regularly updated, including very recently. The presentation style is compact.,This is an ideal program for professionals with a quantitative background and some industry experience - in a nutshell, for anyone who understands ,??and can get started using it. This program is for self-learners. Completed projects, submitted to and reviewed by ,, will be featured, published, and promoted on our network, reaching out to the the largest audience of data science decision makers, peers and hiring managers. Examples of such projects can be found ,??and ,.,Ideal for people who want to change career paths, consultants, people managing data scientists, or students starting an analytic degree. In short, the easy way to become a data scientist for educated self-learners.,Free from ,??or obscure, ,,??,??stats??developed before the era of computers (,-value, GLM, model-based predictive analytics). Instead focusing on building automated, black-box, simple, scalable, efficient, robust, high ROI solutions for many modern applications (,'s, ??,??and real time), leveraging ,??as needed, and using a unified cross-disciplinary approach.??Mostly in NoSQL environments.,Applications: IoT, digital (mobile) and web data, marketing, risk management, fraud detection (including spam, fake reviews, plagiarism detection), ,,??operations research, finance, bioinformatics, healthcare, environmental data science, engineering, business analytics, model-free predictive analytics, business optimization, big data and more. ??,You may learn new robust algorithms and concepts such as,as well as obtaining advice on data creation or identification, data gathering, and blending from various external / internal sources (structured / unstructured; ??, / ,), defining metrics??,, automated exploratory analysis with ,, and delivering insights via visuals or API's, using ??the ,.,??you can connect with more than 1,500 professionals interested in our initiative, share ideas, ask questions, and get support.,.
Here is my top 7 list of daft things that some people say about Big Data.,I think that Big Data does play a role in some businesses. I also think that some of the basic distributed file store and text search technologies can be usefully employed, in non-traditional indexing, counting and correlation. However, there is an awful lot of nonsense said about Big Data.,So, onwards and upwards.,If Big Data is currency, and for most of us, it isn't, then it's more like the hyperinflationary money of the Weimar Republic, rather than something you would take to the bank or try and buy the weekly grocery with.,Big Data might have value, no doubt some of it does ?€? it can't all be dross, can it?But, that doesn't make it a solid financial asset class ?€? that's just dopey. The value of data providers, such as those companies supplying financial market and instruments data, is in the service of providing accurate, appropriate and timely data. Data has no significantly greater intrinsic financial value or exchange liquidity than pints of beer or glasses of wine. It can have time and place utility, of course, as a product or as a service, but it is not like a central-bank backed currency ?€? not even close.,Call me an old-fashioned cynic, if you must, but I don't believe for one moment that we have now learned how to turn lead into gold, or for that matter, Big Data into golden nuggets.,You see, if Big Data contains gold, and it doesn't, then it would be more like FeS2??than anything else. Or, to use the vernacular, it would be more like fool's gold than Welsh gold. Which, considering the quantity of hype surrounding Big Data, is the most apt analogy.,If you want to see what is important for everyone, then start with Maslow's hierarchy of needs, a pyramid of essential human motivators that curiously hasn't been expanded to include Big Data. Maybe because the fact that it's not an essential human need.,I would also like to mention that quite a number of Big Data projects have nothing to do with Big Data or Hadoop at all.,Doing professional taxi-drivers out of a living income is not solving world problems, no matter how many times one chant's??,.,We know what the fundamental world problems are, and we know, more or less how to solve them. In this respect we don't need Big Data to tell us which way is up. Consider this:,Maybe Big Data can inform us that...,"The silent killers are poverty, hunger, easily preventable diseases and illnesses, and other related causes. Despite the scale of this daily/ongoing catastrophe, it rarely manages to achieve, much less sustain, prime-time, headline coverage." Source: Global Issues,Maybe if we knew all of this (and we really do) then maybe we can do something about tackling the problems.,Maybe we should also be less eager to instrumentalise real suffering in order to flog aging technology and price-gouging services, and actually do something about problems we know of, using solutions that are available to us, to the benefit of those less fortunate than us.,Big Data is characterised by its volumes, its velocities of generation and communication and its varieties.,The thing is, data has always been growing in volumes, and there has never been a time when it has actually decreased. Also, data has been increasingly generated at a faster rate, a trend that doesn't look like stopping anytime soon. Lastly, the varieties in format and content of data objects has been growing since the early eighties (that much I can vouch for), and to some significant extent, before then as well.,The thing is, nothing of significance is new. Not that this matters. But data has always about volumes, types and velocity, and those factors didn't suddenly become relevant at the turn of the millennium. Neither is the technology new, most of the technology labelled as Big Data technology is a collection and configuration of technologies that are decades old.,To be precise, this is about variations on the theme of distributed file systems and text search and count (e.g. Hadoop) replacing Enterprise Data Warehousing.,There are two axioms that can be applied when considering Hadoop and Data Warehousing:,In short, using Hadoop as part of the tech stack for an Analytics Data Store makes absolute sense. Trying to shoehorn a mature strategic and tactical decision support platform into Hadoop is just daft.,Whilst we are on the subject, even staging data with Hadoop is overkill, and simply adds another point of failure in the Data Warehouse process.,As for doing ETL with Hadoop and Java? Please?€?.,There are legitimate and ethical proponents of Big Data and indeed some applications seem, at least superficially, to make some good sense. However, there are an awful lot of unscrupulous or wilfully uninformed Big Data punters around, who are in my view giving the impression that there are no limits to what can be achieved with Big Data. As if Big Data were the ultimate universal panacea.,Of course, to put it simply, it's not true, it never was, nor is it likely that this will ever be the case.,It has been proven that Big Data technology is not only useful for some companies, but it's absolutely essential for them. However, not everyone will have the same or similar business models and drivers as Google, Twitter, Facebook and Amazon. In fact, most other businesses are not really like these internet advertising companies in any significant way when it comes to the data they collect and the ways that they can apply it. That is, very few other companies are basing their business models on advertising revenue streams, data brokerage and search.,That stated, efficient distributed file stores, text search and word counting will have their place, it's just that this place isn't for everyone.,As always, please share your questions, views and criticisms on this piece using the comment box below. I frequently write about strategy, organisational leadership and information technology topics, trends and tendencies. You are more than welcome to keep up with my posts by clicking the ?€?Follow?€? link and perhaps even send me a??,invite. Also feel free to connect via??,,??,and the??,??website.,For more on this and other topics, check out my other recent posts:,#Hadoop #BigData #BigDataAnalytics #Decency #Ethics
 , , , , 
Companies have more data on staff than ever before in history and big data analytics is making its way into HR practices fast. Analyzing staff performance is nothing new, but the extent to which we can now collect and analyze such data is going beyond all norms.,Sociometric Solutions puts sensors into employee name badges that can detect social dynamics in the workplace. The sensors report on how employees move around the workplace, with whom they speak, and even the tone of voice they use when communicating. By analyzing data from smart badge technology Bank of America noticed that its top performing employees at call centers were those who took breaks together. They instituted group break policies and performance improved ,. Another company, Humanscale, builds sensors into it?€?s line of office chairs, standing desks and work stations and offers companies their OfficeIQ system to monitor workplace activity such as how much time individuals have spent sitting or standing at their desk as well as how long they have been away from their desk.,In Ireland, grocery chain Tesco has its warehouse employees wear armbands that track the goods they take from the shelves, distributes tasks, and even forecasts completion time for a job. In other sectors, including healthcare and the military, wearables can detect fatigue that could be dangerous to the employee and the job they perform.,Fujitsu has just released ,, a business package that can collect and analyze data from devices such as accelerometer sensors, barometers, cameras and microphones to measure and monitor people at work. For example, data such as temperature, humidity, movements and pulse rate can be used to identify when workers are exposed to too much heat stress. The system can also detect locations and even postures and body movements of humans to sense a fall, track someone?€?s location or estimate the physical load on a body., ,The external monitor parallels what?€?s already being monitored inside the body using health trackers such as Fitbit. Data already show that employees engaged in wellness programs show significantly smaller increases in the cost of their health care than those who aren?€?t. So far, employers can?€?t access an individual employee?€?s health records, but those days may not be far off, when a boss might take you aside to discuss your stress levels or the long hours you?€?ve been putting in at your desk.,As the world becomes increasingly digital, companies have endless ways to monitor their staff. Most things we do in a typical workday already generate a lot of data: we send and receive emails, we make phone calls or we operate equipment. But soon, there will be so many new data sources and so many new ways of cutting that data?€? using cameras, sensors or crowd sourcing data to measure every aspect of someone?€?s performance.,Should companies use this data to monitor staff? Is it even ethical to treat us like copiers and routers? One vendor, Cornerstone onDemand, believes it can help companies predict and improve employee performance. Its analytics software is able to take over half a billion employee data points from across the world to identify patterns and make predictions about hiring decisions and employee performance.,This kind of analysis can be used to identify the most successful recruitment channels or key employees that might be at risk of leaving, but my fear is that many companies will spend too much time crunching all the things they can so easily collect data on, including how much time we sat on our office chair or how many people we have interacted with, rather than the more meaningful qualitative measures of what we did when we sat on the chair and the quality of our interactions with others.
I absolute loved the point made by??
??, ??,??,??, ??, ??,??,??,??,??,??,??,??, , , ??,??, ??,??,??,??,??,??,??, ??,??,??,??,??,??, ??,??
BASICS: DEFINING BIG DATA AND RELATED TECHNOLOGIES., is high-volume, high-velocity and high-variety information assets that demand cost-effective, innovative forms of information processing for enhanced insight and decision making.,Are there examples of Big Data in action at major global IT giants?,It is evident from , that Big Data generates value from the storage and processing of very large quantities of digital information that cannot be analyzed with traditional computing techniques. It requires different techniques, tools, algorithms and architecture. Some of Big Data tools and technologies are ,Facebook and Google heavily rely on Big Data. Let us base our first case study on Facebook--Every time one of the 1.2 billion people who use Facebook visits the site, they see a completely unique, dynamically generated home page. There are several different applications powering this experience--and others across the site--that require global, real-time data fetching. In this section, we will discuss some of the tools, frameworks and applications that Facebook developed to overcome the challenge of processing the huge data:, probably processes more information than any company on the planet and tends to have to invent tools to cope with the data. As a result, its technology runs a good five to 10 years ahead of the competition. Google has come up with quite a few big data processing algorithms such as MapReduce, Flume on which many big data technologies such as Hadoop have been developed. We, in this section, will discuss about some of the big data technology stack at Google:,Google File System is the base of hadoop's HDFS that is being used actively in a lot of big data tools and databases such as HBase, Cassandra, Spark etc.,Flume has been handed over to Apache and there is an active project running on this named as Apache Flume.,Google offers a cloud analytics platform called BigQuery based on Dremel to enable companies get their huge structured data processed at lightening fast speeds.,??HARNESSING BIG DATA FOR SECURITY: INTELLIGENCE IN THE ERA OF CYBER WARFARE.,There can be information without security but the can be no security without information. This is unequivocally supported by the great French military strategist and tactician, Napoleon Bonaparte who rightfully asserted that ?€?War is ninety percent Information.?€?,In the Digital Era that has heralded the dawn of the Information Age and the frontier of Cyber-Terrorism, Napoleon would have obviously noted that there can be No Command without Cyber Command!,Blind movement in the world wide web must not be confused with motion. Intelligence navigation requires real Predictive Analytics. We must not only detect events, but also study, analyze and,??foresee future patterns of occurrence.,With a combination of powerful information technology tools and methodologies such as Data Mining, Predictive Analytics, Artificial Intelligence and Machine Learning, it is possible to fight a smart war against bandits, terrorists, cyber fraudsters and other gangsters at the click of a button.,Will the construction of a physical wall covering the border with Al-Shabaab stop terrorists from attacking Kenya?,As seen recently in a shooting in Texas, USA, even walls in form of big oceans between the Middle East and America have not stopped the Islamic State from launching an attack on American soil! Pietersen gives a snapshot of what it takes to prepare for intelligence-led missions:,Now looking back over nearly 40 years, I think I have learned the following six things.,BIG DATA OR SMART DATA?,Today, data is growing by leaps and bounds in volume, variety and velocity. There can be no meaningful data analysis without proper management of Big Data.,There is no telling who controls the Internet nor who contributes exactly to the millions of websites being created every moment. The more traditional, rigid and bureaucratic government departments are finding it hard to keep with modern, young minds full of dynamism and terrific zeal. The most horrifying fact is that these young minds are being tapped, funded and indoctrinated by jihadist and terrorist organizations to further their evil agenda?€?the agenda of eliminating any individual who does not accept their ideology.,Therefore, it is crystal clear that for security agencies and governments to effectively fight terrorism, they must equally invest in dynamic pool of digital talent that will ignite a seamless network of smart, agile adaptive and disruptive army of Cyber-genius credentials. It is possible! Perhaps you would be happy to work in such an exciting pool of digital talent.,So, how can governments leverage the power of Big Data for the Security needs of our Digital Age?,It is not all about abracadabra solutions. Thinks tanks must be created, digital resources must be mobilized and brows must be knit as the mind retires into depths of thought that would yield remarkable new streak of innovations that will not only anayze the huge gig data piles around us, but also invent brand new intelligence tools that must work smart round the clock to process Big Data into actionable and smart information to enhance security.,Wait a minute! Aren?€?t there enough technologies already about Big Data?,Yes, there are technologies such as Apache Hadoop and it is Open Source! However, it is not enough for it can be developed into an advanced and better innovation to keep pace with the dynamism of our digital era. Let?€?s have a flashback--Recently, an eminent expert gave a great definition on data security:,Security. What is security? Dan Geer defined it best. Keynoting at the Recorded Future User Network (RFUN) Conference in Washington, D.C. Geer said:,?€?Security is about the absence of surprises that can?€?t be mitigated. As such, security that is well thought is security that changes the probability of surprise while foregoing as little as possible.?€?,Therefore, there is no taking chances when it comes to matters security. Planning must be comprehensive. Implementing data security plans must be surgically thorough.,For instance, America is at the forefront of the Data Mining/Big Data battleground through Data Analysis and Research for Trade Transparency System (DARTTS). This is an office affiliated to the Homeland Security Department that works pretty well in prevention of money laundering and trade-related crimes.,Why then can?€?t Kenya use the platform of Big Data to detect, deter and decimate terrorists?,??,REFERENCE,Recorded Future. (2015). , Retrieved online from ,Department of Homeland Security. (2015). ,. Retrieved online on 8, May 2015 from ,Gartner, Inc. (2013). ,. Retrieved online on 11, May 2015 from ,Sain Technology Solutions. (2015). Big Data Fundamentals. Retrieved online 11, May 2015 from 
We've gathered data from our newly created??,, and based on 20,000 search queries over the last four months (most of them in the last 30 days), we discovered that the top queries so far are:,The number in parenthesis indicates the number of queries, over the last four months. Note that some keywords have a high number of queries, because ,??in one of our popular articles. Starred queries were not promoted in any way.??,The growth of this search box is best illustrated in the chart below, showing queries per day:,Today ,, ad-free, where anyone can submit his blog for indexation. We invite you to try it and share it. Let us know which publishers should be added for crawling purposes. Unlike Google search, this engine will focus on specific, relevant websites, rather than generic top domains such as Wikipedia, Forbes, LinkedIn, New York Times, that get a large share of the Google search traffic despite delivering re-posted (non original) or barely relevant content. The purpose of this new search engine is to boost and give credit to the real authors and content creators (not to publishers with tons of money to do deals with Google), and especially display more results from actual data science practitioners, as opposed to journalists. And the cherry on the cake: there is no ads. Also, you can sort results by date or relevancy.,We expect this search engine to have far more traffic than our internal DSC search mentioned in the above paragraph, as we open our doors to any data science blogger, practitioner, or publisher -- big or small. Enjoy the new search feature!

Table of content:
If you categorized how you spend your time at work every day, on which tasks do you spend the bulk of your time? Most business analysts spend??,, which includes pulling, cleansing, manipulating and formatting their data. This leaves them with only 20 to 50 percent of their time to focus on actual analysis and uncover actionable insights. That?€?s quite the dedication of hours.,Wouldn?€?t you like to cut down on that time a bit? Effective data preparation isn?€?t much different from getting into good physical shape. All you have to do is tone up your data strategy to build your analysis muscle. We?€?ve spent years helping our clients chisel away at the time-consuming process of manual data cleansing to free up more of their hours for focused data analysis, and we?€?ve learned some interesting lessons along the way.,Check out our tips below to get your analysis in good shape by reducing time spent in data preparation and opening up more time for your search for actionable insights and profit improvement.,Much like with fitness goals, overnight results rarely happen in big data process transformations. You can?€?t immediately shift focus from data cleansing and preparation to data analysis with just enthusiasm. You need a plan.,Start your robust strategy by asking yourself the following questions:,Outline a path from your current situation to your goal situation, and use this as a reference to keep yourself from veering off course or giving up halfway through the initiative.,The trick to toned muscles isn?€?t necessarily in heavier weights; it?€?s often in better form. Similarly, ensuring your data starts in the best format for analysis dramatically reduces time dedicated to data cleansing and transitioning. Unfortunately, even the most tenacious analysts can?€?t control the data handed to them. If you want to receive the correctly formatted data each and every time, you need to get all contributing systems and teams on board.,We recommend building and distributing a metric guide for your colleagues to ensure their reporting and contributions to your analytics become and remain consistent. This may seem like quite a bit of work upfront, but the payoff saves you exponential time down the road. After explaining and circulating this guideline document, request your colleagues to have their updated reporting metrics in a realistic amount of time. After all, conforming to your data coding preferences will shift their own processes as well, so set a pragmatic deadline.,Building toned muscled depends upon an optimal number of repetitions, and one of the biggest elements of successful data analysis is consistently updating your data. For some companies this means weekly data pulls once a week. For others, it?€?s once a day. If your data is set up and contributed properly, as our second point highlights, this process should be quick and painless.,Remember ?€???,. Much like physical workouts, if you put off pulling your data on schedule, even if it?€?s just for a day, it will be easier to procrastinate again the next time. Data is much easier to manage the first time around than it is to fix later, so stick to your schedule.,Revisit your data management program at the scheduled checkpoints you created for yourself in your plan. Ask yourself the following questions:,If you aren?€?t making incremental progress and gaining more time to search for actionable insights while minimizing time dedicated to data management, it?€?s time to take a step back and focus on a new direction.,If the issue lies with a system or team member passing off incorrectly-formatted data, consider how you could make this process easier for them. The more you can simplify their processes and the fewer obstacles they face, the more likely they are to helping you reach your goal.,Are you trying to force Excel to do everything you need? You?€?re not alone.??,. However, they, as we?€?re sure you do, get frustrated with the manual, slow data entry and management process Excel creates.,You can?€?t build muscle without a strong toolset. It?€?s time to dump your massive spreadsheets tool and use your business intelligence tool for what it does best ?€? compiling the various types of data together and putting in a easy to read format for analysis.,We recommend choosing an intuitive business intelligence solution that simplifies the data upload and management process while illuminating actionable insights for profit improvement quickly and effectively. These solutions can flip the 80/20 time split on its head ?€? allowing analysts to spend up to 80 percent of their time ?€? or more ?€? in analysis and reserving no more than 20 percent of their time cleaning up data.,If you need a better BI solution to help you flex your analysis muscle,??,??powers through data manipulation quickly and efficiently, helping analysts get to the actionable insights they need. Even better, we can have you up and running in less than 24 hours from receiving your data, putting you even closer to profit improvement.,Now that you?€?ve streamlined and standardized your data preparation and management processes, it?€?s time to leverage your increased chunks of analysis time to find opportunities for actionable profit improvement. Save your muscle for your responsibilities that matter ?€? the analysis of your data.,If you have any questions on how to increase your time and capacity for data analysis, or if you?€?d like to receive access to our exclusive KiniMetrix demo,??,.

Today, people are no longer looking to reduce their data consumption. In fact, if anything, they want more data originating from more sources and with more diversity than anyone could have ever imagined. As we pioneer a world where data can be digested easily, software solutions need to be engineered so they can expand to meet the customers demand. Increasingly, and because of this trend, more and more software developers are creating solutions using an elastic cloud infrastructure. In??a report entitled, ,, authors Nikolas Roman Herbst, Samuel Kounev, and Ralf Reussner define cloud computing elasticity as ?€?the degree to which a system is able to adapt to workload changes by provisioning and deprovisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible.?€? Put simply, this flexibility allows software teams to change the state of their deployment in AWS (Amazon Web Services) so it can mirror their specific requirements, whether it is to spin up instances for a new deployment, bringing back ?€?Spark Workers?€? to increase the power of query engines, or to shut down a dev-only deployment at the end of the day. This is primarily motivated by providing customers an optimal experience with respect to performance and allocating resources as efficiently as possible. From a business perspective, the elastic cloud can expand or contract as data consumers do their work meaning the organization can add users, increase projects, dump in more data, remove users, and change sources without worrying about things such as licensing, capacity and cost., , When it comes to deciding if an elastic cloud infrastructure is the right approach for your development team, it is best to start by making some key decisions well before writing any code and answer questions such as what language do we use, and is the app specific to one use case, or should it be general to any interaction we want to have with AWS? Questions should also address whether it should live as a long-running service or an on-demand app that runs the command it is given? We recently embarked on similar project where we opted to apply this elegant design to Paxata?€?s adaptive data preparation solution. In a production environment that stringently employs the use of unit tests and continuous integration practices, here are some of the things I learned along the way: , , , , Previously, all of our AWS-facing code had been written in Python, so originally the thought was to write General Cluster with Python. AWS?€?s boto library is widely used with a very active community, so this seemed like an enticing option., , However, a couple considerations took precedence in the final decision: , A Python (or any other non-compiled) app could be easily modified, but not as easily tested. Since General Cluster might have control over the entire cloud infrastructure that a solution runs on, it was prudent to make sure any significant changes to the app had to go through a couple of hoops to be deployed. Writing General Cluster in Java was another option as it could easily integrate with other applications that were written in some combination of Scala and Java. This gave us the option of re-using any relevant code that had already been written. , , Eventually, we settled on a Scala implementation on top of AWS?€?s Java API. Amazon?€?s Java developer community is pretty strong, and even though it didn?€?t afford some of the conveniences of boto?€?s implementation, it was easy to write an abstraction on the Java API in Scala which provided us the opportunity to extend capabilities in the future., , , , Like the early pioneers who managed to get to the West without GOS we soon learned that the AWS API isn?€?t terribly well documented. Some of the most common use cases are described, but there wasn?€?t much written help to address the granularity of control that we wanted,. There was an API reference, which proved marginally useful, but what really helped was that the code was written very explicitly., , Every command to EC2 takes one object, a ?€?request?€? object, and these are easy enough to make. This means that instead of passing any of the parameters into the command you want to execute, you should set your parameters as attributes on the object, and send it to the command. Similarly, every AWS Command - even those that didn?€?t have any useful return value - returns a ?€?result?€? object, from which you can extract either the information you requested, or an indication of what request you made was actually executed with the right behavior. The API itself is stateless, so if you want to change an attribute of an Instance you created you must send that Instance (or its ID) as part of another request to the system., , While it might seem verbose, it is actually an ideal way to understand what is really going on under the hood. Every piece of information that could be sent to a command is enumerated right there for the development team to inspect and play with, and being able to play with each command on the fly can help the team understand all the different ways that command could be interpreted in the cloud environment. This is especially useful when writing error-handling code, because IT is aware of all the potential ways their requests could error so they are able to account for each of them., , But let?€?s face it, this is no substitution for good, thorough documentation. While the way the code was structured can allow you to build General Cluster without written help, there will always be occasions where good documentation is required., , , My company deployed the General Cluster only one month after starting development. It lived as a small command-line JVM app that took very basic commands, most of which fit into only a couple of use cases. However, it did work. In the spirit of the motivation for this project, we knew that we weren't done; the first pass of the app was messy and untested, and in a couple of minor cases, it flat out did the wrong thing. But we were able to use it within our own operation team, and within the first week or so it was easy to see that choosing to write and deploy General Cluster as a production application was paying off., , Although it doesn?€?t hurt to validate the approach, deploying sooner rather than later made it much easier to see and triage future priorities. Some bugs and pain points were far more important than others, but our past prioritization had no context. Fortunately, running in the cloud as an internal tool provided that., , General Cluster has grown since then. After a rigorous QA process, it is now a persistent service running on Typesafe?€?s Play Framework. It keeps tabs on clusters of instances that it has spun up, and it?€?s resilient to the various unexpected behaviors we?€?ve grown to expect from the Cloud. But like the term elastic suggests, over time we expect to continue development so that we can do even more interesting things within our cloud-based Infrastructure.,??
We reveal some interesting statistics around data loss and also offer some helpful advice about what an effective data backup plan should look like. For example, did you know that this year, 40% of small to medium businesses that manage their own network and use the Internet for more than e-mail will have their network accessed by a hacker? Also, find out what are the main causes for data loss and much more.
The key to making a business case for any Analytics initiative, not just text analytics, is to?? identify specific business problems and pain points and??use??analytics to address them, instead of merely seeking insights.,Companies find themselves in a world where an increasing number of their customers are??using social media, and the one thing, people LOVE doing on social media is??talk (tweet/post/blog/whatever...) They talk about their experiences in dealing with the company and its services or products, about its competitors and about how they really feel.??,So, as a company, you have all this customer feedback??out there, in the form of text,??just waiting to be gathered. The risk??company management faces,??for not capturing this customer feedback, is just too great to ignore.??They face the risk of looking bad (think PR nightmare)??and losing their competitive advantage, if they do nothing about it, which brings me to ,.??While some may argue??that, this is too narrow a focus for the application of Text Analytics and while??other use cases for text analytics??may have greater ROI potential, ,Meta S. Brown, has an interesting viewpoint??on building a business case for text analytics and the pros and cons of taking a cost saving vs. revenue increasing benefit approach, which??can be found ,There are some other major trends that stand out in Text Analytics, from ,?? which are still relevant today and are useful to know when making the business case. I have listed below what I felt to be most relevant ones:,Sentiment analysis is evolving, with vendors, offering sophisticated sentiment analysis on multiple scales, rather than simply classifying a document, or a phrase, as positive, negative, or neutral. Text analytics is being used??to identify "emerging issues" or the "birth of a trend". This helps companies to be??more proactive in dealing with issues before they become real "issues" that could have been "nipped in the bud".,End users have begun to realize the value of analyzing unstructured text data in conjunction with structured data (e.g. sales and demographics data) which??can be used to provide a lift to predictive models.,Companies that are lacking in in-house skills to aggregate and analyze unstructured information are turning to Software as a Service (SaaS) solutions for help.
In an earlier blog post on ,??, I had spoken of the importance of Social Media Analytics and specifically Text Analytics within the context of Social Media.for big and small??business. Social??Media plays a critical role in today's world ??in ??understanding, measuring and influencing the real time perception of your??company and/or brand.??,Social Media contains a wealth of information?? which needs to be analyzed and understood in a broader social and demographic context including, trend identification and receiving feedback from segments beyond what the traditional marketer or customer service center is accustomed to for receiving feedback. Given the sheer volume of data and the large number of users?? talking (posting, tweeting, etc.) about any given topic, visualization can be used very effectively in Social Media Analytics to effectively sort through the clutter and make sense of what is being said.??Visualization enabled Analytics can be used to identify the trends and key influencers that may not otherwise evident.,Network Graph Analysis can be used to identify the most important participants based on their degree of interaction with other social media users for the topic of interest. ,??can be used to identify the most vertices in a graph which translates into the most influential person in a social network.,For example, in the Twitter network graph (built using Python NetworkX and d3.js) shown here for @Walmart, we see a strong tweet connection with Huggies. Network graphs like the one above can be used to study interactions in social media which may lead to insights that can be useful in the real world.??,Another Visualization Aid worth considering is the World Cloud. ??,??and are best used for Exploratory Qualitative Analysis. They are not appropriate in situations requiring any advanced Text Analytics or to make sense of complex topics. For example, within the context of social media, a good use of a Word Cloud would be to identify the most common words usually associated with the topic being studied to take action if those words no longer appear. Word Clouds??can be used in conjunction with Sentiment Analysis to identify the key influencers and the words most associated with a positive brand image and conversely the keywords and users who have a negative??perception of the business.,When used together Visualization tools like Network Graph Analysis and Word Clouds can form the backbone of a strategy for more effective Social Media engagement.
??,??,??,??,??,??,??,??,??,??,??,??,??,??, ??,??
??
Among the new faces making their debut on the??,??list are a growing number who have made their fortunes from Big Data.,These Big Data billionaires are at the top of their game ?€? all of them have built empires on their ability to collect, interpret and use data in ways no one had thought of before.,Most operate in the mainstream consumer markets ?€? where their focus on improving seemingly mundane aspects of everyday life ?€? hailing a cab, or choosing a movie to watch ?€? has proven hugely successful.,Others operate in specialist markets ?€? combating terrorism and financial fraud by providing experts with previously unavailable insights.,So here?€?s a brief bit about how each one of them has gone about putting big data to work to change the world and turn a profit.,The college dropout CEO of Uber leads the charge of newcomers into the rich list, with a net worth of $5.3 billion.,The mobile app-based service which connects people needing a ride with a network of drivers ready to get them to their destination may have angered traditional taxi drivers around the world, but that hasn?€?t stopped it being valued at $41 billion.,Uber stores and monitors data on every journey its users take, and uses it to determine demand, allocate resources and set fares. The company also carries out in-depth analysis of??,??in the cities it serves, so it can focus coverage in poorly-served areas and provide links to buses and trains.,Recently Kalanick has??,??that the new UberPool service, which analyzes ride data, to set up car sharing arrangements could cut traffic on the streets of London by a third.,After a brief stint in the Marine Corps followed by the Peace Corps, Hastings studied computing at Stanford University before forming his first company Pure Software in 1991. Following a public floatation Hastings was made CEO and went on to form Netflix with Marc Randolph, which started out as a DVD-by-mail business. Today, Reed has an estimate net worth of $1 billion and he also sits on the board of directors of Facebook, another company built on big data.,Netflix has been transformed using big data analytics to??,, as the New York Times put it. The company is analyzing detailed viewing data from their??,??to not only optimize their recommendations but also find the next ?€?House of Cards?€? blockbuster.,Predicting what you will want to watch next is the primary goal of Netflix?€?s data strategy. To do this it has employed teams of movie buffs to scour years?€? worth of film and TV and tag the common themes they contain. It also regularly runs??,??with million-dollar prizes for anyone who can come up with algorithms that more accurately predict audience viewing habits than its existing ones.,The 33-year-old co-founder and CEO of AirB?€?n?€?B is said not to have owned his own home since 2010, almost continuously using his own service, which connects travellers with property owners willing to offer short term lets.,24 million bookings have been made in 34,000 cities since the service launched in 2008, meaning that the company has collected a huge amount of data ?€? estimated at half a petabyte ?€? on people?€?s holiday and traveling habits.,This insight enables them to ensure they concentrate efforts on signing up landlords in popular destinations at peak times, and structure pricing so that the use of their global network of properties is optimized.??,are in place across the network to predict fraudulent transactions before they are processed, and a robust recommendation system allows guests and hosts to rate each other to build trust.,It has also just unveiled??,???€? a user-friendly data analysis platform designed to allow all of its employees, not just those trained in data science, access to all of the company?€?s information, and tools to query it with.,Chesky and his co-founders ?€? Nathan Blecharczyk and Joe Gebbia ?€? are said to be worth $1.9 billion each.,Viola founded the high-frequency trading (HFT) specialist Virtu in 2008, and is currently thought to have a net worth of $1.7 billion. His company developed its own proprietary trading algorithms based on big data principles, to trade equities, commodities, currencies and options on international markets. Plans to take the company which made him the first HFT billionaire public were shelved last year but are thought to be back on the table this year.,HFT, which uses algorithms to make thousands of trades, profiting little but often from tiny movements in price, relies heavily on vast datasets culled from the world financial markets. It is said to account for??,??of all US equities traded, and 60% of futures.,Last year, the company announced that it had only suffered one day of overall trading loss in the previous five years ?€? attributing this success to its real-time risk management strategy and technology.,At 24 and 25, Spiegel and Murphy are the youngest newcomers to the rich list. The two met when they were studying at Stanford University and in 2013 they made headlines by turning down a $3 billion offer from Facebook for their social picture sharing app. This turned out to be a good move as the company last month valued itself at up to??,.,Snapchat started with a gimmick ?€? there were lots of picture messaging services but they were the first to spot that not everyone, for whatever reason, might want the pictures they send to friends to remain visible forever. By implementing an automated self-destruct feature they hoped to build trust. There have been a few??,??but the service has been phenomenally successful.,Since then the product has evolved with the inclusion of data-driven features intended to help users connect with others, and to tell??,??based on shared timelines.,Karp had help from the CIA with building Palantir, which uses big data algorithms to combat terrorists and financial fraud around the world. Its biggest customer is the US Government, which uses its software to predict everything from the placement of roadside bombs to fraudulent transactions.,Much of its work is naturally veiled in secrecy, but it is??,??that its routines for spotting patterns and anomalies in data which indicate suspicious or fraudulent activity are derived from technology developed by Paypal. Karp?€?s co-founder at Palantir is Peter Thiel, who also co-founded the online payment service.,Data from DNA databases, surveillance records showing movements, social media data and tip-offs from informants can be drawn together to predict the activities of bad guys, enabling an appropriate response.,A key philosophy of the company is that human intervention is still needed to get the most from data analysis ?€? particularly when you have to think one step ahead of an enemy. To this end, they provide hand-picked expert consultants to work alongside their clients on data projects.,Initially concentrating on security and anti-terrorism for government clients, Palantir now casts its net much wider and offers its services to the financial and pharmaceutical industries.
Over the past 6 months I have seen the number of big data projects go up significantly and most of the companies I work with are planning to increase their Big Data activities even further over the next 12 months. Many of these initiatives come with high expectations but big data projects are far from fool-proof. In fact, I predict that half of all big data projects will fail to deliver against their expectations.,Failure can happen for many reasons, however there are a few glaring dangers that will cause any big data project to crash and burn. Based on my experience working with companies and organizations of all shapes and sizes, I know these errors are all too frequent. One thing they have in common is they are all caused by a lack of adequate planning.,So, in no particular order, here are some of the most common causes of failure in business big data projects that I?€?ve come across.,It?€?s easy to get caught up in hype ?€? and Big Data has certainly been hyped. When so many people (including me) are shouting about how earth-shatteringly important it is, and anyone not on board is likely to sink, it isn?€?t surprising that a lot of people start with the ?€?how?€? without first considering the ?€?why?€?.,What people who fall into that trap often failed to appreciate is that analytics in business in about problem solving ?€? and first you need to know what problem you are trying to solve.,I worked with an airline which had thrown itself into a range of Big Data projects with great enthusiasm ?€? cataloguing and collecting information on everything from meal preferences to the impact delays would have on drinks orders. Another client ?€? a retailer ?€? had 258 separate data projects on the go when they called me in. Some were interesting ?€? such as by mining all of their stock and purchase data they had found that a particular bottle of wine sold exceptionally well on a Tuesday, and even more so if it was raining. But so what? ??The issue is that shelf-space to pre-assigned and can?€?t be increased for this brand for just this one day. The only option is to ensure the allocated shelf-space is regularly restocked on Tuesdays.??In isolation that insight isn?€?t going to provide them with huge growth or positive change.,Sometimes you will get lucky and hit on an interesting insight taking this approach, but it?€?s highly inefficient. In fact it?€?s a bit like sitting an exam and not bothering to read the question, simply writing out everything you know on the subject and hoping it will include the information the examiner is looking for.,A lot of people go into Big Data with a ?€?me too!?€? attitude. The barriers to entry are constantly dropping, which is a great thing ?€? open source software is becoming more accessible all the time, and there are an ever-growing number of ?€?software-as-a-service?€? companies which often hugely cut down the need for infrastructure investment. On top of that, people like me are always saying that if you aren?€?t in, you?€?re going to get left behind!,Well, that?€?s all true ?€? but you still need to establish why there is a particular need for your business to allocate time and resources to it. However much data you collect (but particularly if it?€?s big) you will need to make sure it is kept clean and secure and there will be ongoing costs associated with this that might not be anticipated. In short, you need to know why your business needs to use Big Data before you start doing it. If you don?€?t ?€? wait until you do.,I admit, this is something of a catch-all and can affect any kind of business initiative. But in a time and resource-intensive Big Data initiative (skilled data scientists generally expect to be paid at least $100,000 a year) management failure can have disastrous consequences.,Sometimes it?€?s because those holding the purse strings haven?€?t taken into account some long-term or ongoing cost associated with the project, or sometimes the senior project managers just can?€?t talk productively to the data scientist workers in the lab. Sometimes it will be because senior managers don?€?t trust the algorithms ?€? many got where they are today on gut instinct ?€? and they aren?€?t going to start letting a computer tell them what to do now.,To say the world of Big Data is made up of egghead scientists and corporate suits with dollar signs in their eyes is to use stereotypes which are offensive to both groups ?€? but it works to illustrate the problem here. Big Data in business is about the interface between the analytical, experimental science that goes on in data labs, and the profit and target chasing sales force and boardroom. They are not always natural bedfellows and things can get lost in translation ?€? with sometimes tragic consequences.,For me, the Space Shuttle Challenger disaster serves as an example ?€? it may have been well before the term Big Data was coined but the analysts at NASA were dealing with very large amounts of information for the time ?€? monitoring sensors equipped throughout the shuttle. Their reports to the higher-ups at mission control went into a great amount of detail and included information that would have shown the significant risks of the shuttle breaking up ?€? if the mission controllers had been able to spot it among the superfluous data. If they had been presented with a document titled ?€?The shuttle is likely to crash because ?€??€? things would have ended far more happily.,I feel that lessons have not yet been learnt. Those with responsibility for reporting need to think ?€?who is this data for, and how can I package it to make sure the message gets through??€? Analysts with one of my clients ?€? a healthcare company ?€? recently created a report for senior management which was 217 pages long. By replacing much of the text with infographics, we cut it down to 15 pages which still contained all the essential information but presented it far more clearly.,Or just as fatally, not having the right skills at the right time. As I?€?ve explained in the examples here, companies are often fond of starting up data projects ?€?left, right and centre?€? without thinking enough about how this might impact resources in the future. And skilled data science staff are certainly a very valuable resource. Having started a project with no clearly defined aim, in my experience businesses will often come unstuck when they do come across a valid opportunity for meaningful analysis and find their skilled staff otherwise engaged.,Data science staff are expensive and in??,. Companies can benefit here from some out-of-the-box thinking ?€? for example one of my banking client told me that while they have a lot of business analysts, they aren?€?t trained in Big Data and aren?€?t really data scientists. As there is a huge crossover in skills between the disciplines, I suggested that offering their existing staff specific Big Data training would almost certainly be cheaper than hiring in a whole new team of specialists. We identified the key skills gaps and developed a customised course to move people from business analyst to big data scientists. We also complemented the course with the many online resources where anyone can??,. In addition to the training, the bank looked to universities and colleges which often offer the service of students or academics to provide analytical support to businesses. Today, the bank also sponsors a number of PhD students that are using their business?€?s own data for their study.,Mismanagement can come from many angles and the UK National??,??Service?€?s??,??is a prime example. The plan to bring all patient medical records into a central database was described as the ?€?biggest IT failure ever seen?€? and was scrapped after more than????10 billion ($14.9 billion) had been spent.,But what's your view of on why big data projects fail? Please share your thoughts in the comments below.
Nice infographics by DataCamp. ,??to view the original and commented version.??
Advertisers always search for better deals. The idea behind making marketing strategy work out effectively is to advertise products and services of a company to potential customers efficiently. The ultimate goal of all marketing efforts is to cover all spaces and surfaces with advertising. This has resulted in the development of mobile LED screens.,Due to limitations on static advertising in cities advertisers are seeking new ways of communicating their services to targeted audience. Placing static outdoor signs in any location costs money to advertising company along with taxes afterwards. Mobile LED trucks and trailers provide businesses a unique opportunity to reach larger target area very quickly. Check out the advantages of mobile LED advertising.,The ability to move your advertisements to the location where your customers can view is important for your business. Mobile LED screen trucks, vans or trailers help you promote your business in popular locations in order to gain maximum exposure towards it and increase your brand publicity efforts. Competitive advertising market demands innovation and mobile LED advertising trailers are the most effective way to hit the advertising world.
I?€?m attending??,??on Wednesdays this semester and I?€?m planning to blog the class. Here?€?s what happened yesterday at the first meeting.,Rachel started by going through the syllabus. Here were her main points:,So, what is data science? Is data science new? Is it real? What is it?,This is an ongoing discussion, but??,??is pretty good:,Data science, as it?€?s practiced, is a blend of Red-Bull-fueled hacking and espresso-inspired statistics.,But data science is not merely hacking, because when hackers finish debugging their Bash one-liners and Pig scripts, few care about non-Euclidean distance metrics.,And data science is not merely statistics, because when statisticians finish theorizing the perfect model, few could read a ^A delimited file into R if their job depended on it.,Data science is the civil engineering of data.?? Its acolytes possess a practical knowledge of tools & materials, coupled with a theoretical understanding of what?€?s possible.,Driscoll also refers to??,??from 2010.,We also may want to look at??,:,But wait, is data science a bag of tricks? Or is it just the logical extension of other fields like statistics and machine learning?,.
 , , , , , ,Interested in similar content???
 , , , ,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??
A very warm welcome back to all here in Data Science Central. I decided to post today given that a friend in a common Social network shared with me one , that I thought to be in the interest of the community of good and responsible Data Scientists, as it were.,It concerns a blog post from Quantopian, which is an interesting new crowd-sourced investing platform vendor, a new breed that is emerging in the field of Quantitative Finance and Fund Management. In a somewhat academic but certainly informed pragmatic way the post delves in some of the intricacies of Data Science topics like the relationship between Models and Inference, the problems of how to fit the data in the Model, etc. It was nice reading specially coming from a field that is Data intensive, even if not that much sharing of knowledge intensive, but that is another story. I hope the readers will like and.... well share it.,Here are some excerpts and images:,The link to the video in the excerpt is a recommendation to learn more about Probabilistic Programming as it can be of great help in dealing with Data Science issues and its problems. Well worth a view.,Photo:
The UK?€?s general election took place last week, on Thursday 7 May, 2015. It was an election that had been hyped for being ?€?too close to call?€?. According to the polls, the government was likely to be a coalition of one or more, with no party achieving a majority. It could have gone either way.,Imagine the shock when the BBC announced the exit poll results: a landslide victory for a single party ?€? the Conservatives.,Election polling companies are reliant on various types of data to come up with accurate predictions. Like any business, they must apply quality control to their data. They must cleanse it,??,, and ensure their contact records are up to date. They need to ensure they don?€?t call the same person twice, and they must encourage people to give accurate data in response.,How could so many companies get it so badly wrong? And was the data at fault, or was there another gremlin in the machine?,This is not the first time that polling data has let down the public, politicians and press.,During the US election in 2012, opinion polls predicted a tough campaign for President Obama. He wound up with a comfortable majority. And in 1992, there was a direct comparison with last week?€?s data disaster. A close race was predicted; the Conservatives won comfortably then, too.,Even this year, in March, polling companies in Israel underestimated support for Netanyahu?€?s Likud party. He was, in the end, a clear winner.,Even though polls are not binding, they influence voters in the run up to an election, and can even influence the policies that parties formulate as they seek to capture the mood of the electorate. It?€?s therefore critical that polls can be relied upon. And that makes it a matter of data quality.,Election polling companies get their data from a variety of sources. YouGov has posted an excellent blog??,.,In brief, YouGov (and similar organisations) collect data by determining preference for a particular party. Responses are gathered online, and over the phone.,Initially, it?€?s tempting to think that perhaps different voters have different ways of answering polls. But YouGov adjusts for this already. Labour voters are more likely to use online methods, and Conservative voters the telephone. But it says the data being collected was the same via each method. So we can rule out ?€?mode effects?€? based on this.,Another culprit is a change in turnout. The polling companies take a small data sample and extrapolate the results, based on the size of the actual sample. So they could be filling in blanks in the wrong way.,YouGov says that there may be ?€?methodological failure?€? in the way these polls are being conducted.,Interviewers may be asking questions in a loaded way, or influencing the answers by their mere presence.,In the US, polling companies are legally required to manually dial mobile telephone numbers. They do not have to manually dial landlines. This has lead to the landline being favoured, and it?€?s possible that ?€? culturally ?€? this habit has been persistent in the UK, too. Due to automated calls and marketing campaigns ?€? both relatively new phenomena ?€? some of us view landline telephone calls with suspicion. We want to get off the phone as quickly as possible, so perhaps the data we give is hurried, or we say what we think we should say to get it over with.,They may have asked the wrong questions: how people would vote on local issues, for example, rather than which leader or party was favourable at a national level.,But there may be a more mundane reason. When collecting data, you have to assume that the data is being provided truthfully and accurately. If someone gives you their email address, you need to trust that it will be genuine.,It may be that some people did not supply accurate data to the polling company in the first place: they ?€?said one thing and did another?€?. This is what Peter Kellner, president of YouGov, thinks is most likely to be the problem. Marketers face this problem all the time: people provide fake information in order to avoid being added to marketing lists. Could it be that people are just less inclined to tell the truth?,As??,??we have a saying. ?€?Junk in ?€? junk out.?€?,Whether it?€?s a company balance sheet, a marketing report or an election poll, the outputs that are generated are only ever going to be as good as the data that was used to build them. This is why data quality is so critical to success and profitability.,Think of it in simple terms. If you compile a list of 1,000 contacts and send a marketing letter to all of them, you need a clean list: one free of mistakes, duplicates, misspellings and ?€?gone away?€? records.,If you get 500 letters ?€?returned to sender?€?, you can safely assume that your mailing list is seriously decayed. Not only is that inconvenient, but it?€?s effectively doubled the cost of your campaign: 50% of the effort was wasted.,For polling companies, their entire business model is based on obtaining accurate data, and purifying data to ensure they have a reliable snapshot. They need to take small, frequent samples via polling to gain an understanding of wider trends. This means mistakes are going to be amplified, as they clearly were last week. Now, more than ever, data accuracy is the number one goal.,If telephone polling is less reliable, companies now face a new era, where response rates and accuracy need to be higher. The British Polling Council is conducting an enquiry into the reasons behind last week?€?s failure. Data quality will undoubtedly be placed under the spotlight. Without it, we may never fully trust election polls again.
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
In my experience at??startups and large companies, good analytics often boils down to the availability of organized data to answer business??questions. This is especially important for digital marketers, with the audience data from many channels pouring in and the need to stay on top of key metrics.,Seemingly simple questions can spin up the entire MarTech engineering team!,?€?If I increase my spend on display ads retargeting by 20%, for middle of the funnel prospects, what can I expect for conversion lift? Is??this different by gender, geo or behavioral attributes( highly engaged vs low potentials)?,To answer this question, the analyst has to look at historical data of similar campaigns and conversions. The customer journey by time and event attributes has to be organized so that??a data scientist does not have to spend weeks and months to get it into shape. It boils down to data organization. Without proper information??models and organization it is impossible to understand cause and effect, and the best algorithms are not going to be of much help.,Collecting and organizing data with an eye on the key drivers of user acquisition and retention is hard. Because this is hard, the task of organizing user journey data is left to engineers/IT and soon the cart is before the horse, frustrating everyone.,So what can CMOs and marketers and analysts working in marketing and user behavior analysis do?,The foundation for analytics on user behavior is smart data management, taking as inputs raw events from user activity and converting them to analyzable datasets. Marketers should own the task of describing their data needs, add context to the data and drive analytics templates with minimal IT/Engineering help. The right set of tools, that enable marketing data scientists to be self sufficient, are critical.,For example ?€? In order to fully understand customer journey and the implications of various marketing medium spend on eventual conversion and LTV( Life Time Value), a foundation of first party audience data is critical.,1. Can you micro-segment your audience based on first party behavioral data( how they interact with our website, store, mobile app) and combine 2nd and 3rd party data when needed? Requires modeling all events and attributes from all customer touch points on web, mobile and other offline data., 2. Can you track all referrers for each session your prospect is engaged with you, so you can model and attribute? Requires encoding all campaigns and track them for each session., 3. Can you model a customer journey with time (some segments convert in 1 session, some across 4 or 5 sessions spanning multiple??days). Requires maintaining a path of all the customer activity across sessions.,Can??all this be done without deploying an army of engineers, and is it repeatable or a one time effort?,In this age of the tech savvy CMO teams, its vital for marketers and marketing data scientists to hold and gain expertise in the domain modeling exercise with the right platform and unburden IT and Engineering of this task. The CMO /CIO combination works well when marketing data scientists drive the data strategy and engineers drive the tech stack, scalability and general infrastructure needs.
The 3Vs model is the foundation of big data - Volume, Velocity, and Variety. It is used to express the key features of big data problems - for me, this is about to change. Big Data is not just about size, speed, or formats, the contextual enrichment is the most critical factor of how we unmask the best value out of data. How well you bring seemingly unrelated data together and identify the valuable connections determines how much power you unleash from your data.??,People tend to make connections between objects by instinct, and the instinct is backed by past experiences. Great ideas are often generated by identifying unusual and unique linkages. For example, Charles Darwin and Alfred Wallace associated the notion of overpopulation and their observation of how species differentiate among themselves to deduce the theory of evolution. Another example is how Walmart predicts the boost in sales of strawberry Pop-Tart upon the arrival of a hurricane based on its observation in historical sales data, compared to competitors who stock flashlights by instinct instead of using data.??,The ability to make cross-connections is the key to turning information into knowledge. Calculating the connection between any two data elements however, is an extremely complex task. This is the driver of BigObject Analytics development of what we call ?€?Cross-Link.?€???,Imagine: you have a blog and you cover a variety of topics. As an author, the more page views, the higher your sense of achievement and the better you are motivated. Here is the thing: how do you increase the engagement of your readers? You can easily get a basic sense of user behaviors from the web analysis, but what can be more interesting is to spot the hidden factors behind these logging sessions. You can import the weather data, for example, to see if readers?€? interest varies under different weather conditions, calculate the correlations among the posts, and moreover, see if the correlation differs in different regions, by different genders, or over time.,This type of correlation analysis normally requires heavy table joins in a database system. When the data size doubles, the complexity of the computation can become even more complicated and time consuming. While more and more data is collected today, and multiple-dimensioned data is managed simultaneously, we need an efficient way to harness the complexity of cross-domain datasets.??,The trend of ?€?quantified self?€? is a substantial driver for this type of analysis. While smartphones today make-up plenty of data sources, it can best demonstrate how putting all this data together can assist individuals in every facet of life. An agile data analytic engine empowers the application to maneuver data analysis across different dimensions and provide profound insights. The result is the emergence of intelligence.,We have developed BigObject Analytics based on the philosophy of Cross-Link. We present it in a simple interface: BigObject Shell. With BigObject Shell you can unleash the tremendous power of data via elegant statements. The BigObject Analytics docker image is currently freely distributed.??,References:,?€?,??€? MIT Technology Review, October 20, 2014,<Thinking, Fast and Slow> Daniel Kahneman,?€?,?€? The New York Times, November 14, 2004,??-- It's The Haystack?€? Forbes, January 20, 2015
When I talk about "the institutional response," I am referring to an increasingly common occurrence: a standardized or large-scale approach is supported, promoted, and applied by a particular institution - sometimes governmental in nature - premised on its apparent suitability or superiority to achieve desirable outcomes. I suspect that in recent years, there has been a push to get citizens to file their income tax returns electronically. I know that in Canada, it has become difficult to find any contact information to speak with a live person. For an elderly individual unfamiliar with the internet, somebody who is visually-impaired, or anyone who can't get through the processes developed by system designers, it might be a rough ride. In this blog, I will be focusing on how an institutional response can in principle be data-deprived. ??I don't mean that such as response is brought about without data - sometimes there can be a great deal - but rather that that post-response data collection is not necessarily given any emphasis. ??A lot can go wrong. Yet, due to lack of data collection, those responsible for the system might be unaware of faults in the process - except in relation to metrics anticipated in early development. Writing from my own experience, I noticed that the technical support line to our tax service is busy right up to midnight at which point the support line closes. ??It can be quite difficult to advise the tax service of problems under such conditions.,I said that institutional responses are becoming increasingly common because it represents a cheap and simple approach to complex problems. I appreciate the need to save money and keep life simple. In my prior blog, I wrote about ,??- intended to guide a municipality during an emergency - in search of keywords indicating sensitivity to vulnerable groups. I offered a conceptual reason for doing so. Setting aside the merits of my conceptualization, I also point out that there is a recognized need to consider such individuals in emergencies: e.g. A.6.8.2 of ,. My data scanning is premised on the certain measurable commonalities existing in these plans regardless of the filer. Assuming that there are substantial commonalities, a municipality might be tempted take a ,: without necessarily collecting much data or having detailed discussions, it could copy and customize the plans that others have filed. Since the requirement to file is institutional - as is the submission - I cannot say that such an approach would necessarily lead to complaints. Parts of the submission might be confusing, nonsensical, or out of place. For example, not all municipalities are necessarily located near a nuclear power facility or hydro electric station. The ,??differs from place to place.,In Ontario, there is a requirement for ,. Perhaps the situation is similar in other jurisdictions. The requirement is also covered in Chapter 8 of NFPA 1600. It would be difficult for these exercises and tests not to generate data. There is likely some recognition that the planning process and the plan itself represent only starting points - a beginning rather than end. I know that there might be some debate as to how effectively data gets collected during exercises and in what form. Nonetheless, there is a great opportunity to collect data; and doing so can contribute to improvements to the plan. I do not dismiss the institutional response - assuming that it sensitizes rather than insulates structural capital. ,??to maintain processes in a company even as its employees change. I notice that others sometimes associate it with policies and procedures. I normally use the term to refer to systems of organizational learning.,I personally can understand how, amid the fast-paced and uncontrolled circumstances of an actual emergency, it can be difficult to collect data. The system in place might not be designed to gather data on-the-fly - although really, it should be. It is probably much easier to collect data during an exercise or simulation. For me, the whole point of the undertaking is to gather information - to report and share findings - to help bring about improvement to the plan. In the process, the amount of learning that occurs among participants is likely rather high. I have often speculated - going off on a tangent for just a moment - how the animated nature of infants and unstable nature of adolescents might represent an evolutionary tactic to deal with high vulnerability in the absence of data. There is this situation of high vulnerability and change in the face of all sorts of hazards and dangers in the environment. It is necessary to conceptualize and question existing presumptions really quickly. Certainly, for an organization facing change, perhaps one route to failure is through the implementation of an institutional response that is inherently insulated from both external and internal environments (external being the market - internal being operations). The difference is in the data. Data collection and responding to that data might help organizations achieve worthwhile outcomes much better than simply introducing a "response" - regardless of how authoritative or deliberated that initial plan might be.,I wrote my undergraduate thesis on the effectiveness of public participation in local planning. The important word here is "effectiveness." There is a requirement to give residents an opportunity to participate in developments. The process of development is highly institutionalized. Yet there is this idea that all sorts of data must to be collected not just relating to construction but also the sentiments of those living in the area. Opposition sometimes exists, but of course all sorts of developments still get approved. Participation does not necessarily interfere with development. However, I have discovered that it is possible to have participation in a process where it doesn't actually occur in a tangible sense. The meeting could exist purely to fulfill an institutional requirement.,Apart from cheapness and simplicity, perhaps another reason for the emergence of the institutional response relates to the prevalence of professionals in business. How often does real research get conducted - gathering real data - in a business setting? I find that business solutions are becoming increasingly institutional in nature. Consider a response that takes almost no data: in order to deal with rising costs, cut costs. Some might call this a knee-jerk reaction. ??Never mind about the possibility that specific investments might help to maintain access to particular markets or to competitive advantage. Growth metrics don't necessarily get along with austerity metrics. The need to be sensitive the environment might seem less important if an organization is focused on implementing a plan. I have noticed that, in the face of uncertainty, rather than collect data, decisions might be guided by certain professional biases disguised as competencies. I know that among data scientists, there is sometimes this discussion on the relative merits of evidence-based decision-making versus the use of gut instincts. Well, sometimes a gut instinct is really a professional bias. We give the ,??too much credit.,I started off this blog discussing the push for people to file and handle their tax returns online. I should clarify that I prefer and enjoy doing my taxes electronically. Combining my preference with the underlying rationale - probably to cut costs - I don't question the initial decision. But any mass-service system has to have a means to test for operational problems. This is quite difficult to do without a method of collecting data "outside" the expected parameters. If all problems fell within parameters, these wouldn't really be problems but merely challenging aspects of routine operations. Invariably, there must first of all be a data system to catch the errors, faults, malfunctions, complaints, outages, and difficulties; then there have to be metrics or indicators of reasonable diversity to warn of peculiarities beyond those anticipated during design. The more heavily we rely on institutional responses, the greater the need to incorporate safety, security, and service mechanisms. A response is poor substitute for responsiveness. A plan that has no data to support adaptation and change probably will not do so. There must always be sensitivity in every institutional process; this cannot be achieved without collecting and making use of substantial amounts of data.
The last few years I have been trying to get an handle on the field which encompasses ??analytics , big data, modeling, prediction, machine learning, algorithms , data mining techniques, rules, computational complexity, latency, data products, data engineering, statistical inference, R programming, data wrangling, data hacking, statistical modeling, supervised / unsupervised learning, data visualization, unstructured data and many other subjects that make the world of ??Data Science. Someone said that the term itself is an umbrella term. There are many books out there which would deal with great detail on some of the topics mentioned above, written by their respective masters.,The ??moment I picked the??book..."The Handbook of Data Science" I sensed that this would be different and profoundly impact me. ??I must confess that, my??understanding of this beautiful art/science of drawing insights from data??has gone up to a whole new level.,Data science has been there and??practiced?? much before the term was coined by , and ,r and there are some serious practitioners who have been flag bearers of this art for a ??long time, but if you want to??know about ??the transformation of this discipline into what it has evolved ??into presently ??and where it is headed and the impact it has created for itself on almost every field and who are the people behind this , what is there background, where they come from and how they have impacted and continue to take this field into almost a cult status , to the extent that the Obama government has appointed its own , the first ever chief data scientist, then this??book??is for you. It has as many as 25 stalwarts-- including some real??rock stars, who have rocked the boat having contributed significantly to the art of data science and have single handedly turned their organizations on its head into great success??stories-- ??who have been interviewed by some very intelligent data scientists , who themselves ardently seeking to know the nuances of this art, have done a good job in getting the best from these elite artistes.,The??book??has many versions of definitions of data science like the one from Josh Wills and the conversations carry on from there and the transitions they make from their rich academic backgrounds to the real world where they hone their skills practicing the craft of seducing the information, insights, and the signal ??out from ??complex, unyielding and noisy datasets and creating data products that can capture data from a captive audience and in the process building rich data sources for further analysis.,??Every artiste in this??book??has got his/her own view??on the ??definition ??of data science but broadly speaking they seem to agree on the convergence of the ??fields of Math/Statistics, computer science and domain expertise.,The target ??audience for this??book??ideally can be the following,As I have already said the ??25 artistes themselves come from varying disciplines and there cannot be a better representation of the different backgrounds than the list comprises of. Having said that you are ??treated to some deep diving sessions and one can only marvel at the brilliance of these artistes performing seamlessly. There is something for everyone right from the aspiring data scientist to the elite or just the curious or the connoisseur.,The??book??also courses through an important topic of learning and upgrading the skills required for practicing data science. Making a transition from a purely academic background to the real world of business . Organizations like??,??is specifically designed for helping PhD's transition into industry. At the other end of the spectrum, aspiring data scientists, who ??have enough domain expertise and are keen to pursue this art can take umbrage from the example of??,??who has embarked on a self crafted journey to embrace the art of data science purely on online learning MOOCs. In Fact she has herself come out with a curriculum for data science with the Open Source Data Science Masters--OSDSM- program. These courses can help you to bridge the gap in your learning and practicing the craft.,The OSDSM is a collection of open source resources that will help you to acquire skills ??necessary to be a competent entry level data scientist. You can access the curriculum??,??.,You have to be adept at learning and upgrading on the job and on the fly.??,s ??talks about this aspect when he says..??,.,??who joined the company after listening to the TED talk of Salman , ??had a ??background of finance in the field of high frequency trading adds to the discussion on ??learning new skills and crossing the learning curve of knowing more about data science on the job...,Finally??,??who teaches Statistics hits the nail on its head when he observes that??,....,The metrics for data science depends upon the problem that your customer wants you to solve. As??,, says..??,. She says that in her role, success is very measureable , It also depends on what questions you ask or try to find the right questions and try to make an impact with the answer.,The??book??also touches upon the value of industry and hard work and discipline. You have to be prepared to put in long hours of hard work not only in bridging the gaps in understanding of data science , ??certain missing links in your armory but also with the problem in hand which you are trying to decipher.??,, who lead a team of data scientists in LinkedIn and since then has gone on to become the??,--talks about the same very eloquently, when he says ..,In the??book??there is also the emphasizing of the narration of a problem and the ability to communicate the solution in the form of a story without losing the feeling of passion and curiosity. This is equally important along with the usual skills.??,??the ??New York based scientist and founder of??,??says.., ,She has very valuable advice for the aspiring data scientists..,??makes an important point when he talks about,The rock stars themselves....As I have already mentioned in this post you will find the list a heady mix of who is who in the field., , ,The authors themselves have a interesting and varied background, which has made this book that much more special. They have certainly brought the best out of the data scientists for the benefit of the entire race of this field.??,The diversity of the backgrounds ??of all these artistes is what makes it very interesting- academic, career or domain wise, but still something that ties them all is curiosity and the hunger to satisfy that famished state. These artistes make you think and contemplate.??,Why is data science so important in today?€?s world and economy?,Apart from the above rock stars , you can additionally follow the following Grand Masters who have not been featured in the??book??but are also equally working hard, untiringly for the growth of this industry. You can ??just Google and follow them through twitter or their websites..,You will never ??miss out on their rays of insight ??and the ??sprinkle of stardust on you.,If still your hunger has not satiated then you can follow the??,????of top 50/100 influencers and brands in the industry that will surely get you going.,Finally I leave you with another gem from the,This time it is from 
 , , , , , , , , , , , , , , , , , , , , , 
??,??

This article is a part of an evolving theme. Here, I explain the basics of Deep Learning and how Deep learning algorithms could apply to IoT and Smart city domains. Specifically, as I discuss below, I am interested in complementing Deep learning algorithms using IoT datasets.??,. I will also present these ideas at??,??and the??,Please connect with me if you want to stay in touch on??,??and for future updates,Deep learning is often thought of as a set of algorithms that ?€?mimics the brain?€?. A more accurate description would be an algorithm that ?€?learns in layers?€?. Deep learning involves learning through layers which allows a computer to build a hierarchy of complex concepts out of simpler concepts.,The obscure world of deep learning algorithms came into public limelight when Google researchers??fed 10 million random, unlabeled images from YouTube into their experimental Deep Learning system. They then instructed the system to recognize the basic elements of a picture and how these elements fit together. The system comprising 16,000 CPUs was able to identify images that shared similar characteristics (such as images of Cats). This canonical experiment showed the potential of Deep learning algorithms. Deep learning algorithms apply to many areas including Computer Vision, Image recognition, pattern recognition, speech recognition, behaviour recognition etc,??,To understand the significance of Deep Learning algorithms, it?€?s important to understand how Computers think and learn. Since the early days, researchers have attempted to create computers that think. Until recently, this effort has been rules based adopting a ?€?top down?€? approach. The Top-down approach involved writing enough rules for all possible circumstances. ??But this approach is obviously limited by the number of rules and by its finite rules base.,To overcome these limitations, a bottom-up approach was proposed. The idea here is to learn from experience. The experience was provided by ?€?labelled data?€?. Labelled data is fed to a system and the system is trained based on the responses. This approach works for applications like Spam filtering. However, most data (pictures, video feeds, sounds, etc.) is not labelled and if it is, it?€?s not labelled well.,The other issue is in handling problem domains which are not finite. For example, the problem domain in chess is complex but finite because there are a finite number of primitives (32 chess pieces) ??and a finite set of allowable actions(on 64 squares). ??But in real life, at any instant, we have potentially a large number or infinite alternatives. The problem domain is thus very large.,A problem like playing chess can be ?€?described?€? to a computer by a set of formal rules. ??In contrast, many real world problems are easily understood by people (intuitive) but not easy to describe (represent) to a Computer (unlike Chess). Examples of such intuitive problems include recognizing words or faces in an image. Such problems are hard to describe to a Computer because the problem domain is not finite. Thus, the problem description suffers from the??,??i.e. when the number of dimensions increase, the volume of the space increases so fast that the available data becomes sparse. Computers cannot be trained on sparse data. Such scenarios are not easy to describe because there is not enough data to adequately represent combinations represented by the dimensions. Nevertheless, such ?€?infinite choice?€? problems are common in daily life.,Deep learning is involved with ?€?hard/intuitive?€? problem which have little/no rules and high dimensionality. Here, the system must learn to cope with unforeseen circumstances without knowing the Rules in advance. Many existing systems like Siri?€?s speech recognition and Facebook?€?s face recognition work on these principles.?? Deep learning systems are possible to implement now because of three reasons: High CPU power, Better Algorithms and the availability of more data. Over the next few years, these factors will lead to more applications of Deep learning systems.,Deep Learning algorithms are modelled on the workings of the Brain. The Brain may be thought of as a massively parallel analog computer which contains about 10^10 simple processors (neurons) ?€? each of which require a few milliseconds to respond to input. To model the workings of the brain, in theory, each neuron could be designed as a small electronic device which has a transfer function similar to a biological neuron. We could then connect each neuron to many other neurons to imitate the workings of the Brain. In practise,?? it turns out that this model is not easy to implement and is difficult to train.,So, we make some simplifications in the model mimicking the brain. The resultant neural network is called??,. ??The simplifications/constraints are: We change the connectivity between the neurons so that they are in distinct layers. Each neuron in one layer is connected to every neuron in the next layer. Signals flow in only one direction. And finally, we simplify the neuron design to ?€?fire?€? based on simple, weight driven inputs from other neurons. Such a simplified network (feed-forward neural network model) is more practical to build and use.,Thus:,a)?????????? Each neuron receives a signal from the neurons in the previous layer,b)?????????? Each of those signals is multiplied by a weight value.,c)?????????? The weighted inputs are summed, and passed through a limiting function which scales the output to a fixed range of values.,d)?????????? The output of the limiter is then broadcast to all of the neurons in the next layer.,Image and parts of description in this section adapted from :??,The most common learning algorithm for artificial neural networks is called??,??which stands for ?€?backward propagation of errors?€?. To use the neural network, we apply the input values to the first layer, allow the signals to propagate through the network and read the output. A BP network??learns by example i.e. we must provide a learning set that consists of some input examples and the known correct output for each case. So, we use these input-output examples to show the network what type of behaviour is expected. The BP algorithm allows the network to adapt by adjusting the weights by propagating the error value backwards through the network. Each link between neurons has a unique weighting value. The ?€?intelligence?€? of the network lies in the values of the weights. With each iteration of the errors flowing backwards, the weights are adjusted. The whole process is repeated for each of the example cases. Thus, to detect an Object, Programmers would train a neural network by rapidly sending across many digitized versions of data (for example, images) ??containing those objects. If the network did not accurately recognize a particular pattern,?? the weights would be adjusted. The eventual goal of this training is to get the network to consistently recognize the patterns that we recognize (ex Cats).,The whole objective of Deep Learning is to solve ?€?intuitive?€? problems i.e. problems characterized by High dimensionality and no rules.?? The above mechanism demonstrates a supervised learning algorithm based on a limited modelling of Neurons ?€? but we need to understand more.,:,This is similar to the way a child learns ?€?what a dog is?€? i.e. by understanding the sub-components of a concept ex?? the behavior(barking), shape of the head, the tail, the fur etc and then putting these concepts in one bigger idea i.e. the Dog itself.,The (knowledge) representation problem is a recurring theme in Computer Science.,??incorporates theories from psychology which look to understand how humans solve problems and represent knowledge. ??The idea is that: if like humans, Computers were to gather knowledge from experience, it avoids the need for human operators to formally specify all of the knowledge that the computer needs to solve a problem.,For a computer, the choice of representation has an enormous effect on the performance of machine learning algorithms. For example, based on the sound pitch, it is possible to know if the speaker is a man, woman or child. However, for many applications, it is not easy to know what set of features represent the information accurately. For example, to detect pictures of cars in images, a wheel may be circular in shape ?€? but actual pictures of wheels may have variants (spokes, metal parts etc). So, the idea of representation learning is to find both the mapping and the representation.,If we can find representations and their mappings automatically (i.e. without human intervention), we have a flexible design to solve intuitive problems. ????We can adapt to new tasks and we can even infer new insights without observation. For example, based on the pitch of the sound ?€? we can infer an accent and hence a nationality. The mechanism is self learning. Deep learning applications are best suited for situations which involve large amounts of data and complex relationships between different parameters. Training a Neural network involves repeatedly showing it that: ?€?Given an input, this is the correct output?€?. If this is done enough times, a sufficiently trained network will mimic the function you are simulating. It will also ignore inputs that are irrelevant to the solution. Conversely, it will fail to converge on a solution if you leave out critical inputs. This model can be applied to many scenarios as we see below in a simplified example.,Deep learning involves learning through layers which allows a computer to build a hierarchy of complex concepts out of simpler concepts. This approach works for subjective and intuitive problems which are difficult to articulate.,Consider image data. Computers cannot understand the meaning of a collection of pixels. Mappings from a collection of pixels to a complex Object are complicated.,With deep learning, the problem is broken down into a series of hierarchical mappings ?€? with each mapping described by a specific layer.,The input (representing the variables we actually observe) is presented at the visible layer. Then a series of hidden layers extracts increasingly abstract features from the input with each layer concerned with a specific mapping. However, note that this process is not pre defined i.e. we do not specify what the layers select,For example: From the pixels, the first hidden layer identifies the edges,From the edges, the second hidden layer identifies the corners and contours,From the corners and contours, the third hidden layer identifies the parts of objects,Finally, from the parts of objects, the fourth hidden layer identifies whole objects,Image and example source:??,To recap:,In addition, we have limitations in the technology. For instance, we have a long way to go before a Deep learning system can figure out that you are sad because your cat died(although it seems??,??is heading in that direction). The current focus is more on identifying photos,??,(based on??,),And we have indeed a way to go as??,Today, we are still limited by technology from achieving scale. Google?€?s neural network that identified cats had 16,000 nodes. In contrast, a human brain has an estimated 100 billion neurons!,Now, extending more deeply into the research domain, here are some areas of interest that I am following.,In essence, these techniques/strategies??,1)????????????,??: Time series data (coming from sensors) can be thought of as??,, and image data can be thought of as a 2D grid of pixels. This allows us to model Time series data with Deep learning algorithms (most sensor / IoT data is time series). ??It is relatively less common to explore Deep learning and Time series ?€? but there are some instances of this approach already (,??),2)????????????,??,. Multimodality in??deep??learning algorithms is being explored?? In particular, cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time,3)????????????,In their??,??from Carnegie Mellon University?€?s Department of Electrical and Computer Engineering, propose a new way to identify the intrinsic dynamics of interaction patterns at multiple time scales. Their method involves building a deep-learning model that consists of multiple levels; each level captures the relevant patterns of a specific temporal scale. The newly proposed model can be also used to explain the possible ways in which short-term patterns relate to the long-term patterns. For example, it becomes possible to describe how a long-term pattern in Twitter can be sustained and enhanced by a sequence of short-term patterns, including characteristics like popularity, stickiness, contagiousness, and interactivity. The paper can be downloaded??,I see Smart cities as an application domain for Internet of Things. Many definitions exist for Smart cities/future cities. From our perspective, Smart cities refer to the use of digital technologies to enhance performance and wellbeing, to reduce costs and resource consumption, and to engage more effectively and actively with its citizens (adapted from Wikipedia). Key ?€?smart?€? sectors include transport, energy, health care, water and waste. A more comprehensive??,??are: Intelligent transport systems ?€? Automatic vehicle , Medical and Healthcare, Environment , Waste management , Air quality , Water quality, Accident and ??Emergency services, Energy including renewable, Intelligent transport systems?? including autonomous vehicles. In all these areas we could find applications to which we could add an intuitive component based on the ideas above.,Typical domains will include Computer Vision, Image recognition,??pattern recognition, speech recognition, behaviour recognition. Of special interest are new areas such as the Self driving cars ?€? ex the,??and even??,Deep learning involves learning through layers which allows a computer to build a hierarchy of complex concepts out of simpler concepts. Deep learning is used to address intuitive applications with high dimensionality. ??It is an emerging field and over the next few years, due to advances in technology, we are likely to see many more applications in the Deep learning space. I am specifically interested in how IoT datasets can be used to complement deep learning algorithms. This is an emerging area with some examples shown above. I believe that it will have widespread applications, many of which we have not fully explored(as in the Smart city examples),I see this article as part of an evolving theme. Future updates will explore how Deep learning algorithms could apply to IoT and Smart city domains. Also, I am interested in complementing Deep learning algorithms using IoT datasets.,??I will also present these ideas at??,??and the??,Please connect with me if you want to stay in touch on??,??and for future updates
Coming up with a topic for today's blog post was tough. My last blog about??,??got attention from wine entrepreneurs who had local wineries and collected sales data. Some of them contacted me and wanted to hire me to analyze their data. While it may sound like a nice side project, I could not accept an offer because I did not care about working with wine data all that much. To try it out was a fun experience but I was just not that interested to continue. So as I was debating topics for today's blog post, I decided to write about something I am truly passionate about. No wonder, it was??,. I work in healthcare, I understand healthcare better than any other topic and there is still a ton for me to learn in this field.,One of the most discussed topics in healthcare today is Cancer. Hundreds of companies work hard to invent new diagnostic methods, offer new treatment options and develop tests that can help monitor this nasty disease. Recently I was looking for latest clinical trials in breast cancer for my work project and found an awesome public??,??supported by NIH. The ClinicalTrials.gov currently lists about 190,000 studies conducted across the US and around the world.,Cancer is a horrific and aggressive disease. According to??,, in 2015 there will be 1.6M people diagnosed with some cancer type and only 1M of them will be alive by 2020. Cancer takes half a million lives per year and, despite all the efforts, we still cannot find a cure.,Saying goes,??,. So my goal for this blog post is to understand cancer a little better and share my learning experience with you.,Although ClinialTrials.gov releases data on their website, I found it a little hard to download files from their site. In order to get their data you have to navigate your way through trial categories, topics and once you see a list of trials in a specific category/topic you can download a .txt file with only a few fields summarizing trial name, recruitment status, interventions and whether results were released. Even if you are ok with this level of information it is still very time consuming to pick every category and topic you are interested in, download each file manually and generate one master database with all trial topics/categories. So I looked around and found??,. It is a pretty cool public data aggregator that allows users to find datasets, preview them online and do some basic manipulations on the data (filtering, sorting, descriptive stats) before users decide to download the file. It is free, so go ahead and check it out ???,After doing a little search I found Clinical Trials data on Enigma.io website. If you are signed up for Enigma, you can check out the raw file??,.,For me the biggest challenge with this file was in the fact that it was very poorly structured. While the dataset itself was comma separated, some fields contained multiple values (e.g. Condition field could list multiple conditions), other fields had to be transformed into multiple (i.e. Intervention field listed multiple intervention types and within each type there were multiple interventions). The only good thing about this table was that each clinical trial was uniquely represented. Since I was only focusing on clinical trials in cancer, I filtered the file by cancer types and started manipulating the data.,There were a few things I had to do to my file in order to make it usable:,Finally I loaded all tables into Tableau and started my relational database visualization.,Lets take a look at prevalent interventions across tumor types. From the first visualization below you may see how the top heatmap breaks down cancers by intervention type. Not surprisingly drug intervention was the most common across the board. Procedure interventions were also used for most cancer types. Traditional surgery was the most common procedure intervention (for resectable cancer types) followed by adjuvant therapy (i.e. chemotherapy) and blood stem cell transplantation which is basically a way to restore stem cells after high doses of chemotherapy destroys them.,Biological interventions were next most common. The majority of patients were either treated with??,??that help slow down cancerous cell growth and/or??,used to prevent infections during chemotherapy.,Interestingly, Behavioral intervention (i.e. questionnaires) was mostly used in trials for breast, colon and skin cancers.??,??,suggests that women who had undergone mastectomy are more likely to have higher immunological response if they get psychological support than those who don't. Patients with colon cancer have to monitor their dietary behavior in order to have higher survival rates. Finally, skin cancer patients also need to monitor their sun exposure in order to have a better treatment response.,Radiation was most common in head & neck, lung and prostate tumors (tumors that have high risk of being spread throughout the body and develop metastases).,Device and Genetic interventions were least common in all cancer types.,For those interested in what interventions were used for each tumor type, I added a table on the left where you can specify,??and??,??and see specific interventions applied in selected condition.,Depending on the condition some trials may take significantly longer to complete than expected. The longest trials were conducted in rectal, breast, head, neck and blood cancer types with average trial duration ranging between 4.7 - 5 years. Brain cancer had the shortest trials with average duration of 2.8 years. Another interesting metrics to look at was the difference in expected trial completion date and its actual completion date. In other words, I wanted to see how late studies were in getting completed and how it varied by condition. Rectal, cervical and bladder cancers had the biggest difference in expected and actual completion dates (avg. difference b/w 1-1.2 years). Melanoma trials had the smallest 2.4 month difference.,Demographics was another big factor in cancer clinical trials I wanted to explore. Top heatmap of the visualization presented below displays % of clinical trials conducted within an age group in each condition. For example, 76.39% of all trials conducted on children were in leukemia, 40.9% of adult trials were in breast cancer and 33.96% of trials among senior population were in lung cancer.,This heatmap confirms our understanding of cancer prevalence. Adult population is most often recruited for trials in breast (41%), cervical (8.8%) cancers and leukemia (27%). Children are more likely to be recruited for leukemia (76%) and kidney (7%) trials. Senior patients end up in trials for lung (34%), colon (18%) and breast cancers (17%). Although we see an overlap in age groups in leukemia (children + adults), for kids this type of cancer is the most prevalent. This may potentially be explained by risk factors children have disposition to when they are born, i.e. genetic risk factors such as inherited syndromes or immune system problems. Similarly, breast cancer is prevalent in both adults and senior patients, but adults seem to exhibit higher prevalence than seniors. This one is a little easier to explain: women in reproductive period are more likely to be diagnosed with breast cancer than those who had gone through a menopause.,On the bottom you may see a breakdown of clinical trials by condition and trial phase. As you can see, most cancer trials (40-60%) are conducted in phase II which is about safety of the new treatment and how well it works to treat a specific type of cancer. Brain cancer, however, stands out. There are more brain cancer clinical trials (32%) in Phase I than in any other phase. According to??,, Phase I clinical trials are used to show that a new treatment is safe for a small group of people and to find the best dose and schedule for future research of the drug or drug combination. This may explain why brain cancer trials are much shorter than trials in other tumor types and also may imply that,.,In the visualization below you also have an option to slice trial data by phase depending on patients' age group. When I introduced this filter I was hoping to see more leukemia trials conducted in Phase III for children. Phase III trials compare a new treatment or treatments with the standard treatment in a large group of people. This would have indicated that there are more advancement happening in childhood leukemia treatment. Similarly, I was hoping to see the same picture for breast cancer in adult population and for lung cancer in senior patients. Unfortunately, I could not pick on this pattern. In children only 17% of trials were in Phase III compared to 40% in Phase II. In adults 20% of trials were in Phase III compared to 46% in Phase II. Finally, in senior population 16% of trials were in Phase III compared to 50% in Phase II.,From a preliminary look at the clinical trial data we can already learn something about cancer. Apart from drug interventions, some intervention methods were more prevalent in certain cancer types. For example, behavioral interventions were more common in cancers where patient's lifestyle could improve or worsen treatment outcomes (i.e. breast, colon and skin cancer); biological interventions were more prevalent in aggressive cancers with low survival rates at late stages when they are typically found (i.e. ovarian (17% 5-year survival rate in IV stage), pancreatic cancer (1% 5-year survival rate in IV stage)). Radiation was more common in cancers that typically spread to other parts of the body faster (i.e. lung, prostate, rectal, head & neck cancers).,Most clinical trials last somewhere between 3.7 and 5 years and take on average 8.5 months longer than initially planned. Brain cancer is the only one that stands out: on average brain cancer clinical trial lasts 2.8 years and ends only 4.8 months later than expected. But the reason seems to be not in the fact that brain cancer experimental treatments get patients into remission phase faster. On the opposite, these trials' primary goal is to prove that experimental drug is safe for the patients, whether it works or not is not where science of brain cancer is, yet.,By the way how patients get recruited for clinical trials we can observe general trends in diagnoses patterns. For example, prevalence of kids, adults and senior patients in trials by cancer type is consistent with breakdown of age groups by tumor types. For example, leukemia is the most common cancer in children, breast and cervical cancers are prevalent in adults, lung and colon cancers are more common in senior patients.,Finally, we saw that most clinical trials are conducted in phase II that assesses experimental treatment for safety and effectiveness, but does not compare to standard treatments out there (phase III). Conditions where more trials are in phase III are Anal, Bladder, Cervical, Colon and Rectal cancers but even there % of trials in phase III range between 20 - 24% which is still pretty low.,Cancer research is one of the most rapidly growing areas of clinical research in the US. According to the??,, about $5.6 billion dollars will be spent on cancer research in 2016. This means that in the upcoming years there will be more studies conducted, more treatment options available and better survival rates achieved. If cancer affected you, your family or people close to you and you are looking for new clinical trials, there is a??,??for you below. Click on the cancer type in both filters in the middle of the visualization and find a breakdown of trials by recruitment status. If you or someone you care for are looking for an actively recruiting clinical trial,??,.
As a data scientist who has been munging data and building machine learning models in tools like R, Python and other software(s) (open source and proprietary), I had always longed for a world without technical limitations. A world which would allow me to create data structures (data scientists usually call them vectors, matrices or dataframes) of virtually any size (i.e. big), manipulate them, and use them in machine learning models. A world where I can do all these fancy things without having to worry about whether I can fit them in memory; without having to wait for hours on end for my computations to finish (data scientists are an impatient breed) and without needing to write lots of code to merely compute a dot product between two vectors.,The seeds of my data science utopia were sown many years ago with the advent of Apache Hadoop and the MapReduce programming model. It solved the problem of storing and processing large amounts of data. A few years later, Apache Mahout was developed on top of MapReduce which provided implementations of machine learning algorithms. It all seemed too good to be true.,However IMHO, MapReduce as good as it is for lots of data processing workloads and use-cases, it didn?€?t seem to be best suited for data scientists. Its lack of interactive data analysis capability, coupled with the need to write very verbose mappers and reducers in Java (or in other languages using Hadoop Streaming) was never going to be liked by the Data Science community. This didn?€?t mean that the dream was over?€??€?..Apache Spark came to the rescue!!,For the uninitiated, Apache Spark is an in-memory, distributed, data processing engine, designed to run on top of distributed storage systems like HDFS. As a data scientist, Spark whets my appetite for a number of reasons.,To me, speed of analysis matters. It?€?s no good if you have to wait for hours to get results of a correlation matrix just to forget why you ran it in the first place. The ability to do train-of-thought analysis interactively on large volumes of data is one of the most important features that distinguishes Apache Spark from other data processing engines.,The ability to write succinct code to accomplish data science tasks was never the forte of MapReduce jobs. Spark has nailed this with its high-level API in Scala and Python (a widely used scripting language in the data science community). Add to this, its MLlib package which provides implementations for a number of feature extraction and machine learning techniques.,Finally, Spark processes big data. Don?€?t think I need to say more on this other than, in my view (with a few ifs and buts), more data beats clever algorithms. Maybe more on this in a different post.,As is the case with most things in life, every technology goes through it's peaks and troughs. For now, those data scientists who dream of a similar utopia, will find in Spark a much needed ray of hope?€?..Welcome to Sparkling Land.
Sometime today, shares of the data analysis company Splunk will start trading on the Nasdaq National Market in New York, after??,. The company?€?s offering has raised about $230 million.,Splunk is not your traditional big-data company, in that it is running some variant of Hadoop, like so many upstart companies in that space. Its specialty, as??,, is harvesting data from machines; they can be Web servers or they can be air-conditioning units. Its cloud-based database grabs all the data generated by the machine in the course of its operation, and generates alerts. Those alerts can tell a human operator things like ?€?everything?€?s cool?€? or ?€?ouch, things are not so cool. I need help.?€?,If a machine does something that repeats many times a day, hour or minute, then it is generating data that can be measured. And if it can be measured, there?€?s a pretty good chance that you can make it useful, using Splunk.,Big customers include cloud concerns like Salesforce.com and the Web gaming outfit Zynga. Wireless companies like MetroPCS and T-Mobile use it to monitor their wireless networks. Several government agencies use it to help watch for attacks on their networks. In all, it has about 3,700 customers, in 75 countries.,So, who stands to get rich today?,The two biggest shareholders are the Sevin Rosen Funds and August Capital, which each own 20.4 percent of the shares, worth just a whisker less than $280 million at the offering price. Behind them is JK&B Capital, which owns a 17.6 percent stake worth $240 million. Ignition Partners has a 12.1 percent stake worth $165 million. Sullivan, the CEO, has an 8.2 percent share worth $118 million. Another name that jumped out at me: Patrick McGovern, head of the tech research and publishing house IDG, has 240,666 shares worth more than $4 million.,Source:??


Reporting tool is good at chart & form design, style of landscaping, query interface, entry & report, and export & print. It is one of the tools that are applied most extensively. However, there are quite often complex computations in the report, which raises a very high requirement for technical capabilities of report designers, and is one of the biggest barriers in report design. This article will introduce to ways to solve the complex computations in the report.,A company has a High Growth SalesMan of the Year report, which analyzes, mainly through sales data, the salesmen whose sales amount exceeds 10% for three consecutive months, and demonstrates the indices such as their sales amount(Sales Amount), sales amount link relative ratio(Amount LRR), client count(Client Count), and client count link relative ratio(Client LRR). The report pattern is shown in following table:,The main data source of the report is the ?€?monthly sales data?€?: sales table, which stores the monthly sales record of the salesmen, with salesman and month being the primary key. The structure is shown in the following table:,It can be seen that the calculation of the name-list of the salesmen whose sales amount exceeds 10% for three consecutive months is the most complex part of this report. As long as this name-list is calculated out, it is possible to use the reporting tool to easily present the remaining part.,SQL Solution,A1: Group the data according to salesman. Each group is all the data of a salesman, which is sorted by month in ascending order.,??????????????A2: Refer to the calcualtion result of the preceding step, and select the group that meets the condition from A1. The condition comes from the last cell of A1 operation area, namely, Cell B3. Both B2 and B3 belong to A1 operation area. By writing the condition step by step in many cells, it is possible to reduce the difficulty.,??????????????B3: Conditional judgment. If the LRR of three consecutive months within the group is bigger than 1.1, then this group of data meets the condition. Here ?€?amount [-1]?€? is for the data of preceding record relative to the data of the current record, amount/amount [-1] represents a LRR comparsion. The pselect() is used to obtain the serial number within the group, and whenever meeting the first piece of data within the group that meets the condition, pselect() immediately returns the serial number and stops repeated calculations.,??????????????A4: Obtain the serial number of the salesmen in A2, and this result is returned through JDBC to the reporting tool for use.,??

Interactive analysis is a cycle analysis procedure of assumption, validation, and adjustment to achieve the fuzzy computation goal.,The interactive analysis is the real on-line analysis to solve the complex computation problem in the real world, and it is one of the key points in the business computation.,Let us explain the interactive analysis with a common example in the business activities.,Why the sales volume this month greatly exceeds that of the previous month?,Obviously, this is a fuzzy computation goal with several possible answers. You cannot get the result directly using any analysis mode.,Since there are several possibilities to give rise to the sales volume increase, the analyzer has to check every possibility, such as:,Obviously, a certain level of business knowledge is required to make these assumptions and the keen sense of smell to the circumstances inside and outside the enterprise. This is a relatively personalized effort.,Based on the possibility and characteristics of data, the analyzer will choose a branch to start the analysis, such as Increase of Orders. If the number of orders does not increase through the calculating for validation, then it indicates that this assumption is not correct. You need to validate the next assumption to carry on the cyclic analysis.,For example, by going through the validation on this branch of Appearance of Large Order, the analyzer finds this is correct, and thus this branch can be justified.,These possibilities are usually the apparent cause instead of the root cause. To really settle the problem, you will have to drill down step by step to reach the core. For example, the appearance of large order may result from:,It is obvious that the process of drill-down is a cyclic procedure. The analyzer must judge on the characteristics of data at that specific point to choose the branch of the highest possibility, so as to progress level by level, until the problem is solved.,The procedure of exploration and mining does not require the unlimited drilling down. The whole procedure can put an end once a clear answer enough to make a decision is found. For example, through the validation, the Centralized Procurement in a Certain Sector is determined just the root cause. Then, this is enough for analyzer to make a decision: The sales volume can keep rising by simply beefing up the sales forces and efforts in this sector since the recent sales rise is the result of centralized procurement by the clients in this sector.??,To this step, the computation goal is achieved. However, we can realize more business values through more computation on the basis of the existing results, such as:,As we can see from the above examples, the real world is far more complex than the theory. The commercial opportunity changes unpredictably and comes and goes in a moment of doze. In fact, the computation on the business activities is usually fuzzy. There are few model algorithms from textbook that can be used to solve the real situation. The analysis computation is to solve the problem in the real world. They are characterized with the following points:,Interactive analysis can be always resolved to the fixed algorithm. For example, ranking algorithm is usually used to compute the ?€?Appearance of Large Order?€?; grouping algorithm is usually used to compute the ?€?which sector sees the intensive procurement by clients?€?. ??,The bottom layer of interactive analysis is the fixed algorithm though, the human intervention is necessary. How to break down the target? How to set the priority of branches? Whether to carry on the mining or not? Is the existing result enough to support the decision-making? Is the further computation necessary? Theoretically speaking, the power enough computer programs can implement the above network-like branches, and thus turn it into the fixed algorithm. However, before the The Matrix and Neo born, the analyzers will have to take great effort in it.,Interactive analysis is to solve the problem in the real world. The assumption will have to make on the basis of business status, and the next step computation will be decided on the current data and business experiences. To do this, the abundant business knowledge is required. The qualified analyzer is usually the business expert. The database administer and programmer are more fit to seek the solutions to the fixed algorithm and they are able to provide the assistance in computation but hard to make the most important business decision.,The massive structural data is the data capable to be represented with a 2-dimention structure. Of the massive structural data, the typical examples are the data from database and spreadsheet, and text file. In the business activities of real world, these data are the most common and fundamental, acting as the base of business calculation.,Characteristics of interactive analysis determine its requirements on computation tool:,Based on the requirements on the interactive analysis tool, we can list some common tools, just name a few:,There are many other tools for interactive analysis. However, one point to note is that none of them is perfect on all aspects. For example, the analyst may find that it is easy to grasp Excel but hard to compose SQL statements; esProc lacks of the non-linear model, but it can provide a convenient interactive process; SPSS boasts the abundant fixed algorithms but is not as convenient as SQL in relation query.,The tool suitable for your needs is the correct one. Please refer to the , to join us on choosing the analysis tool suitable for your needs.,As you may find that the interactive analysis is similar to OLAP in some respects. Please refer to another article: interactive analysis and OLAP.
For details or registration, go to??

For many companies going down the big data path, collecting data for its own sake has been a costly exercise with varying degrees of ROI.??,Historic data is great for understanding past events, but as companies mature, their big data efforts move to become more central to business operations: from descriptive, diagnostic analytics to predictive, prescriptive courses of action. In this environment, understanding how real-time external data sources benefit an organization is vital.??,Traditionally, companies engaged management consultants or in-house data scientists to pour over copious amounts of historical, internal data to identify trends and develop models and algorithms to predict future trends and improve the company?€?s operations.????,The problem with historic based models is that without current real-time data, companies cannot effectively prescribe a course of action. To remain competitive in today's business climate, companies need current conditions, which are ever changing and evolving. Historic based data models need real-time triggers.,In real world terms, past traffic data can generally tell you what the fastest average route through a city current traffic conditions require the knowledge of real-time factors including accidents, road blocks, weather conditions and special events to effectively guide vehicles through a city.??,By knowing the key real-time factors that might impact a model, data scientists can now develop algorithms which take into account key data points to arrive at a more accurate prediction of future conditions and create actionable items based upon them. The combination of real-time events with historic patterns allows predictive and prescriptive analytics to emerge. Such evolutionary analytics allows data scientists and managers to solve issues and prescribe solutions in real-time.,Data analytics firm Vitria (,) conducted a study of 235 US based firms and found that 25% are already embarking on real-time predictive modelling; while another 47% are actively exploring the opportunity, with the remaining 28% still sitting undecided how to proceed. The findings are clear: while there is sizable interest and utilization of real-time data analytics, there is still a lot of room for growth and maturity in the field.,To further enhance real-time models, organizations are now combining both internal and external real-time data. Historically, many models only used the company's own internal data resources, but now, more and more organization are finding the only way to more accurately predict the future is to understand the external factors that impact business processes.?? For example:,It is no surprise that historic data on where businesses see the most customers and revenue can be useful.?? However, real-time footfall patterns (the movement of people throughout a given geography) may vary wildly due to a number of factors such as weather, special events and traffic conditions. Not taking these external factors into account means that companies, large and small, are missing the opportunity to maximize revenue and operational efficiency. As an example, by understanding external, real-time factors effecting population movements, mobile food trucks could deploy more efficiently to various locations at any given time.??,Previously, hospital human resource allocation had been based upon numerous factors including current patient load, predicted number of new patients, and resource availability. However, hospitals are now combining internal information with external real-time sources including: weather, disease trends and sister-hospital patient load in order to make life-saving decisions around human resource allocation and staffing levels.??,Historic data on prices of produce do not provide accurate predictions of current prices due to a variety of factors such as: weather, water supply, logistics, soil conditions and disease.?? In contrast, real-time pricing data could provide a much stronger base for buyers to negotiate prices with produce sellers.????,As referenced in the Stella Artois?? case study from WeatherUnlocked.com (see below), weather has a major impact on consumption patterns across a wide range of industries including: food & drink, travel, apparel, healthcare, home & garden, energy and automotive.?? By leveraging real-time weather data, marketers can develop more timely, relevant, impactful communication campaigns and optimize media spend and geographic touch points.????,By mapping historical weather data against sales metrics for its Cidre product, Stella Artois?? discovered that a two degree temperature rise above the monthly norm triggered a rise in sales.?? Digital out-of-home billboards were identified as the optimum channel due to their proximity to stores.?? Using real-time weather data, Stella Artois?? ran a campaign on a cost-per-minute basis, activating only when conditions were right to induce purchase intent.,Results of the campaign were astounding:????,Given the trillions of bytes of streaming data that are now available, focus is key for informed organizations to be successful. Deciding which external data sources are most valuable and how best to exploit them will be a key determinant for future success.?? Creating models based on historic data is only the first step in improving a company?€?s operations. The true insights come by combining both internal and external real time data sources in creating actionable decision-making models companies. In this way, companies across industries can receive a better ROI on their big data efforts.
Good day and my warmest hellos from Montreal, Canada to all members of the Data Science Central virtual community.,I have recently joined this very interesting, bountiful and multi-sub-disciplinary web site in the data science field and admittedly I?€?m still exploring all of its numerous knowledge facets.,In order to introduce myself to all of you I thought it would be ?? propos to use the project I?€?m currently working on as a window to my expertise, which compared to all of you, is minimal at best.,My goal is to begin a mutually beneficial dialogue that will allow knowledge and expertise sharing with my Canadian colleagues/members as well as with all of you south of the border and from other continents, with regards to the BI solution I?€?m developing called: , ,.,Indeed the crazy endeavour I?€?m working on (as I affectionately call it) will also serve to address and resolve the identical and quite serious phenomena that prevail to an even wider scale and higher degree in the US and Europe in the defined pension benefits pension plans arena.,May I remind all of us of the recent casualty In the US, i.e. the city of Detroit who declared bankruptcy in large part due to the weight of its defined benefits pension plan? In the Crain?€?s Detroit Business web posting, dated March 1,, 2015 an article by Ms. Mary Kramer cites the following:,??,.,It is worth noting at this juncture, that both in Canada and in Europe, the current applicable constitutional and governing laws do not allow for S&L governments - using US terminology here - to be placed under bankruptcy protection by the provinces, counties or regions as the case may be. The tax payers simply continue footing the bill!,The current financial situation of the large majority of Canadian and European public sectors?€? DBPP?€?s is one that should be keeping our elected officials awake at night as well.,For a more concentrated US perspective, ,.,For a worldwide view of governments funded public defined benefits pension plans ,.??,In order to mitigate the risk of a breach of intellectual property I unfortunately will remain a tad cryptic on the details of my solution.,??given in September 2014 in English will also support the following with more verbiage if need be.,I will discuss and outline my R&D?€?s conclusions and will provide essentially the infrastructure of the data warehouse that I envision. To reach this key foundational milestone will alleviate ?€? I estimate ?€? 30% to 35% of the pain points encountered with the present set-ups, irrespective of the plan promoters sectors, being private, or public and parapublic; the latter applying mostly to Universities here in Canada.,My professional background rests on 3 main pillars; the financial and administrative management of defined benefits pension plans, project management (Waterfall methodology and Agile concepts and principles) and the IS/IT field.,This combo of skills allowed me to look at the very precarious financial health of these plans and with a retrospective historical view to build some trending analysis. ??The knowledge acquired over the years (mane of grey hair to prove it) along with an initial validation of my vision by a UCLA, Anderson School, Finance professor I met a year ago, justified my almost relentless quest to get to the bottom of this issue for the sake of future generations across the globe.????,Putting the pieces of this puzzle was perhaps more fluid given my professional background; as far the project management community knows I?€?m probably the only resource in Quebec with this DNA for the time being? Can?€?t confirm with a 100 % certainty for the rest of Canada and in the US; will welcome input on this existential questioning of mine from all readers!,Let?€?s begin by tabling the key reasons, which in my opinion, lead us to where we are today without being too technical:,I invite you to read ,??available free.??The author enlightens us all in behavioural economics and how money is, at times, an extremely dangerous motivational factor without the appropriate check and balances.,After reading this amazing book, it was therefore mandatory that the architecture conceived provide the correct operational parameters and accountability matrices to avoid excessive behaviours from all stakeholders!,The technology is indeed disruptive for both the fund management and actuarial sciences?€? established professional communities.,Wearing an IS/IT focused ?€?hat?€? I have attempted to decipher the pain points in the current e-tools infrastructure, at the plan promoter/employer level, that essentially gave credence to the above 7 factors to take on a life of their own in the day-to-day administration of these plans resulting in the current precarious scenario.,??,??,??,Here?€?s an excerpt from an e-mail received the Bank of England whom I had contacted to get data on the statuses of the UK?€?s pension plans in January for an op-ed written for a European French speaking targeted audience.,The defense rests your Honor!,In summary currently the only real-time data supplied to the plan promoter are the investment portfolio?€?s assets each morning after closing of the various markets across the globe. So the spread, i.e. assets vs. liabilities is presently not available on an agreed upon regular basis (bi-weekly is ideal in my opinion) under one single easy to use warehouse.,As more industry experts?€? analysis, reflections and discourses are aiming towards target benefits, hybrids, or shared risks pension plans in conjunction with Liability??Driven Investing??the tools to manage these plans accurately and punctually are mandatory.,A complete ?€?under one roof?€? warehouse regrouping all the basic required data, up-dated regularly, resting on overall accepted and adhered to industry data standards with predictive modelling iterations and dashboard functionalities is definitely the way of the future. ??Such architecture will also mitigate the risks of corruptions that sadly were prevalent in a number of Quebec cities until recently. Project management research conducted by the PMI has repeatedly proven that breaking down silos reduces operational costs, corruption inclinations while increasing best practices adoptions and success rates notwithstanding the field.,Our respective governmental machines are too slow and elephant like to address these issues in a quick and agile manner; furthermore we all know that implementing BI solutions imply a much greater accountability and the introduction of best practices by the users. Are our governments ready for this challenge for the sake of our respective economies and worldwide financial stability for the betterment of our future generations? As in climate related issues, financial and economic sustainability for those who will follow us is a must.,In a recent posting in the Inc.com web site ,.,Having exposed a bit of knowledge and vision, and being a fervent proponent of knowledge sharing and 360?? feedback I invite you to get back to me with your comments, criticisms, suggestions, questions so we can hopefully build a community of evangelists to get our countries out of this retirement planning stalemate for the betterment of our current and future generations. Our expertise to the service of our fellow citizens.??,With my sincere appreciation and warmest regards to all!,Project Manager??|??Business Intelligence??solution,Specialization in retirement savings plan,????,~Calvin Coolidge~
Top 10 Cheap Car insurance states in red marker:,10 most expensive car insurance states in blue marker:,The data set would need to be in the following format:,The steps to create the open street map visualization are simple & are below:,====,??=====

This is a technical post. The purpose is to test??topic modeling techniques with Python on arabic texts in order to grasp the efficiency??of the approach used in my previous work (,) on a different langage.,The??,??may be applied as is to any "brand" by changing the keywords searched when querying??the Twitter API.,My approach is the following:,??in order to learn more about this approach. You can also contact me for further explanation if you are interested in applying this approach to your own brand by analyzing a massive amount of Arabic text...,Philippe
Data is everywhere and growing. As a marketer, this is a dream world. Your audience is generating tons of data from your web visits, mobile apps and you have partners giving you data. This is rich customer journey data.,However commonly used terms like Big Data and Analytics can mask the complexity of realizing your ROI gained from analyzing all this data. The promise of the visual tools and Hadoop based platforms is all great but if you don't have a data strategy in place all the tools are just shelf ware.??,Many smart CMOs and analysts I have worked with, spend most of their time on getting their data right to begin with, working closely with their data team.,The key, is to have a data strategy for your marketing data using the ,1. Managing your data: Is all of your data organized such that it?€?s easy for your analysts to access. Often times there are tons of SQL and NO-SQL /Hadoop instances and then there are a hundred spreadsheets and the analyst, with limited tools, has to make sense of all this data. Incomplete data can lead to inconclusive analysis and you might as well flip a coin to make decisions! We marketers like clean data sets and to go from raw events to aggregated data sets takes a lot of work.,2. Metrics: Do you have a sense of the key metrics to measure? Dashboards, showing nice looking trends of your daily visitors don?€?t count (even if you call them KPIs!),Have you identified important metrics with deeper analytic value such as campaign ROI by segments, retention by segment, customer journey metrics by source of acquisition: metrics that matter to your business? These kinds of deep metrics are the ones you want to organize, measure and monitor periodically. Campaign attribution and analysis of all your metrics using behavioral segmentation of your visitors/users can lead to a huge ROI when done right.,3. Metadata: Are you happy with the quality of analysis and your confidence in the accuracy of the data? What derails many analytics projects is a lack of faith in the data. Where does the data come from and what was the context, how does it get transformed and aggregated and can you trace an insight to the raw data source? If you maintain your metadata well you can trace and reconcile your analysis easily. What is called as ?€?ETL?€?, especially in the Big Data world is essentially a black box, with the metadata hidden away in esoteric scripts. In the absence of such metadata the source data for an insight is hard to trace and so managers can?€?t validate the insights and talk about them convincingly.,Marketers and Marketing heads need to look deep into their data and go beyond easy ?€?low hanging fruit?€? solutions to tackle the hard problems of??,, metrics and metadata in order to derive real ROI on marketing spend.
How many times a day do we ourselves, or hear someone else, utter the phrase ?€?Google it?€?? It?€?s hard to imagine that a phrase so ubiquitous and universally understood has been around for less than two decades. The word ?€?Google?€? has become synonymous with online search, and when we think about why this, it?€?s because Google yields the most relevant, comprehensive results, quickly. Essentially, it has changed the way we find and interact with content and information.,We?€?ve seen the cultural effect Google has had on search and discovery on a broad level, but consider the implications for online media and publishing organizations interested in honing these same powerful search and discovery capabilities, a process referred to as ,. The results can be transformative, but many companies in the space still struggle with harnessing the technology.,Let?€?s take a deeper dive into what semantic publishing is, how it works and most importantly, why it matters.,The idea of dynamic semantic publishing can often be a difficult concept to grasp because its use is not readily apparent to viewers. Rather, it?€?s a process centered on the curation, enrichment and analysis of text and how it?€?s organized even before users interact with it.,Semantic publishing includes a number of techniques and tactics, including semantic markup and recommendations. Through these techniques, computers are able to understand the structure, meaning and context of massive amounts of information in the form of words and phrases.,At this point, you may be thinking, ?€?This sounds a lot like tagging.?€? As news organizations and publishers began taking their content online, tagging was the basic process used to categorize information. Basically, when you type a term into a site?€?s search box, the results returned will contain that word. However, dynamic semantic publishing goes well beyond simple content tagging.,At the heart of this solution are three core semantic technologies:?? text-mining, a semantic database and a recommendation engine. , is used to analyze content, extract new facts and generate metadata that enrich the text with links to the knowledge base. The , stores pre-existing knowledge, such as thesauri and lists of people, organizations and geographic information. It also stores the new found knowledge and the metadata delivered from the text mining process. The , delivers personalized contextual results based on behavior, search history and text that has been interlinked with related information. ??,The text mining process operates behind the scenes and continuously runs.?? Sometimes this process is referred to as ?€?semantic annotation?€?. In essence, it?€?s a pipeline of text.???? Articles are analyzed, sentences are split up, entities are identified and classified in real time.?? The pipelines often uses related facts from other sources that have already been loaded into the semantic database.?? These Linked Open Data sources help resolve identities that are the same but referred to differently.?? During the annotation process, , between entities are discovered and stored such as relationships between people, where they work, live, travel, etc?€? All of the results, known as semantic triples or ?€?RDF statements?€?, are indexed in a high performance triplestore (graph database engine) for search, analysis and discovery purposes.??,This knowledge base is extended with key terms and related concepts, all of which are linked to the original articles or documents.?? ??Oftentimes the pipelines encounter entities that require , ??This is crucial to avoid confusion between Athens, Georgia and Athens, Greece, for example, OR the Federal Bureau for Investigation (FBI) with Federation of British Industries.?? The final result is richly described content all of which is interlinked and stored in the semantic repository.?? ??,To ensure the text mining algorithms are kept up to date, is collected allowing , to automatically adapt and retrain the algorithms ?€? this way the knowledge base becomes smarter as publishers prepare to deliver on the promise of personally targeted content.,When this data is combined with web visitor profiles and user search history, the recommendation engine takes over.?? Massive amounts of data are analyzed to determine the most likely news articles of interest.?? This magical blend of profiles, history, structured entity descriptions, classified facts, relationships and enhanced knowledge delivers a wonderful user experience. Visitors are automatically delivered highly relevant news.??,In a recent article entitled ?€?Financial Times Builds Its Publishing Infrastructure?€?, Jennifer Zaino spoke with Jem Rayfield, the Head of Solution Architecture at the FT.?? She writes:,?€?Among other semantically-infused capabilities available now are recommended reads, based on the concept of semantic fingerprints, Rayfield explains. That is, the Financial Times leverages a concept extraction mechanism to understand what published stories cover and annotates them with identifiers found within the content, which is matched against users?€? reading habits to identify what they?€?d likely want to peruse next. A similar approach semantically matches readers to ads. ?€?Semantic advertising is good in terms of getting very targeted ads to the right people and profiles so that we can charge more for ads,?€? he says. ?€?And serving the right content to the right people gets them to click to read more content so people stay on the site longer.?€??€?,Cleary solutions like this need to ,. ????At the same time hundreds of queries per second are taking place to serve requests on your website, authors are also enriching new content, which is then committed to the database and available for the next search. As they write prose, they are prompted with related news and facts they can use in ,.?? This instant feedback directly correlates with author productivity. If anyone sees something misclassified, they can correct it and commit the change to the underlying triplestore leading to a smarter knowledge base that can drive recommendations.?? ??,Let?€?s look at another real example.,For the 2010 World Cup, the BBC was looking for a new way to manage their web content, including text, video, images and data that encompassed 32 teams, eight groups and 776 individual players. As with many publishing organizations, there was simply too much content and too few journalists to create and manage the site?€?s content.,The BBC implemented a Dynamic Semantic Publishing framework to accurately deliver large volumes of timely content about the matches, groups, teams and players without expanding on the costly manual intervention of editorial.,Semantic publishing is dramatically changing the way we consume information. It automates the process of organizing and deciding what content goes where on the web, so that news and media publishers can quickly and accurately manage content, create more and deliver a personalized user experience.?? While the BBC is just one example, it becomes easy to see the far-reaching implications of online media and publishing companies embracing the same approach.,Specifically, semantic publishing can drive real business results, including: , by enhancing content authoring, editorial and delivery phases; expanded knowledge base and content offerings; personalized content recommendations to coincide with readers?€? interests; and, , to directly impact the bottom line.,As digital publishing companies house an ever-increasing amount of digital information, those that embrace semantic technology will win on so many levels. News aggregators can ,.?? Researchers are able to pinpoint exactly what they are searching for at record speeds.?? Decision makers can accurately assess performance with visibility into the timing and volume of content read. Writers are informed in real time and yield more content. ??Web site visitors get recommendations they never thought were possible and advertisers benefit from increased response.,There is nothing precluding this very same technology from being applied elsewhere. ??, can use it to personalize content.?? , can standardize and classify content using a common language while also delivering relevant information.?? , can index knowledge extracted from complex bio-medical research.???? , can analyze and integrate dozens of data sources useful in diagnosis and treatment. , can search corpuses of intelligence useful in mission critical applications. To learn more about this technology, visit ,., , 
The cornerstone of the small work done for getting the info for these great charts??with ,, was to be able to catch mentioned companies.,The basic idea of relation extraction is to be able to detect mentioned things in text (so called Mentions, or Entity-Occurrences), and later decide if in the text is expressed or not the target relation between each couple of those things. In our case, we needed to find where companies were mentioned, and later determine if in a given sentence it was said that Company-A was funding Company-B or not.,In order to detect those funding we needed to be sure of capturing every mention of a company. And although the , catched most of them, there are always some folks that name their company , or ,, words that are not very easily trackable with a NER.,A good solution is to build a Gazetteer containing all the company names we can get. The idea of working with Gazettes, is that when using them, each time one of the Gazette entries is seen on a text, it?€?s automatically considered as a mention of a given object, ie, an Entity-Occurrence.,From an encyclopedic source; we got more than 300K entries.Great!,The next challenge was that?€? well, in the text to process, a company could be mentioned on a different way than the official one stated on the encyclopedic source. For instance, would be more natural to find mentions of ?€?Yiftee?€? than ?€?Yiftee Inc.?€?,So, after incorporating a basic schema for the alternative names (ie, substrings of the original long name), the number of entries grew up to 600K.,After that, when we felt confident about our gazette and wanted to start processing text, we faced several issues:,So, knowing that we were trading recall[1], we decided to add several levels of filters. Let the pruning start!,First step was to add a second encyclopedic source, not to augment, but instead to add confidence, keeping only the intersection of those 2 sources.,Next, with a precomputed count of words frequency, we filtered out all those company names that were too probable to occur as normal text (we used some threshold and tuned it a bit before leaving it fixed).,With that very same idea of words frequency, we pruned the companies sub-names (the substrings of the original long company names), with a higher threshold; so, for a company listed like ?€?Hope Street Media?€? we didn't end up with a dangerous entry for ?€?Hope?€?, but instead for ?€?Yiftee Inc.?€? we did have ?€?Yiftee?€? on the final list.,With all that done, we reduced the list to about 100k, which was still capturing a really good portion of the names to work with, but reducing a lot the mentioned issues above.,The last step was to pick a sample of documents, preprocess them, and simply hand-check the most used (found) Gazette-Items creating a blacklist for the cases where it was obvious that occurrences were most of the times not the company mentioned, but just natural usage of those words on the language.,We finished very satisfied with the results and also with the lessons learnt. Hopefully some of the tips above can help you.,[1] recall: (also known as sensitivity) is the fraction of relevant instances that are retrieved. , ,original post: 
Bank of America COE Brian Moynihan recently disclosed that its mobile customers grew by 2 million, touching 70 Million this year. Wells Fargo and JP Morgan Chase have already reported robust growth in mobile customers in 2014.,This rise in mobile banking users is welcome news for the banking industry. This growth is not limited to the US alone. Britain also, in the recent past, has seen an uptick in mobile banking users and the number is expected to reach 32.6 million by 2020 as per a study by Fiserv.,Mobile banking is widely used in emerging economies of the world ?€? especially where traditional banking services have not taken deep roots. For example, Sub-Saharan Africa, Kenya and Bangladesh have huge user base in mobile banking.,But the spread of mobile banking here in the US has taken a different route - even arguably slower and maybe even a scenic route. Nevertheless, the growth of mobile banking offers exciting opportunities to banks that are in the throes of massive digital transformation.,A recent study by the Federal Reserve Board has made several far reaching findings. The study examines trends in the adoption and use of mobile payments and shopping behavior in the US and how the emergence of mobile financial services impacts customer interaction with banks and financial institutions. In my view, three key findings of this study stand out because of their larger impact.,Source : Consumers and Mobile Financial Services 2015, March 2015, Board of Governors of The Federal Reserve System,Firstly, the study reported that 87% of the adult population has a mobile phone, with smartphones making up 76%.?? Secondly, this widespread usage of mobile phones is changing the way consumers access their banking and financial services. Thirdly, mobile phones are impacting the way customers make payments.,??But the big question is how will the banks be impacted ??How can banks monetize this boost in mobile customer base??? Has it really benefited banks bottom line? Where do they begin?,??A recent study by McKinsey has pointed out that non-bank players are increasingly threatening the supremacy of banks in the payments business. PayPal, Square, M-PESA (promoted by telecom giant Vodafone) ?€? just to name a few - are some leaders who dominate this space through continuous innovation and delivery excellence. ??In fact, I would argue that they have the advantage currently.,??Banks must reformulate their mobile payments strategy. For example, Ebay expects to reach a transaction volume of $300 billion in 2015. Given the size of the opportunity in mobile payments, we can expect banks to enter in a big way. Banks entry and dominance of this space can potentially redefine the payments industry.,??Analytics is another area where banks have traditionally enjoyed clear advantage. As the customer base increases, mobile analytics will take center stage. Mobile analytics apps can provide real time analytics on financial products, investment options etc.?? Further, analytics can help banks identify payments business segments best suited for them and craft a long term mobile payments strategy.,??Many banks have already seen the opportunities coming. For example, BBVA, a leader in mobile banking, is seeking to offer a ?€?complete banking experience?€? on the mobile phone. Other banks are experimenting with the use of biometric facial and voice recognition apps.,??This surge in mobile customer base is a positive and timely development for banks. It provides a great window of opportunity for banks to innovate and create new opportunities to boost their bottom line. In many ways, it is a reiteration of the customers trust in banks.
One month back Flutura??was in ??a meeting with a customer in Houston where??we were involved in an IOT big data project . It was a very tense meeting ...??,s. The??,??thru a massive magnifying glass. The tension in the room was palpable. The cross functional team had spent a lot of time building a platform and the CEO wanted to see??,??and how it??,??for them. Given the intense market pressure, if the team did not articulate an??,, the project was definitely going to be axed. After about 3 hours of intense conversation the cross functional team of Engineering and IT folks were able to hammer out a??,??to??make the case to the CEOs office .The meeting happened a week later.,....,The beauty about this near death experience in executing the IOT big data project ( We must admit ...It was a quite nerve wracking experience for us at Flutura as we went thru the turbulence ) was that it forced us to focus on the core . We have synthesized our learnings and we would be the first ones to admit that many of them are??obvious but we felt the strong need to??,??: ??Think??,( for example : The project would reduce fuel??consumed by 3 % resulting in $ 34,000,000 savings per year in first 2 years ),??:??Are we????solving??,??which keeps business awake at night ( for example one should not be solving problem no 18????in the problem list from a business perspective ...??,??helps ),??:??Is??,??? ( One would be surprised as to how status quo is an option for many industries, businesses and processes. You may be excited about the possibilities but ),??while speaking to CXO team,These??are??,??: Spend??,??of conversation in??,??of a big data project on,??and not solution or architecture.,These are tough times ...Markets are undergoing??,??... Industries which were immune to recession are??,.Big data and analytics has a huge role to play in driving efficiencies in many industries. The??,. We are reminded of a quote by T S Eliot,We hope the above learnings reinforce your experience??and happy to hear your stories :)
Personal data has an image problem. People are increasingly wary of handing over their details to websites ?€? particularly when they have no intention to engage long term. Many marketers set out to obtain email addresses, yet do very little to ensure the email addresses they get are valid. This is the beginning of a vicious circle for the business they?€?re working for.,Why? Trust has broken down. If customers don?€?t trust you, they will hide their data from view.,Official advice from tech-savvy gurus is to hide your real personal data from the prying eyes of advertisers, by entering false dates and other booby traps. In 2012, , as a way to protect security and stay safe online.,This is a dirty data nightmare for small businesses, and it will only serve to compound the considerable problems we already face with bad data.,According to the Telegraph newspaper, , think their data is at risk. They cite data loss, theft and surveillance ?€? all key issues that have hit the news. Paranoia? Not necessarily. Consumers are not merely influenced by media reporting. They have first hand experience of data abuse:,??,The figures quoted are from Symantec?€?s 2015 State of Privacy report ?€? a reliable industry source.,In some respects, less ethical marketers only have themselves to blame. They have pushed the boundaries of personal data collection, harvesting information for marketing purposes when the customer was unaware, and using phone numbers and addresses to sign people up to mailing lists they never wanted to join.,One key reason for this is the sheer number of agreements we sign up to online. In truth, none of us has time to pore over every single privacy policy, or pick over the contents of every Terms and Conditions pop-up that we?€?re asked to review. Sometimes, we skip over the detail. We place trust in companies that perhaps do not deserve it. And our data is left wide open for abuse.,In fact, according to the same Symantec report, 80 per cent of surveyed users do not read terms and conditions. These may well describe how data is stored and used, but we?€?d never even know.,Social networking sites have come in for their fair share of controversy too, thanks to some liberties taken with advertising tracking ?€? the Facebook Beacon being one of the first. High-profile hacks at companies like Sony and , also serve to make people nervous.,This worry about personal data hasn?€?t stopped people from using the internet or engaging with businesses on some level. They have simply adapted their behaviour to protect themselves.,Over time, Symantec believes that customers will vote with their feet. They will find businesses that manage data properly; businesses they deem to be safe. Customers will then develop new spending habits, and simply migrate their custom on that basis.,Not only is bad data costing you money, but it could force customers to your competitors.,The key take-away is that people , still handing over data to businesses. But the balance of trust has shifted,Because of hacking, or unethical marketing, or poor management of data, consumers have started to mask personal information.,In the future, we will see two different changes. On the one hand, businesses will have more data to sift through and organise, and there will be bigger penalties for its misuse, compounding the cost of poor data management. On the other, customers will start to consider price, quality and data management when making a buying decision.,In order to be profitable going forward, a business will not be able to be the best or cheapest. There has to be a third factor: responsibility to manage data properly.,This means:,Data quality is a barrier to efficiency and accuracy. If you are harvesting email addresses in return for an incentive, and those email addresses are being deliberately obfuscated because trust has broken down, this can cause a ripple effect in terms of wasted effort, wasted materials and a dreadful ROI.,This is not a theoretical problem. It?€?s something that third party resources already face. Integrate analysed more than 775,000 leads generated for marketers working in the B2B sector, and they found that 40 per cent did not meet the client?€?s requirements in terms of quality or accuracy.,If your customers are deliberately making it difficult to obtain good quality leads, this kind of data quality shortfall is only going to become more expensive.,In some ways, it?€?s positive that consumers care about data. And it?€?s interesting that they recognise its value as an asset. (Many businesses would benefit from realising this, too.),More than half believe their personal data is worth more than ???1,000.,Indeed, when it?€?s accurate, and well-managed, personal data could be worth ten times more. It all depends on what the business does with it, and how effectively it?€?s used in automation, segmentation and reporting. And it depends on how well it?€?s cared for, or whether it?€?s left to go stale and wither away.,But without validation, and that critical element of trust, the , may as well be the data from a five year old telephone directory. It simply won?€?t serve its purpose, and it could have negative consequences for customers and profits alike.
This blog was originally posted on our ,, as part of a Text Analysis 101 series.,We recently added a feature to our , that allows users to classify text according to their own labels. This , relies on Explicit Semantic Analysis in order to determine how closely matched a piece of text and a label or tag are.,This method of classification provides greater flexibility when classifying text and doesn't rely on a particular taxonomy to understand and categorize a piece of text.,Explicit Semantic Analysis (ESA) works at the level of meaning rather than on the surface form vocabulary of a word or document. ESA represents the meaning of a piece text, as a combination of the concepts found in the text and is used in document classification, semantic relatedness calculation (i.e. how similar in meaning two words or pieces of text are to each other), and information retrieval.,In document classification, for example, documents are tagged to make them easier to manage and sort. Tagging a document with keywords makes it easier to find. However, keyword tagging alone has it?€?s limitations; searches carried out using vocabulary with similar meaning, but different actual words may not uncover relevant documents. However classifying text semantically i.e. representing the document as concepts and lowering the dependence on specific keywords can greatly improve a machine's understanding of text.,Wikipedia is a large and diverse knowledge base where each article can be considered a distinct concept. In Wikipedia based ESA, a concept is generated for each article. Each concept is then represented as a vector of the words which occur in the article, weighted by their tf-idf score.,The meaning of any given word can then be represented as a vector of that word?€?s relatedness, or ?€?association weighting?€? to the Wikipedia based concepts.,A trivial example might be:,Comparing two word vectors (using cosine similarity) we can get a numerical value for the semantic relatedness of words i.e. we can quantify how similar the words are to each other based on their association weighting to the various concepts., In Text Analysis a vector is simply a numerical representation of a word or document. It is easier for algorithms to work with numbers than with characters. Additionally, vectors can be plotted graphically and the ?€?distance?€? between them is a visual representation of how closely related in terms of meaning words and documents are to each other.,Larger documents are represented as a combination of individual word vectors derived from the words within a document. The resultant document vectors are known as ?€?concept?€? vectors. For example, a concept vector might look something like the following:,Graphically, we can represent a concept vector as the centroid of the word vectors it is composed of. The image below illustrates the centroid of a set of vectors i.e. it is the center or average position of the vectors., ,Source: ,So, to compare how similar two phrases are we can create their concept vectors from their constituent word vectors and then compare the two, again using cosine similarity.,This functionality is particularly useful when you want to classify a document, but you don't want to use a known taxonomy. It allows you to specify on the fly a proprietary taxonomy on which to base the classification. You provide the text to be classified as well as potential labels and through ESA it is determined which label is most closely related to your piece of text.,ESA operates at the level of concepts and meaning rather than just the surface form vocabulary. As such, it can improve the accuracy of document classification, information retrieval and semantic relatedness.,If you would like to know more about this topic check out this excellent , from Christopher Olah and this very accessible , from Egozi, Markovitch and Gabrilovich, both of which I referred to heavily when researching this blog post.,Keep an eye out for more in our ?€?Text Analysis 101?€? series. You can sign up for a free API account ,.
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
`The full-stack BI software category comprises of three different product flavors, which are nonetheless comparable to each other as they all endeavor to provide an end-to-end solution to the data challenges described at the beginning of the guide. The three subcategories are:,Some of these subcategories have seen major technological developments that have taken the BI world by storm. Largely because of the low cost of implementation and because they don't require IT support; ease of use is another key feature encouraging rapid adoption. This means BI has become more accessible to SMB?€?s and enterprises alike where couple years ago the cost and time consuming process made it affordable to enterprises only. This advancement in technology also means there are great number of innovative tools hitting the market with greater speed.,Since our publication of our??,??for buyers, we've asked??BI??community to share with us??their BI??software usage and spend for the year ahead. Here are the key results from the survey,If you want to see a breakdown of BI software usage and investments this year by company size, Check out the full post on the BI survey results on ,.
.,For about 100 years, publishers have generated most of their revenue from advertising. Prior to the early 1900s, the primary revenue source was audience payments, either from newsstand sales or subscriptions. It is our opinion that the changes in the industry brought about by the Internet and mobile delivery platforms will shift the mix of revenue for publishers, both print and digital, back to an audience-centric revenue model despite the growth in advertising technology and programmatic exchanges.,??,The Internet disruption has changed two fundamental characteristics of the publishing industry:?? the competitive landscape and the addressable market. In a print distribution model, the distribution network of the publishers determines the addressable market. Only a few publishers have nationwide delivery footprints, with most publishers, particularly news media, limited to their regions. When advertising revenue was very high, it subsidized the cost of delivery, which caused the footprints of most publishers to grow beyond the area that it would have been under a customer profitability model. The economics of print distribution networks and advertising revenue that predominantly goes to the publisher with the largest audience, create an incentive for consolidation among print news media, which has enabled most newspapers to achieve a near-monopoly position in their markets.?? The mindset of these organizations was that all circulation was good, since that had been the winning formula. Yes, radio and television provided competition for audience and advertising dollars, but they were sufficiently different to limit the market power of publishers. As digital distribution platforms emerged, they provided a means for publishers to extend their addressable market beyond their delivery networks, and most provided their content to the digital audience for free.,??,As Adam Smith observed in ?€?The Wealth of Nations,?€? specialization grows with the size of the market. Pay television enabled networks devoted to sports, news, cartoons and shopping.?? As the pay-TV market has continued to grow, channels now exist that show nothing but individual sports such as golf, tennis and college football. As publishers extend their addressable markets via digital delivery channels, they are finding their competitive landscape includes specialists in certain forms of content. Their sports page now competes with ESPN and their entertainment and lifestyle sections with , , and Oprah. Most significantly, national and world news are ubiquitous as are weather and financial news. These single-topic competitors are category killers, to use the retail term for focused big box stores, and publishers?€? strategy for competing under the new market landscape requires a focus on differentiation and sophisticated audience analysis. This is true for legacy print publishers as well as pure digital publishers.,??,Digital advertising revenue likely will not flow to publishers to the extent it did during the print era due to the audience size commanded by a few large technology firms, despite the emergence of new advertising technology and digital exchanges.?? Publishers will be able to earn premium advertising rates on inventory that they can define using their own audience data, but much of their traffic is from individuals they do not have a recurring relationship with and thus not much data.,??,These trends suggest publishing will be the next industry to transform its business through the use of customer analytics and yield management, following in the footsteps of airlines, hotels and others that segment their audience and price each according to their preferences and price elasticity. Those industries have desensitized the market to differences in price across customers by business-to-consumer companies, so the market will likely accept that publishers identify their high-value customers and treat them differently with an associated premium price.?? Similarly, publishers will identify customers with very high price sensitivity, whether it be from a lower valuation of the product and diminished willingness to pay or an income constraint that limits their disposable income, and target them with a lower priced and less exclusive product.,??,The analytics these publishers will need to achieve audience revenue growth will be focused on individuals and their behavior as customers. Publishers will need to collect in one place the data on an individual or household wherever it exists in their organization. They will need to form a holistic understanding of that customer?€?s experience, both from the content and advertising side, and they will need to predict how they respond to subscription offers and renewal price changes.?? Advertising revenue should not be written off altogether, and much of the analytics around audience will yield incremental revenue on the advertising side, too, through targeting of premium segments.,??,We believe that print and digital publishers can survive with most of their revenue coming from their audience. Leading publishers already have this business model, and in European markets where publishers never had the near-monopoly status most U.S. publishers have enjoyed, print publishers have long existed with 80 percent of their revenue from their paying print customers. We believe this model is more efficient and productive, with the audience and publishers benefiting from their direct financial relationship, than what has happened to the products and services provided by publishers under the influence of advertising revenue.
From time to time, you still come across someone with the opinion that Big Data is nothing more than a fad, which will be forgotten about soon enough.,You might not expect to hear this from me, but they?€?re actually right. Well ?€? half right, at least!,As I?€?ve written before, I?€?m not actually a fan of the term ?€?Big Data?€?, which puts overemphasis on the importance of size. Anyone who?€?s been reading my articles for a while will know that I?€?m firmly of the opinion that what you do with your data, is far more important than how big it is.,And I am sure as more people realize this ?€? as working with extremely large datasets increasingly becomes the norm, rather than something new and exciting ?€? the term ?€?Big Data?€? may indeed fall out of use.,The fact is, Big Data isn?€?t something which has appeared overnight. Ever since we invented digital data storage in the 60s, the amount of data we have been dealing with has increased exponentially with time.,Even before that, if you really want to go right back to the beginning, ancient civilizations strove to horde as much knowledge as they could in great libraries.,Our urge and ability to collate and analyze information seems to have always been something which has distinguished us from other animals, and it isn?€?t going to leave us any time soon.,So the value of information, which is based on data, has always been apparent. However during the last half-century we certainly experienced a rapid acceleration in our capacity to both store larger amounts of data and analyze it in smarter ways. First with the encroachment of digital storage and microprocessors into every aspect of society, then with the Internet and smart phones and now, wearables.,No doubt, there is a lot of hype around Big Data. Big names and brands which have emerged onto the market have profited from this, pushing their own ideas of what Big Data means and how you should go about it.,Wherever you find hype, you find hot air. This is particularly true in the tech world where those who get on board first stand to make huge amounts of money. Not everything that emerges in the early days of something as game-changing as the ?€?Big Data revolution?€? will stand the test of time.,I assume most of my readers are probably old enough to remember the final decade of the last century, when the internet really got popular.,Those of us who had grown up using computers realized immediately how revolutionary it was going to be ?€? that in fact, nothing would ever be the same again. But pundits from outside of the tech world ?€? somewhat wary of the grandiose claims being made ?€? continued for a long time to insist that it was a passing fad.,Many saw the bursting of the dot-com bubble as validation of that belief. But, although a lot of people lost a lot of money, the internet, as we all know, endured.,In the digital age, other terms have risen in popularity only to drop into disuse as the principles they represent become adopted into everyday business use. They?€?re just adopted into everyday business life to the extent that we don?€?t need specific names for them any more. Or ideas behind them are rolled up into newer buzzwords. The big data buzzword has definitely swallowed up most aspects of previous buzz terms like management information systems, business intelligence, or analytics.,So, while in 10 years?€? time the terminology might have changed, we will still be talking about data, and analysis, and the juicy insights we get when we mix them together.,So when someone tells you that ?€?Big Data is just a fad?€?, what they mean is that we won?€?t need the label any more. Data strategy, collection, storage and analysis are here to stay. As will be the businesses which are savvy enough to use it to strike while the iron is hot.
The Internet of Things may be giving over to the Internet of Everything as more and more uses are dreamed up for the new wave of Smart Cities.,In the Internet of Things, objects have their own IP address, meaning that sensors connected to the web can send data to the cloud on just about anything: how much traffic is rolling through a stoplight, how much water you?€?re using, or how full a trash dumpster is.,Cities are discovering how they can use these new technologies ?€? and the data they generate ?€? to be more efficient and cost effective in many different ways. And it?€?s a good thing, too; some estimates suggest that 66 percent of the world?€?s population will live in urban areas by the year 2050.,These are cutting edge ideas, but here are some of the most fascinating ways Smart Cities are using big data and the Internet of Things to improve quality of life for their residents:,This is just the beginning of the integration of big data and the Internet of Things into daily life, but it is by no means the end. As our cities get smarter and begin collecting and sending more and more data, new uses will emerge that may revolutionize the way we live in urban areas.,Of course, more technology can also mean more ,. (Anyone see ,, where terrorists hacked the traffic control systems in Washington, D.C.?) The threat that a hacker could shut down a city?€?s power grid, traffic system, or water supply is real ?€? mostly because the technology is so new that cities and providers are not taking the necessary steps to protect themselves.,Still, it would seem that the benefits will outweigh the risks with these new data-driven technologies for cities, so long as the municipalities are paying attention to security and protecting their assets and their customers.,What?€?s your opinion? Are you for or against more integrated technologies in cities? I?€?d love to hear your thoughts in the comments below.,I hope you found this post interesting. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have.
With the supply of analytics talent in the labor force slowly growing to meet demand, a majority of companies are looking within their own walls for solutions. While there is clearly a need for new graduates and data scientists who know the most cutting-edge techniques of analysis, there is just as much of a need for information workers who know both the business and technical sides of an organization.,Current employees who are able to develop an analytics skill set and combine that with their knowledge of the business can be invaluable when moving analytical insights across the ?€?last mile?€? to decision makers. For companies that find it difficult to lure top talent, their analytics capability can make substantial advances by looking differently at their own existing talent.,Companies that are most successful with analytics have a very different approach to hiring, training, integrating and promoting data workers than other organizations. They are pursuing many paths simultaneously to build and maintain a strong corporate analytics skill set. ??The question for all companies concerned with building their analytics capabilities is this: What is your plan for cultivating analytics talent?,To further understand the challenges and opportunities associated with the use of business analytics,??,??conducted its fifth annual survey of business executives, managers and analytics professionals from organizations located around the globe. The survey, conducted in the fall of 2014, captured insights from 2,719 respondents, from a wide variety of industries and from organizations of all sizes. The sample was drawn from a number of sources, including MIT alumni and??,??subscribers.,In addition to these survey results, we interviewed subject matter experts from a number of industries and disciplines to understand the practical issues facing organizations today in their use of analytics. Our interviewees?€? insights contributed to a richer understanding of the data. We also drew upon a number of case studies to illustrate how organizations are using business analytics as a strategic asset. Earlier results from the research appeared in the Spring 2015 issue of the??,.,Our Global Executive Study and Research Report, entitled??,,??can be read in its entirety here, both on the microsite created for the report (open access) as well as a downloadable PDF (registration required). ??,Comments welcome!
Based on my discussions at , , this blog explores the potential of applying Data Science to manufacturing and process control industries. In my new course at Oxford University (Data Science for IoT) and community (, ), I explore application of predictive algorithms to Internet of Things (IoT) datasets.??,The Internet of Things plays a key role here because sensors in machines and process control industries generate a lot of data. This data has real, actionable business value (,). The objective of Smart data is to improve productivity through digitization. I had a chance to speak to Siemens management and engineers about how this vision of Smart Data is translated into reality,??,When I discussed the idea of Smart Data with , ,?? he spoke of key use cases that involve transforming big data into business value by providing context,??increasing efficiency ??and addressing large, complex problems. These include applications for Oil rigs, wind turbines and process control industries etc. In these industries, the smallest productivity increase translates to huge commercial gains. ??,This blog is my view on how this vision (Smart data) could translate into reality within the context Data Science and IoT., ,??At Messe?? Hannover, it was hard to escape the term ?€?, (in German ?€? Industrie 4.0). Broadly, Industry 4.0 refers to the use of electronics and IT to automate production and to create intelligent networks along the entire value chain that can control each other autonomously. Machines generate a lot of Data. In many cases, if you consider the large installation such as an Oil Rig, this data is bigger than the traditional ?€?Big Data?€?.?? ,?? The ?€?smart?€? in smart data is predictive and algorithmic. Thus, Data is the main driver of Industry 4.0 and it?€?s important to understand the flow of Data before it can be optimized, is already a reality. For instance, ??Industrial Ethernet standards like ,, PLM(Product lifecycle management) software like , ??and Data models for lifecycle engineering and plan management such as ,. To extend the Digital factory ??to achieve end-to-end interconnection and autonomous operation across the value chain (as is the vision of Industry 4.0), we need a component ??in the architecture. ??, ,In that context,?? , Is very interesting. The Open Cloud enables ,?? based on the intelligent use of large quantities of data. The , based on in-memory, columnar database provides analytics services in the Cloud. For instance, the ,to increase the availability of machines through online monitoring, pattern recognition, simulation, ??prediction of issues) and ??, ( revealing hidden energy savings potential),While it is early days, based on the above, the manufacturing domain offers real value and tangible benefits to customers. Even now, we see the customers?? who harness value from large quantities of Data through predictive analytics stand to gain significantly. I will cover this subject in more detail as it evolves.??,Ajit''s work spans research, entrepreneurship and academia relating to IoT, predictive analytics and Mobility. His current research focus is on applying data science algorithms to IoT applications. This includes Time series, sensor fusion and deep learning.?? This research underpins his teaching at Oxford University (Big Data and Telecoms) and the City sciences program at the Technical University of Madrid (UPM). Ajit also runs a community/learning program through his company -??
We all love a good show and we will pay to see one. It is all of OUR data! Its our parents and grand parents children, lovers and fighters. Its your salary, places you have lived and relationships you have made. Its a slice of OUR history together as a nation and it is being sold for large sums of money. The money maybe secondary and it would be nice if it is used for improving the accuracy of the data or making it more secure. Not the case. Or maybe promoting the sanitized use of this data for positive change in medicine, law enforcement, public policy, schools and colleges. Instead it is thrown to the lions. Peanuts for the elephants. Cotton candy anyone?,Contact me if you want details, seriously check it out for your self this is for real.??

There are various offerings out there if you want to use machine learning in your analysis nowadays. Nick WIlson spent his internship at BigML comparing three SaaS Machine Learning Services (BigML, Prior Knowledge and Google Prediction API), with WEKA as a?? benchmark. He wrote a series of blog posts about his findings. In his final post he gives a summary of his work, with links to the different blog posts for details. He let me re-blog his summary here.,In the ,, I introduced myself and the machine learning throwdown. BigML hired me to spend some time this summer comparing their service to the competition. I am very pleased that I was able to write my honest opinions even if they were not always in BigML's favor.,That said, are my results completely unbiased? Probably not. I tried to remain objective, but BigML did pay me to do this comparison. I spent some time with them in their office, I ate their snacks, and I drank their , coffee. Use my advice as a starting point, but play around with these services and make your own decision about which one is best for you.,As a reminder, I compared three cloud-based machine learning services: ,, ,, and ,. BigML and Prior Knowledge are both in beta while Google Prediction API has been out of beta for nearly a year. ,, a time-tested application and suite of algorithms for machine learning, was also included in the throwdown to compare the cloud-based services to a traditional desktop application.,My , looked at getting started with each service and importing your data. Some important considerations include the amount of setup and configuration required to get started, the availability of libraries for your favorite programming language, how strict they are about the format of your data, and the amount of data they can handle.,I am incredibly impressed with BigML in this category. Machine learning is not easy, but they have done more than any of the other services to help make this technology accessible to non-experts. I'm not the only person impressed by how easy it is to use BigML. In a ,, Derrick Harris talks about how he was able to analyze data with BigML from his couch with company at his house and two toddlers running around.,My , talked about the process of turning your data into a predictive model. Models range from black box, where all the details are hidden from you, to completely white box where you can see and understand the model and use it to gain insights about your data. Some other important considerations include how easy it is to create and optimize a model, the type of data a model can learn from, and the types of operations supported by the model.,My , was a fun one. I presented the results of computing cross-validation scores indicating how well each service is able to make accurate predictions. Google Prediction API came in first most of the time, but a closer look revealed that the runners-up were usually not far behind. It turns out that the quality of your data is often the limiting factor rather than your choice of service/model. It might be wise to try your own data on multiple services to see which one makes the best predictions, but this is quite time-consuming and nontrivial because they don't all report cross-validation scores using the same metrics. If you really need to squeeze every last bit of predictive performance out of your data, it's probably time to look into hiring a data scientist (,).,My , covered a few miscellaneous topics including stability, cost, support, and documentation. The big surprise here was that all of the services suffered from multiple random failures while I was evaluating them. They all have some work to do in this area. For now, consider using BigML or Weka if you need completely reliable predictions. Both of these options allow you to make predictions offline without worrying about occasional API failures.,If you have been following along with this series of blog posts and haven't tried any of these services yet, what are you waiting for? Find some data that interests you and see what you can do with it! You can use your own data or find some from a source such as the ,.,Which service should you use? I strongly recommend starting with BigML since it is the easiest to use and everything can be done on the website without writing code. Their interactive decision trees let you visually explore models in ways the other services don't even come close to. Check out other posts on , for examples and browse the ,??for inspiration. ,??if you need help or if there are new features you would like to see.,If BigML isn't your cup of tea, please do try the other services. They each have their own unique features so you should be able to find something that works for you. We're at the beginning of an important era where anyone can use data to help them make decisions. The more people that use any of these services, the better it is for everyone!,I hope you have enjoyed reading these posts as much as I have enjoyed writing them. Goodbye and good luck!



Primed to make a huge entrance in 2015, Data-as-a-Service (DaaS) empowers companies with real-time data to overcome tough challenges with data. DaaS is allowing companies to generate real-time insights and revenue from Big Data. Companies commonly report feeling overwhelmed solely by the mere size of big data, not to mention the processes necessary to use the data. This no longer has to be a reality. With DaaS using big data is no longer a couple month long process.,Most companies and risk systems still source data like it was pre 2003. Yet, the opportunity to refine strategic and operational decision making by taking full advantage of Big Data is compelling.,Within this vast amount of information is valuable and available data. This is the new world of Big Data and the information being created can be used in real-time to generate previously unimagined opportunities.,Many organizations still struggle internally with connecting all the dots within this myriad of data. This is where the thinking behind DaaS comes into play.,DaaS is a service approach in which unique and Hard-to-Find Data (HTFD) assets are sourced and structured to deliver a constant stream of qualified prospects, including a company?€?s own customers, who are actively searching for what they are selling. Distinctly different from list buying, these data sources are a highly customized marketing asset versus disconnected, one-time use prospect lists.,In-market prospects and customers are delivered , to a company?€?s channel systems or digital marketing platforms, allowing marketers to can send real-time messaging, personalized recommendations, and highly targeted content.,DaaS combines three types of data which are uniquely customized to each company:,To really understand the potential of unique data sets sourced through DaaS ?€? both HTFD and fast data, it is important to understand where all this data is coming from. The information being generated from Big Data can be segmented into six specific categories:,DaaS mines these Big Data sources to deliver highly customized data assets. Some examples include:,For years, organizations have been reliant on their internal data or data enhancements from list brokers. This is stagnant data compiled from third parties. DaaS on the other hand is transformational in nature - a revolutionary way of mining today?€?s massive data sets to find qualified prospects in the market now for what a company is selling.,So why model families who may be interested in family vacations when you can send campaigns to consumers who just booked plane tickets? Or why try to figure out who to target for a retail campaign when you can receive daily streams of prospects who are actively searching online for products you (or your competitors) sell? The possibilities are endless.,Rather than focusing on developing and managing an intricate network of data, companies can focus on the business outcomes and marketing advantages of Big Data.?? 
Working and valorising Big Data is , for companies that have built their business model on it. For companies that don?€?t compete on analytics, that is for whom analytics is not a core element of their strategy, it?€?s a huge challenge.,But Big Data is the talk of town nowadays. I think that a part of the growing management interest is due to two factors:,In a recent article in SAS magazine, John Knowles from Allianz insurance gives his view on the ??challenges ??to effectively embrace Big Data in existing business models:,I especially agree with the first two arguments. Dashboards, metrics, segmentations,??propensity scores,..??-?? no matter how attractive or good??- require a deep understanding of the business process in which they are supposed to play a role. Data doesn?€?t speak for itself. Data doesn?€?t propose a decision. Data????itself doesn?€?t bring profit.,Instead, considering the right (amount and type of) data for supporting the right decision at the right time is essential to make any investment in a (Big) Data initiative profitable. Said otherwise, relevant Big Data as well as analytical outcomes from it should be fed back into daily business decisions (,).,First the data, then the decision, then the profit.,??
Nice infographics produced by famous business management consultant and author, Bernard Marr. Click on the picture, then click one more time on the picture, to see easy-to-read version.

First, let's start with??,, entitled ,. It describes the following Excel functions:,The second article that we selected discusses several Excel tricks:,??to read the article. Note that the Excel VLookup function ,.,Finally, we found a ,. It discusses the following topics:,Also, check out our section 6 of our ,. We plan to add many additional examples of Excel spreadsheets, some with advanced computations / modeling, and some for beginners.
In order for a business today to remain competitive, it must be willing to embrace new technologies. Using old or outdated technology can leave a business trailing in the dust of those newer businesses that have emerged to the forefront of the industry, especially when reaping the benefits that new technology affords them. Of course, this means that one must also be aware of new technology and how they might benefit your business, which is not always so easy to do. In fact, there is a term for technologies that tend to blindside companies that aren't yet aware of them?€?disruptive technologies.,Although the term "disruptive" has a negative connotation, one should not think that technology is necessarily a bad thing. It is only bad if you don't know how to harness the power and benefits of new technology for yourself, especially when newer, bolder competitors are doing so in order to maintain their own edge in the marketplace. So here is a little insight into some new disruptive technologies that many companies are already embracing, while others aren't even aware they exist.,??,Chances are you've heard of the Cloud, if not in business, then perhaps in a movie, commercial or TV show. The Cloud is everywhere, but not everyone is using it. Not everyone needs to, but businesses definitely should, and if you knew how beneficial it could be for your own business productivity, you'd be using it too. Don't understand the Cloud? That's okay, not many people do. What's important is knowing how to use it, and knowing what it can do to change the face of how we do business.,??The Cloud is much more than a form of online storage. Using cloud-based applications, you can share documents with others, edit them from any mobile device at any time, communicate better with team members, colleagues and clients, manage teams and projects in real-time, and much more. Take some time to learn about the cloud and some ,?€?you'll be glad you did when productivity improves significantly.,??,Why waste expenses on unnecessary printing, faxing, mailing of documents, and time waiting for documents to be mailed back, when you can transfer documents and have them signed within minutes by utilizing paperless business technology? Features such as digital signage allow you to send a document to someone in a form that they can easily open and view on their own PC, and subsequently sign it digitally and send it right back. This of course saves both time and expense, and mobile options enable this to be done anywhere, at anytime. Invoicing, billing, and other business tasks that used to require immense amounts of ink, paper and mailing can now be done digitally.,The standard business telephone system is a thing of the past. Virtual phone systems, which are phone systems that work over the Internet rather than through traditional telephone lines, are not only more cost-effective, but also offer you more features and functionality that can improve communication, productivity, and marketing efforts. ,, you'll gain a wide host of features that can make your company appear more professional, as well as ensure that time is not wasted waiting for employees and team members to return phone calls when they are out of the office. With call forwarding and mobile phone numbers, you can be sure to be able to reach your employees no matter where they might be.,With consumers finding more businesses these days on their mobile devices rather than through their desktop PCs and the ancient tome you might recall as the Yellow Pages, it makes sense to have a mobile web presence for your business, and possibly even a mobile app, depending upon your industry and what you offer your clients and customers. Additionally, you will find that productivity improves when employees are able to conduct their work on their mobile devices, which can easily be done by way of cloud-based applications, and communication facilitators such as virtual phone systems. See how today's technology all works in conjunction?,Big data is essentially a multitude of data collected and compiled from multiple sources, which can then be used by , as well as better understand current ones. The information gathered can be used for better marketing and sales forecasting, as well as developing new methods of doing business to be better prepared for possible future events. The analytics help companies and organizations work smarter, and work faster, with more efficiency and cost-effectiveness.
In a meeting with Airbus last week I found out that their forthcoming A380-1000 ?€? the supersized airliner capable of carrying up to 1,000 passengers ?€? will be equipped with 10,000 sensors in each wing.,The current A350 model has a total of close to 6,000 sensors across the entire plane and generates 2.5 Tb of data per day, while the newer model ?€? expected to take to the skies in 2020 ?€? will capture more than triple that amount.,In an industry as driven by technology as the aviation industry, it?€?s hardly surprising that every element of an aircraft?€?s performance is being monitored for the potential to make adjustments which could save millions on fuel bills and, more importantly, save lives by improving safety.,So I thought this would be a good opportunity to explore how the aviation industry, just like every other industry, is putting data science to work.,There are 5,000 commercial aircraft in the sky at any one time over the US alone, and 35 million departures each year. In other words the aviation industry is big. And given that every single passenger on each of those flights is putting their life in the hands of not just the pilot, but the technology, the safety measures and regulations in place are extremely complex.,This means that the data it generates is big, and complex too. But airlines have discovered that with the right analytical systems, it can be used to eliminate inefficiencies due to redundancy, predict routes their passengers are likely to need, and improve safety.,Engines are equipped with sensors capturing details of every aspect of their operation, meaning that the impact of humidity, air pressure and temperature can be assessed more accurately. It is far cheaper for a company to be able to predict when a part will fail and have a replacement ready, than to wait for it to fail and take the equipment offline until repairs can be completed.,In fact, Aviation Today reported that it can often take airlines up to six months to source a replacement part, due to inefficient prediction of failures leading to a massive backlog with manufacturers.,On top of this fuel usage can be economized by ensuring engines are always running at optimal efficiency. This not only cuts fuel costs but minimizes environmentally damaging emissions.,In the case of Airbus, they partnered with IBM to develop their own Smarter Fuel system, specifically to target this area of their operation with Big Data and analytics.,Additionally, airlines closely monitor arrival and departure data, correlating it with weather and related data to predict when delays or cancellations are likely ?€? meaning alternative arrangements can be made to get their passengers where they need to be.,Before they even take off, taxi times between the departure gates and runways is also recorded and analyzed, allowing airlines and airport operators to further optimize operational efficiency ?€? meaning less delays and less unhappy passengers.,This sort of predictive analysis is common across all areas of industry but is particularly valuable in commercial aviation, where delays of a few hours can cost companies millions in rearrangements, backup services and lost business (The FAA estimates that delayed flights cost the US aviation industry $22 million per year). ??,Specialist service providers have already cropped up ?€? , is one ?€? aiming to help airlines and airports make the most of the data they have available to them.,They aggregate data sets including weather information, departure times, radar flight data and submitted flight plans, monitoring 100,000 flights every day, to enable operators to more efficiently plan and deliver their services.,In marketing, too, airlines are beginning to follow the lead of companies such as Amazon by collecting data on their customers, monitoring everything from customer feedback to how they behave when visiting websites to make bookings.,Now we are used to generating and presenting tickets and boarding cards through our smartphones, more information about our journey through the airport, from the time we enter to the time we board our flight can also be tracked. This is useful both to airport operators, managing the flow of people through their facilities, and to airlines who will gather more information on who we are and how we behave.,So businesses in the aviation industry, including Airbus, are making significant steps towards using data to cut waste, improve safety and enhance the customer experience. 10,000 sensors in one wing may sound excessive but with so much at stake ?€? both in terms of profits and human lives ?€? it?€?s reassuring that nothing will be overlooked.,I hope you found this post interesting. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have.

The objective of my final project at Metis from weeks 9 to 12, is to??,??- their driving style and the type of roads that they follow.,The challenge associated with this objective is to??,??(and hence his proper ?€?driving behaviour?€?)??,located inside the car.,My idea to solve this issue is to experiment Topic Modeling techniques especially??,??(LSI/LSA) and??,(LDA) and explain the observed trips by the unobserved behaviour of drivers.,The following is an executive summary ?€? you can also browse throught the??,??on the 7th of April 2015 during the Career Event, or check the Python code available on my blog:??,The raw data received for each trip is a csv file of (x,y) coordinates logged every one second.,My approach consists of first preprocessing the data using statistical smoothing and compression algorithms:,- Kalman Filtering and,- Ramer?€?Douglas?€?Peucker,,then extracting Road and Driving Style features:,- per Segment: Length, Slip Angle, Convexity, Radius,- per Meter: Speed, Accelerations (tangential and normal), Jerk, Yaw, Pauses,then, binning the ouput to generate the ?€?Driving Alphabet?€? (ex: d0, d1, d2?€? v0, v1, v2?€? a0, a1, a2?€? etc),,and finally, building the Driving Vocabulary - made of ?€?Driving Slides?€? (ex: d3L4v2n3y1) for various preprocessing sensitivities or features combinations (the langages).,Then I translate trips from GPS log into documents; tokenize, filter, ?€? the data is ready!,I will use the GENSIM library to transpose trips into an LDA or LSI space where each trip becomes a combination of ?€?Driving Behaviours?€? made of ?€?Driving Slides?€?.,In order to validate my model I am using it to compete in the AXA Kaggle challenge where I need to come up with a ?€?telematic fingerprint?€? capable of distinguishing when a trip was driven by a given driver, knowing that among the 200 provided trips for each of the 2736 drivers, a few number of trips was not driven by this driver.,Submissions are judged on area under the ROC curve calculated in a global manner (all predictions together).,My approach is the following:,- transpose all trips into the new Driving Behaviours Space,- take one by one each trip from a selected Driver,- build a prediction model trained with all other trips in the dataset:,Trues if they belong to the selected Driver,Falses if they do not belong to this Driver,- predict with the trained model, the belonging of the selected Trip to the Driver, then Ensemble several predictions using various sensitivities to enhance the score ?€?,For performance reasons I will proceed by batches of 10 or 20 selected trips and compare each time to a randomly selected limited number of False trips.,Other outlier detection / clustering techniques appear to be less performing,3.3 M generated documents are kept in a MongoDB and parallel processing set up on 4 DigitalOcean Droplets with 8CPU each.,An AUC of 0.9 has been measured by Kaggle without any ensembling technique which confirms the robustness of this approach ?€?,Philippe.
It?€?s all well and good to talk about customer experience and managing inventory flow, but what has big data done for me??,?,I?€?ve rounded up seven of the most interesting ?€? and unique ?€? applications for big data I?€?ve seen recently and how they may be impacting your life.,Outdoor marketing company??,??is using big data to define and justify its pricing model for advertising space on billboards, benches and the sides of busses. Traditionally, outdoor media pricing was priced ?€?per impression?€? based on an estimate of how many eyes would see the ad in a given day. No more! Now they?€?re using sophisticated GPS, eye-tracking software, and analysis of traffic patterns to have a much more realistic idea of which advertisements will be seen the most ?€? and therefore be the most effective.,Apple?€?s new health app, called??,, has effectively just turned your phone into a biomedical research device. Researchers can now create studies through which they collect data and input from users phones to compile data for health studies. Your phone might track how many steps you take in a day, or prompt you to answer questions about how you feel after your chemo, or how your Parkinson?€?s disease is progressing. It?€?s hoped that making the process easier and more automatic will dramatically increase the number of participants a study can attract as well as the fidelity of the data.,The website??,??combined public information from the U.S. Department of Agriculture, municipal tree inventories, foraging maps and street tree databases to provide an interactive map to tell you where the apple and cherry trees in your neighborhood might be dropping fruit. The website?€?s stated goal is to remind urbanites that agriculture and natural foods do exist in the city ?€? you might just have to access a website to find it.,Ski resorts are even getting into the data game. RFID tags inserted into lift tickets can cut back on fraud and wait times at the lifts, as well as help ski resorts understand traffic patterns, which lifts and runs are most popular at which times of day, and even help track the movements of an individual skier if he were to become lost. They?€?ve also taken the data to the people, providing websites and apps that will display your day?€?s stats, from how many runs you slalomed to how many vertical feet you traversed, which you can then share on social media or use to compete with family and friends.,Applications have long used data from phones to populate traffic maps, but an app called??,??taps into sensors already built into Android phones to crowdsource real time weather data as well. The phones contain a barometer, hygrometer (humidity), ambient thermometer and lightmeter, all of which can collect data relevant to weather forecasting and be fed into predictive models.,Whether you want to hang with the hipsters or avoid them, Yelp has you covered. With a nifty little search trick they call the??,, you can search major cities by words used in reviews ?€? like hipster. The map then plots the locations for the reviews in red. The darker the red, the higher the concentration of that word used in reviews ?€? and when it comes to hipsters, ironic tee shirts and handlebar mustaches.,Website??,??is using big data to help women find better fitting bras. Statistics show that most women wear the wrong bra size, and so the website has stepped up to try to solve that problem. Customers fill out a fit questionnaire on the site, and based on the responses, an algorithm suggests a selection of bras to choose from. The company?€?s in-house brand is even developed and designed based on feedback from customers and data the company has collected.,The possibilities of using big data are endless and it might be time to find the big data applications in your business. Have you seen any fascinating or unusual big data projects lately? Let me know about them in the comments below!
Cosine Similarity is a measure of similarity between two vectors that calculates the cosine of the angle between them. Similarity ranges from ???1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.,We have shared data sets, sample code & an example case study in implementing Cosine Similarity.,:,We are looking to find a place to settle down in California. We like a place called Montecito, CA and want to find similar towns & cities to look for places. How would we go about doing it?,We would first try & understand the factors & variables important to us. Variables could include median value of home, percentage of homes that are 2/3/4 Bedroom, Age of home etc?€? We prepared a sample set of fields & example values to give you an idea of the variables we used.??, , , , , , , , , , , , , , , ,The Full Data Set to test the Cosine Similarity Algorithms??,.??
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
Ever wanted to use Excel to examine big data sets? This tutorial will show you how to analyze over 300,000 items at one time. And what better topic than baby names? Want to see how popular your name was in 1910? You can do that. Want to find the perfect name for your baby? Here?€?s your chance to do it with data.,There are professional data analysts out there who tackle ?€?big data?€? with complex software, but it?€?s possible to do a surprising amount of analysis with Microsoft Excel. In this case, we?€?re using baby names from California based on the United States Social Security Baby Names Database.??,??As you?€?ll see, the Social Security database, which goes back to 1880, has some weird and wonderful anomalies that we?€?ll discuss.,This tutorial is for people familiar with Excel: those who know how to write, copy and paste formulas and make charts. If you rarely venture away from a handful of menu items, you?€?ll learn how to use built-in Excel features such as filters and pivot tables and the extremely handy VLOOKUP formula. This tutorial focuses on what?€?s called ?€?exploratory analysis?€?, and will clarify the steps to take when you first confront a huge chunk of data, and you don?€?t know in advance what to expect from it. We?€?ll also show you how to use these tools to find the flaws in your data set, so you can make appropriate inferences. If you want to improve your Excel chops with some big data exploration, you?€?re in the right place.,??This tutorial uses Excel 2013. If you?€?re using a different version, you may notice some slight differences as you go through the steps.
I just got back from my vacation in Barcelona, Spain where I spent about 3 days, then rented a car and drove up north through the South of France. My last stop was Nice, France. The trip was a lot of fun and now I intend to find some data to help bring back great memories (hm...it sounds more geeky than I thought but anyway).,Barcelona is located in Catalonia region of Spain famous for its earthy dry reds as well as Cava - world's most delicious bubbly drink. I am a big wine fan which is why I thoroughly enjoyed the trip and decided to write about wine today.,If you wanted to analyze any wine-related topic which one would you choose? Would you map popular vineyards around the world on the heatmap? Or maybe you will play with different grape varieties and figure out which ones people like the most?,Every time I enter a wine store I always wonder - why do certain wines get carried by almost every liquor store and others look brand new to me (I've never seen them in other stores)?,So what does determine a given store's wine selection? Do managers already buy popular wines most people drink and are, therefore, trying to "spice up" their menu with bottles other stores will definitely not have? Or maybe they study customers and select bottles based on consumer preferences?,My big question is??,What I am trying to solve here is a basic economics problem about demand, supply and equilibrium. If a store wants to be successful it will eventually buy only wines customers prefer and will sell in quantity and at the price optimal for the market. However, in order to find that equilibrium I need data for both supply and demand.,??is one of the biggest wine e-commerce retailers in the US. It allows customers to buy wine according to any price range, grape variety, country of origin, etc. It even has a??,??that helps developers gain access to a large wine database and sync it up with their applications. In their wine catalog wine.com captures the following data:,In the past I received quite a few emails from the readers asking for datasets I used in my analyses. Now I figured out a way to give my readers access to the original data right in my blog post. So I am happy to announce,A few stats about the dataset:,From the interactive visualization below we can see that only wines from US, Western Europe, Argentina, Chile, Australia, South Africa and New Zealand are sold in the US. No Eastern Europe (how one can forget amazing Kindzmarauli from Georgia?), Asia or Middle East were on the list. I wouldn't be surprised if I saw this heatmap 2 weeks ago, but during one of my trips to spanish vineyards our guide mentioned that China is a rapidly emerging wine market and is expected to be the world's largest producer in the next 5 years.,A cool thing about this visualization is that you can pick any grape variety you like and check out where this wine is being produced the most. For example, I really like Merlot but I've always thought that it is mainly produced in the US, France and Italy. From the map it looks like Argentina and New Zealand also produce this wine which gives me another reason to go to a wine store this evening.,Bottom left scatter plot shows each bottle plotted based on price and rating it received from customers. If you look at all types of wine you may describe relatioship between price and ratings as y = ?????x, but if you start segmenting wine by each type you will see diverging patterns. For example, with red wines it is generally true that higher price results in higher rating scores, but with white wines we see that some cheap bottles under $30 receive 90+ rating scores (e.g. Emilio Lustau Sherry from Spain) and some expensive bottles for $55 barely hit 90 in rating score (e.g. Louis Jadot Chardonnay from France). With bubbly drinks things get even more interesting. The pattern looks similar to red wine but is taken to the extreme. Vast majority of sparkling bottles are not pricy and get ratings somewhere in mid 90s, but there are also some serious outliers like Dom Perignon Limited Edition or Louis Roederer Cristal Brut that will cost you $200+ and will be worth the price.,Finally bottom right chart is our top 15 wine list (based on customer ratings) for crazy wine lovers. Gold, Silver and Bronze from different??,??all go to France and also cost a fortune. But there is one spanish bottle in the 11th position which I may try tonight as it is relatively cheap compared to its peer winners -,.,Now when I played around with data I can start answering my question about??,. I will make the following assumption:,So my analysis question is:??,I will try the following model for my hypothesis testing,I used??,??function to perform the analysis and assess each factor's impact on the post's popularity. Here is the output of my regression analysis.,R-squared in this model was 0.53 which is pretty good given we only had price of a bottle, reviews and discount as potential predictors of a rating score. We see from the result output that baseline rating score of wine.com bottles is 87.5. Every dollar added to the price increases bottle's rating by ???0.72 = 0.85 points and each additional review bumps rating score by 0.04 points (or 25 additional reviews increase rating by 1 point). The latter finding is probably not that actionable since it takes a lot of reviews to move rating just a little bit. Presence of a discount does not affect wine's rating score.,But remember I mentioned earlier that depending on the type of wine rating score can have very different relationship with price? So I decided to run individual regressions on each wine type. Here is what I found,For both red and sparkling wine my model was pretty well explained by the data I had. R-squared with reds was 0.58 and with bubbly it was 0.77. But in case of white, ros?? and dessert wines R-squared was significantly lower (0.3, 0.3 and 0.43). So while my overall model was sufficiently well explained by the data when I dug into each wine type I got somewhat high variability in the ability of the data to explain my dependent variable.,Coefficients also had different magnitude. While final price was always a significant predictor of a rating score, it affected ros?? wines to a lesser extent than dessert wine. # Reviews did not explain wine's rating score in every wine type. For example, it did not affect ros?? and dessert wine ratings. It did, however, affect sparkling wine rating score negatively, but given how little this variable changes rating score overall, this finding may just be an artifact of my data.,The most interesting variable was presence of a discount. Remember it did not correlate with rating score in my general model? Once I segmented by wine type, it turned out to be a significant predictor of white, ros?? and sparkling wine ratings. Discounted bubbly wine was rated higher while discounted white and ros?? were rated lower.,Based on all findings I gained from this analysis, here is my insight into??,While my findings may or may not benefit this large online retailer, I'm sure Wine.com is already doing a bunch of analytics on their sales operations. Local businesses are the ones who would benefit from this type of analysis and will be able to tailor their sales strategy to customer needs if they understood them better.,According to??,, there are about 1,150 local wine stores in New York. They all have websites and try to drive customers into the store. Most of them don't collect data for analytical purposes and don't use data to make critical decisions. I would love to give them analytics they need but there is one problem. Unlike Wine.com local businesses follow different website structure and, therefore, scraping needs to be customized for each and every website. I can't automate and scale scraping programmatically, but we can do it together! ???,:,Once you upload your datasets I will injest them on my end and will aggregate them into 1 master dataset which I will then email to all contributors.??,This is a pilot batch of 25 wine stores so if each of us spends half an hour to scrape one website then in 30 minutes we will have a dataset one person will have to spend 25*0.5 = 12.5 hours on.,Originally posted ,.
In the old days, you knew who to contact to make business. There were a few job functions, and a few levels of seniority. A name with job function was usually enough for you to hit the bullseye more often than not. But as businesses diversify and become more complex, this becomes more difficult; and even for the ?€?data-aware?€?, old habits die hard.,I had an interesting conversation this week with a potential business partner I was trying to reach out to. This was an organisation that I had done a bit of old school research on, understood their target market and modus operandi as best I could from their website and LinkedIn, and decided that it was definitely worth investigating further. So with my ?€?sales research?€? head on, I made the call to their London office with a name & job role in my hand that I thought would give me more information and might be able to progress the idea of a partnership. When researching, I always start with people who can give me the most information and confirm / correct my assumptions, rather than diving in cold.,So I speak with John, tell him what I do and where I think there may be a great fit with his company. John goes silent for a moment, and then puts his hand across the receiver and says to his colleague ?€?this chap wants XYZ, what?€?s the name of the fella that deals with this? He?€?s in Europe right??€?,After a bit more muffled deliberation, John gives me the name and number of the man who thinks he can help me, though he?€?s not sure of his exact role but it?€?s ?€?definitely more his area than mine?€?,In this reality, I put a call straight through to my newly identified contact. The vagueness of the information made me think ?€?I?€?ll call this guy, he?€?s probably not the absolute contact, but I?€?m definitely one step closer?€?. We have a great conversation over 20 minutes once he gets the general idea of my proposition. He?€?s a lovely guy, and it turns out that what I?€?m proposing is possibly something that would be of mutual benefit to our companies. I send him an outline of how we work and we promise to speak again in a week, although I may have to bear with him as he is travelling a lot and has a fair few other priorities. As we are closing the call I tell him I?€?m just dropping on to LinkedIn to connect with him. And that?€?s when I discover that he is the CEO of this business.,In my alternate reality, I go to LinkedIn 1,to search specifically for this name. I discover that ?€?that fella in Europe?€? is the CEO. This is all well and good, but I then decide to get more of a hook. I search for him in our online business universe (which I can use to populate CRM automatically).,, as well as 7 web references related to him that lay out his history and future vision for the business in blogs, interviews and articles. Armed with this information, I sharpen my pitch substantially, and make my call.,Data is the lifeblood of the modern economy, but the arrangement of that data into information and knowledge is what really helps to make those better, more memorable connections. Research is key. The important point here is that, for the busy sales professional, that information is available in neatly packaged bundles. Somebody else has done it for you, so that you can make more calls and enter validated, more accurate, more,. No longer do you need to spend vital time trawling the internet, if you have the condensed information at your fingertips. And for the data-shy salesperson, you don?€?t even have to re-key the information anymore. You still have to click the ?€?update CRM?€? button with your hand however, but I?€?m sure someone somewhere is working on that.
What on earth makes the world go round?,Well, your guess is not far from the truth! Gone are the days when big machines were the gizmo for nerds and geeks! Today, there is a revolutionary paradigm shift in favor of small machines with capacity for big data!,Businesses are going online at a rate never imagined before in the entire history of the universe. Who thought that electronic commerce would be the most effective way to transact business from the developed world to the least developed world across the various corners of the globe--does the globe have corners anyway?,Big Data has provided numerous answers to a myriad of questions and reflections of ordinary and not so ordinary people. The power of computing coupled with the essence of predictive analytics has now brought to life numerous careers in the computing world! What is more, the so called corners of the globe have vanished and a global village has been created!,Now it is never too far if you live in Japan and want to shop in Sweden. Imagine you are riding a boat at the spectacular Kenyan waters off Mombasa island as you socialize through Skype and other social media platforms to other friends in Europe, America and Asia. Would all these be possible if Google, Yahoo, Microsoft and Facebook were not leveraging the power of Big Data?,Finally, let me say Big Data will be the next president of the United Sates....you see any aspiring president of the U.S in this day and age has to get Big Data experts. They have to intelligently sift through terabytes of internet data and tailor custom-made campaign messages that resonate with the electorate. Thus, any serious presidential candidate has to leverage the power of Big Data to win big!,Last but not least, if you are reading this article, it is because of Big Data! I will tell you why when I write the next article! However, you can use Big Data to find out for yourself!,By Christopher Alvin Mokaya,e-mail: afromediablog@gmail.com

Interesting Infographics received from??,, regarding a complex machine learning problem: automated translations.
This guide was originally posted on the ,. It was written as a how-to guide for using RapidMiner and AYLIEN to scape and Analyze online content.,One of the major challenges with mining the Web and Social Media for insights is trying to get all of your data into one place. To do this, you need to extract information from multiple sources in order to gain an accurate and holistic view.,Combining multiple data sources and analyzing their content can be a daunting task, but thankfully data mining frameworks such as , and , make it easy to extract information from multiple sources in a quick and straightforward manner.,In this blog post, we're going to show you how to use AYLIEN's , from within RapidMiner to analyze text gathered from sources on the web.,The , for RapidMiner provides access to internet sources like web pages, RSS feeds, and web services. In this tutorial, we're going to use it to make HTTP requests to the Text Analysis API. In part 2 we will use it to scrape information from web pages such as Rotten Tomatoes.,The Web Mining package provides you with an operator for invoking external web services. This operator is called "Enrich Data by Webservice" and can be found in the Operators panel under Web Mining > Services > Enrich Data by Webservice.,Here we are basically calling the , endpoint of the Text Analysis API to analyze the sentiment of some text in order to find out if it's ,, , or ,.,Now that our API call is setup, we need to provide the operator with some input text.,Now that we have everything setup, it's time to run our process by clicking the Run button.,As you can see, "I love puppies!" was deemed to be positive and the result is now accessible in RapidMiner for further analysis and reporting. You could use one of the many other methods provided in the Text Processing package to generate any number of documents and analyze their sentiment in the same fashion. Also, by changing the , parameter in the API call you can access any other endpoint from the Text API (Concept Extraction, Classification, Summarization and so on).,In the , of this series, we're going to crawl Rotten Tomatoes with RapidMiner to extract movie reviews and analyze their sentiment to gain some interesting insights.
The quote from Sun Tzu seems to suggest how a military leader gathers data; adapts to different situations; and makes decisions weighing the circumstances. It says that the balancing of chances depends on "calculation." I notice that military leaders are sometimes portrayed almost as dictators: the person in command asserts his or her authority leaving others to silently obey. They might be described as "calculating" although I haven't heard about nerdy military leaders relying heavily on their calculation skills. Perhaps leadership has become popularized as the ability to direct others rather than balancing competing risks and opportunities. In this blog, I will be separating two roles that some might lump together: 1) the manager's role to set criteria sometimes in the absence of data; and 2) the advisor's role to confirm the effectiveness of criteria on the presence of data. I will approach the distinction mostly in relation to the process of "quality control," where I consider the separation straightforward and easy to explain. In practice, the idea of quality is about defining and labeling: e.g. suitable versus non-suitable; acceptable versus non-acceptable; compliant versus non-compliant. On the surface the focus seems to be on the products being tested or checked. But actually, quality "control" is really about control over criteria: it is this control that determines how products are recognized. Any data collected pertains not to quality as an absolute but rather how facts relate to the criteria.,An ,??describes how airport security personnel have been using a behavioural scoring system to determine which passengers might be terrorists. I know this might not leap out as a quality control situation. I would say that comparable processes are involved. There is a standard or listing of criteria. Instead of applying this list to different products, it is used on passengers. The scoring represents "criteria-driven recognition" of terrorists. Some people would question if the application of such criteria is genuinely effective. I certainly believe that airport security requires some kind of criteria to operate. I merely point out how having criteria does not in itself mean success. Airport security might be doing precisely the opposite of screening for terrorists: it could be setting a straighter path for them to follow or not follow. In the allocation of resources towards the detection of aberrant behaviour, airport security seems to require that terrorists be distinguishable from the general population by superficial characteristics. The Nazis distinguished between Aryans and other people using ,. Arguably, this process holds some similarities to screening methods for quality control.,The presence of an institutional response does not mean that the underlying problem, issue, or concern has actually been addressed. When organizations decide to collect "only the most relevant data," it is important to recognize that in fact they might be collecting just the opposite. The setting of criteria has nothing to do with adaptation or improvement but rather ensuring that a particular course of action or method is pursued. The ,??was considered by many as an act of religious extremism and maybe terrorism. Some conversations in Canada questioned whether such cartoon portrayals represent a hate crime against a particular cultural or ethnic group. It's not a radical assertion. Even Pope Francis had some ,. The social contexts of two nations might attach different meanings to the exact same events. It's possible that the disenfranchisement of minority groups can contribute to violent outbursts and deviant behaviours; but this reality would be "invisible" if social conventions allow the events to be construed only as acts of terrorism. Thus, the ensuing institutional response in France would seem to entirely support and reinforce preexisting social fragmentation. I could for instance just for laughs do a cartoon portraying French children as prostitutes.,The term "quality" is used in relation to the outcomes of production. When discussing the contributions of people in production processes, the usual term is "performance." Consequently, there are conceptual commonalities between performance and quality. I recall ,??previously approved by a standards agency. The helmets conform to criteria as embodied by our safety standards. The new research indicates that most of helmets seem to provide little if any protection from a type of injury common to those that play hockey. In human resources, there is a question on the extent to which tests can or should be used to distinguish between employees. In Canada, in a ,, the employee was terminated from her job due to her inability to pass an aerobic test; however, it was determined that the testing was unrelated to her ability to perform her job. The employer was found to be in violation of human rights. I imagine that in both cases, the scientists involved in the testing did not refute but rather promoted their tests regimes. They weren't actually testing to gain insights but rather to confirm particular assertions. What is true of science is likewise the case with quality. Quality isn't in the object being tested but rather the criteria applied during testing.,In the United States, there was a multi-billion dollar ,. I am familiar with a ,. I would like to contrast the budgeting criteria against those of interested stakeholders. The allocation of funds might be determined on the basis of engineering risk assessments; this is not necessarily in agreement with regional objectives perhaps more aligned with long-term development. Thus the data obtained to determine compliance on the basis of one body of criteria might be out of place in relation to another even using exactly the same facts. Perhaps the same can be said of stimulus spending where one stakeholder might be interested in using money to fund expansion of transit services while the body providing the funding could be more interested in generating employment opportunities outside the city core. The data collected does not have an absolute value that is shared and interpreted the same way by all parties. This furthers the idea that data exists to support a particular management regime; this data tends to reflect the priorities and objectives of those that set the criteria.,Similar types of conflicts can occur within a particular agency for example in the department responsible for the military spending: should $20 million go towards the clean-up of a firing range or to dig up leaking underground storage tanks? If the criteria giving rise to quality conformance data were genuinely relevant (that is to say, connected in some way to reality), it should be possible to draw comparisons based on the nature of the relevancy. One would know whether the benefit of a particular project exceeds the benefit of another. But the application of conformance criteria is not at all about benefit. It's about ensuring implementation and completion in a specific way by a specified time. Since the conceptual assertion of "quality" from conformance data extends from criteria, this particular type of data would tend to reinforce contextual focus established by management. By the way, when I use the term "management," I do not mean people but rather the activity of managing things. Those individuals responsible for managing a firing range are not necessarily managers in a personnel administration sense. So "management criteria" are the standards, rules, guidelines, requirements, practices, policies, and procedures that determine how resources are managed.,In all of the cases above, the criteria serve to shape reality and therefore the meaning of any data collected. Criteria can cause somebody with certain behavioural characteristics to be labeled a terrorist. It can make products appear safe. A good employee can be portrayed as inadequate. Suffice it to say, there are many examples of criteria being used to assess aspects of suitability. Sometimes, the criteria are out of place. Or perhaps, they were appropriate at one point only to become less so as a result of change. I have described "projection" as the regime where criteria give rise to data relating to prescriptive conformance. I call the resulting data the "metrics of criteria." The prevalence of projection highlights the need to have certain specialists serve as a balance for managers. I'm unsure if these advisors should be described as managers themselves since they don't necessarily manage anything. However, there should always be somebody or perhaps something filling the advisory role.,A manager responsible for a particular production line might decide, after discussions with executives, that certain data must be collected for compliance while additional data would be desirable to have for decision-making. In terms of compliance, there are health and safety standards that necessitate the collection of data such as the number of accidents and injuries. This manager is also concerned about attendance, inventory, and production. It should be apparent in this example, it is not necessary for a manager to have data in order to set criteria. In fact, this manager is setting the criteria in order to collect data. It is quite difficult to separate the criteria from the intent of the resulting data: the data is meant to help the manager satisfy specific needs. These needs precede and give rise to the data. Production demands dictate what data gets counted and therefore what aspects of reality materialize on the radar.,There can be a separate individual, an advisor or specialist, sometimes responsible for the collection of the data but in any event responsible for the data. If the arrangement in an organization is hospitable, this advisor might be able to carry out the following:,i) confirm the extent to which the data collected seem to satisfy management criteria;,ii) determine what criteria the data collected seem to genuinely satisfy; and,iii) ascertain the type of data that should be collected to satisfy the criteria.,Notice how I avoid a debatable point: iv) revise or create new criteria in response to the data collected. Some probably consider this last item more of a management function; yet it really seems to belong on the advisor's list. This leads me to suggest that in certain respects, the advisor can in fact perform a management function maybe even better than a traditional manager. I believe that there is a spectrum of different types of jobs pertaining to projection. Perhaps a large number of jobs are more related to the regime containing the advisor, which I have described in previous blogs as "articulation.",It is understandable to think that effecting control over a production environment might also yield control over the outcomes of production: e.g. that tightly regulating the pace of an assembly line might help the manufacturer assert or maintain its placement in the market. The fact that an organization conforms to a regiment of behaviours can provide no assurance of success. If a company is struggling and headed in the wrong direction, following prescriptive criteria might guarantee failure. If an organization is experiencing criteria-related difficulties, I doubt that any amount of restructuring can alter the outcomes of production. It is possible for an organization to produce products that are faulty, dangerous, or that people do not care to purchase; and it might carry out these activities really well.,There are particular ways to collect data using defined criteria. Surveys containing closed-ended questions can be used. Simple checklists can confirm status or state perhaps using a behaviourally anchored list. If the production environment is repetitive or highly automated, data-loggers would be feasible. Any kind of reoccurring tabulation - e.g. number of cans, boxes, or skids - gains its meaning from external sources. A lot of counting confirms the presence of management controls over processes. The data-gathering tools and methods seem to be compatible with mass production and the commodification of labour. One would expect to find defined criteria in tightly controlled production environments: a food processing plant or pharmaceutical manufacturing facility can be expected to have many defined criteria ensuring the quality and safety of its products. Another reason to conform to prescribed criteria is to meet non-production objectives that are nonetheless socially desirable: e.g. international standards promoting environmental stewardship; rules and guidelines to meet local regulatory requirements; codes of conduct to give employees a sense of security and fair process. I am certainly not disputing the need to have criteria.,I believe that it is routine for management criteria to be set without any means to confirm effectiveness. Even when confirmation occurs, there is no reason to believe that the criteria will remain effective perpetually. It is also important to accept the likelihood that conformance data, once it is collected, cannot be easily adapted to accommodate changes to criteria. This advisor might have to confront the following challenges: 1) collecting data beyond the scope of current criteria; and 2) confirming lack of effectiveness from data likely within the scope of criteria. I believe that these are challenging tasks. Consider how common it is for a new manager to approach a complex situation involving large amounts of data. This individual would hardly have a command over every aspect of production. However, he or she would likely have specific expectations as embodied in management criteria.,In a number of blogs, I describe ,. I think that my colleagues would consider it my attempt to "keep track of everything." In fact, I try to "keep track of everything in relation to everyone." This is an ambitious undertaking involving many obscure algorithmic operations. Rather than elaborate on the technique here, I just want to underline how an advisor can or at least should attempt to adapt to any kind of management criteria. For the purpose of providing balance, it is not relevant whether or not the advisor agrees with the management criteria; nor is there a need for direct involvement in setting criteria for the role of the advisor to be useful. Keeping track of everything in relation to everyone is already difficult goal - and is itself a unique challenge. The balance is in the balancing - not in the controlling.,A major grocery food chain in my area recently ,??This is fruit that doesn't appear to be quite right. It seems that large amounts of fruit are routinely discarded due to odd shapes, discolorations, markings, and other apparent deformities. However, in all likelihood, the fruit is actually still fine to eat. A conventional criteria-driven approach to quality control can be used to screen out those misfits that seem out-of-place and unacceptable for consumption. But it's not necessarily the fruit that came to exist outside the norms of society. It might be that society has emerged outside the norms of the fruit. Not only this, our preconceptions have become so narrow that few shoppers have any idea what the "normal" fruit really is. Normal for us involves unnatural selection, genetic modification, and heavy amounts of screening.,Perhaps the most common use of management criteria for quality control has little to do with management or achieving quality. I suggest that in the quotidian - in a factory, an interview room, or some other setting - the main purpose of criteria is to exclude details - i.e. to screen out people, facts, methods, and products. In an effort to achieve its goals, I consider it common for a company to initiate processes of elimination. Hiring criteria might be justified to "get the cream of the crop" - i.e. exclude the least suitable candidates. These days the main purpose of criteria is probably to simplify decision-making in an increasingly complex business environment. Simplifying decision-making doesn't actually mean the problem has become simple. It means that the people doing the analysis have chosen to pose the problem as simple. The outcomes of excessive simplification might be entirely nonsensical and inappropriate.,I will share a recent experience purchasing some stocks. I'm unsure how others approach stock selection. At the initial stage, I usually consider the key statistics: e.g. the P/E ratio; changes to revenue; diluted EPS; and for me dividends. Investor data enables criteria-based quality checking: it supports the sort of rapid assessment that makes mass screening possible. Assuming I have selection criteria, I can quickly dismiss many dozens of candidates without giving the matter much thought. For some investors, this process of screening might be the entire selection process. It is a tempting proposition to be able to find a "winning formula" in criteria. It should be no surprise when organizations direct resources towards this type of intellectual capital. For me, the key statistics only provide a particular perspective. I recognize that the numbers have limitations. It is difficult to embed a great deal of information in data intended to conform to standard reporting methods.,I was studying a bank stock. I noticed some peculiarities in the statistics: i.e. negative numbers. For some investors, the story would end here. A certain percentage of people, unable to meet criteria, will choose to terminate their acquisition plans. After reviewing the company's latest quarter, their website, and other sites covering details about the company, I eventually decided to buy shares. I found opportunities for this bank that are not entirely banking related, taking recent technological acquisitions into account along with its unique social, cultural, and historical business setting. Some might question why I didn't just do the detailed analysis right from the beginning. There would be too many candidates to study. I would likely still be thinking matters over or considering giving up. It's important to have screening criteria: its development and determination can strategically affect what data gets collected; the meaning attached to that data; the conclusions drawn from the data; and the decisions made from those conclusions.,??but??rather that people kill people. ??However, in society, people come and go. It's the structures that persist to oppress transients. Conformance data should be considered in relation both to its intent and capacity to limit the articulation of important environmental conditions. Data conceived from ineffective criteria can acquire permanence particularly in social settings, which I suggest includes many production environments. Although I work in evaluating quality, I don't consider myself a professional in this area. Discussions relating to quality do indeed tend to focus on adherence, conformance, and compliance usually to particular standards. I am quick to support safety standards and accept the importance of conformance data to promote public safety. However, I believe that businesses tend to place inadequate emphasis on balancing criteria and tangible outcomes.,Consider for example all of the effort that goes towards building an automobile - and the ,. Is the quality control system being used to truly test for safety, or is it simply ensuring conformance to production standards? I believe that at the point of checking, the ,??for instance to ensure safety. Whether or not the criteria actually lead to safer vehicles is a separate question; this is perhaps determined once accidents occur, insurance claims are filed, and law-suits are launched, as if production were insulated from the outcomes of production. I suspect that in some organizations, quality checking has merely become window dressing. If a database contains only conformance data - confirming that the automobiles conform to all listed criteria - that data might not mean a great deal. Management cannot possibly anticipate every problem during the development of criteria. It is a fundamental question, actually, the extent to which any top-down regime can bring about desirable outcomes.,Consider the prevalence of annual audits. In retail, these audits might be done to confirm inventories and their locations. However, it is quite a different level of checking to assess methods, procedures, and processes using codes, standards, and practices. Imagine enduring rigorous annual quality testing only to continue losing business against a competitor that makes no use of such things. How about a company implementing a professional dress code only to discover its customers turning to online services? In my part of the world, a leading cola maker recently ,??to contain less sugar. It seems that consumers have been turning away from their products due to health concerns. It seems to have taken several decades the company to come to its conclusion - so completely insulated its structural capital is from reality. Nonetheless, the sugar-to-profits growth concept remains pervasive. I know that a leading donut retailer makes some of the most horrifically sugary donuts imaginable. It's a preconception that people spend more for sweetness. Sadly, the company has no way of knowing how I and other customers feel except through lack of orders. There is no adaptation. There is only trial and error. Really, a lab rat going through a maze can exhibit as much intelligence. Save yourself a suit and get a lab rat. I know that last sentence might not make sense to everyone.,I started off the blog with a quote from Sun Tzu about "balance." I said that lack of balance can cause any data collected essentially to extend management (i.e. projection): the data can become purely instrumental rather than representative of any underlying phenomena. Balance helps phenomena to participate in the data (i.e. articulation) - to exist outside the scope of prescriptive criteria. In order to reinforce the conceptual delineation between projection and articulation, I created an embodiment for balance - a managerial counterpart called an "advisor." The advisor makes use of special tools and methods to determine the extent to which managerial criteria are effective given different contexts. I gave a number of examples where the criteria appear to be disassociated from their outcomes; there seems to be no self-awareness or self-correction process to deal with faulty criteria. I said that as work and production environments become increasingly complex, it seems that the disconnection is likely to be furthered as managers attempt to pose the complexity in simpler terms. The simplification provides evidence of lack of balance. "These helmets are safe because the tests say so." "These people are terrorists according to our scoring system." Everything is straightforward when there is no need to examine outcomes, impacts, and consequences.
Some companies really get??,. Not only do they realise size matters ?€? they understand you also have to know what to do with it. Here?€?s a list of seven companies I think are at the top of the game, when it comes to cutting-edge use of data to strategically achieve business goals. If you run a business yourself and are interested in big data projects, there is something to be learned from every one of these. So in no particular order ?€?,GE ?€? with its fingers in every pie from finance to aviation to power, is perfectly positioned to benefit from its championing of ?€?,?€?. They clearly see that IOT ?€? the concept that every device can be networked and learn to communicate with other devices in the same way that computers do ?€? is key to huge efficiency savings and potentially revolutionary business change.,As a result they are heavily investing in what they call the Industrial Internet ?€? the subset of IOT dedicated to industrial devices and equipment. Aircraft engines are being fitted with arrays of sensors capable of detecting and measuring the slightest changes, meaning that fine-tuning for efficiency is possible to a higher standard than ever before. And the same is just as true with their medical equipment and power station turbines. In 2012 the company announced it was investing $1 billion into its data projects over four years.,Like other companies mentioned here, it also makes the technology driving its data operations available to other businesses, by licencing its GE Predictivity services. For more on GE see my article:??,.,In 2003 50,000 IBM staff took part in online interviews where they were asked about key business issues, and the direction they thought the company should be heading in. Those interviews were fed into textual analysis software designed to pick out the most common phrases and themes, which became new company objectives.,This was forward-thinking in many ways and encapsulates the idea of a company transforming itself into a data enterprise. Those at the top had come to the conclusion that data in most fields will always trump opinion ?€? even??,??opinion ?€? and surrendered themselves to something of a destruction of the ego; ?€?letting go?€? (temporarily) of the reigns and seeing what direction the company would head, steered by science and statistics, rather than the possibly jaded or entrenched ideas and opinions of directors and senior managers.,Since then IBM has reinvented itself as a data powerhouse, at the forefront of the current boom in business-to-business data infrastructure services. It offers hardware and software for maintaining big databases, such as its DB2 database application and SPSS analytics application, among many other products and services.,It has also become an ambassador for the concept of big data, publishing several papers on how companies can exploit its potential for innovation and increased profits. Books have been written on the turbulent history of this particular tech giant, but by embracing big data with such enthusiasm, they are entering a new chapter.,Amazon not only brought big data to the masses, it made it personal ?€? and customer service was changed forever. One of the shortfalls of online shopping for early adopters of the habit was the lack of a sales assistant or shopkeeper to explain the products and, by getting to know you, helping you find whatever it is you need to solve a particular problem in your life.,With its recommendations and reviews-base structure, Amazon introduced us to the super-powered sales assistant ?€? equipped with a super memory retaining every customer transaction and able to offer lightning-quick, and most importantly accurate, suggestions. In fact, it?€?s got so good at this that according to rumor (based on patent applications) it is planning to begin predictive shipping ?€? automatically sending out parcels of books, DVDs, videogames and gadgets based on what it thinks its users will want to pay for.,Amazon is clearly not blind to the vital role data has played in its success, and has used the vast revenues (if not profits) built up by pioneering online retailing to invest in also providing data services. Much like IBM mentioned above it provides infrastructure to allow other businesses to capitalize on data gathering, storage and analysis enterprises. For more on Amazon, see my posts??,??and??,.,Facebook has revolutionized the way we communicate with each other, from staying in touch with relatives to organizing weekend activities with friends. There was instant messaging and email before it, but Facebook invited users to build the world?€?s largest directory of people. It then made them all accessible to each other - depending, in theory, on privacy settings determined by each user. With 1.32 billion active users, it is still by far the world?€?s largest social network.,In the process, it has collected probably the biggest database of personal information in the history of the world. Its users upload 30 billion pieces of content between them every day, resulting in over 300 petabytes (3 million gigabytes) of information. It has used this information to draw in advertisers, generating $2.68 million in advertising revenue during the last quarter.,This year the company made moves in an unexpected direction by purchasing the upcoming Oculus Rift virtual reality technology for $2 billion. Speculation says the company is looking ahead to times when we want to be able to experience greater levels of interaction with our data (or our friends, in their Facebook digitized form) than current flat screen technology allows. For more on Facebook, see my post,.,No list of the top big data businesses would be complete without mentioning the still-undisputed king of search. Like Facebook, it turned data collection and analysis into a business model by providing a service ostensibly for free, then selling on information it gathers about us by monitoring the way we use that service.,Search is still the key service it provides ?€? and since the early days when its algorithms were first recognized for their superiority at matching what the user is typing, with what they are looking for, they have continued to evolve ?€? moving towards a standard of ?€?natural language processing?€? which is planned to one day let us converse with computers as easily as with people.,Its activities have often caught the public imagination. From the blistering speeds that it reports (?€?smugly?€? as one comedian described it) it has trawled millions of web pages to find what you?€?re looking for, to the breathtaking scope of Google Earth, consistently providing services that people want to use ?€? for education, business or just passing time.,Google offers a range of services ?€? now collected at the Business Hub ?€? to aid with promotion, and has also moved firmly into providing more heavyweight big data services to businesses. These include BigQuery ?€? its analysis engine, and Google Cloud Storage services. For more on Google, see my article:??,.,Less well-known than the other companies I?€?ve mentioned here, Cloudera has emerged in recent years as one of the most prominent suppliers of Apache Hadoop solutions. Apache Hadoop, as I?€?ve mentioned before is a suite of software applications designed for running big data enterprise operations. Although open-source (free) in its raw state, an industry has sprung up providing companies with custom-configured systems, intended to simplify the process of data gathering and analysis. Cloudera is a leader in this field, and clearly realises the obligation it owes to the free technology on which it is built, returning a share of its profits to the voluntary foundation which maintains Hadoop. For more on Hadoop, see may article:??,.,Another newcomer ?€? built from the ground up as a big data business, rather than a dinosaur forcing itself to evolve. Kaggle pioneered data science as competition ?€? offering rewards for solving various challenges faced by industry.,Companies post problems they are attempting to overcome ?€? for example, to match movies on a streaming service with what the customer may want to watch next, alongside sample data sets. Prize money is then awarded to the solution which most comprehensively trumps their existing methods.,Clients who have benefited from the 150,000-strong army of data scientists ?€? some professional, some amateur ?€? Kaggle can call for help with any problem, include NASA, Google, Wikipedia and Microsoft. For more on Kaggle, see my article:??,.

There are some things that are so big that they have implications for everyone, whether we want them to or not. Big Data is one of those concepts, and is completely transforming the way we do business and is impacting most other parts of our lives.,It?€?s such an important idea that everyone from your grandma to your CEO needs to have a basic understanding of what it is and why it?€?s important.,?€?Big Data?€? means different things to different people and there isn?€?t, and probably never will be, a commonly agreed upon definition out there. But the phenomenon is real and it is producing benefits in so many different areas, so it makes sense for all of us to have a working understanding of the concept.,So here?€?s my quick and dirty definition:,The basic idea behind the phrase 'Big Data' is that everything we do is increasingly leaving a digital trace (or data), which we (and others) can use and analyse. Big Data therefore refers to that data being collected and our ability to make use of it.,I don?€?t love the term ?€?big data?€? for a lot of reasons, but it seems we?€?re stuck with it. It?€?s basically a ?€?stupid?€? term for a very real phenomenon ?€? the datafication of our world and our increasing ability to analyze data in a way that was never possible before.,Of course, data collection itself isn?€?t new. We as humans have been collecting and storing data since??,. What?€?s new are the recent technological advances in chip and sensor technology, the Internet, cloud computing, and our ability to store and analyze data that have changed the??,of data we can collect.,Things that have been a part of everyday life for decades ?€? shopping, listening to music, taking pictures, talking on the phone ?€? now happen more and more wholly or in part in the digital realm, and therefore leave a trail of data.,The other big change is in the kind of data we can analyze. It used to be that data fit neatly into tables and spreadsheets, things like sales figures and wholesale prices and the number of customers that came through the door.,Now data analysts can also look at ?€?unstructured?€? data like photos, tweets, emails, voice recordings and sensor data to find patterns.,As with any leap forward in innovation, the tool can be used for good or nefarious purposes. Some people are concerned about privacy, as more and more details of our lives are being recorded and analyzed by businesses, agencies, and governments every day. Those concerns are real and not to be taken lightly, and I believe that best practices, rules, and regulations will evolve alongside the technology to protect individuals.,But the benefits of big data are very real, and truly remarkable.,Most people have some idea that companies are using big data to better understand and target customers. Using big data, retailers can predict what products will sell, telecom companies can predict if and when a customer might switch carriers, and car insurance companies understand how well their customers actually drive.,It?€?s also used to optimize business processes. Retailers are able to optimize their stock levels based on what?€?s trending on social media, what people are searching for on the web, or even weather forecasts. Supply chains can be optimized so that delivery drivers use less gas and reach customers faster.,But big data goes way beyond shopping and consumerism. Big data analytics enable us to find new cures and better understand and predict the spread of diseases. Police forces use big data tools to catch criminals and even predict criminal activity and credit card companies use big data analytics it to detect fraudulent transactions. A number of cities are even using big data analytics with the aim of turning themselves into Smart Cities, where a bus would know to wait for a delayed train and where traffic signals predict traffic volumes and operate to minimize jams.,The biggest reason big data is important to everyone is that it?€?s a trend that?€?s only going to grow.,As the tools to collect and analyze the data become less and less expensive and more and more accessible, we will develop more and more uses for it ?€? everything from??,??to better healthcare tools and a more effective police force.,And, if you live in the modern world, it?€?s not something you can escape. Whether you?€?re all for the benefits big data can bring, or worried about Big Brother, it?€?s important to be aware of the phenomena and tuned in to how it?€?s affecting your daily life.,What are your biggest questions about big data? I?€?d love to hear them in the comments below ?€? and they may inspire future posts to address them.
With the launch last week of??,??I thought it would be worth putting an MD?€?s perspective on the product ?€? how we got here, what the philosophy is that lies behind it, and where we hope to go with it. For a more formal view of the academic and commercial background see our??,.,??,??,Datascape has undoubtably grown out of Daden?€?s virtual world heritage ?€? and my own interest in data and data visualisation. Over the years we?€?ve used virtual world platforms such as VRML, Active Worlds, Second Life and OpenSim to create a variety of data visualisations, probably culminating in our original Datascape virtual command centre (which won a prize at the US Government?€?s Federal Virtual World Challenge), and the visualisation of Twitter data we did in OpenSim for the Royal Wedding in 2011. These examples and experiments, and those of others, together with an MOD funded research project we did in 2011 within Aston University doing a quantitative comparison of immersive and non-immersive 3D visualisations spaces convinced us that there was definitely something in immersive data visualisation.,In then moving from ideas and demonstrators to a full blown product I think that there are 4 key ideas that have informed our journey.,??,Datascape is about immersion. It is about putting you inside your data, allowing you to move around and through your data and view it from any angle, from inside or out. When in navigation mode there is no user interface ?€? there is just your data ( possibly the ultimate expression of Edward Tufte?€?s Data Ink idea). This sense of immersion appears to help the brain see the patterns and anomalies in the data, because the data behaves like the real world ?€? it stays still whilst your eye travels through it.,??,??,Datascape does not constrain you. If you want to map latitude to colour and longitude to shape you can do it. The heart of Datascape is the mapping screen, where you assign the fields in the data to the features of a plot point ?€? its position, rotation, shape, size, colour, image and labels. With a full set of spreadsheet like functions at your call, and self-populating look-up tables, the plots you can produce probably really are only limited by your imagination. That flexibility does mean that initially there might be a bit more to learn, but we?€?ll be posting ?€?recipes?€? and ?€?how-to?€?s?€? on our web site to help you create the more common visualisations, and as we release successive versions of Datascape we may well start including wizards and templates that get you more directly to those common views.,??,Given that we needed good graphics and processing capability we took the decision early on that this would initially be a PC application, not something for the web or your tablet. However by basing Datascape on Unity we have got a path available to develop a web and/or tablet versions of Datascape if the demand is there. We have also been keeping a watching brief on HTML5 and WebGL and one feature under serious consideration is being able to export your completed workspace as a standalone HTML5 virtual world to share more easily with friends and colleagues.,??,One thing we have found as we begin to look at more and more data in Datascape is that we may need a new visual language to describe what we are doing with data in a 3D space. In 2D we are all used to line graphs and bar charts, pie charts and scatter plots. Whilst we can do these in 3D as well, they do not (except for the last) typically take fullest advantage of the medium.,For instance one problem we?€?ve found in 3D is that whilst the virtual space let?€?s us plot a long line of data stretching off into the distance, looking at the whole line is hard, you have to scroll as you do in 2D, unless we compress it (but then we lose the detail that the spread out 3D display brings). One solution that we have found is to plot the data as a cylinder, or even as a spiral, with the viewer in the centre. You can then take in a lot of data in one go, and just fly up and down the cylinder to other data ?€? which is typically an easier action to control that horizontal flight. What other standard forms will we find, and how will we determine which form suits which type of data, and which type of enquiry?,Another difference is axes. In 2D the axes form a frame in which your data sits ?€? and the same for non-immersive 3D cubes. But in an immersive space you are usually inside the data and the axes are nowhere in sight. So how do we maintain orientation within the data, and understand where the data points sit on the axes (that is if we actually need enumerated axes). There are no doubt a wide number of solutions to explore, and within Datascape we have distant XYZ markers so you can easily tell which direction you are looking in, whenever you hover over a point it can tell you its X,Y and Z values, and you can also have the point drop reference lines down to the axis or reference planes. One other thing we have tried, but not perfected enough to release, is a 3D compass, and another that we are looking at for future releases is the use of mini-maps, not just as a top-down (XZ plane) view but also as YZ and YX views as well. But can you cope with seeing your data in four directions at once?,??,We thought long and hard about this tag line, just as we did about whether or not to have avatars. We didn?€?t put avatars in the single user version since we felt that a) you got enough of a sense of immersion from the navigation alone and b) for most corporate users we spoke to avatars are still a turn-off and too closely associated with gaming environments. However ?€?virtual world?€? (most emphatically in lower case) did seem by far the most appropriate way to describe what you can create with Datascape, a virtual world populated solely by you and you data.,In multi-user mode we do provide you with a very basic humanoid avatar ?€? but it is very much a place-holder, a glyph, for where you are in the world and what direction you are looking. We deliberately kept clear of an avatar that was human enough for you to start worrying about what gender, or race or age it was, and what clothes it should wear! The resulting avatar is enough to let you know where your colleagues are and what you are looking at, no more, but even so it?€?s not long before you?€?re playing hide and seek amongst the data.,Going forward we may well increase the virtual world sense ?€? for those who want it ?€? with better avatars, more 3D scenery in which to place your visualisations ( closer to the original Datascape), and persistency and controlled sharing of your data and workspaces. But let?€?s start simply, and with something that everyone can hopefully relate to.,??,So hopefully that gives you some insight into our thinking as we developed Datascape, and some clues as to where we might take it in the future. Please ,??(there is a free community version with a 6000 point limit and a paid pro version with a 65,000 limit - although we have had it running up to 250k points) ??and give it a try, and hopefully it will open up a whole new world of data visualisation for you.,??





Analytics, big data, data science... All of them were posted in major news oultets (WSJ, MIT, etc.) in the last 10 days. Enjoy the reading!
Let's get over the??hype. Unless you are a data driven giant like Amazon or Yahoo, there is no need to spend millions to dig and fill a big data lake. You might be better off with data junk yard strategy. It will let you start early and cheap so you don?€?t miss the big data bus. It may not sound impressive but play it right and a junk yard will end up a gold mine with an optimum balance of cost, benefits and speed.????Here is how it makes sense:,Hadoop,??the core of Big Data platform, promises??to acquire and store anything digital at a low cost hardware with minimum efforts. This was not possible??before with??traditional data platforms. They sit on expensive hardware and demand substantial skilled resources to organize the data before loading.??Hadoop allows to ingest and load without asking for transformation in advance. It?€?s cheap to store and you don?€?t have to clean it unless you need it. A case in point: A data junk yard.,No need to overthink. For most cases you will come up??with, there will always be an argument that it could be done in your traditional darling databases. ??Use cases that will get traction be??either around large public datasets or about complicated forms of unstructured data. ??Both will??require some hands-on research as well as use and throw flexibility. No point in investing too much time and efforts on use cases. Bring the data sets with some perceived value and invest in data discovery. What's junk and what's gold will emerge in the process. ,??,There is a host of public data that might be valuable but it?€?s difficult to assess the real value without having a capability of playing around with it or explore and analyze it with the internal data sets. It's like bringing??home a side table from??yard sale and see if it fits with your couch.?? Also, the public and social data is like water flowing through the river. Most of it is free for now. If you do not capture it in time, it will flow past you. Investing in big data is about future and not present. ,.,The key is to start early and keep steady with the purpose of building reservoir of data that has a perceived value. Who cares what you call it. It could be a lake, reservoir, a winery or a junk yard. The bottom line is to capitalize on??the two basic big data capabilities: an inexpensive way to store any digital form of data. I am sure you will find some gold nuggets in that junk yard.
 broad business acumen, corporate expertise, know-how of corporate and government culture, a feel of the fluctuating government political climate and its low(s) & high(s), Popular Industry KPI(s), well versed in the area of large scale multi-million $ budgets,, Advanced Mathematics, Law, Econometrics and Finance, Software Development experience using Java, Python and C; Database experience should include DB2 and Oracle. Other useful expertise includes:?? Wireless Communications, SAP background, SAS, knowledge of File Data Transfer Technology, Compliance and Cloud Management.,: MBA (Well Known Reputable University), B.S (Math, Econometrics, Operations Research, Finance, Law, Computer Science),: An ability to comprehend State Government (50 States) generated data aimed at identifying data insight(s), data patterns, trending behavior, KP Indicators, and other hidden valuable insights embedded in data not readily visible, which could be combined with Federal government Data to be used for making critical decisions of national importance.,Able to establish data relationship between data elements to be used for ?€?If then?€? analysis; an ability to establish data models to be used for data projection, predictions and forecasting future events, alerts and or guidelines for future course of actions; development of Insight embedded (leveraging) statistical models and visual analytics; capable to scan multiple sources of diverse data concurrently and spot trends to approximate future course of actions. ?? ?? ?? ?? ?? ?? ??, Hadoop, NoSQL, and Teradata ????
All analytic professionals, students and academics -??Participate in the??,., ,including best practices shared by respondents on analytic success measurement, overcoming data mining challenges, and other topics.?? The Summary Report for this 2015 Data Miner Survey will be available to everyone Fall-2015.
People often ask ?€?What technology & tool skills do I need to develop to be a data scientist??€?. We decided to go straight to the source of job descriptions & check what requirements are being asked for while people hire data scientists. We analyzed about 1660 job postings which had ?€?Data Scientists?€? in the job title and decided to further search for specific technology & tools skills that are required in those job descriptions.,Our first analysis involved understanding what programming and scripting languages were required in these Data Scientist job postings. Unsurprisingly, the number one listed programming language was Python with 1014 job postings because of its inherent friendliness to data analysis & supporting libraries such as NumPy, SciPy & Pandas. The second most popular language was Java, followed by C++, Perl, Ruby & C#.,We then performed a similar analysis on statistical tools required for these jobs. The tool most required in these job postings was R (1077 Job postings), followed by SAS, Matlab, SPSS, Stata & Minitab.,??Here are some job Description for Data Scientists to give you a flavor of the skills required:
"We are excited to announce that all 1000+ functions on Blockspring can now be run from your spreadsheet.,You'll be able to create interactive data visualizations, run algorithms, pull data from the web, automate tweets and emails, call APIs, and more. In a nutshell, you get the full power of programming from the comfort of a spreadsheet.,To get started, install the??,??add-on from the Chrome Store. Excel is coming soon.,Example use cases,
,
,."
Read more??,??and also ,.
More than a thousand keywords with detailed explanations, and hundreds of machine learning / data science books categorized by programming language used to illustrate the concepts.,10 keywords starting with A, this is indeed a small subset of all the keywords starting with A.,To check the entire list, with clickable links, not just titles, visit
From all indications, 2015 is well on its way to becoming the year of cloud computing. The feverish pitch of activities at key players on one hand and the data as well as observations of industry pundits affirm this. There are apparently a handful of reason to keep the IT industry leaders awake at night. ??,For starters, per , 2014 revenues for cloud services grew by 60 percent. ??The global cloud computing market, per Forrester, is expected to grow to over $191 billion by 2020. IDC also foresees a robust market for cloud computing services. ,The cloud computing stacks - Infrastructure as a service (Iaas), Platform as a service (Paas) and Software as a service (Saas) ?€? are all driving the healthy cloud computing market growth. But Saas appears to be the winner with the highest projected growth in the medium term, with Goldman Sachs forecasting global SaaS software revenues to reach $106 billion by 2016.,Analysts believe the cloud analytics market will grow at a CAGR of ,% percent over the period 2014-2019. Cloud based analytics is at the center of this growth. This is obvious. As the layer that harvests insights, the analytics layer will always be preeminent and reside on top of the information value chain. It will continue to reap the benefit from the immense innovations in the underlying technology and data layers. It has also immensely powered this robust growth forecasts.,Cloud computing offers innovations ?€? a new way of doing things for companies; it offers tremendous operational flexibility and the opportunity to cut down expenses. Further it provides users the ability to use software from any device, either via a native app or a browser. Hence, users will be able to carry over their files and settings to other devices in a completely seamless manner, which is a big bonus. Companies no longer need to maintain huge teams to manage their technology assets and their upkeep.?? ??It can now be outsourced to the cloud and/or vendors.?? The companies can now focus on what they do best ?€? run their business with renewed focus.,It is no surprise that the market leaders ?€? Amazon, Microsoft, Google, IBM, Salesforce, Oracle and other majors - are stepping on the gas. Amazon is the market leader with the biggest share of the market. Analysts have valued its web services business alone at $40-$50 billion.?? IBM has announced that it will invest $4 billion on its cloud services, data analytics and mobile businesses to reach targeted revenue of $40 billion by 2018. It would not be a hyperbole to state that the outcome of this dogfight to win customers will decide the future contours of IT business.,How will this impact the Analytics business??? How will the banking and financial services sector be impacted? How about other industry verticals? Admittedly it is tough to predict the future; however, if past performance is any guide for the future, the impact would indeed be big. That is not to say the cloud analytics adoption and hence revenue accrual would be easy.,It is well known that banks and financial services institutions have taken a cautionary approach to cloud.They have been slow and deliberate in adopting the technology for good reason. The main concerns have been around data security and discomfort in sharing private and often sensitive and confidential financial data. The periodic high profile hacking of customer data, including those on cloud, have not help boost confidence. However, with innovations such as hybrid cloud and new data security tools are changing mindsets.?? Banks and financial institutions are projected to adopt cloud and more specifically cloud based analytics in a big way. ??Other industry verticals ?€? particularly life sciences, heath care and insurance are already showing strong signs of resurgence in 2015.,Many IT industry analysts, particularly the analytics and insights industry watchers will be surprised at this optimism and robust projections. It is easy to understand this perspective because the year 2014 has not been a great year for analytics for IT Majors. Many have already restructured or initiating this exercise. However, as pointed out in an earlier essay, it is more to do with leadership than business opportunity. It may not be surprisingly to find that these majors may not even have a spot in the boxing ring in 2015; they have to be content to watch as bystanders as their future is being fought and snatched away right in front of them.,The coming clash of the titans in cloud computing will hog the spotlight for most of 2015 and beyond. The clash will have profound impact on the industry and the outcomes will seek to reshape it. This will be keenly watched by the pundits and all. Stay tuned folks.
Vozag downloaded CRAN data from the R project to understand the top projects & which ones had the most discussions. Given below is a list of the top 20 packages downloaded in a single day. The full list of the top ,.,??,We also decided to then analyze Stack Overflow data to understand most discussed packages and analyze the one with the most questions & unanswered questions.,GGPlot was ranked first with the most questions at ~7200 questions followed by Data table (2135), Plyr (1213) & Knitr (1136). Other packages had less than 1000 questions each. We also looked at the unanswered questions. The packages with the highest percentage of unanswered packages were Knitr, Lattice and iGraph at 24.2%, 23.5% and 19.8% respectively.??,Comparing the top downloaded packages with the most discussed packages shows little correlations between them. For Instance, GGplot has the most questions asked & is the second highest downloaded package but ?€?Data.Table?€? package (the second highest ranked R package for questions asked) is not even in the top 100 packages downloaded. Knitr is another example which is in the top 5 questions asked, but is 27, ranked in downloaded packages.,So- does the R community need to focus on packages that have the highest questions to resolve their issues rather than the ones with the most downloads?,===
.,With the exponential growth of IoT and M2M, data is seeping out of every nook and cranny of our corporate and personal lives.??However, harnessing data and turning it into a valuable asset is still in its infancy stage of development.?? In a recent study, IDC estimates that only 5% of data created is actually analyzed. Thankfully, this is set to change as companies now have found lucrative revenue streams by converting their data into products.,Many companies are unaware of the value of their data, the type of customers who might potentially be interested in those data, and how to go about monetizing the data.??To further complicate matters, many also are concerned that the data they possess, if sold, could reveal trade secrets and personalized information of their customers, thus violating personal data protection laws. ??,The most common approach for companies who have embarked on data monetization is to develop a dashboard or application for the data, thinking that it would give them greater control over the data.??However, there are several downsides to this approach:,What many companies have failed to realize is that the raw data they possess could be cleansed, sliced and diced to meet the needs of data buyers.??Aggregated and anonymized data products have a number of advantages over dashboards and applications.,Monetizing your data does not have to a painful and drawn out undertaking if you view data itself as the product.??By taking your data product to market, data itself can become one of your company?€?s most lucrative and profitable revenue streams.??By developing a data monetization plan now, you can reap the rewards of the new Data Economy.,:
The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
In this post, we?€?ll use an unsupervised machine learning technique called kmeans clustering to find naturual structures in our data. In the other blog posts, we used supervised machine learning techniques like logistic regression and linear regression to predict??,??or??,.,Often time, we don?€?t even know what the structure of our underlying data is telling us.,Let?€?s load in our small sample set here and see the first 5 rows of data:,We see that our data set is evenly split between those who owsn a riding mowers and those who don?€?t. This is all good, but perhaps there are more subtle natural groupings that we are not aware of.
Next month marks the 100th anniversary of??,??first home run.,This year, opening day in baseball signals the ?€?closing day?€? for one of the classic truisms among sports statisticians: the belief that there is no such thing as ?€?hot hitters?€? or ?€?batter?€?s slump?€? in baseball, or the ?€?hot hand?€? in basketball.,This belief is based on famous research by Gilovich and Tversky in 1985?€??€??€??€?,.?€? They studied successions of foul shots in basketball and concluded that players?€? chances of making a foul shot were unaffected by whether they had made or missed recent foul shots.,Wikipedia still has a whole??,??devoted to the ?€?hot hand fallacy?€? based on this research.,More recent work, though, by??,asserts that, in baseball, a batter?€?s prior 25 at-bats are a significant predictor of performance on the next at-bat. For a hot hitter, his chances of reaching base rise 25%-30%. Similar effects were observed for home runs, and for pitching performance. And??,??by Bocskocsky, Ezekowitz and Stein demonstrated a similar, though smaller, effect in basketball.,The original Gilovich and Tversky conclusion may not die so quickly, as statisticians continue to test the new conventional wisdom. One point to note is that they were testing only foul shots?€??€??€?a relatively small component of the flow of action in basketball game, and so was not the most powerful test of the hot hand hypothesis.,And it is worth bearing in mind that just the perception that a hitter in baseball is on a roll (regardless of whether this is accurate) may, in fact, affect his chances of success. For example, pitchers will ?€?pitch around?€? a hot hitter (i.e. throw pitches that are not strikes), increasing his chances of reaching base by a walk.
The Unix Philosophy, summarized by Doug McIlroy in 1994:,This is considered by some to be the greatest revelation in the history of computer science, and there?€?s no debate that this philosophy has been instrumental in the success of Unix and its derivatives. Beyond Unix, it?€?s easy to see how this philosophy has been fundamental to the fantastic success of the Hadoop ecosystem.,Nevertheless, a recurring problem in Computer Science is latching on to a great idea and applying it indiscriminately, even when it doesn?€?t make sense. I?€?m going to argue that while the Unix philosophy works for batch workflows, it is a poor fit for stream processing.,In 2008, when I started work on what would become VoltDB, there was a tremendous amount of buzz around Hadoop. At the time Hadoop and ?€?MapReduce?€? were almost synonymous. Seven years later, ?€?MapReduce?€? is being subsumed by better technologies, but the Hadoop ecosystem is strong.,If MapReduce is waning, what has made the Hadoop ecosystem so successful? I?€?m probably not going out on a limb to suggest that HDFS is the key innovation of Hadoop. HDFS jump-started the Cambrian explosion of batch-processing we?€?re now experiencing by doing three things:,These three traits are why HDFS enables so many Hadoop ecosystem tools to do so many different things.,Between HDFS replication and data set immutability, no matter how flawed your software or hardware consuming the data is, the original data is safe. A successful batch job takes one immutable data set as input and produces another as output. If a job fails or corrupts due to hardware or software problems, the job can be killed, the output deleted, the problem addressed and then the job restarted. The input data is still pristine. HDFS and the batch nature of the work allows this robustness. Contrast this with a system that modifies a dataset in place, and you see that immutability mostly absolves developers from managing partial failure.,So developers building Hadoop ecosystem tools to run on HDFS are absolved from the most difficult problems of distributed systems. This enables new batch-oriented tools to be developed quickly. End-users are free to adopt new tools aggressively, as they can be run in addition to an already-working system without introducing new risk.,HDFS enables a vast ecosystem of tools that can be combined into billions of different combinations, and adjusted as business needs change.,Unsurprisingly, not all apps are a great fit for batch. Many data sets have value that must be extracted immediately or opportunities and insight are lost. We call these apps ?€?Fast Data?€? apps, as opposed to ?€?Big Data?€? apps.,Unfortunately, low-latency and HDFS don?€?t mix. HDFS isn?€?t magical and, like all things in engineering, comes with tradeoffs. Almost all of the tools built around HDFS as a repository for data accept that the latency between ingestion and the desired end response is measured in minutes or hours, not seconds or milliseconds.,This is typically because input datasets must be loaded before processing, and the output result must be fully written before it is accessed. This process can be chunked to reduce latency, but in order to maintain efficiency, these chunks are still at least seconds?€? worth of work to process. The second latency issue is that most batch systems favor retry over redundancy when handling faults. This means, in the case of hardware or software failure, users will see significant spikes in latency.,Thus we?€?ve ruled out HDFS for latency-sensitive processing problems. So what do we use instead? Surely, without HDFS as the unifying foundation, the pattern of cobbling together many special-purpose systems to build a larger system using glue code and luck won?€?t translate to Fast Data, right?,You may have guessed this was a rhetorical question.,Let?€?s examine Storm, an exemplar tool for stream processing. Storm offers a framework for processing tuples. A set of processors, each with inputs and outputs, is connected in a topology. The user supplies the per-tuple processing code and the framework manages where that code is run, much like a MapReduce Hadoop job. Storm also manages the optional re-processing of tuples when software or hardware fails. Storm has a real ?€?MapReduce?€? for streams feel to it, but there is a crucial difference: Storm is responsible for the data it is processing, where MapReduce is processing data kept safe by HDFS.,Keeping with the Unix and Hadoop philosophy of specialized tools, Storm focuses on distributing data and processing to a commodity cluster of Storm workers. It leaves other key functions to other systems.,A recent talk from a Twitter engineer described a problem his team solves with Storm. They count unique users of mobile apps for given time periods. The tricky part is the volume: reportedly 800,000 messages per second, making this a poor fit for more traditional systems. The stack they use involves Kafka, Storm, Cassandra, and of course, ZooKeeper.,We can assume ZooKeeper, Kafka, Storm and Cassandra all use at least three nodes each to run with reasonable safety. Three is a bit of a magic number in distributed systems; two node clusters have a harder time agreeing on the state of the cluster in failure scenarios. So now we?€?re operating and monitoring four systems, at least twelve nodes, and the interops/glue between all of the above. If a network between two Cassandra nodes fails, are the symptoms the same as if the network between Storm nodes failed, or ZooKeeper nodes failed? Each of these systems has different failure semantics, and can cause different symptoms or cascading failures in other parts of the stack.,While four systems wouldn?€?t be odd in an HDFS-based batch system, the crucial difference here is that user data ownership is passed between systems, rather than being wholly the responsibility of HDFS. Sometimes state only has meaning if data from multiple systems are combined. Development and testing stacks with four systems isn?€?t 33% harder than stacks with three systems either; it can be as much as four times as hard, depending on how the systems connect with each other.,Builders of such systems readily admit this, but some of them have found a ?€?solution?€?.,So batch is too slow, and Fast Data offerings are less than robust and a bit too opaque. What to do???,, formerly of Twitter and creator of the Apache Storm project, proposed the Lambda Architecture as the solution. Rather than address the flaws directly, you simply run both the batch and streaming systems in parallel. Lambda refers to the two systems as the ?€?Speed Layer?€? and the ?€?Batch Layer?€?. The Speed Layer can serve responses in seconds or milliseconds. The Batch Layer can be both a long-term record of historical data as well as a backup and consistency check for the speed layer. Proponents also argue that engineering work is easier to divide between teams when there are two discrete data paths. It?€?s easy to see the appeal.,But there?€?s no getting around the complexity of a Lambda solution. Running both layers in parallel, doing the same work, may add some redundancy, but it?€?s also adding more software, more hardware and more places where two systems need to be glued together. It?€?s this lack of natural integration that makes Lambda systems so difficult. The fact that the work can be divided amongst teams is itself a tacit acknowledgement that Lambda solutions may require ?€?teams,?€? plural.,So why is the Lambda Architecture gaining acceptance? I believe there are two ways to answer. First, it?€?s because Lambda has some real advantages. For starters, it can scale. The number one rule of 21st century data management: If a problem can be solved with an instance of MySQL, it?€?s going to be. With Fast Data, we?€?re often talking about hundreds of thousands of events per second to process, something well beyond what MySQL can handle. There is a set of Lambda proponents who know that Lambda is complicated and operationally challenging, but are aware of few other options.,The alternative reason people turn to the Lambda Architecture is habit. People who work with Big Data are used to going to the Apache Software Foundation?€?s website, picking from a menu of software products, and connecting them up. Some people just want to wire things together from smaller cogs, whether it?€?s the easiest path or not.,Many voice similar arguments about the complexity and cost of typical Lambda solutions. The best way to back up such an argument is by pointing to an alternative. And the way to build a successful alternative to Lambda seems to be to improve the efficiency and robustness of the speed layer.,LinkedIn/Apache has Samza. Spark has a streaming flavor. Google has Millwheel. All of these make arguments that the speed layer can be less burdensome on a developer. Perhaps you?€?re not aware of VoltDB?€?s pitch as a Fast Data engine, but we feel that the design of the VoltDB system makes it one of the best possible approaches to this problem.,, discussing VoltDB in details.??
At this point, I suspect a lot of us have heard of the three, four, or even , of big data. The original three V?€?s ?€? Volume, Velocity, and Variety ?€? appeared in 2001 when , analyst Doug Laney used it to help identify key dimensions of big data. ????, and others added Veracity. ??Then Viability, Value, Variability, and even Visualization got included.?? They definitely all matter, particularly as we consider designing and implementing processes to prepare raw data into ?€?ready to use?€? information streams.,But there seems to be something missing.?? It?€?s as if these V?€?s were developed to explain things before we fully explored what data is available, where it can come from, why it is so darned big, and why it is going to get a heck of a lot bigger and impact our lives in ways we can?€?t even imagine yet. Or maybe it?€?s just that I need more clarity on the Volume or Variety part.?? Or maybe all seven V?€?s.,Bottom line: I think we?€?re underestimating the impact of big data.?? It?€?s inevitable at this juncture. I know some enterprising data geeks have tried to quantify it.?? Take this , for instance. Or these ,. And there are plenty of others, all pointing to the inevitable truth that big data is really, really big.,How big? No one really knows. Or can. At least right now. Why? Because it?€?s not just the number of potential sources that are growing exponentially.?? It?€?s also the methods that are growing to exploit them and enable insight.?? And then there?€?s the way that the same data source might be used differently based on the insight you?€?re trying to glean. It?€?s daunting.,Perhaps the closest we can get to any realistic quantification or definition right now is to first consider the vastness of each of the five data characteristics below:,Thinking about the vast amounts of data and the potential solutions in any of one of these categories individually is heady stuff.?? But now think about them in combination with each other. And then think about the exponential growth that is likely to occur from there.,(BOOM! Mind blown. At least mine is.),So now what? It?€?s about putting this data to work and finding the streams and tools to extract insight from them that are relevant to different business processes and use cases. I contend that every industry, public, private, or non-profit sector can and indeed should take advantage of data driven solutions now. But designing and implementing any data driven system has to start with an understanding of what you want to do and what is possible with current and future tools.?? There is no one size fits all in data driven solutions, and every strategy should take into consideration the resources and costs and benefits of different approaches.??,Because ultimately, if all of this data can?€?t help in the decision making process, it?€?s just more noise.,Originally posted on??,.

Machine learning algorithms are parameterized so that they can be best adapted for a given problem. A difficulty is that configuring an algorithm for a given problem can be a project in and of itself.,Like selecting ?€?the best?€? algorithm for a problem you cannot know before hand which algorithm parameters will be best for a problem. The best thing to do is to investigate empirically with controlled experiments.,The caret R package was designed to make finding optimal parameters for an algorithm very easy. It provides a grid search method for searching parameters, combined with various methods for estimating the performance of a given model.,In this post you will discover 5 recipes that you can use to tune machine learning algorithms to find optimal parameters for your problems using the caret R package.
Organizations are struggling with a fundamental challenge ?€? there?€?s far more data than they can handle.?? Sure, there?€?s a shared vision to analyze structured and unstructured data in support of better decision making but is this a reality for most companies??? The big data tidal wave is transforming the database management industry, employee skill sets, and business strategy as organizations race to unlock meaningful connections between disparate sources of data.,Graph Databases are rapidly gaining traction in the market as an effective method for deciphering meaning but many people outside the space are unsure of what exactly this entails. Generally speaking, graph databases store data in a graph structure where entities are connected through relationships to adjacent elements. The Web??is a graph; also your friend-of-a-friend network and the road network are graphs.,The fact is, we all encounter the principles of graph databases in many aspects of our everyday lives, and this familiarity will only increase. Consider just a few examples:,In the simplest terms, graph databases are all about relationships between data points. Think about the graphs we come across every day, whether in a business meeting or news report.???? Graphs are often diagrams demonstrating and defining pieces of information in terms of their relations to other pieces of information.,Traditional relational databases can easily capture the relationship between two entities but when the object is to capture ?€?many-to-many?€? relationships between multiple points of data, queries take a long time to execute and maintenance is quite challenging.?? For instance, if you wanted to search for friends on many social networks that both attended the same university AND live in San Francisco AND share at least three mutual friends. Graph databases can execute these types of queries instantly with just a few lines of code or mouse clicks. The implications across industries are tremendous.,Graph databases are gaining in popularity for a variety of reasons.?? Many are schema-less allowing you to manage your data more efficiently.???? Many support a powerful query language, SPARQL. Some allow for simultaneous graph search and full-text search of content stores. Some exhibit enterprise resilience, replication and highly scalable simultaneous reads and writes.?? And some have other very special features worthy of further discussion.,One specialized form of graph database is an RDF triplestore.?? This may sound like a foreign language, but at the root of these databases are concepts familiar to all of us.?? ????Consider the sentence, ?€?Fido is a dog.?€? This sentence structure ?€? subject-predicate-object ?€? is how we speak naturally and is also how data is stored in a triplestore. Nearly all data can be expressed in this simple, atomic form.?? Now let?€?s take this one step further.?? Consider the sentence, ?€?All dogs are mammals.?€? Many triplestores can reason just the way humans can.???? They can come to the conclusion that ?€?Fido is a mammal.?€? What just happened??? An RDF triplestore used its ?€?reasoning engine?€? to infer a new fact.?? These new facts can be useful in providing answers to queries such as ?€?What types of mammals exist??€??? In other words, the ?€?knowledge base?€? was expanded with related, contextual information.?????? With so many organizations interested in producing new information products, this process of ?€?inference?€? is a very important aspect of RDF triplestores.?? But where do the original facts come from?,Since documents, articles, books and e-mails all contain free flowing text, imagine a technology where the text can be analyzed with results stored inside the RDF triplestore for later use.?? Imagine a technology that can create the semantic triples for reuse later.?? The breakthrough here is profound on many levels: 1) text mining can be tightly integrated with RDF triplestores to automatically create and store useful facts and 2) RDF triplestores not only manage those facts but they also ?€?reason?€? and therefore extend the knowledge base using inference.,Why is this groundbreaking??? The full set of reasons extends beyond the scope of this article but here are some of the most important:,Your unstructured content is now discoverable allowing all types of users to quickly find the exact information for which they are searching.?? This is a monumental breakthrough since so much of the data that organizations stockpile today exist as dark data repositories.,We said earlier that RDF triplestores are a type of graph database.?? By their very nature, the triples stored inside the graph database (think ?€?facts?€? in the form of subject-predicate-object) are connected. ?€?Fido is a dog.?? All dogs are mammals.?? Mammals are warm blooded.?? Mammals have different body temperatures, etc?€??€??? The facts are linked.?? These connections can be measured.???? Some entities are more connected than others just like some web pages are more connected to other web pages.?? ??Because of this, metrics can be used to rank the entries in a graph database. One of the most popular (and first) algorithms used at Google is ?€?Page Rank?€? which counts the number and quality of links to a page ?€? an important metric in assessing the importance of web page.???? Similarly, facts inside a triplestore can be ranked to identify important interconnected entities with the most connected ordered first.???? There are many ways to measure the entities but this is one very popular use case.,With billions of facts referencing connected entities inside a graph database, this information source can quickly become the foundation for knowledge discovery and knowledge management.?? Today, organizations can structure their unstructured data, add additional free facts from Linked Open Data sets, combine all of this with a controlled vocabulary, thesauri, taxonomies or ontologies which, to one degree or another, are used to classify the stored entities and depict relationships.?? Real knowledge is then surfaced from the results of queries, visual analysis of graphs or both.?? Everything is indexed inside the triplestore.,Graph databases (and specialized versions called native RDF triplestores that embody reasoning power) show great promise in knowledge discovery, data management and analysis.???? They reveal simplicity within complexity.?? When combined with text mining, their value grows tremendously.???? As the database ecosystem continues to grow, as more and more connections are formed, as unstructured data multiplies with fury, the need to analyze text and structure results inside graph databases is becoming an essential part of the database ecosystem.?? Today, these combined technologies are available and not just reserved for the big search engines providers.?? It may be time for you to consider how to better store, manage, query and analyze your own data.?? Graph databases are the answer.,If there is interest, you can learn more about these approaches under the resources section of 
This is part two of the series. In part one, we used linear regression model to predict the prices of used Toyota Corollas. There are some overlap in the materials for those just reading this post for the first time. For those who read the part 1 of the series using linear regression, then you can safely skip to the section where I applied neural networks to the same data set.,In this post, we will use neural networks! Skip to the Nueral Network analysis section if you?€?ve read part 1 of this series.,Let?€?s load in the Toyota Corolla file and check out the first 5 lines to see what the data set looks like:,Price, Age, KM(kilometers driven), Fuel Type, HP(horsepower), Automatic or Manual, Number of Doors, and Weight in pounds are the data collected in this file for Toyota Corollas.,In predictive models, there is a response variable(also called dependent variable), which is the variable that we are interested in predicting.,The independent variables(the predictors) are one or more numeric variables we are using to predict the response variable. Given we are using a linear regression model, we are assuming the relationship between the independent and dependent variables follow a straight line. Here. with neural network, we DO NOT assume a linear relationship. In fact, that?€?s part of the power and flexibility of a neural network is that it can model nonlinearities in data very well.,But before we start our modeling exercise, it?€?s good to take a visual look at what we are trying to predict to see what it looks like. Since we are trying to predict Toyota Corolla prices with historical data, let?€?s do a simple histogram plot to see the distribution of Corolla prices:,One of the main steps in the predictive analytics is data transformation. Data is never in the way you want them. One might have to do some kinds of transformations to get it to the way we need them either because the data is dirty, not of the type we want, out of bounds, and a host of other reasons.,In this case, we need to convert the categorical variables to numeric variables to feed into our linear regression model, because linear regression models only take numeric variables.,The categorical variable we want to do the transformation on is Fuel Types. We that there are 3 Fuel Types: 1) CNG 2) Diesel 3) Petrol,So, we can convert the categorical variable Fuel Type to two numeric variables: FuelType1 and FuelType2. We assign CNG to a new variable FuelType1 in which a 1 represents it?€?s a CNG vehicle and 0 it?€?s not. Likewise, we assign Diesel to a new variable FuelType2 in which a 1 represents it?€?s a Diesel vehicle and 0 it?€?s not.,The next step in predictive analytics is to explore our underlying. Let?€?s do a few plots of our explanatory variables to see how they look against Price.,Now, let?€?s feed the normalized data into our neural network,Let?€?s see what it looks like(looks like a complex brain with all its neural connections):,RMSE is root mean squared error. A mean squared error(MSE) is the average of the sauared differences between the predicted value and the actual value. The reason we square is to not account for sign differences(negative differences and positive differences are the same thing when squared). RMSE brings it back to our normal unit by taking the square root of MSE>,MAPE stands for mean absoute percent error and express the forecast errors in percentages.,On average, our model had a forecast error of only 11%. Not bad for a first pass at this data set using neural network. With neural network, we usually need more features engineering than the linear regression. Here, we did no features engineering to see how it would perform. And it performs very similar to the linear regresion. Are there ways to improve the model performances? Yes, to both linear regression and neural networks. We will cover those topics in future blog posts.

When Apple CEO Tim Cook finally unveiled his company?€?s new Apple Watch in a widely-publicized , earlier this month, most of the press coverage centered on its cost ($349 to start) and whether it would be as popular among consumers as the iPod or iMac., saw things differently.,?€?I think the most significant revelation was that of ResearchKit,?€? Indurkhya said. ?€?It allows the iWatch to gather huge amounts of health-related data from its sensors that could then be used for medical research, an area that has traditionally been plagued by small samples and inconsistent and costly data collection, and for preventive care.?€?,Indurkhya is in a perfect position to know. He teaches text mining and other online courses for Statistics.com and the Institute for Statistics Education. And if you?€?ve ever wondered about the origins of a term we hear everywhere today ?€? Big Data - the mystery is over. Indurkhya, along with Sholom Weiss, first coined "Big Data" in a predictive data mining book in 1998. (I never anticipated Big Data becoming a buzzword,?€? he said. ?€?although we did expect the concept to take off.?€?),The ResearchKit already has five apps that link users to studies on Parkinson's disease, diabetes, asthma, breast cancer and heart disease. Cook has , other health benefits from Apple Watch, including its ability to tap users with a reminder to get up and move around if they have been sitting for a while. ?€?We've taken (the mobile operating system) iOS and extended it into your car, into your home, into your health. All of these are really critical parts of your life,?€? Cook , a Goldman Sachs technology and Internet conference recently.,That helps explain the media fascination over another new Apple product. But it also tells us the importance of learning about Big Data. Having access to large amounts of raw numbers alone doesn?€?t necessarily change our lives. The transformation occurs when we master the skills needed to understand both the potential and the limitations of that information.,The Apple Watch exemplifies this because the ResearchKit essentially recruits test subjects for research studies through iPhone apps and taps into Apple Watch data. The implications for privacy, consent, sharing of data, and other ethical issues, are , The Apple Watch likely won?€?t be the only device in the near future to prompt these kinds of concerns. It all leads to the realization that we need to be on a far more familiar basis with how data is collected and used than we?€?ve ever had to be in the past.,?€?We are increasingly relying on decisions, often from "smart" devices and apps that we accept and even demand, ??that arise from data-based analyses,?€? Indurkhya said. ?€? So we do need to know when to, for example, manually override them in particular instances.,?€?Allowing our data to be pooled with others has benefits as well as risks. A person would need to understand these if they are to opt for a disclosure level that they are comfortable with. Otherwise the danger is that one would go to one or the other extreme, full or no participation, and have to deal with unexpected consequences.?€?,The Big Data questions raised by the Apple Watch are similar to the concerns over access to and disclosure of other reams of personal information. Edward Snowden?€?s leaks most famously brought these kinds of worries into play, publicizing the spying on ordinary Americans by the National Security Agency. There?€?s also commonly expressed fear that Big Data is dehumanizing, and that it?€?s used more for evil than for good.,These fears, Indurkhya noted, have seeped into the popular culture. Consider this , of Big Data movies: ,, in which a super computer is given control of all United States defense assets. ??,, in which a data scientist hacker hopes to eventually bringing down the entire U.S. financial system. Even Batman gets into the act, , every cell phone in Gotham.,First, he said, there are strong parallels between the Big Data revolution and the industrial revolution. Look at history. Despite all the dire predictions, machines aren't "taking over the world" and neither will Big Data.,Second, it?€?s also helpful to appreciate what Big Data gives us. It provides us with better estimates - they are more accurate and our confidence in them is higher. Perhaps more importantly, it provides estimates in situations where, in the absence of Big Data, answers were not obtainable at all, or not readily accessible. Think about searching the web for ??"Little Red Riding Hood and Ricky Ricardo."?? Even in the early days of the internet, you would have gotten lots of results individually for "Little Red Riding Hood" and ",," but it was not until Google had accumulated a massive enough data set, and perfected its Big Data search techniques, that you could reliably get directed to the "," episode where Ricky dramatically reenacts the story for little Ricky.??,Data specialists can set policies and procedures that protect us from some of the risks of Big Data.?? But we also need to become much more familiar with how our data is collected, analyzed, and distributed. If the Apple Watch rollout proves anything, it might be this: Going forward, we?€?ll all have to be as smart about data as our devices.
The comparison is performed on a data set where linear regression works well: salary offered to a candidate, based on programming language requirements in the job ad: Python, R or SQL. This is a follow-up to the article ,.??The increased accuracy of linear regression estimates is negligible, and well below the noise level present in the data set. The Jackknife method has the advantage to be more stable, easy to code, easy to understand (no need to know matrix algebra), and easy to interpret (meaningful coefficients).,Jackknife is not the first regression approximation developed by the author: ,??pages 172-176 for other examples.,We compare three different methodologies:,The results are displayed in the comparison tab, in the same spreadsheet, as well as in the above figure.,:,The dependent variable (to be estimated or predicted) is the overall salary. The data was simulated and represents reality extremely well based on recruiting strategies (see data tab in the spreadsheet to see how this was done), using a model where each professional, in a specific education / years of experience segment, has a salary simulated based on 5 semi-random components:,Cross-correlations were introduced between Python, R, and SQL, and can be fine-tuned (increased or decreased) by playing with the parameters in cells L1:M1 (highlighted in orange background) in the data tab. There are eight types of professionals, encoded using the formula??,Code = Python + 2 * R + 4 * SQL,where Python, R, and SQL are binary variables representing whether or not the data scientist has Python, R or SQL skills. When you change the cross-correlations, make sure you have at least 2 or 3 professionals for each code, otherwise the results in the comparison tab will show some errors.,We use modern metrics to show the yield for regression or Jackknife, above the base model (base model = salary, for each professional, is estimated as the average salary across all professionals).,In each case (Jackknife, regression, base model), the model estimates the individual salary based on which skills the data scientist owns, from a pre-selected list of skills: Python, R, and SQL.,.,A few comments about the data:,Also, note that??,??are used in the spreadsheet, in the comparison tab, to compare models; these L1 metrics should be preferred over traditional L2 metrics such as R-Squared. L2 metrics are sensitive to outliers.,The formulas and details about Jackknife are found in my article??,. The model is so simple that we implemented it in our Excel spreadsheet. Linear regression is also implemented in the same spreadsheet. Note that we used the most rudimentary version of Jackknife, using 0 parameters. The regression technique involves three parameters (not including the intercept in both models). Codes with a large number of onservations, as expected, have better predicted value, especially for the Jackknife.,Here's an additional summary table:,Even though standard regression seems to be performing much better, predictions for individual salary - regression versus Jackknife - are not far off, as illustrated in the top figure. Both for regression and Jackknife, only 8 different estimated values are generated, since we have just 8 codes. Note that if we boost correlations to the point that Correl(Python, R) = 1, then the linear regression model will crash, while the Jackknife will perform nicely.,Rudimentary, approximate methods such as Jackknife regression (not to be confused with Efron's bootstrap) are just nearly as good as so-called , models such as traditional regression, for predictive modeling. The reason is because data is anything but exact, and statistical models are approximate representations of the reality: all models are wrong, some are not as wrong as others. Approximate solutions provide substantial advantages: easy to code (even in SQL) and understand, robust, and easy to interpret. In short, they are a good choice for inclusion in black-box, automated data science.,Finally, the following table pretty much summarizes the small differences between Jackknife and standard regression, for predictions. By error, I mean the absolute value of the error, averaged ??across the 8 codes,??and weighted based on the number of observations in each code. So it's a weighted average. The base model corresponds to the case where the salary estimate is identical (and equal to the mean), for all data scientists.

NOTE: This post can be more beautifully read via Medium here:??, 
Did you know that our bodies have over 150 trillion gigabytes of data within it? That is enough to fill 75 billion 16GB iPads.,Although we cannot harness this kind of data to help us store data or to help build complex internal databases, we can use it successfully in healthcare.,This practice is in its infancy at present, but has opened up significant opportunities that are going to grow in the future.,The way that people currently interact with their doctor is simple, they turn up to the surgery, talk to them for 5 minutes, have some tests, then leave.,The likelihood is that they spend less than an hour with their doctor each year, meaning that constant monitoring is not even close.,Through sensors and the ways in which data can be processed, it is possible for a doctor to monitor a condition or simply your wellbeing without even needing to contact you. The benefits of this go beyond convenience and actually allow for the effects of medication to be noted, the state of existing conditions monitored or even just general health checkups done remotely.,Case studies are a vital part of treatment. They let doctors know the best way to treat conditions or diseases alongside the different potential side effects that these treatments can cause.,The majority of case studies consist of individual cases or small sample studies, which are useful, but do not have the same resonance as large datasets.,Big Data allows these large datasets to be be created, meaning that treatments can be more specific as the data can be mined for specific purposes.,For instance, somebody with two particular conditions may be something that one doctor has not experienced or have not been widely covered in case studies. With a database consisting of millions of sufferers and their treatments, it will soon be easy to see which work and which don?€?t.,Gene sequencing is the next ideation of this movement, the idea that through modelling the way that diseases have evolved in the past can predict the way that they will behave in the future.,This is commonly used in cancer treatments today, but is expensive, making it prohibitive to a large proportion of the population. If this is done more frequently and the price decreases, this kind of work could help in preventing diseases and slowing the spread of epidemics.,Imagine if we had this kind of power when Ebola first broke out? It would have been possible to see how the disease was likely to have evolved, the best treatments and the most successful ways to stop its spread.??
Business Intelligence Software enables organizations to centralize data and understand key business metrics that drive performance improvements. It empowers businesses to gain meaningful insights and make better decisions. Selecting the right BI solution could mean the difference between business growth and a wasted opportunity. Here are 8 factors you can consider while selecting Business Intelligence (BI) Software.,????, ?€? Today, almost every organization uses multiple data sources and has users who access data from various locations. A good BI software should not only support the data sources that you use but also provide cross-platform data access that your users have currently.,??, ?€?According to a ,, over 50% CFO?€?s need BI to monitor ongoing Business Performance. Key decision Makers and stakeholders like C-level executives rely on the latest information to make decisions. A BI system that provides real-time Business Intelligence can boost your Business Performance rapidly. It allows you to stay up-to-date with what?€?s going on in your business, without the need to rebuild or re-configuring anything.,??, ?€? Business Intelligence helps you gain insights from data. Insights trigger questions, discussions & feedback. Therefore, a BI solution must provide allow users to communicate & collaborate with each other. Generally, BI creators are different from BI consumers. BI creators are mainly analysts, developers or data scientists who analyze the data while the consumers are Managers and Executives who look at the final reports. A Business Intelligence Software must provide collaboration features like the ability to share insights, comments & questions; share reports with others in multiple formats.,??, ?€?Business Intelligence Solutions provide various deployment options. They are available as cloud-based / on-premise / standalone desktop applications. Cloud-based solutions make it easy to setup, manage and share BI without depending on IT. They provide for a lower cost of ownership through reduced resources, rapid deployments, and are suitable for small & medium businesses. On-premise BI solutions are self-hosted solutions generally deployed over intranet by the IT team. They are suitable if you have an IT team to manage the solution and your business deals with sensitive data that you may not be comfortable leaving your network. It is important to find a Business Intelligence Software that complies with the IT policies of your organization.,??, ?€? Every organization is driven by key business metrics and these metrics are governed by specific business rules. A Business Intelligence solution should provide the analytical capabilities to calculate these metrics. Building a proof-of-concept is a great way to evaluate all the features of a solution in detail. It also helps you discover the problems your users may face, if a solution is adopted.,??, ?€? Multiple stakeholders and decision makers use Business Intelligence Software to gain insights from various data sources in your organization. As a result, it is important to control which BI user has access to what kind of information. A Business Intelligence Software should, at least, let you assign or revoke permissions to control who can view and who can update your BI system.,??, ?€? The output of every Business Intelligence platform is a set of reports or dashboards used to communicate insights & trends to key stakeholders. Without powerful reporting capabilities, you may be unable to correctly communicate insights to decision makers. It is important to choose a BI Software that provides a variety data visualization and report generation features. This allows you to communicate various kinds of information to different types of users.,??, ?€? Different type of users consume information through various platforms and devices. For instance, Analysts and Developers use widescreen monitors like the iMac while Managers and Executives mostly rely on tablets & phones. Your Business Intelligence software needs to be responsive enough for Analysts to analyze data on their iMacs and Managers to view reports on their tablets or phones. A responsive user interface lets you build once and view anywhere.,??,The right BI solution can help your business become more efficient, spot areas suitable for cost savings and identify new growth opportunities. Being able to monitor all the important information about your business, on-demand, in one place, enables you to know precisely how your business is doing, which areas are growing well and what to fix. It motivates your team to stay focused and grow your business faster.,About the author:,Sreeram is the founder of??,, a web-based Business Intelligence & Reporting Application.

The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.??,These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.,??,??|??,??|??,??|??,??|??,??|??,??|??
Hadoop has been the foundation for data programmes since Big Data hit the big time. It has been the launching point for data programmes for almost every company who is serious about their data offerings.,However, as??,??we are seeing that the rise in in-memory databases has seen the need for companies to adopt frameworks that harness this power effectively.,It was therefore no surprise that Apache have launched Spark, a new framework that utilizes in-memory primitives to deliver performance around 100 times faster than Hadoop?€?s two-stage disk-based version.,This kind of product has become increasingly important as we move forward into a world where the amount and speed of data has been increasing exponentially.,So is Spark going to be the Hadoop beater that it seems to be?,This kind of technology that allows us to make decisions quicker and with increased amounts of data is going to be something that companies are clamouring for.,It is not simply in principle that this platform will be bringing about change either. As an open source platform, it has the most developers working on it across every Apache product.,This suggests that people support the idea through their willingness to dedicate their time to it. It is common knowledge that many of the data scientists working on Apache products are the same ones who will be using it in their day-to-day roles at different companies, which could suggest that they are going to adopt this system in the future.,One of the main reasons for the success of Hadoop in the last few years has been not only due to its ease of use, but also that companies can get it for nothing. This is because you can run the basics of Hadoop on a regular system and will only need to upgrade when they ramp up their data programmes.,Spark runs on-memory systems which requires a system with high performance, something that companies new to data initiatives are unlikely to invest in.,In my opinion, Hadoop will always be the foundation of data programmes and with more companies looking at adopting it as the basis for their implementations, this is unlikely to change.,Spark may well become the upgrade that companies who move to a stage where they want, or need, improved performance will adopt. As Spark can work alongside Hadoop this seems to have also been in the minds of the guys at Apache when coming up with the product in the first place.,Therefore, it is unlikely to be a Hadoop beater, but will instead become more like its big brother. It is capable of doing more, but at increased cost and only necessary for certain data volumes and velocities, is not going to be a replacement.??
People often talk about a startup?€?s evolution in terms of funding,??,, or??,. But companies also change over time in the ways that they interact with data.,Increasingly, the companies that achieve massive growth are anticipating future data needs and progressing quickly through four distinct phases of data.,Whether you?€?re an early-stage founder and or an analyst hired after this evolution has already begun, take time to identify where you are in theses phases. Setting up the right team, tools, and processes during each phase can help your entire team more effectively use data to achieve other outward-facing milestones.,Before you launch your product, the majority of your data is qualitative. As you??,, you?€?ll (hopefully) start to see the feedback center around specific topics.,These qualitative data points?€?the problems for which you are creating a??,?€?are likely the inputs that will inform your initial data strategy.,Once people actually start using your product, their feedback can lead product development in unexpected directions. You?€?ll want data to validate this feedback, so make sure your product is instrumented in a way that will allow you to retroactively measure new things. Build robust data tracking into your culture. Encourage engineers to make detailed??,??an integral part of the development process from day one. The longer you wait, the harder it becomes.,Before we launched Mode, we made a decision to classify ?€?Run?€? events?€?query executions against a database?€?into a few categories: runs against private databases, runs against the public database, and runs from the report view. Analysis of this granular event data, corroborated by feedback, helped us identify two strong user types: analysts who write queries and business folks who view them. As a result, we?€?ve been able to better understand in-product behavior and build features to engage each group.,After your product launches, your first challenge is to figure out what?€?s happening. Questions in this phase often sound like:,With out-of-the-box reporting products like Google Analytics or Mixpanel, you?€?ll be able to get to a lot of answers quickly. Qualitative information from ongoing customer conversations will still guide many of your decisions?€?and simple measures of engagement like DAUs are enough to ensure you?€?re headed in the right direction. Be careful not to over-engineer metrics just yet?€?at this point, agility matters, and you can?€?t afford??,.,As you progress through this phase, you?€?ll likely start to measure actions that are unique to your product. In building these dashboards, try centering them around,. Picking the??,??can be tough. It can be tempting to focus on revenue, for example, but it?€?s a??,??and won?€?t be helpful when making decisions about what to build next (,).,Once built, get these dashboards in front of the whole company on a regular basis. When health-of-the-business questions crop up, answer them with context??,data. By talking about metrics early and often, you?€?ll begin to cultivate a culture of data literacy and??,??for data (and the people working with it). As the team becomes accustomed to seeing data, they?€?ll recognize patterns, more quickly notice anomalies, and navigate decisions substantiated with data.,So, once you?€?ve nailed the ?€?whats,?€??€?you will start hearing more ?€?whys:?€?,The tools that helped you measure standard metrics aren?€?t flexible enough to answer this new class of questions?€?they?€?re built to be broadly applicable rather than tailored to your business. If you?€?ve built custom dashboards, those are often a good starting place for answering the ?€?whys.?€? You can use the methodologies that drive the dashboards as a jumping off place to answer these tougher questions.,The questions to be answered are often unique and involve bespoke research?€?we see companies at this point really dive into their databases and use SQL to answer these questions. Depending on the state the data is in, we see many companies start building out their analytics teams with??,.,Lots of ad-hoc work can get tedious and block high value projects, so it?€?s important to begin automating solutions to common problems and democratizing access to the information. Folks will be able to self-serve some answers on their own?€?the ?€?whats?€? are still important but there?€?s not much reason for analysts to be spending their time reactively pulling data.,Often, this phase sneaks up quietly. The nuanced difference between the ?€?why?€? stage and this one is proactive discovery.,To move into stage three, you?€?ll need to make sure a few things are in place:,With these achievements unlocked, reverance for data pervades company culture. Decision makers are empowered to make data-driven decisions independently and analysts can spend more time on forward-looking, high-leverage projects. This is the sort of thing happening at Uber, Facebook, and other companies with strong data culture.,In phase three, you also start to see your time horizon increase. You can start to dedicate parts of your analytics team to focusing on product decisions that affect work half a year out.,As an example,??,??has run experiments on page load times to understand how site performance impacts behavior. They?€?ve used this information to make better decisions about allocating resources towards site performance improvements. Intentionally making your site worse may sound crazy, but it?€?s incredibly efficient in identifying the opportunity for improvement. For companies who have solved most recurring problems and are advanced enough to pull this off, the dividends can be tremendous.,Not everyone has the benefits of building data infrastructure and an analytics team within a company from day one. If you?€?re jumping into a company mid-evolution start by assessing where your company lies between these phases. With a complete view into how your company currently works with data, you?€?ll be able kick off meaningful projects?€?be it building the right team, developing stronger tracking and warehousing practices, or implementing new??,. You?€?ll empower everyone to use more data more effectively, allowing your whole team to achieve faster growth and dominate your market.,This post originally appeared on the ,.

??2015 will be the year of the Big Data revolution, when it crosses over from being the preserve of the experts to a mainstream issue for every business. It?€?s the subject being discussed in every boardroom and written about in every business journal.,It?€?s a game-changer for companies, such as Amazon, with access to unimaginable amounts of data ?€? they will soon be able to predict what you?€?ll buy accurately enough to despatch it before you?€?ve even bought it ?€? but many business leaders feel overwhelmed and intimidated by the subject. They realise the use of data and analytics offers huge opportunities for their business, but have no idea where to start or how to manage the sheer scale of information at their disposal.,That?€?s the argument of big data expert and best-selling author, Bernard Marr, who helps leading international brands develop their analytics strategies. His clients include the likes of Microsoft, Mars and Orange, as well as the Bank of England, the NHS and the Ministry of Defence.,But his new book on Big Data is just as relevant to the smallest start-up as a global giant because he believes any company can transform its business, irrespective of how much, or little, data it has at its disposal.,It?€?s a very practical, accessible guide that any manager or business owner can learn from. It?€?s a book for the layman, not just the big data specialist. The purpose of the book is to explain what Big Data is in very simple terms so anyone can understand how to use it.,What is Big Data? Marr defines it as ?€?the idea that everything we do is increasingly leaving a digital trace which we can use and analyse to become smarter.?€? He says the driving forces are access to ever-increasing volumes of data and an ever-increasing technological capability to mine the data for commercial insights.,But, according to Marr, most businesses make the mistake of starting with data, instead of starting with strategy. The key is to first understand your business objectives and then find the data that will help you answer your key strategic questions.,He explains how to use his Smart model: Start with Strategy, Measure Metrics and Data, Apply Analytics, Report Results, Transform Business. Creating a strategy board allows you to identify the relevant data, but the mistake many companies make is to bury the results in impenetrable 50-page reports that no-one reads rather than presenting them in accessible info-graphics. His guide to data visualisation is designed to empower CEOs and business owners; they need to take control of the data, not just leave it to the data scientists.,Marr, who is chief executive of the Advanced Performance Institute, argues that companies which embrace the Big Data revolution ?€? or Smart Data, as he prefers to call it ?€? will wrong-foot their rivals. He gives practical examples of the work he has done with companies, helping them find the , data to solve their business problems. Working with a small fashion retail company, which wanted to increase sales but lacked smart data, he devised a series of smart questions: How many people pass your shops? How many stop to look in the window and for how long? How many come into the shop and how many of those buy?,By installing a device into the shop windows that tracked cell phone signals, they discovered how many people were stopping and how many of those were coming in. By combining that with transaction data, they could measure the conversion ratio and test which window displays and offers were most successful in increasing conversion rates.,Practical examples like this make the book an invaluable guide for the business leader, or frontline manager, just as much as the data scientist. But it will also help data scientists explain the relevance of what they do, allowing them to think about the strategic application of their work. Henrik von Scheel, an advisory board member at Google, calls the book: ?€?The go-to-guide on data for 2015?€?.,Marr?€?s message is clear: successful companies understand where their customers are, what they are doing and where are they going. Companies that won?€?t embrace the smart revolution will be left behind.,To celebrate the launch of this new book you can now get a second, supplementary case study book, for free. Here?€?s how it works?€?. If you order a copy of the book (paperback or Kindle) from Amazon on or before Friday, March 21th and send a screen shot of your order confirmation to bigdatabook@wiley.com the publisher will send you a free and exclusive book of case studies ?€? a 32 page e-book showcasing some of the best uses of big data by companies all over the world. You?€?ll learn how each company has implemented their big data strategy and the results it has brought.,You can read a , 
Great article published in Vox.com. Here are a few of the most spectacular ones.,1. Privatization of the Internet backbone (1994),2. Chrome taking over the world (animated image: click on picture to see animation),3. Places with no broadband access in 2011,4. Fiber optic cables,Read full article ,??(40 maps total). Also, you can find a great selection of articles about data visualization ,.

The National Center for Injury Prevention and Control (CDC's Injury Center) is pleased to announce that it will sponsor two supplemental issues of the journal Injury Prevention. The first supplement will highlight population-level change in injury prevention while the second supplement will focus on data.??,Call for Papers: ?€?Injury surveillance: Next generation?€? - In recent years, we have seen dramatic changes in the knowledge economy brought about by the increased capabilities of communications technology, and by increased recognition of the potential of information to empower society to achieve its goals (including use of new data and data sources, real time data access, big data, visualization, electronic health records, data linkage, and syndromic surveillance). This supplement will provide an opportunity for people to demonstrate that surveillance is an essential element of contemporary injury prevention practice, and will demonstrate how the new technologies can be harnessed by injury surveillance systems to achieve even better injury prevention benefits.,The journal invites authors to submit proposals for the supplements. Potential authors should submit a 1000 word proposal by email to the guest editor, Dr. Rod McClure, at rmcclure@cdc.gov. Submissions must be received by April 30, 2015 and include the following:??, The surveillance methods used, and evidence of the extent to which it has been or could be??applied to enhance injury prevention practice.,The guest editor will review the proposals and invite selected authors to submit a full manuscript. The guest editor will support the authors during the formative phases of the manuscript preparation. The submitted final manuscript will be subject to formal independent peer review in accordance with the Journal?€?s usual practice.,Proposal submissions should be submitted by April 30, 2015 to Dr. Rod McClure at rmcclure@cdc.gov.,Deadlines and Publication Date, ?€? Proposals due: April 30, 2015 , ?€? Response to proposals: May 31, 2015 , ?€? Full manuscripts due: August 31, 2015 , ?€? Supplements published: mid-2016,Injury Prevention is an international peer review journal, offering the best in science, policy, and public health practice to reduce the burden of injury in all age groups around the world. The journal publishes original research, opinion, debate and special features on the prevention of unintentional, occupational and intentional (violence-related) injuries. It is the official journal of the International Society for Child and Adolescent Injury Prevention (ISCAIP) and the Society for Advancement of Violence and Injury Research (SAVIR).??, ??, For more information about the journal supplements, contact the guest editor, Dr. Rod McClure at rmcclure@cdc.gov.,For more information about the CDC Injury Center visit ,.
This list was published ,,??and it is a bit old (2013). Yet still very valuable. Below are the top 25 in the list, with the number of downloads on the right-hand side.,??to access full list.
The sheer volumes involved with Big Data can sometimes be staggering. So if you want to get value from the time and money you put into a data analysis project, a structured and strategic approach is very important.,The phenomenon of Big Data is giving us ever-growing volume and variety of data we which we can now store and analyze. Any regular reader of my posts knows that I personally prefer to focus on Smart Data, rather than Big Data - because the term places too much importance on the size of the data. The real potential for revolutionary change comes from the ability to manipulate, analyze and interpret new data types in ever-more sophisticated ways.,I?€?ve written previously about my SMART Data framework which outlines a step-by-step approach to delivering data-driven insights and improved business performance.,Understand your customers better, optimize business processes, improve staff wellbeing or increase revenues and profits.,My work involves helping businesses use data to drive business value. Because of this I get to see a lot of half-finished data projects, mothballed when it was decided that external help was needed.,The biggest mistake by far is putting insufficient thought ?€? or neglecting to put any thought ?€? into a structured strategic approach to big data projects. Instead of starting with strategy, too many companies start with the data. They start frantically measuring and recording everything they can in the belief that big data is all about size. Then they get lost in the colossal mishmash of everything they?€?ve collected, with little idea of how to go about mining the all-important insights.,This is why I have come up with the 90/10 rule ?€? When working with data, 90% of your time should be spent on a structured strategic approach, while 10% of your time should be spent ?€?exploring?€? the data.,The 90% structured time should be used putting the steps outlined in the SMART Data framework into operation. Making a logical progression through an ordered set of steps with a defined beginning (a problem you need to solve), middle (a process) and an ending (answers or results).,This is after all why we call it Data Science. Business data projects are very much like scientific experiments, where we run simulations testing the validity of theories and hypothesis, to produce quantifiable results.??,The other 10% of your time can be spent freely playing with your data ?€? mining for patterns and insights which, while they may be valuable in other ways, are not an integral part of your SMART Data strategy.,Yes, you can be really lucky and your data exploration can deliver valuable insights ?€? and who knows what you might find, or what inspiration may come to you? But it should always play second-fiddle to following the structure of your data project in a methodical and comprehensive way.,I think this is a very important point to make, because it?€?s something I often see companies get the wrong way round. Too often, the data is taken as the starting point, rather than the strategy.,Businesses that do this run the very real risk of becoming ?€?data rich and insight poor?€?. They are in danger of missing out on the hugely exciting benefits that a properly implemented and structured data-driven initiative can bring.,Working in a structured way means ?€?Starting with strategy?€?, which means identifying a clear business need and what data you will need to solve it. Businesses that do this, and follow it through in a methodical way will win the race to unearth the most valuable and game-changing insights.,I hope you found this post interesting. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have.
This idea came to me out of the blue. I was scrolling through??,??board in search of inspiration. I noticed that some posts (very insightful ones, I thought) ended up at the bottom of the list, other posts were popular but had no comments and some triggered lots of comments but were ranked very low. I can imagine that being popular on Hacker News means a lot to a contributor: s/he gets a ton of views, the post generates heated discussion and may encourage other people to share the content with others or reference it in their work. I knew this was my topic. I decided to find out??,. Lets see if I managed to crack this one up :),I think everyone would agree that the best way to analyze posts on Hacker News is to transform the website into a database since information captured there is pretty well structured. Just like with my??,??I used??,??for scraping data from Hacker News.,My original dataset contained 1,163 posts submitted by 900 users from 680 sources between July 2010 and March 2015.,Each row captured the following information:,Generally every dataset downloaded (or scraped) from the web needs a certain amount of cleaning and transformation. In my??,??(about Law School Admissions) I was mainly focusing on data transformations, whereas today my data doesn't need as much massaging. Instead, I have to do a fair amount of standardization to make the data uniform and "analyzable".,And here goes...,With this dataset I had to standardize the following fields:,I know transformations like these sound trivial, but I have come across many situations where analysts transform text fields into numerical ones but then forget to standardize them to match the same measurement unit. This is especially common in standardizing healthcare data where patients' test results displayed in mg/dL or g/L get analyzed together without any standardization. This is a silly mistake that leads to pretty bad misinterpretations of the data.,For this blog post I actually prepared 2 datasets. The original dataset looks very similar to the one displayed above (after standardization & cleanup) and the other dataset is prepared specifically for text mining.,My goal was to decompose News Topic field into a selection of key words each topic name consisted of. I used??,??package for it. Here are the steps I had to take to parse News Topic field.,This matrix lists every word parsed out of News Topic field along with Post ID which I can later on link back to my original dataset (R preserved the order of records, so that row 21 in this dataset corresponds to 21st row in my original table).,I then obtained basic counts on these words and identified top 30 most frequently used ones. I also excluded frequent words that don't carry any critical information (e.g. "the", "and", "with", etc). I ended up with 53 most common words used in Hacker News posts. Because every post in my new dataset had to have at least one of these 53 words my overall sample size went down from 1,163 to 594 posts (2x down). This is not a bad sample size reduction given I went from a list of 3,031 words to 53 words (57x down).,As I stated in the beginning, the goal of my blog post today was to understand??,.,Here is the visualization I put together. It is interactive, so feel free to play with it.,Top charts show Top 10 Sources (on the left) and Top 15 Users (on the right) who posted on Hacker News based on the # posts they submitted. Color coding represents average # points each source or user got per post.,Bottom left scatter plots show all posts submitted based on the # Points and # Comments as well as the # Points and Post Recency (# minutes elapsed after post submission). By clicking on any bar on the top you will be able to filter posts on the bottom chart from a selected source or a user.,Finally, bottom right table outlines top 10 key words used in Hacker News Topics based on average # of points and comments a post using this word received.,For text mining descriptive statistics I put together a nice visualization in??,??that sizes words used in Hacker News posts based on frequency.,GitHub is rocking the stage and not just by building a community of talented developers who share their work with others, but also by telling the world about awesome things github does. No wonder, github submitted the biggest number of posts (N posts = 40) and also ranked 1st in average # points per post (Avg N points = 112). Medium.com and techcrunch.com were next on the list. It is quite interesting that while top posting sources were tech-oriented (maybe except for Medium), we still observed large presense of media platforms like Bloomberg, Washington Post, NY Times and BBC.,When it comes to posting sources, there seems to be a pretty strong correlation between # posts and average # points each post from that source receives. However, this is not always true for users. People who post the most are not always the ones getting the most attention. For example, top user??,??submitted 64 posts but on average received only 21 point per post (which is still pretty good by the way), whereas??,??only had 10 posts but got an average 181 points per post. Out of curiousity I looked at all posts by??,??(by clicking on??,??bar on the top right chart which filtered scatterplot by this author's posts) and realized that most points came from the post named after Sir Terry Pratchett (now I absolutely have to read it!).,Speaking of Topic News, I would like to move to Wordle viz for a minute. It seems that Hacker News contributors like to use actionable words to name their posts. Verbs like "show", "ask", "use", "get" make it to the post names most frequently. Users also mention techy words like "google", "startup", "bitcoin", "app", "develop", "code", "javascript", "data" and "web". From the bottom right table I also found that posts that talked about development, code, google, javascript, open (source?) and tech won hearts of readers and, hence, obtained most points and comments. Well, this is Hacker News in the end...,From the scatter plots on the bottom it looks like there is a positive correlation between # points a post obtains and # comments it receives. Post ranking, however, doesn't seem to impact either point or comment volumes. I also observed some correlation between recency of the post and # points it obtained. This is intuitive because I would expect older posts to receive more points (just by the nature of being on the website longer). But there seems to be a chunk of posts that don't get many points no matter how long they hang on the news board. On the other hand, some posts that gain popularity are in fact not that old.,Based on my descriptive analysis I generated the following hypotheses:,Since there seems to be a linear relationship between post's popularity (# Points) and other factors, I chose linear regression model to address my hypotheses. So my model looked as follows:,??- # Points a post received,??- # Comments a post received,??- whether a post contained techy words or not (based on the viz above),??- # Minutes elapsed since post submission,??- A group of posts based on their ranking on the news board (e.g. Top 100, 100-249, 250-499, etc.),I used??,??function to perform the analysis and assess each factor's impact on the post's popularity. Here is the output of linear regression analysis.,No wonder, comments add to post's popularity. With each additional comment the post on average receives 1.25 more points (p<0.01). When contributors use tech words in their News Topic names their post gains 14.68 more points (p<0.01). Here is the list of tech words that will help your post gain popularity:??,.,Recency of post submission also seems to contribute to the # points it receives, however, the magnitude is low. On average, every minute your post "ages" it gains a fraction of a point. Put it differently, with every additional week your post sits on the website it gains 0.00032*60 (hour) *12 (day) * 7(week) = 1.6 points (p<0.01). This finding is somewhat contradictory to the next one telling us that Top 100 posts are likely to gain 21.41 points more than bottom ranked posts (p=0.05). But how will your post be in the top 100 if it has been sitting on the website for a week? I think the post ranking effect in this case outweighs the effect of post recency.,For curious souls the R^2 of my model was 0.42 which is not that bad given I knew almost nothing about posts themselves (except, of course, for key words in the post name).,Based on all findings I gained from this analysis, here is my recipe to??,:,I feel like I have done enough talking for today. What do you think about this analysis? See if you can poke holes, I promise I will not be offended :)
.,Big data makes a noteworthy contribution to the usefulness of an application, but its presence can make the design of a clean and usable interface rather difficult. Today, many web applications are built on the platform of big cloud-based data, which leads to the question: how can a designer deliver all the necessary data in an application without making a train-wreck of everything?,Creating a balance between complex data requirements and a simplified user interface is a big challenge for most web app designers. To simplify the user interface, one may have to strip away most of the information, but doing this takes away from both usability and functionality of the resulting application. Conversely, maintaining the mountain of data as is can lead to creation of an impracticable application.,Below are a few tips that you can implement to help create simpler user interfaces for applications relying on complex and big data for their operation:,The principle behind having a good interface design bases on a thorough understanding of the problem at hand, which the application will solve. Starting with the kind of design you want as a client or the data to use will lead to poor decision-making based on flawed criteria.,To begin with, define the user case, which will dictate the design process. Find out how most people will use the application, understanding that it is impossible to cater for the needs of every single user. The benchmark for a good design is one that pleases and is usable by most users at most times.,Using big data for web app design resembles the work of a sculptor ?€? you start with this massive block and then slice away until you have only what you need. For every section of the application, map out all the data that could be fitted into the section, and then prioritize and organize according to the main use case.,This includes all information remotely related to the actual application use ?€? you will probably end up with a gigantic mess. From this, sculpt down, sort, prioritize and arrange the information. Hide some items, change typefaces, colors and font weights and add interactive elements where necessary, basing every decision on the use case.,When using big data, you don?€?t need any extra chrome ?€? including several textures, sweeping gradients, drop shadows among other elements will only increase the complexity of an application that?€?s complex already. This is not to say that you simply have to go with black text against a plain white background.,Instead, invest effort towards improving typography, using color to distinguish data groups and reducing words to iconography where possible so that the content itself creates a visually stimulating layout.,No design is ever final; redesign is an inevitable part of design. As the application is used, there will be aspects that will need further simplification and changing to make the design cleaner and better. Therefore, to make work easier, break all elements down into their smallest components in the backend to make rearrangement easier every time a change is necessary., , , 
The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.,These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.,??,??|??,??|??,??|??,??|??,??|??,??|??
A Visual Studio 2013 demo project including the WebpageDownloader and LinkCrawler can be downloaded ,.,The US digital universe currently doubles in size approximately every three years [1]. ??In fact, Hewlett Packard estimates that by the end of this decade, the digital universe??will be measured in ?€?Brontobytes?€?, which represent one billion Exabytes or around??two quadrillion years of music [2]. Each minute, internet users send over 204 million??emails, Google receives over 4 million search requests, YouTube users upload 72 hours??of video, and 277,000 tweets are posted on twitter [3]. It is estimated that in 2012,??only 1/2 a percent of the data in the US digital universe was analyzed [1]. Data mining web content efficiently is becoming an all to common task. ??When crawling web pages or downloading website content, a single threaded approach can often leave one waiting unreasonable amounts of time for critical information. In previous articles, I have written on many topics about extracting valuable information from the data contained in webpages (references are included below). ??In the machine learning world, we typically refer to this as extracting "features" from webpages.,One of the most basic features which can be extracted from web content is a webpage's HTML which can simply be downloaded using C#. ??Webpage HTML content can also be processed further to produce more elaborate and complex features. ??For example, the HTML downloaded from a webpage contains much different information than the text and images which are displayed in the browser using the webpage's URL. I have provided??just a few examples of valuable features which can be extracted when mining webpages in the list below including some references for further reading on some topics:,However, almost all successful webpage feature extractions begin with simply downloading webpages. ??The download process can also be the largest bottleneck in feature extraction systems, especially when you are downloading content that you did not create and may not have any idea where the actual content is located. ??Sometimes HTML can be full of errors, missing scripts, malicious content, or located on very old servers, down servers, or a large number of other reasons which could potentially cause much grief during the download process. ??In fact, when you are mining thousands, tens-of-thousands, or millions of webpages, you are guaranteed to run into webpages that cause the download process to either fail or hang. The remainder of this article demonstrates how to successfully mine large volumes of webpages by downloading them in parallel, keeping a detailed log of your progress, and even retrying downloads that fail or time out.,I have created a tool in C# called the "Webpage Downloader". ??This class can be used within any C# program to download large volumes of webpage content in parallel. ??Creating the class in a C# program is very simple as shown in Figure 1.,Each webpage downloaded by the??Webpage Downloader is returned in a WebPageItem class. ??The webpage item class includes all of the relevant details for a particular download and could easily be further extended to meet business requirements for a particular project. ??Figure 2 shows each of the current properties supported by the WebPageItem class which are visible in the class constructor.,The??WebPageItem in Figure 2 contains each downloaded webpage's URL, HTML, the server's response url (important to detect a redirect), an error flag, error message, a flag indicating if the server provided a successful response, and the URL's byte array data, if the requested URL is downloaded as binary file. ??Any number of new properties or methods could be added to the WebPageItem class to extent the webpage related features??collected during the download process.,Figure 3 shows the asynchronous DownloadURL() method which is used to download a single webpage when provided a valid URL. ??It also writes any download errors to a log file (when provided). ??The URL's content can be downloaded as either binary or text data. ??This allows the user to download HTML as text or download other file types such as images in a binary format.,Portions of this method are??asynchronous and the method??itself is marked with the "," keyword in its signature. ??Using the C# HttpClient, a timeout is also set to ensure that when a download "hangs" for any reason, it will eventually be timed out using a duration specified by the user. ??The method also includes a true/false or boolean parameter named "binary". ??This parameter allows the URL to be downloaded ??asynchronously as either a byte array or as a string. ??The C# "," keyword allows us to utilize the HttpContent's ??asynchronous read / download methods ReadAsByteArrayAsync() and ReadAsStringAsync() to download multiple webpage's content simultaneously in the background. ??The DownloadURL() method could also be extended to make the download type determination dynamically based upon a URL's file extension instead of using the "binary" parameter. ??For instance, image file extensions could be downloaded as binary data while HTML file extensions were downloaded as text. Regardless of the download's format or success, the DownloadURL() method returns a WebPageItem which contains any??downloaded data, the server's response, any redirect URL which may have occurred, and any relevant error messages. ??This is a very important feature which allows each download to be tracked. ??When downloading large volumes of URL's in parallel, this becomes very important since the user will want detailed control over things such as how long to wait before timing out, the download format to use, and how many times to retry downloads which fail.,Since the DownloadURL() method executes asynchronously, we can now download multiple webpages at the same time. ??The Webpage Downloader accomplishes this task using two thread safe collections and two lists as shown in Figure 4.,The thread safe , "downloads" contains each successful webpage download as it occurs. ??Since the collection is thread safe, multiple worker threads can safely place downloaded WebPageItem content into the collection at the same time. ??This also allows the user to successfully create a parallel producer / consumer??style processing pipeline where downloaded webpages can immediately be further processed in a manner specified by the user. ??This is described in greater detail later on.,The??_RequeueCounts data structure keeps track of how many times a webpage download is attempted for a given URL. ??When the download attempts is less than the maximum attempts specified by the user, the URL is added back into the _ReTry list for another download attempt. ??Otherwise the URL is added to the log with a message that the download failed after exceeding the maximum attempts, along with the last error message received.,The??_DownloadTasks list is used within the??DownloadUrl,() method to act as a "concurrency" throttle. ??When the DownloadUrl,() method first executes,??one??DownloadUrl() task is created for each URL to be downloaded up to the maximum number of downloads specified by the user. ??Figure 5 shows the creation of these download tasks using the DownloadUrl() method.,Once the??_DownloadTasks list is full, a second while loop executes which "," any executing download task to complete. ??When the first task completes, it is removed from the??_DownloadTasks list and a new download is started. ??The completed download is also added to the "downloads" ,??for further downstream processing or possibly re-queued in the event of an error. ??This process is illustrated??in Figure 6.,In Figure 7, the WebpageDownloader created in Figure 1 is used to download a list of unique links. ??First the??DownloadUrl,() method is called to begin downloading webpages in parallel. ??According to Figure 1, the??WebpageDownloader will download up to 100 webpages at a time retrying each download up to 3 times. ??Downloads will also timeout after 60 seconds, and each download will be documented in the downloadLog.txt file.,In the Figure 7 example, as soon as the first download is completed, a second ,??loop immediately processes the downloaded content. ??Keep in mind that other URL's could also be in the process of downloading at the exact same time since the DownloadUrls() method is asynchronous and running in a separate thread. ??In this simple example, the second??,??loop saves each of the downloaded items to an output directory specified by the user.,The DownloadUrls() method recursively calls itself in the event that not all URLs were successfully downloaded and additional retries were specified by the user. ?? During processing any URLs placed into the??_ReTry collection are simply provided as input to another recursive call to the same DownloadUrl,() method as shown in Figure 8. ??In this manner, failed downloads will continue be re-attempted until they are either successfully downloaded or the maximum attempts are exceeded.,Have you ever wondered how a search engine might crawl the web looking for links which are connected to a single webpage? The following section demonstrates using the WebpageDownloader to start with a single webpage URL and crawl the??links within every connected webpage until the collection reaches a certain maximum size. ??The purpose of this demo application is merely to illustrate using the Webpage Downloader class for processing downloaded webpages. ??There are multiple improvements which could be made to this demo project for crawling links more efficiently. ??However, they would also add many more lines of code which may make understanding the illustration more complex.,In Figure 9 part of the CrawlLinks() method is shown. We begin by downloading a single starting URL. ??Next, a for each loop is used to process any downloaded WebPageItems which are provided back to the for each loop by the WebpageDownloader. ??For each of the downloaded WebPageItems we call the FindAllLinks() method to extract the URLs from any <a> link tags in the downloaded html. ??This method also filters out only common html file types as well for the demo.,However, ??the FindAllLinks() method is only called when the program has not yet exceeded the maximum number of downloads specified by the user. ??Each new link found is added to a list of new URLs to download. ??In the event that all provided URLs have been processed in the current run of the CrawlLinks() method, it next checks the ??"newLinks" list to determine if there are any additional links left to crawl. ??When "newLinks" are available and additional downloads are required, the CrawlLinks() method recursively starts over using the "newLinks" as the starting URLs and continues crawling. ??Once the method has collected enough link files, the for each loop is simply exited ignoring any remaining files.,Using my own website ,??as the starting URL, I performed 100 link crawl downloads using the Webpage Downloader. ??For example, Figure 10 below shows the??CrawlLinks() method being used in combination with a??WebpageDownloader to download 100 crawled links in parallel using 100 concurrent download workers for the WebpageDownloader.,I performed benchmarks to identify the first 100 links associated with ,??using 2, 50, and 100 concurrent download workers. ??The elapsed times are shown below:,Looking at the??Webpage Downloader's log in Figure 11 we can see that the LinkCrawler() located 39 new links to download from the first URL provided. ??Since the crawler had not yet located 100 links, it continued to download html and search for new links within the 39 new URL's. ??The crawler then??found an additional 2541, links contained in the html of the 39 links during its third recursive execution. ??However, only a fraction of those links were actually downloaded since we set a download threshold of 100 total webpages., , , , , , , , , , ...,It is clear that sizable performance gains can be achieved by using a parallel webpage download approach when mining data from the web. ??However, there is a diminishing return on adding additional concurrency workers to the download process. ??The optimal threshold is based upon numerous factors including both processing power and connection speed. ??I think it is also worth mentioning that in certain situations utilizing an anonymity network such as , [8] can be very beneficial to avoid facing download performance penalties or blocks which may be placed at the ip level by certain webpage providers. ??While moving from 2 concurrent download workers reduced execution times from 44 down to 16 seconds, increasing concurrent workers from 50 to 100 workers actually increased our total download time to 17 seconds. ??Carefully choosing the appropriate parameters can dramatically impact execution times when dealing with larger volumes of data.
When we try to build classification models from training data, the proportion of target classes do impact the accuracy levels of predictions. This is an experiment to measure the level of impact of these proportions., , , 
"Asking questions is easy. It?€?s so easy that, as askers, we often don?€?t think about the quality of our questions. Poorly framed questions waste everyone?€?s time?€?yours included?€?because they require the answerer to make assumptions. When it comes to asking analysts to explore a problem you?€?re trying to solve, better questions will drive better analysis and, ultimately, more actionable answers.,Here?€?s an example:,?€?How many people converted from paid ad campaigns in the last week??€?, ,???€?6,000.?€?,
,???€?Oh, that?€?s interesting. I was expecting more. Can you please break it down by channel??€?,This kind of question is expensive. Not because it?€?s hard to answer, but because it limits your analyst?€?s ability provide real value.,To ask better questions, start by identifying your goal:,Let?€?s assume the marketer?€?s goal is to spend advertising dollars most effectively. If the marketer had asked a narrow question about conversion rates, the analyst might only research that aspect of the campaigns.,But because the analyst knows that the actual goal is to optimize ad spend, she might proactively explore churn rates from customers acquired through various paid channels. By adding lifetime value to her analysis, the analyst might discover that ads with great conversion rates actually turn out to be less valuable in the long-run. Together, the marketer and analyst could use this information to optimize the campaigns.,Even when asking a well-motivated question, it?€?s important to provide context. Without it, analysts may take the wrong approach or make bad assumptions that lead to incorrect results. Moreover, context helps analysts do what they do best?€?frame questions and look at them from many angles.,For example, the marketer above could be asking about advertising spend for lots of reasons. Explaining why she?€?s asking the question can help the analyst focus her analysis and reach a faster, better answer:,???€?Last week we decided to push our Twitter and Facebook advertising in a new direction. We added 10 new image/copy combinations and increased our spend by 50%. I?€?m curious how the new stuff is performing relative to the ads we?€?ve been running for awhile. Also, which image/copy combos are leading to the most conversions??€?, ,???€?What?€?s the primary message of the new ads? Other than purchasing, what sort of behavior are you hoping to change??€?,Open-ended questions like these have two benefits. First, context helps analysts understand how to handle the details of your question. If someone arrives through two different marketing channels, should they be counted once or twice? Your result could be dramatically different depending on the method you choose, but analysts often won?€?t know the right method without context.,Second, analysts answer a lot of questions?€?and about many aspects of your business. With your context, she?€?s better able to connect your question to others that she?€?s been asked before.,The analysis should get you asking questions. Don?€?t be afraid to ask lots of follow-ups.,Because the analyst understood your goal, she will have been empowered to explore your initial question from a number of different angles. She?€?ll probably have answers at the ready or be able to determine which follow-ups will lead to the most valuable discoveries.,Even if this approach feels laborious at first, it pays dividends in the long run. If you form a strong partnership with your analytics team, together you can begin to connect dots across business lines in ways that uncover unexpected value.,This post originally appeared on the ,."
In this article, we discuss various strategies used to generate exponential traffic growth, while preserving traffic quality, and user loyalty. Our growth hacking engine is a combination of,The results are best illustrated in the graph below representing ,, one of our profiles, and the largest data science profile on Twitter, as well as ,.??,Our DSC network has more than 50,000 live articles at any time, and growing by more than 2,000 new articles per year. Our intern Livan analyzed our Google Analytics statistics, and found more than 2,000 articles each with more than 150 page views - and some with more than 100,000 page views. As we have a Twitter account with 60,000 followers (growing by 5,000 new followers per??month at the current growth rate), and a LinkedIn group with 160,000 members (growing by 6,000 new members per month), we asked ourselves the following question:,The answer, from our first tests, is an immediate 10% traffic boost. We could tweet 100 articles per day from that same list, not just 25. We could tweet from multiple accounts, not just ,, and we could also post on LinkedIn or Google+. With Hootsuite, this process can be fully automated. What would be the impact? Of course there is an optimum: too much tweeting will create dilution. But given the large number of new followers each day, and the fact that the top 2,000 articles could be replaced by entirely new articles after one year (because we produce new articles every day, and we are in the process of automating some postings, such as new books or new salary surveys), is it a clear indicator that 25 tweets a day is well below the optimum. And indeed, we have 50,000+ live articles, so we could tap in the whole list, not just the top 2,000.,Optimizing this tweeting process is discussed later in this article. Note that the way tweets work, it is OK if a user sees a same tweet 2 or 3 times over a one-year time period, as long as on average, he sees many tweets from us only one or two times. And given the fact that tweets are short-lived, even with 100 tweets per day (out of a list of 2,000 tweets updated monthly), randomly selected (according to some selection mechanism slightly favoring, new, or very old, or popular, time-insensitive tweets), we should be fine, if we proceed carefully, incrementally, with constant adaptation to new web traffic conditions whenever they occur.,The idea that very old, time-insensitive articles with few (say 150) page views are worth tweeting again today, is because our traffic grew up by 500 percent over the last several years, thanks to the techniques described here. So old articles were not seen by most of our new visitors. This concept is best explained in our article about the ,, discussing traffic decay and how to increase the lifetime and yield of old blog posts. For instance, by having top articles listed in a footer in each new article, ??as we have at the bottom of this very article - a footer that can be updated at once across thousands of articles, when needed, using an shtml include or iframe to load the adaptive footer stored in one web location: more on this soon. ????,The process consists of five steps:,The score can be used to slightly favor (over-tweet) articles that are more recent, or popular. But it is random enough that any article has some chance to eventually be tweeted one day. The score reflects the fact that not all articles are created equal.,The final implementation will consist of a fully automated machine-to-machine communication service (between Google Analytics, Hootsuite, and Twitter), powered by robust black-box analytics, automated machine learning (hash tag creation, detection of time-sensitive articles)??and automated, adaptive statistical scoring.??,The number of tweets?? can be slightly adjusted each day (increased, decreased, or change in scoring parameters)??as a response to performance. Performance is measured in terms of daily clicks arising from this activity (the stats are readily available from Hootsuite analytics), and the resulting average session duration for traffic coming from Twitter (available from Google Analytics).,This algorithm is used to score articles based on page views (denoted as P), creation date (denoted as T for time), and a random number denoted as R (uniform deviate on [0, 1]). Note that older articles tend to have more page views, so P and T are not independent. The score S is computed as follows:,S??= (b + R) * P^a / (T-Offset)^c,The parameter a, b, c are chosen so that the top 25 articles selected each day (for tweeting) have, on average, a median P (historical page views count)??about twice as high as the median P computed across all 2,000 articles. This way, we slightly favor popular articles, but not too much. Details are in the spreadsheet described below. Offset is chosen so that T = Offset, for our oldest article. You must use the median for P, not the average, because it has a ,. Note that page view decay occurs, especially for not popular article, though ,, in our case.,You can download our??,??with 2,000 articles, featuring the following fields, for each article:,The parameters a, b, c are in cells J2, J3, and K2 respectively. A low value for J3 will produce more random scores. Cross correlations are displayed in cells L1:O4, and the median score for top 25 articles, and for all 2,000 articles, are displayed in cells M8 and M7 respectively.,Note that the cross-correlations are not very useful: even when correlation(P, S) is as low as 0.04, the median P for ??the top 25 articles (those with highest S) is twice as much as the overall median score S computed on all articles. This is because traditional correlation is a poor indicator in this context, sensitive to the numerous outliers in the P numbers, caused by the fact that P has a Zipf rather than Gaussian distribution.,You can also download a??,??(for members only) that contains the full text (not just the title), for each article. It is used for clustering articles (see section 3).,Our intern Livan wrote some Python code??to process??Google Analytics reports, and??scrape DSC articles to extract relevant fields (creation date, channel, and title). ,??(rename this text file with a .py extesion after downloading).,We can make this system more powerful by,This section quickly describes the other fundamental component required to make our system (described in section 1) work. It is the creation and growth of at least one massive Twitter account, with highly relevant, high value followers, and use of automated tweeting systems. There is a feedback loop in the sense that having a lot of valuable content to tweet, helps generate large volume of good traffic to your website, and helps boost your Twitter growth, which in turn further fuels the traffic growth for your website.,Here, a significant part of our growth (150 new Twitter followers per day) is generated via Twitter advertising: we spend a little more on Twitter than on Google AdWords. With Twitter, it is possible to target US-based profiles (and their followers) that are similar to pre-selected profiles, and you can upload a list of pre-selected profiles when starting your advertising campaigns. Our list has hundreds if not thousands of pre-selected data science profiles. Such lists are easy to find, and regularly published on various websites. But ours also includes top profiles - indeed the very largest, most relevant ones - that are missing in the traditional published lists, as well as people who re-tweet or like our tweets.,The growth and volume of our two main Twitter profiles, ,??and ,, is displayed in the figure below. It is a few months old: now our number of followers have more than doubled, and we are well above ,??in terms of number of followers.????,The strategy described in section 1 delivers more than 1,000 extra clicks per day to our network, at the current low levels (25 tweets per day).,We also use LinkedIn and Google AdWords, but for a different goal: generating new members, US-based in the case of AdWords. But we have encountered a number of issues with AdWords (low conversion), thus we have reduced our budget, ,??(adding negative keywords and conversion tracking, more on this coming soon), and shifted money to Twitter and to acquire high quality content. Read our article on ,??to understand how we blend domain expertise, business hacks, machine learning, engineering, and modern statistical science, to efficiently solve business problems in general. And in particular, to discover how we optimize our bidding strategies for Google keywords (how much to pay for a keyword).,One of the challenges is to populate these channels with new content. While we use syndicated feeds for this purpose, we also want to add our own content. One way to do so is to perform a clustering of all our articles, and assign them a category: visualization, data plumbing, big data, Hadoop and so on. Once the articles are categorized, we can publish (re-post) some popular articles from DSC on the appropriate sub-channels. Our intern Livan is actually working on this, adding a category field to the list of 2,000 top DSC articles.,Here we describe a very simple and highly scalable NLP (natural language processing) technique, called indexation, to perform this clustering task. It works as follows.,I call this technique , because it is very similar to the creation of a search engine; another word that could be used is ,. We also have used and described this technique in the context of ,??(source code provided).,Instead of using this algorithm, you can just use ,??for your website, and once installed, search for , to find articles in your website, that are a good fit for the data plumbing category or channel. ,.,Also add 3-token keywords in your dictionary. For 3 tokens keywords, you have 3! (factorial 3) = 6 n-grams. Usually, only one or two of these 6 n-grams will show up in the articles, for any keyword (, will show up, but , won't).??,This DSC growth engine illustrates that data science is not just about programming. Indeed, here, programming is a small part of the project, compared with designing algorithms that efficiently make API's communicate with each others, based on data automatically gathered, with insights automatically extracted, and automatically leveraged. It also shows the limitation of traditional statistical science, with correlations (see the sub-section about the scoring engine) that are useless, and replaced by something else.,It certainly shows that there are ,, and that indeed,??,.??It also shows how business and domain expertise are critical. For instance, if you don't know about the Twitter advertising capabilities, nor the Hootsuite product, you will never even think of doing this kind of stuff, no matter how much you know about coding and algorithms, thus missing on a big opportunity. If you work in a bigger organisation, of course finding and convincing the right person to start a project like this one, is a challenge, no matter how much business savvy you are. But my experience is that big organisations tend to hire specialists rather than people like me.,Finally, we invite you to test our list of 2,000 articles, and see which tweets (that is, which articles) resonate best with your followers. It would be interesting to see if articles with high page view counts perform better on your Twitter account (just like they do on ours). And it might be a way for you to further attract followers, by posting stuff that they and many people like to read.
Before jumping on the Big Data bandwagon, I think it is important to ask ??the question of whether the problem you have requires much data. ??That is, I think its important to determine when Big Data is relevant to the problem at hand.,The question of relevancy is important, for two reasons: (i) if the data are irrelevant, you can't draw appropriate conclusions (collecting more of the wrong data leads absolutely nowhere), (ii) the mismatch between the problem statement, the underlying process of interest, and the data in question is critical to understand if you are going to distill any great truths from your data.,Big Data is relevant when you see some evidence of a non-linear or non-stationary generative process that varies with time (or at least, sampling time), on the spectrum of random drift to full blown chaotic behavior. ??Non-stationary behaviors can arise from complex (often 'hidden') interactions within the underlying process generating your observable data. ??If you observe non-linear relationships, with underlying stationarity, it reduces to a sampling issue. ??Big Data implicitly becomes relevant when we are dealing with processes embedded in a high dimensional context (i.e., what's left after dimension reduction). ??With higher embedding dimensions, we need more and more well distributed samples to understand the underlying process. ??For problems where the underlying process is both linear and stationary, we don't necessarily need much data at all.,The wrench here is in knowing when you are dealing with a non-linear or non-stationary process. ??So, a little thoughtfulness and discovery work can tell you whether you have a Big Data problem, and then you can go about finding ways of actually collecting the required Big Data. ??Knowing whether you have a Big Data problem (or not) informs your approach for actually learning from the data.,This is an especially important question for Data Architects/Strategists to think about when building their roadmaps against the kinds of challenges they hope to tackle. ??Not all paths to good Data Strategy lead to Hadoop. ??I think a good guiding principle of design is to "design with the end user in mind." ??In this context, the end "user" is actually the algorithm learning from the data. ??In principle it is empowering to believe that one can learn anything from an infinite box of all kinds of data (let's call it a 'universe box'), in practice, you will want to reduce the data to the essential subset that lets you do something meaningful with it. ??Just because an algorithm , learn anything, ,.,There's a really intuitive paper on Generative vs. Discriminative classifiers by Ng and Jordan that has stayed with me since grad school.,I like this paper for the intuition it provides.,Aside from their performance characteristics, a preference for discriminative vs. generative models to some extent reveals one's beliefs, and must inform one's design choices.??,Models can reveal something about ourselves, i.e., our sense of existentialism. ??I would think an engineer is more apt to believe that there is a truth, that it has a mechanistic basis, and that we can , the model (see ,). ??Key word: ,. ??Statistician's ,. ??Key word: ,. ??This almost suggests (at least in my mind) that somewhere in their heart of hearts, they don't believe that an exact (perhaps, noisy) mechanism actually exists. That humans just like to see mechanistic explanations where there are none to be found. ??I think they are more comfortable with randomness, and even embrace it. ??Would an engineer be comfortable saying, for instance, that "all models are wrong?" I think they have to believe that , are actually right.,Some tangents to think about.
In a previous article I argued that in order to ensure the?? highest performance, decision makers need to balance the data driven actionable insights with intuition driven insights. In this article a new insight is added to the decision tool box. This insight is represented by an acausal rare, coincidental event that can have a huge impact on the decision maker's enterprise, although absolutely free. Such an event is called a synchronicity, first introduced, defined and modeled by Swiss psychologist Carl Jung. Most of the ideas presented in this article are summaries and adaptation of excerpts from ,Synchronicity", a book by Jessica Satori , and " Synchronicity: Nature and Psyche in an interconnected universe " by David H. Rosen. Besides integrating the concepts presented?? in these books into a decision making aid, the additional contribution here is the derivation of metrics to measure and evaluate whether an event is a synchronicity. Such an evaluation boosts the awareness of these life and business changing events thereby potentially increasing the frequency of their use in decision making and resulting beneficial impact,??,An important characteristics of an event is the presence or absence of a cause. An event is the result of either a causal or acausal relationship. Causal relationships are based on action and reaction. The event is acausal (without a cause) when it is surprisingly unexpected, and almost surreal that it cannot be the result of any imaginable?? cause. Such an event is called a synchronicity . Swiss psychologist Carl Jung defined synchronicity as?? a purely acausal, rare and irreproducible event that's meaningful (to the participant) , and seemingly numinous (supernatural origin), where the inner state of the observer and the external event play a role such that nature and psyche are together connecting in a meaningful event,While intuition?? and parapsychological events preserve a causal explanation based on unknown forces or paranormal causality,?? synchronicity?? remains strictly acausal.,The semantic relatives in the family of rare events are: Synchronicity, Chance, Coincidence,?? Serendipity, and Grace. These rare events are defined below and compared to synchronicity.,Every businessman and entrepreneur believes that "timing is everything" and that time and place are crucial ingredients in the success of a business endeavor. How can we then explain that synchronicity as is beneficial for business, and is there a way we can make it occur more often??? To answer these questions, a look at the concept of time is needed and helpful given the role time and space play in synchronicity.,Two types of time concepts are at play: Chronos, the linear time we measure and that define the chronology of events, and Kairos, the time we sense when we are not driven by a clock that is we are not measuring it. The latter is an instance of "flow" when time seems to go incredibly quickly . While Chronos is measurable and denotes chronology (sequence, age), Kairos is not measurable,?? but ontological that is dealing with the nature of being. It's a feeling of letting?? go of the control of Chronos, of what has to happen in a certain length of time. This is the kind of time that refills and inspires. In a world where the most coveted value is in the effective use of time, most people leave on Chronos alone. However, both Chronos and Kairos?? are important for balance in daily life.,Synchronicity can occur at the intersections of Kairos and Chronos time. People are in their busy Chronos lives, going about their day, and suddenly something "hits " them.?? It may be after a comment, an article, a letter, or an unexpected connection that suddenly they can find themselves transformed by an outwordly event that "beams us up" into the uncontrolled Kairos time.?? With synchronicity, they find themselves in the right place at the right time to create life changing personal growth or business opportunities.,As some entrepreneurs put it, "several events ??seemingly totally unrelated, and yet when they happen together, conspire to bring us the result we are looking for. Thus a whole series of things seem to coalesce into a happy coincidence, to bring what we are looking for" ,.,As per psychologist Jung's definition of synchronicity as "an acausal, numinous (supernatural) and "meaningful coincidence", it is conditioned by in the inner state of the participant. The series of events preceding the coincidence may seem normal and unrelated. Yet the outcome may seem surreal, which makes synchronicity hard to believe in.?? Synchronicity is elusive and cannot be controlled as we want to. People involved in synchronicity must have the inner psychic states enabling it to happen.,Thus Jung views synchronicity as the meaningful (to the person who is concerned by the event) coincidence between an inner psyche sate and an outer physical manifestation or event that's acausal and numinous (magical),Something meaningful is personal in nature, synchronicity denotes that something happening outside resonates with something already inside, that seems beyond or in defiance of probabilities, that's amazing and makes our heads spin?? in wonder. Time and space are fundamental to our reaching and understanding of the synchronistic phenomenon. You arrive at a moment in time when an entrepreneur, a technology and the needs of society coincide. Few businessmen attest to the great value of synchronicity where success is not defined by the equation:,There is meshing of the inner psychological state (feelings) of the participant and outer states (external happenings. The two work in a meaningful concordance?? of sorts , the conjunction and coalescence of which?? cannot be explained by anything then acausal synchronicity.,The relationship and interplay between internal thoughts and feelings with outer events is important. The inner thought is meaningful to the outer event and vice versa. The "meaningful coincidence"?? can be seen in the inner state of the participant and the impact of the event.,Being aware of the onset of a synchronicity will offer the biggest and absolutely ??free insight ever, where the personal as well as corporate benefits can be of surreal magnitude. If an data driven insight or an tuition driven insight are a blessing in disguise, a synchronicity is a huge blessing in plain clothes.,??,The multi-billion dollar questions are then how can we detect the onset of a synchronicity and how to know when an event or a coincidental event represent a synchronicity.,To decipher whether an coincidental event is a synchronicity situation, and therefore take advantage of occurring synchronicities around us as if to making them 'happen' , that is use them more often , ??there must be a willingness and desire of the participant to accept it and act on it. As Jessica Satori , put it: ",??,Moreover, as synchronicity can occur at the intersections of Kairos and Chronos time, decision makers, entrepreneurs as well as ordinary mortals must find the right but important balance of Chronos and Kairos in their daily lives. Kairos moments are therefore key to allowing psyche to connect in a meaningful synchronicity. Contrary to data driven insights which are derived from?? the past and projected in the future, living synchronistic insights are derived from living in the present.,??,The second important question concerns the "detection" or recognition of the onsite of a synchronicity.?? This can be done by analogy to a consumer's market (shopping) basket and the items it includes. Here synchronicity is the analog of the market basket, while the attributes and the events preceding it are the analog of the items in the basket. Let's list the attributes of the synchronicity before defining a metric indicator of the onset of a synchronicity.,The following elements (attributes and conditions) are what make synchronicity different than a routine events and coincidences .,The metric to measure the level or intensity of the synchronicity character of an event are defined here by the analogy to the market basket where the notion of support and confidence are used to measure the quality of an association rule such as: If the set Y of items are in the basket, then the set of items X is also in the basket. The most popular metrics are support , confidence and conviction.,??,The proposed?? metrics for the synchronicity character of an event are as defined below:,Letting X be the number of elements present in the event under consideration and Y those that are not. Note that X + Y = 10,??, is the ration of the number of contributors to the number of non-contributors to the event being a synchronicity. If support is low, the likelihood?? that the event is a synchronicity is low. The higher the support is, the higher the odds are for the event being a synchronicity,?????????????????????? Support = X/Y = X/(10 - X), is an estimate of the likelihood that the event is a synchronicity.,?????????????????????? Confidence = X/Y = X/10,Conviction is the product of support by confidence thus providing a measure that takes into account the impact of support and confidence thus reinforcing the conclusion as to the nature of the coincidental event under consideration.,In this article, an addition to the decision maker's toolbox is explored. In addition to data driven and intuition driven insight tools, synchronicity provides a powerful for highly profitable decision insight that's available absolutely free of charge. All that's required from the decision makers is to be alert and open to receiving and following up on the synchronistic events that present themselves as rare and acausal situations or seemingly surreal opportunities for unfathomable success in personal or corporate affairs. Living in the present and making a balance of Chronos time and Kairos times in daily life will help increase or detect more of these situations. The benefits can be huge and the ROI is infinite as these insights are a free gift from nature to those who can spot them and act on them. Three simple metrics that can boost awareness of these synchronicities are defined based on the characteristics of such situations.
Throw them all out I say. ??Big Data is really just defined by , letter, ,To use 3 (V)s, when ,??(D) is sufficient, is , unforgivable.,In the world of Predictive Modeling and Big Data, there are two curses that really stand out.,1)??,Coined by that original professor of the Dark Arts, ,, in his work on dynamic optimization. It refers to the fact that as dimensionality increases we see a problem of data sparsity. ??I frequently run into this problem when trying to build models using traditional data sources. ??The challenge there is that the majority of business processes/practices involve human discretion and judgment against a limited set of actions, leading to decision makers repeatedly "doing what they've always done." ??In an experiment, you would try to cast a wider net to determine how things behave under different circumstances. ??Due to the homogeneity intrinsic to traditional conservative decision-making approaches, large parts of these problem domains remain not well understood (under-sampled). This can derail efforts to develop robust statistical models.,2) ??,As ,, leads to spurious correlations. ??In other words, the problem of seeing things that are not real, and missing things that are real. Ghosts. Hallucinations. Other magical things.
According to MarketsandMarkets the global Big Data market will reach $46.34 billion by 2018. I wanted to check the local DC Metropolitan job market to verify the hype related to the shortage of professionals with Big Data skills. I went on a job board and searched for the number of job postings that listed Big Data tools as part of the requirements.?? I compared the numbers with the numbers that I had recorded on September of 2012.,The?? broader IT market didn't change much in the past year, but there is significant growth in the Big Data related tools (Hadoop, R, MongoDB). I didn't record the number of postings?? for Data Scientists last year, but the number stands at 29 this year-- some are posted under Data Analyst because Big Data is new and some folks don't know the difference between the two; the only way we would know is to read the job description. One interesting fact pertaining to the geographic spread of the Big Data jobs; the jobs are concentrated in few areas. Below is the ranking according to Dice. The sorting was based on Per Capita Ratio.,Big Data will create jobs in three areas: Data Infrastructure, Data Management, and Data Presentation (visualization). Large data sets need storage, Database software to store the data, and Analytics tools to turn data into meaningful information for businesses. Additionally, there are other parts of IT that will be impacted by the surge of Big Data adoption such as Network, Security, and the Cloud.,I divide the required skill sets into two categories: Immediate opportunities and long term opportunities. If you are interested to build your??, for today's market, you might want to take into consideration this list: Java, Hadoop, Linux, C#, R, Python, Php, Hbase, Cassandra, MongoDB, Amazon ERM, and Mapreduce.,Planning your career for long term engagement will involve several other tools that might be very much in demand in a couple of years. Windows Azure, IBM PureData, Oracle Big Data Appliance, HPCC, Cloud Foundry, Pivotal, Sap HANA, Google BigQuery, OpenStack, Mahout, SAS, Excel 2013, Tableau, Ruby, postgreSQL, MariaDB, Pentaho, MySql, Jaspersoft, Talend, and LAMP.,If you are the leading-edge type and you want to know , with startup companies, I recommend following the developments on these startups. SiSense, Parstream, Skytree, Platfora, VOLTDB, nuodb, Hortonworks, Cloudera, MapR, Splunk, Mu Sigma, Opera Solutions, 1010data, Alteryx, Datameer, and fusion-io,Software Defined Networking, Software Defined Storage, and Cloud Security are among the promising fields in support of Big Data in the coming years.,See 
By now you already know how important Big data processing and analytic is in running a lot of industries in the world. It is a multi-sector industry that helps manage big data, monitor trends, improve business operations, and even fight crimes. Big data is used in so many industries that it is considered as one of the ,.,But just like anything else in modern technology, Big data trends are ever changing. It is right to keep on learning the trends and process on how you can adapt to the changes in this industry. Thankfully, there are online courses that can help you.,The courses range from basic big data courses, to advanced classes that will talk about the application and trends.,Here are some of the best big data classes offered online:,The course is perfect for people who wants to begin a career in Big data technologies. Udemy, a one of the leading virtual schools, brushes the fundamentals to understand the cryptic Big data problems and how Hadoop can solve them. The best thing about this class? It is free!,Coursera's take on Big data course focuses on web-intelligence applications essential in social media, mobile devices and sensors. This is based on the map-reduce parallel programming pattern. It is also anchored on distributed file systems, no-SQL database and stream computing engines. This 9-week long class is offered as a self learning course, which does not require superior essay and exam skills.,Duke university in partnership with Coursera lets you delve more into the wonders of Big data technology. In this 10-week course, you'll have a deeper understanding of data collection, and data limitation methods and it affects range of interference. You will also have hands-on training on estimation and testing methods to test single variables that helps you make data-based decisions. This course is for free.,Big data can quickly crush centralized computing approaches. That is why learning about massively??,??is a very helpful big data skill. This hands-on course under the famous name of Harvard School of Engineering and Applied Sciences, covers parallel programming models, multi-thread programming, cluster and cloud computing, and MapReduce through Hadoop.,The class is divided into eight levels to better understand R programming, for data analysis and visualization, great from statistical computing and images. Every class gives you a deep and practical knowledge on the use of R language. Though the course is free, and focuses on fundamentals rather than advance lessons, its interactive format makes the lessons worthwhile. Plus, you don't have to worry about writing ,Massachusetts Institute of Technology offers an advance lesson on data structures and its important application on Algorithms, like that of Google. The study also talks about current directions of research and findings in data structure. The course, however requires you to pass an undergraduate algorithm class, or the open Courseware's Design and Analysis of Algorithms class.,There are so many lessons you can learn from getting back to class again. Sure, your profession gives you a personal insight on the latest trends of Big data process but an interactive, and informative class discussion that these virtual schools offer, helps improve your perspective of the industry you belong to.
I heard an accountant once say that people in his profession are generally bad investors. I am uncertain if this is true. I never really bothered to confirm his assertion. He said that his reasons for believing so relate to the nature in which accountants interpret data, which he implied was rather literal. I personally almost always ignore "book value" - that is to say, the cost of acquisition. For me, the book value is similar to a figure of speech: the investment value never has to be aligned with the book value. Some investors might regard a stock trading at half its book value as a bargain rather than sign of potential asset impairment. Conversely, a stock trading at double its book value - like a successful NHL or NFL franchise might be appraised at many times book - might be regarded as expensive. Yet it seems to me, an accountant should be well aware of the distinction between book and market; it is possible I was simply speaking with a bad investor who preferred to blame his profession rather than himself for bad decisions. I have never personally known a statistician heavily into the stock market; yet I am certain that some have been hired specifically to do market research. It would be interesting to study the investment skills between individuals from different disciplines. It is important to note that in relation to stock prices, the prices have little to do with the underlying stocks. A company that has a high stock price is not necessarily managed any better or is even more profitable than others trading lower. The price measures how people feel about the company. It is a socially constructed index just like volume. In order to explain this perplexing sentence, consider the interesting illustration below.,I will explain how I came up with the illustration a bit later in the blog. But for the moment, I ask readers to go along with me in terms of the algorithmic operations: on the x-axis, there is an algorithmic gradient of daily trading price; on the y-axis, there is the same but using volume. Essentially, this chart is designed to indicate which stocks sort of "dance the same way" in relation to price and volume. (The prices and volumes are not necessarily similar in absolute terms.) I circled and coloured in red what we in Canada refer to as the "Big 5" major chartered banks: Royal Bank of Canada; TD Bank; CIBC; Bank of Montreal; and Scotiabank. Also for comparison, I coloured two smaller banks in pink: Laurentian Bank and the National Bank. I colored one insurance company in white: Manulife Financial. The illustration indicates that the Big 5 banks "fly together" while the other financials are sort of doing their own thing. Again, the technical trading data tells us nothing about the companies; but I suggest that we can reasonably infer how the market lumps the Big 5.,In showing where these companies appear on the illustration, I am not actually saying anything about the companies themselves. I have back-tested performance in relation to different positions on the chart. I find that the positions are not particularly relevant to performance. So please do not use the chart to "externally define" the meaning of the positions in relation to investment. The algorithms have no information about the companies. It is impossible for trading algorithms to tell us about the companies. Consequently, the illustration derived from algorithms tells us nothing about the companies. As I pointed out earlier, I merely offer a type of insight on how people might feel or perceive players in the market. People interpret the relationship between stocks based on their personal situations - experiences, education, income, and risk tolerance. I personally always read financial statements and bulletins especially for what I call "cover-my-butt disclosures." An executive might avoid jail time by saying that he or she provided full and timely disclosure, buried though it may have been in hundreds of pages of fluff. So my interpretation of statements affects how I feel about stocks.,I claim that prices and volumes are products of social construction. Yet I do not dispute how the same data can be handled in a manner quite insulated from society. If the trading data were purely reified, one might ignore for instance how a company is losing customers. Well, the financial statements tell us when revenues are declining. I mean loss of customers in a bigger sense that provides us with reasons. It is a complicated argument to make - for example that educational levels and sense of social membership can affect how people feel about products and therefore the companies that make them. For example, consumer sentiment surveys were once used as a bellweather for the markets: this type of reading could theoretically predict the demand for durables and new home construction. However, for the remainder of the blog, I will be discussing in greater detail how I reached the illustration.,I got to the illustration using a complicated route; perhaps it is not the most efficient approach. Let's consider the idea of "trading volume," which perhaps makes no sense at all for a great many investors. Does high volume mean that people like the stock? As far as I can tell, volume doesn't mean anything about liking or disliking stocks. Sorry. I know what it should mean or what I would like it to mean: as prices go up, demand should decline. However, I simply couldn't find anything during back-testing. So never mind about making money. Volume is actually an interesting measurement from the standpoint of determining common dance. I used a running total of the volume. Just to demonstrate how this works, I generated a thousand random numbers from 0 to 10. Below on the very top chart, I distributed the random numbers from 0 to 10. The equation "f(x) = m x + b" for the trend line reasonably indicates "m = 0.0004" and "b = 4.9967." If on the other hand the points are added as a running total, in the middle chart we have "m = 5.1989" while "b = -37.142." The value for "b" is actually irrelevant since, for any given line segment, this amount whatever it might be gets cancelled out. Consequently, "b" is near 5 for an ordinary longitudinal distribution; "m" is near 5 for the running total.,I added a third image above, the running total volume for the S&P 500 since 1950; this is mostly to show that the running total in real-life doesn't necessarily result in a straight line. So in a manner of thinking, the effect of volume is probably similar whether the individual points are used or the running total. The charts merely have to be interpreted a bit differently: e.g. a flat-line on a running total price chart means death for a stock - the shares being literally worthless. For me this is important, when the running total is used, distortions caused by timing become less relevant. For instance, if an institutional trader wants to buy 1 million shares of a stock, doing so all on single trading day might be possible although probably not feasible. There might not be enough people interested in selling on that day; this could force the trader to offer higher prices thereby driving the price up. If the trader spreads the purchase over maybe 6 months, the likelihood of doing so at a reasonable price is probably better. For other investors studying the individual elements of volume, that 6-month acquisition might be barely distinguishable from background noise. But if a running total is used, the cumulative impact noticeable. Below I show the running total volumes for stocks in a number of different industries. Imagine what may have led to the bumps.,Of course, "plumes" and "algorithms" are complicated right at the outset. For the types that I use, the math isn't complicated; but it is difficult to describe or explain. At this point I can write the code with my hands tied behind my back. (I can program with my eyebrows.) This methodology is a variation of the approach I described for the Storm family of algorithms, which I will attempt to summarize here. Imagine entering the data points from the running total into slots. I used 50 slots. Slot 0 is the most recent (the lightest). Slot 49 is the least recent (the heaviest). As one goes down the data stream, the numbers in the slots shift. I hope I'm not already losing people. The "Paranoid" sequence shows the following slot combination: 0/1, 0/2, 0/3, 0/4, 0/5, 0/6, and so forth. The "Reactionary" sequence is as follows: 0/1, 1/2, 1/3, 2/4, 2/5, 3/6, 3/7, 4/8, 4/9, and so forth. The "Reluctant" sequence is 0/1, 1/2, 2/3, 3/4, 4/5, 5/6, 6/7, 7/8, 8/9, and so forth. A plume is created using an algorithmic lattice - as I said from 0 to 49 - a new row being created for each trading day. Thus, over the many trading days, a 3-dimensional plume emerges. I create a price plume in exactly the same way.,Ignoring the fact that the running total can significantly reduce temporal distortion, I think it would be much easier to use correlation to evaluate similarity. However, correlation doesn't help to indicate placement on the chart. Or at least it doesn't help on my chart. To determine similarity, the plume differentials can be extracted (by subtracting one plume from another) line by line for each trading day. The extent to which two stocks are similar is indicated by how often the differences fall within a specified range. In order to demonstrate the effectiveness of this approach, I tried to get the stocks on my database that "dance" most and least like Microsoft. I found the pricing most like the National Bank (correlation = 0.906) and least like Cameco (correlation = -0.275) during the comparison period (2010-01-01 to 2015-01-01 using pricing adjusted for dividends and splits). I present the individual price amounts rather than running total (although the running total was used for pattern recognition.) By the way, National Bank as I mentioned earlier is one of Canada's smaller but prominent banks. Cameco is one of the world's leading suppliers of uranium for nuclear reactors. Currency differences were ignored, this being just a pattern comparison.,Positioning or ranking is determined by how often other stocks fall above or below the individual slot values as one goes across the sequenced array. Consequently, this is not merely a comparison of differences but also direction. However, the position falls within the boundaries 0 to 1 for both price and volume: 0 might be regarded as last place while 1 is first - although as I mentioned earlier placement doesn't actually seem to relate to performance.,While it is not necessarily always the case, sometimes things that fly together are in fact birds of the same feather. I believe there are many ways to apply this general principle. While birds probably fly and stay together by choice, the factors contributing to those choices could be environmental. In this blog, it would understandable to suggest that banks fly together because they share some similarities. But actually, the technical data contains no information about the banks; there is nothing to suggest that they are at all similar. Instead, the similarity is drawn by investors. There is a social environment - an elaborate ecology - influencing placement. Our perceptions bring forth the existence of connections and similarities. We therefore provide placement not for the banks but our perceptions. Through technicals, we measure not the stocks but ourselves. We invest in our dreams and flee from our nightmares. We also invest for financial security and possibly for some even happiness - whatever any of this means. Given our expression of things inside influencing or decisions outside, it may very well that stock market fluctuations extend from phenomenology. But of course, those things occupying the deeps of an individual might be the product of an externality perhaps both pervasive and ubiquitous. It's such an interesting thing to pay analysts a great deal essentially to decide whether to buy, sell, or hold. It is so important to be sensitive to the world and ourselves. The algorithms have an important role to play. But I think it can be short-sighted to reduce investment decisions to simple mathematical problems.
View video:??,??(by??Amr Awadallah, Strata + Hadoop 2015)
Most statisticians are great professionals, working on various data-intensive projects, and they don't care about their job title. You can say the same about data scientists, and me in particular. However, there is a small cluster of statisticians - , seems to be their leader and their only influencer - who have been challenging us, even publicly insulting us recently.,So my criticism here applies only to this small clique of practitioners, most of them being Ph,D. statisticians who have worked for the same non-profit organizations for a long long time, and with limited experience and exposure to the real world. Some great statisticians such as Diego Kuonen, while strongly and justly defending statistical science, don't belong to that clique.,This incident started a few months ago, when these die-hard statisticians claimed that we, data scientists, know nothing about statistics, and that they know everything. Their science is an arcane mix of thousands of non-unified techniques that are kept almost secret, so as they can continue doing their costly man-made analyses with no regard to ROI. Many consider applied statistics as a plague.??,They even claimed that ,. That was the starting point. I acknowledged that indeed, I did not know anything, because ,??is totally different from theirs: it's all about automation, model-free predictions, and big data applications, safe to use by the non-initiated..,Now they've changed their mind, and they claim that actually, what I do is statistical science. Yet their old statistics is a small portion of data science: business hacking, domain expertise, machine learning, data engineering, new statistics and core data science being the main components of what I do. But they even went as far as to say that my model-free confidence intervals were wrong, when ,. Likewise, they claimed that my ,??was an old technique developed by Bradley Efron. Yet it has nothing to do with Efron nor re-sampling. Andrew Gelman himself claimed that I stole ideas from his research (it is a classic syndrome for famous academic statisticians, they believe that they are the only ones having original ideas, and that anything remotely close to what they do is plagiarism).,What a bunch of arrogant, close-minded people! I don't even read Andrew Gelman's publications. They are disseminated to a very small audience in obscure journals that pretty much no mainstream people read, and it's written in convoluted English. I might sometimes re-invent statistics, but it is easier, faster, and better than wasting days looking after old publications and digesting / translating / adapting to modern data.,What these few statisticians don't understand is that these journals are no longer the outlet for many modern scientists, including me. Just compare ,??with one published by a traditional scientist, ,, independently and at the same time. You will see that mine is far more useful, provide code to make much faster, longer videos, and is in essence, of superior quality. You may disagree, and you are welcome to say so in the comment section below. Not that I tried to submit and got rejected, I actually never send my material to these journals anymore. I no longer have time for this, nor to write , book or peer-reviews. Their motto is ,??(and they must please their grantors, so innovation is dangerous for them), while my motto is ,.??
Big Data still causes a lot of confusion in people's heads: What really is it? What is new and what is old wine in new bottles? In order to bring a little more clarity to the concept I thought it might help to describe the 4 key layers of a big data system - i.e. the different stages the data itself has to pass through on its journey from raw statistic or snippet of unstructured data (for example, social media post) to actionable insight.,The whole point of a big data strategy is to develop a system which moves data along this path. In this post, I will attempt to define the basic layers you will need to have in place in order to get any big data project off the ground.,Although people have come up with different names for these layers, as we?€?re charting a brave new world where little is set in stone, I think this is the simplest and most accurate breakdown:,This is where the data is arrives at your organization. It includes everything from your sales records, customer database, feedback, social media channels, marketing list, email archives and any data gleaned from monitoring or measuring aspects of your operations. One of the first steps in setting up a data strategy is assessing what you have here, and measuring it against what you need to answer the critical questions you want help with. You might have everything you need already, or you might need to establish new sources.,This is where your Big Data lives, once it is gathered from your sources. As the volume of data generated and stored by companies has started to explode, sophisticated but accessible systems and tools have been developed ?€? such as Apache Hadoop DFS (distributed file system), which I cover in??,???€? or Google File System, to help with this task. A computer with a big hard disk might be all that is needed for smaller data sets, but when you start to deal with storing (and analyzing) truly big data, a more sophisticated, distributed system is called for. As well as a system for storing data that your computer system will understand (the file system) you will need a system for organizing and categorizing it in a way that people will understand ?€? the database. Hadoop has its own, known as HBase, but others including Amazon?€?s DynamoDB, MongoDB and Cassandra (used by Facebook), all based on the NoSQL architecture, are popular too. This is where you might find the Government taking an interest in your activities ?€? depending on the sort of data you are storing, there may well be security and privacy regulations to follow.,When you want to use the data you have stored to find out something useful, you will need to process and analyze it. A common method is by using a MapReduce tool (which I also explain in a bit more depth in my article on??,). Essentially, this is used to select the elements of the data that you want to analyze, and putting it into a format from which insights can be gleaned. If you are a large organization which has invested in its own data analytics team, they will form a part of this layer, too. They will employ tools such as Apache PIG or HIVE to query the data, and might use automated pattern recognition tools to determine trends, as well as drawing their conclusions from manual analysis.,This is how the insights gleaned through the analysis is passed on to the people who can take action to benefit from them. Clear and concise communication (particularly if your decision-makers don?€?t have a background in statistics) is essential, and this output can take the form of reports, charts, figures and key recommendations. Ultimately, your Big Data system?€?s main task is to show, at this stage of the process, how measurable improvement in at least one KPI that can be achieved by taking action based on the analysis you have carried out.,If you set up a system which works through all those stages to arrive at this destination, then congratulations! You?€?re in Big Data. And hopefully, ready to start reaping the benefits!,I hope this was useful? As always, please let me know your views on the topic.,Here is a ,??that summarises the key points, which you can download or share:
The analytics community has long been discussing whether analytics is about art or science. Analytics is more an art than a science in its ability to form conditions to drive business toward an action that is based on the confidence that the action will improve business performance. This ability to be actionable have recognized recently as the most important aspect in analytics [1]. The concept is known as , [2] shares some similar statements with ,, but some meaningful differences are present as well.,Classification rules plays a significant role in practical predictive analytics. The main advantage is that, of all possible pattern types , classification rules are closest to business rules, and the most comprehensible for business managers. Classification rules propose not only performing a high qualitative prediction to know what will happen, but also why it will happen , through exploration of the classification rules and achieving knowledge about fundamental reasons for a predicted future.,Mining classification rules from data is an important analytics task, but their further analysis can provide crucial knowledge for analytics project managers to tune the project's efforts at its different phases [3].,Generally, analysis of classification rules is performed based on set of objective interestingness measures [4]. Objective measures are those that depend on the structure of a pattern and can be quantified by using traditional statistical methods. Objective measures are a starting point for classification rules examination by a business manager. Such an examination is the first step in subjective evaluation of classification rules toward discovering actionable knowledge and driving the business user to take action. A consistent success in this activity will move the organization into different phases of analytics maturity [5].??,In practical terms, each classifier or tool will come with its own rules presentation and a very limited set of rules measures to describe some classification rule or set of rules. Even if some classification model supports PMML Tree Model [6] or PMML Ruleset [7], some work is still needed to calculate all types of relevant measures of interestingness.,To address all the goals that are named above, some solution must appeal to further requirements:,To demonstrate the idea behind , solution let's use R's C5.0 Tree [9] algorithm implementation [10] and playing tennis dataset [8], adopted for illustration purposes . The goal is ??to predict the "play" feature :,The R's C5.0 Tree model algorithmis applied to get the following decision tree:??,The tree can be presented as a set of , classification rules as follows:,Assuming conjunction between rules' predicates , the rules set can be presented as:,This relational form allows unified rules set presentation , visualization and calculation of classification rules counts, confidences, probability estimations, and a set of other interestingness measures. See a portion of them below, where CLASS presents the predicted value of target feature:,Referring to Figure 5, some measure (LIFT_CNT) denotes the ratio of the proportion of the predicted class in the rule to the proportion of the predicted class in the original dataset. The CLASS_COVERAGE measure expresses the ratio of the number instances of a predicted class, forming the rule to the overall number of instances within apredicted class in the original dataset. Generally, these two measures would be high , as possibly we are talking about some interesting rule. So in this way, a Rule with ID = 5 would be very important for the prediction of a "no" value and its analysis would continue.,Three data sources: original dataset (Figure 1), rules set presentation (Figure 4), and rules set evaluation (Figure 5), when linked together , propose a powerful framework for the exploration and managing of classification rules. This enhanced classification rule presentation is a step toward the design of a classification rule management system that benefits an analytics project manager and business users.,The solution can be extended in the following ways:,:,[1] L. Cao, "Actionable Knowledge Discovery and Delivery," ,, Vol. 2, No. 2 ,pp. 149-163, March 2012.,[2] A. Basu, "Five Pillars of Prescriptive Analytics Success," ,, (March / April 2013), Informs , 2013.,[3] S. Sharma and K.M.Osei-Bryson, "Toward an integrated knowledge discovery and data mining process model," ,, 25(1), pp. 49-67, 2010.,[4] L. Gengand H.J. Hamilton, "Interestingness measures for data mining: A survey," ,, 38(3) , Article 9 , 2006.,[5] T. H. Davenport, J. G. Harris, and R. Morison, ,. Harvard Business Press, 2010.,[6] PMML Tree Model, ,[7] PMML RuleSet Model, ,[8] J.R. Quinlan, "Induction of Decision Trees,",, 1(1March) , pp. 81-106, 1986.,[9] "Data Mining Tools See5 and C5.0", ,[10] R's C5.0 implementation, ,[11] B. Liu, W. Hsu, and Y. Ma, "Integrating classification and association rule mining," In ,, New York, pp. 80 - 86, 1998.


 , 
We've made a new interactive visualization of data science patterns.??,The ,??allows you to zoom in to find the various patterns; see snapshot below:
Leaflet is a modern open-source JavaScript library for mobile-friendly interactive maps. It is developed by??,??with a??team of dedicated??,. Weighing just about??,, it has all the??,??most developers ever need for online maps.,Leaflet is designed with??,,??,??and??,??in mind. It works efficiently across all major desktop and mobile platforms out of the box, taking advantage of HTML5 and CSS3 on modern browsers while still being accessible on older ones. It can be extended with a huge amount of??,, has a beautiful, easy to use and??,??and a simple, readable??,??that is a??joy to??,??to.,In this basic example, we create a map with??,, add a marker and bind a popup with some text to it:,For an interactive map and source code in text format, ,.,Learn more with the??,, check out??,, or head straight to the??,.??,If you have any questions, take a look at the??,??first.
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
I have been following the news on data science and wanted to share some of the titles here : The sexiest job of the 21th century, Ten Trends on Data Science, How to become a data scientist, What is data science, so on..,I wonder how and when data and science became sexy and everybody started to have a stake in data science. Is it because young energetic technology companies of Silicon Valley HAS named it that way? Do we need more data or do we have more data nowadays? Whether the data make science sexy or the otherway around?,All of these questions in my mind, I was sitting in my library looking at my books that never looked so sexy before. The second edition of Schaum?€?s Statistics book published in 1988 and purchased in 1993. I remember using it over and over to develop correlation algorithms to identify types of bread molds. I liked what I did in late 90s as a researcher (back then people developing algorithms were called researchers); but, I don?€?t recall anyone envying my job or calling it sexy. My attention moves to another forgotten book by Oppenheim: Signals and Systems. That book always meant long days and nights to understand the nature of Fourier transform developing algorithms and taking it a step further to Ceptrum Domain ( a logarithmic approach to separate frequency and phase components of signal/data).,I remember the days of Bezdek, fuzzy c-means clustering. My humble team developed algorithms to classify landmines in Angola. We spent a lot of time looking at the data, matrices and vectors before selecting a random sample group. Principal component analysis was another popular method to compress the data to decrease the cost of algorithms. It was not too long ago that I wrote my dissertation on it in 2010.,Within all those algorithms and applications my favorite is a very simple method called clipping. When I realized that outliners might have some information to develop forecasting algorithms I was so impressed with the power of clipping. It is basically a fuzzy thresholding. You identify a threshold (there are a lot of ways of identifying thresholds; averaging the data, averaging chunk of data, etc.) and change your values to zero if the value is smaller than the threshold; otherwise, keep it as it is. It was so sexy to me that I had a higher resolution in my data and I could recover more features. It made the algorithms slower and costly; but, who cares in this age of cloud and powerful computers.,These were the days that MATLAB crashed over and over, had problems with averaging and filtering. We all needed to validate what we were doing. I was wondering if we still need to validate what we are doing with data and try to learn from the nature of the data? Or else, are we a step further that all datasets are the same? Can we trust to commercial products and press a button to puke graphs and histograms? Is that why data science became so sexy?,All in all, the message I am trying to give is that data science is becoming a cluster of a lot of things and nothing. We forget about data itself and focus on how many??
Recently we did a project with??,??to visualize their global impact. Here is an animated globe showing readers using Worldreader mobile, based on??,??by the Google Data Arts Team. (Best viewed in Google Chrome.),We were fascinated to see the amazing adoption around the world, and especially growth in India. Way to go Worldreader! 2.2M readers globally via phones and e-readers... on the way to 3M. Wow!,.
Submitted by ,. Enjoy!
This is an update to our December 2013 article: ,. ??Microsoft and IBM still dominate, but we've seen some shift over the last 12 months:,Note that ,. Also, the number of data scientists per company (the statistical distribution), as a function of company size, ,.???? ??,We will soon publish a more detailed article with a breakdown per industry, and based on two LinkedIn profiles, not just mine. This will allow for increased accuracy, and more refined trend analysis. Below are the top 20 companies today, based on 10,000+ connections on my LinkedIn profile (the drops or gains in rank, among the top 20 companies, ,, are not statistically significant except for one company).,. We will also use this data set (a more granular version) to update our list of ,. Many more interesting articles are in our pipeline, including:,, to not miss these great articles.
 , , , , , , , 
 , , , , , , , , , , , , , , , , , , , , , , , , , , , , 
.,Data Science Central is one of the largest communities of analysts and data scientists in the world. We set about analyzing our database to understand a little bit more about the data scientist community. This is part of our continuing effort to give more insight into the ,.,Here is the distribution of members for the Top 50 locations in the US. The markers in the map show relative density of members. For e.g. Chicago has 423 members & is represented by a Red marker.,We also correlated our data with external job posting sites such as Indeed.com. We find that the overlap between our community and the larger market for data scientists jobs is strong. The below chart shows about 1400 open job postings with ?€?Data Scientists?€? in the job title. We have shown the top 13 locations representing about nearly half of the total population. As suspected New York & San Francisco / Bay Area are the top clusters for data scientists which also jives with our membership figures.,In the top 50 locations from the Data Science Central membership database, we found a strong cluster in the San Francisco ?€? Silicon Valley ?€? Bay area. When we group all of these locations together, we find that there are more than a 1,000 members of the top 50 locations in this area. This is by far the largest cluster of data scientists & does not include places like Los Altos and Pleasanton which are in the bay area but did not make it to the Top 50 locations. We find that these patterns are also being strong in the Indeed data as well.,Concentrations around the country included the Austin ?€? Dallas ?€? Houston area, the Washington DC, Philadelphia area & the Los Angeles-Irvine-San Diego belt. Other notable city specific pockets of data scientists included Boston, Seattle, Atlanta & Chicago.,What do you all think? Any key areas you think we are missing that we needed to focus more on?
The other day,??I found myself feeling exceptionally tired, not getting much work done??even though it was 11:30??in the morning. I thought this strange because this seems like a time when I should have be most productive. I then experienced a burst of energy in the afternoon (no coffee or energy drinks). I wondered what the conne?€?ction was and so I decided to explore some research. The results were not entirely what I expected, and even helped me re-adjust my day accordingly in order to be more efficient with my time.,According to a recent study conducted by Hypersoft and its OmniContext users, the most productive time of day is at 9:00am??with an average work intensity level of 52.04% working 6 times per week. At 1:00pm we are working 3 times??per week with an average work intensity of 52.92%. Lastly at 8:00am, we work 2 times??per week with an intensity of 57.1%. The take-away from this study could be that working on your most important tasks in the morning, and early afternoon is the most efficient method of productivity.,With the rise in individual??competition and a faster paced lifestyle thanks to technology, we are always trying to get caught up with something or someone. Whether it be concerning our job, our personal life, or even just catch up on some sleep. Personally,??I have accepted the fact that I cannot do everything.??However, I would like to be able to manage the things I can do, efficiently. ??If I could calculate which hours??I am most productive, I could better organize and??prioritize my tasks for the day. Theses statistics above can be assumed for the average person, however??individual working habits, sleeping patterns, mobility, and other factors all play a role in determining personal peak hours of productivity. After a short time using OmniContext myself and examining??my own analytics, I determined that my highest work intensity hours are between??9:00?€?10:00am??and 3:00?€?4:00pm?€??€?. I now re-organize my priorities of the day, working on the most important projects during these peak hours.?€?,What times are you feeling most productive throughout the day? Let me know in the comments section, or find out what your personalized peak hours of productivity are by checking out??,.?€?
A recent report predicts that a 100 million jobs could be at risk of being made redundant by automation. ??These are expected to include roles from supermarket cashier to accountant, as robotics and machine learning become increasingly able to perform routine tasks more efficiently than we can.,But surely if there is one job title which will be safe from the inevitable rise of the machines it will be data scientist or big data analyst ?€? after all haven?€?t I been championing that career path as one with a bright future for some time now?,And computers will always need a human to tell them what to do at some point, right? Sure a computer can learn, but won?€?t it always need a human to teach it how - and tell it what - to learn?,Well, recent developments do indeed seem to be indicating that computers may be getting better at some of the analytical functions which human input has, until recently, been necessary for.,The aim of Google?€?s , is to create ?€?an artificial intelligence for data science?€?. Specifically, it is creating software algorithms that can spot patterns, and report them in simple, easy-to-understand text. For example ?€?The data shows that Saturdays were consistently warmer than Sundays throughout the year and this correlates to higher turnout at outdoor events?€?.,As well as humans, machines can also interpret these results ?€? and use it as the basis for further analysis, by automatically selecting appropriate models and predictions to test it against.,The program was developed by a team of scientists at Cambridge University collaborating with others at MIT, who earlier this month were awarded $750,000 from Google?€?s Focus Reward Program to further their research.??,I have often said that humans are still better than machines at recognising patterns ?€? well this could mark the point where that starts to change.,?€?Statisticians, you?€?re next?€?, predicted one person on the somewhat pessimistic Google+ discussion group ,.,After all at the start of the industrial revolution, , when their jobs were threatened by newly-developed power looms, which allowed one worker to do the job of many with the help of machinery.??,So, am I now ready to stop recommending data analysis as a career path and suggest everyone starts training as something else instead? Well, of course not! No career is future proof, and data analysis (by humans) will be a fast growing field for a long time yet.,What this is likely to do (as is often the case with automation) is free up analysts from a lot of the more routine side of the work. With machines potentially writing their own tests and selecting their own relevant data, a lot more data analysis is going to be happening.,Operational data scientists would spend far less time getting their hands dirty with low-level modelling and simulation, and more time overseeing the output of hundreds of experiments being carried out automatically by AI analysts.,And strategic data scientists, who are used to considering the ?€?big picture?€?, will find that the picture has grown considerably, if they are able to count on the support of reliable automatic analytics algorithms, and all the additional data they could yield. The horizon will be greatly widened, in terms of top-level strategies available.,It is very early days yet for this kind of technology, but as with the whole field of machine learning, it is moving very quickly ?€? spurred by big bucks investments such as Google?€?s, from companies which understand the vast benefits that being first to use methods like this could bring them.,At the moment, the Artificial Statistician works with manually-fed data which is selected by a human. There?€?s no reason, though, that one day it won?€?t be fully online and have all of the data of the internet at its disposal. Teaching it the best places to look if it is looking for statistics to help with analysis on a particular subject will be an important step at some point along the line.,In reality, it will be a long, long time (if ever) that a computer can completely fill the world?€?s need for statisticians and analysts. It probably won?€?t happen unless we reach the point that science fiction from Matrix to Terminator has warned us of, and machines actually take over the world!,The point of analytics will always be to make things more efficient, easier or effective ?€? for humans. This means that we will always be the ones who select the ultimate questions that we want answered, and make a decision about which of the answers we receive is ?€?right?€?.,In my opinion this development opens up far more doors to opportunity than it closes for wily and adaptable analysts.,Remember the weavers I mentioned earlier? Well they weren?€?t all destitute following the invention of the power loom. Many of them saw that as the industry exploded in size following industrialization, there were still plenty of jobs. They were just different jobs than before. Many went on to become hugely successful by operating their own mechanical enterprises, backed up with the knowledge of the traditional trade they had practiced their whole lives. Try to follow the example of these adaptable individuals, rather than those who saw only the slippery slope into redundancy.,I hope you found this post useful. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., : , is a globally recognized expert in strategic metrics and data. He helps companies manage, measure, analyze and improve performance.,His new book is: 
(Reprinted from ,??),Data Innovation Day was celebrated on January 22, with many excellent events, including these:,In honor of the event, I prepared the following attempt at Big Data Humor...,10. ??CDO (Chief Data Officer)??speaking??to??Data Innovation??Day??event manager who is trying??to??re-schedule??the??event??for??Father?€?s??Day:???€?Hey! It?€?s pronounced???€?Day-tuh?€?, not???€?Dadda?€?.?€?,9. ??CDO??speaking??at the company?€?s??Data??Innovation Day event??regarding an acronym that was used to list his??job title??in the event program guide: ?€?I am the company?€?s Big??Data??,??guru ('Big Data AAS'), not the company?€?s??Big??Data??,??guru.?€?,8. ??Data Scientist??speaking??to Data??Innovation Day session chairperson: ?€?Why are all of these cows on stage with me? I said I was planning to give a??,??demonstration.?€?,?€?7. ??Anyone speaking??to you: ?€?Our organization has always done big data.?€?,6. ??You speaking??to anyone: ?€?Seriously???The title of our Data Innovation Day Event??is??,??€?,5. ??New cybersecurity??administrator??(fresh from college) sends this e-mail??to company?€?s??Data Scientists at 4:59pm: ?€?The security holes in our Hadoop system??are??now fixed.??It will now??automatically block??all ports from accepting??incoming data access??requests between??5:00pm??and??9:00am the next day.????Gotta??go now. ??Have a nice evening. ??From your new BFF.?€?,4. ??Data Scientist??to new HR??Department??Analytics???€?Specialist??regarding the truckload of??tree seedlings??that she received as her??end-of-year company bonus: ???€?I said in my employment application??that??I like Decision Trees, not Deciduous Trees.?€?,3. ??Organizer for the huge??Las Vegas??Data Innovation Day Symposium speaking to the conference keynote speaker: ?€?Oops, sorry. ??I blew your $100,000 speaker?€?s honorarium at??the poker tables??in the??Grand Casino.?€?,2. ??Over-zealous??cleaning crew speaking to Data Center Manager??arriving for work in??the morning after Data Innovation Day event that was held in the company?€?s Exascale??Data Center:???€?We did a very thorough job cleaning your data center. And??we won?€?t even??charge you for the extra??hours that we spent wiping??the dirty??data from all of those??disk drives that you kept talking??about yesterday.?€?,1. ??Announcement to University staff regarding the Data Innovation Day event: ???€?Dan Ariely?€?s??keynote talk,is being moved from room B002 in the Physics Department to??the Campus Football Stadium due to overwhelming student interest.?€?
Though often the focus of the urban noise debate, Caltrain is one of many contributors to overall sound levels along the Bay Area?€?s peninsula corridor. In this investigation, Cameron Turner of Palo Alto?€?s??,??takes a look at this topic using a custom-built Internet of Things (IoT) sensor atop the Helium networking platform.,If you live in (or visit) the Bay Area, chances are you have experience with the Caltrain. Caltrain is a commuter line which travels 77.4 miles between San Francisco and San Jose , carrying over 50 thousand passengers on over 70 trains daily.,I?€?m lucky to live two blocks from the Caltrain line, and enjoy the convenience of the train. My office,??,, is just one block away. The Caltrain and its rhythms, bells and horns are a part of our daily life, and connect us to the City and with connections to BART, Amtrak, SFO and SJC, the rest of the world.,Over the holidays, my 4-year-old daughter and I undertook a project to quantify the Caltrain through a custom-built sensor and reporting framework, to get some first-hand experience in the so-called Internet of Things (IoT). This project also aligns with??,?€?s broader ambition to build out custom sensor systems atop network technologies to address global issues. (More on this??,.),Let me note here that this project was an exploration, and was not conducted in a manner (in goals or methodology) to provide fodder for either side of the many ongoing caltrain debates: the electrification project, quiet zone, or tragic recent deaths on the tracks.,My interest in such a project began with an??,??in October 2014. The article addressed the call for a quiet zone in downtown Palo Alto, following complaints from residents of buildings closest to the tracks. Many subjective frustrations were made by residents based on personal experience.,According the the??,, the rules by which Caltrain operates, train engineers ?€?must begin to sound train horns at least 15 seconds, and no more than 20 seconds, in advance of all public grade crossings.?€?,Additionally: ?€?Train horns must be sounded in a standardized pattern of 2 long, 1 short and 1 long blasts.?€? and ?€?The maximum volume level for the train horn is 110 decibels which is a new requirement. The minimum sound level remains 96 decibels.?€?,Given the numeric nature of the rules, and the subjective nature of current analysis/discussion, it seemed an ideal problem to address with data. Some of the questions we hoped to address including and beyond this issue:,Our methodology to address these topics included several steps:,We developed a simple sensor based on the??,??platform. A baseline Uno board, equipped with a local ATmega328 processor, was wired to and??,??w/adjustable gain.,We were lucky to be introduced through the??,??event to a local company:??,. Backed by??,??et al, Helium is building an internet of things platform for smart machines. They combine a wireless protocol optimized for device and sensor data with cloud-based tooling for working with the data and building applications.,We received a Beta Kit which included a Arduino shield for uplink to their bridge device, which then connects via GSM to the Internet. Here is our sensor (left) with the Helium bridge device (right).,With our instrument ready for deployment, we sought to find a safe location to deploy. By good fortune, a family friend (and member of the staff of the Stanford Statistics department, where I am completing my degree) owns a home immediately adjacent to a Caltrain crossing, where Caltrain operators are required to sound their horn.,Conductors might also be particularly sensitive to this crossing, Churchill St., due to its proximity to Palo Alto High School and the??,, recently.,From a data standpoint, this location was ideal as it sits approximately half-way between the Palo Alto and California Avenue stations.,We deployed our sensor outdoors facing the track in a waterproof enclosure and watched the first data arrive.,Through a connector to Helium?€?s fusion platform, we were able to see data in near real-time. (note the ?€?debug?€? window on the right, where microphone output level arrives each second).,We used another great service, provided by??,, (now a part of??,) a San Francisco-based monitoring and metrics company. Using Librato, we enabled data visualization of the sound levels as they were generated. We were able to view this relative to its history. This was a powerful capability as we worked to fine-tune the power and amplifier.,Note the spike in the middle of the image above, which we could map to a train horn heard ourselves during the training period.,Next, we took a weekday (January 7, 2015), which appeared typical of a non-holiday weekday relative to the entire month of data collected. For this period, we were able to construct a 24-hour data set at 1-second sample intervals for our analysis.,Data was accessed through the Librato API, downloaded as JSON, converted to CSV and cleansed.,First, to gain intuition, we took a sample recording gathered at the sensor site of a typical train horn.,Click??,??to hear the sample sound.,Using matplotlib within an ipython notebook, we are able to ?€?see?€? this sound, in both its raw audio form and as a spectrogram showing frequency:,Next, we look at our entire 24 hours of data, beginning on the evening of January 6, and concluding 24 hours later on the evening of January 7th. Note the quiet ?€?overnight?€? period, about a quarter of the way across the x axis.,To put this into context, we overlay the Caltrain schedule. Given the sensor sits between the Palo Alto and California Avenue stations, and given the variance in stop times, we mark northbound trains using the scheduled stop at Palo Alto (red), and southbound trains using the scheduled stop at California Ave (green).,Initially, we can make two converse observations: many peak sound events tend to lie quite close to these stop times, as expected. However: many of the sound events (including the maximum recorded value, the nightly ~11pm freight train service) occur independent of the scheduled Caltrains.,On the Y axis above, the sound level is reported in the raw voltage output from the Microphone. To address the questions above we needed a way to convert these values to decibel units (dB).,To do so, a??,??was obtained from Fry?€?s. Then an on-site calibration was performed to map decibel readings from the sensor to the voltage output uploaded from our microphone.,Within R Studio, these values were plotted and a crude estimation function was derived to create a linear mapping between voltage and dB:,The goal of doing a straight line estimate vs. log-linear was to compensate for differences in apparatus (dB meter vs. microphone within casing) and overall to maintain conservative approximations. Most of the events in question during the observation period were between 2.0 and 2.5 volts, where we collected several training points (above).,A challenge in this process was the slight lag between readings and data collection with unknown variance. As such, only ?€?peak?€? and ?€?trough?€? measurements could be used reliably to build the model.,With this crude conversion estimator in hand, we would now replot the data above with decibels on the y axis.,Clearly the ?€?peaks?€? above are of interest as outliers from the baseline noise level at this site. In fact, there are 69 peaks (>82 dB) observed (at 1-second sample rate), and 71 scheduled trains for this same period. Though this location was about 100 yards removed from the tracks, the horns are quieter than the recommended 96dB-115dB range recommended by the FRA. (With caveat above re: crude approximator),Interesting also that we?€?re not observing the ?€?two long-two short-one long?€? pattern. Though some events are lost to the sampling rate, qualitatively this does not seem to be a standard practice followed by the engineers. Those who live in Palo Alto also know this to be true, qualitatively.,Also worth noting is the high variance of ambient noise, the central horizontal blue ?€?cloud?€? above, ranging from ~45 dB to ~75 dB. We sought to understand the nature of this variance and whether it contained structure.,Looking more closely at just a few minutes of data during the Jan 7 morning commute, we can see that indeed there is a periodic structure to the variance.,In comparing to on-site observations, we could determine that this period was defined by the traffic signal which sits between the sensor and the train tracks, on Alma St. Additionally, we often observe an ?€?M?€? structure (bimodal peak) indicating the southbound traffic accelerating from the stop line when the light turned green, followed by the passing northbound traffic seconds later.,Looking at a few minutes of the same morning commute, we can clearly see when the train passed and sounded its horn. Here again, green indicates a southbound train, red indicates and northbound train.,In this case, the southbound train passed slightly before its scheduled arrival time at the California Avenue station, and the Northbound train passed within its scheduled arrival minute, both on time. Note also the peak unassociated with the train. We?€?ll discuss this next.,Perhaps a more useful summary of the data collected is shown as a histogram, where the decibels are shown on the X axis and the frequency (count) is shown on the Y axis.,We can clearly see a bimodal distribution, where sound is roughly normally distributed, with a second distribution at the higher end. The question still remained why several of the peak observed values fell nowhere near the scheduled train time?,The answer here requires no sensors: airplanes, sirens and freight trains are frequent noise sources in Palo Alto. These factors, coupled with a nearby residential construction project accounted for the non-regular noise events we observed.,Click??,??to hear a sample sound.,Finally, we subsetted the data into three groups, one to look at non-Train minutes, one to look at northbound train minutes and one to look at southbound train minutes. The mean dB levels were 52.13, 52.18 and 52.32 respectively. While the order here makes sense, these samples bury the outcome since a horn blast may only be one second of a train-minute. The difference between northbound and southbound are consistent with on-site observation-- given the sensor lies on the northeast corner of the crossing, horn blasts from southbound trains were more pronounced.,Before making any conclusions it should be noted again that these are not scientific findings, but rather an attempt to add some rigor to the discussion around Caltrain and noise pollution. Further study with a longer period of analysis and duplicity of data collection would be required to statistically state these conclusions.,That said, we can readdress the topics in question:,Are train horns sounded at the appropriate time?,The FRA recommends engineers sound their horn between 15 and 20 seconds before a crossing. Given the tight urban nature of this crossing this recommendation seems a misfit. Caltrain engineers are sounding within 2-3 seconds of the crossing, which seems more appropriate.,??Are Caltrains coming and going on time?,Though not explored in depth here, generally we can observe that trains are passing our sensor prior to their scheduled arrival at the upcoming station.,??Are the Caltrain horns sounding at the appropriate level?,As discussed above, the apparent dB level at a location very close to the track was well below the FRA recommended levels.,??How do Caltrain horns contribute to overall urban noise levels?,The Caltrain horns generate roughly an additional 10dB to peak baseline noise levels, including period traffic events at the intersection observed.,Due to their regular frequency and physical presence, trains are an easy target when it comes to urban sound attenuation efforts. However, the regular oscillations of traffic, sirens, airplanes and construction create a very high, if not predictable baseline above which trains must be heard.,Considering the importance of safety to this system, which operates just inches from bikers, drivers and pedestrians, there is a tradeoff to be made between supporting quiet zone initiatives and the capability of speeding trains to be heard.,In Palo Alto, as we move into an era of electric cars, improved bike systems and increased pedestrian access, the oscillations of noise created by non-train activities may indeed subside over time. And this in turn, might provide an opportunity to lower the ?€?alert sounds?€? such as sirens and train horns required to deliver these services safely. Someday much of our everyday activity might be accomplished quietly.,Until then, we can only appreciate these sounds which must rise above our noisy baseline, as a reminder of our connectedness to the greater bay area through our shared focus on safety and convenient public transportation.,?€?,Sincere thanks to Helen T. and Nick Parlante of Stanford University, Mark Phillips of Helium and Nik Wekwerth/Jason Derrett/Peter Haggerty of Librato for their help and technical support.,Thanks also to my peers at The Data Guild, Aman, Chris, Dave and Sandy and the Palo Alto Police IT department for their feedback.,And thanks to my daughter Tallulah for her help soldering and moral support.,??,??
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
When we launched Mode?€?s??,, we knew we were putting an MVP (minimum viable product) editor into the wild. As soon as we released it, we started thinking about how we could make it even easier for analysts to create and share work. But, the beta gave us something we couldn?€?t get otherwise: usage data. By analyzing the way analysts have been using Mode, we?€?ve been able to validate some of our early feature ideas, throw out others, and uncover completely new ones.,This process isn?€?t unique?€?software companies are constantly digging through their data looking for product guidance (in fact, we built Mode to help companies do exactly that). In the spirit of??open-sourcing some of our analysis code, open-sourcing some of our analysis code, we?€?re happy to share some of the analysis that went into??yesterday's product updates.yesterday?€?s product updates.,In addition the schema browser and report builder features, we changed the editor?€?s layout. One of the toughest decisions we had to make?€?and one where our own opinions were strongest?€?was how the editor should be oriented. Some people wanted the query stacked on top of the data, some wanted a side-by-side view, and others (like me) waffled back and forth. So we decided to see if our data held the answer. What kind of editor would most users want?,After looking, we discovered the answer: it depends. But surprisingly, not only does it depend on the person,??,.,When analysts are first writing a query, the queries tend to be short and the results usually include a lot of columns. Short queries and wide tables are better suited for a horizontal editor.,But as analysts edit their reports and get closer to the final version, queries get longer and datasets get narrower. Both of these attributes are better suited for a vertical editor.,This data guided the design of a flexible editor. It also inspired us to incorporate a horizontal/vertical orientation option into the UI rather than putting it in a settings menu off the editor.,One of the first things we learned when we started looking at user behavior was that analysts (people who write queries) and business users (those who primarily view results) interact with Mode in different ways. Analysts start sessions on Mode?€?s home page more often than business users, while business users tend to enter the product via links to specific reports.,While we expected this behavior, we were still surprised by exactly how many business users come via report links. It?€?s the most common entry point?€?and about twice as common for business users as for analysts.,Knowing this, we looked into how we could change the product to be most useful to both sets of users, and how we could make the interactions between these two groups better.,Because analysts tend to come to the homepage, this area is the best place to help analytics teams stay on the same page with current projects. We added an activity feed that shows when someone publishes a report, updates a report, or creates a list. Clicking on timestamps in the activity feed shows the??,??for each edit.,Small improvements can matter as much as big feature additions. Extra clicks and small annoyances hurt a user?€?s experience?€?and we sought to rid Mode of as many of these as possible with this launch.,After creating hundreds of charts for our own internal reporting, I became acutely aware of exactly how many clicks it took to make the charts I wanted. While much larger charting changes are on the way, we worked to smooth over small annoyances in the meantime.,Of all the reports in Mode that have charts, the ?€?Candy?€? color scheme is the most popular (we limited the analysis to charts with titles to weed out scratch-work charts that the analyst had no intention of saving). Given that a user has to click two buttons to change chart colors, that?€?s a remarkable rejection of our default colors?€?and a huge vote for the ?€?Candy?€? scheme. Based on this data, we changed the default chart color to ?€?Candy.?€?,More features often means more complexity. While we added features, we were careful to keep Mode from becoming overwhelmed with??,. This meant making some tough decisions to cut features.,Though it?€?s a popular among folks at Mode, we haven?€?t heard much from our customers about the report pinning feature. When we looked into it, we realized why: people use pins almost exclusively for pinning tables. More than 90% of manually created pins are tables.,When we looked into ways to design the editor without report pins, we found a seamless way to incorporate table pins into the new schema browser. By building pinning into an otherwise standard schema browser, the editor automatically surfaces information about the tables you?€?re querying. It?€?s a powerful tool?€?but adding it came the cost of pinning reports. Looking at our data helped reassure us that we?€?re making the right tradeoff.,We used data to help guide as many decisions as we could when developing Mode?€?s new workflow. But in some cases, data doesn?€?t reveal the full picture. New or novel features may be outside of the scope of the data produced by our current product. Different data points could tell conflicting stories. And data only reveals what our users do, not what they want. In these cases, we often turned to user research for guidance. But the question we ask our users and our data is always the same: how can we make analysts' lives better?
Machine learning isn't a set-it-and-forget-it operation. Even with solid examples, ML algorithms can still fail and end up blocking important emails, filtering out useful content, and causing a variety of other problems.,??(,In this report, industry analyst Ted Cuzzillo examines real-world examples of active learning and you'll discover, the point at which algorithms fail is precisely where there's an opportunity to insert human judgment to actively improve the algorithm's performance.
At the CES 2015, I was fascinated by all sorts of possible applications of IoT ?€? socks with sensors, mattresses with sensors, smart watches, smart everything ?€? it seems like a scene in sci-fi movies has just come true. People are eager to learn more about what?€?s happening around them and now they can.,??,While I was at there I attended a talk given by ,???€? he is awesome. He pointed out that the prevalence of smartphone is the key to the realization of the phenomenon called ?€?Quantified Self.?€? I agreed with him. Smart phones play a vital role as a hub where all our personal data converge and present, seamlessly. The fact that you carry your smartphone around all the time and that the screen size perfectly reveals all the information results in a catalyst for wearable devices, IoT or what we like to call it, ,.,??,It?€?s all relevant; Big Data, IoT, Wearable, Cloud Computing?€? While most data is uploaded to the cloud, the client devices are generally powerful enough that the computing can be decentralized. That said, small data (client side) and big data (server side) form an eco-system where small data triggers the knowledge base cultivated by big data and does the predictive analysis and decision making in a timely manner. Furthermore, your smartphone gathers versatile data and is able to analyze cross-app data to personalize your application settings. For example, what about optimizing navigation based on my physical condition? Or how about suggesting the best route according to my health along with the weather? These individual data records might be small, but collectively they enrich the content of analysis and contribute some amazing value. We at BigObject really appreciate this context of Big Data.,??,??once said, ?€?I think we are all underestimating the impact of aggregated big data across many domains of human behavior, surfaced by smartphone apps.?€? For us here at BigObject, the next big thing in big data is to find out a methodology that can link multiple data sources together and identify the meaningful connections between that data. Most importantly it must be responsive enough to deliver actionable insight and simple enough for people to adopt. That is the key to fulfill a connected world.??
??,?? 
In the film Silver Linings Playbook, Jennifer Lawrence?€?s character, Tiffany, attempts to win over the Philadelphia Eagles-obsessed family of her friend, Pat, by claiming that her time with him is bringing the team good ?€?juju.?€?,?€?The first night Pat and I met at my sister?€?s, the Eagles beat the 49ers handily, 40?€?26,?€? she says, pacing around a living room filled with Pat?€?s rapt relatives. ?€?The second time we got together we went for a run and the Phillies beat the Dodgers 7?€?5 in the NLCS. The next time we went for a run, the Eagles beat the Falcons 27?€?14?€??€? And so on.,With the Super Bowl approaching, it?€?s easy to find other fans who fervently cling to supposed causal connections between events and victories. And should their teams prevail, it?€?s also sometimes hard to disprove the superstition. Consider the Redskins rule, as explained by John Elder, founder of the data mining firm??,: for over 70 years, if the Washington Redskins won their last home football game, the incumbent party would win the presidential election. One didn?€?t actually cause the other, but for generations, they just happened to line up.,The rich realm of sports superstitions and rituals this time of year highlights the increasing need for all of us?€??€??€?not just those glued to their Super Bowl screens?€??€??€?to grasp and understand the basics of statistics, well beyond the boundaries of any playing fields.,Ill-prepared consumers have been forced in recent years to master the financial literacy skills necessary to handle their own retirements, investments, and other complex financial instruments as companies shifted these responsibilities to them. These days, statistical literacy is emerging as an equally key skill. An avalanche of big data and a regular stream of media reports on statistics and research forces us to glean truth from the numbers on everything from whether certain vitamin supplements affect heart attack rates to the risks versus benefits of regular mammograms.,But like consumers??,??to figure out how much they?€?re really paying in fees for their IRAs, folks who don?€?t know the basics of statistics find themselves at a serious disadvantage.,Statistics and statistical theories serve as the basis for everything from passenger profiling in an era of terrorist threats to the effectiveness of new programs to reduce the rate of hospital errors. They can shed light on whether a hedge fund?€?s success is genuine or due to chance. They predict whether a given subscriber will leave this year, or an insurance claim is likely to be fraudulent.,They can mislead, just as they enlighten, and we need to know the difference. Even in completely randomly-generated data, interesting patterns appear. If the data are big enough and the search exhaustive enough, the patterns can be very compelling. But they could be nothing more than a mirage that disappears with time and further investigation.,Well-known??,??from the statistics world include studies that show children with bigger feet are more proficient in spelling and states with higher divorce rates have lower death rates. But older children, who tend to have bigger feet, naturally spell better than younger ones, and states with higher divorce rates have larger shares of a younger population cohort. Don?€?t rush to your lawyer?€?s office; getting divorced won?€?t help you live longer.,I?€?ve been teaching statistics for over two decades. What I?€?ve seen is that for many people, learning statistics is as obscure as reading the fine print in their financial documents. Usually that?€?s because much of the teaching method is forced, artificial, and divorced from what most students will end up doing in the workforce, or in their daily lives. But the rapid growth of data science and analytics is opening up new ways to teach and learn statistics. People can figure out how to derive statistical meaning and comprehension not just from mathematics, but also from context and purpose. It?€?s not just an academic exercise. If you don?€?t understand the statistical world around you, you don?€?t really know how things work: Whether watching television actually causes violent behavior. How your gender might impact your earnings. Even the??,??time of the day to exercise to lose weight.,And in a world where misinformation spreads quickly through the media, our failure to comprehend statistics regularly leads to controversy, and consequences.,Parents in a Maryland suburb recently allowed their children, ages 10 and 6, to walk home alone from a neighborhood park, sparking a debate on our understanding of risk. We might feel like the world is more dangerous for children than it was a few decades ago, but the numbers don?€?t bear that out. Still, that didn?€?t stop the police from showing up at the family?€?s door.,Then there?€?s the vaccines debate. A study last year created a stir on the Internet by purporting to show that African American boys had a greater risk of autism associated with the time they were vaccinated, and alleging the Centers for Disease Control covered up the findings. The journal that published it eventually retracted it. Among other??,??the study was incorrectly designed, and incorrectly analyzed the data it produced, according to??,??a nonprofit group that promotes statistical literacy. But that doesn?€?t mean people won?€?t still think it?€?s true.,?€?These flaws will be obvious to statisticians and to scientists who understand statistical analysis,?€? STATS.org director Rebecca Goldin??,??in a recent post. ?€?The problem is how to undo the damage among a public that is skeptical of scientific authority, and is suspicious or even hostile toward vaccination.?€?,I?€?m launching a series of posts to offer some examples of the practical application of statistics in the world around us, in an effort to help us all understand them better. I?€?ll explain methods I employ, like resampling and bootstrapping, that make statistics simpler and more transparent.,The Silver Linings Playbook example shows how coincidences happen all the time in life?€??€??€?often to a greater extent than we thought possible. But I will explain that with things like resampling?€??€??€?the computer equivalent of drawing numbers from a hat?€??€??€?we can actually decode some of the mystery of statistical analysis, and know with much greater certainty what is true as opposed to chance. We do that through the process of taking repeated samples from observed data?€??€??€?or shuffling that data?€??€??€?to figure out what effect random variation might have on our statistical estimates, our models and our conclusions.,For example, I use a black magician?€?s hat I usually don at Halloween to??,??the permutation test?€??€??€?combining two or more samples in a hat, shuffling the hat, and then picking out resamples at random. I used this to test whether a program to reduce medical errors was effective, or due to chance, but it can apply to many other questions as well.,In future posts, I?€?ll talk about how you can determine for yourself the validity of political surveys and polls as campaign season heats up. I?€?ll cite the recent explosion in subprime car loans to revisit how data was misused and misinterpreted during the subprime mortgage crisis. I?€?ll talk about the ways predictive analytics and modeling techniques can go right?€??€??€?and wrong. There are ways to take away the intimidation of talking about statistics. You really can cut through the fog and be able to understand exactly what those significance tests, p-values, confidence intervals and other difficult concepts are all about. And overcoming the fear puts you at a genuine statistical advantage.,(Peter Bruce is founder of The Institute for Statistics Education at Statistics.com, the leading online provider of analytics and statistics courses since 2002. He also is the author of the newly-released Introductory Statistics and Analytics: A Resampling Perspective. (Wiley),Follow Peter:,Twitter: @petercbruce, @statisticscom,Websites:??,,??
??,??,This book is intended for a variety of audiences:,(1) There are many people in the technology, science, and business??disciplines who are curious to learn about big data analytics in a broad??sense, combined with some historical perspective. They may intend to??enter the big data market and play a role. For this group, the book provides??an overview of many relevant topics. College and high school students who have interest in science and math, and are contemplating about what to pursue as a career, will also find the book helpful. ??,(2)??For the executives, business managers, and sales staff who also??have an interest in technology, believe in the importance of analytics,??and want to understand big data analytics beyond the buzzwords, this??book provides a good overview and a deeper introduction of the relevant??topics., (3) Those in classic organizations?€?at any vertical and level?€? who??either manage or consume data find this book helpful in grasping the??important topics in big data analytics and its potential impact in their, organizations., (4) Those in IT benefit from this book by learning about the challenges??of the data consumers: data miners/scientists, data analysts,??and other business users. Often the perspectives of IT and analytics??users are different on how data is to be managed and consumed.??, (5) Business analysts can learn about the different big data technologies??and how it may impact what they do today., (6) Statisticians typically use a narrow set of statistical tools and??usually work on a narrow set of business problems depending on their??industry. This book points to many other frontiers in which statisticians??can continue to play important roles., (7) Since the main focus of the book is high-performance data mining??and contrasting it with big data analytics in terms of commonalities??and differences, data miners and machine learning practitioners gain a??holistic view of how the two relate., (8) Those interested in data science gain from the historical viewpoint??of the book since the practice of data science?€?as opposed to the??name itself?€?has existed for a long time. Big data revolution has significantly??helped create awareness about analytics and increased the need??for data science professionals.
Job titles for data scientists, including details about the simple but powerful classifier used to categorize these job titles. This analysis provides a break down per job category, and granular reports that you can download for free (job titles broken down per company, category and level), as well as NLP (natural language processing) source code. It is based on analyzing connections from multiple LinkedIn profiles - totaling more than 10,000 professionals. The first study ,.,The table below shows the top job titles in the , category. The full list has 700+ job titles shared by at least two practitioners, across the 11 following categories,The full table ??,??(Excel spreadsheet). If you include job titles shared by only one person, we have 7,000+ job titles: this is another example of a system ,, with very long tail. A very interesting spreadsheet with full details (including job title, job category, level, and company name) is ,. If you are not yet a member, ,??to access the spreadsheet.??,In order to identify job??categories and levels, we first created a ,??of all one-token and two-token keywords found in job titles, ranked by frequency, after filtering out tokens that make no sense (such as ,, because it is always associated with ,, in job titles containing ,).,The top 2-token words are displayed in Figure 2:,The full list, including both one- and two-token words (totaling 15,000 words),??,??(Excel spreadsheet).,The job??categories, levels and cleaned job tiles were computed with the following Perl script, in section 3. While this is a clustering problem (creating a taxonomy of job titles for data scientists), because of our simple and scalable approach, from a computational point of view, it looks more like an indexing problem, rather than pure clustering., , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 
In this post, I discuss a possible new approach to teaching Programming for Data Science.,Programming for Data Science is focussed on the??,.?? Everyone seems to have a view including the venerable??,??(,).,On first impressions, this??,??(ability to master multiple languages) sounds complex.,Why teach 3 languages together? ??(For simplicity ?€? I am including SQL as a language here),Here is some background,Outside of Data science, I also co-founded a social enterprise to teach Computer Science to kids????,. At Feynlabs, we have been working with ways to accelerate learning to Code. One way to do this is to compare and contrast multiple programming languages. This approach makes sense for Data Science also because a learner can potentially approach Data science from many directions.,From a pedagogical standpoint, this approach is similar to??,??who stressed the importance of prior knowledge in being able to learn new concepts: ??,But first, we address what is the problem we are trying to solve and how that problem can be broken down,Data science involves the extraction of knowledge from data. Ideally, we need lots of data from a variety of sources.?? Data Science lies at the intersection of multiple disciplines: Programming, Statistics, Algorithms, Data analysis etc. The quickest way to solve Data Science problems is to start analyzing data as soon as possible. However, Data Science also needs a good understanding of the theory ?€? especially the machine learning approaches.,A Data Scientist typically approaches a problem using a methodology like??,. Some of these steps are common to a classic data warehouse and are similar to classic??,??approach. However, the modelling and interpreting stage are unique to Data Science. Modelling needs an understanding of??,??and how they fit together. For example: Unsupervised algorithms (Dimensionality reduction, Clustering) and Supervised algorithms (Regression, Classification),To understand Data Science, I would expect some background in Programming. Certainly, one would not expect a Data Scientist to start from ?€?Hello World?€?. But on the other hand, the syntax of a language is often over-rated. Languages have quirks ?€? and they are easy to get around with most modern tools.,So, if we try to look at the problem / big picture first (ex the Obtain, Scrub, Explore, Model and Interpret) stages ?€? it is easier to fit in the Programming languages to the stages. Machine Learning has 2 phases: the Model Building phase and the Prediction phase. We first build the model (often as a batch mode ?€? and it takes longer). We then perform predictions on the model in a dynamic/real-time mode. Thus, to understand Programming for Data Science, we can divide the learning into four stages: The Tool itself (IDE), Data Management, Modelling and Visualization,After understanding the base syntax - it?€?s easier to understand the language in terms of its packages and libraries. Both Python and R have a vast number of packages (such as??,) ???€? often distributed as libraries (,). Both languages are interpreted. Both have good IDEs such as??,,??,??for Python and??,??for R. If using Python, you would probably use a library like??,??and a distribution of Python such as the??,. With R, you would use the??,?? and install specific packages using R?€?s ??,??package management system.,After the Data modelling stage, we come to Data exploration and visualization. Here, for Python ?€? the??,??package is a powerful tool for data exploration. Here is a simple and quick??,. Similarly, R uses??,??and??,??packages for Data exploration and visualization.,Finally, much of this discussion is a rapidly moving goalpost. For example, in R, large calculations need the data to be loaded in a matrix (ex nxn matrix manipulation). But, with platforms like Revolution Analytics ?€? that can be overcome. Especially with??,???€? and with Microsoft?€?s history for creating good developer tools ?€? we can expect development in R would be simplified.,Also, since both R and Python are operating in the context of Hadoop for Data science, we would expect to leverage the Hadoop architecture through HDFS connectors both for??,??and R Hadoop integration. Also, one would argue that we are already living in a??,??especially for Real time calculations and that??,Here is a??,??and a post about??,. Interestingly, the??,??includes integration with 3 languages (Scala, Java and Python) but no R. But the power of Open source means we have??,??which integrates R with Spark.,The approach to cover multiple languages has some support - for instance, with the??,??. You could also achieve the same effect by working on the command line for example in??,Even in a brief blog post ?€? you can get a lot of insights when we look at the wider problem of Data science and compare how different approaches are addressing segments of that problem. You just need to get the bigger picture of how these Languages fit together for Data Science and understand the ??major differences (for example vectorization in R).,Use of good IDEs, packages etc softens the impact of programming.,I hope to teach this approach as part of??,Programming for Data Science will also be a separate module talk over the next few months at??,,??,,??,,??,??and??,For more schedules and details please sign up??
Can the recent four-point Super Bowl victory by the New England Patriots clue us in as to how many people will die in helicopter accidents?,The??,??between helicopter accident fatalities and Super Bowl point spreads is the latest??,??from Tyler Vigen, who compiles a,??list of things that things that appear to be related, but most assuredly are not. (The number of people who drowned by falling into a swimming pool and the number of films Nicholas Cage appeared in is a personal??,.),As I mentioned in my earlier??,??it?€?s easy to fall for supposed causal connections between events. And we have a long history of doing so, separate from the Super Bowl and other sports events. In the 1950s, for example, the incidence of polio was on the rise. Disease rates were found to be correlated with the consumption of ice cream?€??€??€?the more ice cream consumed, the more polio cases. Some medical authorities advised parents not to feed ice cream to their children.,The misunderstanding of correlation when it comes to public health clearly persists today?€??€??€?we see it in the current and heated controversy over whether vaccines are safe for children. That debate shows us how mastering the basics of statistics is becoming increasingly important. The decisions we face on a regular basis often depend on the level of our understanding of risk, and other statistical measures. Our choices directly affect our quality of life?€??€??€?and, as the measles outbreak illustrates, the lives of those around us.,If you have any doubt that we all need to become more statistically literate, consider a different vaccine debate, the one over the flu shot. In the mid-1970s, for reasons that remain unclear, there was an increase in the incidence of Guillain-Barre syndrome, a debilitating??,??among people who got the swine flu vaccine. But you can also get GBS from contracting the flu itself. So which is riskier, getting the flu shot, or getting the flu? As the Washington Post??,??last month, scientists at the Ottawa Hospital Research Center studied the numbers and concluded it?€?s almost always better to get the flu shot. But decide for yourself?€??€??€?they also developed an??,??that allows you to calculate your personal risk, based on your age, gender, and other factors.,As you encounter statistical dilemmas like these going forward, there are some basic theories you can learn to help you think them through. When you are presented with correlations that seem, well, spurious, for example, it?€?s important to remember this: Correlation is not causation. Correlation?€??€??€?even statistically significant correlation?€??€??€?does not necessarily imply anything about causation.,Consider a study that found that infants who slept with the lights on were more likely to develop near-sightedness later in life. The study caused many a parent to switch off the lights in the nursery at night?€? but that missed the point. The real cause was not the light, but a genetic link to myopic parents. Lights in infants?€? rooms were more likely to be left on by myopic parents than parents with normal sight, a followup study discovered. The lights left on correlated with the development of myopia?€??€??€?but this was not the cause. The real cause was the parent?€?s myopia.,So when you are sizing up correlations, question whether there is a reasonable theory to explain the correlation, and whether some third factor might cause it.,Think about the polio-ice cream link I described. Can you think of an external factor that might be correlated with both ice cream consumption and polio incidence? Let me know your theories, and I?€?ll write about them in an upcoming post.,In the meantime, regarding the supposed link between helicopter deaths and Super Bowl point spread?€? forget it. But when it comes to real-life correlations we often encounter, there are ways you can determine for yourself whether trends in data are real or due to chance, and I?€?ll explain that in upcoming posts.,(Peter Bruce is founder of The Institute for Statistics Education at Statistics.com, the leading online provider of analytics and statistics courses since 2002. He also is the author of the newly-released Introductory Statistics and Analytics: A Resampling Perspective. (Wiley),Follow Peter:,Twitter: @petercbruce, @statisticscom,Websites:??,,??
Many a times, data is presumed to be big.?? The Big data that cannot be handled effectively by traditional software are usually images, weblogs, Facebook feeds, tweets and internet of data in vast magnitude.?? ??In real case scenarios, servers with monstrous configurations are threaded in parallel to create redundancy and ability to handle multiple requests in structured and semi structured data.?? In this article I will talk briefly about using parallel thread processing in base SAS to process datasets in order of billion rows.?? Along with parallel thread processing, hash joins, inner joins and views are also used where applicable to improve processing time.???? Also check out 1,.,Example configuration contains 5 servers connected in parallel thread architecture where SAS creates nodes for each instance initiated from either base SAS or through SAS EG.???? Also the data layer we are discussing below resides on scalable performance data server architecture or SPDS.?? ??For example if I am running 5 data intensive queries and data cleansing, I start with signon command to initiate sessions ( 5 sessions opened on 5 individual server nodes).?? One caveat to this is usually SASgrid is setup in such a way that each session is posted to receiving server node and if submitted at same time, it will distribute on available nodes (might or might not spread on 5 different servers).??, start_sessions(count); /* Here count can be an automatic calculation or a fixed number.*/ /*here count=5*/,?????????? %do i = , %to &count;,?????????????????????? signon sess&i signonwait=no connectwait=no cmacvar=scaproc_signon_&i;,?????????? %end;, start_sessions;,Here by starting a new session, all previous macro variables, variables and other temporary data is lost.?? Sasrc file can be used to automatically assign system credentials and path to some common marco variables but we should use %syslput to specifically invoke a previous session variable into the new session. Next step would be to process a session as below,rsubmit sess1 wait=no;??,%include "&app_path./xxx.sas"; /*this will contain the macro codes*/,%, (x=,); ?? ????,endrsubmit;,If there are ?€?n?€? more sessions, we can refine the code using macro facility as below., submit(count);??,?????????? %do i = , %to &count;,rsubmit sess&i wait=no;,%syslput k=i;,%include "&app_path./xxxx.sas";,?????????? % ,_specific_macro_here (tt=a|b|c|d|e,j=&&&k.);/*here macro variable tt is passed to the xxxx.sas code which contain macros which will divide a|b|c|d|e into 5 variable with specific functionality */,endrsubmit;,%end;, submit;,waitfor _all_ sess1 sess2 sess3 sess4 sess5;,??,?€?waitfor?€? statement can be included in macro as well if you are coding by ?€?n?€?.?? Later the sessions are all closed.??, scagrid_waitfors(count);,?????????? %do i = , %to &count;,?????????????????????? signoff sess&i;,?????????? %end;, scagrid_waitfors;,When processing a large dataset, _n_ can be calculated and can be divided into multiples of say 1 million rows and spread on the parallel nodes.?? Macro facility can be used to define the predefined analysis process and can be input with segregated data.?? Example, 1 to one millionth row runs on node1, 1 millionth ?€? 2 millionth rows runs on node2 and so on.?? Note that all endrsubmit will wait till all the submitted data is processed.?? Later the analyst can decide if he wants to move all the data into one dataset using PROC APPEND (usually expensive) or create a cluster on the processed datasets residing on SPDS data layer.?? Notes on clustering and un-clustering ,.??,Combining a mix of technical expertise in SAS, data mining and Hadoop technologies like Pig, Hive, Hbase and MapReduce, we can bring out analytical insights with greater inclusion of determinants coming from claims, adjudication data, diagnosis codes data, health sensors data, census data, weblogs and customer reviews etc. for solving a specific predictive analytical problem in health care.
2015 Survey of Data Scientists Uncovers What's Holding Them Back in Their Jobs and How Organizations Can Empower Them to Deliver Greater Strategic Value,CrowdFlower??today released its 2015 Data Scientist Report. Findings revealed that data scientists saw messy, disorganized data as a major hurdle preventing them from doing what they find most interesting in their jobs: predictive analysis and data mining for behavioral patterns and future trends. The majority of data scientists surveyed also acknowledged the skills shortage within their field.
I have just a little over three hours left to live. That?€?s the half-life of content these days, I am told.,A few hours ago, I was another small idea, floating in your head. Then you checked Buzzsumo to find what is the trending content of the hour. What will go with today?€?s coffee.,Voila, you had it. The themes and news that will drive traffic, maybe even stop traffic, for a few hours!,You started working. A few quick keyboard strokes later, you posted me, on all the social channels at your disposal, all ten of them. But you were not done with me yet.,Of course, you have a documented content strategy now. And like almost every other B2B marketer, you are increasing your content marketing budget for the year. And you are all set to create more content, a regular Tsunami of content. Content sells, right? That the order of the day. And of course you have tools, cartloads of them. Like a regular Ninja, you now have a basket of tools to measure the holy grail, content marketing ROI.,Tools to create buyer persona, tools to drive automated emails to segmented audience, tools to track page views, referrals, visitor flow, likes, comments, shares, downloads, leads. So yes, long after my death, you?€?ll continue to nurture me and post me in the hope of breaking the content ennui.,But you see, I know the last thought that rushed through your head before you clicked Submit and Share and started my countdown to death.,-Will I go viral?,-Will I cross the dangerous and unknown road from likes to leads?,-Will I help you break the content molehill?,-Will I escape the horrible fate, an unliked death?,And none of your new fancy tools can answer that question for you. Can they? Which brings me to my question.,I maybe just an almost-dying piece of content. But tell me, shouldn?€?t I really start with the story you absolutely want to tell? The story that encourages, influences, educates, inspires, makes someone smile during a long day? The story you really believe in?,With all your dynamic armour of tools and techniques, are you forgetting how it all started, with a simple and powerful story? You are still a storyteller, aren?€?t you? They do say ?€?Great stories happen to ones who can tell them.?€? See??,.
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
??asked a question ,??on the distinction between Decision Science and Data Science.,My 2 cents. Others may disagree, given their particular orientation. Given a choice, we are in Decision Science domain; Given a firehose we are in Data Science domain. I would suggest that Decision Sciences ends where Data Science begins. Decision Scientists don't necessarily work with Big Data. Data Scientists are specialized to work with Big Data (or recognize when something isn't truly Big Data) and all the problems associated with that domain.,Decision Scientists build decision support tools to enable decision makers to make decisions, or take action, under uncertainty with a data-centric bias. Traditional analytics falls under this domain. Often decision makers like linear solutions that provide simple, explainable, socializable decision making frameworks. That is, they are looking for a rationale. Data Scientists build machines to make decisions about large-scale complex dynamical processes that are typically too fast (velocity, veracity, volume, etc.) for a human operator/manager. They typically don't concern themselves with whether the algorithm is explainable or socializable, but are more concerned with whether it is functional, reliable, accurate, and robust.,When the decision making has to be done at scale, iteratively, repetitively, in a non convex domain, in real-time, you need a Data Scientist and lots of compute power, bandwidth, storage capacity, scalability, etc. The dynamic nature of the generating process which leads to high volume, high velocity data, in my mind, is the differentiating factor between the two domains.,The transition from manually input data sources to sensor-driven real time data sources is the underlying theme of the "Internet of Things" and this is especially true with the "Industrial Internet of Things" with complex machines interconnected with hundreds of sensors relaying data continually. The underlying math may be somewhat sharable, but doing it at scale, at velocity, etc. requires an end-to-end approach, and a greater emphasis on high speed algorithms. This is particularly true when you are talking about IoT, and especially IIoT, where you do not want humans to be making decisions "on the fly".,A Decision Science problem might be something like a marketing analytics problem where you are segmenting a customer base, identifying a high margin target market, qualifying leads, etc. Here the cost of a bad 'decision' is relatively low. I view Decision Science from the perspective of "decision making under uncertainty" (see Decision theory) which suggests a statistical perspective to begin with. A Data Science problem might be more like "How do I dynamically tweak a Jet engine's performance in flight to ensure efficiency/uptime during flight, to achieve stable and reliable output, to optimize fuel consumption, to reduce maintenance costs, and to extend the useful service life of the engine?" The cost of a failure in flight can be pretty high. Here, you would want to have a reliable machine making those computations and decisions in real time, relying on both ground and in-flight streaming data for real time condition monitoring.,In reality, practitioners tend to be "Agnostic" Scientists that are transferring skills from one domain to another, with varying degrees of comfort with the different tools out there. However, a Data Scientist is more likely to have a diverse background that spans multiple disciplines including statistics, computing, engineering, etc. In these examples, the decision maker is a key differentiating factor in my view. A Decision Scientist typically provides a framework for decision making to a human being; a Data Scientist provides a framework for decision making to a machine.,When machines become more human, this distinction may be called into question, but for now, I think it works.,What do you think? Did I just give you a choice? Or, did I just deluge you with data? ??Your call.,---,Note: In the above marketing analytics example, I am not referring to clickstream data; rather, I refer to historical records stored on an RDBMS somewhere.
Let us see what are the major steps in customer journey & which are moments of truths ( customer interactions) that are:,Unlike old days, they buy the product mostly online.,??Organizations engage with customers via omni-channels such as: Calls, Mobiles, Emails, SMS, Surveys, Social Media etc.,Lego understand what is and what is not important to customer journey and design a ?€?wow?€? experience to improve it.
There are lots of roles that involve working with analytics and big data, and people doing these jobs give themselves many different names.,If you want to be employed working with data, searching the job sites will yield many possible roles ?€? data engineer, information architect, data analyst ?€? and the distinction is not always clear.,So here?€?s a selection of some of the most commonly used, as well as (in my experience) what they are likely to spend their time doing!,Probably the most common title, though the responsibilities, salary and requirements can vary significantly. This person could find themselves part of a large team or, with a smaller company, taking sole responsibility for generating data-driven insights.,Most vacancies require a degree in mathematics, statistics or computing as well as relevant experience (up to several years for more highly paid positions). Common skills that you will be asked for include SQL, R, SAS and Excel, and often Hadoop, particularly if the job is geared to big data (as they increasingly are).,According to??,, in the United States the salary for roles with this title ranges from $36,139 to $77,696, with the median at $51,224. Having skills in statistical analytics, data modeling and SAS in your portfolio will push you towards the top end of that pay scale.,Again, this could vary wildly depending on the organization, but a data scientist is more likely to work at the strategic side of a data-driven initiative, rather than the operational.,While, as the name implies, an analyst will spend a lot of their time combing the results of the analysis for insights, a data scientist is likely to spend time creating the algorithms that will produce the results. As the rather grandiose name implies, they are a scientist ?€? which essentially means they will come up with theories, and run experiments in order to test them.,People with the job title data scientist tend to attract a higher salary than those titled data analyst, reflecting that this is often a more senior position within an organization, or that more experience is often required for these roles.,As well as the analytics skills that an analyst will be expected to have, data scientists will often be expected to have programming skills such as Java or Python and knowledge of machine learning.,They can expect to earn salaries between $63,192 and $142,118 in the US, with a median salary of $96,579.,A data architect is usually specifically responsible for designing the way a company or organization?€?s information will be stored. Relational databases, data warehouses and distributed storage systems are the tools of the trade. They may also be responsible to some extent for verifying the data and complying with regulation. However as with all these roles there are massive differences between companies and many data architects will also be involved with analytical projects.,Successful applicants for jobs with this title usually have a degree in computer science and expertise in databases. Those with knowledge of OracleDB are likely to out-earn those with Microsoft SQL experience and the salary ranges from $65,928 to $157,868 in the US, with a median of $105,581.,The CIO is a senior business executive who takes overall responsibility for the data strategy. They are also often the senior member of staff overseeing all technical aspects of a company from basic IT infrastructure to big data analysis, though large firms may have a chief information officer and a chief technology officer. They will generally be in charge of a large team, with senior data scientists and analysts reporting directly to them, while they in turn report to the board of directors.,Their vision and expertise will play a key part in driving the business, and they will have been hand-picked for this role by the board. They will also be responsible for setting IT policies and ensuring compliancy with laws and regulations.,Salaries at US firms for people with this job title range from $81,226 to $269,033 with a median of $142,269.,There are many other titles you may come across ?€? I have seen vacancies for data engineers, insight directors, big data analysts, principle analysts, data expert, chief data officer ?€? but I believe they all (broadly) fall into one of the categories I covered above.,There is a lot of overlap between these roles, as individual companies will all have their own ideas about how to distribute their workload between their staff. And in particular the titles data analyst and data scientist are often used as catch-alls, but I expect the distinctions will become more defined as data strategies become more widely adopted by business.,Follow us on Twitter:??,??|??
The years leading up to 2015 affirmed and reaffirmed that we indeed have lots of data. And with lots of data all around us, organizations should really be thinking about how to leverage data to get ahead not only of competition but to get ahead of everyday transactions and to be able to predict and control what's coming down the pike. Almost every transaction in society now is ?€?datafied.' The automation and datafication of transactions and actions are giving us the opportunity to understand processes and things in measurable and predictable ways. But some companies are still behind in cultivating a culture of analytics within their organization so here's a simple framework to get rolling and essentially spark a cultural revolution within their organization.??,There are 4 key stages that must happen in order to cultivate a culture of analytics and these can be summarized as , The 4 C's of analytics can serve as a simple roadmap towards being able to advance your organization's analytics culture. For fans of mnemonics, '4Cs' is also in a fitting mnemonic format to help with adoption as well as to keep in mind the goal of being able to 'foresee' what's ahead. ??,The first C--,-- is usually the first stage for analytics. ??Most companies should be at least at the COLLECT level and some companies might not even know it but they could already be collecting data. Data collection is not easy for many companies but it is still possible for some companies to be lagging. With some of our automated and electronic systems, data collection sometimes just happens even without anybody knowing it. ??Transactions that are automated or in electronic format sometimes just have to be structured in order to make sense. ??Some data have to be actively collected such as employee surveys, competency data, etc. Hiring data, demographics, etc, which most companies should already be collecting anyway, are usually in format that lend themselves to be analyzable. But be careful, not all data being collected, however, are meaningful. Data for the sake of data can be junk so be careful in choosing and defining the metrics. And just because there's that 'big data' hype doesn't mean that it's all good and meaningful for you. ??Sometimes having too much data can cause analysis paralysis so before you collect data, you ??must first define what problem you need to solve. ??And don't forget to use unstructured data because these are data that you can collect and connect as well.,The next C--,--, comes once you have identified the data you are collecting. ??Data that you have collected can now be analyzed and 'connected' by doing some very basic statistics. ??Some basic descriptive statistics should be a good place to start. Do some basic head counts, slice it and dice that data in ways that are meaningful to you or your stakeholders. And then, go beyond the first slice of data to do some multivariate statistics. ??Keep it simple yet meaningful. Refine that question further. ??Data visualization is also another way of connecting. ??In fact, for many of your decision-makers, they would probably prefer to look at colors, bars, and circles rather than rows and columns. ??Connecting is not just all left brain activities (analytical). ??There's also something for the right (creative) side of the brain (creative) to do. ??And when you have these data charted and visualized, you are ready for the next level to CORRECT.,The , stage requires some action but at least at this stage, having collected and connected the data, the action should now be data-driven. With the data you have collected and connected, decision-makers can now be equipped with data and information and they would not have to rely solely on gut feelings. The CORRECT stage is also an excellent opportunity to look at how you can operate in leaner ways and eliminate waste. With data collected and connected, review the data where you can have reduction in waste or complete elimination. In this CORRECT stage, look for ways to streamline and become leaner and more efficient.,Once you have the first 3 stages in place of COLLECTING, CONNECTING, and CORRECTING, you should have a working system that is streamlined, collaborative, efficient, and effective. You now have a system where you can now be in , mode. It's almost like cruise control from here but it does not necessarily mean you stop driving because the level of effort to collect, connect, and correct have to be maintained and sustained. It gets easier but it does not mean taking your eyes off the road and your hands off the steering wheel. Once you achieve that desired speed of 65mph, you would still have to sustain that. You would still want to track the data you are collecting. Continue to make the connections and continue to be more proactive and predictive. Keep your eyes on the road, read the signs, and just be aware of what turns and bumps are coming up so that you can prepare. Always strive for continuous improvement. You don't necessarily need to increase your speed but you want to make sure you travel in the most efficient way. Continue to monitor your gauges (your data) and continue to evaluate your progress in order to keep control. For those who know Six Sigma, this is a very popular term. The purpose of Control (as in the ,) is to sustain the gains, track progress, and make continuous improvements. It also allows you to practice agile analytics where you can adjust and adapt based on the corrections you make and on the things you learn from your data. ??The last 2 stages are actually concepts that are popular in Lean Six Sigma practices. At the Correct and Control stages, it is about being lean and agile, optimizing your processes, minimizing defects and errors, and continuously improving.,Being able to Collect, Connect, Correct, and Control your operations data allows you to foresee the road ahead and do some descriptive, predictive, and prescriptive analytics. With the data you are Collecting, you get your descriptive analytics. Being able to Connect allows you to do the predictive analytics. Correct and Control allow you to be prescriptive. Some of these steps are not that simple but the 4 C's can serve as a simple guide to transforming your organizational culture to a culture of analytics.
It is the time of the year when businesses take stock of their performance in the year that went by and also see what the New Year may bring in. How did the Analytics & Insights business fare in 2014 and what are the learning that can help us forecast performance in 2015? In review I think 2014, in spite of some hiccups, has been a great year for the business. Definitely the industry experienced greater acceptance of analytics across verticals as a vital tool in helping organizations make sharper and well informed business decisions. In fact its sway over the market is so intense that even the term ?€?dashboards?€? is now being replaced by ?€?,.,Three key drivers - revenue, innovations / new trends as well as the enterprise preparedness to convert the opportunities - provide insightful assessment of the industry?€?s performance.This piece is pivoted around these three drivers.,In 2014, specifically in the banking and financial services sector, IT honchos dug into their deep pocket books to invest in new gizmos ?€? from analytical decision platforms to cool analytical BI tools integrated with underlying data to provide fast predictive insights. Quite a few vendors focused on this sector - mid-tier and boutique players in particular - have benefited from this largesse.,The year 2014 has also seen several innovations in this space. ??Several cloud based solutions have hit the market. Cloud based analytics service in itself was kosher with industry majors announcing their own products. Watson Analytics, Salesforce Wave, Oracle Cloud offering are key highlights in the market that I have discussed in more detail in another piece. ????ApplePay, CurrentC are other key innovations that come to mind that impact analytics in a big way by broadening the marketplace. As companies seek to offer personalized customer experience, these innovations will increase the thirst for deeper insights. 2014 also saw high profile acquisitions where the IT industry majors acquired analytics companies to broaden their reach and capabilities.,In 2014 many banks had initiated a comprehensive internal review of their analytics capabilities ?€? human assets, extant platforms and have authored all the findings into a roadmap for the future. It is interesting to note that the emphasis of many Fortune 100 banks appears to be on revamping analytical technology platforms. Road maps include using big data technologies, incorporating social data in customer acquisition/ collections and integrating real-time predictive analytics capabilities that can instantly provide personalized offers that will be the new norm in customer management. Further, freshly inked digital strategy roadmaps seek to go far beyond channel optimization and emphasize revenue generation. All this, together with a rebounding economy, portends to a busy 2015 for Analytics in the banking and financial services vertical.,But how are the IT majors prepared to meet the opportunity? As already mentioned mid-tier and boutique consultancies continue to have the advantage and are better poised to exploit the opportunities. But they have their share of problems in scaling and retaining top talent. ??It would be logical to expect swift growth and acquisitions in this space.,It is an entirely different story for the IT majors who continue to be plagued by several problems. High profile executive turn over, revenue slide, multiple flopped product/solution launches and a leadership that is completely at woods with analytics are key issues that continue to bedevil the majors. To add to their troubles, a series of strategic initiatives launched to push analytics revenue have been non-starters. While these are known devils, their resolution does not appear to be near. Further, there are also reports of huge layoffs, albeit in offshore centers, that point to acute revenue pressures that do not augur well for 2015 performance.,While there seem to be no dearth of market opportunities, the major players as well as mid-tiers have their own laundry list of fires to put out. Unless they get their acts right, 2015 revenue will be in jeopardy.?? Stay tuned as the industry rides through the turbulence in 2015.,??

Interesting article??,, Data and Products guy at LinkedIn. The author concludes that data scientists typically do the following:,My belief is that this type of conclusion applies to only one type of data science: what I call ,, that is tactical - as opposed to strategic - data science. The article is definitely worth reading and very interesting, it also features top skills ranked by importance: data mining, machine learning, R, Python, Data Analysis and so on. It reminds me vaguely about another article ,. I also produced a similar list a while back, ,. However, I believe that there is another dimension to data science, which is the decisional aspects. It is not captured by LinkedIn because data scientists rarely list these business skills in their LinkedIn profile.??,For instance, most of the skills that I use, as a data scientist, are different: domain expertise, business acumen, data intuition, use of vendor dashboards, finding the right data, making conclusions and applying results to my decision process to run a business. The systems that I develop (computational marketing, growth hacking) rely on a few principles: data-driven rather than model-driven, simplicity, robustness, scalability, efficiency, fast implementation. Some processes do not involve coding, but instead making tools communicate together, for instance,My article on ,??brings a different, fresh perspective to this.,Finally, everything you can learn from a textbook (R, Python, Machine Learning, and so on) is at risk of being outsourced or automated. That's what vendors are trying to do, and myself as well ,. So if your skill-set consists only of stuff available in textbooks, your career prospects don't look too good.??
maelstrom
Nowadays, the big buzzwords in business analytics are insights topped with actionable insights. These used and sometimes abused words have become a must have and must do in every large or small business that has an analytics organization. However, their meaning and use are as varied and widespread as the businesses that proudly promote them. As some business leaders got intoxicated with looking for and feel in dire need of business insights by any means, others would seek them at any cost, then dream up the associated actions and decisions to use in the hope of reaching the dream land of significant competitive advantage without the recourse to a sound "Insight-to-Action" strategy . This situations looks like a repeat of yesteryears management science and quantitative analysis rollercoaster that flourished during the years of plenty, then vanished in the ensuing droughts. It seems like the more things change, the more they stay the same, or do they? A wake up call and a light at the end of this windy path is badly needed, but has possibly arrived with Mr. Taymour Matin's "From Insights to Action Becoming Data-Driven, Not Driven by Data (Right Brain Analytics Book 1)" to the rescue.,In this stimulating and superbly written paper, Mr. Matin has cleared the way to developing the rightly valuable business insights, through the right mix of data analysis and business intuition, as well as the associated efficient actions that are most likely to improve business performance in order to maximize the chances of reaching the desired business goals and achieving a competitive advantage in the market place.,The proposed recipe for the best data and intuition-driven insights, prescribes putting in place the technological, strategical, professional and managerial processes aligned with the required structures in order to enable the identification of the most beneficial insights and actions that will lead to the best outcomes and the best ROI possible. These steps from Insights to Action are well explained and illustrated through easy to understand charts and business examples in this paper.,In order for insights to improve business performance, the author argues that an organization must first understand the nature of insights in order to distinguish between those that are actionable and those that are not. Armed with this understanding, the organization can then proceed to build data-driven processes that identify and make use of actionable insights that would impact fundamental company performance outcomes. Moreover, not all actionable insights are "created" equal" and should be evaluated on two important criteria.,First, Actionability vs. Traceability as depicted in figure 1 below, and second, on their Risk/Return score as illustrated in figure 2.,Actionability is necessary because an insight can be defined as using knowledge to affect an outcome. However, while actionability of an insight is necessary, it is not sufficient; If the awareness levels are not traceable to the primary objective (e.g. the impact of increasing awareness through promotion cannot been correlated with a corresponding impact on sales), the effort is built on blind faith as represented by quadrant (B) in Figure 1 above. Therefore, only "Star" insights should be considered and further evaluated on a second criteria of Risk/Return.,Second: Risk vs. Return: Even if an insight is a "Star" on the Actionability/Traceability chart, care should be taken to prioritize the pursuit of actionable insights according to their respective Risk/Return score as illustrated in figure 2 below.,This capacity of leveraging data-driven insights must also be created with the full realization of the importance of team work, group dynamics and collaborative relationships.,To complicate matters even further, the above data/analytics based methodology is often pitted against business intuition approach which are often viewed as competing opposites. However, rather than viewing analytics and intuition as diametrically opposed approaches, a way must be devised to have the two work together synergistically. In this sense, "analytics can be thought of as the barge, while intuition is the tugboat that nudges the enterprise in the right direction".,If correctly executed, the application of Mr. Matin's own insights on how and what the challenges are to go from relevant insights to competitively advantageous actions, would provide the ever sought after formula to sustainable analytics practices which will then enable analytics groups and businesses to prosper and endure in good times as well as bad times. Business leaders as well as analytics practitioners alike should then be able to operationalize a sustainable survival in this era of big data,/high tech intense competition.
 , , , , , , , , , , , , , , , 


For the past two years,Tableau Software has been publishing a monthly list they call the "Best of Tableau Web". Essentially, this list of nearly 500 publications represents the best articles, blog posts, techniques, etc. created by users of Tableau software. The selections of these sources have been made by Tableau employees and represents an unbiased assessment of the web-based activity related to Tableau Software.,All available sources have been compiled into a database and a quantitative analysis has been completed of the information. Emerging from the noise of this data are a list of the top 10 most consistently selected Tableau bloggers, which represents two companies and eight individuals. Due to the rapid growth and expansion of Tableau worldwide, there are also a number of emerging blogging superstars identified.


Any large global company would have a list of 10-15 companies that they deem as their??,. Let?€?s take the Investment Management industry. The usual suspects would include BlackRock, Vanguard, T.Rowe Price, PIMCO, Fidelity, JP Morgan, Goldman Sachs and a few more such global powerhouses.,Chances are that the competitive intelligence teams would spend their time analyzing the details of each company, from assessing the information on the quarterly earnings calls to analyzing the firms?€? financial performance, sector and fund performance, strategic initiatives, key leadership changes, new funds launched. Competitive insight that can shape and influence strategy. Only, does it provide a strategic insight or does it just report the past as it becomes history?,Does the combined set of competitive intelligence reports resemble this in length and size???,Many CEOs may not be looking for a copy of War and Peace along with their morning coffee. Leaving most competitive intelligence reports doomed to just decorate the piles of unread papers on busy executives?€? desks.,There is something that can show the landscape more clearly.,How does it help?,In the book called ?€?Lead with a Story?€?, the author narrates the story of CEO of Bristol Myers Squibb who had no time to read such lengthy competitive strategy reports. What worked was a powerful summarization of the insights and recommended action, slipped under his room in the guise of his favorite morning paper, ?€?Financial Times.?€? This approach of smart visualization did get his interest and his attention. Using a host of readily available??,??such as Tableau, Qlikview and the works, static Competitive Intelligence can change to dynamic and crisp data stories.,Analyzing the data on growth in assets under management and fund performance from Morningstar, SimFund, Pension and Investments might throw a surprise. A few firms employing strategies such as??,??asset categories such as alternatives or multi-asset funds are gaining assets as well as demonstrating strong performance. DoubleLine Capital, MFS Investment, Dimensional Fund Advisors ?€? names that may not have featured on the traditional must-research competitor list are suddenly in the news. Data analysis might make it easier to identify them before they become glaring headlines.,Google trend analysis and,??of key news items will show other companies; a slew of low-cost, online investment advisors who are using technology to drive portfolio construction. They may be dubbed as robo-advisors derogatorily but the recent fund infusions that companies such as Betterment, Learnvest, Wealthfront, Personal Capital have seen, indicate that this could, indeed, become a trend to watch more closely.,Institutional Investor spoke about the investment management industry under utilizing the power of web and??,. If the index picked stocks a decade ago and created the phenomenon of ETFs and technology is picking stocks now and creating the robo advisors, can the social media pick stocks in the future and create more companies such as Motif Investing? Motif Investing ?€? firm that has created a social investment platform using investment ideas from everyday trends. The ideas or motifs available include ?€?Caffeine Fix?€? (coffee stocks), "Modern Warfare?€? which focuses on companies that make smart bombs and drones and ?€?7 Deadly Sins" which includes fast-food companies, cigarette makers and gun manufacturers. Alpha powered hashtags indeed.,Seems completely unreasonable? Or can Analytics coupled with deep domain insights help competitive intelligence analysts to paint a new landscape? One without bars? One which involves no looking over the shoulder?,Would love to hear your suggestions for driving innovations in Competitive Intelligence.
The biggest threat to your job might come from an unexpected place. I believe that there is a hidden assassin lurking in the background waiting to finish you off in your job. Let?€?s face it; job security is high on everyone?€?s wish list right now, especially at times of economic hardship when it might not be so easy to quickly find another one.,So how secure is your job? Where is this hidden threat coming from and how can you put yourself into the best possible position to keep your job? I believe the biggest threat to your job, indeed most of our jobs, is coming from an unforeseen eliminator. I believe that our improved ability to capture and analyze data will allow us to automate most jobs. And I am not just talking about the manual and un-skilled jobs but any job, including the jobs of knowledge workers, doctors, journalists and even sports coaches.,I don?€?t blame you if, at this point, you might think ?€?what the heck are you talking about??€? So, let me give you some examples that should make it clearer, but be aware, they might send a few cold shivers down your spine.,I think you are getting the picture, right? Some of these examples might paint pictures of the future, while others are already here and are redefining our job market as you read this.,I can?€?t think of many jobs that we can?€?t automate using big data analytics, artificial intelligence and robots. So where does this leave us and our jobs? Will we all become programmers? No. Could we all simply not work and let the machines do our jobs? Unlikely. I find this all a little scary but at the same time trust in our human ability to adapt. We managed to adapt during the industrial revolution when we moved from farm work to industrial labor. We also adapted when we moved from the industrial era to the knowledge economy.,What's clear, however, is that there is a call to action. You need to ensure you advance your career in a way that positions you at the forefront of these developments and that you stay away from jobs that will be the first to go. Overall, I am excited to see how we adapt to the world of big data robots!,What do you think? Please let me know your thoughts. Are you scared or excited? How do you think our world will change with the emergence of big data robots?
Today?€?s case studies about data-driven marketing present stories of leaders and their successes. After all, history is written by the winners (or their vendors). However, many CMOs may be feeling like they?€?ve been sucked into the jet engine of this Big Data thing, and are struggling to create some order from a chaos they?€?re chasing.,One form this pursuit takes is a rush to hire world-class data science/marketing analytics talent. This is hard because the envelope?€?s edge of this still-forming discipline encompasses so many complementary skills: domain expertise, infrastructure management, data acquisition and transformation, statistical modeling, visualization, and perhaps most importantly synthesis and communication. Yet this complex expression of the recruiting challenge makes us blind to three more fundamental talents, which if missing or out of balance you should get after immediately.,For short, we can call these capabilities ?€?Experiencers,?€? ?€?Optimizers?€? and ?€?Builders:?€?,1. Think of , as artists. They think in terms of customer persona portraits and journey map/purchase funnel landscapes to describe their work.,2. Your team should also include ,. The quant traders of the marketing world, they think in terms of ?€?selling?€? expensive media, and ?€?buying?€? cheaper sources of demand.,3. Lastly, you need , who think first of the capabilities you need, technically and process-wise, for a successful analytic operation.,The very best data scientists have all of these talents, developed well past threshold levels. But even if your data scientists can beat up your competition?€?s, they alone will be unproductive unless the rest of your marketing team?€?s mid-to-senior leaders (as well as key partners) don?€?t also have at least a spike on one of these dimensions and an appreciation for the others. With this balance, you?€?re more likely to be aligned on opportunities to pursue, fueled with the right data, and flexible enough operationally to realize the ROI your analytic investment promises.
This post is in a series ,.,Here, we discuss IoT devices and the nature of IoT data,?? ?? Business insider , some bold predictions for IoT devices,?? ?? The Internet of Things will be the largest device market in the world. ??,?? ?? By 2019 it will be more than double the size of the smartphone, PC, tablet, connected car, and the wearable market ?? ?? combined.,?? ?? The IoT will result in $1.7 trillion in value added to the global economy in 2019.,?? ?? Device shipments will reach 6.7 billion in 2019 for a five-year CAGR of 61%.,?? ?? The enterprise sector will lead the IoT, accounting for 46% of device shipments this year, but that share will decline ?? ?? as the government and home sectors gain momentum.,?? ?? The main benefit of growth in the IoT will be increased efficiency and lower costs.,?? ?? The IoT promises increased efficiency within the home, city, and workplace by giving control to the user.,And others , internet things investment will run 140bn next five years,??,Also, the term , ?€? but it's important to remember that ,. M2M is a telecoms term which implies that there is a radio (cellular) at one or both ends of the communication. On the other hand, IOT means simply connecting to the Internet. When we are speaking of IoT(billions of devices) ?€? we are really referring to Smart objects. So, what makes an Object Smart?,Back in 2010, the then Chinese Premier ,?€?Internet + Internet of things = Wisdom of the earth?€?. Indeed the Internet of Things revolution promises to transform many domains .. As the term Internet of Things implies (IOT) ?€? IOT is about Smart objects,??,For an object (say a chair) to be ?€?smart?€? it must have three things,??,For example ?€? the chair may have a pressure sensor indicating that it is occupied,Now, if it is able to know who is sitting ?€? it could co-relate more data by connecting to the person?€?s profile,If it is in a cafe, whole new data sets can be co-related (about the venue, about who else is there etc),Thus, IOT is all about Data ..,How will billions of devices communicate? Primarily through the , and ,.,Certainly not through the cellular network (Hence the above distinction between M2M and IoT is important).,Cellular will play a role in connectivity and there will be many successful applications / connectivity models (ex ,??which primarily require a SIM card in the device).,A more likely scenario is , ,(which could be deployed by anyone including Telecom Operators). ??Sigfox currently uses the most popular European ISM band on 868MHz (as defined by ETSI and CEPT), along with 902MHz in the USA (as defined by the FCC), depending on specific regional regulations.,Also, when ,??are deployed (beyond 2020) - Cellular will provide wide area connectivity for IoT devices,In any case, Smart objects will generate a lot of Data .,.,In the ultimate vision of IoT, Things are identifiable, autonomous, and self-configurable. Objects ??communicate among themselves and interact with the environment. Objects can sense, actuate and predictively react to events,??,Billions of devices will create massive volume of streaming and geographically-dispersed data. This data will often need real-time responses.,There are primarily ,: periodic observations/monitoring or abnormal event reporting.,Periodic observations present demands due to their high volumes and storage overheads. Events on the other hand are one-off but need a rapid response.,In addition, if we consider video data(ex from surveillance cameras) as IoT Data, we have some additional characteristics.,Thus, our goal ,. This ultimately entails using IoT data to make better decisions.,I will be exploring these ideas in the , when it's launched.,Comments welcome. In the next part of this series, ,??
As a programmer stepping into the world of data science, I'm following some Massive Open Online Courses (MOOCs) on various provider websites. Here are some I've shortlisted:,In addition to these courses, you can find many more on these websites or even other MOOC providers.,Hope you enjoy.

What is a temporal database?,A??,??is a database with built-in support for handling data involving time.,This??,?? is from Wikipedia. It is simple and straightforward. Since we expect every database to have some kind of support for time operations, we could say that all databases are temporal databases based on this definition.,The reality is a lot more subtle and complex. Temporal databases enable you to see the data as it was seen in the past, while also enabling you to update even the past in the future. A temporal database will show you the actual value back then, as it was known back then, and the actual value back then, as it is known now. A temporal database allows you to know what your organization was forecasting for the future at a certain time in the past. Temporal databases support a multi-faceted view of time that more closely mirrors real life.,Why does anyone need a temporal database?,Most organizations manage data that changes over time. If you are in a regulated industry that does not allow you to physically delete data, you must maintain a temporal database that considers when a record is/was/will be valid (valid time), in addition to the dates on which the data was edited and the data itself (transaction time). If you are required to show what your data looked like on a certain date ?€? to identify what information your organization possessed at a certain time in response to an investigation, audit or legal inquiry ?€? you may not be able to do this without a temporal database. A very good description of this concept ?€? transaction time vs. valid time ?€? is available??,. ??A temporal database records facts about when the data was valid (valid time) as well as the data itself (transaction time), and provides structured queries to interact with both aspects via bitemporal tables involving both concepts of time. This concept is very useful when data changes over time.,An even more utilitarian example that every organization can relate to is the storage of customer addresses. For a lot of applications it is acceptable to store only your current address ?€? your previous addresses aren?€?t needed. However, there are many organizations that need to track historical data for customer service, financial, security and audit purposes.?? This is a classic case of requiring two concepts of time.?? When did you provide a certain address (transaction time) and over what period did you say that you lived there (valid time). Many companies (and even government agencies) have issues tracking your previous data after you change your address, your name, or make corrections to personal data. To avoid a true temporal implementation, many companies store the latest data in one table, and all other historical data is stored in different tables, or even in other systems. Although the information is there somewhere, they lack the tools to properly connect the data.,What if I don?€?t implement a temporal database?,????is a member of the Department of Computer Science at the University of Arizona and is a thought leader in the temporal database space. His book,??,??, provides a very good example of the consequences of a failure to implement a time-varying database. In June, 1997, Hudson Foods had to recall 25 million pounds of beef in an attempt to stem an outbreak of E. coli, at that time the largest recall of food in the United States. Back then, the product that was recalled was valued at $25 million ?€? around one fifth of Hudson?€?s annual production. Hudson knew that only a much smaller portion of the beef needed to be recalled based on a specific supplier. However, because their database only supported a current version of the data ?€? which beef came from which sources today ?€? and didn?€?t show a view of the data as it existed on the day the processing occurred several weeks prior ?€? the entire product set had to be recalled. In short, the absence of an appropriate time-varying database cost Hudson Foods 20% of their annual production.,The challenges of time in a regular database,With the development of SQL and its use in real-life applications, database users realized that when they added date columns to key fields, some issues arose. Adding temporal considerations to traditional databases created a lot of complexity. For example, if a table has a primary key and some attributes, adding a date to the primary key to track historical changes can lead to creation of more rows than intended. Deletes must also be handled differently when rows are tracked in this way.,Time has remained one of the neglected areas in the SQL standard. For decades the database vendors did not agree on a de facto standard for defining time-based tables and their corresponding operations and tools. Thus, every vendor implemented its own libraries, used its own semantics, and applied data type concepts that may not be mutually compatible.,In 1992, this issue was recognized but standard database theory was not yet up to resolving this issue, and neither was the then-newly formalized??,. In 1992, Richard Snodgrass presented a proposal to the ISO/IEC committee: an extension to SQL-92 for temporal databases. The committee developed these extensions, named TSQL2, in 1993. After many revisions, a definitive version was published in 1994.,There was an attempt to incorporate a subset of TSQL2 into the SQL:1999 standard, a subset named SQL/Temporal. TSQL2 was heavily criticized and was not incorporated. One idea proposed was the inclusion of an OVERLAPS predicate that would test if two time periods match or overlap at least in one point of time. For example, something like ?€?(DATE ?€?2014-01-01?€?, DATE ?€?2014-01-03?€?) OVERLAPS (DATE ?€?2014-01-02?€? DATE ?€?2014-01-04?€?)?€? would yield true. OVERLAPS never made it into the language.,A new Standard ?€? SQL:2011 ?€? Not Widely Supported Yet,?? was adopted in December of 2011, and it finally included clauses for the definition of temporal tables as part of the SQL/Foundation: valid time table, transactional time table, and bitemporal table. There were many new features included for temporal databases, which finally opened the way for standard support from the major database vendors.,The problem is that as of today ?€? August 2014 ?€? it is not yet widely adopted. Only a few vendors support SQL:2011, and only in their most recent versions. If you want to use a real temporal database, then you will have to use the latest version provided by your database vendor, or a database extension, or a special-purpose temporal database. However, if that is not possible, you can incorporate temporal considerations into your current database., But some do. Adding temporal considerations adds complexity in the database structure. There are many cases where it is OK to keep only the latest valid data.,However, when designing a database, it is always good to ask yourself the following question:??,??You don?€?t want to find out the answer when it is too late.,Our next article in this series will explain how you can make changes to the design of your existing tables in order to make your database (or a least a subset of your database) a temporal database.,Temporal Databases: Why you should care and how to get started (Part 2 of 3),Coming Soon!
This picture below represents a piece of paper where I wrote down my ideas for new articles, in the last few days, when making my 10 am tea pause. The tea kettle leaked on the sheet, slowly creating the brown stain. Many of my previous ideas were written with a purple pen, creating the purple stains.??,What is amazing is the very realistic fractal shape of the coastline, assuming this would be a fictitious map representing some island. The physical process that created these stains is identical to the simulated Markov Process ,??published in October, with the final frame shown below.,Enjoy!
This is the second article in a series. The first article is available??,.,How to implement a temporal database,Not every database requires a temporal database implementation, but some do. We can help you get started. As discussed in our previous article, the SQL-2011 standard included clauses for the definition of temporal tables as part of the SQL/Foundation. However, this standard is very new and not yet widely adopted. For now, most of you will need to extend your current database tables to incorporate temporal concepts.,In this article we'll focus on temporal tables. These tables are the building blocks of temporal databases.,Temporal Tables ?€? The Important Theories,Theory #1: Valid-Time State Tables, From Wikipedia: ?€?Valid time is the time period during which a fact is true with respect to the real world.?€? A valid time state table lets you manage data whose values change over time. For example, the interest rate on a loan can be 5% for the first year, and 6% for the second year. During the second year you still want to know that the rate was 5% in the previous year.,Theory #2: Transaction-Time State Tables, From Wikipedia: ?€?Transaction time is the time period during which a fact stored in the database is considered to be true.?€? When you successfully capture the sequence of states for a changing table, you have created a valid transaction-time state table. The tables by themselves now contain the required information to go back in time, or to ?€?rewind?€? to a certain moment and see the data that was valid at that moment.,Theory #3: Bitemporal Tables, From Wikipedia: ?€?Bitemporal data combines both Valid and Transaction Time.?€? Valid-time state tables and transaction-time state tables are orthogonal. You don't have to implement both at once; you can do one or the other, and even if you do both, you don't have to keep the information in a single table. For example, it is common to find designs where the transaction information is stored in different tables. In such cases, only the latest valid information is stored in one table while the other table contains the historical records.,If you decide to make a table both a valid-time and a transaction-time table, then you have created a bitemporal table. Using Snodgrass' words, ?€?a bitemporal table allows a glorious expressiveness when exploring and extracting information?€? from such tables.,How to Implement a Temporal Table,In the following three sections, we briefly describe what you need to do with your tables to implement a temporal database.,We will refer to the Hudson Foods beef recall mentioned in our first article. This is an example provided in Richard Snodgrass?€?s??,, illustrating a clear case of an organization suffering significant financial penalties as a result of not having a time-varying database. This example involves tracking cattle between different pens, and how that data changes over time. The information below is a super simplification of the example included in Chapter 2 of the book.,In a feed yard, cattle are grouped into ?€?lots?€?. Cattle from one lot can be divided into multiple pens. We define a table LOT_LOC that tracks how many cattle from each lot reside in each pen of each feed yard. Cattle from each lot are moved from pen to pen, thus the data is varying over time.,LOT_LOC(LOT_ID_NUM, PEN_ID, HD_CNT, FROM_DATE, TO_DATE),Let's focus on two columns: FROM_DATE and TO_DATE. These two columns render the table a ?€?valid time state table?€?: it records information valid at some time in the modeled reality, and it records states; that is, facts that are true over a period of time. The FROM_DATE and TO_DATE columns delimit the ?€?valid time?€? or ?€?period of validity?€? of the information in the row. A key concept in this design is considering your ?€?temporal granularity.?€? You should pick the right granularity for your problem. This could be days, hours, minutes, seconds, or whatever you need as a granularity level. For the previous table, the granularity level is a day.,Consider the following three rows:, (100, 3001, 32, 2014-06-01, 2014-06-02), (100, 3001, 30, 2014-06-02, 9999-12-31), (100, 3002, 02, 2014-06-02, 9999-12-31),We can tell the following from the data: from June 1st, 2014, to June 2nd, 2014, there were 32 cows from lot 100 in pen 3001; on June 2nd, 2014, the cattle from lot 100 were split in two different pens. Thirty cows remained in pen 3001, and two cows were moved to pen 3002. This is the latest valid data. The first row is now invalid for the current date. As you can see, we can continue to add and modify this data. It will be possible to query the data such that we see an actual view of the data as it existed at a certain time. We could write queries that would allow us to trace back all the pens containing cows from a certain lot prior to or on a particular date.,The Design Works. But It?€?s Complicated.,This design would have solved Hudson Foods data issues, and would have resulted in a more focused and reduced food recall. This design concept can be used to implement temporal concepts into any standard database table. There are, however, challenges to this design, which we will discuss in the next article.,??-> Temporal Databases: Why you should care and how to get started (Part 3 of 3), Coming Soon!,Authors:??,, Angela Holmes
This is the third article in a series.?? The first article is available ,. The second article is available ,.,Not every database requires a temporal database implementation, but some do. We can help you get started. As discussed in our previous two articles, the SQL-2011 standard included clauses for the definition of temporal tables as part of the SQL/Foundation. However, this standard is very new and not yet widely adopted. For now, most of you will need to extend your current database tables to incorporate temporal concepts. We have shown you that extending your current database tables is relatively easy.,In this article we?€?ll focus on the challenges in extending your current tables into temporal tables, and why implementing a true temporal database is easier., ,Extending your current database tables to address temporal requirements is complex, but it will work. You would be able to re-wind your data to any point in the past accurately and completely.,However, using true temporal database tools will make your life much more simple. Vendor provided tools will enable you to take advantage of the real power and value of a temporal database. Many vendors provide extensions to their database products that allow you to have access to features that you would expect from a temporal database. In other cases, the latest commercially available version of the database provided by your database vendor may already include temporal features, and in that case, you don?€?t need any add-ons.,Oracle, PostgreSQL, Teradata, and IBM are among the few major database vendors that provide temporal support in one way or another. For example, DB2 and Oracle already include temporal features as part of the standard product version, but you need to read the documentation carefully; they are not necessarily compatible or 100% compliant with SQL:2011 because of syntax differences.,At Sullexis, we have experience with Oracle Workspace Manager. This particular Oracle extension lets you create ?€?sessions.?€? These sessions represent data valid until a certain point of time, and they allow you to have many users working concurrently on the same data and making changes to their own copy, which later can be merged with other sessions or with the original tables. With Oracle Workspace Manager you can keep current, proposed, and historical versions of data in the same database. This is amazing for ?€?what if?€? scenarios and other analysis tasks. Companies and application vendors that need to store session data in relational tables also heavily use this feature. For more information about Oracle Workspace Manager, read this ,.,We have shown you the basic concepts and provided a design that would allow you to extend your database table to become temporal tables. The table design described in this article would have allowed Hudson Foods to ?€?rewind?€? and query the data as it was at a certain point in time. The lack of this feature cost $25 million. In other industries or other kinds of incidents, this inability to rewind the data could result in even steeper penalties or even bankruptcy. Consider whether your organization needs this functionality. If it does, you can extend your current tables, or you can implement a true temporal database.
Hi Folks,,I have a query around whether to learn R from scratch or should I leverage my basic python knowledge to extend into Data Science with scikit,numpy ,pandas? So I am bit confused ... I am not shy to learn New programming language like R etc bur really need to know who edges out whom in market. Maybe i should learn R too along with Python so ??your valuable opinion matters.,?? ?? ?? ?? ?? ?? Also i am playing around with IBM's MessageSight product for Internet of things so interested in REAL Time Streaming Analytics. May be this will help to whether i should choose Python or R?,?? ?? ?? ?? ?? ??So please enlighten me to see the unseen ....cheers and thanks for the help.,Here is my brief Profile:,I started my career in Microsoft .Net technologies for 11 years .From last 3 year currently working in Django,Python and IBM's ESB product i.e. Datapower and API managements. I have graduate course in Statistics (starting to refresh the basics). I have done lot of Web crawling in VB6 using DOM manipulation back in 2003 for AOL search engine's widgets.

ting may look like.,In writing news, what form does bias take? In the most narrow terms, and as you can see from the experiment mentioned later in this article, you can find bias in word-choice. Does the reporter use positive or negative terms or highly emotional words? These are all clues; they are clues that not only give us hints to how an author of an article may feel about the topic, but, also their organization who puts conscious or unconscious pressure on them, not to mention the societal norms the author lives by.,What kind of articles attract biased reporting? Even when handled by professional journalists (who try to take precaution against it), articles covering controversial topics are at the most risk of being biased. What is a controversial topic? They are usually topics involving social, political, or ideological issues, and, here is the big clue; they provoke arguments!
Keeping your eye on your competitors is a vital strategy for helping your business grow. By watching what they're doing and looking at their successes and failures, you'll be able to keep a leg up and a competitive edge. That being said, we're going to look a little more in-depth into why you need to be incorporating competitive research into your SEO and digital marketing strategy, some metrics you should be looking at, and actionable results that you can look at to know that this works?€?and you don't have to take our word for it.,Increasing the rankings of your keywords is a focal point of any ,, no matter the industry your business is in. There are a lot of different ways to increase your keyword rankings; it's a good idea to do link-building on-page optimization, and even spend some time working on your social media presence, but really there is no one-size-fits-all strategy when it comes to increasing keyword rankings.,Know your SERPs. All search engine results pages are different. By really understanding your SERPs at more than just a superficial level, you'll better know what different strategies to use to boost different keywords. Every keyword's competition is going to be a little different, so individual keywords will call for perhaps dissimilar methods. To be able to make informed decisions, you'll need to conduct in-depth competitive research. It may seem that it is a huge investment of time and effort up front, but the investment is worth it.??,When doing competitive research, there are a lot of different metrics to measure. It's a good idea to at least look at the following:,Just glancing at this list may be overwhelming, but the data you'll get from doing this research will be invaluable. And if looking at charts full of numbers isn't your thing, ,??like DataHero to get a more easily understandable?€?and more esthetically pleasing?€?look at your data. After researching and compiling all the data, you'll be able to analyze it and determine which factors are guiding your competitors to rank higher. You'll be able to find which metrics need to be improved upon, and you'll be able to plan a strategic attack.,Let's take a look at some of the hard and fast results, beginning with a company we'll call ?€?Business A.?€? This business decided to do some competitive research, specifically looking at internal linking. The business looked at a certain keyword and then looked at the top 5 competitors that showed up in the SERPs. What Business A found is that while they had only 3 internal links pointing to this keyword's page, the average among Business A's 5 top competitors was 20 internal links. Their top competitor had 32 internal links.??,By getting this insight, Business A was able to build up their internal linking strategy. Here's a look at how their organic traffic was going leading up to when they adjusted their strategy:,And here's a look at their organic traffic for a couple months after implementing their strategy:??,In just 60 days after adjusting their internal linking structure, Business A's organic traffic went up 74%.??,Next, we'll look at ?€?Business B.?€? This business decided to do competitive analysis looking into social media. At the time, their social media presence was non-existent. In doing competitive research, Business B found out a little about their top competitors in the SERPs and what their social presence was looking like.??,At this point, Business B entered into the social media game. Here's what their revenue looked like leading up to when they changed their social media strategy:,And here's a look at Business B's increase in organic revenue after implementing a social media strategy:??,After becoming ,, Business B's organic revenue went up 17% in 60 days.??,Finally, we'll look at ?€?Business C.?€? Business C did competitive research looking at external links. They found that they had links coming from 7 different domains with a ratio of domains to links of just 16%, meaning a majority of their links were coming from very few domains. By contrast, they found that their top competitors from the SERP had links coming from an average of 17 domains with a ratio of domains to links of 75%.??,Business C decided to start getting links from a variety of domains of varying authority, and here's a look at what happened in the couple weeks after getting 5 links from good domains:??,And then here's what happened in the 60 days after getting those 5 links from the right domains:,In just 60 days, organic revenue went up 207%, or $116,878.??,Competitive research works. Get to know your SERPs and dig down into looking at what your competition is doing. Invest some time, look at the right metrics, and you'll be able to make more informed decisions that will improve your rankings?€?and your results will show it.??
Given the right data being correctly collected, and analyzed using sound predictive models, what can be predicted, and what can't be predicted no matter what?,I believe that I have an answer to this question. All systems and processes that rely on some energy source can be predicted, and the other way around. Note that energy ,.,Energy brings structure to systems. Without energy, we are faced with chaos and randomness. Structured systems can be predicted. However, systems powered by high energy sources are currently difficult to predict. Likewise, systems powered by low energy sources are equally difficult to predict, if one uses predicting modeling and data capture techniques of this century. It will be easier in 50 years.,Let me illustrate my theory (at this stage, it's more a personal opinion than a theory) with a few examples:,: Weather is powered by high energy levels (getting higher if global warming is true), making it harder to predict locally or short-term. Global, long-term predictions might be easier if performed by data-savvy and domain expert data scientists. This is typical of high energy systems: local predictions are harder to make, compared with global predictions.,: Earthquakes are powered by high energy. Prediction is very difficult because, as of today, no one is able to collect the right data: history of stress and forces acting in faults. When this data becomes available, Earthquake prediction will become easier.,: Relatively easy to predict in real-time; the energy source is collective human brain power used to optimize individual car trips.,: Significant collective human brain power is used to beat the market, making it harder to predict (in real-time) than car traffic patterns. Such systems belong to a category of processes commonly known as arbitraging.,: Are these digits random? If yes, it would mean that there is no energy force behind it. They look extraordinary random, which means that they can't be predicted, or the energy behind it is so powerful that it's hard to predict. Some people will claim that there is extraordinary energy behind it - they call it God - and some other people claim that these digits??,, confirming that indeed, there's hidden energy behind it.,: Very stable and predictable structures (crystals) can be found at apparently very low levels of energy, in deep space. These atoms are actually subject to extreme nuclear forces and energy levels that bind protons and electrons together, as well as interactions with other atoms. This energy, when properly leveraged, can produce atomic bombs.
Here I compare these 5 rules published in 1999, with the new 2014 version. Data has changed so much that the opposite rules are now followed. Yet many statisticians and big businesses still stick to the outdated rules.,These rules were initially published in the featured book (see picture) first published in 1999, when software (e.g. SPSS) could not adapt to data, but data had to adapt to software.,This book, published in 1999, is priced at $72 on Amazon. The new version published in 2012 is $59. Maybe the 1999 version is considered an antiquity, and thus commands a higher price. But in my opinion, these prices don't reflect demand, and are not determined by the market, but rather by production costs. I bought the 1999 version for under $10, as a used book.,From the book, pages 15-18.,Failing to comply with these specifications would make software to crash or behave erratically.?? It was also a time when variable names could not be longer than 8 characters (SPSS requirement)??and were usually named VAR001, VAR002 and so on. Now, descriptive, precise names are preferred. Imagine a data set with one million variables!,Other than that, the book is still interesting. Back then, the two big statistical procedures were ANOVA and regression, according to the author (see page 197). Yet it contains a section with advice that still applies today (page 290):
From time to time I keep pondering on what could be the future and I am sure lot of us get this science fiction imagery where the future data analyst will be given just a pair of holographic gloves and perform three dimensional analysis.,Let us stop day dreaming and get to the basics., Dashboards have come a long way. These days lot of vendors are catering towards consuming big data etc. but I remember the days when the common target of BI vendors was "Excel". Looks like the focus has totally shifted from "," to ","., #1, In my opinion, , and because Excel solves other big problems in any organization it is going to co-exist for a long time. Look at how the open data sites have done it. They have democratized the availability of data through portals and you can download your data via csv files, XML or JSON dumps., A , whether it be Excel/CSV or any other text file into a central repository. If this can be achieved then "Excel" is no longer your enemy and it becomes just another vehicle for your data set., For e.g Earlier your marketing department exported your monthly google analytics data into Excel and did some independent powerpoint presentations. It is this powerpoint presentation that is being circulated around the company. This is a big problem as no one can go back to the source and validate if the data set was manipulated, what kind of calculations were done and so on., But in an altered scenario, if the person, simply uploaded the excel file into a common BI repository where you can continue your data visualization and then simply pass this URL around, now this is revolution. Not only you have untainted data set, but anybody can jump in and perform their analysis on the same dataset., #2, Visualizations will continue to improve and no longer be a differentiating factor among vendors. , #3, Dashboards will continue to be used for monitoring real time events as well as publish scorecards and performance metrics. , #4, Eliminate desktop based development and ,. Desktop based development provides lot of friction for the end user to jump into any kind of visualizations and quick analysis. Imagine if all business analyst were able to login into the browser and quickly connect to any data source and publish visualizations.,Can you see how a small change in deployment model can change the whole attitude of your business community? This way you are nurturing the use of data and the idea that ,.,The author of this article is affiliated with ,.??,. ??
Hello Data Science,,Thanks for allowing me the opportunity to be a part of Data Science Central!,Recently, I have embarked on a journey to become a Data Scientist! In doing so, I have begun to write an article about my findings to help those interested in becoming a Data Scientist as well, but don?€?t know where to start.,One thing I would love to include in my article is the backgrounds, opinions and teachings of real Data Scientists. In order to capture this, I have put together a quick ten question survey.,So please, if you are or know some Data Scientists that can help me along my journey, please take a few minutes or so to answer/share my survey to help others in becoming a Data Scientist!,All responses are??,??and I will only use the data holistically??when??writing my article,Participate in the??,.,Thanks!,Anthony P. Dutra
Twitter is one of the fastest and most effective ways we disseminate news across our world. If this world of news and information were run by trolls and pranksters, we would of given up on it long ago, (or, worse, we would have believed it and become a society like the one portrayed in Mike Judge?€?s film ?€?Idiocracy?€?). In short, Twitter and credibility must go hand in hand.??,??
The purpose of this article is to clarify a few misconceptions about data and statistical science.,I will start with a controversial statement: data science barely uses statistical science and techniques. The truth is actually more nuanced, as explained below.,But the new statistical science in question is not regarded as statistics, by many statisticians. I don't know how to call it, "new statistical science" is a misnomer, because it is not all that novel. And it is regarded by statisticians as dirty data processing, not elegant statistics.,It contains topics such as,I have sometimes used the word ,?? to describe these methods.,While I consider these topics to be statistical science (I contributed to many of them myself, and ,), most statisticians I talked to do not see it as statistical science. And calling this stuff statistics only creates confusion, especially for hiring managers.,Some people call it statistical learning. One of the precursors of this type of methods is Trevor Hastie who wrote one of the first data science books, called??,.,Including the following topics, which curiously enough, are not found in standard statistical textbooks:,These techniques can be summarized in one page, and time permitting, I will write that page and call it "statistics cheat sheet for data scientists". Interestingly, from a typical ,,??about 20 pages are relevant to data science, and these 20 pages can be compressed in 0.25 page. For instance, I believe that you can explain the concept of random variable and distribution (at least what you need to understand to practice data science) in about 4 lines, rather than 150 pages. The idea is to explain it in plain English with a few examples, and defining distribution as the expected (based on model) or ??limit of a frequency distribution (histogram).,Funny fact: some of these classic stats texbooks still feature tables of statistical distributions in an appendix. Who still use such tables for computations? Not a data scientist, for sure. Most programming languages offer libraries for these computations, and you can even code it yourself in a couple of lines of code. A book such as ,??can prove useful, as it provides code for many statistical functions; see also our ,, where I plan to add more modern implementations of statistical techniques, some even available as Excel formulas.,In particular, OLS (ordinary least squares) , Monte-Carlo techniques, mathematical optimization, the simplex algorithm, inventory and pricing management models.,These techniques are not considered statistical science, they are often referred to as , or ,.,Examples of old statistical techniques that I have never used recently in my data science career:,Now don't get me wrong, there are still plenty of people doing naive Bayes, linear or logistic regression, and it works on many simple data sets, and you'll get a job if you know these techniques, more easily than if you don't know them, because progress is slow. But the future is in uniting these techniques under a single methodology, simple, robust, with easy-to-interpret results, available as black box to non-experts, and easy to automate. This project (I'm working on it, some computer science people at Cambridge University also work on this) is sometimes referred to as the ,.,But just to give an example, naive Bayes (old stats, still widely used unfortunately) is terrible at detecting spam and categorizing email because it wrongly assumes that rules are independent, while a modern version called , (new stats) has been very successful (combined with pattern recognition) at identifying massive Botnets. Some modern techniques such as recommendation engines sometimes fail (unable to detect fake reviews) because they still rely on old, poor statistical techniques rather than modern data science. Though the fix to this issue is reworking the business model, rather than improving data science algorithms.,Finally, old statistics use a top-down approach, from model and theory to data, while new statistics or data science use a bottom-up approach, from data to model or algorithm.,Based on what many statisticians think statistical science is, and is not, I am tempted to say that modern data science barely uses statistical science. Instead, it mostly relies on statistical principles that are not considered statistical science by most people who call themselves statisticians, because of their rigid perception of what statistics is, and their inability to adapt to change.??,To the contrary, for non statisticians (computer scientists, engineers and so on), it is clear that data science has a strong statistical component. In my heart, I also believe that , statistics is also a core component of data science. Yet when talking to hiring managers, I tell them that statistics is another animal, because in their mind, statistics is , statistics. And , statistics is barely used anymore in modern data science. Likewise, when talking to statisticians, I tell them that data science is not statistics, to not upset them or waste my time in fruitless argumentation.
 

These predictions were published by the??,??(IIA). They produced a nice infographics,??featured below, and re-tweeted many times by various bloggers, using the hash tag ,. Other interesting predictions include ,, those by ,, as well as ,.,Here are IIA's predictions for 2015, in plain text:,and in visual format (for 2015):,Their predictions for 2014, issued a year ago, are depicted below:
On the face of it, John Kotter?€?s seminal book ?€?Our iceberg is melting?€? is a simple tale of a group of penguins who are scared about losing their home, their iceberg, and yes, even more scared of the changes that could entail. But through that simple story and their struggle for finding their new home, the story delivers a more powerful message that could be increasingly relevant for today?€?s companies as they search for their isolated Icebergs of Analytics that don?€?t melt.,Fred, an unusually observant, curious and creative penguin.??,??Well, Fred, using his powers, observed that their iceberg, their home, was melting. Not one to just wait for his daily quota of squid, he spoke to one person in the leadership council who he felt would listen to him.,Enter Alice, one of the leaders of the colony, a practical and tough bird.??,. Of course Alice initially wondered if Fred was suffering from a personal crisis or if he missed his morning fish meal. But she did give him a patient hearing which rapidly changed to alarm when she saw for herself the data - the cracks and the fissures in their iceberg.,Alice brought Fred?€?s concern to the leadership council and here entered the other cast of characters in the book.,Louis ?€? Formal colony leader, well-respected and yet conservative.??,NoNo- One of the colony leaders and negative influencer who kept trying to poke holes in Fred?€?s theory.??,??They are typically conspicuous by their alarming presence and doomsday predictions, all the time probably sipping their cup of coffee in the office watering hole.,Buddy ?€? Well liked, influencer, but not very powerful. T,The Professor ?€? Intellectual influencer.??,Scout and Sally Ann: Helping, excited, part of the change.??,Well, the unlikely team of penguins waddled their way to a miraculous solution in the book, enjoying quite a few squids on the way as well, showing that in order to drive change, you need a vision and a team that can drive that change.,Now let?€?s cut to our world. The CMOs' world where the Marketing Iceberg is melting today in a plethora of confused paradigms. Alice, the CMO, know she probably has an average tenure of less than 60 months in her role. Data is staring at her across channels from transaction information to marketing automation and digital marketing platforms. She has to balance the Art of Marketing; historically the branding and the positioning of the company with the emerging Science of Marketing driven by data to personalize customer insights and drive improved Marketing Effectiveness. And maybe, there is no Fred in sight. What can Alice do?,Let?€?s look at the 8 steps for change outlined in the book and see how they will apply for an organization trying to improve their Marketing Effectiveness Strategy,Let?€?s hope Alice finds Fred and the magical team of penguins that can drive data driven marketing??change and impact over a nice meal of tasty squids. Would love to hear from all of you who are seeing the melting icebergs in your companies.,Can we stop the marketing icebergs from melting and learn from the nomadic seagulls as our penguin friends did? Is it time to look for Fred and his team?
The field of Big Data requires more clarity and I am a big fan of simple explanations. This is why I have attempted to provide simple explanations for some of the most important technologies and terms you will come across if you?€?re looking at getting into big data.,However, if you are completely new to the topic then you might want to start here:,??...and then come back to this list later.,Here they some of the key terms:,A mathematical formula or statistical process run by software to perform an analysis of data. It usually consists of multiple calculations steps and can be used to automatically process data or solve problems.,A collection of cloud computing services offered by Amazon to help businesses carry out large scale computing operations (such as big data projects) without having to invest in their own server farms and data storage warehouses. Essentially, Storage space, processing power and software operations are rented rather than having to be bought and installed from scratch.,The process of collecting, processing and analyzing data to generate insights that inform fact-based decision-making. In many cases it involves software-based analysis using algorithms. For more, have a look at my post:??,Google?€?s proprietary data storage system, which it uses to host, among other things its Gmail, Google Earth and Youtube services. It is also made available for public use through the Google App Engine.,Using technology and analytics to identify people by one or more of their physical traits, such as face recognition, iris recognition, fingerprint recognition, etc. For more, see my post:??,A popular open source database management system managed by The Apache Software Foundation that has been designed to handle large volumes of data across distributed servers.,Cloud computing, or computing ?€?in the cloud?€?, simply means software or data running on remote servers, rather than locally. Data stored ?€?in the cloud?€? is typically accessible over the internet, wherever in the world the owner of that data might be. For more, check out my post:??,Data storage system designed to store large volumes of data across multiple storage devices (often cloud based commodity servers), to decrease the cost and complexity of storing large amounts of data.,Term used to describe an expert in extracting insights and value from data. It is usually someone that has skills in analytics, computer science, mathematics, statistics, creativity, data visualisation and communication as well as business and strategy.,The process of creating a game from something which would not usually be a game. In big data terms, gamification is often a powerful way of incentivizing data collection. For more on this read my post:??,Google?€?s own cloud computing platform, allowing companies to develop and host their own services within Google?€?s cloud servers. Unlike Amazon?€?s Web Services, it is free for small-scale projects.,High-performance Analytical Application ?€? a software/hardware in-memory platform from SAP, designed for high volume data transactions and analytics.,Apache Hadoop is one of the most widely used software frameworks in big data. It is a collection of programs which allow storage, retrieval and analysis of very large data sets using distributed hardware (allowing the data to be spread across many smaller storage devices rather than one very large one). For more, read my post:??,A term to describe the phenomenon that more and more everyday items will collect, analyse and transmit data to increase their usefulness, e.g. self-driving cars, self-stocking refrigerators. For more, read my post:??,Refers to the software procedure of breaking up an analysis into pieces that can be distributed across different computers in different locations. It first distributes the analysis (map) and then collects the results back into one report (reduce). Several companies including Google and Apache (as part of its Hadoop framework) provide MapReduce tools.,Software algorithms designed to allow computers to more accurately understand everyday human speech, allowing us to interact more naturally and efficiently with them.,Refers to database management systems that do not (or not only) use relational tables generally used in traditional database systems. It refers to data storage and retrieval systems that are designed for handling large volumes of data but without tabular categorisation (or schemas).,A process of using analytics to predict trends or future events from data.,A popular open source software environment used for analytics.,Radio Frequency Identification. RFID tags use Automatic Identification and Data Capture technology to allow information about their location, direction of travel or proximity to each other to be transmitted to computer systems, allowing real-world objects to be tracked online.,The growing tendency of software producers to provide their programs over the cloud ?€? meaning users pay for the time they spend using it (or the amount of data they access) rather than buying software outright.,Structured data is basically anything than can be put into a table and organized in such a way that it relates to other data in the same table. Unstructured data is everything that can?€?t ?€? email messages, social media posts and recorded human speech, for example.,I hope this was useful? As always, I would love to hear your views. Would you add any terms to this list, if so ?€? then please feel free to do so in the comments.,I really appreciate that you are reading my post. Here, at LinkedIn, I regularly write about management and technology issues and trends. If you would like to read my regular posts then please click ',' and send me a??,??invite. And, of course, feel free to also connect via??,,??,??and??,.
We all know that calculating error bounds on metrics derived from very large data sets has been problematic for a number of reasons. In more traditional statistics one can put a confidence interval or error bound on most metrics (e.g., mean), parameters (e.g., slope in a regression), or classifications (e.g., confusion matrix and the Kappa statistic).,For many machine learning applications, an error bound could be very important. ,??makes a great point, using an example of a??company developing a method of acquiring customers.,If this is an interesting topic, you can read the ,??on how-to use bag of little bootstraps methodology to compute error bounds on machine learning tasks??and access the whole project on ,.??

In my consulting work in the Enterprise IT space, I am seeing a definite trend of growing interest in Data Product/Advanced Analytics Design and Development which is becoming increasingly mainstream. Even as I view this a positive, it comes with its own set of perils and pitfalls that will need to be avoided. ??,Enterprise IT Application Development is often bureaucratic and involves multiple and redundant levels of management through the design, development and testing phases. Conflicting requirements from multiple stakeholders can make even the simplest of development efforts an intractable effort. For example, enforcing standardization is an underlying premise in most Enterprise IT Initiatives. In my experience standardization efforts undertaken without creating context and meaning around the standardization for the end users is self defeating in the long run.??While Agile practices are in vogue and show some promise, they are a long way from dealing with the fundamental issues involved when dealing with large teams.??Users want personalization and technology to solve their problems. This is particularly true in the area of Advanced Analytics and Data Products.??,The Enterprise IT organizations I refer to here are not software product companies like Microsoft, Google or Facebook. They are typically IT organizations within Service companies like Banks, Retailers, Health Insurance Providers, Utilities and so on.,In his book ,??, DJ Patil gives some simple guidelines which are very applicable ??to Advanced Analytics Design and Development within an Enterprise which I have listed below and added my own observations.,While many of the rules listed here may seem like common sense to adopt and follow, in my experience, more often than not this is not the case and is something all of us would be well served to keep reminding ourselves about.
The definition of 'best' depends on which school you follow. Data science and classic statistical science are at the opposite ends of the spectrum. So let's clarify what 'best solution' means in these two opposite contexts:,:,:,There are other differences. An interesting and recent discussion by Andrew Gelman (one of the most famous statisticians) generated some criticism against data science:,In the end, there is no such thing as a real data scientist or statistician. It's all about a personal feeling reflecting your career. Some statisticians are more data scientist than me. You can say the same thing about any profession. Some have tried to create laws about appellations, but that's the wrong approach. In the new language that I promote, called ,, anyone can call herself lawyer, doctor, married, data scientist, bank - you name it. Just do your due diligence before hiring or talking to someone who claims to have some credentials, whatever these credentials might be.,I will finish this article with the following statement, which epitomizes the data science approach, as well as the reluctance by a few change-adverse people to try it:
The top tech companies by market capitalization are IBM, HP , Oracle , Microsoft , Cisco , SAP , EMC , Apple , Amazon and Google,All of the top tech companies are selected based on their current market capitalization with the exception of Yahoo. The year 2014 is not included as part of this analysis.,??,Data: The source of this data is from the public financial records from ,??,All the sales figures are normalized and reported in USD billions unless noted otherwise.,??,To begin with let us see the current total employee count at each of the top tech companies,[click on the image to see enlarged]??,??,As evident, IBM and HP dominate the treemap area,??,In short, the employee strength equation as of 2013,IBM + HP + Oracle > Microsoft + Cisco + SAP + EMC + Apple + Amazon + Google + Yahoo,??,Putting this in perspective,[click on the image to see enlarged],??,Let us see how the employee growth has been across the tech companies,??[click on the image to see enlarged],??,Some key observations,??,What is interesting to observe is the big spikes in Amazon's employee strength growth!,??,Now let us see the performance metrics related to employees,The sales-per-employee ratio provides a broad indication of how expensive a company is to run.,[click on the image to see enlarged],??,Companies with higher sales-per-employee ratio are usually considered more efficient than those with lower figures. A higher sales-per-employee ratio indicates that the company can operate on low overhead costs, and therefore do more with less employees, which often translates into healthy profits.,??,Looks like the big spikes in , is not justified by the down trend of sales-per-employee ratio.,??,Does it mean that it has become more expensive for Amazon to keep hiring at this pace?,??,Finally, let us see the last performance metric related to employees,Income per employee measures company's ability to use their employee resources effectively to create profits for the company.????,[click on the image to see enlarged],??
The plot goes something like this ?€? Sandra Bullock plays a computer expert Angela Benett, her life changes when she is sent a program with a crazy glitch to ?€?de-bug?€?. Soon she finds out some vital government information on the disk, things gets nutty as fruitcake, her life becomes a nightmare with her records getting erased and she is given a new identity of some chick with a criminal record. That was 1995 and now it?€?s the end of 2013, imagine if something like this was possible back then, what can be done now.,Today, each one of us has so much personal digital data flowing out there that it is possible for someone to steal an entire online identity and cause real damage offline.,You already know that your personal information and references to your social media presence on Facebook, LinkedIn, Twitter, Instagram, Flickr etc. are all over google. This is the data that you know about and it is just a fraction of what can be unearthed with a little drilling, what is really scary is the data that you don?€?t know is constantly being collected, like your location data. Multiple apps collect location data and track your movements 24x7. Apart from your mobile phone if you use a smart card to pay road-toll or access public transport, you can be tracked by that as well. Some companies are taking this to the next level by using location data to confirm that employees not in the office are actually where they claim they are.,On the financial front, every time you swipe a credit or debit card you release more digital information. A marketer may analyse credit card purchases and deduce likely interests. Online retail giants like Amazon, Ebay uses such deduction algorithm when it offers hints like ?€?people who viewed this product also looked at the following products. Indian players like Myntra and Flipkart use similar analyses. Recommender systems can help people to find interesting things. Amazon?€?s recommendation system has helped the technology giant to reap billions in sales increase, NETFLIX is another such success story.,Put all these bits and pieces together with just a little online snooping and you could create a detailed composite of an individual?€?s identity. This may sound like a crime fiction but the basis for it is visible everywhere, if you know where to look. Maybe it?€?s high time you give a thought to the question ?€? ?€?how public is your private life.?€?,??,On the hind side there is something called ,, a technology which aims to make life easier. With wearable technology, learning more about yourself has not only become high tech but also real time. From devices and apps that help you track heart rate and food consumption details to gadget that monitor your mood and even surrounding air.,??????????????????????,I believe there are major four ways wearables can help improve our life:-,Today the biggest market in wearable technology is health and fitness. Big companies are putting wearables to work to figure out how to use these kinds of gadget to improve their business. They are giving wearables to employees and customers to gather subtle data about how they move and act and then use that information to help them do their jobs better or improve their buying experience. However there is a big risk involved. People will naturally resist real world intrusion into their privacy, so businesses needs to be very careful about asking employees and customers to strap gadgets on their head, chest, wrists etc. This compels me to think that we need to truly evaluate the real need of wearable technology. Much of what is being done with wearable devices is happening simply because it can be done. However, several users still are not sure about wearables and whether they want to walk around with devices strapped to them all day. Is this the paradox of wearables?,Having said all this, I see a silver lining, with wearables every individual becomes a data generator and transmitter. We generate data that is continuously collected by various government agencies and private companies. This data can be monetised and can also be used to make life easier for the people, what we need to make sure is that the data do not get manipulated or misused because no one would want to be in Angela Benett?€?s shoes.,Rohit Yadav
We believe 2015 will be a big year in data science. We?€?re hitting critical mass when it comes to the sheer volume of data that companies are amassing and the focus is finally going to shift from ?€?how much?€? data to ?€?how valuable or usable,?€? is all of that data.
Over the past year, as the head of analytics at a tech startup, I've had many conversations with analysts about what they want to learn from their data. Perhaps unsurprisingly, a lot of companies have similar questions?€?What drives retention? How do customers interact with products? How do we better understand sales pipelines? What's the lifetime value of a user?,These questions were familiar to us and we'd worked on many of them ourselves. To find answers in our own data, we wrote queries, built reports, and shared visualizations internally. We view the best of the bunch daily while others helped answer critical questions in the moment. Because of these reports, we know more about what our customers are doing, about what is working, and can build a better product and company.,But it soon occurred to us: if we've already done much of the work to answer questions other companies are asking, why should they start from scratch? Software developers rarely build tooltips, or databases, or web frameworks from the ground up; they start with what's been open-sourced and tweak, add on, and extend. Can analytics be the same? Can analysis be open source?,The ,??is the start of our effort to find out. Each Playbook report includes a SQL query and a visualization. They're built on top of an example users table and event stream?€?a data structure that's common in many companies. Because users can represent customers, accounts, or users, and events can be logins, purchases, clicks, screen views, or any combination of actions, each Playbook is designed to be flexible enough to fit many different businesses and products. If you have a SQL database with these two concepts, you can make a few simple changes to the reports we provide and have access to the same set of analytical tools we've built over the last year.,At the same time, we know data analysis can never truly be one-size-fits-all. Businesses and products are nuanced and the analytics tools that support them should be, too. Open source tools can provide starting frameworks, but the final analysis needs the adjustments and additions from domain experts?€?and in nearly every case, that's you, not us. For this reason, we expose every step our analysis, starting with the raw SQL query. This not only makes data manipulations and aggregations completely transparent, but also makes them infinitely customizable. In cases where we've added a custom chart, we also provide the HTML, CSS, and Javascript code that powers it, enabling the same level of flexibility for the visualizations.,Beyond providing queries and code, we hope a repository like this can provide ideas. Our conversations with other analysts have shown us new ways to approach old questions; we never would have come up with some of our methods without this inspiration. By open sourcing our code, we hope that we can also open source new ways of thinking about retention, growth, sales, and other areas of analysis.,But we know we didn't get everything right. Other methods could be more insightful and lead to clearer actions; our queries could be smarter, more efficient, and more robust; our visualizations could be made more engaging and dynamic; and god help our??,. Some folks have already helped out: analysts at??,??added additional metrics to our??,??that tracks how users move through your site, while??,??helped us cut 20 lines out of our??,.,We're very grateful for these contributions?€?and hope it's just the beginning. To make it easier for others to share their ideas, we've also added all the source code to??,. If you have comments, suggestions for improvements, requests for additional reports?€?or best of all, analysis that you too would like to open source?€?we'd love to hear from you, either on GitHub or at??,.,Of course, unlike open source software, analysis is usually dependent on proprietary data. But this shouldn't be a barrier to sharing methods, ideas, and best practices. We've built all of the reports on randomly generated users and events tables that mimic the structure of real data. For anyone interested in open sourcing work built on a different data structure, let us know and we'll be happy to help.,These reports are just our first step. We're already working on opening up other internal projects we've done on growth, A/B testing, SaaS finance metrics. As we grow, we'll undoubtedly have more questions to dig into, as will thousands of other companies. By sharing ideas, we hope we can turn these question into answers?€?and not just for us, but for anyone with a few tables and a bit of curiosity.,We're excited to see where it goes.
On Tuesday 12/16, I attended??Pivotal?€?s??,??webinar.,The webcast was ran by??leaders from the Pivotal Data Science ??team ?€? Annika Jimenez, Kaushik Das and Hulya Farinas ?€? who??shared their insights on the key Data Science industry trends for the coming year. The webcast came off as a bit??scripted, but one could tell that these three individuals??have a passion for Data Science discipline and it?€?s future.,In this post, I?€?d like to take a few??points from their discussion and share??some of my key takeaways??in regards to their predictions:,:??,- Come 2015, the team predicts that the shortage of deep analytical skills will result in an increased focus??on making machine learning available for practitioners besides Data Scientists.??That is a big idea. The tools utilized in Data Science today require a steep learning curve, and for someone like myself who doesn?€?t exactly have an advanced degree in mathematics, will tell you that sometimes learning these tools can be exhausting. Yes, it comes with the territory??and I enjoy being??challenged by something new, but utilizing less brainpower on ??machine learning portions??allows for more time to develop the data?€?s narrative.,??Throughout the discussion of point #4, the team had a cute little saying: ?€?with Big Data powers come with Big Data responsibility?€?, which is true for any aspect of analytics. The point here is that people now have access to information beyond our wildest dreams and with the right??training, we can do whatever we?€?d like with it. There are already stories today of Big Data being used against individuals, which is concerning. We??live in a connected, always sharing world (whether we are aware of??it or not). Luckily, a non-profit professional group the??,??published a Code of Conduct (link to that is at the end of the post) which many organizations are adopting as their policy. It?€?s comforting to know that the??ethics of this discipline has been nipped in the backside early, for the most part.,??,Here is a point??that I felt should have went hand in hand with point #4, but it seemed to be a completely different topic. I believe video and image data is much more dangerous than any text. People have pictures and videos out there that they aren?€?t too proud of and stating that enterprises will be exploiting these forms of data seems terrifying. I?€?m not suggesting that corporations??are shady entities??mining through images and video it?€?s employees to to slander them, they certainly aren?€?t. In my opinion, there is just too much power in a picture. If there are algorithms in the works today (and we all know there already are) which can mine through images and videos,??what is being done during development of these tools/algorithms to discourage misuse?,Overall,??Pivotal?€?s??,??webinar was fantastic. It left me questioning sides of Data Science that generally don?€?t come to mind as often as they should.,??,??
The word ?€?Force Multiplier,?€? in military usage, refers to an enabler or a combination of enablers, which make a given force more effective than that same force would be without it. ??,Based on our experience in??curating intelligent data products for the industrial world we feel a Data Product needs a healthy dose of??,??things,1.??,??= Seeing the??,( Surface actionable signals previously undetected to bake into a data product ),2.??,??= Making it??,( Making data product relevant to the daily context of tasks a person does to accomplish concrete business outcomes ),While there has been a lot of conversations around data science, we at Flutura felt that it was important to balance this with conversations around behavioral science which in our opinion is a force multiplier in driving adoption of data products.Having created ??intelligence platforms??for the industrial world, we have our own share of successes and failures. One key learning from our failed data products was the need to weave behavioral science into data products. Allow me to share 3 real life stories from the trenches which shaped our thought process.,Why is Behavioral Science important to Data Products ?,1. It??,??data.,It puts Big/Small data, Structured/Unstructured data, ??Low velocity /High Velocity , ??Machine learning, Map reduce, Collaborative filtering, Text mining, Graph theory,??Apriori??analysis and all the innovations which has enabled us to solve technical challenges ??and neatly ??bundles them it into a nice "box" which solves a real world business problem.,2. It aligns??,! Enhances "Consumer Resonance Index",??of data products think differently from the??,??of data products ! Its a simple fact but as with many things simple, often overlooked. ??Incorporating learning's??from behavioral science can heighten the resonance the data product has with real front line users which impacts a business outcome.??( CRI- Consumer??Resonance Index as we??at??Flutura call it )??,3. It makes the data??,!,Data fades into the background and jobs which need to get done comes into the foreground,It really should not "smell" of algorithms or data.??Behavioral science understands these subtle nuances and the neural pathways which need to get activated in order to drive repeat engagement with the data product and get daily jobs done !,1.??,??- How do we humanize a data product to heighten its engagement ?,2.??,??- Are we hooking the data product to daily "ritual" being performed by the data product consumer ?,3.??,??- Are we amplifying the visibility of the outcome achieved based on the action signals sent by the data product ?,More at,??
Here I describe a case study: a solution based on high-level data science. By high level, I mean data science not done by statisticians, but by decision makers accessing, digging into, and understanding summary (dashboard) data to quickly make a business decision with immediate financial impact. There is also a section on smart imputation techniques, with patentable, open intellectual property that we created after investigating this problem.,This article is articulated in three sections,I have discussed various breakdowns or categorizations for data science:,Here I introduce a new type of distinction: high-level versus low-level.,Most people think that data science is low-level data science only, but that's not the case. Note that low-level data science is to low-level programming what high-level data science is to high-level programming. The low-level layer is more technical and more complex: it's the layer on which the high-level rests. But the high-level layer requires different skills, including business acumen, leadership and domain expertise. ????,The problem studied here was solved by the decision maker (a data scientist), using , only, that is, highly summarized data. The data scientist in question monitors carefully selected compound web traffic metrics (KPIs, usually ratios) and noticed a spectacular drop for one of these metrics,Access to highly granular (low-level) data was not easy to get, and dashboard summaries, carefully selected and crafted, were sufficient to detect and address the issue with a one-week turnaround, doing a number of tests described in the next section.,More specifically, we used the Google Analytic dashboard. We did not access granular metrics such as IP address, detailed log-file transactions, or summary statistics broken down by user agent / referral combinations (not available from the dashboard). But we did use session duration, number of pages, and conversions, per day per referral, probing the summary data sometimes 2-3 times per day to check the results of a number of tests and fine-tuning, in short to check and quantify impact on performance. Performance here is measured as the number of real (not bogus) conversions per click, or conversion rate. We also looked at conversion rate, per paid keyword per day, available from the dashboard. The statistics per user agent per referral would have been very useful, but was not available. The user agent alone proved very useful ,. ??,The detection of the problem is straightforward if you monitor the right KPIs, as all of us data scientists should do: see Figure 1. It might even be made easier (earlier detection) if there is a system of automated email alerts in place, designed by high-level data scientists. It is always a good idea to have an email alert system in place, for core metrics impacting revenue. Note that in Figure 1, none of the variations between Day 1 and Day 2 are significant, except the one for AdWords - both sharp and statistically significant (because based on a large number of sessions).,It was initially believed that the performance drop (click conversion falling from above 20% to below 10%) was due to click fraud, though we did not exclude a reporting glitch initially. A reporting glitch was ruled out - as a contributing factor - when actual conversions, measured internally (rather than via Google Analytics), were also down. We also tested our own ads, simulating conversions, to see if they were properly tracked by Google. They were.,Anyway, we eliminated most major sources of click fraud: traffic from affiliates, and mobile traffic. We ended up with clicks from US-based IP addresses only, from Google.com only, and not from a mobile device. The pattern of poor performance continued nevertheless. We assumed it could be an advertiser trying to deplete budgets from its competitors, hitting all data science related keywords and advertisers (we've proved in the past, with a real test, that you can indeed eliminate all your competitors on Google AdWords). Such schemes take place when a domain becomes very popular, and the bidding more aggressive.,So we created another, smaller campaign - let's call it campaign B - with a slightly different mix of keywords, and especially using a redirect URL (,) rather than the actual destination URL ,, just in case the perpetrator was targeting us in particular. We noticed that when pausing the campaign for several hours, the performance came back to normal levels when resuming, but was again quickly falling down after a day or so. The use of an alternate campaign with a redirect URL did not help much. We tested using campaign A on odd days and campaign B on even days, with limited success.,We decided not to discuss the issue with Google (despite our high ad spend) as we thought - based on experience with other vendors - that it would be a waste of time. What was surprising is the fact that even with the new campaign B with fewer keywords, our daily budget was not depleted (we could not reach our daily ad spend target, we were well below), and the ads were still showing on Google searches as usual, for our target keywords. Only the conversions were missing. This is not consistent with click fraud, and to this day, we still don't know the cause. Maybe we now show up for keywords with poor performance: keywords for which our bid was previously too low, preventing (and protecting) us from winning the auction in the past. Maybe competitors abandoned these keywords last week, and now we "inherit" them. But I've only identified two such high volume, poor-performing keywords, and the issue is definitely broader than that.??,We reduced our ad spend on Google and boosted our very effective advertising on Twitter, as it is not subject to click fraud. Unlike Google, you can't predict when our ad is going to show up on Twitter, and it shows up only on certain highly relevant profiles - if you are not one of them you won't see it. It makes it much harder to generate fraudulent clicks.,We also diverted some of our Google ad spend to editorial initiatives: these two compete, and when ROI on Google AdWords drops too much, editorial initiatives win (to drive traffic to our network). We are also confident, based on our observations, that the problem will fix itself on Google AdWords, and that we will be able to resume to higher levels of advertising (with Google) after careful testing and permanent monitoring.??,Finally, we optimized our bidding to maximize total conversions (on Google AdWords), rather than total clicks. It is still too early to check if this strategy is working. Theoretically, it should automatically optimize our keyword mix, and abandon poor performers.??,This bias impacts user engagement statistics. Figure 2 below shows a 40% bias in the way Google Analytics (and many of their competitors) measure session duration, on simulated but very typical web traffic. In short, the last page of a visit is not accounted for in the Google Analytics reports. It would be fine if visits had dozens of page views on average. Unfortunately, one-page visits are by far the most common on many websites, because pages per visit ,. And in one-page visits, the first page is also the last page (not accounted for).,This bias is a bigger issue for digital publishers that own multiple channels, each one having its own domain name, rather than a sub-domain, and where visit paths typically span across multiple channels (e.g. from AnalyticBridge.com to DataScienceCentral.com to BigDataNews.com).,These zero-seconds, one-page visits (as well as 10-minutes, either two-pages or fifty-pages visits) scare digital publishers and advertisers, as it looks like artificial traffic. Correcting this Google Analytic error would be a win-win, both for Google, publishers and advertisers. And I have a proposal below on how to fix this. The challenge is to convince a company like Google to embrace statistical science techniques to make the change. These companies would rather use ,??numbers (but actually very wrong, as previously described), rather than approximations that are far more accurate - in this case resulting in a 40% error, see figure 2.,We focus here on measuring the duration of one-page sessions only (currently measured as zero), as this will solve the whole problem.??,The idea is to,The , step works as follows (see figure 2). If in a particular bucket, 2-page sessions last 1.60 minute (on average), then the 1.60 minute is actually spent on page 1. So instead of assigning a session duration of zero minute to one-page sessions, you now assign a duration of 1.60 minute. Likewise, the duration of a 2-page session is increased by 1.60 minute. However, this is a very rough and biased approximation: many one-page sessions are hard bounces or user errors (clicking the wrong link) that last just 1 second. So you need to look at the full distribution of duration - as a function of number of pages per session - to make a better extrapolation. The end result will likely be a value closer to 0.80 minute for one-page sessions, for the traffic bucket in question. ??,The , step (sometimes called multivariate binning) consists of identifying metrics (and combinations of 2-3 metrics) with ,, combine and bin them appropriately, to reduce intra-bucket ,??while keeping the buckets big enough. This is complex data science, and you can find details in my article??,. It certainly helps to have domain expertise, and in this case, a bucket of traffic can simply be defined as traffic occurring on a same day, from a same referral. Small referrals must be categorized (paid, organic, search, social, mobile, syndicated traffic etc.) and then aggregated by category, to have big enough buckets. And as in all inference processes, don't forget the very important cross-validation step.??,You can go one step further by defining a similarity metric between data buckets, and extrapolate using multiple, similar buckets, rather than just the target bucket alone. This will automatically give you ,??for the duration of a one-page visit, at the bucket level. You will likely need Hadoop or some Map-Reduce architecture for these computations, if you have more than a few million buckets: you will have to create a giant hash table of all buckets, to store, for each bucket ID, its list or 5-10 most similar bucket IDs.


 *
This week, I have the opportunity to represent Pivotal and team with other experts from ,, ,, and , to spend a week at ,. We will be applying data science to the science of phenology?€?the study of periodic plant and animal life cycle events and how these are influenced by seasonal and inter-annual variations in climate. Ultimately, the work will help scientists and researchers to better collect, store, manage, and monitor data, helping us all understand how and why our climate is changing and what the impact is on plants, animals, and humans., , , , , , ,Tomorrow, I'll have more updates from the field about our data collection activities, a look at the different data sources we'll be working with during our time here, and provide an assessment of the challenges., , , 
In the ,, we gave the background on our data science expedition to Acadia National Park, and now we are seeing its transformative potential.,As a representative from , and ,, our goal is to help a team of phenology scientists improve the way they use big data platforms as well as data science tools and techniques to improve their research and fast-forward our understanding of climate change. In this post, I wanted to share what we experienced in the field for Day 2?€?actually collecting data on bird migration and aquatic life in tidal pools, as well as thinking about how to automate and improve the quality of these data collection processes. I?€?m happy to report, in just 2 days, we?€?ve begun formulating ways to use a network of stationary cameras, image processing technology, data lakes, and mobile apps to help automate the process?€?ultimately helping scientists spend more time on science and less time on administrative tasks.,As we went through orientation sessions yesterday, we heard from ,, one of the principal investigators from Acadia National Park. He pointed out that scientists spend a lot of time capturing and cleaning up data. In fact, 50% of their time is spent on data cleansing. That is a lot of time in the field making observations and manually counting results that could be put to better use. It became clear that technology can play a stronger role here in the process and make phenology scientists more productive in a significant way.,Capturing data on bird migration was our first task for the day. After breakfast at the Schoodic Research Institute, we hiked for a mile to the southern tip of Winter Harbor and Schoodic Point, a picturesque location for observing bird migrations along the Atlantic. There, we set up binoculars and telescopes to spot birds and paired in teams of two.,Each team, one observer and one note taker, catalogued sightings of three different migratory bird species?€?the Common Eider, the Northern Gannet, and the Common Loon. After only a few minutes, the challenges in this approach became evident. Bone-chilling temperatures and whipping winds made it uncomfortable to keep our eyes focused on the horizon. Overcast skies, water reflections, fog, clouds, and distance made compounded the challenge for accurate observation. As a result, all 5 teams reported considerable variability in the number of sightings. Moreover, the reports were only small slices in time. In order to get a complete picture, teams would have to endure days or weeks out in the elements. Combining the 5 teams into a single unit and adding an expert to the team both helped raise the accuracy, but intensified the amount of manual labor for the task. In short, manual collection proved to be error prone, incomplete and uncomfortable.,After lunch, we took a first hand look at studying ,. Schoodic Institute field team leader and education project manager, Hannah Weber, led us on a mile-long hike to Acadia?€?s Diagon Alley?€?a tidal pool speckled with ,, ,, and a variety of seaweeds. Again, we worked in pairs to count the presence or absence of our target species. We used the , method with a quadrat to record our observations. While we found that it was easier to be accurate with this type of data collection, it still took a lot of people to brave the elements and a lot of time to make it happen.??,This experience of walking a day in the life of the research teams helped us to see a lot of ways in which technology, particularly a data lake storing large volumes of automatically captured images, could greatly help scientists and ecologists better record and study these types of data sets.,In both cases, it was clear that technology and automation could change the game by putting a greater volume of more accurate information in the hands of researchers more quickly.,In the bird watching environment, stationary cameras could take high resolution images every few seconds throughout the day and then be ingested into a data lake or big data environment. Through object recognition and image processing, the system could separate bird images and run a , , , , (CBIR) to match the detected objects against a database of images of migratory birds observed in the region. Furthermore, they could be queued for researchers to review and override misclassifications, thus concentrating their efforts on improving accuracy in the final stage of data collection. As a result, a much greater volume of data could be captured with far less effort and error. As well, an open data lake approach would open these raw data sets up to be easily used by other researchers, expanding the usefulness of the data.,For the tidepools, a tablet or smartphone app could be used to collect pictures of the quadrats in the tide pool. Again, an image processing program could automatically fill out a matrix of hits and misses for the different species of organisms. Once placed in a data lake, this time-stamped data, along with the GPS coordinates of the tide pool, would be available for researchers worldwide.,After one day in the field, it is clear there are many possibilities for: automating the data collection process; building larger, more complete and more accurate data sets; and improving the productivity of the researchers by allowing them to spend more time on science and less time on administrative work. Tomorrow, we?€?ll continue to look at the scientific research process here in Acadia by visiting , for our field trip. Later, we will return to the Schoodic Research Institute in the afternoon and continue our brainstorming on the climate data lake and how it can speed our effort to learn about climate change.??,You can read articles from my data science colleagues or find out more about what open source software and products we use at the ,.
Notice the big jump in median salary with better than college degrees.,This is a perfect example of how a single visualization can tell so many interesting facts.
 , , , , , ,??, , , , , , , , , , ,There are common challenges here. One, fluctuations can happen regardless of environmental variables?€?the species density alone affects survival and reproduction rates. In addition, observers can create manual errors as we discussed yesterday., , , , 
After getting oriented to the research problems of phenology, understanding data collection and storage, and discussing the statistical methods and approaches during the past few days of our expedition to ,, we dug into solutions and designs on day four.,Fundamentally, more complete and accurate data sets around bird migration, barnacle abundance, weather, duck population, and water resource data all help us understand the impact of climate change. Today?€?s effort was focused on the questions to seek answers to, the data sources to ingest, the models to build, and the visualizations to share with others, ultimately leading to a solution and approach.,As we dove into the selection for a pilot, we believed that each problem domain could be improved, but it was clear that a template approach could be applied across domains once the first area was developed. With this in mind, the team chose the measurement and prediction of climate change on hawk migration as the starting candidate project. From the viewpoint of Acadia and , scientists, the underlying business case supports the goals of the National Park Service, making the data consumable by scientists, educators, and citizen scientists.,We outlined an approach of using a web-portal to host interactive visualizations showing within-year and inter-year variability on hawk movement with a dependence on climate factors. A user could drill into a region of interest to then see or compare migrations from the past and predictions of the future given certain climate conditions. In addition, added the concept of decomposition reports to help identify the various climate levers on migration. Within the architecture, we believe open source visualization tools like , will operate within a web portal run on ,, and the data lake would be served up by the , running on the ,. This includes running the data on Hadoop with Pivotal?€?s SQL on Hadoop engine, HAWQ.,The initial, target data sources will include , and , for bird migration data, , (NCDC) or the , (BADC) for weather related data, and , for data related to plant and animal observations such as food for hawks. We had also , the use of field-based, stationary or mobile cameras, image processing, and object recognition techniques to help offload the burden of data collection, and these approaches could be applied to the architecture over time. With the initial, target data, there will be several operations required as data is moved into the system?€?include standardization, conversion to a common frequency, imputing missing values and more. Of course, these functions will need to be addressed during the extract, transform, and load (ETL) process. Once data is loaded into the data lake, the wider team would have a robust platform for joining relevant table data, generating features of interest, and preparing models and visualizations.??,Within this architecture, analytic development using , or libraries within the , and , ecosystems can operate at very high scale and high speed. For example, using these tools, we could quickly build a regression model to predict the time of arrival of a certain species at a given site in a given year, given climate factors such as temperature, precipitation, hours of daylight, wind speed vector, and more.,Ultimately, the team believed in building a community of citizen scientists who can participate and become advocates in combating climate change. Whether a larger group of scientists can contribute models and visualizing data or citizens help collect and properly attribute data as a crowd-sourced method, the , can benefit from enabling others to participate without being able to physically travel to the park. These types of programs connect with people who have a passion for the cause and assist scientists and ecologists with resource and budget constraints.??,As someone who loves data and national parks, I couldn?€?t have asked for a more interesting and rewarding experience in the field. With a Maine lobster dinner (or a salad for vegetarians like me) to end the week, we will be taking the formative plans and learning back to our respective organizations to identify the next steps for building out a climate data lake. We thank the teams at ,, the ,, and , for their hospitality and insight. Our generation must stay on top of the challenges of climate change, and I feel fortunate to have made a minor contribution.,You can read articles from my data science colleagues or find out more about what open source software and products we use at the ,.
SAS UK & Ire,land recently ran a competition to find the region's 'top data scientist'; the competition challenge was to , based on the data provided.?? Competition for this coveted award was fierce; with the winner claiming a trip to SAS Global Forum in the USA and the chance to feature their submission on the ,.,I recently caught up with Dr. Mohammad Abbas to discuss how he solved the challenge.,?? Could you tell us a bit about your background?,?? I hold a Master?€?s Degree in Organic-Geochemistry and a Ph.D. in Inorganic Chemistry.?? While working in the public sector as a chemical analyst in an animal health laboratory, I developed a strong interest in how statistical applications and experimental design are used in animal health. I pursued this interest by gaining a Diploma in Statistics from the Open University and I?€?ve since devoted considerable time experimenting with analytics using data sets drawn from various disciplines.,hy did you choose to enter this competition?, Well, I saw the Top Data Scientist Competition as an opportunity to test drive my skills in Big Data??Analytics.?? Tackling a large analytical project in a predefined time scope was a worthy challenge. It offered me the opportunity to constantly re-evaluate my skills and identify ways to achieve a result., The challenge was to forecast energy consumption in 2020, how did you go about tacking the problem?, Having spent some time examining the 47 or so datasets and doing some background reading on energy consumption, I was in a position to develop some approaches to tackling the problem. In essence, it consisted of three key phases: exploratory data analysis, identifying the key model parameters and then selecting a model., An interesting approach, could you tell me a bit more about each phase?, Generally, exploratory data analysis is by far the most important step in any analytical process and I started by investing a significant amount of time in understanding and visualising the data. It was through this step that I was able to build data blocks and make logical connections between data objects.,Next, I needed to identify the key model parameters.?? With energy data, there are a lot of variables which can be used at a later stage in the modelling process. The task at this stage was to be able to ask questions of the data?? and subdivide those answers into clearly defined groups. For example, what impact do economic factors have on energy consumption??? How should factors such as gross domestic product, housing, population and disposable income be taken into account,?? How was energy 'intensity' (that is energy consumption across the domestic, industrial, transport and services sectors) calculated and presented in the data sets? What was the relationship between energy consumption in primary equivalents and final energy consumption?, What do you mean by energy consumption in primary equivalents and final energy consumption?,?? By this I mean, the difference between the amount of energy generated and the final amount consumed.?? Some energy is lost in the production and transmission of power; burning coal to generate electicity looses some of the coal's energy in the process and further power is lost when that electricty is transmitted via pylons,????for example.,II needed to answer all of these questions and more to choose the best variables. Based upon these findings, I subdivided the key parameters into three distinct groups:,?? OK, so how did you go about selecting the best model?, SAS offers a wide array of modelling procedures; and choosing which model to use depends upon a clear understanding of the analytical problem and how much you know about the various statistical modelling methods available. Of course, you also need solid diagnostic skills.,To meet the challenge, it was essential to reduce the number of variables analysed to as few as were relevant; this is known in statistical parlance as ,.?? I also needed to take data quality into account and also standardisation was needed as some figures were expressed in thousands and others in millions. Also, some energy consumption data was expressed as tonnes of oil equivalents while others as Terawatt-hours so conversion of these units was needed.,?? How did you go about reducing the number of variables, the 'dimensionality' as it's called?, There are a number of ways to reduce dimensionality, one of which is a model that combines both dimensionality reduction techniques and regression models.?? You can use methods such as , and , which can be applied individually to reduce dimensionality, or combine them with a , model to obtain obtain a powerful unified approach known as a ',.?? Of course, SAS provides the ability to do all of this.,?? So which fundamental questions were you trying to answer?,?? I was trying to address two key questions,?? Firstly, how much variation within the , (those variables which explain the values of other variables, sometimes known as independent variables) could I explain.?? For example, atmospheric temperature could explain energy consumption, as it gets colder, more people put on their heating and hence use more power.?? Secondly, how much variability in the target variables could be explained by my choice of predictor variables.?? In other words, my target variables concerned energy consumption in 2020, so to what extent did the predictor variables I had chosen help to explain, and hence forecast, that?, So what results came out of this process?, My dimensionality reduction techniques reduced the large number of variables into a handful of factors.?? Then the partial least square model generated what are known as factor loadings, weights and scores, which helped me to explain how much each factor contributed to the final forecast and how accurate those forecasts would be.?? Also, examining the various models' outputs and their associated diagnostic plots helped me to shape the final prediction process.,Obviously, trying to predict a single value (energy consumption in 2020) has a large amount of uncertainty associated with it.?? So, I ran the model a number of times using different inputs.?? I tried broad economic factors, electricity consumption and energy intensity (consumption) for each specific economic sector and finally I used randomisation as a means of assessing my model's ability to differentiate between irrelevant (noise) variables and those with real predictive power.?? This allowed me to forecast electricity consumption for the UK in 2020 with a difference of approximately 80 TW-h (,) between the highest and the lowest predicted value., Amazing, so what did you find out?,?? I predict that the overall demand for electricity in the UK in 2020 will be 527 (+/- 30 TW-h).?? This represents an increase of 14.6% relative to 2013. Given the potential growth in population, housing and usage of electrical devices in the UK in the next few years, I think this is pretty accurate.,Finally, I would like to say, I am delighted to have been named as the first winner of this competition. From my experience, the most appealing about this competition was the challenge of taming a large volume of data and to be able to draw valuable insights and relate those findings to the real world we live in. This is what Big Data Analytics is all about.
Logi Analytics' recently published its second annual executive review of embedded analytics trends and tactics. It's called ",". In this report, they make an interesting claim:,But are ALL??software applications becoming analytic applications as Logi claims? ??The supporting claim that ?€?The median value of analytics as a percentage of overall product value is 35%?€? appears to support that directionally. Take note however of the buried context ,: , , The report overall is of some value??in that it is pointing to the need for more effective decision-making in business. Producing observations is no longer enough. The importance of applications is in their ability to help people to make better or more competitive??assessments and thereby act more effectively to achieve their objectives., , In the report they??coined a term ?€?,?€? which appears to be defined as??"embedded analytics within core workflows and application functionality". It is confusing. It plays with??an??erroneous intuition that action takes place inside some sort of software. Here is where Logi burrows??into the world of marketing rhetoric., , ,. The root of all action is in people and their language. This is a valuable perspective offered by ,. Action exists??in the language and narratives of??people. Therefore, in designing effective analytics applications, we need to understand people?€?s intentions, their roles, their capabilities and their social context as part of that context. In Logi?€?s figure on p.6 of their report all of the arrows need to be bi-directional and the user needs to be represented in a broader context. This is where leaders in this space need to take us., , So while serving information needs may be "about creating applications that users love?€? so that they use it ?€? this again is more marketing rhetoric. What to aim at first is the effectiveness of fulfilling people?€?s and businesses?€? intentions. Logi may or may not help you get there.
This exercise was done to understand the software skills that are in high demand for Data Science. Analysis was done by extracting the job postings from popular online websites. The findings are interesting. R continues to be the most popular skill, found in 70% of the postings. Python follows as a close second. Surprisingly, in spite all the talk about "Big Data Science", SQL comes up third. This shows that traditional RDBMS still continue to be the base for machine learning work today.,Details of the analysis can be found here :??,. Below are some highlights.??, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,.

o tap data science and consider using this rubric next time they want to go down that road.??
 
 , , , , , , ,??,??, , , , 
Companies build or rent grid machines when data length doesn't fit into HDFS, or the latency of parallel interconnects is too slow in the cloud. This review explores the overlap of the two paradigms at the ends of the parallel processing latency spectrum. The comparison is almost poetic and leads to many other comparisons in languages, interfaces, formats, and hardware, but there is amazingly little overlap., , Your Laptop Is A Supercomputer, , To put things in perspective, 60 years ago, "computer" was a job title. When The Wu Tang Clan dropped 36 Chambers, the bottom ranking machine in the TOP500 was a quad-core Cray. Armed with your current machine, you should be able to dip your toes into any project before diving in head first. Take a small slice of the data to get a glimpse of the obstacles ahead first. Start with 1/10th, 1/8, 1/4.. until your machine can't handle it anymore. Usually by that time, your project will encounter problems that can't be fixed simply by getting a bigger computer., , DIY HPC, , Depending on the kind of problem you are solving, building your own Beowulf cluster out of old commodity hardware might be the way to go. If you need a constant run of physics simulations or BLAST alignments, a load of wholesale off-lease laptops should get the job done for under 2000$.,Some Raspberry Pi enthusiasts have built a 32 node cluster, but it has limited use cases, given the limitations of the RPi's ARM., Password hashing and BitCoin farms use ASICs and FPGAs. In these cases, latency of interconnects is much less important than single-thread processing., , Move To The Cloud, , You don't need to go through the hassle of wiring and configuring a cluster for a short-term project. The hourly cost savings of running your own servers quickly diminish as you struggle through the details of MIS: DevOps, provisioning, deployment, hardware failure, etc. Small development shops and big enterprises like Netflix are happy to pay premiums for a managed solution. We have a staggering variety of SLAs available today as service providers compete to capture new markets., , Cloud Bursting, , When your cluster can't quite handle the demand of your process, rent a few servers from the cloud to handle the over-flow., , , Cloud Bridging, , Use your cluster to handle sensitive private data, and shift non-critical data to a public cloud., ,GPU Hosting, , Companies like EMC use graphics cards in cloud clusters to handle vector arithmetic. It works great for a specific sub-set of business solutions that use SVMs and other kernel methods., , Vectorization, , Vectorization is at the heart of optimization parallel processes. Understanding how your code uses low-level libraries will help you write faster code. De-vectorized R code is a well-known ,., , Julia: The convergence of Big Data and HPC, , "Julia makes it easy to connect to a bunch of machines?€?collocated or not, physical or virtual?€?and start doing distributed computing without any hassle. You can add and remove machines in the middle of jobs, and Julia knows how to serialize and deserialize your data without you having to tell it. References to data that lives on another machine are a first-class citizen in Julia like functions are first-class in functional languages. This is not the traditional HPC model for parallel computing but it isn?€?t Hadoop either. It?€?s somewhere in between. We believe that the traditional HPC and ?€?Big Data?€? worlds are converging, and we?€?re aiming Julia right at that convergence point." -,., , Julia is designed to handle the vectorization for you, making de-vectorized code run??, than vectorized code., , New Compile Targets via LLVM, , Scripting languages are built on top of low-level libraries like ,, so that under the hood, you are actually running FORTRAN., , Python can be efficient because libraries like NumPy have optimized how they use underlying libraries., LLVM is acting as the??, between the scripting languages and machine code., , Asm.js runs C/C++ code in the browser by??, into a subset of JavaScript with surprising efficiency.,OpenCL and Heterogenous Computing, , AMD has bet their future on the convergence of the CPU and GPU with their heterogeneous system architecture (HSA) and OpenCL. Most Data Scientists will never write such low-level code, but it is worth noting in this review.
Trying to deploy a??pervasive analytics strategy for your entire organization, or even a department, is not an easy task. That being said, changing the way people work has never been an easy task, imagine cloud computing 15 years ago. Until the average employee can physically see how this new technology can change their live for the better, there is always friction in adoption because of traditional habits. This friction is why organizations need to make sure they have a well thought out change management plan when rolling out analytic solutions. Besides choosing the right technology, a well thought out plan will help guarantee mass adoption and long term success.,After working with organizations across the globe, we have seen this transformation take place first hand. Looking across the successes we have uncovered 5 key change management tips that have helped facilitate efficient organizational shifts.,Leadership needs to make sure there is a clear vision in place that everyone can rally around. This vision needs to describe what the ideal end result will look like. This will not only help put meaning behind the team?€?s work for motivation, but also give employees the ability to be proactive and personally identify opportunities that will help the organization achieve this end goal.,What does the decision making process look like for rolling out analytic solutions? The process of deploying a pervasive analytics strategy cuts across multiple departments that usually have different internal structures. Understanding who has final sign off will ensure that the entire team is on the same page as analytic solutions continue to move from idea to production.,Deploying analytics across an enterprise takes a mix of skills. You need to make sure that you have both the technical and business expertise required in order to deploy the right end solution for the end user. If you are lacking one or the other, then it?€?s important that you find outside help or resources (consultants, trainings, etc.) in order to complement your teams existing skill set.,Visions on their own can often look overwhelming at the start. It is important that you break up the vision into phases so that you can track progress and show early returns. Planning these phases out will also allow you to find commonalities across analytic solutions so that you can reach economies of scale and tackle similar projects at the same time.,Executives on the team need to meet regularly in order to track progress and keep projects on track. As multiple projects might be kicked off simultaneously across multiple departments, it is important that everyone learns from each other so mistakes aren?€?t repeated progress continues to be made.,Use these as a starting point to help you build out your own internal change management processes as you begin deployment of your analytic strategy. But the list doesn?€?t have to stop here. What are your tips and tricks for navigating the corporate landscape and deploying analytic solutions? Drop them in the comment section below and share them with the world.
Authors: Dr. Vijay Srinivas Agneeswaran, Director and Head, Big Data Labs, Impetus ,Ghousia Parveen Taj, Lead Software Engineer, Impetus { ,},Sai Sagar, Software Engineer, Impetus {sai.sagar@impetus.co.in},Padma Chitturi, Software Engineer, Impetus, {padmapriya.chitturi@impetus.co.in}.,Deep learning is becoming an important AI paradigm for pattern recognition, image/video processing and fraud detection applications in finance. The computational complexity of a deep learning network dictates need for a distributed realization. Our intention is to parallelize the training phase of the network and consequently reduce training time. We have built the first prototype of our distributed deep learning network over Spark, which has emerged as a de-facto standard for realizing machine learning at scale.,Geoffrey Hinton presented the paradigm for fast learning in a deep belief network [Hinton 2006]. This paper, with the advent of GPUs and widespread availability of computing power, led to the breakthrough in this field. Consequently, every big software technology company is working on deep learning and every other startup is using it. A number of applications are being realized over it, including in various fields such as credit card fraud detection (see for example ,), multi-modal information processing etc. This excludes areas such as speech recognition and image processing, which have been already transformed by the application of deep learning [Deng 2013].,The team at Google lead by Jeffrey Dean came up with the first implementation of distributed deep learning [Dean 2012]. Architecturally, it was a pseudo-central realization, with a centralized parameter server being a single source of parameter values across the distributed system. Oxdata has recently released its H20 software which also comprises a deep learning network in addition to several other machine learning algorithms. They have also made the H20 software to work over Spark, as evident from this blog on ,. To the extent we have explored, only the , comes close to a fully distributed realization of a deep learning network .,Spark is the next generation Hadoop framework from the UC Berkeley and Databricks teams ?€? even the Hadoop vendors have started bundling and distributing Spark with Hadoop versions.?? Currently, there is no deep learning implementation either in MLLib, the machine learning library on top of Spark or outside of MLLib that we are aware of.,We have implemented a stacked Restricted Boltzman Machines, similar to this paper [Roux 2008]. The architecture of our deep learning network over Spark is given in the diagram below.,??To achieve our desires of a fully distributed deep learning implementation, we have relied on Hadoop's distributed file system and Spark's in memory computation for our parallel training.,The input dataset is stored as a HDFS file, and thus distributed across the cluster. Each node in the cluster runs a Akka Actor. The role of this ?€?Actor?€? is to share the training results on this node with every other node in the cluster. On receiving a request to train the network, the deep learning framework initializes the initial set of weight matrix, and the same is made available on every node's local file system. The training phase is nothing but a Spark application that loads the input file in HDFS to Spark RDD. Once training for a single RDD partition is complete, the results (weight matrix) is written to HDFS, and the local Actor publishes the update message to every other node on the cluster. On receiving the update message, Actors on other nodes copy the weight matrix and updates the local weight matrix accordingly. For subsequent, partitions the updated weight matrix is used. The training output will be the final weight matrix after training every dataset block.,We have eliminated the central parameter server from the deeplearning4j, which itself is a realization of Geoffrey Dean?€?s paper. We have built a publish-subscribe system which is implemented using the Akka framework over Spark ?€? this is responsible for distributing the learning across the different nodes. The weight matrix represents this learning and is shared at a location in the HDFS in our current implementation ?€? we may augment the Akka distributed queue to take a larger file in future.,This is the first attempt at realizing a distributed deep learning network directly over Spark, to our best knowledge. We shall be augmenting the first cut implementation with more work especially w.r.t achieving high accuracy of the deep learning network. We shall also be building a few applications including image search and NLP (to provide natural language interface to relational queries) to show case the power of our deep learning platform.,??,[Dean 2012]?? Dean, Jeffrey, et al.??,??Advances in Neural Information Processing Systems. 2012.,[Deng 2013] Li Deng and Long Yu, Deep Learning : Methods and Applications, Foundations and Trends in Signal Processing, vol 7, no. 3, pages 197-387, 2013.,[Hinton 2006] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for deep belief nets, Neural Computation, 18, pages 1527-1554.,[Roux 2008] Le Roux, Nicolas, and Yoshua Bengio. ?€?Representational power of restricted boltzmann machines and deep belief networks.?€???Neural Computation??20.6 (2008): 1631-1649.,??,??


I?€?ve been thinking a lot about data, where it comes from, and what it looks like.?? I can?€?t help it.?? I?€?ve been a data geek for almost 15 years.?? And I find data beautiful.?? Not necessarily in its raw form, mind you. Then it?€?s just messy and more often than not a pain to deal with, especially when it gets really, really big.?? But when smart, creative people start to clean it up and use it in different ways to find the hidden stories that make sense, it can help us learn things in ways that we never expected.?? And that can be exceptional thing.,Take for instance , work visualizing traffic flows in Singapore or identifying demographic trends of individuals who sign White House petitions.?? I heard Siegel speak at last week?€?s , event in Washington, DC.?? Clearly he?€?s a guy using both sides of his brain to tell very compelling, stunning stories with data.?? Or for that matter look at what , has been doing to turn mobile data into economic insight on usage behaviors.?? Perhaps it?€?s not as visually pretty as Siegel?€?s stuff, but there?€?s no doubt his novel collection and use of mobile data can help inform decision making in international markets for the better.,As wonderful as these examples are, they only hint at the potential of world data and what we can do with it.?? Like many of the examples popping up in the open market, they are the result of structured data that is either already formalized into tables of rows and columns in a database somewhere, manually created by legions of patient souls who plug numbers into a spreadsheet, or the output of well-structured metadata. Structured data is likely to be the backbone of and exponential growth in most data-driven storytelling, especially in the current context of the booming market in the Internet of Things. ??But there is significantly more to explore in unstructured data, especially as it relates to understanding the meaning behind words.?? Because words are exactly the form in which a large portion of the world?€?s data comes.?? ????,My colleagues and I recently completed a study on foreign data in which we compared the availability and detail of data in several key countries. During the course of the study we were pleasantly surprised to find an incredible richness of content in multiple data types.?? But the type and usage of data varied considerably by country and culture.?? For example in Nigeria, where fixed internet is notoriously unreliable and mobile usage grows exponentially, it makes sense to consider approaches that use mobile based applications to actively or passively acquire data for analysis of consumer behaviors. ??Likewise Indonesian citizens use cell phones considerably more than fixed internet services, which is dominated by Indonesian businesses. ??As the Indonesian cell phone market continues to grow, mobile-based data collection approaches could be very effective across consumer sectors. ??,But those same approaches will not be sufficient for appropriate analysis of Indonesian business markets where data is largely text based, whether in the form of articles and other unstructured reports.?? Nor would mobile based approaches likely work currently in Azerbaijan where internet penetration and usage is low, data available for analysis is almost exclusively in text form, and many Azerbaijanis remain suspicious of technology.?? Even in Nigeria, approaches that focus solely on structured data would miss the rich and free exchanges and resulting informative data that is available on local websites such as Nairaland. What this richness in text argues for is an integration of approaches that tailors insights to available data and appropriate techniques to collect and analyze structured and unstructured data within the local context.??,The fact that data sciences in the open market currently focuses on exploiting structured data is no big surprise.?? Structured data is much easier to ingest into tools and to figure out algorithms that will work when the content of rows and columns are well-defined numbers.?? ??And in contrast to other data science skill sets, there is a much larger population of talent in statistics and computational techniques to exploit structured data and find interesting patterns.?? Those that have the combined skill set of computational techniques as well as specific business or public sector experience needed to master and apply multiple data science techniques are a significantly rarer breed.??,What truly makes this new wave of big data exciting is the potential of bringing together existing computational talent with the disparate, relevant skill that encapsulate the world of expertise in areas that do not easily lend themselves to standardization in columns and rows.?? For example, how will we extract content, meaning, and context from the written or spoken word in the comments sections of surveys? Can we warn of impending events in different parts of the world by analyzing not just the number count of reports but the actual content of local conversations??? How does foreign language usage change meaning? Can we get better answers by asking better context laden questions?,These are just some of the areas of research into unstructured data that have been the subject of intense scrutiny in data science projects and labs for the last several years.?? And while some have made it to the open market ?€? techniques related to IBM Watson being perhaps the most well-known ?€? the research and potential is still largely untapped.?? Why? There are many reasons.?? Unstructured data is really messy. Different words mean different things in relation to each other even when we speak to each other.?? So imagine a computer trying to translate that into meaning.??,Despite current advances, dealing with text is a really hard problem.?? And it?€?s made even more so when you consider that almost all of the current machine learning, natural language processing, and other text related techniques for extracting and analyzing text data have almost solely been researched using the English language. Clearly while some techniques are ready for the open market, many others require additional and ongoing funding to explore fully.??,Bottom line: as a community of data scientists, we have just begun to exploit the multiple types of data that are available, let alone the methods we can use to capture and analyze global data.?? And the potential of data science does not necessarily mean we must choose one method in lieu of another. ??What it does mean is that we should be conscious that we are just at the beginning of understanding how we can tell stories with data in the global context.?? To that end, we should ensure that whatever data science solution is proposed, that it properly recognize the reality of how users interact with technology and produce data in different parts of the world and the specific needs and business processes that are driving the solution.?? We should also remain aware that not every approach will work everywhere, and that there is as much to be explored and accomplished with current approaches as there is with emerging ones.??
One of the biggest decisions that a data scientist need to make during a predictive modeling exercise is to choose the right classifier.There is no best classifier for all problems. The accuracy of the classifier varies based on the data set. Correlation between the predictor variables and the outcome is a key influencer. The choice need to be made based on experimentation. There are two main selection criteria here.,While accuracy of the algorithm is important, sometimes meeting an accuracy threshold is all that is required. The amount of false positives and false negatives generated is all a consideration. For some applications, false positives (like medicine) ??are not acceptable and in some, false negatives (like fraud detection) are not acceptable. When classifiers produce similar close accuracy ranges, statistical evaluation need to be made to see if the difference is significant., The speed of building models and predicting outcomes is of vital importance especially since a number of classifiers, especially ensemble classifiers take significant times to run. Model building and predictions might need to be made in real time or within set thresholds. In these cases, accuracy is compromised for performance.,An experiment to compare different classifiers can be found here??,Thoughts?,??
Source:??,.
As a long-term member of the ,,??which has evolved from W3C's Semantic Web, the latest developments around Data Science have become more and more attractive to me due to its complementary perspectives on similar challenges. Both disciplines work on questions like these:,When taking a closer look to the approaches taken by those two 'schools of advanced data management' one aspect becomes obvious: Both try to develop models in order to be able to 'codify and to calculate the data soup'.,While Linked Data technologies are built on top of knowledge models ('ontologies'), which try to describe first of all data in distributed environments like the web, are Data Science methods mainly based on statistical models. One could say: 'Causality and Reasoning over Distributed Data' meets 'Correlation and Machine Learning on Big Data'.,In contrast to this??supposed contradiction,??correlations and??,to overcome the problem with rigid data structures which can hardly adapt to the needs of dynamic knowledge graphs. Whenever relational databases cannot fulfill requirements about performance and simplicity, due to the complexity of database queries, graph databases can be used as an alternative.,Thus, both disciplines make use of these increasingly popular database technologies: While Linked Data can be stored and processed by standards-based??, stores like??,,??,,??, or??,, are the most popular graph databases for Data Scientists mainly based on the ,, for example:??,??or ,. Some vendors like??, support even both graph models.,Both graph models are similar and can ,, but they try to solve slightly different problems:,I can see at least two options where methods from Data Science will benefit from Linked Data technologies and vice versa:,We want to learn more about the opinion of various stakeholders working in different industry verticals about the status of Linked Data technologies. The main question is: Is Linked Data perceived as mature enough to be used on a large scale in enterprises? The results will contribute to the development of the Linked Data market by reporting how enterprises currently think.

In this article, I compare two approaches (with their advantages and drawbacks) to compute a simple metric: the number of unique visitors ("uniques") per year for a website. I use the word user or visitor interchangeably.,The problem seems straightforward at first glance, but it is not. It is a complex big data problem because the naive approach involves sorting hundreds of billions of observations - called transactions or page views here. It is also complicated because there's no 100% sure way to identify and track a user over long time periods: cookies and IP addresses / browser combinations both have drawbacks. It's much easier if the user is identified by an email address. Also, fake traffic needs to be detected and filtered out.,This metric is important for trending purposes and to assess the value of a company, such as Google or Facebook. A computed value that is 50% below the exact number can have terrible consequences for the company valuation and its stock price. In addition, if numbers reported by competing companies are wrong (inflated), it might be important to correctly guess the inflation rate, so that you can apply it to your company when reporting quarterly results to Wall Street analysts.,On the plus side, a rough estimate (with 10% error rate) is good enough - except if the small error results in a tiny decline 2014 versus 2013, when the reality is actually tiny growth.,Anyway, here are two mechanisms to compute the number of unique visitors: data science, and statistics.,I provide here two ways to solve the problem using data science. Let's assume that we have one billion transactions per day, corresponding to (say) 0.3 billion users per day. A transaction is defined here as one page view on the target website.,You just sort the 366 billion logfile transactions, by cookie or user ID. It is assumed that you have pre-sorted each day in the past. So this step consists of merging/de-duping the 365 pre-sorted data sets, say 2 at a time, to end up with 184 sorted files, each containing 2 days worth of unique users. Repeat this step to end up with 92 sorted data sets. Repeat one more time to end up with 46 sorted data sets, and so on until problem solved. This is done with Map-Reduce (e.g. Hadoop) where the yearly data is initially mapped onto 366 subsets (*Map* step), then the merge/aggregate corresponding to the *reduce* step. Note the merging and sorting two pre-sorted data sets is very easy and is O(n), not O( n log n) unlike traditional sort, where n is the number of transactions (how to do it? - this is a good job interview questions for data scientists, and the solution is easy to find on the web on stackexchange.com, or in ,). The drawback of this approach is that it is more complicated than it should be; it's just like killing a fly with a nuke.,Sample N = 1,000 users (less than one out of 10 million users) and extract all their transactions. Compute in your sample the proportion of users with one page view over the 366-day time period. Denote this proportion as p(1). Likewise compute p(2) - the proportion of users visiting two page views during the 366-day time period - and then p(3), p(4) and so on. Denote as Q the quantity Q = SUM{ n*p(n) } over all n = 1, 2, etc. Let V be the total number of pageviews (transactions) over the time period in question; V is very easy to compute because pageviews - unlike unique users - is an additive metric. The number of uniques over the 366-day time period is then U = V * N/Q. Note that f = N/Q is a very interesting quantity, telling you how many page views on average, a user visits in a given time period. It depends only on L, the duration of the time period in question, if traffic is flat. It should be plotted for various values of L.,This way of solving the problem requires only a one-time analysis that can be performed in a couple of hours despite the massive volume of data (petabytes), unlike the previous approach which was more database-intensive and involved summarizing data every day.??Comparing the results obtained with sampling, versus the previous (exact) solution is likely to show very little discrepancy, less than a 0.5% error.,: Sampling users must be done correctly, and it is different from sampling page views (the latter will result in biased statistics, as heavy users would be over-sampled). The easiest way is to have the user ID field stored as a sequence of successive integers (increased by one for each new user), and extract one out of every 10 or 20 million of these integers (representing users) ,??Then, count page views, for each of these users, for the time period in question.,: Using various values of L (L computed for 24 hours, one week, and four weeks) and extrapolating it to one year, provides yet another data science solution to this problem, using 12 times less data than the sampling approach, albeit less accurate.,This approach uses statistical modeling: survival models, churn, birth and death processes, Markov processes with states including "new user", "active user" and "dead user", as well as sampling, to better understand the mechanisms at play, and particularly, the trends. Some advanced statistical models might even include events (event modeling) that significantly impacted user growth, such as merger, change in the definition of user (to include international users) etc. I believe that this is too much modeling, and the statistician will spend many days to get results that won't be better than those obtained via the data science approach. It is worth the effort only if

These tips are provided by ,, who brings 20 years of varied data-intensive experience working with successful start-ups, small companies across various industries, and eBay, Visa, Microsoft, GE and Wells Fargo.,??
Thermometers and scales to measure weight appeared in retail outlets long ago. Blood pressure monitors perhaps came later. Pedometers and heart-rate monitors seem more recent - possibly closer to my time. I saw several devices while doing this blog intended to electronically record among other things hours of sleep; these are designed to be worn on the body all the time. A couple of weeks ago, I bought something to give the heart rate and blood oxygen saturation level. I consider it a real game-changer due to its ease of use. I just clip it to a finger. I can take readings a few times a day without attracting attention. So it is something "secret." I guess this is the real essence of the change. I control the device. The data belongs to me. I hide it. We live in a world where the average person will soon have access to large amounts of personal health data. This is not data from clinical trials or field studies involving many people. If a person diligently maintains records day after day, he or she would quickly amass more personal health data than any doctor. How can this be given that a doctor has many patients? My emphasis is on the word "personal." A doctor has all sorts of data but little about me specifically. His or her advice is premised on normalcy and uniformity - that I am rather like everybody else. This is how a doctor can give advice without knowing much or anything about my body "specifically." Prior to the development and widespread availability of personal health monitoring devices, a person had little choice but to accept a doctor's authority. In this blog, I don't intend to question medical professionals. But I want to discuss the emerging role of data derived from individual experiences.,Many people do share some physical and mental similarities. For instance, a significant number of people can get to places by walking, made possible of course by their common ability to walk. Buildings can be designed to satisfy their needs; however, those that are mobility impaired might experience difficulties if neglected from the process of budgeting and the allocation of resources. Mass-consumerism is premised on the notion that, despite their individual and possibly unique traits, there are somewhat homogenous segments of the market responsive to mass-products and mass-marketing. In a trip to the Philippines, I can still remember going to a tailor as a child and having clothes specially made to fit me. It seemed like such an interesting concept. Of course these days, most people buy things off the shelf. Businesses are optimized to serve the needs of mass populations. I believe that much the same can be said in relation to the medical industry. The data gathered from people seems to play a role in social engineering: the footprint of any individual must yield to the tracks left by the herd or the herded. Data contributes to the herding process: for instance, it helps to get people into and through the health care system. In particular, it gets the majority of people across by dealing with their most common ailments and health concerns. Data from individual experiences might receive less attention quite simply because it isn't feasible to give more - the same way it is costly and impractical for people to go to tailors to have clothes made. The emergence of personal health data creates potential conflict in relation to the existing power structure. To be an individual is to dispute the value of being part of the collective and those who profit from collective dynamics.,When we have systems designed to serve the majority, it can be annoying to belong to a marginalized or disenfranchised minority. However, in relation to health care, being disadvantaged statistically is particularly unsettling. Consider a scenario for instance where there is a distribution of residual genes from different types of humans in our current make-up. I know this must seem like a random interjection of sorts. I heard this argument on a Canadian radio program a few years ago. I'll share it with readers. Apparently, some people have more Neanderthal in them than others depending on their ethnic background. There is a genetic footprint from early humans to us today. The radio program mentioned other types of early humans that continue to persist in our genes. ??The commentator suggested that the interaction between these different backgrounds can lead to competitive advantages and disadvantages depending on circumstances. I think a way of committing genetic "genocide" so to speak and ensuring purity is to impose a bias towards a normative. For instance, a health care system could be optimized to ensure that the healthiest "white people" live comfortably. I have the term in quotes since there is no single type. I am just making an argument. A "black person" might register as a sickly white person. I fall below the normative on several fronts. I'm short. I have flat feet. My eyesight is terrible. I snore. I learned to cope with my peculiarities. But compared to other people, I am not necessarily the finest human specimen. I wonder if a science can develop around my data as an individual. In environmental studies, there is the concept of a monoculture where homogeneity can sometimes exist in species. When a disease affects one person, it affects everybody due to their uniformity. Social engineering promoting the normative can compromise the viability of the species.,I mention in a number of blogs that I regularly maintain data pertaining to my personal health. I use a research prototype called Tendril, a program that is running this moment as I write. One of the joys of using this prototype is the relative absence of normatives. I guess such a statement is easy to misinterpret. Of course, I want high levels of oxygen in my blood. I want my weight to be as low as possible. I want my breathing to be clear and effortless. I would describe these as contextual references for the proper "direction" of data but not any specific "level." For example, I don't have a preset weight target. I am not trying to live up to an externally defined standard. I don't know what my ideal weight should be given my personal situation. Perhaps it takes a tremendous amount of discipline and faith to allow an algorithm to determine ideal metrics such as weight. I am committed to developing a science around myself. It is a science of me. But I am not actually making medical decisions. Over the quotidian routines of life, a person might make choices that some would say reflect personal tastes and preferences. I do certain types of exercises. I eat particular kinds of foods. I am fond of certain activities. Perhaps many people know what makes their own bodies happy. I sometimes don't. I use a computer program to build a database. I actually don't know the reasons behind the lists generated by the prototype. But I take the items appearing on these lists seriously. I am particularly observant of everyday items that are flagged as potentially hazardous. I'm not saying that the items are hazardous in relation to the normative majority. I don't have anything to say about the majority. The data is about me and my experiences.,I think sometime in April of this year I wrote about how my life has become influenced by algorithms. I have become a product of mathematics. I present above some pictures of me over a 10-year period. The lower right image was taken just last month; it presents a version of me that interacts routinely with algorithms. There is a fair amount of technology supporting my day-to-day routines. I'm uncertain if the weight loss is apparent. I am about 30 pounds lighter compared to the preceding two snapshots. I don't credit my weight loss to algorithms mind you. I believe that my technology has helped me keep the weight off. Just this morning I ate a couple of chocolate bars. I had fried chicken this evening. I sit around behind a computer all day. So I need all the help I can get. I would say that my algorithmic contrivances have given me "true freedom." I manage the parameters of my computer programs. I let the software do the rest of the work. I spend time often wondering what the parameters should be. I think about what I would like to get out of myself psychologically, physically, and socially. "How would this person best fit in his current environment?" is the sort of question that I would ask myself. A person cannot physically evolve to adapt to the environment; but he or she can make adjustments to behaviour and decision-making.,I take a blood pressure reading every day usually late in the evening. So far, I have about 227 days of blood pressure data. I'll share some details on how the system operates to demonstrate my own management approach to personal health data. The worst reading occurred on June 24, 2014: 135 / 90. I think most people would consider this a high reading although not terribly so. The lowest reading occurred on May 13, 2014: 95 / 70. The distribution on the illustration below shows that my blood pressure fluctuates quite a bit from day to day. The numbers on the y-axis are not pressure readings but gradient values: 0 indicates the worst day; and the reading just about 100 is the best. Taken as a whole, there seems to be some improvement. In fact, the distribution appears to be lifting off the bottom the chart. For technical reasons that will be explained shortly, it is inappropriate for me to have as my ultimate objective "making the pressure as low as possible." Tendril is designed to let the pressure decline to an ideal point given the systemic impacts to other physiological metrics. I actually don't know what the "ideal blood pressure" for my body is, assuming it is static. My body will make that determination for me. I just have to be observant and sensitive to changes and outcomes. The science I build around my body is different than the science that would be applied to a large number of people. My placement on a statistical distribution is not in itself relevant. I don't have any external ideals. To the extent that it is possible to do so, given my level of personal autonomy, I try to influence the mathematics of my life.,My objective has never been to serve science per se but rather the needs of my body. So I don't deliberately experiment on myself to determine the validity of assertions the way medical researchers have sometimes taken chances or experimented on people. I make routine life choices. Should I have cheesecake or chocolate cake? Should I do sit-ups or crunches? Will I go to the library or to the grocery store? I take some food supplements although surprisingly few rather infrequently. I don't have a stance on supplements: e.g. "I make sure to take my supplements" or "I never take supplements." If something seems beneficial, I often take it again. If it seems hazardous and it cannot be reintroduced in non-hazardous terms, I generally avoid it. I would say that most of items I have purchased off-the-shelf have led to negative rather than positive outcomes. This is not to say that the items would be unbeneficial to others. But as I already pointed out - or it should be obvious by now - I keep exceptionally good records. I not only keep track of what I take, in some cases I actually note specific brands. I'm not afraid to say at some point, "It seems I wasted my money on this." However, money is never truly wasted. Determining that something is hazardous or unhelpful is valuable information.,I find myself wishing that I had come up with the technology maybe a decade earlier. Since I don't actually have significant knowledge of human physiology or drug treatments, I have a pure data science approach to personal health data. I can only learn from experience; by discovering things sometimes by accident; by making mistakes; and of course by guessing and deliberately taking risks. In some cases, a researcher might be able to analyze data without ever being close to its source. For instance, the data might be electronically conveyed from a remote location to a central research facility. I notice that at times, data scientists in particular seem quite concerned about the pace of disease; predicting the impacts of outbreaks; finding efficiencies in how people might be handling disease. The role of the data is different once disease becomes a lived experience. One starts to consider cures, remedial actions, and even lifestyle changes. The idea of publishing a paper or developing a drug treatment doesn't really enter the picture. So in a manner of speaking, the increasing availability of data to those most affected can help to restore the original purpose of medicine. It isn't to attract funding, bolster careers, or perpetuate industries. The original idea is to save lives. A life worth living is certainly a life worth saving.,I remember working as a summer student for the federal government. A rather frantic elderly gentleman came to my desk saying that he had to speak with somebody right away. I asked him how I could be of assistance. He said that he had a fantastic idea to remove contaminants from groundwater. He proposed freezing and excavating the water. In retrospect, now being aware of the high cost of excavation, the idea is problematic from a purely financial standpoint. Also, moving the problem doesn't actually resolve it; shipping contaminants is not without risk; plus there is the question of where to safely put it. But I thought to myself, I was in the presence of a highly motivated individual, practically evangelical in this convictions and beliefs. His ideas didn't exist only on paper. He lived his ideas; maybe he event dreamt about them. He was obsessed. I almost didn't have the heart to tell him that I was just a summer student. I didn't even know how to get to the bathroom let alone direct him to the most appropriate department. There is normally a separation that occurs when a person is detached from the underlying phenomena: it is a kind of data detachment or estrangement. One becomes focused on departmental objectives or what his or her profession might have to say. The terrain changes when the analyst is much closer to the phenomena - living within and enduring the data. I think we can expect more passion. Perhaps, there is a need for that fire to burn bright in people sometimes. So I felt myself privileged to meet somebody passionate about his ideas. People see different things in the numbers - see themselves playing different roles. It's like encountering a door to a different universe.,I mention the job for another reason. I was hired to set up a database to help the government with a program to clean up contaminated sites. I went through the process of survey development, distribution, database construction, and data collection. I trained a couple of people to handle the data-entry. Some might wonder what any of this might have to do with personal health. This is my main point, actually. I don't want it to seem like I'm preoccupied with medical professionals. Taken as a whole, these individuals are close to their markets and the people that they serve. But let's be open-minded and consider the clean-up of contaminated sites as a form of health care - because site contamination can lead to real-life health consequences. I was working on my degree in environmental studies. My personal focus was on public impacts. I seemed to me at the time that a $500 million program could have many positive "health" outcomes. I knew for instance that native communities sometimes came across issues relating to higher mercury levels in fish and unsafe drinking water supplies. One way to assess contaminated sites is in relation to their health consequences. I was naturally inclined towards this perspective. However, the department that I worked for was much more engineering-oriented. It is possible to allocate funds based primarily on containment and mitigation objectives - not necessarily taking into account local populations and health impacts. In other words, it is possible to examine a health care issue in purely engineering terms. As I have discovered more recently, health care can even be evaluated in completely budgetary and administrative terms.,If the data driving the allocation of funds were closer to those affected, the purpose of funding and its expected outcomes might be portrayed or interpreted differently. In my summer job, I was part of a big wheel mostly to help initiate the process of first-contact. My role was to compile a database of clean-up companies, their technologies, and capabilities. Those listed on the database represented the "target market" for the department. I don't want to over-generalize a complex situation; but I think that it is sometimes possible to do a lot, spend a lot, and in the end accomplish much less than expected. This can happen when there is an attempt to centralize problem-solving through the distribution of capital. The success of funding is inferred through the compliance of these disbursements to criteria. A program can be designed to deliver an institutional response. I would say this is true both from the standpoint of management and data collection. Bureaucratic agencies in particular - and I don't simply mean government - can carry out their affairs without ever considering the underlying impacts of their operations. They might not be judged by real or tangible success. I sometimes find myself among the first to criticize "neoliberal ideals" - such as those pertaining to performance measurements and competition. But I want to make it clear that my concern is more related to the methodology than intent. In fact, in a world of scarce resources, we should indeed be concerned about the effective use of capital. There are responsible and irresponsible ways of using capital.,I am therefore suggesting that effectiveness of health expenditure is achieved to some extent by reducing the distance between those portrayed in data and those collecting the data. Bringing personal health data closer to the average person is part of a much broader movement of liberalization decades in the making. I would argue that recent developments bringing data and people closer together represent aspects of our social evolution. Nobody is forced to remain compliant members of society. It makes a lot of sense to participate in the benefits. But those benefits are premised on institutional sensitivity to individual needs. As participation levels increase and there is a need to gain efficiencies, I believe that the risk of structural estrangement becomes more relevant; this can contribute to radicalization and loss of social commitment towards common goals and values. It might for instance become more difficult to fight domestic terrorism if people stop caring, or if they cannot meaningfully participate in data. However, even if one dismisses the risk of alienation, society must still confront the decline of public resources to support "diverse needs." As efficiencies increase, I suggest that the diversity of the needs served start to decline; and people become oppressed not so much by ideology but scarcity. The oppression starts by distancing the beneficiaries of society from the data intended to represent their interests.,In my personal health database, blood pressure is one of many concerns that I follow. This is not to say that my blood pressure has ever been much of a concern. I decided to be proactive in light of some family history. When data emerges from individual experiences, I get to decide which experiences to follow. Researchers might be interested in aspects of people related to specific areas of research and perhaps expertise. I don't diminish the importance of their concerns except that mine might differ. I sometimes wonder about the merits of certain areas of research; and I periodically question whether public funds are being used wisely. When the data is quite close to me, I am interested in all sorts of issues that never really get much public attention. Below, I provide a short listing of the main areas of improvement made possible through one's closer proximity to data.,I will share some of the more interesting health concerns that I follow on my research prototype: 1) enthusiasm and positive outlook; 2) inner peace; 3) ability to speak coherently; 4) sugar drain - e.g. felt shortly after eating a chocolate ??clair; 5) something called "luck advantage"; 6) a condition that I describe as funny elbow - no need to elaborate; 7) head aches; and 8) sensitivity to humming noises or some might call it tinnitus. Old electronic equipment such as transformers can actually wake me up if left on. Or I might detect high-pitched sounds in particular locations. I understand that exposure to certain work environments and medications can lead to hearing abnormalities. I don't actually try to rationalize the causes. I currently follow about 100 different health contexts. I am really interested in my quality of breathing perhaps because some family members had issues. I incorporate their experiences into the construction of my health contexts, which in turn power Tendril's algorithms. My point really is that I have control over what gets included. It is a control that would not exist if I left it to chance in the hands of the broader research community. I have data, technology, and more choices.,When Sean Connery appeared in the movie Medicine Man, he was trying to protect a rare plant species that he thought could protect people from cancer. Of course, many natural habitats are being destroyed rapidly in order to accommodate competing demands such as land to raise cattle and for cities. I believe that when data is placed in the hands people - their own data in their own hands - some percentage will become creative in ways that cannot be anticipated. Fostering a movement to bring data closer to people is about tapping into power and imagination. The more inclusive we are as a society, the greater the potential benefits. But this isn't just a search for solutions. We create roads to help solutions rise up from the deeps of society. I remember Scrooge dealing with the ghosts of Christmas past, present, and future. I think that in many respects, there are always intercessors. Societies and organizations are given their fair share of creative people. But if left in a setting where they cannot contribute their talents, we sometimes miss opportunities that can never be recovered. ??If deprived to tools and data, event their talents lose relevance.,Apart from controlling health contexts, I control what events from my day-to-day life get included as data. I don't regularly take any medication. I keep track of things like espresso, cookies, eggs, lotions, soaps, chocolate, chicken, and nuts. I note my exercises. Perhaps, more important than what I take and do are the things that I don't take and do. Because my life generates the data, I can be specific and spontaneous the way no outside researcher can be. The data can be whatever comes to mind or seems worth recording at the time. The ease at which I include all sorts of events is related to the technology that I use. However, the technology behaves the way it does because of my personal needs. My individual requirements have given shape to the database and its environment. The system might be completely different in the hands of a researcher motivated by departmental and professional priorities.,For me, the term "systemic" is a specific algorithmic concept that can be incorporated into a computer program. As I mentioned earlier, I follow about 100 different health contexts. Sometimes, attempting to promote one health objective can detract from another. I can give an example just off the top of my head although it's not one that has affected me: a person might choose to lose a lot of weight in a short period of time without considering the immediate adverse impacts. Drastic weight loss can cause loss of energy and maybe even disorientation. On the other hand, he or she can focus on events that result in broad systemic improvement. I believe that access to systemic benefits increases when one is close to the data; this is due to the ability to rapidly detect and connect the impact of events to different health contexts. I call this the "Push Principle," and it is an important aspect of the prototype that I use. Indeed, when I decide to avoid something, it probably had a negative impact on me in a number of different ways. The things that I decide do often have many positive benefits. There is a kind of liberation that emerges from these dynamics - knowing that the technology is working on a systemic level - for it reduces the likelihood of unintentionally or accidentally causing harm.,I remember in introductory environmental toxicity being introduced to a measurement called the LD50 (the lethal dose that kills half the population in a sample). The rationale pertains to statistical distributions. Alcohol for instance is quite toxic - according my toxicity professor - in light of how the body responds to it. A certain segment of the population would experience adverse impacts right away even if just a small amount of alcohol is consumed. Some people are able to drink large amounts without feeling or getting ill. Then there is a spectrum of people between the extremes. Well, the same principle can be applied in relation to all sorts of substances and perhaps even life events. For instance, for some kids, certain interactions might not seem like bullying at all; but for others even the slightest conflict might qualify as bullying. In relation to health treatments, attempts are likewise made to arrive at generalizations. We get all sorts of debate for instance surrounding the question of whether or not coffee is good for "people." The idea of coffee being terrible for some people and great for others doesn't seem to fit the discourse. The individualization of health seems unprofessional perhaps because it is difficult for a profession to express an opinion in relation to particular individuals. Here then we encounter something of a structural limitation on the value of an aggregate perspective. Reducing the distance between people and their representation in data makes the data more relevant to the lives of individuals.,There is no shortage of talk on ways to make money from data. I believe that some ideas presume that similarities exist between large numbers of potential clients: i.e. there are untapped mass markets. In such a scenario, a major player in the data industry might attempt to take an entire market. Let's consider the possibility that clients might not be particularly similar. Further, to the extent that they are uniform, their common needs might already be served by existing products and services; this leaves only those opportunities related to areas of individualization. I believe that opportunities do most certainly exist, but development will emerge in response to the data. Normally a database gets constructed after a mass-market product is acquired: the data-gathering infrastructure along with the actual data collected must conform to the specifications and capabilities of the product. But I believe that in the future, some practitioners in the area of health might not be medical doctors at all but rather data specialists focused on the individual circumstances of clients. This is not to say that such specialists will give medical advice. They might help clients engage complex situations that must of course incorporate the advice of medical professionals.,I have faith in doctors in relation to certain types of problems. I believe however that lifelong health decisions generally fall outside their scope; and if these things were within their scope, there might be some reluctance among sponsors and insurers and pay for the costs. For example, should a person ride a bicycle? It is healthy to do, a doctor might say almost instinctively. That's not really true in a busy city. Riding a bicycle is quite dangerous if we consider pollution, cars, and poor road conditions. So will anybody pay a doctor to give advice about riding a bicycle? How about spending time on a computer; commuting to work; working as an accountant; wearing loose clothing for the winter? Long before situations in the lives of people become clinically relevant, there might be all sorts of important day-to-day life decisions. It is just extremely inconvenient and outside the scope of the medical profession to cover the quotidian aspects of life. I would say that at this time, nearly all of the data resources are mass public resources (such as the internet) and therefore somewhat ineffective to deal with highly individual problems. The move to bring data closer to the people sets the stage for more intervention opportunities.,Around the world, there has been more collection of personal information by government agencies. Normally when people spend a lot of time watching reality television shows, we say that they might be wasting their time. We have been reducing the distance between governments and the data of its people; but the distance is increasing between people and their data. When 911 occurred, the lasting impact was on US intelligence. It was a successful attack on the support structures for metrics and detection. So what does this have to do with personal health data? Well, personal health data cannot fight terrorists or indeed many other things; but it like anything else can be configured to give governments an apparent mandate. That mandate starts to lose potency when the data becomes more closely connected to the individual; for there would be fewer aggregate details to extract. My rather abstract argument here is that when the distance between data and a regulator declines (the projection of proxy), the distance between the data and those being regulated increases (the articulation of phenomena), until the data itself become estranged and irrelevant. This is a principle relating to social disablement, which I believe is one of the most important issues pertaining to the use of massive data today. It is a complex ontological concern that I briefly touched in my introduction of the Universal Data Model (the "exclamation model") shown below. I will elaborate on this model in the future.,In this blog, I juxtaposed between the alienated administrative use of data affecting public resources and the personal lived experiences of people. I said that health can be regarded purely from the perspective of departmental or professional needs, which can limit or place constraints on the meaning of the data and its relevance to people. I was once asked to come up with different ideas to quantify risk. There was some interest expressed not just in the cost of cleaning up contaminated sites but also taking into account softer issues such as liability - if this can be described as a soft issue - politics and other impacts. Some might be surprised to discover that the project was initiated and funded by the military. I think as a matter of survival, some agencies are at times forced to gather and consider diverse data in their assessment of risky situations. More often than not, the lives we lead tend to be estranged from the data regulating our day-to-day affairs. Others make decisions for us. They don't just perform a service. They make it possible for us to give up control.,Others determine our placement, the value of our involvement, really our very existence in the data that is gathered; for the data can exist without ever taking our needs into account. One of the reasons why it is so difficult to quantify different aspects of risk relates to the phenomenological nature of impacts and consequences. In this case when I mean phenomenological, I mean highly personal. This isn't simply a disposition that can be changed on a whim. Some of us literally have a low tolerance for toxicity while others can stand a great deal. Some people can fight through social barriers while others collapse under its weight. Phenomenology is not merely emotional but also mathematical - something that can be detected in the statistical distribution of internal responses to external stressors. Moreover, tolerance to external stress is transient and dynamic; it is dependent to some extent on the different support structures that exist in our lives at the time. The erosion of such supports can reduce one's tolerance.,I suggest that austerity and efficiency initiatives on a broader societal level can bring about a level of harm and neglect that hasn't been seen in centuries; and this is not a society that can exist in a
 and impersonal use of data can lead to adverse health consequences. If the idea is to cut back on the social support
I describe here the projects that I worked on, as well as career progress, starting 25 years ago as a PhD student in statistics, until today, and the transformation from statistician to data scientist that occurred slowly and started more than 20 years ago. This also illustrates many applications of data science, most are still active.,My interest in mathematics started when I was 7 or 8, I remember being fascinated by the powers of 2 in primary school, and later purchasing cheap russian math books (,) translated in French, for my entertainement. In high school, I participated in the mathematical olympiads, and did my own math research during math classes, rather than listening to the very boring lessons. When I attended college, I stopped showing up in the classroom altogether - afterall, you could just read the syllabus, memorize the material before the exam and regurgitate it at the exam. Moving fast forward, I ended up with a PhD summa cum laude in (computational) statistics, followed by a joint postdoc in Cambridge (UK) and the National Institute of Statistical Science (North Carolina). Just after completing my PhD, I had to do my military service, where I learned old data base programming (SQL on DB2) - this helped me get my first job in the corporate world in 1997 (in US), where SQL was a requirement - and still is today for most data science positions.,My major was in Math/Stats at ,, and I was exposed between 1988 and 1997 to a number of interesting projects, most being precursors to data science:,When I moved to Cambridge university stats lab and then NISS to complete my post-doc (under the supervision of Professor ,), I worked on:,Note: ,'s logo represents the mathematical bridge in Cambridge.,I was first offered a job at MapQuest, to refine a system that helps car drivers with ,. At that time, location of the vehicule was not determined by GPS, but by checking the speed and changes in direction (measured in degrees, as the driver makes a turn). This technique was prone to errors and that's why they wanted to hire a statistician. But eventually, I decided to work for CNET instead, as they offered a full time position rather than a consulting role.,I started in 1997 working for CNET, at that time a relatively small digital publisher (they eventually acquired ZDNet). My first project involved ,, to send automated email to channel managers whenever traffic numbers were too low or too high: a red flag indicated significant under-performance, a bold red-flag indicated extreme under-performance. Managers could then trace the dips and spikes to events taking place on the platform, such as double load of traffic numbers (making the numbers 2x as big as they should be), web site down for a couple of hours, promotion etc. The alarm system used SAS to predict traffic (time series modeling, with seasonality, and confidence intervals for daily estimates), Perl/CGI to develop it as an API, access databases, and to send automated email, Sybase (star schema) to access traffic database and create a small database of predicted/estimated traffic (to match with real, observed traffic), and of course, cron jobs to run everything automatically, in batch mode, according to a pre-specified schedule - and resume automatically in case of crash or other failure (e.g. when production of traffic statistics were delayed or needed to be fixed fitst, due to some glitch). This might be the first time that I created ,.,Later in 2000, I was involved with market research, business and competitive intelligence. My title was Sr. Statistician. Besides identifying, defining, and designing tracking (measurement) methods for ,'s, here are some of the interesting projects I worked on:,I worked for various companies - Visa, Wells Fargo, InfoSpace, Looksmart, Microsoft, eBay, sometimes even as a regular employee, but mostly in a consulting capacity. It started with Visa in 2002, after a small stint with a ,??company (,), where I improved time-to-crime models that were biased because of right-censorship in the data (future crimes attached to a gun are not seen yet - this was an analysis in connection with the gun manufacturers lawsuit).,At Visa, I developed multivariate features for credit card ,, especially single-ping fraud, working on data sets with 50 million transactions - too big for SAS to handle at that time (a SAS sort would crash), and that's when I first developed Hadoop-like systems (nowadays, SAS sort can very easily handle 50 million rows without visible Map-Reduce technology). Most importantly, I used Perl, associative arrays and hash tables to process hundreds of feature combinations (to detect the best one based on some lift metric) while SAS would - at that time - process one feature combination over the whole weekend. Hash tables were used to store millions of bins, so an important part of the project was , - doing it right (too many bins results in a need for intensive Hadoop-like programming, too few results in lack of accuracy or predictability). That's when I came up with the concepts of ,, ,??of a feature, and testing a large number of , simultaneously. This is much better explained in ,??pages 225-228 and pages 153-158.????,After Visa, I worked at Wells Fargo, and my main contribution was to find that all our analyses were based on wrong data. It had been wrong for a long time without anyone noticing, well before I joined this project: Tealeaf sessions spanning accross multiple servers were broken in small sessions (we discovered it by simulating our own sessions and look at what shows up in the log files, the next day), making it impossible to really track user activity. Of course we fixed the problem. The purpose here was to make user navigation easier, and identify when a user is ready for cross-selling, and which products should be presented to him/her based on history.,So I moved away from the Internet, to Finance and fraud detection. But I came back to the Internet around 2005, this time to focus on traffic quality, click fraud, taxonomy creation, and optimizing bids on Google keywords - projects that require text mining and NLP (,) expertize. My most recent consulting years involved the following projects:,During these years, I also created my first start-up to score Internet traffic (raising $6 million in funding) and produced ,.,As the co-founder of DataScienceCentral, I am also the data scientist on board, ,??and??traffic growth with a mix of paid and organic traffic as well as ,. I also ,??and manage a system of automated feeds for automated content production (see ,??page 234). But the most visible part of my activity consists of??,I am also involved in ,??and ,??(,). I actually wrote my first API in 2002 to sell stock trading signals: read ,??pages 195-208 for details. I was even offered a job at Edison (utility company in Los Angeles) to , on their behalf. And I also worked on other ,, in particular click arbitraging.
A few websites catering to analytics and data science professionals have experienced tremendous growth recently. Organizations such as INFORMS or AMSTAT have seen their traffic explode, targeting high school students to join the ranks of data scientists. Niche publishers providing high quality, actionable content - and run by true data scientists rather than journalists - have also seen spectacular growth.,By data science, I mean all disciplines focused on optimizing value through data analysis. It includes operations research, machine learning, data engineering, biostatistics, data mining, business analytics, predictive modeling, data plumbing, statistics and many more. For a full list, check the following articles,The chart in figure 1 shows growth for one of our channels (representing a fraction of all our web traffic), after filtering out ,. While in our case, growth is partly fueled via ,, much of it is still organic. In figure 1, you can see periodic dips in traffic (December and July), a spike around March/April 2014 when ,??was published, and a spike in March/April 2015 (well, that one is not visible yet) when we will release ,.??,As this point, the ,??has or is in the process of exploding - so we are losing this traffic, which is great. The traffic that we lost is more than compensated by new traffic generated thanks to new trends, including,Indeed, we even thought of having members pay a tiny symbolic fee, not to generate revenue, but to filter out members that do not bring value to our community. We won't do it, instead we focus our efforts in attracting high quality members. All our ads target US-only professionals, including those aimed at recruiting new members. And we are in the process of launching a few new channels:??,We are also careful about not inflating our traffic statistics as reported by Alexa. In particular, we do not split our articles into multiple pages to boost page views count, as this would be unfair to advertisers purchasing display ads. So ,.,Other indicators of growth include ,??(number of searches on Google), and number of job openings (there's data available on Indeed, but??,). Finally, figure 1 offers the most realistic picture: growth rate is constant. LinkedIn and Twitter charts (showing increasing growth rate over time) do not take into account inactive users.
In this article, I share some of my unusual views about big data and data science. My next article will be about new trends in data science.,And used with success. It includes,Healthcare, HR, and law will also benefit from big data, to create customized drugs and reduce healthcare costs, to hire the right candidates by automatically analyzing his tweets, and to better identify criminals (currently, many cases are not pursued because the evidence, based on expert opinion but not on data, is not strong enough for conviction),They don't necessarily need internal data. Third party research data (though not always accurate or filtered) will tell you a lot about your market and your competitors (I use Quantcast and other similar vendors, in my case). It is a great source of competitive intelligence. If you outsource some of your processes (newsletter management) to a vendor, your vendor will provide detailed reports about conversions, clicks, unsubscribes, broken down per segment. This data can be enhanced if you combine it with Google Analytics tracking of your websites.,Interestingly, as a digital publisher, data is our #5 asset, after our members, people working with us, our content, and the volume/quality of our traffic. And we use data to understand our community, identify opportunities and trends, and to produce research reports valuable to our clients (it has an impact on what material they promote with us, help them understand trends and opportunities, and eventually, boost the ROI that they derive from us).,Some think that a data scientist is a statistician or a machine learning guy. Data science overlaps with many fields, and has its own core. See the following two articles for detail:,Here are three technical articles that show how the data science approach is different from the statistical approach:,Much of data science is actually about automating the job of the statistician or other analytic experts, providing simple, robust, black-box solutions that can be used reliably by non-experts. Data scientists have also business and domain knowledge. If you hire a pure geek because he has R, Hadoop, and Python on his resume, and pay him a $160k salary, your ROI on this employee might not be positive. Plenty of great candidates are undetected by HR radars because the way automated resume filtering currently works. At the end, companies believe that data scientists are unicorns, and data scientists believe that jobs are scarce and hard to get. Many times, I suggest data scientists to become consultants, create their own company, sell data, become a publisher, or develop some data-intensive apps or systems (for instance, a platform that would display the price of most medical procedures for each hospitals, ,).,Because many traditional data science programs are just a relabeling of operations research, statistics or computer science curricula, taught by adjuncts that are paid very low salaries and have no business experience, we have now a bunch of candidates that are ,, compounding the myth that (real) data scientists are unicorns.,Things are changing for the better: programs like Zipfian Academy (sponsored by LinkedIn, Facebook etc.), or our ,??are project-based, free, online, on-demand, last only a few months, and allow students to work on projects that benefit the parent organization. A win-win for the students and training organization. Finally, lot of free data science material can be found on DataScienceCentral, some state-of-the art, some research-level, but most applied to real, modern business problems.

 , , , , , , , ,Bill Vorhies, President & Chief Data Scientist ?€? Data-Magnum - ?? 2014, all rights reserved.,??,About the author:?? Bill Vorhies is President & Chief Data Scientist of Data-Magnum and has practiced as a data scientist and commercial predictive modeler since 2001. ??He can be reached at:,The original blog can be viewed at:


! Which ones should I learn? Which ones do I need to land the job, to impress the client, to prepare for the future, to stay relevant? What programming languages should I learn? What technologies should I master? What business books should I read? Is there a course I can take, or a certification I can enroll in? Should I focus on being a specialist to ensure I am always the "go-to person" despite commoditization, or should I concentrate on generalist skills so I can always see the forest for the trees? A mixture of both? Is there a Roadmap? A Bible? A Guru? Help!!!,Look. Languages change, technologies evolve, and so-called experts come and go. Just when that awesome course ends something new pops up that the course didn't cover. Just when you became an R ninja, Python came around the corner and became the de facto. Just when you finally mastered how to lay out a kick-ass data pipeline using Hadoop, Spark became the new thing. Just when you figured out how to sell "personalization" to clients, some bad press killed the buzz word. What is an aspiring professional to do? There will only ever be one answer to this...you need to learn to??,.,But you want specifics. You want to know where to focus your efforts. But this isn't the right way to think about it. If you start with specifics you become myopic to the skills you DO have and try to apply them to every problem. As??,tells us, using one tool for all purposes is fraught with danger.,Problem solving is THE skill you need to learn and the ONLY way to learn it is to jump into problems and attempt to solve them. I promise you that everything will fall into place if you make problem solving your 'career lifestyle.',When you are trying to solve challenges you don't use a language because it happens to be hyped. You don't use a technology stack because some expert of the day said this is how you do big data. You don't use a specific implementation of??,because you read an article by a millionaire who swears by it. When you are solving problems the only thing that matters is SOLVING THE PROBLEM. What is it the client actually needs? Put all the toys that the cool kids are playing with to the side and have an honest conversation about the problem that needs a solution. This is the only criterion that should govern what approach you take and...here's the key...your skills in the decided-upon tools of choice will??,??as a result.,Not only will this be a much better way towards the solution, but it will deeply ingrain in you the specifics of that particular language or technology or overall approach. You will learn where it works and and where it doesn't. You will witness the weak points and the strong points first-hand, and will actually??,??how the solution maps to the pain points of the organization you are trying to assist.,When I was younger I used to tutor university students in math, physics and chemistry. They would look at the problem and then look at me with a blank stare until they finally asked "so how do I solve this?" I would answer "I don't know." The look of "what the hell am I paying for" washed over their face for a few seconds until I said look...there is your textbook, here are your notes, and here is the problem. I bet we can figure out how to solve this...shall we try? Of course I??,??know how to solve these problems but that wasn't the point. I wanted the students to realize that the only skill they should be leaving university with is the ability to use the resources available to them to solve a challenge. THIS is the skill that is??,??and will ALWAYS serve you in the future. Not a particular approach, language or technology; those things change.,Okay, but what about the fact that companies are looking for specific skills and if you don't have them you won't land the job. First off, any company worth your time knows that they are hiring you for your ability to use available resources to solve a challenge and you should be able to cite examples of this. But even this aside, does it make sense to start with these languages and tools? Sure. Those languages and tools are popular for a reason. But the point is it won't always be that way. If you want to learn Python, go solve a real problem with the language. Go grab a public database and see if you can find something interesting or even make a cool data-product out of it. If you do great, if you fail even better. Jump in and fail constantly and you will be awesome at Python...but more importantly you will be awesome at solving problems. So when Python goes away you bring your ability to look at data, think about the problem, talk with domain experts, work through solutions and build something cool with you.,Show me any R ninja, Python guru, Hadoop wizard or Six Sigma Blackbelt and I am not impressed. These are just the tools and trends of the day and they won't be here tomorrow. Show me how you try and solve problems. Show me what you have built and most importantly tell me how you have failed. If I threw you into the jungle with unknown legacy tools or a hot new language that just popped up could you solve the company's challenges?,Instead of listing languages and tools in an attempt to engineer your future go solve a problem. Go solve a hundred problems.??,??take a look at the list of skills you have; the languages you know, the technologies you've mastered, and the approaches you take. Your career will always be a byproduct of the challenges you've tried to solve.
I created this blog to further discuss the issue of mass data assignments, a methodology that allows qualitative data events to be incorporated into metrics such as performance indicators. These assignments are routine for me now after having developed a prototype. However, I am unaware of the prevalence of this or similar techniques in the broader community. So I periodically work the topic into my blogs to help stimulate discussion. When quantitative data exists, it means that we had something to quantify. The data is less an aspect of the thing being measured than an extension of the people doing the measuring. It is not truth but an elaborate portrayal to help satisfy our needs. It is reasonable for people to have a need for data, but it is necessary to distinguish the metrics from the underlying reality. Further, to the greatest extent possible, the larger reality should remain connected to the metrics. In this blog post, I will be exploring the idea of maintaining steady qualitative events but exploiting variable metrics formats. As an example, I will be discussing the notion of mass data assignments in relation to an organization that no longer exists - based on its surviving management records. This is not an ideal situation decades after the closure of the organization. I did some research during my graduate studies that brought me to the agency's records, which are currently inaccessible to the general public. I hope readers find the material interesting despite its historic nature.,I remember when I first started studying stocks "really closely" many years ago, I had some interest in a technical approach making use of both price and volume. I described the product of a daily price fluctuation and its volume as a shift in sentiment. I later referred to the sum of these shifts as sentiment. I started using this metric en masse on many different stocks. I was surprised one day to discover - in the process of running an imaging system I had designed for sentiment - that the price of a stock created a reasonably close approximation; this reduced the need for the volume, which was perfectly fine given that I didn't always have volume available. But it was a fundamental shift in thinking for me to regard data in terms of both fluctuations and running totals. Once no longer shackled to volume, I found myself successfully using the imaging system on all sorts of data including data pertaining to earthquakes, electrocardiograms, and tidal levels. This experience led me to conclude that sometimes it can be helpful to step away from fluctuations; it can be better to think about the data as a net effect or sum of fluctuations. Conversely, it might be useful at times to dissolve or deconstruct data to reveal its fluctuations; or the fluctuations of its fluctuations. Being creative with metrics might seem illogical in a purely arithmetic sense - since things stop adding up in a customary manner. However, if the objective is to find a reasonable behavioural or algorithmic match, there is no need to cling any particular format.,Some readers might look at the image above and suggest that it contains a great deal of noise. The amount of noise is actually format-dependent. The focus of the illustration is to find patterns in the fluctuations. Given the absence of any cohesive pattern, does this mean that the data has no value? Of course my argument is that we are looking too closely at the illustration. This is not to say that stepping away from the screen is going to help. I am not saying that people are too close in terms of their physical proximity. The same way I made use of price fluctuations in my assessment of sentiment and then later decided just to use the price, I can do much the same using the above data. In the next illustration, although I use the same underlying data, I decided to plot the number of monthly cases rather than the monthly case fluctuations. As we go through these examples, consider the different potential uses given the "format" of the data. My point at this initial stage is really about the choice of formats and how this affects our perception.,Putting the issue of suitability and soundness aside, above we have the sort of illustration that is more hospitable to linear regression. Over the course of my real-life responsibilities, I normally evaluate data in this second format: it is well-suited for determining whether business is speeding up or slowing down, confirming seasonality, and assessing short-term directions. As confusing as the chart appears, perhaps many decision-makers can relate to its general premise: in order to evaluate the condition of a business, it is reasonable to compare monthly loads. Some might complain about my use of line- rather than bar-graphs; but it is quite difficult to see through bar graphs with this much activity. A bar graph is more plausible for shorter sampling periods. I accept the argument that this type of illustration might be more coherent and convincing if it had less data. (It might be easier to make a case using less complicated graphics.) The next image is from the sum of the caseloads rather than the sum of the fluctuations - yet another format from the same data. There is quite a metamorphosis.,On the above illustration, in what I call a "plough chart," we greatly impair our ability to detect seasonality; but on the other hand it is possible to obtain an overall perspective of business missing from the previous illustration. I can identify on this chart declining capacity for childcare and "surplus capacity" for consulting. My inferences are debatable decades after the surrounding events took place. I suggest that the agency assisted clients with their childcare needs less often because there was less childcare available. An alternate perspective that I cannot confirm is that the clients required less childcare because they were no longer having or raising children. I would say that rehabilitation capacity remained constant; and services operated at full capacity. I will go out on limb and propose that more consultants became available as the agency operated; there had to be greater demand for consulting services in order for the chart to appear as it does. On a more generic business level, I would say that the pattern for "consulting" reflects a developing market. This chart offers a significant amount of business guidance. The next illustration contains not the sum of the caseloads but rather the sum of the sum of the caseloads.,On the bar-graph insert, I highlight details from fluctuations between the last two periods, providing us with the same totals as the third illustration. While perhaps not particularly meaningful, this fourth format provides us with a rather curious running representation of performance over the recorded period of the organization's service history. The y-axis is no longer useful. Nor is there any activity within the line patterns that provide business or operational guidance. Although this particular illustration is not all that relevant for my own purposes, this is not to say that the format might not be significant in relation to some other type of data or application. The point really is that I might not already know how deconstructed the source data is; so it might indeed be necessary to add the totals of totals.,I once built a model replica of an actual landfill site. After completing the model, I was disturbed by its unusually flat appearance. Having walked over the landfill site on many occasions, I understood the area to be rather hilly. I asked a geography professor specializing in aerial interpretation and remote sensing for her thoughts. She explained that the world is actually rather smooth although it might not seem so for those walking over it. Relative to us, there appear to be hills; and indeed these are hills from our perspective. As we migrate from the smoothest illustration to the choppiest, it is like descending from high above the planet to its deepest parts. The bits of data important to us likely occupy particular stratum. The choppiest format can be found in the smoothest in a mathematically compressed form; therefore, it is possible to gain some understanding of all formats from just one. If data seems closely aligned to one particular format but not another, nonetheless we would have some insights. This is the general idea behind my use of variable metrics formats. At this time, the prototype is not designed to automatically scan different formats; this is mostly due to lack of processing power. Also, I tend not to incorporate quantitative metrics in my personal data, where the prototype has had the most influence. I normally use qualitative categories: e.g. terrible, bad, normal, good, and terrific. However, I have some quantitative metrics: e.g. pulse, pressure, and weight. On my prototype, a quantitative metric essentially reflects a prescriptive qualitative regime.,My rationale behind the use of different formats relates to the non-symmetrical or incongruent nature of qualitative events compared to metrics. Just trying to explain the basis for comparison is challenging - so dissimilar a qualitative event seems from a quantitative metric. I cannot assume that an event occupies the same ontological placement as a metric. If there is no prior knowledge of the algorithmic impacts of interaction, it is necessary to obtain a feel for "discursive congruence": this is the extent to which internally conceived phenomena coincide with the substantive boundaries set by an externality. It is difficult to say with any certainty that a discussion around the dinner table somehow led to the purchase of a new car; how a person's reflections after a television program may have motivated the decision to pursue a 4-year degree; and how an awkward moment during a blind date triggered a violent rampage in a school the following day. There is a fundamental need to overcome the disconnection: this is because the instruments and systems that hold the fabric of our particular society together are driven by numbers; yet the contributing events and resulting outcomes often evade quantification, thereby becoming invisible or imperceptible to our structural capital.,I will now provide some details of the organization responsible for the data. This real data is from the management records of an agency that operated for a number of decades - from the 1960s to the early 1990s. However, the data reflects only a 12-year period. The agency provided counselling services to support employees dealing with alcoholism, depression, and various workplace difficulties including stress. There was some talk of terminating the organization and offloading its responsibilities to other agencies. A case was made for decentralization and outsourcing: those responsible for financing the agency felt that the cost of counseling and disability benefits had gone out of control. The third illustration is useful for showing broad systemic patterns: near the end of the organization's operating life, some but not all of the costs associated with its services to employees were indeed sloping higher than the historic trend. This is despite the fact that services were meant to increase employee performance and reduce costs; it seemed like the investment in services had somehow led to higher costs.,From the illustrations, it should be apparent that a significant change occurred around the 8th or 9th year of operation. There was indeed an important development taking the world by storm at the time: computers had been introduced into work environments; this greatly increased the ability of organizations to handle higher workloads and offer services using surplus capacity. I believe that this is a notable administrative change taking place in the background. There was also something increasing demand for counselling services: computers in the work environment brought about a radically different kind of work setting - inside cubicles, behind desks, facing computer monitors, tapping all day long. So it is really exciting being exposed to the numbers decades later. I believe that the charts reveal the pangs of technological change.,I was writing some software originally intended to assist with performance evaluation and quality control. However, something unusual happened that caused me to change my use of the code. In quality control, it is customary to generate and keep track of events pertaining to quality: e.g. dented, loose, chipped, and peeling. On the other hand, in performance evaluations connected to quality, there is an attempt to assign quality events to departments, processes, and individuals. "Sarah" might be held responsible for 10 chipped units of product. Perhaps almost unintentionally one day, I decided that instead of using people or departments, I could assign events to different performance grades: e.g. 10 chipped units to "acceptable." Then instead of determining the performance of a team of individuals - e.g. James, Edna, Lucille, Ben, and Julia - I decided to assess a group of grades - e.g. terrible, bad, acceptable, good, and excellent. Thus, I could determine the events that seem more connected to excellent than good. This is my rather sketchy overview of how I started off on the road to mass data assignments. Sometimes in my blogs I refer to this process as "massive data assignment" or - although I deny any ownership - "big data assignment.",In terms to this current blog, my objective is to steer this discussion of mass data assignments towards the use of "variable metrics formats" rather than qualitative categories or grades. Recall near the end of the previous section, I speculated on the factors contributing to caseload increases: a combination of higher capacity and demand both probably related to technological change. Here then is one situation where it would be desirable to deliberately connect the metrics to a trail of events on which to base our conceptual understanding of the quantities expressed. Rather than simply know that caseloads increased during particular periods of time, researchers should be able to ascertain the extent to which historical circumstances are connected; and this information would be preserved in perpetuity. Mass data assignment is about giving body to metrics. Both the assignments and the determination of relevance are performed systematically. Abundant computer processing power is necessary to power the practice.,As I mentioned near the beginning of the blog, I am uncertain about the prevalence of mass data assignments (using a "mass" approach) in the broader community. I have certainly written on the subject for some time. I hesitate to describe myself as the person who first developed a mass approach; but I suppose it is safe to count myself among its pioneers. I leave it to members of the community to self-identify. I will always omit details that would allow for a complete emulation of my specific efforts. Even if a person had all of the software and existing files, it takes a great deal of work to maintain the system. At question really is level of commitment and passion for the cause. As some might discover at some point in the future, it is also necessary to have important character traits: honesty, willingness to embrace failure, and the ability to come to terms with truth.,Would it be possible for an honest person to miss the emergence of Isis? It takes a pathological liar to systematically miss something obvious; for the objective is not to understand but rather feel in control; to be rewarded for being in control; to receive praise and adulation. I remember a series of weight loss commercials where the celebrity promoter was obviously starting to gain a lot of weight; but she kept praising the weight loss program as if reality didn't matter. I suppose she had a contractual obligation. The fact that I can assign an enormous amount of data to different metrics doesn't alter the underlying weakness currently in the process: I have to select the metric intended to hold the assignment. So a mass approach requires a strong "operational" sense of ethics. What is the honest thing to do with the data? One must determine what is faithful and true in different operational circumstances. Even an extremely capable data scientist, if he or she lacks ethics, can reach faulty conclusions. On Tendril, because choices involving metrics are made repetitively, character weaknesses no matter how deeply buried can lead the algorithmic environment astray. That's my perspective anyways.,In the illustration above, I precariously provide some names for the different formats to assist with referencing. I believe that all of the formats have algorithmic value in relation to mass data assignments but not necessarily in terms of the resulting charts for visual analysis. Perhaps particularly perplexing is the fluctuations of fluctuations offered by trance (from where the data cannot get much choppier); and the sum of the sums by race (from where the data cannot get much smoother). Yet if an event seems strongly manifested in trance, it might also be present in trace albeit as a compressed aspect. If an event can exist in race, it might also be found expanded in pace. This is an interesting area for research and software development. I read in blog posts and forum threads all the time about how professionals in marketing wonder whether their strategies and investments are working; this is not a question that can be easily addressed through the use of metrics alone. The level of inference would have to be quite high without an approach connected to qualitative events. I added a fifth image on the bottom containing a distribution of qualitative events formed using a mass approach. This is mostly to give readers some reassurance that there is a greater purpose; the choice of metrics is only a starting point leading to a much higher level of analysis. In the chart, I identify a family of related events that all seem to have adverse consequences in relation to a number of related metrics. So that's an actionable insight: "doing this is harmful.",I have found that some phenomena can be connected to patterns resembling the different formats but which do not strictly follow the idea of fluctuations giving rise to sums (FTS). For instance, considering the totals for a month, somebody selling a particular product might sell more than 5 units 75 percent of the time; more than 10 units 50 percent of the time; more than 15 units 25 percent of the time. So I am describing a plough pattern or pace. The patterns for the individual days would more likely resemble trace. If we review the individual sales amounts, we might find a pattern like trance. The bulk orders and shipments perhaps behave like race. I suspect, the more lag exists in response to current events, the greater the likelihood of obtaining a sum-of-sum (SOS) pattern. If the events can be expressed as instances of something faster or more frequent, the greater the likelihood of a fluctuations-of-fluctuations (FOF) pattern. I am forced to speculate given the lack of resources for research; but I hope to pin down the dynamics in the future both in terms of the field scanning of metrics and also in relation to mass data assignments.,This entire blog has been about adapting metrics in order to try to better "fit" qualitative events. I think it is reasonable for a person to ask, why bother? Quantitative data is useful to the extent that we already have a strong understanding of the data, and our understanding reflects the nature of the data. For instance, I know what 3 cm represents: it is the physical length of something measured. If we ask a group of people what bullying represents, they might have varying responses; and so the nature of a score of 7 on bullying scale from 0 to 10 is evasive. I wrote about how qualitative events can sometimes have discursive congruence with metrics; it is this congruence that causes the metrics to reflect the nature of the data. The political conversations between coworkers in a tavern after work might have some relevance to actual election results to the extent that these discussions exhibit discursive congruence. The coworkers can of course talk about going fishing, their all-time favourite pastries, and the high cost of public transit. The congruence then becomes less direct and perhaps evasive. Nonetheless, life as we know it exists within this realm of vernacular discourse - sometimes shared, periodically highly personal, at times concealed from others. These are important albeit intangible events that are difficult to quantify. When we don't have a strong understanding of the data, or our perceptions only involve tiny aspects of its nature, the act of quantification leads us to faulty conclusions. We live a fabricated truth by imposing quantity over matters outside our epistemological authority.,In the past, I think there was a tendency to dismiss data that was quantitatively evasive. I don't wish to question members of the community that persist down this road. But there is room now given our computer processing capabilities to push the boundaries. Beyond the technical challenge of meaningfully incorporating qualitative events, there is also the market impacts of consider. Remaining in one frame of mind means selling the same software; perhaps all to the same market; causing companies to reach similar solutions; thus reducing the competitive benefits. It is worthwhile to diversify. There is no reason to be uncompromising. Ultimately, some companies will try new and different methods. However, I promote a mass approach not for companies specifically but rather the data science community. I think that the integration of qualitative and quantitative data will radically alter the discourse surrounding the collection and accessibility of data. My hope is to help the community make use of symbols and objects capable of containing high levels of abstraction perhaps as deep as thought. In previous blogs, I discussed the task of determining what events to throw in order to help define the parameters of systems. In this blog, I have been focused on finding the most appropriate metrics. However, since I am starting to reach the processing limits of my equipment, there will be fewer posts pertaining to these specific concerns in the future. I will therefore move on to less hardware-dependent topics. It is like I'm stuck in the 60s and 70s without proper equipment. So I leave it to the industry to keep up with my needs.

Having found a dataset on Ebola cases, thought of checking it out quickly what the statistics really look like.,The dataset contains 3 countries and within each there are multiple regions.,So just using the high level information at the country level this is what we can see in a simple line chart.,In the below Chart,,The blue line > Total Death cases,The green line > Total Cases,The Orange line > Currently admitted,And the Red line > Total recovered.,The dataset can be found here,And here is the actual link to the??
The "big data" and/or "analytics" buzz have been around for a while now and have clearly made a major impact on many things. However, in HR, I'm not entirely sure what kind of impact it has made. On some level, I think the big data buzz, at least, pushed HR to be more strategic and to think more strategically. With the increased emphasis on data brought on by the big data movement, it led many, including HR to be analytical and to use data and evidence to make informed decisions and develop strategies.,That all makes sense to me but what is HR's relationship now with data? How did big data move the needle for HR? How is data necessarily driving HR decisions? And how is data-driven decision-making aligning to company strategy and company success? Was all that buzz just another fad for HR to follow? Was it just another way for HR to stay relevant or keep its existence? ,, what does any of it mean? Were they just reports that HR produced to impress the executives? It is time to revisit where HR is in terms of how it is coping with big data and/or analytics. What has your HR department done to grapple with analytics? Do you have the latest and fanciest analytics tool? And if you do have nice graphics and reports, what do they all mean? How are they informing decisions? Or are they informing decisions? And with all this data analytics, how are you ensuring the metrics align with strategies? If you are quantifying engagement and job satisfaction, how are you correlating those with company success? Do you need to inject statisticians or economists into your HR department so that they can work with your data and develop algorithms and predictive analytics? And do you need to build sophisticated algorithms in HR?,I think that by revisiting HR analytics at this point and asking some of these key questions, we'll be able push HR analytics to the next level. Now that we know that there is "big data" out there (and they're not always "big" in HR), and that we can quantify and measure many of the HR-related activities, it's time to make sense of the data. One thing that made the analytics game extra challenging for many HR departments is that HR is unique. HR, by nature is not known to be as quantitative as the finance department or as good in handling data as the IT department. HR, historically, is known primarily for its administrative functions, for being the experts in pay, benefits, recruiting, etc. But now, with this so-called 'big data' movement, HR has the opportunity to be seen as more of a strategic partner. Now that with our data and reports, we got the attention of the executives and they have now granting us a seat at that table. We must now don our strategic hats and think strategy. It's a lot more than head-counting or manipulating spreadsheets or putting together fancy presentation slides. It doesn't always have to be about big data, although the big data movement deserves some credit for pushing HR to think data. ??Sometimes small data is just fine and insightful. ??We need to think more about the insights that we, as HR professionals, bring to the table. After all, we are the HR subject matter experts and should be the most knowledgeable about attrition or about job satisfaction. And when we sit at that table with the executives, we need to know how HR is making an impact on the company's strategy and the company's bottomline. We need to be able to deliver insight and demonstrate what all this HR analytics means. It is a lot more than reporting what the human resources look like but it's about making the connections and understanding the relationships. It is good that we can describe the data but there's an opportunity for us HR professionals to move beyond the descriptive data and onto being more predictive and ultimately being prescriptive., 
The end goal of??,??is simple and will change the way in which the world operates today. By feeding individuals the right information, at the right time, analytics become invisible and embedded into every application and workflow of every user. It is a vision, a goal, a strategy that every individual across every industry can rally around in order to drive the business metrics that matter through the use of data.,Across every organization, business leaders are??,??of pervasive analytics. Last month in October, Gartner released their 2015 strategic technology trends and pervasive analytics made the list. Gartner??,, ?€?Every app now needs to be an analytic app,?€? said David Cearley, vice president & Gartner Fellow. ?€?Organizations need to manage how best to filter the huge amounts of data coming from the IoT, social media and wearable devices, and then deliver exactly the right information to the right person, at the right time. Analytics will become deeply, but invisibly embedded everywhere.?€? Because let?€?s face it, Big Data and the Internet of Things (IoT) isn?€?t a strategy on it?€?s own. Let?€?s take a look at IoT for example.,Sensors are powering the word of IoT, however, as sensors continue to be commoditized sensor data alone will become less of a differentiator. Organizations will derive true business value and company differentiation by being able to build value on top of all of this data. With only 12% (1) of enterprise data being leveraged for analytics today, there is massive opportunity for organizations that look across their org chart to see where more data can be used to make a more valuable and predictable business. Imagine if Google didn?€?t use their web data to improve their search algorithm, or if Amazon sold their consumer purchasing data to make a quick buck 10 years ago? Would they be the same companies they are today just because they collected that data? Most likely not.,Leading organizations have already begun to see serious returns on deploying a pervasive analytics strategy. Boston Consulting Group surveyed 1,500 global senior executives in 2014 and found that information-driven enterprises that leverage analytics generate 12% higher revenues than their counterparts (2). As the underlying platform that facilitates pervasive analytics, Cloudera and our partner ecosystem have helped countless organizations reach these benefits by implementing an??,??to deploy a pervasive analytics strategy.,??is one of the largest home automation providers in North America. They collect and analyze 20-30 diverse sensor readouts from around 800,000 homes in order to supply intelligent home automation solutions. Before Cloudera, Vivint?€?s agility was limited by traditional technology hindering its ability to experiment and add new analytics and intelligence to it?€?s solutions. But not any more. ?€?Vivint has been at the forefront of the connected home for decades, and now with the emergence of IoT, we are truly able to innovate by collecting and analyzing vast amounts of data from sensors embedded in our devices,?€? said Brandon Bunker, senior director, Customer Analytics and Insights, Vivint. ?€?We?€?ve taken that one step further with Cloudera and can now look across many data streams simultaneously for behaviors, geo??location, and actionable events in order to better understand and enrich our customers?€? lives. This platform has differentiated our business and given us a tremendous competitive advantage.?€?,By implementing an enterprise data hub, powered by Apache Hadoop, Vivint has now unlocked the flexibility they need in order to leverage all of their data to discover and deploy analytics across their organization. Whether that is additional product value for their customers or internal organizational value. ?€?Hadoop is capable of producing value quickly, and for us the ability to look across data and track inferences brings immediate value to our customers,?€? said Bunker.,In order for us to make the societal impact that we are all after with Big Data and the IoT we need to change the way we are thinking about it. We need to shift our thought away from the data itself and back to the employees and customers that we aim to impact. With advancements in modern technology the questions can now move away from how I store, manage, and process this new data to what should I do with this newfound information. The new world of data and this paradigm shift requires that we start asking bigger questions. How are employees doing their job or interacting with our product today? What analytics do they need in order to make their lives easier? What data do I need to start collecting to power this change?,We understand that this is easier said than done. We recently had Forrester on a,??to discuss the value of pervasive analytics and how information-driven enterprises have broken this overwhelming task down into bite size pieces. One key is having a long term strategic partner in order to help you through the process of implementing a pervasive analytics strategy, and we hope to be that partner.,?€??€??€??€??€??€??€??€?-,???€?The Forrester Wave???: Big Data Hadoop Solutions, Q1 2014?€?,??,??
Over the past six months we have been working with the reliability and maintenance organization within a large oil and gas client to build out their Master Equipment List (MEL). Like many asset-intensive organizations, they have implemented an Enterprise Asset Management (EAM) tool to give them visibility and control over their capital equipment to optimize their maintenance strategies, reduce operating costs, and better manage their workforce and spare parts inventories. The challenge is that the software tool was implemented many years after the assets were constructed/procured. The data ?€? the MEL - has to be built. Achieving the benefits of an EAM tool rely on a complete and detailed MEL populated with all critical pieces of information ?€? type of equipment, manufacturer, model, serial number, vendor, criticality, spare parts listings.,The project sounded quite straightforward when we provided the bid. Find the missing data ?€? major things like manufacturer, model, serial numbers, vendor information, purchase orders ?€? for their offshore equipment and populate the Enterprise Asset Management (EAM) tool. At Sullexis, we are data experts. We know how to merge it, scrub it, format it, load it. We are accustomed to pulling data from a complex array of sources and defining a common model. We know how to develop extract, transform and load (ETL) routines. We know a lot about EAM tools, reliability and maintenance strategies, and the oil and gas industry. This seemed like a very simple request.,But it wasn?€?t. We have been working to find data on assets that were built/procured 10+ years ago. The POs are not clear, the vendor documentation is inconsistent and few of the people who worked on these original projects are still in the same roles. There were many companies involved in these large projects. The concept of ?€?information management?€? didn?€?t really exist 10 years ago in any of these companies. Because the equipment is offshore, it is not feasible to actually walk down the information. We needed to find the original data sources.,And thus began our role as a private detective agency. We began a series of interviews to reconstruct the creation of these offshore platforms. Who did the design work? Who did the procurement? What artifacts were produced in those activities? Who did they transition their data and documents to? How was the equipment commissioned and who was responsible? Who were the vendors to build the skid packages? How was documentation stored in the shared drives and/or in document control? These interviews often begin ?€?Where were you in 2005 when this platform was in the design phase??€? and continue to ?€?Do you know the location of any data or documentation related to this equipment? If so, where did you last see it??€?,??,In this interview process, we have encountered the full spectrum of interviewees. The eager to help, but know nothing. The reticent types who respond to an essay question with a yes/no answer. The naysayers who just laugh and say it is an impossible task and we should give up now. The naysayers also like to spend most of the meeting time explaining all the reasons why we will fail and why every other team has failed in this endeavor, which actually makes them helpful as we frantically take notes of every previous mistake to ensure we don?€?t repeat them. And finally, after we relentlessly pursue meetings with every possible person who might be able to help us, we stumble across a few genuinely knowledgeable people with an actual memory who help us.,??,Through these interviews, we were able to identify a set of systems that contained a small piece of the overall puzzle we needed to assemble. The final data sets had to come from the documentation. And it was massive. Our team has had to filter through 100,000+ documents. Some of these documents are 1,000s of pages long and are typically low quality scanned PDFs that are not searchable and not well indexed. Some are even hand-written ?€? and we?€?re not planning to give any gold stars to this team for penmanship. We had to manually review these documents to capture the relevant equipment data into spreadsheets to feed our data analysis team.,????,But our persistent reverse engineering of a set of major projects over a decade ago is paying off. We are succeeding. We have already identified over 40,000 sets of equipment data, and we have started the spare parts phase of the project. And ?€? music to a consultant?€?s and a client?€?s ear - we are on track to meet our original budget and timeline.,??,If your organization needs help to define your MEL as part of an Enterprise Asset Management implementation, make sure your data team includes some private detectives.,Author:??
"From episode 10 of my , on YouTube.,
,I think I do - and it is the ?€?appification?€? of analytics. What I mean by this is the reduction of a complex analytic activity such as market segmentation, down to a single button on your computer interface. Very much like the Apps on your smartphone, tablet or increasingly your desktop.,That?€?s what it looks like but the impacts are more profound. That?€?s because it makes it possible for analytics to be successfully done by people who may not understand how it works, but do understand the ?€?why?€? and ?€?when?€? they need to do it.,For example, a marketer in a company can access more sophisticated views of their campaigns without the need of a specialist analyst. Appification extends the range of analytic things that a non-specialist can do.,This appification is made possible because of three things that have emerged in recent years:,The last enabler needs a little further explanation. R is a free software programming language and software environment for statistical computing and graphics. It contains thousands of packages (10,000?) specializing in topics like econometrics, data mining, spatial analysis, and bio-informatics. Nobody knows how many R users there are, but a reliable estimate (see ,) puts it in the millions. Many thousands have helped R develop over the years.?? I think that this sort of large-scale self-organising open source effort is beginning to teach the world how to use analytic algorithms.,The above is all supposition, but I can back this up with evidence. Here are 4 examples of algorithm markets - or at least they exhibit varying degrees of ?€?algorithm marketness?€?.,In conclusion, there is still one issue needing resolution before algorithm markets take-off: How will the world?€?s business people get data into and out of these algorithm apps? I?€?m not sure yet, but I think the answer will be more apps. Apps that themselves appify the transformation and loading of data into and out of algorithm apps.,Confused? Well don?€?t be, like most new and shiny things, what we are talking about is just the next generation of ETL - something business intelligence people like myself have been building for the last 20 or more years.,What?€?s old is new again? Maybe, but from my perspective the future looks exciting for analytics.,My , contains tips from a 25 year veteran of the analytic profession."
As a product manager in the domain of predictive analytics, I own the responsibility to build predictive analytics capabilities for consumer facing and/or enterprise platforms; the business applications vary among item recommendations for consumers, prediction of event outcomes based on classification models, demand forecasting for supply optimization, and so on. We usually see the applications where the predictive model built using machine learning technique(s) is leveraged to score the new set of data, and that new set of data is most often fed to the model on-demand as a batch.,However, the more exciting aspect of my recent work has been in the realm of real-time predictive analytics, where each single observation (raw data point) has to be used to compute the predicted outcome; note that this is a continuous process as the stream of new observations continuously arrive and the business decisions based on the predicted outcomes have to be made in real-time. A classic use case for such a scenario is the credit card fraud detection: when a credit card swipe occurs, all the data relevant to the nature of the transaction is fed to a pre-built predictive model in order to classify if the transaction is fraudulent, and if so deny it; all this has to happen in a split second at scale (millions of transactions each second) in real-time. Another exciting use case is the preventive maintenance in Internet of Things (IoT), where continuous streaming data from thousands/millions of smart devices have to be leveraged to predict any possible failure in advance to prevent/reduce downtime.,Let me address some of the common questions that I often receive in the context of real-time predictive analytics., A data scientist requires an aggregated mass of data which forms the historical basis over which the predictive model can be built. The model building exercise is a deep subject by itself and we can have a separate discussion about that; however, the main point to note is that model building for better predictive performance involves rigorous experimentation, requires sufficient historical data, and is a time consuming process. So, a predictive model cannot be built in ?€?real-time?€? in its true sense., Again, model building is an iterative process with rigorous experimentation. So, if the premise is to update the model on each new observation arriving in real-time, it is not practical to do so from multiple perspectives. One, the retraining of the model involves feeding the base data set including the new observation data point (choosing either to drop older data points in order to keep the data set size the same or not drop and keep growing the data set size) and so requires rebuilding of the model. There is no practical way of ?€?incrementally updating the model?€? with each new observation; unless, the model is a simple rule based; for example: predict as ?€?fail?€? if the observation falls outside the two standard deviations from the sample mean; in such a simple model, it is possible to recompute and update the mean and standard deviation values of the sample data by including the new observation even while the outcome for the current observation is being predicted. But for our discussion on predictive analytics here, we are considering more complex machine learning or statistical techniques.,Second, even if technologies make it possible to feed large volume of data including the new observation each time to rebuild the model in a split second, there is no tangible benefit in doing so. The model does not much with just one more data point. Drawing an analogy, if one wants to measure by how much the weight has reduced from an intensive running program, it is common sense that the needle does not move much if measured after every mile run. One has to accumulate a considerable number of miles before experiencing any tangible change in the weight! Same is true in Data Science. Rebuild the model only after aggregating a considerable volume of data to experience a tangible difference in the model.,(Even the recent developments, such as ,, that are making efforts to move forward from Apache Mahout and similar tools (limited to only batch processing for both model building and prediction) are focused on real-time prediction and yet rightly so on batch-based model building. For example, Oryx has a computational layer and a serving layer, where the former performs a model building/update periodically on an aggregated data at a batch level in the back-end, and the latter serves queries to the model in real-time via an HTTP ,), It is when a predictive model (built/fitted on a set of aggregated data) is deployed to perform run-time prediction on a continuous stream of event data to enable decision making in real-time. In order to achieve this, there are two aspects involved. One, the predictive model built by a Data Scientist via a stand-alone tool (,, SAS, SPSS, etc.) has to be exported in a consumable format (, is a preferred method , these days; we have done this and also via other formats). Second, a streaming operational analytics platform has to consume the model (PMML or other format) and translate it into the necessary predictive function (via open-source , or , or , or other interfaces), and also feed the processed streaming event data (via a stream processing component in , or similar) to compute the predicted outcome.,This deployment of a complex predictive model, from its parent machine learning environment to an operational analytics environment, is one possible route in order to successfully achieve a continuous run-time prediction on streaming event data in real-time.
In the last few weeks we have seen new products launched by IT majors that have the potential to greatly influence the fortunes of the Analytics and Insights business. IBM?€?s Watson Analytics and Oracle?€?s Cloud based Analytics are just two new solutions/ platforms that have hit the market.?? We can rest assured that many more such offerings will be launched in the coming weeks. As the products keep coming, they raise some important question for the analytics industry. What impact they will have? Is the Analytics & Insights business ready for these changes at all?,It is almost two weeks since Big Blue launched its Watson Analytics. It is a natural language-based cognitive service that can provide businesses instant access to powerful predictive insights and cool reports.?? Launched as a freemium service, it will provide predictive insights for a small fee and hopes to leapfrog user base.,Watson Analytics is all set to take full advantage of IBM?€?s acquisitions - SPSS will provide the horsepower for predictive analytics while Cognos will supply the visualization.?? It will enable business users to upload data to their cloud. Once data quality is established, the user can input his/her requests via an interactive screen and in return obtain predictive insights and reports via pleasing visualization.,??Oracle also has just announced its Analytics Cloud services with similar capabilities.?? The product features and details may vary but at a high level they will help business users get their insights easily and quickly.,??These products promise to be game changers because the business user does not require the intermediation of modelers to build predictive models or coders to write the code for the models. Across industry verticals, they will empower business users to make well informed decisions with ease and speed. They hold the promise of greatly widening user base. While the platforms are slick, it remains to be seen how the market responds. ??Ultimately, the revenue performance will determine if they are successful.,??From a Banking and Financial services (BFS) perspective, many banks have long been seeking similar tools. In the past few years many have invested top dollars in similar solutions tailored for their specific requirements. However, a huge majority of banks do not have such a tool / product. For now, ??IBM and Oracle being early birds, clearly are way ahead of competition and will reap full benefits for their investments and efforts.??,??These products have not emerged overnight. It is obviously the result of powerful research / understanding of customer requirements backed by significant investments. Business user communities across industries and particularly at banks are asking the logical question ?€? why did it take so long for such a product to hit the market??? ??We don?€?t have easy answers to these questions.,??For IT vendors and outsourcing majors these products have created new challenges and opportunities. How can they help their clients derive maximum value from these innovations? ??,IT outsourcing vendors who have a stake in the Analytics and Insights business should quickly develop a game plan to address these challenges. In my view these new products have pushed the majors, willingly or unwillingly, to a cross road. ??Many of these majors have a lot of homework to do ?€? they still have to get their house in order and bring in the correct leadership. Running the Insights business as an extension of IT data warehousing shop will not cut ice. If past experience has taught anything, it is that domain experts with hands-on business expertise may have better odds of success at this business. Those who are prepared and have the right combination of leadership and domain expertise will be the ones to meet and take advantage of the opportunities. The rest, as they say, will miss the bus again.,??
UPDATE: I fixed the links, as they were not being displayed correctly.,All was fine in the endless gardens of salads and breadsticks (I can't be the only one who orders extra breadsticks to take home with leftovers, right? ... Right...??). 55 , quarters of increased same store sales. Kudos to the olive farmers!,All was fine...,Until that sour year of 2009, which initiated a continuous erosion of that once fertile soil. Ever since that year, as if caused by a mysterious plague, the sales have been declining, graying out the brand as time flew by.,The question we ask ourselves right now is: what is happening to that chain of Italian restaurants, home of the never-ending salad, breadsticks and, as of a couple of weeks ago, pasta and toppings? What has been causing this decline in sales quarter after quarter? More importantly: what insights on what is driving this 23-quarter long erosion??can we uncover when analyzing Olive Garden's historical data?,We are trying to answer just that (by "we" I mean myself and the other 5697 data scientists at CrowdANALYTIX). We have gathered sets of data which include financial information, as well as demographic info of places surrounding the stores. Competitors' data is also present, as well as many other information that could hold some insights on what is causing this. The data is there. Now we have to analyze it.,We are doing this in a form of two contests, which you can join by becoming a solver in our community. Now, before you ask: nope. There are no fees for joining the community or the contests. No hassle. No... nothing. You just create your account and join the contests. That's it.??You can even gather your friends, co-workers or students and form a team (no hassle here as well).,There's just one catch: if your solution(s) or insight(s) gets picked as a winner, you get rewarded. But that's not much of a catch. More like a "tension builder". But seriously, there's no catch.,We'll be running this for two more weeks. More than 150 data scientists have already joined us from around the globe and they're already picking their brains on this. So, if anything, look at this as an exercise for your mind.,I hope this is of interest to you. Here are the links:,Have fun and enjoy the challenge.,By the way, that guy getting stuffed with breadsticks at a table near you might be me, so drop me a note when you see me.,Cheers,,Eric
The following comprehensive listings were produced by analyzing our large member database, extracting websites that our members mentioned or liked, and for each web site, identifying,The design of the member database (non-mandatory sign-up questions and choices offered to new members on sign-up) was done by our home data scientist (me) long ago, precisely with the purpose in mind??of??performing analyses like this one, down the road. Other analyses produced in the past include: 6,000 companies hiring data scientists, best cities for data scientists, demographics of data scientists, and 400 job titles for data scientists: see , at the bottom of this article.,Seed keywords were used to identify, for each website, whether one or more of the keywords in our list was found on the front page, using a web crawler. This helps categorize websites - the final goal being the creation of a data science webste taxonomy.The seed keywords that we used (hand-picked) are very popular data science related keywords:,We used a web crawler to browse all the URLs, after identifying and cleaning the websites fields (URLs listed by members), in our member database. ,??to get the script used to sumarize the data, as well as a sample of raw data. Note that improving this study is now a new project added to our ,: In short, it consists of creating a niche search engine for datascience, better than Google, and a taxonomy of these websites. Candidates interested in this project will have access to the full data. Because this is based on data submitted by users, the raw data is quite messy and requires both cleaning and filtering. Details are found in my script - it's a good example of code used to clean relatively unstructured data.,Here we categorize the websites in four major clusters:,We provide direct clickable links for domains in category 1 (above and below) only. The choices of these various parameters is to guarantee robustness in our results, filter out noise, and for internal security reasons: listing hundreds of little know websites (with clickable links) can get you penalized by Google, can results in many requests for link removal, and many might of these links can die in the next few months, creating a bad user experience (and additional Google penalties).,Here are the links to the four major categories of data science websites:,The field between parentheses represents the year when the website in question was first mentioned - it does not represent when the website was created, thought it's a good proxy to tell how old the website is. The member database goes as far back as 2007. The list of keywords attached to each website represents which seed keywords were found on the front page, when crawling the website. The number of stars (1, 2 or 3) represents how popular the website is: it's an indicator of how many members mentioned it. Of course, brand new websites might not have 3 stars yet.??,Source code (two scripts including a web crawler / parser / summarizer, and code to produce final HTML pages), as well as raw, intermediate and final data (samples, screen shots), and details about the 3-step procedure used to publish these listings, ,.,Our methodology, to build our semi-categorized website listing, has the following additional features:,Uncrowlable websites, bad domains,There are various ways to improve my methodology and the quality of the results. Here I mention a few:,Also, if you want your website to be listed, create a DSC profile and publish your website on your profile (look at the question about "favorite website", ,).,Finally, if interested, join our Data Science Apprenticeship to work on and improve this project, and turn it into a search engine and full taxonomy, changing automatically every day based on data gathered by the web crawlers. ,??in our list of business/applied projects. Time permitting, I will publish more advanced web crawling based on my ,??(currently paused).??
Here is how to begin your data science journey:
"Has the availability of more data improved democracy around the world? Watch the talk in which Conrad Wolfram discusses how computation for everyone can help people get answers relevant for them and how allowing people to make informed decisions in the key to our future democracy.,If you're in the UK and would like to hear more from Conrad, he's delivering a talk 'Master Your Data with Computation' in London on 21 November. Details at??,
"
Top data science bloggers, authors, websites, or Twitter profiles worth following is now a popular topic, sure to attract lots of attention. We've published our share, including??,Many of these , (published by would-be journalists) don't have any sound data science foundations behind them: they have major gaps and other drawbacks, especially those attempting any kind of (arbitrary) ranking.,Here ,??to identify 200+ interesting bloggers on our network, reprsenting a fraction of our contributors. This is a follow up to our article??,, which listed bloggers , of our network.,To appear in this listing, bloggers must have had at least two accepted postings. The stars after the name indicates the volume of activity. DSC is for a Data Science Central profile, while AB is for an AnalyticBridge profile. Only DSC profiles (and not all of them) have their short bio listed here. We plan to add more bios, as well as not showing the bio for bloggers who request it. New bloggers (since September 2014) are not in the list yet, but they will be in our next updated version. New bloggers can also be found ,??and its , section, or by reading our ,. Our next version will provide the date of last posting for each blogger, to help you identify active versus inactive bloggers.,The listing below is ordered by number of stars (representing the number of postings), then in reverse alphabetical order. The data was extracted and processed by our intern Livan, and I helped with the starring system and automated profile selection (for inclusion in the list). When you click on a name, you can find the blogs he/she has posted.
These are important questions to answer in a time when the lines are blurring between consumer and enterprise applications; business-to-business and business-to-consumer communications; and open-source and freemium and commercial software.,Actuate recently invited four industry experts to discuss the future of creating the next generation of data-driven applications and how they are influenced by open source and freemium software.,And more importantly, we asked them to look into the future a little bit in terms of what these apps are going to look like, where we?€?re going to get them and who will build them.?? Are we all builders? Are we all makers now in this new world?,Allen Bonde (,), VP Product Marketing & Innovation at Actuate, moderated the panel which included:,So the theme of this conversation is accelerating the development of data-driven apps, leveraging the potential of freemium and open-source and some of the awesome tools, the visual tools that you?€?ve seen today.?? And the inspiration really is that we?€?re going to create apps that you?€?ve never seen before.?? And the title of the panel was ?€?Building the Next Big App.?€??? And so the opening question that I?€?ll ask the panelists is: What does this look like??? What does it mean? ??What makes a great app?,Have you seen it??? Does it exist now??? What do you think??? I?€?m actually going to start with you, Loie, because Loie, you look at this from a creative perspective, from an end consumer perspective.?? What makes a great app?,Well, the first thing is that a great app has to be something that you can absolutely not live without.?? And when you find that, it enhances your life so greatly that you just can?€?t abandon it and that you have to stick with it, and that you utilize it almost every day.?? I think that?€?s one of the things that makes a great app.,Cool, can?€?t live without it.,There?€?s more to it than that.?? There?€?s not probably the next big app, but there?€?s probably the next small app.?? The next big app is something like what Starbucks has done.?? Anybody here have the Starbucks app??? The Starbucks app is basically a very small app.?? It does only three things ?€? it finds the nearest Starbucks, it lets you charge it, and it lets you see what your rewards are.?? That?€?s it.?? It?€?s a small app, it?€?s not a big app, but actually the smaller the app is, the more it can function in big apps, and that?€?s the part that becomes really interesting going forward.,Because of flat technologies and because all these wonderful things, this data processing, we can get very small apps that we can benefit from, whereas before it would?€?ve been a humongous app that requires a desktop to run.?? Now you can find it on your phone and it does exactly what it needs to do.,What do you think, guys??? Is this the era of the special purpose??? If I put words in Esteban?€?s mouth, this is the small special purpose app?,????I would say about the special purpose aspect to me, one of the biggest drivers for applications moving forward is going to be data.?? So in other words, how many in the audience have used ??ber??? Anybody??? All right, so we?€?ve got a bunch of hands.,So the thing with ??ber, you know, everybody knows you get cars, it basically routes and dispatches very efficiently.?? But the interesting part of ??ber to me is what it can tell us collectively about each other.?? And I don?€?t know if they still do this, but one of the ways ??ber essentially picks which cities they go to next is that they simply look at the data generated by the users.,So in other words, they look at it and say, ?€?Hey, in Chicago we?€?re having X number of people opening the app and looking for a car today, therefore we?€?re going to start up there.?€??? So what format it takes moving forward I think is open to debate, but for me it?€?s going to be data-driven,So that sounds like a pretty good setup for the notion ?€? they looked at the data, and they see something attached to that value. They looked at the data, they created something that sort of fit that data.?? Presumably they?€?re watching the data, right??? From their markets, they?€?re adjusting where they?€?re putting cars, where they?€?re recruiting drivers, what the competition?€?s doing.?? Is Lyft getting into their markets??? All about the data.,Mike, is this essential for this new app, is it?€?s got to be data-driven, so to speak?,????I don?€?t know if they have to be data-driven, but I think an app, even the most simple apps that generates data for whoever the sponsor of that app is, that they can then utilize, monetize, or analyze, and allows them to create new and interesting business models, is definitely the way it?€?s going to go.?? The app itself can be very simple, very narrowly focused, but if the data that?€?s generated is being generated by the app out to whoever is sponsoring that app, is allowing them to analyze it in ways that is creating new business opportunities, or even new social opportunities, then yeah, absolutely, that?€?s the way things are going to go.,What do you think, Esteban??? You?€?re getting that look.,The thing to recognize with all these apps, the only purpose of the app is to capture the data.?? The data already exists.?? You don?€?t create data by running ??ber.?? You already know the people who need ??ber or the places you need to open up shop. It?€?s there.?? All you?€?re doing is actually creating these apps, to channel the collection of data, put it in the right format you can use, and then smart people will analyze that.,??The app is a proxy to get the data back.?? Without the app, they can?€?t collect the data to analyze it and do something with it.,??And I think what that ultimately does, it creates continuous improvement for the end user and the consumer, and that data just doesn?€?t sit somewhere.?? It actually can inform better user experiences and better services, and maybe, to your point about spinning off new business ideas, new opportunities, I think that?€?s what the benefit is.,And the thing that?€?s nice about that is that it used to be new product ideas were basically based on conjecture.?? You?€?re guessing that this might be an opportunity.?? Now there?€?s a lot more hard data that you can base that guess on.?? So it?€?s much more data-driven innovation as opposed to inspiration-driven innovation.,??But then would that be considered conjecture??? Do companies that release apps consider the adoption for this app and what it?€?s doing for organizations??? If you have a company and you have a product and you want to know what your users are thinking or what they?€?re doing, this collects data and drives your decisions.?? The tools that we saw today here makes it so simple to go and look at what the data says and make a decision based on that, and then change it, change it again, and change it again as many times as you want.,And that?€?s the critical aspect.?? The data existed forever in the network, but with the Internet of Things today, we called it Internet four years ago, everything is connected.?? Everything?€?s been connected for like 150 years.?? Now all we have to do is collect that and do something with that.?? That?€?s the revolution.,For more insight, here is a separate conversation between Bonde and??Kolsky.,- See more at: 
Actuate recently invited four industry experts to discuss the future of creating the next generation of data-driven applications and how they are influenced by open source and freemium software.,Allen Bonde (,), VP Product Marketing & Innovation at Actuate, moderated the panel which included:,??I?€?ve said this at a couple recent presentations that ?€?fast is the new big.?€? When it comes to data, do you guys agree with that or not??? Is it worth more to have more data or to do things more quickly?,??The problem here with Big Data is that you can call it whatever you really want to. I don?€?t really care.?? But I mean yes, fast ?€? the whole concept of big data stems from the fact that we can process, store and manipulate data hundreds of times faster than we could before, because of evolutions in data management, data storage and processing power.?? So it?€?s not big data, it?€?s the same thing that we had before.?? We can just manage it better and faster.?? And that?€?s what ?€? when people tell me big data, the first thing I say is that you need real-time, because that?€?s what it means to me.,??Yeah.?? And I would add to that.?? I think on the one hand you have the experiences ?€? there?€?s an engineer from Wal-Mart that came out at one point and said, ?€?You know what??? More data beats better algorithms.?€??? And that?€?s something you hear data scientists say over and over again.?? Whether or not it?€?s true I think depends on the situation.?? But in general, yes, you want more data rather than less.?? That being said, to the point earlier, a lot of the data that many of the companies we deal with are looking at is not even remotely big data.,We?€?re not talking about petabytes, we?€?re not even talking terabytes.?? In some cases you can actually have data sets that are hundreds of gigabytes in size that actually give you huge input.?? One of the guys who ?€? was it Paulson??? There?€?s a hedge fund manager, and he was essentially trying to glean insight in terms of market performance by trying to correlate market ups and downs to taxi dispatch data in New York City, specifically on Wall Street.?? This guy?€?s got tons of data.,But it was the insight to say, ?€?This is something we can do with it.?€??? So faster, big or better, I don?€?t know, but I think it varies.,??Personally, I don?€?t think it matters whether it?€?s fast or big or anything else.?? It?€?s what you do with it.?? Anytime you put together charts where you have a bunch of squiggly lines that follow each other around, the first thing you have to think about is correlation is not causation.?? Correlation is not causation.?? Matching lines up, there?€?s a whole website that?€?s dedicated to correlating completely bogus things.,??You mean like the shark bites and ice cream sales?,??Exactly.,??Turns out that eating ice cream is dangerous.,?? You?€?re making a great point.?? It?€?s not the data, but what you do with it.?? I think that?€?s the most critical aspect, because one of the reasons we see things like this emerge is because most users don?€?t know what the data does is what to do with it.?? You know that you have customers, you know they have problems.?? You know that you have stores, you know that the stores are selling products to your customers.,You don?€?t know then that SCA-slash?€? ID ?€? slash?€? 22.?? It?€?s your customer name.?? And you don?€?t know need to know that.?? That?€?s what the idea is behind these improvements. It?€?s like, you know, we have much better databases that allow anybody to understand what the data is, and you can do it the first time.?? But again, it goes back to what he said.?? If you don?€?t know what you?€?re doing, it really doesn?€?t matter what data you see.,?? Let me build on this idea of the interface.?? There?€?s a lot of buzz around the consumerization of IT, for example, and the influence of consumer trends.?? When we have our consumer, our shopping role and how that sets expectations when we start to work, for example.?? We saw the global adoption of tablets, with the sort of merging and blending of consumer and business.???? I?€?ll address this to you, Loie, because again, you have this perspective of the consumer perspective.,How much is the consumer world influencing everything we?€?re talking about??? And not just pretty pictures and ease of use, but is it more than that, or is it mostly that?,?? Well, I want to add on to what they were discussing.?? It?€?s the visualization of data, because you used to have to have an analyst that you paid hundreds of dollars an hour to analyze the big data, break it down, tell you what you need to be doing.?? Now I think with the way we?€?re building tools, you can actually create a visualization of that data and/or allow individuals to carve it up however they need to, or their business needs.,I think that there?€?s a democratization of that, essentially, with these apps coming out, and being so much more user-friendly.?? It is kind of picking up on what the consumer population has to have in order to adopt consumer products, now that there are actually advances in the design of technology, and that it?€?s no longer okay to just have graphs and charts and numbers and code.?? We actually are seeing a change, a shift.,?? How much do you think the open source movement and the community has driven this innovation towards ease of use and ubiquity?,?? Well, I don?€?t know?€? And actually, I think it would not be fair of me and not accurate of me to say that open source has necessarily driven good design or usability.?? What I will say is that I think open source has really driven a bottom-up adoption cycle, and a bottom-up adoption cycle has profoundly influenced in terms of the way technologies are adopted, the way technologies are procured.?? Where, look, if I?€?m an enterprise company ten years ago, I could sell to a CIO, I?€?m always basically selling to one person.,What the product looks like isn?€?t all that important, because at the end of the day I just need to get this one person, and that person doesn?€?t care about things like ascetics usability and so on.?? Today it?€?s a lot different.?? Today we a market that?€?s really looking at evaluating products by month, and very much selling an iPhone, I don?€?t sell an iPhone to the CIO and he rolls that out.?? I sell an iPhone to each individual in the company.?? And therefore it?€?s a very different process.?? Things like design matter.?? Things like availability matter.?? Things like installation, ease of use, etc., all those things matter.,But it all, to me, comes back to the bottom-up adoption cycle, which has been driven in part by open source.,???? I agree with that.?? I forgot to say that I agree with you.?? But one other auxiliary point about the role of open source has played in innovation in big data, is that big data?€?s really one of the first major industry movements that came from open source first.?? Because the whole market segment called big data really started with the Apache Hadoop project, and Apache?€?s done a fantastic job of growing a community of projects around this particular domain.,And we see companies like Cloudera and Waterworks coming out of this sector.?? That entire innovation cycle was driven out of open source.?? So I think it?€?s really the first example that I can think of where a major new and novel innovation in the enterprise software market came first from open source and became mainstream directly from open source.?? So it?€?s a subtly different point than what you?€?re making.?? And a lot of that, again, it was bottoms-up acquisition.?? It was developers realizing the power of what they could do with these tools that made it happen.?? But it was definitely driven by open source.,For more insight, here is a separate conversation between Bonde??and Maxwell.,- See more at: 
Actuate recently invited four industry experts to discuss the future of creating the next generation of data-driven applications and how they are influenced by open source and freemium software.,Allen Bonde (,), VP Product Marketing & Innovation at Actuate, moderated the panel which included:,Let?€?s look forward a little bit.?? We?€?ve got a community been thinking about data, they?€?ve been thinking about application development. They?€?ve been thinking about customers. They?€?ve been thinking about engagement and driving in some cases virality, the ultimate sort of word of mouth application everybody has to have.?? What are one or two pieces of advice ?€? we?€?ll just go down the line ?€? that you would give to designers, developers, people who are thinking about creating the next big app??? What should they think about??? What should they not think about??? I?€?ll start with you, Mike.,??So my answer?€?s going to probably be a little bit off-topic.?? One of the coolest things I?€?ve seen done with BIRT recently was the demo that actually did with EuroTech at EclipseCon where we hooked up the Internet of Things components that we?€?ve got going on at the Eclipse Foundation with BIRT as a reporting dashboard tool, so we could do real-time analytics of the people flow through our conference, because we had sensors that were detecting as people were going from room to room.,And so I think coming up really soon, internet of things is going to be a big deal.?? There?€?s going to be an enormous amount of data generated from all these sensors that are going to be surrounding us.?? And the analysis of real-time decision-making that is going to be made possible by the combination of tools like IoT sensors and gateways coupled with analysis tools, like you get with BIRT and iHub, I think that?€?s going to be an absolutely, it?€?s a huge opportunity over the next decade.,????Yeah.?? I would say if there?€?s one lesson I would impart to developers building data-driven applications, it?€?s that in many cases the best outcome from a question is the next question.?? So what I mean by that is that whenever you build a report or whenever you?€?re putting together from a data-driven application perspective, you?€?re not likely to answer perfectly a given user?€?s question.?? But what you want to do is make them think and say, ?€?Hey, that?€?s actually something I didn?€?t know.?? Let me explore that,?€? and give them the opportunity to follow up with the next question, and the next question from there, because that?€?s when you get to a level of insight that simply many of your users will never have gotten before, because they have not had the ability to ask all the questions.,They?€?re presented with a [stacked] report and asked to make decisions off that.?? So think about the ability to generate the next question.,????My advice would be, in my experience working with a couple founders at startups, the ones who invest in user experience and invest in their brand and what their audience will adapt to or adopt, and actually fall in love with, are the ones who actually see the greatest amount of adoption.?? And why I say that is because I?€?ve seen in some situations where that?€?s the last thought, it?€?s an afterthought.?? And when there?€?s actually a balance between the user experience and all the technologies that we?€?re developing, I find that the company and the brand do much better.?? They perform better and see results quickly.,So I think that?€?s something that can often become an afterthought, and it really should be right at the forefront.,????I would say I agree with everything you guys said, but I would take a different approach.,What?,????Well, I?€?ll take a different approach, because you don?€?t want to build the next big app.?? You want to build what your customers come and tell that they need.?? When Groupama [a French insurance company] came up with an application idea or application, which was innovation that every insurance company today has, nobody had done that.?? Nobody had thought about it.?? They opened the application data and said, ?€?Do you have a person who was in an accident, who was next to the car that has a computer in their hand and is capable of logging location, taking pictures, and videos, and all these things??? Why don?€?t we use that, why don?€?t we leverage that??€?,Like Starbucks did.?? They said, ?€?Everybody has a reward card.?? Let?€?s make an app that you don?€?t have to carry money, that lets you pay for it, right??? Now everybody has the same.?? But you?€?ve got to take a different approach.?? Don?€?t look just look at the data say that is all that you have. But when your customers come in and tell what they need, then deliver that as the next big app, and that?€?s the one that?€?s going to actually become very useful.,For more insight, here is a separate conversation between Actuate Director of Technical Marketing Mark Gamble and Milinkovich.,- See more at: 
The accuracy of a model is controlled by three major variables: 1). First and foremost the ability of your data to be predictive.?? There is an unknown and fixed limit to which any data can be predictive regardless of the tools used or experience of the modeler.?? 2.) The experience and skill of the modeler.?? 3.) The tools selected.?? Some tools are designed to give very quick if somewhat approximate results.?? Other tools are inherently more accurate if somewhat slower.??,Models can frequently be improved through better selection or preparation of the data including the addition of appended data.?? However, even when the data is exactly the same, the selection of the modeling tool can be critical.,Many modelers tend to utilize only one tool in creating their models, frequently the one they are most comfortable with or were initially trained on, logistic regression, neural nets, decision trees, Bayesian classifiers, support vector machines, or genetic programs.?? Not all tools create equally accurate answers when applied to the same data sets.,How important is accuracy??? This case study illustrates that a change in fitness of only 0.01 point can mean a financial improvement of nearly 8% in campaign ROI.?? Greater increases in model quality will translate into higher percentages of financial improvement.?? The benefit each user actually receives will depend on how much the model can be improved and the financial details of the offer, but this example should make one thing clear, small increases in model quality can translate to large increases in financial performance.,This example is based on actual data from a major technology and services company pursuing cross sell or up sell opportunities with their existing customers.?? It would be equally true of initiatives aimed at new customer acquisition or customer retention (churn/defection prevention) campaigns, or to any of the other major uses of scoring (regression) models such as fraud detection, credit scoring, or billing review.,The data is from a large direct mail test where the overall response rate was found to be 1%, very typical for this type of campaign.?? In our simplified example we assume a full mailing to all available targets would be 250,000 pieces at a cost of $3.00 per mailing, and with a gross profit of $300 per successful sale.,This means that a mailing to all 250,000 targets would require an investment of $750,000 and would return $750,000.?? Most business managers would regard this as a bad investment and would elect not to conduct the full mailing, counting the cost of the test mailing as the sunk cost of an unsuccessful promotion.,To illustrate the difference that small improvements in accuracy can make, we developed two models, one with a fitness measure of .195064 and the other with a fitness measure of .182995, only .012069 between them.?? The fitness measure is the remaining unexplained difference between the actual data and the model.?? Lower scores are better.?? A fitness measure of 0.00 means the model completely explains and predicts the actual data so both these models show good and useful predictive ability, explaining more than 80% of the difference between the actual results and the model.,In the table below, the business manager evaluates the less accurate of the two models and finds that his mailing can yield a good profit, $163,043 if he only mails to the top 50% of the list.?? The model has scored all prospects from 0 to 1 based on their likelihood to buy, and after evaluating the net profit (projected profit from sales less the cost of mailing) for each decile of the list (a decile equals 10% of the list, a very common division for this analysis) sees that the bottom half of the list is a money-losing proposition but that the top half is profitable.?? This table is known as a lift analysis.,??,However, if the manager had the benefit of the better model (table 2), and better by only 0.012 points in fitness, he now forecasts a profit of $175,679, an improvement of 7.75%.,??,Small improvements in model accuracy can make big improvements in financial outcome.?? Be sure to ask the question:?? Is this really the most accurate model that can be created from my data?
Hadoop has been a great solution to analyzing big data, but is too much for most companies. Most don?€?t have petabytes of data, or a team of engineers and PhD?€?s to set up and maintain a cluster. So what options are left for these companies? Until recently..not many. There are a few startups looking to change this.,A distributed cloud platform built independent of Hadoop, , makes data science accessible to everyone who can program. They currently support Ruby and Python, and will support Javascript in the future. To help users get started they offer web data sets for analysis and a trial period to test the product.,Currently in Beta,??, makes collaboration easier with their platform. They are building a web platform to make collaboration on projects and integrating with existing data science tools easy. They also support Hadoop integration, making it easier to use their infrastructure, but without deep knowledge of the Hadoop ecosystem.??You can sign up for an invite to test their platform., is a cloud platform for rapidly deploying predictive models. ScienceOps supports Python and R, hitting the two most popular languages data scientists use. ??They are strongly focused on model management, even making version control easier for users. You can sign up to try it on their public sandbox.,We?€?re seeing an explosion of interest in data science. There are new resources being posted and published, communities (like this one) have formed and help to define the field, and now platforms are catching up. All of these platforms focus on rapid deployment and on demand scalability, filling the space Hadoop leaves in the field. They are the next tools developers and data scientists will look to for their big data processing needs, and will lower the barrier for companies using data effectively.
We want to create a repository of apps / APIs that provide forecasts and predictions for a broad class of questions that concern all ??human beings. We would also like to create our own predictive API - so if you are interested in this project, contact us at vincentg@datasciencecentral.com, we might have a gig for you! ,??of what we did in the past. We also designed apps for stock market predictions (see ,, page 195-208) and copyright infringement detection.,Basically, we see , that are based on data science:,Here we are interested in #1, though we are also in the process of designing a generic predictor based on ,??and ,, with source code available at no cost for everyone (open intellectual property).,This is a starting point. Please let us know (in the comment section below) if there are other apps you'd like to share, or provide a link. Here are a few ones to begin with:,I would imagined many of these apps would be based on data collected via crowdourscing, using decision trees for the predictive engine. I would think that some of these apps would be offered for free, with monetization via advertising.??,Finally, ,, consisting in selling or licensing refined data (that you collected) or predictions.
Post the financial crisis, banks in the US have faced increased regulatory scrutiny that has resulted in broader and tougher regulations. Bankers are fully aware of the investments and efforts they have to put in to comply with these regulations. Consequently, compliance function in banks is evolving towards a broader risk canvas that is now seeking tighter coordination between the first and second lines of defense. This poses new challenges to banks ?€? from being compliant to getting the optimal returns from their investments. ??The million dollar question on everybody?€?s minds is?? - How are banks rising up to this challenge?,??Recent studies have highlighted the enormity of the challenge this has created for banks. For example one study by Accenture shows that 92% of banks will be compelled to increase their compliance spend in 2014. In another report by Continuity Control, the new regulations have imposed an additional financial burden for just the last quarter (Q4) of 2014 is $241 million.,??Enhanced regulatory scrutiny may be a necessary evil to watch over the much-maligned banking sector, but has spawned its own unintended consequences. ??The huge anxiety of banks to be compliant and avoid penalties and the resulting hike in compliance spend has and will continue to impact ROE and profitability of US banks for years to come.,??How are banks responding? A whole ecosystem of changes is taking place in this area. ??Banks are deploying analytics to help them meet the challenge and enable them to make the right data driven decisions. Three important changes are on their way.,?? First, bulk of the new spend has gone towards upgrading technology platforms. Banks are integrating extant analytical and compliance platforms so they can deploy data mining and analytics to get the right insights. ??For example, analytical models are being deployed to proactively identify and monitor UDAAP compliance in customer engagements / acquisition.,??Second, Banks are bringing new structural alignment between first and second lines of defense.?? Compliance is now a broad based enterprise activity that will report to the Board or CEO and will include operational and business risk professionals. This is a significant change because in my view, it facilitates wider & deeper use of analytics to help banks stay compliant and out of regulatory trouble.,??Third, data silos ?€? the usual suspects - are posing roadblocks for banks in their new quest to be compliant. Integrating structured and unstructured data for analytics is also an urgent initiative at banks. Banks are aware of these challenges - these are known devils anyway for some time now; but a renewed urgency backed by fat budget approvals is evident.,??Banks need to keep a watchful eye on the expanding compliance management function. Technology upgrade and structural changes, while necessary, are only part of the solution and not a panacea by themselves. Banks need to look at compliance as an enterprise wide culture that every associate lives by 24/7. In an era where changes are swift, where disruptive innovations are continuous and almost a way of life, the best insurance for the banks is an open mind to change and adapt to win the customers?€? heart. In a way, it is the same old wine, but in a new fancy carboy.
The celebrity of Toronto's mayor has certainly drawn a lot of attention to the city in recent years. Several candidates are now running for Rob Ford's job. Since the mayor is currently undergoing treatment for cancer, he decided to withhold his candidacy in the upcoming municipal election. Being a longtime resident of Toronto, and being aware of the city's wealth and poverty, I'm always interested in how these competing needs play out when it is time to vote. ??Consider the interesting dynamics of elections on a more broader level: people carrying different needs, desires, and expectations assert their presence in society by placing a tick market on a ballot. The ballots are then tabulated to determine the distribution of power. An alien species from another galaxy, perhaps with limited knowledge of humans, might try to gather insights from society by examining the election results. It would be quite difficult to explain to these intergalactic creatures the many dimensions behind the casting of ballots. How can a person possibly hope to deal with problems such as social inequity, homelessness, and poverty by placing tick marks on a slip of paper? Such metrics are quite detached from the lived realities of people. Elections serve as a tool to support the existing power structure and assert authority. Their effectiveness in terms of dealing with problems is debatable. In this blog, I hope to clearly differentiate between the idea of a data proxy (i.e. metrics) and the underlying data events (e.g. lived realities that people face). I will then discuss how data can be weaponization through the deliberate and systematic use of proxies. While it might seem that proxy-phenomena differentiation and data weaponization are two separate topics, the latter is actually made possible by the former as I will explain in some detail.,Mr. Howard, a former math teacher of mine, once said that the decision to use symbols to represent quantities was an important development in human civilization. He was referring to the use of "2" to indicate two items; "3" for three items; and "4" for four items. When we say that we have 8 items, we really mean that we assert a value of 8. In real life, it isn't unusual for our asserted value to differ from reality. A bag holding 8 grapefruit might actually contain only 7. For me, I sometimes find bad grapefruit included in the bag; and although the package might not distinguish between fresh and rotten grapefruit, obviously I feel short-changed if the bag contains bad fruit. In any event, though this symbolic abstraction, I can boldly write that XYZ Car Dealership has 10,000 cars - just hypothetically. The use of symbols helps to facilitate our detachment from reality. We accept that "8" represents 8 items for the sake of argument; we assign 8 to the symbol 8 although it could have been assigned to W or Q. If the closing price of a stock is $14.50, what does this represent? There was a barking dog; crying child; transit delay; lady misplacing her apartment keys; maybe a snowstorm. What are these things? They possibly contributed to the $14.50. It isn't an unreasonable assertion. In real life, $69.95, the amount I paid recently for a vacuum cleaner, includes the cost of raw materials, administration, storage, utilities, shipping, financing, marketing, and so forth. I am saying that a metric is often something simple representing a much higher level of complexity.,In prior blogs, I referred to data proxies as the "head" of data. I logically described data events as the "body" of data. The body is much larger than the head, in case it isn't apparent on the illustration above. My objective in the image is simply to loosen the bonds between body and head; part of this is to facilitate later weaponization where decapitation will occur. We participate in data decapitation all the time. I recall not that long ago, there were several deadly factory fires in the garment industry. We import cheap clothes rather detached from the enormous social context: uneducated women and children working long hours for little compensation and apparently minimal health and safety protection. There is a vast amount of data concealed behind the proxy. Are these people and their interests irrelevant? When a tanker full of oil starts leaking crude over pristine waters, we often measure the positive economic benefits apparently oblivious to the ecological ramifications. This is due to the proxy-phenomena disassociation. I'm not the first person to mention this pervasive socio-economic alienation. However, I am probably more unique in terms of my focus on data alienation.,If we had a metric such as consumer sentiment, clearly this quantitative expression is based on a substantial body of data. It is necessary to differentiate between the proxy and its events. The proxy is the thing that we would like to influence. So any attempt to improve consumer sentiment would involve addressing the individual components of the index. The fact that consumer sentiment might be increasing is actually pointless without knowing the connection to its body of events. Sometimes, the need might not be to influence the proxy but simply monitor it or improve our understanding. For instance, another type of proxy is a geographic area having specific boundaries. The proxy isn't merely a means to convey a simplified assessment of the underlying phenomena occurring within the spatial parameters; but it helps us to divide and arrange the reality in a cohesive manner. We might conflate the outcome (the assessment) with the management of data (the division of facts intended to further our understanding). When we say that the value of something is 8, this generally means that 8 is the value that we would like to assert. But we can also say, for instance, I have 2 terabytes of data to associate with 8. It is possible to infer value over data assignments.,There has been a tendency to try to tackle problems by studying metrics; but the lack of body can muddle remedial efforts. For instance, studying the number of workplace accidents doesn't actually offer any insights in terms of reduction or correction. Yet we often become aware of problems through their metrics. I think that this can sometimes lead to the illusion that the problem is in the metrics. Thus, a government strategy to create jobs might be to pay one person to a dig hole and another to fill it. Place these people on a deserted island, and the pointlessness of their activities should become apparent since they would both starve. Digging a hole merely for the next person to fill is unproductive. The embodiment principle is that data generally exists as head and body, and these parts can sometimes become detached. Conversely, the data is normally in the form of either head or body, and these parts can sometimes be attached. My emphasis really is that the parts don't necessarily exist connected. Embodiment involves structurally associating events with metrics. This might be described as embodiment or capitation.,Since I tend to use the same proxies repeatedly, my personal database maintains a "proxy distribution file": this file currently contains about 60 proxies. Events are distributed in bulk to all of the proxies at least each day. There is a master proxy called a "main" that ensures the event-data has a place to stay even in the absence of other proxies. During those instances where I might want to make use of a foreign proxy, my system can generate a list of all valid batch dates to match against the foreign proxy. There is then a process called "digestion" where the system distributes event-data from the master to the foreign proxy. In this manner, it is possible to associate massive amounts of event-data for example pertaining to a public transit system to a proxy such as fares purchased. I would say the most sensible application of this technology is to "scout" the surroundings. This is a peaceful path for those that wish to use data to understand the world around them.,The coding behind assignment is straightforward. In fact, there is almost no coding. It is more an issue of ontological reasoning than coding. We wouldn't question the rationale of having a text file named "A" containing "8" thereby resulting in "A = 8." On the other hand, depositing the King James Bible in A might be a bit confusing. Placing the bible in a file named 8 probably makes no sense at all. If we distinguish the contextual evaluation of 8 from its hierarchical functionality, we discover that the contextual meaning is actually an assertion of proxy; but the hierarchal function supports the containment of events. Our data is 8: the data can carry both intended and actual meanings. The question really is the extent to which the items contained in 8 (articulating truth and reality) are relevant to the instrumental framework giving rise to 8 (projecting perceptions).,I will soon be discussing the replacement of the instrumental aspect of data (its proxy) rather than the underlying events. I want to point out that proxy replacement is something that can be done in order to determine which proxies best fit and explain the phenomena. I call this the metrics of phenomena. As the phenomena changes so too can its metrics. Although we might not fully comprehend the greater reality, we nonetheless can form a relationship that takes it into account. On the other hand, proxy replacement can merely define the reality in which we wish to perceive the phenomena: for example, I would say that this "narrowing" frequently occurs in relation to performance evaluation. "Performance" cannot rely on floating metrics since this would render the assessments non-comparable. I describe this alternate use of proxies, which I believe is prevalent in society today, as the metrics of criteria. Yet another reason to replace proxies is to gain advantage or control over the thing being assessed; this application of replacement is the focus of this blog pertaining to the weaponization of data. In this case, the objective is to understand aspects of the phenomena that can do it the most harm or help the practitioner gain the greatest benefit.,Warping is an art - mostly because I don't have the hardware to fully satisfy the processing requirements. In order for me not to waste time and resources, I am forced to go about my warping thoughtfully and selectively. Warping is the term that I chose to describe a process of continuous proxy replacement to test for opportunities and vulnerabilities; this is for the purpose of gaining material or strategic advantage over something or somebody. It represents a kind of data weaponization by virtue of intent and also by the aggressiveness of the methodology. According to my own observations anyways, some individuals habitually go about this process although perhaps not deliberately and probably not using algorithms. From an algorithmic standpoint, warping simply involves assigning large numbers of events to metrics as I have been doing for some time. However, the choice of metrics is much more varied; and presumably the ontology behind the recognition of events is more focused. For example, I would be biased towards events that I can clearly control, which is generally not the case for me now. It is important to note the following: although it might be possible to warp, this doesn't mean that worthwhile insights are necessarily forthcoming. Indeed, there might be no firm relationship between events and a particular foreign proxy. A highly systematized approach is simply to keep grinding the data continuously - essentially mining - even to gain the slightest advantage.,I have an interesting "real life" example to share with readers. I have been collecting my own health event-data for about 15 months. I normally associate these events with health-related proxies to examine different concerns such as sleep perceptions, eyesight, and weight. Just to emphasize the "structural" nature of warping, I will replace my sleep perceptions with a proxy derived from the Toronto Stock Exchange index. I consider this a bold and perhaps rather nonsensical thing to do at least on the surface. The ability to change the proxy means that I should be able to determine the relevance of event-data to the market. I would not expect to find any relevance; but I should be able to make the determination. Well, this is not the sort of thing that just any software can do. I make use of a research prototype specially designed to assign data-events to a given proxy. While reading about my health data, please consider the wider implications of this methodology perhaps in relation to organizational needs.,I downloaded closing prices for the TSX from an online source: the period is from 2013-06-24 and 2014-09-19, which coincides with entries on my health database. The rather unexciting image of the trading pattern appears below. I decided to use numbers from the stock market mostly because it is so accessible and something a lot of people can understand or relate to. A market index contains a basket of stocks, which makes it less volatile than an individual stock; nonetheless, there are day-to-day fluctuations. Perhaps a more coherent application of warping would be to link a company's business events or aspects of its operating environment to its share price fluctuations.,I chose not to use the closing price itself as a proxy. Clearly the trading pattern above exhibits considerable stability. The stability of the price represents a problem for anyone trying to test cause-and-effect: the "effect" part of the evaluation would be fairly non-repetitive, resulting in fewer opportunities to confirm causality. I chose a measurement of volatility as an alternative to the closing price: C2/C1 - 1, this being the current closing price divided by the previous close less 1. In contrast to closing price, volatility as shown on the next image contains some repetitiveness. I set all of the resulting numbers side-by-side in order of volatility. The pattern below is derived from actual trading data. My objective at this point in the blog is simply to show that a foreign proxy can be transposed over another. In this transposition, the entire proxy-phenomena relationship changes. What events were relevant using a health proxy might be meaningless in relation to a volatility proxy.,The next image represents my end-goal or objective. The foreign proxy has now replaced the health proxy. I created a contextual gradient on the y-axis: in the case of the current example, this "gradient" is made up of volatility levels. This is not to say that the volatility level is its gradient value; but rather, these levels give rise to gradient values through the use of an algorithm. The algorithm determines the relevance of events using the different volatility levels. In light of the above distribution, presumably an event might coincide with volatility purely by chance; however, the likelihood of coincidence declines as more events occur over time. Interestingly, at the end of the process, I still found myself with this curious situation where certain health events still seemed connected to particular market fluctuations. I will reveal additional details at this point: the research prototype had been set with a 1-day lag so I might "predict" next-day fluctuations from present-day health events.,The above illustration shows, most of my personal event-data has no apparent relationship to the market: the distribution is quite close to the zero line. Moreover, in terms of those events not near the zero line, a fair number of them are non-repetitive sporadic events that might logically be associated with market fluctuations purely by chance. This then leaves with me with the oddballs that appear to precede the fluctuations. Since I am using health event-data in relation to a market proxy, there are no firm lines of reasoning explaining how the events might be related to the market. Further, the fact that there might be a relationship doesn't mean it is something exploitable; because as I mentioned earlier, the database contains all sorts of event-data where I might have no clear control. Even if I did have control, this does not mean that controlling the event-data then leads to control over market fluctuations. I immediately recognize the lack of causality in this particular proxy-event combination. But I want to suggest that sometimes, depending on the exact selection, there might in fact be some level of causality.,Nothing about my personal health events can be logically identified as antecedent to the market in terms of influencing fluctuations or closing prices. But let's consider a principle related to causality that I call the "double metering effect": both the market and I might behave in a coincident manner to certain deeper but poorly articulated phenomena. I share a relationship with others around me, being exposed to the same world events and having certain predispositions. We buy the same coffee, eat the same donuts, and probably watch the similar television programs. Even if we didn't, there might nonetheless be a stable relationship. I know there are all sorts of valuation schemes such as the present value of expected income streams; relative-market valuation differentials; sentiment and momentum. I'm not dismissing any of these perspectives. I merely suggest that a market is sometimes a market because there is a market - that is to say, a bunch of people with certain desires and expectations obtaining specific things perceived to be important in their lives. Perhaps many of them don't come with financial calculators. There are day-to-day shifts in valuations that simply cannot be explained using conventional methodologies.,This reminds me of a funny story about science fiction movies. When I was in high school, I frequently felt out of place in English class because science fiction wasn't really taken seriously. I don't have any issues with Great Expectations or Macbeth. However, these days it seems like many books, films, and television shows are meant for people like me. When I invest, I assume that others might rush ahead of me or follow behind; indeed, I might be the person lagging. There is a flow. It is a social flow. I find myself captivated seeing dozens, at times hundreds of birds flying together almost like a convective current. I have such respect for these little societies that form among wildlife. I'm saying that there might be coincidences of predisposition rather than pure happenstance. It affects the simplest creatures. Perhaps people aren't much different.,Christopher Columbus over the course of his journeys wasn't just sight-seeing. These expeditions represented significant capital investments. Irrespective of his original destination, at some point he decided to search for gold and slaves. He participated in a rather violent form of exploration. Data-gathering has likewise been associated with discovery, but clearly it can sometimes serve to exploit others. It's no wonder why intelligence must occur clandestinely. I'm not actually questioning why people collect data. I just want to emphasize that there is an "aggressive" form of discovery; this can sometimes conceal important aspects of surrounding phenomena because it was never meant to truly enhance understanding. On the other hand, how can an organization be blamed for collecting the wrong information or for doing it poorly or in a malicious manner when it might not understand its own circumstances? Lack of careful thought can be destructive; it can cause an organization to harm itself and others.,The most aggressive form of discovery involves systematically combing through proxies in search of opportunities and vulnerabilities - both in others and oneself. I believe that generally speaking, an organization isn't faced with the question of whether or not to be aggressive. But rather the organization might be at a total loss - literally confused and disoriented. I don't consider it unusual for an organization to lack knowledge about itself. During periods of instability, perhaps when confronting market barriers, existing intellectual capital might offer little guidance. I consider the use of both indigenous and foreign proxies worthwhile especially for organizations that have little sense of market placement. I'm big on environmental sensitivity. But no organization wants to know about the environment per se; rather, it needs to understand how to strategically place itself. Almost inevitably it is necessary to contextualize at least partly from foreign proxies. The question of how one might go about doing this is addressed by warping.
We have produced a 90-second video (,) showing a 'random walk' (a particular case of a Markov process) evolving over 400,000 steps. Figure 1 below shows the last frame (out of 2,000 frames, each one with 200 new steps).,A basic, two-state (going up or down), one-dimensional , is defined as follows: You start at time t=0, walking along the X-axis (representing time). At each iteration (also called step), you move up with probability p, and down with probability q, along the Y-axis. The Y-axis could represent gain/losses in a gamble (throwing a dice), stock market gains etc.,When p = q (same chance of moving up or down), this is called a ,. In two dimensions, you add two additional potential moves: going left or right.,Markov processes is a modeling tool extensively used in data science. In particular, it is used,:,You will need a very high quality random number generator if you want to produce billions of steps, to learn the long-term properties of these processes. Such random generators ,??(also, this ,??is worth reading).,In Figure 1, we are dealing with a realization of a random walk starting at x=0, y=0 (top left corner). Even after 400,000 steps, we never go above y=0 or below x=0, except at the very beginning. In short, we seem very quickly stuck in the South-East quadrant. This is clearly shown in figure 3. Figure 2 also shows that me make great progress in the South-East quadrant, reaching x values as high as 200, and y values as low as -150, throughout the 400,000 steps. Yet, after 200,000 steps, we seem stucked: no new extreme is reached. This has to do with the peculiar??,??(or distribution of records).,In theory though - over trillions of trillions of trillions of trillions of steps, over an eternity indeed - you are going to visit all quadrants, each quadrant being visited infinitely many times, reaching any arbirary large or negative x and y values. How weird!,In figures 3 and 4 below, the horizontal axis represents the time (measured in steps, from 0 to 400,000). The vertical axis represents extremes (biggest maxima, minima)??,??Extremes are either for x or y values. At each step, we move in both directions (x and y) by a random quantity uniformly distributed over [-0.5, +0.5]. The random small moves (in both directions) are independent. The mechanics are easy to understand if you read the source code (see next section).,Here are the ,, regarding this type of processes:,The figure below attempts at answering some of the questions in the one-dimensional, integer-value case, where at each step, we move up by one unit (+1) with probability 0.5,or down by one unit (-1) with probability 0.5.We simulated 10,000 realizations of such random walks, evolving over 5,000 steps (horizontal axis in figure 4). And we computed average, median, 25- and 75- percentiles for the all-time maximum M(n) for each of the 10,000 random walk realizations, attained after exactly n steps (n=1, ... , 5000; vertical axis in figure 4). The expectation for the maximum is very well approximated by the function,E[M(n)] = SQRT(n/2). ??,Note that the average (green curve) is always above the median (red curve), just like for salary distributions. The computation of E[M(n)] would greatly benefit from using a distributed architecture such as Hadoop. With my 10,000 x 5,000 = 50 million data points, I was able to approximate E[M(n)], and in particular its factor SQRT(1/2), up to only 2 decimals. Using 1 million simulated random wlaks, a great random generator, 50,000 steps for each simulated random walk, and Hadoop, one should obtain much more accurate results. But for practical purposes, my little experiment is good enough to conjecture that ??E[M(n)] = ,For those interested, see below the computations (script) performed to produce figure 4. This is a direct application of our ,??algorithm:, , , , , , , , , , , , , , , , , , , , , , , , ,All the source code (script to produce the data set used in the video, and R code to produce the video) ,.,For details about how the video was captured from the R GUI using an open-source screencast tool,??,. ??At each iteration, the 200 points in the current frame are displayed in red (using??,, to help you visualize the progress on the screen.??,:
Andy Stanford-Clark (,) is the Chief Technologist for Smarter Energy in IBM?€?s consulting business in energy and utilities for IBM U.K.?? He?€?s also an IBM distinguished engineer and a master inventor, ?€?Which means, I?€?ve got quite a lot of patents, and I invent cool things for IBM,?€? he quipped. Stanford-Clark is also known for creating the IoT connectivity protocol,??,.,Stanford-Clark is scheduled to keynote at the??,??event where he will discuss the importance of the Internet of Things to the development community. Developers from about 30 countries??are expected to attend the event. Most work for companies and enterprises that build commercial products or internal tools based on the??,, such as??,.,Actuate got a chance to catch up with Stanford-Clark and chat about the Internet of Things and where developers should focus their efforts.,???????????????????????????? First off, what?€?s your general overview of the Internet of Things, and what kind of impact that it is having on the businesses that you?€?re working with?,??So, I?€?ve actually been working in what we now call the Internet of Things for about 15 years.?? Back in 1998, I moved back to the U.K. from three years in the U.S., and started working on remote telemetry systems for oil and gas pipelines for SCADA (supervisory control and data acquisition)??systems.?? And it quickly became apparent that we needed to create something which brought that industry into what was then the modern world, with TCP/IP, and packet-switched networks and so on.,So, I developed a protocol called MQTT.?? But that was something I developed in 1998.?? And the good news is, this year, it?€?s about to become??,??standard.?? So, finally, it?€?s made it through all the hurdles and the amount of market penetration and so on to get to a state where it?€?s standardized.?? That?€?s very exciting for me.,But, along the way, over 15 years, we?€?ve called it many things: remote telemetry, pervasive computing, ubiquitous computing (ubicomp),??,???€? even M2M [machine-to-machine] is still around.?? IBM came up with the term Smarter Planet about six years ago, where we talk about instrumented, interconnected, intelligent devices in the world.?? And that was effectively our name back then for what is now the Internet of Things.?? That term has taken over, and it?€?s become the sort of lingua franca for all that stuff.,So, largely it?€?s the same things as we?€?ve been doing all along, but a shiny new name for it.?? And the Internet of Things is the current buzzword du jour for that technology.?? So, nothing new here, but in terms of the penetration, and people?€?s understanding of it, and the divergence of products, and services, and thinking, and ideas, and companies all over the world, wherever you look, that?€?s really exciting.,And I think it?€?s really snowballed into something which is kind of on everyone?€?s lips.?? Everyone?€?s thinking about it, and people are understanding where it fits into their world, and what the benefits of it are, and how you can integrate data from lots of different senses to provide better services for the public, or for your clients, or whatever it is.?? So, that kind of sets the context.,???????????????????????????? So, where do you feel the tipping point has come for everybody to now acknowledge IoT as lingua franca? Do you have richer conversations now that people are getting their heads around what Internet of Things might mean to their organizations?,Yeah.?? So, I think the tipping point has really been the commoditization of technology.?? So, when I started this stuff for real, I started doing my own home automation and home energy monitoring stuff, which was about 10 years ago.?? I was kind of soldering all the circuit boards, and, you know, inventing the sensors, and linking it all together by hand, whereas, now, you can buy off-the-shelf, very cheaply, a home energy monitor, and link it up to the Internet, and get the data from it in a nice JSON feed, or MQTT data, or whatever it is.,And so that really becomes the tipping point.?? So, we moved from the domain of the rich geeky guy to anybody to be able to go out and buy this stuff, and start to integrate stuff into their homes without needing a PhD in computer science or electronics to get it all working together.?? So, that?€?s really been the tipping point, from my point of view.,In terms of richness of conversation, first of all, people are starting to look at applications in a broader context than what they did in a traditional client-server or web e-business. They are starting to think about an agenda around mobile computing, social media, the analytics that wrap around that, the idea of hosting services in the cloud ?€? as part of the system is kind of out there in the field, and the other part is up in the cloud.,[Because of cloud computing],??you just have this huge ubiquitous connectivity to get to it. ??That broadens the scope for people, thinking about how they interact with their customers, their clients, their employees, the rest of the world.?? And people are starting to think about, ?€?What is my Internet of Things agenda??€??? ?€?What?€?s the strategy for the next few years??€?,So, rather than building, say, an app to access your banking system, you start to think in much broader context about, how does the Internet of Things include mobile and start to become integrated into the whole business context.?? And to my thinking, that becomes real, when a real-world event becomes a business event in the enterprise.?? So, something kicks off a work process, or a transaction; something in your enterprise messaging system, which is caused by some event picked up from some sensor, or an RFID tag read, or a person?€?s phone entering a geo fence, or whatever it is, out in the real world.,?????????????????????????? Your expertise has been within the energy systems, and utilities, and such.?? Anything specific there that?€?s made a shift now that people are connecting the dots for the Internet of Things?,Yeah, very much so.?? Now I see Smart Energy as being sort of an application of the Internet of Things.?? So, I happen to have specialized in the energy sector after the last few years.?? That in itself is a sort of subset of Smarter Cities, which then is part of the Smarter Planet kind of agenda.?? So, it kind of fits nicely.?? It?€?s like Russian dolls inside each other.?? It?€?s all Internet of Things, really.,And the interesting thing from my point of view there is that the smart grid that we talk about isn?€?t one thing; it?€?s a whole load of things, a really heterogeneous mix of sensors, actuators, people, things, components from different manufacturers like solar panels, wind turbines, electric vehicles, grid storage mechanisms.?? All that stuff, in order to operate holistically as a grid system, all needs to be instrumented, interconnected, and then use intelligent algorithms to model the activity and behavior of those things, bringing in information like weather forecasts, and demand forecasts, and so on, and run the whole thing as one big holistic system. Which, of course, means you?€?re going to have sensors on everything that?€?s then all connected into other central systems, or, more likely, federated hubs of information to make intelligent decisions about how to operate components on the grid in order to make the whole thing work as one big happy system.,To my mind, the smart grid is a classic example of how the Internet of Things needs to come together across this really heterogeneous mix of different manufacturers making different devices, in order to make an ecosystem operate holistically.,???????????????? Can you provide a concrete example of how IoT is succeeding in this today??? Do you have a personal favorite, something maybe you?€?ve worked on, or something that a partner might be doing?,??In terms of IoT at scale, I don?€?t think I have any good examples, but one area very close to my heart is home energy monitoring and some of the sort of spinoff things you can do within the home to get further insight.?? So, for example, if you?€?re monitoring the appliances in my house, you might notice that the TV?€?s on or the kettle?€?s been on to make a cup of tea or something, and that?€?s kind of interesting, but not particularly valuable.,But say I?€?m monitoring my aging parents, and their system?€?s wired up as well in their house, and at 10:00, I get a text saying, ?€?Hey, your mom and dad didn?€?t have a cup of tea yet today; maybe you should give them a call.?€??? So, you start to get into this sort of remote monitoring of elderly or sick people who have difficulty coping with normal life, being able to, not monitor them electronically, but, to get a head?€?s up when something?€?s maybe amiss, so that you can intervene more rapidly.,I think there?€?s huge potential there, in terms of if a window?€?s been left open, maybe if the front door opens in the middle of the night; maybe something bad is happening.?? Just to be aware of those things happening.?? If you?€?re away from home; say you?€?re monitoring your holiday home, if you have one of those, to just find out what?€?s going on, to build that picture of information derived from the data.?? And it might be completely innocent, or it might be something that you actually need to know about.?? And rather than getting deluged with tons and tons of information, little data points, you actually get meaningful notification.,We term it actionable insights.?? So, this means you take data, transform it into information and knowledge about what?€?s happening.?? And then if something important happens that you should know about, we create an actionable insight, which is maybe a text alert, or a tweet, or a red flashing light in my room, or maybe a notification to my car dashboard, something which tells me there?€?s something happening that I should know about.?? And that, to me, is a really exciting, sort of multidimensional data opportunity for the Internet of Things which is really just emerging at the moment.,???? If the Internet of Things is fueling even more of these actionable items, what part do you feel that visualizations and dashboards can play in the success of those IoT developments?,??So, what I?€?m seeing is, the dashboards and visualizations are extremely important to understand the data in the first place.?? Remember, I talked about the three ?€?I?€?s of instrumented, interconnected, and intelligent.?? The first step in intelligence is to know what?€?s going on.?? And we found visualization in various forms, from traditional graphs, through ambient devices ?€? a glowing orb device in my house that shows me red, amber, or green, how much power we?€?re using ?€? through to things like tweets, which also are a form of visualization of information in textual form, through to banks of multicolored LEDs which have a picture of data rippling across them to give you real-time information.,All that stuff gives you an understanding of what?€?s happening.?? And I found it very valuable to then develop the algorithms so the computer can sit watch those dashboards and those lights for me, to give me those actionable insights, to send me the notifications.?? And so it?€?s just kind of the first step in understanding the data, to look at it, visualize it, graph it up, and see what?€?s happening.?? And then you can say, ?€?Ah, okay.?? We?€?re seeing that kind of spike, or that kind of ramp, that kind of event happening.?? I can see how to program the computer, to define some rules, to look for that kind of thing.?€?,In a wider context, being able to see what?€?s happening is extremely important, and converting the data into pictures.?? I use the horrible clich?? ?€? a picture?€?s worth a thousand words ?€? but it really, really is.?? And watching the visual information is far more likely to show the anomalies in??the information that you?€?re trying to see, if it?€?s in some kind of visual form.,??,- See more at: 
Data science is the result of a new paradigm taking place in IT. The question was raised recently, and here I explain how and why data science is part of this new paradigm, and not recycled material.,Many data science techniques are very different, if not the opposite of old techniques that were designed to be implemented on abacus, rather than computers. These new tools are often model-free.,For instance, new tools include,Indeed, old techniques such as logistic regression and classification trees don't even belong to data science, more stable techniques are used in data science. You can find many of them published as open intellectual property, ,.,The way (big) data is processed has also dramatically changed: it requires optimizing complex Hadoop-like architectures, and computational complexity is not an issue any more in many cases (as long as you use efficient algorithms). It's the time that it takes for data to flow back and forth in , systems, that is now the bottleneck.,Saying that data science is not creating a new paradigm shift, is like saying that if we claim Earth rotates around the sun rather than the other way around, there's no change in paradigm, because after all, we are still dealing with 2 celestial bodies and 1 rotation - nothing changed. According to this, using an abacus or a computer means no change in paradigm: we are still dealing with automated computations to obtain more value faster.,The change in paradigm that I am referring to, consists of moving away from models, to focus on data. It is the data-to-algorithm approach (bottom-up) rather than model-to-data (top-down), and in the process ,. It also involves working with messy, unstructured data.,Also, big data has caused an explosion in spurious correlations and wrong analyses / conclusions, by people still using the old paradigm. The new paradigm allows you to (just to name a few),They might see it coming but are afraid: it means automating data analyses at a fraction of the current cost, replacing employees by robots, yet producing better insights based on approximate solutions. It is a threat to ,.,In addition, ,??that were previously considered as independent silos, and adds ,, and delivers knowledge (e.g. open-source intellectual property) outside traditional academia. Data science is also a cross-disciplines field:??,. It might appear that nothing new is created if you follow academic research closely, but the reason is because innovation is now done outside academia.
During eras of global economic shifts, there was always a key resource discovered that became the spark of transformation for groups of individuals that could effectively harness it. Today, that resource is data. In no uncertain terms, we are witnessing a global data rush and leading companies realize that data will grow enterprise over the next several decades as much as any capital asset. These forward-looking companies realize that to be successful, enterprises must leverage analytics in order to create a more predictable and valuable organization. In some cases they must package data in a way that adds value and informs employees, or their customers, by deploying analytics into decisions making processes everywhere. This idea is referred to as pervasive analytics. But to drive a pervasive analytics strategy and win the data rush, successful companies also recognize the need to transform the way they think about data management and processes in order to unlock the true value of data.,Leading enterprises have already realized the growth potential of developing and operationalizing analytics in order to drive new value through data. Cloudera customers like Opower, an energy analytics provider, have seen this shift first hand. They provide consumers descriptive and predictive analytics to reduce energy consumption; and through these analytics they have already saved over $500 million dollars. Using Cloudera as the central platform, they not only built these external facing analytics but also internal facing analytics that help business users do their job better. But the world of the enterprise hasn?€?t yet caught on to exposing more of their business to data and analytics; they?€?re holding back game-changing insights and innovations because they don?€?t think they are ready. The interaction between data insights and those in charge of making key decisions hasn?€?t yet reached a level of seamlessness.,The way traditional architectures and processes have been constructed has made it extremely difficult to implement a pervasive analytic strategy that can grow with your business. The reason this is so difficult is because data silos make it challenging to access the data needed; enterprise don?€?t have the right frameworks for data processing and analytics; and fragmented security and administration takes significant IT resources to manage.,These barriers lead enterprises to make costly upgrades and build one-off analytic pipelines that take months to move into production and don?€?t always perform as expected. In order for organizations to overcome these common challenges, they must prepare their enterprise architecture and people in order to deploy a pervasive analytics strategy. Preparation comes in the form of extending your architecture to capture larger volumes of structured and unstructured data, providing an environment to foster analytic innovation, and finally, empowering individuals by operationalizing analytics and embedding analytics into end users?€? workflows.,Can your architecture handle large volumes of structured and unstructured data? This is the first element that any organization must address when they are preparing themselves for analytics everywhere. This includes the ability to keep all data online, the ability to bring in structured and unstructured data sources, and the ability to process data in either batch or stream depending on the business needs.,The next element that you need to think about is analytic innovation. Enterprises need to bring more applications and users to the data so that these data savvy employees can access and interrogate this information to build the proper analytics needed. These employees are using multiple data sources and analysis techniques in order to build a report, model, or rule that they then want to put into production. This process requires organizations to enable employees to discover enterprise data and interrogate it using a variety of techniques ranging from simple search, to SQL, to machine learning.,Once the decision-oriented analytic has been created, the enterprise needs to put the analytic into production. Single discovery-oriented analytics have their place in an enterprise, but should not be the end goal. The end goal is to empower individuals by operationalizing analytics and embedding these decision-oriented analytics into the end users?€? workflow, whether that be your customer or employees. If this data sits in a silo or doesn?€?t reach the end user in time, then the process of collecting, storing, and analyzing the data will be all for nothing.,When the analytic is put into production, the enterprise will start to see the significant data returns by bringing analytics to the masses. If the operational analytic is not moving the KPI it is suppose to be influencing, then the analyst has to immediately remove that analytic from production and optimize that it. This means that they have to either include additional data or change the analysis technique in order to optimize the analytic.,Cloudera?€?s implementation of an enterprise data hub, powered by Apache Hadoop, provides an underlying platform that provides the analytic hub needed in order to achieve a pervasive analytic state. Cloudera?€?s enterprise data hub provides unified data, multiple processing and analytic frameworks, and the security and administration needed in order to deploy analytics across the enterprise.,??Today, only structured data is operationalized across the organization and it is stored in siloed systems that make it difficult to discover and interrogate. But with an EDH, imagine that data is not just a way to support strategic decisions ?€? it drives strategic decisions. By unifying larger volumes of structured and unstructured data with the power of Hadoop, it becomes easier for data to be discovered and new insights to be built. Hadoop provides the underlying technology needed in order to scale processing and storage of any type or volume of data.,??At your company you have limited, scattered frameworks in order to support the analytic lifecycle that usually stops at one-off, ad-hoc reports. An EDH and our partner ecosystem enable you to support all of the different frameworks that are needed to turn raw data into decision-oriented operational analytics. This includes the ability to process data, explore and analyze data, train and test models, and respond to data in near real-time.,??Today, IT is likely juggling multiple data systems that expose their enterprise to security risks and that are time-consuming to manage. With an EDH, you have unified management and security across all of your frameworks and data. This includes role-based authentication, encryption, and key management.,We are in the middle of a data rush. When you are right in the center of a storm, it can seem overwhelming. Where should I start? What do I need to think about? What is the best long-term bet? But don?€?t forget that more data should mean great news. More data should mean more insight, more guidance, and more strategic direction. However, more data doesn?€?t automatically rally your entire business around common goals and insights. You need a platform and architecture that can support a thriving, analytic-driven business culture that embraces a pervasive analytics strategy. Cloudera has made pervasive analytics a reality for hundreds of businesses. Allow us to make it happen for yours.,??,- See more at: 
Based on requests from clients - vendors of data processing platforms and products - as well as trends in popular blogs, ??job postings, and my own reading. Here are a few topics recently gaining strong traction (items beyond #13 were recently added)::
The United Nations and World Bank are part of a growing globally-oriented Open Data movement.?? After World Bank meetings last week in Washington, DC, those involved in defining the international Big Data Revolution asked for inputs.?? This is what I posted:,There is no doubt that big data offers significant and exponential potential for international development work across multiple fields.?? That this Independent Advisory Group has been formed is clear evidence that the United Nations is ready to see how big data can help, an exciting initiative for any of us who have straddled the worlds between international development and big data technologies in their life's work. ??,In this initiative, I believe we are hoping for and seeking paths for a robust collaboration that brings local, regional, and global community groups, national governments, and international representatives together with the gamut of current and future big data technologies that will enable them to do better, live longer, be happier, or otherwise make a difference. There is no end to the technologies, platforms, or research initiatives that are available to enable robust use of big data within international development.?? And likewise there is no end of international development use cases to which big data technologies could be fit.,But as we move forward, we must remember that much of our success will fall upon a common understanding of the potential and current limitations of big data.?? To that end, I would like to offer four key insights from my last 10 years of working on the technical side of massive open and public data projects for use in decision making circles.,1) Data Quality and Assurance Matters,It is commonly known that roughly 80% of a data scientist's, engineer's, or analyst's time can be spent cleaning up data so it can be used in conjunction with a software platform to produce a compelling story.?? Of course, resolving this issue varies greatly depending on the size of the project or whether the project is being produced from a single license platform from a laptop or an enterprise-level solution within which multiple users are networked.?? It could also depend on the level of technical skill in databases and Extract, Transform, and Load (ETL) processes, the mechanisms for otherwise ingesting and aggregating data, the access and security protocols in place for assuring that data is protected, and the multitude of other issues involved in using big data to create good narratives.???? But bottom line, if the data isn't workable to begin with, the user will loose trust in the data and in the process, and we will end up with a phenomena which is frequently referred to as "garbage in, garbage out". This benefits no one.,2) Data Provenance Matters,In many data science circles, and particularly in many data science labs, the source of the data matters not.?? Those working on the methods and algorithms to make the data sciences come to life don't care because it is not important to making the technology operational.?? As a result, few platforms provide the means for documenting data provenance from the inception of a data science project.?? While it doesn't matter in the research world, for the purposes of the governments and international organizations who would use big data outputs to make policy, it matters greatly.?? Indeed, many big data projects have failed to be adopted in policy making circles because of the lack of understanding among big data platform developers of how important it is to be able to trace back to a source and determine its reliability when making life-affecting decisions. ,3) Context and Use Cases Matter,There is no one size fits all approach to big data, especially when it comes to international development.?? And much of the success of adopting big data approaches will depend on providing experts and world citizens with the right tools to match a need.?? For example, current analytics software and data visualization packages are great for producing nationalized, aggregated demographic type of?? data or metadata outputs and beautiful visuals.?? But they will not likely be as useful for tracking the moment by moment of humanitarian disaster relief as a geo-spatial mapping platform, tracking agricultural price fluctuations in a micro community as crowd sourcing solutions, or providing a mechanism for the International Criminal Court to upload and sift through the exorbitant amounts of physical data they gather during a field mission to find the nuggets relevant to their cases.?? These are all different types of big data problems, each of which requires an understanding and formalization of existing processes, what parts can be automated, which will remain manual until advances in big data enabling technology can catch up to needs, and where humans will continuously need to intercede in the process.,4) Flexible Open Data Standards Matter,As discussions move forward on the foundations for a legal framework for agreement, I urge stakeholders to assure that whatever outcomes produced do not restrict innovations in big data technologies.?? The international community is ready to adopt technologies as they stand now, but there is a lot that big data innovators can learn about how the international community uses and needs to use big data.?? Aside from some forays into international business marketing, international data and how it is used in different countries and within different domains is a largely unexplored area.?? Certainly, the technology that exists today can provide significant insight, but there is so much more to learn.?????? To that end, I hope that whatever standards are developed take into account that the standards that will assure legal, international collaboration are not necessarily the same as those that will enable collaboration for the purpose of technological innovation.?? Big Data innovators will need open standards for data formats, accessing data, developing APIs, and other considerations that fall under the auspices of GNU Free Documentation License developed by the Free Software Foundation or a Creative Commons license.,The pendulum has swung dear readers.?? Big Data technologies have reached a moment where inputs from international stakeholders can help fuel a new generation of advancements, many of which have yet to be defined.?? And the opportunities presented in Open Data initiatives such as the UN Secretary General's Independent Expert Advisor Group are in a great position to help frame the myriad of international problems needing practical solutions.?? I look forward to a process that enables the adoption of big data technologies and open data initiatives within broader international circles and to the Big Data innovations that could emerge as a result.,Anne Russell

The market for data visualization software has bloomed. I'm suspicious.,Companies like Tableau, Spotfire, SAS Visual Analytics, Qlik and Zoomdata are positioning their tools far beyond traditional business intelligence.?? Capabilities for graphically navigating data, recognizing patterns and finding relationships are growing in both functional and economic scope.?? These new tools can provide charting forms only imagined in the last decade like word clouds, circular hierarchies, tree maps and stream graphs.?? Check out the , (data driven documents) javascipt library for inspiration.?? All this innovation begs a critical question:,Is data visualization,On the one hand, visual is not new.?? In 1983, Tuffle wrote "," which never stopped selling.?? In 1987, Rockhart and De Long offered "," and launched the very user centric EIS age.?? Comshare, IRI, Pilot and Arbor Software launched the 90's OLAP generation with its own concepts.?? And, the last decade, we've seen familiar players in Business Intelligence leap frog each other, continually competing on presentation. Face it - "visual" sells software.,Analytic visuals aren't new either. Archimedes had charts. He just used a pen. The statistical suites all have rough but ready graph capabilities. Basic, and un-pretty, plots are among the first steps of exploratory data analysis. So, treating data visualization as innovative comes with a very high burden of proof.,I could buy the an argument that Big Data has a consequence of Big Graphics.?? We are capturing more detail, we can store it and we need to study it.?? So, data science has a ground to need advanced visuals.?? But, I'm guessing that data visualization licenses outnumber data scientists by a hundred fold.??,Tufte's original book included a famous??,.???? The picture illustrated, for Tufte, irrelevant and useless presentation.?? So far, I haven't seen reasoning to treat data visualization as much more than a next generation duck.

In this data analysis a very interesting and powerful fact emerges. The ,.,And check this huge Treemap, YCombinator occupies more than half of the entire visualization.
Seasoned HR professionals have always been expected to make some instinctual decisions when it comes to hiring and managing employees. However, with the rise of Business Intelligence (BI) software, such practices may be at an end. Now that analytics have entered the comfortable cubicles of HR, there is far less room to make decisions based on hunches or personal opinions.,The future of HR is analytical and, well, more intelligent.,Business intelligence software in Human Resources is reshaping both reporting practices and the hiring process. Instead of simply looking out for ?€?red flags?€? during the interview process, companies can now use software to identify character traits and behaviors which the interviewee and the company share, resulting in better hiring decisions.,Companies such as , have found that learning from consistent behavioral patterns can drastically reduce the amount of errors made in the recruitment process, and therefore eliminate the snowball effect that occurs when mismatched employees are brought onboard.????And they?€?re not the only ones who have adopted this practice.,Neiman-Marcus, Gold's Gym, and many other retailers have reported up to a 35 percent increase in employee retention rates, as well as up to 40 percent reduction in the number of early dismissals as a result of using behavioral based systems.??,The idea of BI designed for HR isn?€?t a new concept. People have tried to use BI to improve analytics and decision-making in recruitment and HR for decades. However, most BI tools have been ineffective for these purposes. This is partially due to the complex, highly technical nature of most past BI solutions. Often it was simply wasn?€?t worth the time and opportunity-cost for IT to set up and manage such software for the HR department.,What HR professionals needed was an out-of-the-box solution that delivered useful information without requiring additional assistance from the IT staff. Users should be able to decipher the dashboards and charts with ease so that they can quickly find the information they need. Predicting trends, analyzing risks, and identifying future opportunities are all possible results that HR professionals can attain when paired with user-friendly software and a modest amount of training.,Knowing how pertinent information is best received, be it in pre-defined metrics, scorecards, reports or even charts, was another hindrance of BI solutions in the past. Now, many solutions offer advanced users the ability to create their own custom dashboards and visualizations for analyzing trends.,Traditional, people-based, instinctual methods of managing HR processes are quickly becoming a thing of the past. Now that , has become user friendly, even the most seasoned HR professionals realize they need to get on board before it?€?s too late.
Large Scale Data Sharing made safe and simple.,As the web gets more personal some ideas spread like wild fire and others just fizzle. Things like tokenized payment systems that take many years in concept for success all it takes is one large corporation to adopt it to make it mainstream. The mobile application movement is a lot like this expanding into data sharing with concepts like menu sharing among families and friends for dinner. This?? can be considered by some as a sign of a new trend. Personal level sharing with select personal discrimination is natural human behavior and good design. These concepts could soon expand into all kinds of new sharing and approval applications where a document or menu is shared for short time to get an order or input, bid, or opinion. This type of sharing is cognitively natural. With these new implementations and new password less applications renders us on a new era of application design.,Looking closely, the You Create You Own (YCYO) concepts are just simplified subsets of existing data logic with a cached or isolated client. Its almost like previous designers went too far at first. keeping everything on the server and implicitly saying ?€?trust us.?€? This has been clearly violated recently and there is a clearly a need for change. The blame if that could be a concept that applies here. The loss of data is clear, disturbing and very costly to society now and in the long term. So what is next, is it going to be the same excuses and lack of security?,Now the current collective task may point to pulling back with a new emphasis on the simple, the secure, the easy to implement with respect to the large scale. A gross example of this is the current password implementations with vulnerabilities at every turn from forgotten passwords to vulnerable old defunct accounts threatening the whole business process and leaving a wake of lost data. The huge cost of all this identity theft threatens the whole system of privacy. There is really no blame except maybe it was designed wrong, maybe the emphasis was on cheap untested with respect to large corporate bonuses, regardless all those important records could be and were downloaded in less than a minute.,The key here is responsibility. Natural ownership and responsibility has some unique properties ranging from the legal to general personal control. Using two concrete concepts, a remote level of indirection (multiple mappings) and validation or where the data came from or authentication. An example, A HIPPA profile with all its legal and ethical ramifications. Meaning if there is a breach the organization in question is required to tell you there was a breach and what was stolen. The information has to have a level of security or there are consequences this is all the law can specify. Yet if the information is stolen without notice the system, the law, the expense of implementation, is a joke. To change a complex system like this requires even more time and expense. Markets and people can only handle massive changes this so many times before rebellion. Then people will revolt. Lets all revolt.,In the You Create You Own (YCYO) storage pattern/scenario if someone creates an object that they own there is an implicit level of trust. If the owner/patient can share information securely with only the doctors that need it. The doctor has his ability to add results of tests, thoughts and communications. Luckily the world is changing. Emerging are new web authentication methods like SQRL and others which require a simple registration and decentralized client authorization and validation schemes within a browser and mobile applications. This may trigger a new computing realm where personal data is managed by the person that owns it.,With the current social legal ramifications of personal information storage and management security may eventually lead to better more distributed designs where the keys are not all in one place and distributed preventing massive data loss. With smarter faster computing clients along with secure local storage keeping a unique object secure is trivial an inexpensive. Also linking important objects to other objects along with a private object cache that is used to modify, delete and associate with other objects is very trivial with the phat client. To be useable the objects have to be transferable to a new device, sharable with others and associated with other objects in a personal way.??,These new potential client designs are robust mature and ready for large scale effort and contain an exciting possibilities for large data and personal security with a long term legal view. One aspect of personal secured objects is natural limiting, as a group one person can only use a limited set of objects over time. There is also only so many distributed devices can be accessed over a set time. In contrast to a server where all personal objects can be accessed and completely downloaded in a few minutes. This new concept is worthy based on this alone. Making a lifetime of work in the future what can be now done in minutes.,Also a business may want to use a set of objects to provide and procure business revenue. Therefore, a family or organization can own only what they need to accomplish the task at hand. This may be to place and order, order the same order again, or pay a purchase order, make a deposit, order the company lunch or ask for attendance at a meeting. All of these examples have a wonderful natural pattern. Create, use, share and archive. Each of these object events can contain a final event like the food was delivered the order fulfilled and money was paid the meeting adjourned. They also contain a linked chain of command as in a parents controlling a siblings behavior online and else where. Maybe something that matches the current laws would be more appropriate to everyone.??,A simple analogy may lie in a concept is similar to a personal password manager that grows up into a personal information and sharing manager with realtime notification and events. Something that can control what was shared with whom and what was changed or added. Then when the sibling turns 18 they are in control of their own profile. As gross and potentially scary as it is we know the current system is not working. Even with the ability of an information manager to remain flexible there is a time and date stamp on the event or decision. Each new rewrite or change of our personal information systems will migrate to something. Who would not want to simplify things and and make them more legally and physically secure.??,Conclusions: The future fight is to rally for the personal security manager and global standards to support it. This may be baby steps to take back personal security and the internet so expect a fight. A personal dynamic cryptographic, easy to use and change information safe that helps everyone manage personal information is technically real. There are forces that doe not want this type of personal control. We all need to learn to use our personal data responsibly on a mass level for this to work and we have the right to know when our data is being used sold exchanged or manipulated. The internet was created and maintained by people for people lets use it responsibly.??,Multilingual Secure personal information manager that is supported by laws and standards.??,World Wide secure personal information standards, devices and open source software.??,A publicly funded international organization that lobbies for personal information security.??,Clear and appropriate uses of peoples information, contacts and meta data.??,In a recent search there is nothing but scattered pieces of hope.??,Recent events by government and business reveal by example how much they collectively respect and are able to protect the American Public and its most precious commodity its people, their time and future. More and more it feels like pawns in a game without rules. ??This seems impossible yet we have done impossible before. ??Thank you for your time.??
For long, the real money gaming industry had carried the persona of bright and shiny casinos, roulettes, long rows of slot machines, high stake poker tables, etc. However, in the world where internet has penetrated into each and every aspect of our lives, gaming is no exception. Online gaming has gained tremendous eyeballs over the past few years. In its latest avatar, online gaming has added another subset that involves real money. While it has been existent worldwide for quite some time now, countries such as the U.S. had banned it in the past. Nonetheless, with the Government?€?s recent move to , involving real money we might witness the coming of age of the online gaming and an exponential upsurge in the number of loyalists in the gaming world.,Research experts value the online gaming industry?€?regulated and involving real money?€?to be somewhere between $30-40 billion, as of 2012. Yet, the actual market size is difficult to predict given the nature of business, hence it could well be five to ten times the current estimation.,The rise of casual online real money gamers is pertinent due to increasing number of customers wanting to indulge; this is augmented by the mushrooming of mobile devices such as tablets and smartphones. Certain other drivers that have resulted in increased online gaming activity among this segment of gaming aficionados are: ease of access and instant gratification at the convenience of our fingertips; amplified reach among casual and serious gamers on account of burgeoning innovative games; and an unparalleled social and fun experience for the customers.,The online real money gaming industry is at the cusp of a large scale disruption, but it can certainly co-exist and complement its offline counterparts in increasing engagement and profitability. Here I have tried to capture different ways in which this can be realized by leveraging , too.,??,Online services need to accept and adapt to a change and rewrite existing operating models and practices in order to thrive and gain competitive advantage over others. Much like other industries that have ridden the internet wave?€? unsettling existing business model?€?casinos that are getting into the online space, and their customers, can be bucketed under different categories in the adoption curve: innovators, early adopters, majority and laggards.,Few companies in the technology and manufacturing space were able to capitalize on this phenomenon while leveraging the online space. Using pre-built models they were able to understand the demand dynamics of a product and user acceptance levels of the products & services. Gaming companies too can capitalize on this understanding of the customer and sail the internet wave by answering key questions such as: ?€?what?€?, ?€?when?€? and ?€?how?€? to cater to a diverse set of audience?€??,??,Banking and retail companies have a definite edge over their counterparts in other industries by way of operating with an integrated approach to unify customer experience. In that respect, traditional casinos can stand to benefit more over pure internet gaming companies by providing a richer customer experience offline. For instance: a trip to a preferred destination or special / exclusive invite to an event.,Loyalty and reward schemes are not new to engage and in fact delight a customer. However, it?€?s a challenge for pure online companies as it could mean additional costs and may entail striking new partnerships. Traditional casinos who are trying to foray into online gaming can leverage their existing customer network?€? many of who might already be a part of some reward program ?€? and try to bridge the offline reward programs with the online one. ??,Adoption of online gaming is only half the battle won ?€? the gaming product itself should transform, while giving users a unique experience. Case in point is music industry where iTunes store manage user experience as opposed to music labels simply being sold online.,Further, companies poised to develop breakthrough online game need to understand certain ground rules, such as,,??,The online gaming, as we know, is poised to become the vanguard of a revolution, as the space ?€? both from a service provider and service seeker perspective ?€? is increasingly getting crowded. A lot of best practices can be learned and improved upon from other industries and can be used to create unique and personalized experience(s) for the gamers. It also leaves a lot of scope to ideate and innovate via cross-industry learning and cross pollination of thoughts, galvanizing the growth.?? This, backed by technological breakthroughs, such as , can help scale the success of such online gaming platforms and define a new means for gauging (and enhancing) customer experience.
Interesting listing published on??,. Only the top 12 are listed below. It would be nice to have a separate blog for voice recognition APIs. I've been thinking at using voice rather than passport or driving license, as a more secure ID. The voice has a texture unique to each individual.
The old world original genus names for fruit trees were all named after women. Representing an implicit importance of relation a fruit bearing tree and its fruit to the human condition. A simple and elegant description between human perception and the reality of living.?? Now we take liberty with fruit as it is available from all over the world any time of year. To look at a lovely ripe peach on a wonderful late summer day and to taste its wonderful essence over shadows the long rich story of survival. For this peach has arrived to you from many generations of trees and cultivated for your enjoyment for humans by humans. This is the essence of wild data. ??,Taxonomy or classification of plants has been in flux and full of disagreement for thousands of years. For the fruit bearing trees and all plants in general are now divided into complex conflicting incomplete taxonomies. The newer taxonomies based on DNA and region of origin, seed type, or some attribute and are still incomplete and maybe always will be incomplete. The taxonomy still does not describe why some trees survive and others perish. What is important here is that the classification is just that a classification. This alone is significant because the classification may change yet the plant continues on with its search for an environment to bear fruit. Some years more fruit is produced than other years. The overall fact that seeds are produced wrapped with what will help the tree survive. Sometimes in wild data it is hard to tell if you are looking at the seed or the fruit or a combination of both.,The low hanging fruit is sometimes the most vulnerable and important to finding a story. Regardless of how a plant is classified in reality fruit trees have survived to bring forth fruit. This simply means that all the right circumstances have come together to make fruit. The bees were there to pollinate, the male tree survived in the right place so the wind could ensure pollination if the bees were not available. Water, soil and sun all contain the right qualities to make the fruit produce the fruit. In wild data how pollination happens may have an influence on how much fruit is produced yet it still does not tell you that the orchard was pruned, picked and seeds distributed to other orchards by a guy named Johnny Appleseed 100 years ago.,So metaphor can be offensive to some people yet in this case it is important because there is typically a purpose to scrubbing of the data. Overall it really ends up being more about interpretation of the results. Human cycles typically have a beginning middle or end or to be more specific there are threads or recurring cycles that show up that are not necessarily linear. This also means all the math in the world still may not bring meaning or interest to your work. In some projects the beginning piece may be available or an end piece yet no middle piece. Then again once you get the analysis has started the whole question may change.,The analysis process always starts with a question or a search for a story or justification of a story. In real data situations the data in question always has a partial context and typically has to be mixed with some other data that also has a context to answer a question or find a story.?? It is important to ask good questions and to make sure that the questions have a basis in a human view of the world or the real world. The statistics or math side may still matter yet may also influence in the negative or say this is impossible and there are no answers. Although to give up too soon you may miss the fruit at the top of the tree. It is human nature that someone will pick that fruit if you do not.??,For example in the mixing of 100 years of UFO encounters with close to a 100 years of missing person data. The original question was Is there any correlation between UFO encounters and missing persons. This led to a refined question of, Is there any missing persons in the same location as UFO sightings in and around the same time as these sightings. What it took to do that was seemingly simple task was more than a trivial exercise.?? With just one example of the world wide data set on UFO encounters was converting the sighting times to a consistent value. The task was unbelievably difficult because at first pass almost half of the 80,000 UFO records were not useable because of the time duration. The other task was the geolocation of the records which also had very high error margins. An error margin in this context is a whole record that could be converted to something useful. Useful in this case is a valid start time, duration time and location for UFO records and a valid date missing and location for missing persons.??,So after the first pass the error margins were important for a quality outcome. Realistically under ten percent error margin was desirable. So how do you get from a large error margin down to a small margin without spending a year hand correcting the data. The exercise is left up to the reader. The clue is solved like we solve everything else with human ingenuity and fortitude. I failed three times to convert the data. Each attempt the result set or the code became more polluted and it all pointed to something very simple in the end. Take the low hanging fruit, then get the ladder and circle the tree look at what you have left and process. Then on to the next tree until the whole orchard is picked and leave what is on the ground and unusable.??,The complete database, result set, and selects is on ,/planetsig for verification. There is also a complete write up on ,sig ?€?Is it a bird, a plane or superman.?€??? Most of the tools used were standard Unix tools like bash, awk, perl, python and shell scripts. Some of these tools have their limitations so keeping each task/step separate and simple was better because each could be modified moving the result forward with each iteration. When the iterations of the data spit out an error margin below ten percent there was a little fiddling to see if there was a lower level. Then finally, the left over ten percent of the records were examined and manually corrected. Then the process of indexing and selecting the matching data sets became priority. Ending with some interesting results. By shear numbers there is no correlation yet the result set left more interesting questions than answers.?? Now that?€?s wild data.??
It should be detailed, featuring appropriate and properly scaled business charts, organized in a meaningful way and commented:
For a very long time, businesses had their documents filed in folders and stored in huge metal cabinets. But thanks to advances in technology, they were eventually coded and stored digitally. As we advance through the Age of Information, the traditional digital storage devices like the floppy, compact, and flash discs evolved into cloud storage. Eric Griffith, a contributor at PCMag.com, defines cloud computing as a ?€?metaphor for the Internet.?€? In his ,, he briefly explained that instead of accessing programs and data from a user?€?s hard drive, the cloud allows anyone to access or store data online. ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ????,The Cloud Infographic recently published a , revealing that in the next five years, the cloud will grow up to $8 billion dollars, and almost 70% of businesses are already utilizing cloud computing technology.,??,Despite the cloud?€?s advantages, there are some businesses that are still hesitant in adopting cloud storage for their businesses, especially the smaller ones.,However, since most of them can be exposed to data breaches and malicious attacks?€?according to a previously published ,?€?it?€?s still one of the safest and most efficient ways of storing pertinent data. Now, what important things should they remember before committing to the cloud? ,??,??,Nowadays, most companies are adopting BYOD or Bring Your Own Device schemes to boost efficiency and productivity. ?€?BYOD to Drive Data to the Cloud,?€? a , featured on Wired.com states that ?€?,?€? This particular trend can benefit SMBs or Small-to-Medium businesses where they don?€?t have enough IT staff. Local storage-based solutions for the enterprise are costly, and most of them really want to stretch their dollar. Take , for example; it shows us that this device is also built for business professionals, thanks to its security features. With the ,, IT managers can now rest easy knowing that their network will always be secure. Also, it?€?s easier to host business-related apps on the cloud, which provides faster and cheaper roll-out costs to suit your needs.,??,??,Bruce Campbell, Clare Computer Solutions?€? VP for Marketing, said in a DailyTech , that ?€?,?€? He believes that although, most businesses are still using on-premise IT infrastructures, cloud solutions are slowly changing this trend. Also, Campbell has listed the three benefits that cloud computing can bring to SMBs:,??,They can switch costs between CapEx (Capital Expenditures) to OpEx (Operational Expenditures). This type of strategy will surely create a better cash flow for businesses, and possible tax advantages. The cloud offers them the ability to become more flexible; it allows businesses to add services, upgrades, or other changes in their existing IT network. Basically, it allows them to be more agile. It works well with existing BYOD schemes and helps empower employees. It allows them to access information and applications from almost anywhere. The cloud makes your workforce more mobile.,??,??,Compared to physical, on-site data servers, the cloud is safer than the former. Since data will be stored in the cloud, it will be less likely for Trojan attacks and rogue employees from accessing it. Although, it may provide adequate security for any type and size of business, IT managers should still implement stricter security measures. With this new technology, old approaches to security won?€?t do and the cloud grows on a faster pace. The first thing to do is to identify databases that are highly-sensitive and to provide them with an extra layer of protection. Rolling out monitoring and encryption measures can help track data traffic within the network. Adding layers of advanced analytics can also allow entrepreneurs to have real-time visibility in both their cloud and devices connected to it. Remember that security will always be an important security perimeter.,??,The cloud isn?€?t just about personal computing anymore; it has found its way into small enterprises as well. Now, it?€?s all about ?€?personalized computing?€? and it is the future of storage. It?€?s also important to understand that it won?€?t replace traditional storage; it allows anyone to optimize different storage solutions for our data needs.,??,??,Images courtesy of Isabel XD, Manager Web, Amer Sandhu ??via flickr licensed under creative commons


This happened tonight, shortly after Facebook took the same decision. Even Bit.ly itself is banned, see picture below. This happens only with Chrome, but not with other browsers such as IE or Firefox. The ban will probably be lifted in several hours.,:,For your curiosity, you can check , to verify if a website is safe or not, according to Google (replace bit.ly with any domain you want to check). And maybe what Google is trying to do with the bit.ly ban, is to get people to visit their , website security app.,Maybe this is more a "business politics" issue or the result of competitive wars, rather than a data science / true security issue. After all, everyone knows that our bit.ly redirects are safe, as we use bit.ly only for internal redirects. Unless of course, if bit.ly has been so badly attacked by hackers, that many of their redirects have been re-redirected. But I know that's not the case. ??
Given the nature of the community, presumably many visitors already have a strong understanding of the nature of quantitative data. Perhaps more mysterious is the idea of qualitative data especially since it can sometimes be expressed in quantitative terms. For instance, "stress" as an internal response to an externality differs from person to person; yet it would be possible to canvas a large number of people and express stress levels as??an aggregate??based on a perceptual gradient: minimal, low, medium, high, and extreme. We have to recognize the rationale for doing so relates to a pervasive quantitative normative. Within the framework of an organization where much that transpires does so in qualitative terms, we nonetheless produce metrics to help characterize that reality within the normative. There is however a problem. The metrics can convey aspects of state and progress while providing few insights and alternatives. A company could for instance gain a solid grasp of its employee accident levels from the numbers alone. The organization might decide to allocate or budget more resources to a responsible department in response. But taking correction action requires awareness of the underlying events and circumstances; and depending on the configuration of the data system, the company might not have the foggiest idea what aspects of production seem to be contributing to accidents. Arguably, one of the reasons why companies are required to report accidents to regulators might be to encourage adequate book-keeping; this imposition can help them become more aware of prevention opportunities.,It would be reasonable to suggest that companies are in the business of making money rather than maintaining records; and this might explain the bias towards quantitative details. However, I would argue that a fair number of companies actually don't have any idea why their clients buy their products. I would say, there are specific individuals in some companies such as in the marketing department with reasonable perceptions explaining the demand for products. These perceptions are reasonable until clients stop buying, of course. Then it would be necessary to pull different perceptions out of a hat to explain the change. The point is, there might not be a place either in the data system or in the rational ecology of boardrooms to accommodate qualitative facts. Ignoring the events giving rise to the metrics is a structural weakness I believe triggered by an overemphasis on the metrics and a frail belief that people can control reality by exercising authority over its depiction.,I will use an example I have given a few times in other blogs. In my own research, in light of increasing costs for disability claims, there was this idea that the problem could be addressed through organizational restructuring. Restructuring can have the effect of redistributing of costs - on the surface making the amounts seem smaller and therefore easier to bury. However, I found that about half of the insurance claims related to problems that could be reasonably attributed to the work environment: e.g. circulatory and musculoskeletal problems. How exactly corporate restructuring can reduce repetitive strains and poor circulation is baffling - and it points to the quasi-intellectualism of people estranged from the realities faced by workers. It also shows the impact of being focused on the metrics although disconnected from its qualitative details. In other words, the business problem was at least in part a data problem: the clarity of the quantitative metrics influenced decision-making; the obscurity of the qualitative events concealed the solution. Statistics only provide half the answer. We routinely ignore the phenomena giving rise to the stats I believe due to lack of a compatible data system.,This blog describes a simulation environment that I will be developing based on what I have learned from a research prototype called Tendril. I routinely create simulations usually designed to generate streams of data that resemble real-life conditions in quantitative terms. For instance, stock-market data is quite easy to imitate by following some basic trading criteria: the trading price should be near the previous price; small fluctuations are common while large changes happen only periodically; prices tend to be choppy rather than smooth and predictable. The simulator on the other hand is going to be unique in that it is meant to make use of actual work data that can be both quantitative and qualitative in nature. Similar to Tendril, the simulator will be "event-driven"; this is something that I will explain further in this blog. The simulator will employ a rather non-statistical approach focused on organizational and business events.,Tendril has a number of screens. The image below is the "original" screen that was introduced about 18 months ago. The name of the simulation environment is Shatterdome. I picked up this term from a science-fiction film called Pacific Rim. As indicated on its title, Tendril is Shatterdome Enforcement. Perhaps this relationship might make sense closer to the end of the blog.,I am providing details of the simulation environment here despite its preliminary nature. It would be fair for readers to question how I can be certain that the simulator will work even before I commence writing the code. Well, the prototype was meant to be my trial balloon. My confidence stems from the success of the prototype. Tendril already performs certain jobs that I would expect the simulator to do; however, it doesn't generate events, this being the main task of the simulator. Normally, events are based on real-life situations pertaining to myself. Shifting these events to study an organization is simply a matter of redirecting my attention. So instead of generating events relating to my personal circumstances, I can carry out the same process for different departments of a company. (I can do all of this for a fictitious company with imaginary departments, of course.) For "simulation" purposes, bundles of events will be released at particular times under specific circumstances. I consider the event-distribution method fairly straightforward.,Tendril handles two types of data: event data and metrics. The theory behind the "proxy-phenomena differential" is fairly in-depth. I leave it to readers to check out my earlier blogs if they are interested in the conceptual distinction between proxies and events. Similar to Tendril, the simulation environment will accommodate both types of data. Below is another screen from Tendril on the left kind of reinforcing that Shatterdome imagery: in Pacific Rim, the Shatterdome housed super-robots. On the right I present the contents of an actual data file showing a list of events. It deconstructs processes converting them into little fragments of data - like pieces of glass from a shattered dome. The robot by the way is only a splash image. Other images appear in place of the robot depending on the circumstances - if Tendril finds certain things during processing. ??I should explain that I often leave Tendril for long periods of time so I can get other things done. ??I find it useful having large images that I can see from across the room to tell me what the processor is doing.,I'm not aware of this type of simulator already being in use. Part of the reason I am posting this blog is to get feedback from the community. So if after reading a bit further any readers find that they have come across similar systems, by all means please send me details. I mostly want to avoid the unnecessary invention of terms if others already exist. I expect the Shatterdome to make no effort to mimic the behaviour of people or machines; rather, it attempts to generate the same data that people and machines produce. The program will draw from large spools of events based on current policies, protocols, practices, and production patterns. It should at least in theory be possible to create a simulation of any organization from its observable routines and artifacts.,As indicated on the image below, event data can be gathered through questions and observations using a clipboard, pen, and sheets of paper. Or, the events can be collected and conveyed electronically. I have worked in all sorts of production settings. I realize that conditions are not always ideal for research purposes. I think that some simulations attempt to be "scientific" in that there is some effort to create elaborate experiments. In contrast, I am concerned more about compiling non-experimental field data. Noise is fine. I don't have problems with production peculiarities. I consider complexity a challenge rather than an opportunity for simplification. It is true that life would be easier if the data were clear-cut. But then again, it would hardly be necessary to hire scientists and other professionals to deal with straightforward situations. The fact that a highly skilled individual exists in a process I feel is due in part to the messiness of the data and how the facts are blurry. I rather enjoy data weirdness - scenarios in which people are so baffled, they are willing to take chances on something new.,I my blog on the "Geography of Data," I describe the idea of transposition. Consider as a foil the concept of reductionism where there is some effort to gain the most salient aspects of particular phenomena. I find that this process of simplification tends to be instrumental in nature: the information we extract might only serve our immediate production needs as efficiently as possible. Within this characterization of reality, we might embed little of the reality that exists; for we focus only on the aspects that we need and want. The potential for social disablement is quite high in such an instrumental framework. Moreover, we bring to the new technology our old methodologies which might likewise limit the expression of phenomena. Yet another potentially alienating force is the distance that can emerge between data scientists and the production settings that give rise to data.,The idea behind transposition is to create some means of retaining not just metrics but the relevance of events to those the metrics. For instance, it isn't good enough just to have sales figures; it is necessary to also maintain data about the contributing events a-n-d how those events are relevant to the metrics. Relevancy is a type of data that is difficult to capture. We are sort of stuck with tradition in terms of current methods, technologies, and behavioural normatives from antiquated belief systems. For instance, there is a great emphasis on metrics rather than gaining an understanding of the underlying phenomena; I believe this is due in part to the longtime emphasis on exploiting resources. In terms of the Shatterdome, I plan to deliberately map out both circumstantial events and their multifarious proxies.,I will be focused on two general types of events during this project: articulation and actuation events. An articulation event is a discrete piece of data reflecting something that has happened. An actuation event on the other hand is a response to something that has happened. Below I show the basic triggering process for Shatterdome. If an event occurs, the program will generate actuation events at specific times and "locations." I apologize if the narrative seems almost like a computer program. Suffice it to say, this simulation environment is meant to be event-driven - not based on logic or psychology but rather production and reality. It hardly matters if a book tells me that people respond in a certain way to given stimuli or that a production system should behave in a particular manner. My objective is to create a realistic simulation based on tangible evidence.,What do I mean by locations? In the diagram below, think of Tendril as that box called "Hub" to the very right. It can receive data from many different locations: the environment or business setting; the organization itself; and different departments such as human resources. (Throughout development, I will in fact be focused on events derived from human resources.) The actuation events can be destined for any part of the organizational system. Actuation events at Organization (the location) will be mostly focused on production whereas events at human resources (a department) will pertain to personnel. Now, I know that in terms of statistical distribution, there is the idea of maintaining different performance metrics for employees. However, irrespective of whether or not employees exist or how many of them exist, the events pertaining to their behaviours can nonetheless be incorporated into the Shatterdome. I make no attempt to make the program mimic individual employees.,In order to run the simulation, Shatterdome would commence its actuation of events. There is a question as to how one can be certain that particular events will lead to the desired metrics. At the moment, I hope to use an approach that I call metric actuation. This means that metrics will be triggered by events. There might be a range or spectrum of metrics associated with different events. I would obtain not really a number but rather a distribution of potential results. It is going to be a really interesting journey. The deeper point of this exercise is to create an operating abstraction of the real thing. Once there is a stable simulation of a particular organization, I hope to run many different types of hypothetical situations. This is why I can't simply create "a simulation." A simulation might not reflect the organizational reality. I need something that has the capacity to parallel the organization.,The primary objective at the different locations shown above - the environment, organization, and department - is to generate events that to the greatest extent possible reflect real-life conditions. Since conditions are dynamic, one would expect the events to be likewise. The starting point for the simulation doesn't have to be hypothetical. Ideally, data-collection specialists would routinely canvas the organization for data to feed into the simulation. If intervention occurs, existing events will change, and more events will be generated. The Shatterdome might be useful not merely for simulation purposes but also to manage change - to keep track of event data relating both to environmental transformations and specific business initiatives.,Apart from the human resources applications, I will try to use the same approach in relation to security. It can be quite expensive and impractical to stage mock emergency scenarios for example in a mall or university. A "response protocol" involves generating events en masse as a reaction to an emergency or urgent situation. I am not too interested in tinkering with little events but rather creating different response protocols as part of a risk management strategy. (At this time, I'm not certain how feasible this approach would be in relation to a non-enclosed environment.) Tendril, which is used to detect and analyze data, might never be released to the public. However, there is a chance that I might make the Shatterdome public domain. I certainly hope that others find the project worthwhile in relation to their own efforts.,I have given a brief overview of the Shatterdome project. I now want to take a few moments to discuss the technological setting. Usually in relation to big data, the conversation tends to focus on the need to handle enormous amounts of data - amounts that many smaller organizations might never encounter. I consider the "discourse of enormity" rather exclusionary. It immediately distances large segments of the business market from big data. On the other hand, I have observed an extreme perspective in the opposite direction: some practitioners seem obsessed with the reduction and simplification of data. This philosophy likewise discourages businesses from embracing the technology. In both scenarios, the focus seems to be on perceived normatives rather than substantive business issues. If data reduction can make money, it is a good idea. If data expansion can make money, the option has to remain on the table. If dancing like a duck can make money, that too is worthy of some consideration. There is a lot of talk about what people should and shouldn't do - as if there is some kind of prize waiting if an argument is won. Essentially, if an approach fails, its proponents become irrelevant to the production process. Their beliefs and arguments become superfluous. The problem with data science, if it can be characterized as a problem, is how easy it is given the availability of data to confirm failure.,So I personally find the data reductionists and data enormity people rather curious in their deliberate estrangement of the market. The idea of creating a simulation through observation and asking people questions might seem a little bit peculiar. No, the peculiar part is creating a simulation without making observations and asking questions. The average person in an organization has much to offer both in terms of information and perspective. We also have to consider the great opportunity to bring people closer to the data. Data should be something to help liberate and deliver - not impose and alienate. I hope the idea of the Shatterdome helps to normalize involvement and participation in a substantive way towards positive business outcomes. I don't have a problem with case studies, gut instincts, tarot cards, horoscopes, tea leaves, or whatever fanciful ideas come to mind. But why not also try out a simulation, if not for business insights then at least to facilitate administrative initiatives. Even if the simulator is never used, not even once, I'm going to suggest that something like the Shatterdome would be worthwhile purely as an administrative tool. Making business decisions based on metrics without any understanding of the contributing events might be one of the most entrenched non-business-like practices around today. It is this glass dome that I hope to shatter through the Shatterdome project.
The case study presented here - including root cause analysis and solution - was performed for a digital publisher. It offers a different perspective on what data scientists are capable of. The expert involved here is not a coder, certainly not a production guy, yet is able to leverage his business acumen and domain expertise to,Such a data scientist who can save billions to a company, is usually not hired, for the following reasons,This is why companies erroneously think that data scientists are unicorns, because they won't even interview such a professional when a position becomes vacant. Yet this guy does not consider himself a unicorn. And it's not the story of a smart data scientist, highly capable, misunderstood and poor, unable to find a job. It's the opposite: a data scientist (granted, not on any payroll, not considering himself a ,), who will compete aggressively against whoever (small or big) is in his path, and win time and over, including financially.??,This story also illustrates that data science is not necessarily about big data, big statistical models, or big Python code. It might indeed involves none of them. Anyway, this "unicorn" shares his secret with you in this article. Bookmark it, it might become very handy one day! And you won't need to find that elusive unicorn: just follow his recipe below.,Helping a digital publisher, this data scientist (let's call him the ,), among other things, monitors the company finances, using a dashboard ??- ,??- that advertises itself as "bookkeeping in the cloud". And it's true that Freshbooks is fantastic, making many high level KPIs easily accessible. One set of metrics is in the "invoicing tab", where you can see all invoices submitted , sorted by date. Gaps between subsequent invoices range between 1 to a7 days. So by looking at the distribution of these gaps (no need to develop a stats model) it is obvious after a 30 second visual inspection, that something happened if the most recent invoice is two weeks old.,This was the starting point for this investigation; the discovery of an issue with invoicing. Note that the discovery and the choice of the data tracking platform (Freshbooks), was the job of the ,, in this company. In all other companies, it takes much more time to react, as data scientists look at sales, rarely at invoicing. In short, detection took place one month earlier than in most companies. Interestingly, the alarm was raised at a time when revenue was higher than ever before - but there's a 40 days lag between invoicing and revenue.,The unicorn then imaged scenarios, and the kind of data needed to rule out most of them, and detect the real culprits:,Since lack of inventory was the issue, growing traffic by adding a relevant channel, and offering new advertising products, was the proposed solution. Also, blocking some inventory in advance for potential large clients won't solve the invoicing issue, but can boost total revenue in the long term. Again, the unicorn came with the solutions.,Note that this investigation required collaborating with many teams: sales, inventory management, finance, marketing, competitive intelligence. As always, multiple factors were involved, with two dominating factors (inventory sold out, bookkeeper working with client less frequently because she has acquired more clients).,Note that this is a causal analysis, not just a search for correlations. Easier to perform quickly if your company has fewer than (say) 400 employees.


I've been writing a Tableau and Alteryx-focused blog for 1.5 years on Wordpress and haven't thought of writing anything here on DSC. I just completed a two-part series that discusses solving problems using innovative approaches with Alteryx and Tableau, which were my 99th and 100th blog posts. They are longer than usual but offer a good insight into my background and why I write a technical blog.,My blog is focused on solving business and science problems using Alteryx, Tableau and R. I write a lot about technical techniques that I have developed using these tools. I have extensive experience with Tableau, beginning in 2008. I have less experience with Alteryx and R but I am deeply involved in using them for some amazing applications.??,.
 , , , , , 


The idea of environmental determinism once made a lot of sense. Hostile climates and habitats prevented the expansion of human populations. The conceptual opposite of determinism is called possibilism. These days, human populations can found living in many inhospitable habitats. This isn't because humans have physically evolved. But rather, we normally occupy built-environments. We exist through our technologies and advanced forms of social interaction: a person might not be able to build a house, but he or she can arrange for financing to have a house constructed. "Social possibilism" has enabled our survival in inhospitable conditions. Because humans today almost always live within or in close proximity to built-environments, among the most important factors affecting human life today is data. The systems that support human society make use of data in all of its multifarious forms; this being the case, data science is important to our continuation and development as a species. This blog represents a discussion highlighting the need for a universal data model. I find that the idea of "need" is highly subjective; and perhaps the tendency is to focus on organizational needs specifically. I don't dispute the importance of such a perspective. But I hope that readers consider the role of data on a more abstract level in relation to social possibilism. It is this role that the universal data model is meant to support. Consider some barriers or obstacles that underline the need for a model, listed below.,I certainly don't suggest that in this blog that I am introducing the authoritative data model to end all models. Quite the contrary, I feel that my role is to help promote discussion. I imagine even in the list of barriers, there might be some disagreement among data scientists.,(1) Proxy reductionism triggered by instrumental needs: I believe some areas of business management have attempted to address highly complex phenomena through the simplification of proxies (i.e. data). The nominal representation of reality facilitates production, but also insulates an organization from its environment. Thus production can occur disassociated from surrounding phenomena. I feel that this nominalism is due to lack of a coherent model to connect the use of data to theory. ??We gain the illusion of progress through greater disassociation, exercising masterful control over data while failing to take into account and influence real-life conditions.,(2) Impairment from structurally inadequate proxies: Upon reducing a problem through the use of a primitive proxies, an organization might find development less accessible. I believe that a data model can help in the process of diagnosis and correction. I offer some remedial actions likely applicable to a number of organizations: i) collection of adequate amounts of data; ii) collection of data of greater scope; and iii) ability to retain the contextual relevance of data.,My graduate degree is in critical disability studies - a program that probably seems out-out-place in relation to data science. Those studying traditional aspects of disability might argue that this discipline doesn't seem to involve big data, algorithms, or analytics. Nonetheless, I believe that disablement is highly relevant in the context of data science albeit perhaps in a conceptual sense. While there might not be people with apparent physical or mental disabilities, there are still disabling environments. Organizations suffering from an inability to extract useful insights from their data might not be any more disabled than the data scientist surrounding by tools and technologies disassociated from their underlying needs. Conversely, those in the field of disability might discuss the structural entrenchment of disablement without ever targeting something as pervasive as data systems. However, for those open to different perspectives, I certainly discuss aspects of social disablement in my blogs all the time. Here, I will be arguing that at its core, data is the product of two forces in a perpetual tug-of-war: disablement and participation. So there you go. I offer some cartoon levity as segue.,I recently learned that the term "stormtroopers" has been used to describe various military forces. For the parable, assume that I mean Nazi shock troops. I'm uncertain how many of my peers have the ability to write computer programs. I create applications from scratch using a text editor. Another peculiarity of mine is the tendency to construct and incorporate elaborate models into my programming. It is never enough for a program to do something. I search for a supporting framework. Programming for me is as much about research through framework-development as it is about creating and running code. In the process of trying to communicate models to the general public, I sometimes come up with examples that I admit are a bit offbeat. Above in the "Parable of the Stormtrooper and the Airstrip," I decided to create personifications to explain my structural conceptualization of data. The stormtrooper on the left would normally be found telling people what to do. Physical presence or presence by physical proxy is rather important. (I will be using the term "proxy" quite frequently in this blog.) He creates rules or participates in structures to impose those rules. He hollers at planes to land on his airstrip. I chose this peculiar behaviour deliberately. Command for the soldier is paramount, effectiveness perhaps less so. In relation to the stormtrooper, think social disablement; this is expressed on the drawing as "projection.",On the other side of the equation is this person that sort of resembles me and who I have identified as me although this is a personification of an aspect of data. He is not necessarily near or part of the enforcement regime. His objective rather than to compel compliance is to make sense of phenomena: he aims to characterize and convey it especially those aspects of reality that might be associated with but not necessarily resulting from the activities of the stormtrooper. There are no rules for this individual to impose. Nor does he create structures to assert his presence over the underlying phenomena. In his need to give voice to phenomena, he seeks out "ghosts" through technology. If this seems a bit far-fetched, at least think of him as a person with all sorts of tools designed to detect events that are highly evasive. Perhaps his objective is to monitor trends, consumer sentiment, heart palpitations, or patterns leading to earthquakes. Participation is indicated on the drawing as "articulation.",So how is a model extracted from this curious scene? I added to the drawing what I will refer to as the "eye": data appears in the middle surrounded by projection and articulation. Through this depiction, I am saying that data is often never just plain data. It is a perpetual struggle between the perceiver and perceived. I think that many people hardly give "data" much thought: e.g. here is a lot of data; here is my analysis; and here are the results. But let us consider the idea that data is actually quite complicated from a theoretical standpoint. I will carry on this discussion using an experiment. The purpose of this experiment is not to arrive at a conclusion but rather perceive data in its abstract terms.,A problem when discussing data on an abstract level is the domain expertise of individuals. I realize this is an ironic position to take given so many calls for greater domain expertise in data science. The perspective of a developer is worth considering: he or she often lacks domain expertise, and yet this person is responsible for how different computer applications make use of data. Consequently, in order to serve the needs of the market, it is necessary for the developer to consider how "people" regard the data. Moreover, the process of coding imposes distance or abstraction since human mental processes and application processes are not necessarily similar. A human does not generate strings from bytes and store information at particular memory addresses. But a computer must operate within its design parameters. The data serves human needs due to the developer's transpositional interpretation of the data. The developer shapes the manner of conveyance, defines the structural characteristics of the data, and deploys it to reconstruct reality.,I have chosen an electrical experiment. There is a just single tool, a commercial grade voltmeter designed to detect low voltages. The voltage readings on this meter often jump erratically when I move it around a facility full of electrical devices; this behaviour occurs when the probes aren't touching anything. Now, the intent in this blog is not to determine the cause of the readings. I just want readers to consider the broader setting. Here is the experiment: with the probes sitting idle on a table, I took a series of readings at two different times of the day. The meter detected voltage - at first registering negative then becoming positive after about a minute. As indicated below on the illustration, these don't appear to be random readings. Given that there is data, what does it all mean? The meter is reading electrical potential, and this is indeed the data. What is the data in more abstract terms regardless of the cause?,Being a proxy is one aspect of data. Data is a constructed representation of its underlying phenomena: the electrical potential is only a part of the reality captured in this case by the meter. The readings from the meter define and constrain the meaning of the data such that it only relates to output of the device. In other words, what is the output of the device? It is the data indicated on the meter. It is a proxy stream; this is what we might recognize in the phenomena; for this is what we obtain from the phenomena using the meter. From the experiment itself, we actually gain little understanding of the phenomena. We only know its electrical readings. So although the data is indeed some aspect of the articulated reality, this data is more than anything a projection of how this reality is perceived. It is not my intention to dismiss the importance of the meter readings. However, we would have to collect far more data to better understand the phenomena. Our search cannot be inspired by the meter readings alone; it has to be driven by the phenomena itself.,Another problem relates to how the meter readings are interpreted. Clearly the readings indicate electrical potential; so one might suggest that the data provides us with an understanding of how much potential is available. The meter provides data not just relating to electrical potential alone but also dynamic aspects of the phenomena: its outcomes, impacts, and consequences. This is not to say that electrical potential is itself the outcome or antecedent of an outcome; but it is part of the reality of which the device is designed to provide only readings of potential. We therefore should distinguish between data as a proxy and the underlying phenomena, of which the data merely provides a thin connection or conduit. There is a structure or organizational environment inherent in data that affects the nature and extent to which the phenomena is expressed. The disablement aspect confines phenomena to contexts that ensure the structure fulfills instrumental requirements. Participation releases the contextual relevance of data.,I have met people over the years that refuse to question pervasive things. I am particularly troubled by the expression "no brainer." If something is a no-brainer, it hardly makes sense to discuss it further; so I imagine these people sometimes avoid deliberating over the nature of things. This strategy is problematic from a programming standpoint where it is difficult to hide fundamental lack of knowledge. It then becomes apparent that the "no brainer" might be the person perceiving the situation as such. Keeping this interpretation of haters and naysayers in mind, let's consider the possibility that it actually takes all sorts of brains to characterize data - that in fact the task can incapacitate both people and supercomputers. If somebody says, "Hey, that's a no brainer" to me or anybody else, my response will be, "You probably mean that space in your head!" ??(Shakes fist at air.),I provide model labels on the parable image: projection, data, and articulation. I generally invoke proper terms for aspects of an actual model. "Disablement" can be associated with "projection" on the model; and "participation" with the term "articulation." The conceptual opposition is indicated on the image below as point #1. Although the parable makes use of personifications, there can sometimes be entities in real-life doing the projection: e.g. the oppressors. There can also be real people being oppressed. In an organizational context, the issue of oppression is probably much less relevant, but the dynamics still persist between the definers and those being defined: e.g. between corporate strategists and consumers. Within my own graduate research, I considered the objectification of labourers and workers. As production environment have developed over the centuries, labour has become commodified. In the proxy representation, workers have been "defined" using the most reified metrics; but there is a counterforce also, for self-definition or some level of autonomy. Data exists within a context of competing interests as indicated on point #2,From the experiment I indicated how data is like a continuum formed by phenomena and its radiating consequences: I said that readings can be taken of dynamic processes. This is a bit like throwing stones in a lake and having sensors detect ripples and counter-ripples. An example would be equity prices in the stock market where a type of algorithmic lattice can bring to light the dynamic movement of capital. Within this context, it is difficult to say whether what we are measuring is more consequence or antecedent; but really it is both. I believe it is healthy to assume that the data-logger or reading device offers but the smallest pinhole to view the universe on the other side. Point #3 shows these additional dynamics. There is a problem here in terms of graphical portrayal - how to bring together all three points into a coherent model. I therefore now introduce the universal data model. I also call this the Exclamation Model or the Model! The reasons will be apparent shortly.,The Exclamation Model visually resembles an exclamation mark, as shown on the image below. For the purpose of helping readers navigate, I describe the upper portion of the model as "V" and the lower part as "O," or "the eye" as I mentioned previously since it resembles a human eye. The model attempts to convey all of the different things that people tend to bundle up in data perhaps at times subconsciously. An example I often use in my blogs is sales data, which doesn't actually tell us much about why consumers buy products. There might be high demand one year followed by practically no demand the next; yet analysts try to plot sales figures as if the numbers follow some sort of intrinsic force or built-in natural pattern. Sales figures do not represent an articulation of the underlying phenomena, but rather it causes externally desired aspects of the phenomena to conform to an interpretive framework. Within any organizational context, there is a battle to dictate the meaning of data. If an organization commits itself to the collection of sales data and nothing beyond this to understand its market, it would be difficult at a later time to find a suitable escape route leading away from the absence of information. The eye is inherent in the structure of data extending in part from the authority and control of those initiating its collection.,As one goes up the V, both projection and articulation transform to accommodate the increasing complexity of the phenomena; but also while going up, there is greater likelihood of separation between the articulated reality (e.g. employee stress) and the instrumental projection (e.g. performance benchmarks) resulting in different levels of alienation. As one travels down the V, there is less detachment amid declining complexity, which improves the likelihood of inclusion. In this explanation, I am not suggesting that alienation or inclusion is directly affected by the level of sophistication in the data. The V can become narrower or wider depending on design. Complexity itself does not cause alienation between data and its phenomena; but there is greater need for design to take complexity into account due to the risk of alienation. It might be tempting to apply this model to social phenomena directly, but actually this is all meant for the data that is conveying phenomena. Data can be alienated from complex phenomena.,I realize that the universal data model doesn't resemble a standard input-process-output depiction of a system; but actually it is systemic. Projection provides the arrow for productive processes sometimes portrayed in a linear fashion: input, process, and output. Articulation represents what has often been described as "feedback." Consequently, the eye suggests that the entire system is a source of data. In another blog, I support this argument by covering three major data types that emerge in organizations: data related to projection resulting from metrics of criteria; data from routine operations as part of production processes; and data from articulation from the metrics of phenomena. The eye is rather like a conventional system viewed from a panoramic lens. The V provides an explanation of the association between proxies and phenomena under different circumstances.,The simplification movement has mostly been about simplification of proxies and not the underlying phenomena. Data as a proxy is made simpler in relation to what it is meant to represent. Consider a common example: although employees might have many attributes and capabilities, in terms of data they are frequently reduced to hours worked. The number of hours worked is a metric intended to indicate the cost of labour. A data system might retain data focused on the most instrumental aspects of production thereby giving the illusion that an organization is only responsible for production. I feel that as society becomes more complex and the costs associated with data start to decline in relation to the amount of data collected, the obligation that an organization has to society will likely increase. This obligation will manifest itself in upgrades to data systems and not only this but improved methodologies surrounding the collection and handling of data. The model provides a framework to examine the extent to which facts could and should have been collected. Consider a highly complex problem such as infection rates in a hospital. The hospital might address this issue by collecting data on the number of hours lost through illness and sick days used. But this alone does not provide increased understanding of infections; some might argue therefore that such inadequate efforts represent a deliberate form of negligence apparent in the alienation of proxies.,I have a habit of inventing terms to describe things particularly in relation to application development. Experience tells me that if I fail to invent a term and dwell on its meaning, the thing that I am attempting to describe or understand will fade away. I am about to make use of a number of terms that have meaning to me in my own projects; and I just want to explain that I claim no exclusive rights or authority over these terms. In this blog, I have described data as "proxy" for "phenomena." I make use of a functional prototype called Tendril to examine many different types of data. Using Tendril, there are special terms to describe particular types of proxies: events, contexts, systems, and domains. These proxies all represent types of data or more specifically the organization of aspects of phenomena that we customarily refer to as data.,The most basic type of proxy is an event. I believe that when most people use the term "data," they mean a representation quite close to a tangible aspect of phenomena. I make no attempt to confine the meaning of phenomena. There can be hundreds of events describing different aspects of the same underlying reality. I consider the search for events a fluid process that occurs mostly on a day-to-day level rather than during design. Another type of proxy - i.e. a different level of data - is called a context. Phenomena can "participate" in events. The "relevance" of events to different contexts is established on Tendril using something called a relevancy algorithm. I placed a little person on the illustration to show what I consider to be the comfort zone for most people in relation to data. I would say that people tend to focus on the relevance of events to different contexts.,The idea of "causality" takes on special meaning in relation to the above conceptualization. Consider the argument that poverty is associated with diabetes. Two apparently different domains are invoked: social sciences and medicine. Thus, the events pertaining to social phenomena are being associated with a medical context. The social phenomena might relate to unemployment, stress, poor nutrition, inaccessible education, violence, homelessness, inadequate medical care: any reasonable person even without doing research could logically infer adverse physiological and psychological consequences. Yet the connection might not be made I believe because the proxy seems illegitimate. How can a doctor prescribe treatment? If human tolerance for social conditions has eroded, one approach is to treat the problem as if it were internal to the human body. Yet the whole point of the assertion is to identify the importance of certain external determinants. Society has come to interpret diabetes purely as a medical condition internal to the body. This is an example of how data as a proxy can become alienated from complex underlying phenomena. We say that people are diseased, failing to take into account the destructive and unsustainable environment that people have learned to tolerate.,Since there is no ceiling or floor on the distribution of proxies in real life, the focus (on contexts and events) does not necessarily limit the data that people use but rather the way that they interpret it, not being machines. I feel that due to its abundance, people habitually choose their place in relation to data; and they train themselves to ignore data that falls outside their preferred scope. Moreover, the data that enters their scope becomes contextually predisposed. Consequently, it might seem unnecessary to make use of massive amounts of data and many different contexts (e.g. in relation to other interests). But this predisposition is like choice of attire. The fact that data might fall outside of scope does not negate its broader relevance; nor does its presence within scope mean that it is relevant only in a single way.,It is not through personal strength or resources that a person can get a road fixed. One calls city hall. There is no need to build shelter. One rents an apartment or buys a house. In human society, there are systems in place to handle different forms of data. These systems operate in the background at times without our knowledge enabling our existence in human society and offering comfort. Our lack of awareness does not mean that the systems do not exist. Nor does our lack of appreciation for the data mean that the structure of the data is unimportant. In fact, I suggest that the data can enable or disable the extent to which these systems serve the public good. Similarly, the way in which organizations objectify and proxy phenomena can lead to survivorship outcomes. An organization can bring about its own deterministic conditions.,The universal data model - really just "introduced" in this blog - is meant to bring to light the power dynamics inherent in data: the tug-of-war between disablement and participation. I have discussed how an elaborate use of proxies can help to reduce alienation (of the data from its underlying phenomena) and accommodate greater levels of complexity to support future development. This blog was inspired to some extent by my own development projects where I actually make creative use of proxies to examine phenomena. However, this is research-by-proxy - to understand through the manipulation of data structures the existence of ghosts - entities that are not necessarily material in nature. I attempt to determine the embodiment of things that have no bodies - the material impacts of the non-material - the ubiquity of the imperceptible. It might seem that humans have overcome many hostile environments. While we have certainly learned to conquer the material world, there are many more hazards lurking in the chasms of our data awaiting discovery. However, before we can effectively detect passersby in the invisible and intangible world, we need to accept how our present use of data is optimized for quite the opposite. Our evolution as a species will depend on our ability to combat things beyond our natural senses.
Is analytics yet another fad? Is there ,. It does seem so when you look around you as a consumer.,Marketers still don?€?t care, as much, about being relevant to you. You get that umpteenth credit card solicitation from the bank which has already sold you a card. And nothing about a physical Retailer shopping experience makes it personal for you!,And yet your online persona seems to be treated differently & when you go to Amazon & other sites you do get a feeling of getting offers being recommended for you. And as a consumer you flit between your online & offline avatars & this becomes more & more obvious.,What?€?s the difference? , says that only ,This is what software architect Grady Booch had in mind when he uttered that famous phrase: ,A recent Ad Age article carried this comment:
This analysis shows the distribution of data scientists per country, city, gender and company. It is based on ,??(DSC) member database, ,, on sign-up. Not all members provide information about their location, company or gender. A small majority does, and these members tend to be over-represented in US. Differences with our main channel - Analyticbridge, created in 2007 - are highlighted. Data Science Central was created in 2012 and its members tend to be adopters of early data science, machine learning and Hadoop techniques. ,??(AB) has a more technical audience regarding statistical sciences, operations research, predictive analytics, data mining and more traditional analytical techniques, including a thriving forum section featuring technical questions about regression and clustering. Other DSC channels are not included in this study. Both AB and DSC now have similar member counts, though DSC membership is now growing much faster, about 6 times faster.,This analysis was completed by our intern Livan very recently, and supersedes any previous analyses that we did.,US represents more than 50% of members on DSC, a little less on AB.,Fast growing countries include Singapore, Spain and Ireland. Senior data scientists, executives and entrepreneurs are typically found in US, while India has a higher proportion of junior practitioners. Chinese people are very reluctant to provide personal information, which might explain why they are under-represented.,The Bay Area has the highest proportion of data scientists per 1,000 inhabitants. New York and London have more data scientists on DSC in absolute numbers (many in the finance industry), but a smaller proportion per 1,000 inhabitants.,75 % of the top 20 most represented companies are shared for both databases: DSC and AB. Here we share the results for DSC. Note that the top 20 companies (dominated by IBM, Accenture, Oracle, AT&T, Microsoft, EMC, Teradata, SAS, Deloitte, HP) represents vendors (EMC, Teradata, SAS) and giant companies with relatively fewer data scientists per 1,000 employees (IBM, Microsoft, HP). While Facebook, LinkedIn, Twitter and Google might be the most sexy to work for, they are not in the top 10 in this study, although ,??based on 10,000 LinkedIn profiles connected to Dr Granville, distributed across 6,000 companies. The top 20 companies account for about 10% of all data: this data has a ,??(very thick tail). Many if not the majority of data scientists work in small companies, as entrepreneurs, freelancers, consultants, or in education (professors) or government agencies.,Most do not even have , as job title: read the following articles for detail:,Distributions on AB and DSC are almost identical. Here we show the statistics for DSC. How to address gender imbalance is a difficult question. The same is true with racial imbalance (STEM disciplines dominated by people of Asian origin, in US).??
With the development and growth of ecommerce platforms like Shopify, the number of small- and medium- sized ecommerce businesses is growing at an impressive rate. But, with this growth comes a growth in market opportunities for the online villains and fraudsters out there who are looking to make a quick buck. It used to be that only huge corporations had the resources they needed to detect fraud and protect themselves from its damages. But, in this era of big data and , for all, even small mom and pop ecommerce shops have access to the tools they need to protect themselves from evil fraudsters. This article introduces some common sources of fraud problems in ecommerce, and how you can use data science technologies or techniques to protect your business (or soon-to-be business) from risk.,On first glance, it?€?s somewhat difficult to imagine the types of fraud to which a typical ecommerce business is exposed. I mean, you really don?€?t hear much about ?€?ecommerce fraud?€?, do you? Well, don?€?t let the silence fool you. As Elli Bishop guestblogged on the ,, online fraud caused $3.5 Billion of damages to the ecommerce industry in 2012 alone ?€? and the damages are increasing on an annual basis. Let?€?s take a look at the ways in which fraudsters are defrauding honest, and upstanding online merchants.,One serious problem in ecommerce is Card-Not-Present (CNP) fraud. CNP fraud is a type of credit card transaction fraud, oftentimes where fraudsters use stolen card numbers to make online purchases. For each instance that a transaction like this goes undetected or un-prevented, the selling merchant is held financially responsible for refunding the fraudulent credit card charge and for the losses from the merchandise that they?€?ve already shipped out to the fraudster. Since CNP fraud represents a double loss to selling merchants, most merchants want to do everything they can to prevent fraudulent CNP transactions.,Ecommerce businesses are also exposed to fraud problems associated with ,. Account takeovers occur in cases where fraudsters have successfully stolen account credentials and then used those credentials to unlawfully login into clients?€? accounts and make fraudulent purchases. ??As with CNP fraud, ecommerce merchants are left liable for the cost of the merchandise that was shipped out to the fraudster, plus the expense of reimbursing customers for the fraudulent charges that were wrongfully accrued in the account takeover.,Yikes, ecommerce sounds like it can be risky business, right? Well, that?€?s not entirely untrue. This is why it?€?s extremely important to make wise decisions when it comes to your ecommerce solution provider. Ecommerce is hot right now, so new vendors are popping up left and right. While some of these vendors offer very competitively priced packages, more mature vendors offer you an array of options for the support, analytics integrations, and best-in-breed ecommerce security add-ons that you need to keep your business safe, secure, and protected.,Sift Science and Feedzai are two excellent fraud detection and prevention add-ons for ecommerce businesses. They?€?re available to all ecommerce businesses that run on the ,. Since Shopify has been in the ecommerce game longer than most other solution providers, they?€?ve had time to put together a solid suite of support offerings. In fact, Shopify offers over 800 add-on applications that its customers can purchase to make their lives easier and more worry-free.,For ecommerce fraud detection and prevention, the Feedzai add-on application is an excellent selection. Feedzai prides itself on its delivery of powerful machine learning, data science, and big data solutions to help small- and medium- sized businesses. Feedzai?€?s star offering is a fraud prevention software that runs off of Feedzai?€?s proprietary risk and fraud detection engine. While of course Feedzai isn?€?t giving away its secret sauce, they?€?ve published a white paper that at least clarifies their basic methods. To summarize the white paper in a few brief words, Feedzai has combined behavioral modeling / profiling, machine learning clustering algorithms, and a rule engine to detect and prevent ecommerce fraud for businesses that run on the Shopify platform.,In fact, it?€?s not reasonable to expect that you can detect and prevent online fraud simply by deploying a few simple machine learning or statistical algorithms. Ecommerce fraud is a lot more complicated than that ?€? you should expect to incorporate behavioral modeling, a rule engine, and some solid domain expertise to even begin moving towards finding a solution that will work. This said, machine learning and statistical algorithms are one essential ingredient.,And, what type of algorithms are useful for detecting and preventing ecommerce fraud? That?€?s a good question ?€? there are many options depending on the approach you?€?d like to take. You can use time series anomaly detection algorithms to automatically detect suspicious or unusual events and trends as they occur. ????If your time series is periodic, then you?€?re likely to get good results by using an aggregating window function and then following that with a k-nearest neighbor algorithm. In R, you can use the following 2 families of window functions to aggregate your time series data.,As a general approach, you could first you?€?d divide your time series into windows, and then use a similarity function to calculate an anomaly score for each window. In this manner, it is possible for you to perform automated anomaly detection of time series.,And, just in case you have never used R before, here is a quick intro to get you going in a hurry. The easiest way to set up R on your machine is to download and install the ,. The k-nearest neighbor algorithm is in R?€?s ,. You can actually just copy and paste the sample code below to start playing around with classifying data using R?€?s knn() function.
 , , , , , , , , , , , , 
" Episode 9 looks at the best places for analytic competitions.,Here are my top 4 sites. They range from the the well known to some more specialist sites.??There are 3 marketplaces I know of, and a 'world championship' currently running.,Kaggle is the world's largest community of data scientists. They compete with each other to solve complex data science problems, and the top competitors are invited to work on the most interesting and sensitive business problems from some of the world?€?s biggest companies through Masters competitions. Their most famous competition to date was run for netflix where a prize of $1 million was awarded. ??Details of the winning team and how they did it can be found ,.,Claims to have a global network of millions of problem solvers, a proven challenge methodology, and cloud-based innovation management platform combine to help their clients transform their economics of innovation through rapid solution delivery and the development of sustainable open innovation programs. Clients have included Astra Zeneca, Booz Allen Hamilton, Cleveland Clinic, Eli Lilly & Company, NASA, Nature Publishing Group, Procter & Gamble, Scientific American, Syngenta, The Economist, Thomson Reuters, and the US Department of Defense.,Also claims to be the world?€?s largest Crowdsourcing Development, Design, and Data Science Platform. The [topcoder] community gathers the world?€?s experts in design, development and data science to work on interesting and challenging problems for fun and reward.,The Big Data Analytics World Championship is run by the Professional Services Champions League (PSCL) and involves two online qualification rounds and a Live World Finals event held in Texas, USA in November 2014. Participants from more than 100 countries will compete individually in two online qualification rounds.,They also run other ?€?world championships?€? such as , (Financial Modeling World Championships) and , (Loyalty and Gamification World Championships).,??,
,My , contains tips from a 25 year veteran of the analytic profession."

Defining big data is now a hot topic. Berkeley University posted ,??(including me). Here our goal is to offer a very detailed, comprehensive definition that (hopefully) suits everyone.,:,This is about collecting data via sensors, log files, or any other data/signal capture mechanisms. It involves RAW internal data (NASA videos of the night sky to identify exo-planets) and external data (vendor or third-party data, Internet data such as tweets about your company).,Typically, Level 1 is big data, but ,, ,??(with low content value, read??,), or rather static (,) making this data looks large - but possibly shallow - rather than big.,Also note that there are two ways for data to qualify as big data:,Collecting level 1 data is mostly a question of ,.,Here we are dealing with data summarization: deciding which metrics to track (raw and compound metrics), how to store and blend the somewhat cleaned, curated data (using database architecture - SQL, NoSQL, NewSQL, Hadoop, grapth databases). This is the , step.,At this stage, we are dealing with highly refined data, possibly no longer big data - available as reports, visuals, email alerts, automated (machine-to-machine) bidding, or detection of taxpayer accounts worth an IRS audit. In short, actionable data and insights. In some cases, it is still big data, e.g. in the context of scoring all credit card transactions, predicting the value of any home in US (including trends), or producing a LinkedIn profile connection gragh. Most of the time, however, it is not big data. Whatever data it is, this is the , step.,Data from level 1 is rarely accessed by level 3 data scientists. But sometimes it is, for instance accessing raw log files (level 1) to analyze a fraud case (level 3). So there are passerelles - and feedback loops - between the three levels.,We will also publish an article about the uplift provided by big data - broken down by industry - over the baseline consisting of leveraging small data only. It is our belief that ,, compared with small data.??
The Zipf's law states that in many settings (that we are going to explore), the volume or size of entities is inversely proportional to a power , (, > 0) of their ranking. This has important implications in predictive modeling, discussed below.??,In the following example - based on real data from the DSC member database - we will see that this law is accurate for the 3 metrics that were expected to follow the law: members per company, per city or per country. In particular, it shows that the distribution of DSC members by company, very closely follows a Zipf distribution, where the number of DSC members, for a given company, is proportional to ,^{-,}, where , is the company rank, and , = 0.7, see figure 1.,The ,??is characterized by a rank decay well modeled by an inverse power function of parameter , > 0. It applies to the following settings.,There are , atoms (e.g. DSC members) distributed over ,??active entities (e.g. companies). By active entities, we mean (in our example) companies with at least one DSC member; those with 0 member are not taken into account. Both , and , are very large, with , much bigger than ,. Indeed, generally??,/??, tends to infinity as , tends to infinity. A few entities are huge (IBM, Accenture, Oracle in Figure 1), yet there are tons of entities with very few atoms.,Another example where Zipf's law applies is Internet domains: a few domains like Google, Facebook, Twitter, LinkedIn, and Yahoo dominate, with Google being the big king (in terms of monthly users or pageviews) based on Alexa ranks, while billions of small websites barely get any traffic, even when combined together.,Note that if , tends to infinity and , < 1, the Zipf distribution does not make sense from a mathematical point of view, as no moment exist. It must be regarded as a stochastic process in that case, and the explanations regarding how the process is generated still do make sense. It's only a mathematical annoyance, but not causing any problems for modeling or predicting purposes.,We have done simulations to try to understand the mechanism, and our conclusion is as follows.,Zipf's law also applies to celestial bodies in the solar system, because the process is very similar to the way companies are created and evolve, involving mergers and acquisitions. Here's how it works, described in algorithmic terms, applied to companies, and celestial bodies alike. But it applies to many other frameworks as well.,How these entities may grow over time is illustrated in the following ,, and explained ,.??,Evidently, this works only for certain metrics and settings meeting the requirements about , and ,, as described in the previous section. That's why it works for,but not for age, salary or gender distributions. Indeed, if you perform simulations with multiple layers of clustering, you won't be able to replicate Zipf laws , over successive iterations. Instead, rather than a Zipf distribution, you would end up with something that looks like a rotated S curve.,. It features the distribution of DSC members (for a specific channel, not disclosed here) per company (see top 10 companies in Figure 1 above), city (worldwide) and country - by itself a very interesting data set.,For each of the three fields (company, city, country), model-fitting with various distributions was attempted, using both Excel trendlines and mean-square errors (which tend to not work great in contexts like these with highly skewed distributions - instead, I recommend to use ,??rather than R-Squared). Nevertheless, it is obvious that the Power trendline in Excel offers the best fit in each instance, far better than linear, exponential, logarithmic or polynomial trendlines. ??,The challenge of the week, this week, consists of ,, resulting in Zipf distributions. More specifically, we ask you to perform the following simulations to assess whether our assumptions are correct. Alternatively, a mathematical proof is OK.,Let's assume that we have , = one million atoms, for instance space dust particles., (write it in Perl, R, Python, C or Java; test it)??,Each particle is assigned a unique bin ID between 1 and 1,000,000. Each particle represents a cluster with one element (the particle), and the bin ID is its cluster ID.,Iteration: repeat 20,000,000 times:,Once the algorithm stops, the final cluster configuration represents the current solar system, or companies in US - most very small, a few very big.,:??can this cluster process simulation algorithm??be adapted to a distributed environment, say Hadoop?,Structures suspected to be generated by such clustering processes can be modeled using hierarchical Bayes models and simulations (MCMC) to fit model parameters with data, assess goodness-of-fit with Zipf distribution, and then predict the evolution of the system - or at least some of its core properties such as mean. This might require adding a time dimension in the simulations (represented for instance by the iteration, in the above algorithm).
 , , , , , , , , , , , , , 


In this blog post I summarize my current findings in the Zipf's law clustering process.,I have done some extensive simulations on clustering of entities, which should lead to Zipf's law..,To summarize the simulated clustering process (which complies to the guidelines dr. Granville provided in ,):,I define the similarity in size of clusters in a very straightforward way:,In the similations I have done X was 1.1 and X was 3. When X is 3, this means a lot more cluster combinations are seen as similar, and thus the combinations of clusters go more rapidly in time (I define time as the iterations of the for loop), as p>q.,This way, we end up with a vector size of 1 after approx. 1.4e6 iterations of the for loop when X was 3 and 2.2e6 when X was 1.1 (approx. because the clustering is a stochastic process). After every 1e4 iterations I save the current state of the vector of clusters and I plot them subsequently in a .GIFs, which you can see below. In these .GIFs we have size as amount of entities in the cluster, frequency the amount of times a cluster with a corresponding size appears in the vector of clusters and time the iteration of the for loop.,We immediatly notice the linear trend between log(frequency) and size, except at the very end of the time scale. In the end the clustering process leads to a cluster that has all entities. I guess this could be compared to a monopoly or a sort of black hole. This is also where I see some problems with the analogy with user of a social network, f.e. users of DSC and their company affiliation, as actual clustering between companies seems more farfetched here. To have more flexibility, I would insert an extra hierarchy in the model: a clustering between companies and a clustering of members in the network within one company. The differences, if any, with the one-level model could be interesting to see.,Moving forward, I believe the crux of this story lies in the definition of "similarity" between two clusters and the values of p and q. At this moment I believe p (0.8) and q (0.3) are defined a little too high, although in an actual clustering process the correctness of the definition of p and q would also depend on the resolution of the "time"-variable. In terms of similarity there are a lot of possibilities and it would be interesting to see if these all lead to distinct clustering phenoma, other "similarities" could be the absolute difference in sizes of two clusters or perhaps adding an extra variable to each cluster that is updated in terms of time (i.e. the cost structure of a company that is reorganised after some time or stars that age) and f.e. only define similar cluster if this extra variable is similar. This extra variable also introduces an extra hierarchy in the model.,As for the parallelisation of these simulations, in essence parallelising the for loop, especially in a MapReduce context, we can look at distributed MCMC: see f.e. ,,??,??and??,. It should be noted that this parallelisation is not very straightfoward as each step depends on the previous one (which is in essence inherent to MCMCs).,All these simulations where done in R, I can of course share the code if wanted. Any comments/feedback are more than welcome. In terms of scientific work, this might lead to some sort of publication.,The clustering process for X = 1.1,The clustering process for X = 3:
Graphs are everywhere, used by everyone, for everything. Neo4j is one of the most popular graph database that can be used to make recommendations, get social, find paths, uncover fraud, manage networks, and so on. A graph database can store any kind of data using a Nodes (graph data records), Relationships (connect nodes), and Properties (named data values).,A graph database can be used for connected data which is otherwise not possible with either relational or other NOSQL databases as they lack relationships and multiple depth traversals. Graph Databases Embrace Relationships as they naturally form Paths. Querying or traversing the graph involves following Paths. Because of the fundamentally path-oriented nature of the data model, the majority of path-based graph database operations are highly aligned with the way in which the data is laid out, making them extremely efficient.,The complete blog post at??,??demonstrates an??use case that is based on modified version of StackOverflow dataset that shows network of programming languages, questions that refers to these programming languages, users who asked and answered these questions, and how these nodes are connected with relationships to find deeper insights in Neo4J Graph Database which is otherwise not possible with common relation database or other NoSQL databases.,Topics in the above Use Case:
For explanations about the methodology, including source code and possible improvements, ,. It also provides links to our other three listings.

Spencer Greenberg holds a B.S. Magna Cum Laude in Applied Mathematics & Computer Science, from Columbia University, and a Ph D. in Machine Learning, from NYU. Prior to ,, he was Software Developer, Neuberger Berman, LLC and Engineer in The Investigative Project for Terrorism. Spencer has been interviewed ??on CNBC, Bloomberg News, Canada?€?s BNN, and in the Wall Street Journal. He has also lectured at Columbia School of Business, and the NYU Stern School of Business.,A. We apply our own proprietary machine learning approach, which performs a form of Bayesian probabilistic modeling. We have found that off the shelf machine learning solutions usually do not work very well in our problem domain.,A. I first became interested in machine learning outside of an academic context, and began to study it intensively on my own. It quickly dawned on me how incredibly powerful machine learning techniques are, and how important they will be as technology continues to advance. I decided to pursue graduate studies in machine learning to improve my knowledge rapidly.,A. Machine learning addresses the question of how to make accurate predictions from data by having a computer automatically learn from examples. The range of applications that this paradigm applies to is tremendous, from face recognition, to game playing, to product recommendation, to language translation to stock trading.,A. Machine learning has many very interesting future applications:??,A. There are exciting developments in machine learning applied to extremely large data sets, sizes not tractable with classic machine learning methods. Additionally, deep learning methods, which automatically extract important features from data as they learn to make predictions, have been seeing considerable advances.,A. It's tough to say, but it's clear that decades of fruitful research into machine learning lie ahead, and hundreds of further applications.,A. It is not so much that machine learning has a single killer application that has radically changed our lives. Rather, machine learning has become integrated into hundreds of different applications. Often we are not even aware of it being there, even as it helps us. For instance, Netflix uses machine learning to improve its movie recommendations for us, Amazon applies it to recommending better products, Google uses it to translate languages, and digital cameras relies on it to identify faces in photographs. We're using the output of machine learning algorithms all the time, we just don't realize it.
For explanations about the methodology, including source code and possible improvements,??,. It also provides links to our other three listings.
D3.js is an awesome library for visualization. However, it requires a developer to perform the magic. So far, it has not been so popular with traditional data analysts.,At vida.io, we set out to bring D3.js to less technically savvy users. We want to enable them to create amazing data visualizations with D3.js. Our approach is template.,It?€?s a D3.js visualization that you can reuse by just plugging in new data. For example, one of our customers want to be able to compare metrics of two different entities. We made a template with multiple side-by-side bar charts. Below are comparisons of India vs. China and Apple vs. Orange.,India vs. China,Both visualizations share the same JavaScript and Stylesheet. User can clone India vs. China template, add new data, update colors to get Apple vs. Orange.,Data for India vs. China:,Data for Apple vs. Orange:,You can see data format for two visualizations are identical.,You can create new D3.js visualizations in 3 quick steps with vida.io??tool.,Here?€?s a complex template used by the??,??to display metrics of US states.,With template, D3.js is easy to use, reuseable, and affordable. Add it to your toolbox for business intelligence and data analysis!,.
This blog is about the peculiar nature in which software sometimes gets developed. I hope that many readers will recognize the relevance of data science in the examples taken from my own projects. I propose that development is the product of creativity more than accreditation. Creativity is something complicated that interacts with a person over his or her life circumstances. Many people know how to write . . . sentences and paragraphs. However, the ability to write well does not necessarily make an individual a good novelist or creative writer. Taking into account the relevance of creativity as force in the process of development, it is necessary to recognize the importance of a person's life whether or not its events are specifically related to any particular undertaking. I will be detailing the meandering way that I personally tend develop software. I don't follow a straightforward path characterized by planning, schedules, and deliverables. Development for me tends to be sporadic, meshed in the details life, and closely tied to my interests. ??So it is rather non-business-like.,I am not a professional programmer. I am a bit of peculiarity. I program therapeutically as a form of relaxation. I discovered that programming helps to take me away from stress. Yet I would almost never describe programming as "enjoyable." In fact, I hardly receive any enjoyment from it. It is a powerful diversion that often provides relief from more mundane matters. I later found that it also helps me to maintain my concentration. These days driven by some desire to handle large amounts of data in an unconventional manner, I periodically make use of my coding activities to help me produce worthwhile applications. For instance when I was considering an academic career, I didn't want to rely on platform-specific commercial software to keep track of my notes and references. Neither did I wish to rely on my recollection. So I created a search engine to store and access files in exactly the manner that I required: a header file containing citations in various styles along with my observations; quick-click access to documents that satisfy the search parameters; and portability so I can access the data on different machines and operating systems for the rest of my life. It is the perpetuity that I needed most. I wrote programs to access the data in Java and Visual Basic. Under the right conditions, I am both productive and relaxed while programming; and although I don't have any exact formula to explain its development, these are usually the same conditions that give rise to greater levels of creativity.,I mentioned in previous blogs that I created a visualization environment called "Storm" to study all sorts of dynamic data. I actually use the term "storm" to describe a family of algorithms originally developed to evaluate trading activity; this was not for trading purposes exactly but more to facilitate a type of accounting. I was searching for different ways to characterize "capitalization" - that is to say, the build-up of perceived and actual value - in order to summarize the impacts of flows. When a financial company holds assets for many people, I find at least two major operations: 1) holding the assets in trust physically or by name; and 2) keeping track of ownership interests. (I apologize for not knowing the industry jargon.) So a securities company might hold 100,000 shares of a particular equity; but then it has to keep track of the clients having interest in that equity, for example 1,500 clients for the 100,000 shares. These distinct duties can be distributed to different companies. In the case of operations maintaining a record of entitlements or interests, much of the work is "pure data." The company has no clients, and it has no assets of its own: it just keeps track of who owns what and maintains supporting transactional records. A problem occurs when the basis is other than cash: for instance, the basis of ownership might be mutual fund units or shares. The cash-flows then become detached from the unit-flows. One day I decided that it might be worthwhile to maintain all sorts of parallel valuations reflecting a variety of interests - one type being abstract perceptions of worth or capitalization.,I wrote a trading simulation in order to give body to my conceptualization. While examining segments of trading data against lines representing capitalization, I discovered some interesting behavioural peculiarities. (I never actually used the term "algorithm" until I had to explain these different lines - all formed using the storm family of algorithms. So it is interesting how I was trying to develop a program for back-office operations, which then became part of a trading simulation.) A good lesson here is how ideas for software can emerge during development itself rather than planning. Perhaps this is already obvious. Let me express things a bit differently. I almost never create an actual plan to guide development. A lot of companies spend a great deal of time planning. I largely skip this process! Some might ask, how does one know what the outcome will be? In fact, I never exactly know this. So I definitely find myself a bit surprised sometimes by the outcomes. We need to put things into perspective. When a person deals with more data than he or she can immediately understand, the overhead involved in planning to deal with an unknown is both ironic and wasteful. Programming for me is part of the discovery process. I use it for research not just for production.,I want to explain for those that don't program, it occurs at the fingertips. People can spend all sorts of time strategizing, building their cases, arguing to take development in a particular direction. But in the end, a person has to rely on his or her fingertips. What if the software doesn't do the job? Change it. That's what "soft" part means. What if the developer is just wasting time by producing something that nobody wants? Well, my understanding is that people debating matters are often working out what they want. If they know what they want, pray tell and get the process rolling. If they don't know what they want, then of course they should discuss matters to their satisfaction. For the record, when I do my own developments, I figure out what I want during development, which is never-ending. What I want tends to change along with the capabilities of the software. If I knew exactly what I wanted and the software did precisely this, there would hardly be a need to do research or make improvements. In short, the process of development for me is, I'm not sure what the best word might be, both happenstance and agile. I don't spend much time sorting out details for software that doesn't exist, to perform processes yet to be done, to control data that I barely understand. However, once the software takes form, it often becomes a "living" part of my life, changing to accommodate my relationship with the data. I now want to delve deeper into the story of Storm and explain its incarnations over the years. I hope readers enjoy its history, which leads up to an application called "Earthshield"; this will all be explained.,I posted some structural details about the storm family of algorithms in an earlier blog. Since a structural explanation would be too much of a digression here, I will simply detail some of the general behaviours: E for "eccentric" (later called "paranoid") represents an algorithm that exhibits sudden jerking movement in response to trading activity; R for "resistant" (later called "reluctant") shows exceptional insensitivity to trading; and finally, A for "average" (later called "reactionary") lies at some point between the extremes of E and R. E and R are mathematically rigid while A can be set as a fractal - i.e. A is not a mathematical average but the result of a process bounded by two extremes. I present the three lines below for a particular publicly-traded stock. Notice how E leaps ahead of R and A. E routinely presses against the boundaries. E provides early warning, but it is also prone to false positives and negatives in relation to the broader trend. E provides the most plays but at the greatest level of risk. On the other hand, A tends to touch but not press against the boundaries except under exceptional circumstances; so some might use A for market-timing although I certainly don't make any recommendations. R almost never touches the boundaries except in extreme cases. R offers the highest level of safety but the least amount of opportunity. See how it is all so mathemagical strictly from an algorithmic-behavioural standpoint.,The illustration above shows only three lines. Yet I said that A can be set as a fractal thereby offering many more lines if the user prefers. If a large number of lines are set side-by-side in a lattice, the trading activity results in a 3-dimensional plume stripped of its amplitudes to reflect kinetic persistence, which in the past I have described as "sentiment." (I have described paranoia, reluctance, and reactivity as simulated aspects of investor sentiment. I don't want to complicate matters, but I call an amplitude-free plume a displacement plume; this is algorithmically extracted from a pricing-free plume called a relational or differential plume.) In the next set of images, the left-side pattern shows trading prices for a particular equity in the high-tech sector. This equity experienced a terrible day during the sampling period as indicated by the cliff. The corresponding displacement plume is shown to the right. I decided to flip the displacement plume horizontally and add some reference lines to show that it is indeed derived from the same trading data: the only difference is that the plume indicates when the technicals are "bottoming out" and "peaking" by distributing the kinetics over the entire visualization field. It's not a magic trick. It's also not sophisticated math. It's due to a different and perhaps more complicated perspective on the meaning of the data. I want to point out however that plume sentiment and price are quite disassociated: the ride between boundaries might involve huge changes in price or little. So 3-dimensional displacement plume might benefit from an extra dimension: one comes to mind although, as I will explain shortly, I'm not developing this technology anymore.,Here are some general observations about displacement plumes: 1) they are designed to fit within the boundaries although the algorithms do not know the upper and lower price limits in advance; 2) the plumes are visually in opposition to the market but not negatively correlated due to lack of amplitude; 3) for the same reason, opposition is achieved whether the trading pattern is extremely sharp or gentle; and 4) since the algorithm doesn't know the future price, an E-type plume is often forced against the boundaries, thereby providing a means of auto-adjustment if desired. One way to regard the behaviour of A, R, and E is to interpret the lattices as people with different levels of sensitivity to trading activity. Pricing data tends to receive much public attention; but plumes as a characterizations of the underlying phenomena should probably be studied more closely. (My blog in a couple of weeks will describe a "Universal Data Model" providing a basis to examine different types of phenomena against the invoked metrics. Please consider reading it.),This blog is about how life circumstances affect development; it is not about trading. Nonetheless, since I have crossed paths with the market so often, many concepts associated with trading continue to have a strong influence on my projects. I had a great interest in derivatives at one time. I passed a number of industry-standard courses to become a stock broker. I thought about continuing my education and maybe entering the field of portfolio management. I was drawn to the whole idea of trading as a way to manage risk. (If I really had to describe my state-of-mind, I would say that I was "finding myself.") As the preceding plume images show, there is often a great deal of blurriness in the patterns - conceptually simulating differences in sentiment. But at some point during my studies, I noticed the "stars coming to alignment"; there seemed to be a significant amount of clarity. I apologize for not recalling exact details except a particular date - 911. The events of 911 are suspicious by their relationship to a technical peculiarity in the markets. ??The next set of images is for a market index or composite. (Notice the more wave-like displacement plume generated by a composite compared to an individual stock.),The displacement plume should not have known about the terrorist attacks in advance; yet 911 occurred quite near boundary-contact. There are a few ways to think of the 911 "point" shown above. Before reaching it, a person might rationalize the situation as follows: "The market seems to be heading to the boundary. I should anticipate it hitting the boundary and take this opportunity to clear my position." Another perspective that is ironically quite the opposite is follows: "The market will be hitting the boundary. I should take a position since the market seems unlikely to push beyond it." So a bet can be justified either way depending on the perspective and exact timing. I recall the one horrific thing about 911 from a trading standpoint that could not be predicted. My derivatives textbook failed to mention anything on the subject: it involved the closure stock markets. I had failed to take it into account. Since a person can neither clear nor assume a position, closure throws timing out the window. It seemed like the house could invent and enforce rules capriciously. I took this as an omen never to purchase derivatives again. 911 actually caused me to steer away from an investment career. However, I was left with this unusual algorithmic imaging environment.,I think that the ambivalence a nation has to its creative people is related to mass consumerism. There is an expectation of products going to market, and then others can pick and choose from the items they see on the shelf. However, that is a production model that has little to do with the development of creative solutions. I know there has been a strong parallel between production and innovation where it almost seems like production triggers innovation. Certainly from my standpoint, production never has to follow development. There is much to support the assertion that I am simply obsessed with my prototypes and with data. Doesn't it seem more reasonable for innovation to extend from obsession? I think that a certain percentage of the population is passionate about things that make them borderline pathological. But there are pervasive social normatives that suppress or at least fail to support those natural traits and instincts. On one hand, we want people in society to reflect that society and therefore share some commonalities. Conversely over the course of normalization, we might deprive society of the forces that drive change. I read it in posts and blogs all the time. Having a lot of data is pointless, the usual argument goes. That's actually a normative construct rather than an educated opinion; it is meant to prevent the transformation of our society. In any event, I think I'm perilously close to an entirely different kind of blog. So let's get back to the narrative about me with this amazing imaging environment but nowhere to use it - in a sad world that doesn't care.,Like Leeuwenhoek in his days collecting samples for his microscope, I started gathering different sorts of data to examine using my new imaging environment. Fortunately, the world was really starting to embrace the Internet. I found myself with access to all sorts of interesting data: tidal levels, storm speeds, and earthquakes. I was amazed to find that it often didn't matter what kind of data I fed into Storm. I would usually get pretty interesting plumes. Above is an image I believe from tidal levels I recall from a military database. I was using an unusually large lattice just to see what would happen. As indicated, I found an algorithmic anomaly visually resembling a squirrel. Readers can judge for themselves. I placed an actual squirrel image on the left for comparison. One wonders what other curious animals await discovery. I was actually wondering at one point whether the formation of liposome membranes and cell lattices might be explained mathematically - like the unpacking of a compressed file. The image looks rather biological.??I generated the next image from raw data that I found posted on the Internet: respiration (left); heart rhythm (middle); and blood pressure (right). Apart from facilitating a new form of visualization, I was also interested in using plumes for triggering purposes - i.e. early-warning signals. I considered two different strategies at the time to interpret heart rhythm problems: 1) erosion from baseline patterns; and 2) planting kinetic traps at strategic parts of the visualization field. Since I am not a medical professional, I found it unlikely that my contribution would be taken seriously regardless of the level of success or how much time I spent on the research. So I didn't put too much effort into the development of pattern-matching or trapping. Due to issues of financial instability at the time, I was not in a position to simply pursue my personal interests regardless of the long-term benefits to society. Rarely have I had all of these things together: time to pursue development; freedom to choose my direction and pace; and resources to keep me going. In particular, lack of stability has almost always delayed or terminated projects.,The time that I started studying earthquakes represents a transitional period for me: it forced me to come to terms with the idea of "social obligation." I was back in school learning a skilled trade and also recovering from some health problems. (I had been pinned under a minivan. Perhaps I was dealing with certain psychological issues related to having a car over me.) In those days - and I believe the situation continues to be true today - the whole idea of predicting earthquakes evoked scepticism and ridicule. It's a bit like the whole argument against big data: it's impossible to extract value just by collecting more; therefore stop collecting more, so goes the argument. Extracting value is indeed unrelated to collecting more data. The whole idea of acquiring supplies is to support production. The question of how to facilitate production is quite separate from the issue of supply levels. In any event, just to get the details straight, I was 1) incapacitated; 2) unemployed; 3) studying earthquakes in my spare time; and 4) being told that there's no point trying to make predictions. Meanwhile during my vocational studies, there was a massive earthquake that killed an enormous number of people. This forced me to detach myself from the world in order to make sense of my role in it. I came to accept that sometimes a person's role is just plain limited. However, I tried to create reasonable opportunities for social intersection by doing what comes naturally - following my obsession with data. ??Since I was expecting to enter a career servicing furnaces, I decided that I would eventually be short on opportunities to study earthquakes. So I tried to satiate my interests as quickly as possible. Earthquake data is noisy especially it seems near populated areas. I "assume" (not being a geologist) that rippling is more coherent in an open countryside if the ground exhibits geological homogeneity. I experimented with ways to obtain wave patterns as shown on the image below. I should explain that I was doing this research "on the fly" - not really keeping good documentation (having planned to service furnaces for a living.),"WesternQuakesAFI" on the image above is from Canada. "AFI" means Ambient Flux Index. Notice the seismic readings at the right side of the image, which I hope will show that the plume is actually adapting to the activity. I call it a relational plume because it isn't based on absolutes. I interpret the data almost like stock prices where there is no ceiling; the plume continuously re-sensitizes itself, focusing more on propensity than magnitude. The only thing that can cause this particular plume to lose its grip is absence of change. The AFI must seem rather impressive to make sense of chaotic ground signals. (I'm not exactly certain if I can recall how I came up with it now so many years later.) Try to determine the logic that I rather expertly used to determine the points to click on the interface to show relative lows and highs. I make use of a particular strategy. The technique doesn't work on induced readings such as heartbeats. A displacement plume is premised on the predictability of propensity once amplitudes are removed: this bifurcates magnitude from behavioural tendency, which are so habitually conflated in prediction. I might elaborate on another blog at some point.,After successfully studying earthquake data ("success" being defined by my ability to study the data rather than predict quakes), I developed something called an "Earthshield." A plume is generated using data from a single data-logger or monitor. So it occurred to me to systematically go through all of the data-loggers in a particular area to produce a map: through this mapping process, one might be able to see "risk progression." An Earthshield is a map of risk. I understand the reluctance of organizations and seismologists to predict earthquakes. Prediction is certainly a good circus trick. Although people go to see the circus, their focus on the performance is unproductive. When would ambulance and other emergency services be most incapacitated is the question - not when exactly when they will have to deal with an earthquake. If there were an ice storm today, would these deployable assets be in a position to deal with the challenge? We're sort of focusing on the natural disaster as if we have the ability to control it. Does creating the software and collecting data mean that we know how to predict earthquakes? It means that we will have the means to develop our understanding. Below I show an Earthshield over the State of California. The GUI reveals the data settings in this case: between 2004-11-01 and 2004-11-25. It's really interesting. I apologize for not remembering the colour scheme. I'm uncertain whether blue or red indicates more riskquake.,I was impressed by the Earthshield - and rather surprised by own handiwork. So I tried to introduce it in a setting to encourage worthwhile social outcomes. I made an appointment to see a university professor from geography. Obviously not being a seismologist or geologist, I was rather out of my element in regards to earthquakes. So I said that I wanted to study potholes. I hoped to use city data to produce an Earthshield to show risk of potholes. At the same time, others in the academic community would eventually gain access to the technology for their own purposes - perchance even to predict earthquakes. The professor was surprisingly cooperative and responsive to my interests. However, the city that I hoped to use as a source of data seemed . . . maybe "apathetic" is word. I didn't receive any response to my enquiries. I try to "create reasonable opportunities for social intersection by doing what comes naturally to me." It wasn't lack of data per se that thwarted me but signs of indifference and apathy. The city wasn't just a potential source of data but also the main beneficiary for my research. My whole undergraduate experience had been about a 4-year commitment that the market was quick to devalue after I graduated. Maybe I didn't want to spend another 6 to 8 years of my life developing something nobody wants.,I started off the blog describing a type of accounting application. However, here near the end, I'm discussing risk of potholes. This is not the sort of progression that one would find in a "business environment" or indeed any other type of professional setting. I believe that business would tend to question not only the program itself - its relative value to operations - but also its nature, how it came about, and the qualifications of the developer. The contextual focus of organizations limits not just aspects of development but also developers. That which exists does so only within the framework permitted by the setting or environment; and what is true of software I believe is equally so in relation to developers. If a work of art is produced by a painter or sculpture, the work often speaks for itself. Neither it nor the artist is defined by the environment. Development is a creative process where the work should stand on its own. But this is not the case at all, and neither is a developer free to simply follow his or her passions. Creativity in data science is essentially awarded rather than recognized. "Based on your qualifications, you seem to be an exceptional individual." This line of reasoning can both inflate and deflate the scientist. Unless every data scientist has a computer science background that can be confirmed on paper, it might be easy at some point to dismiss non-conforming individuals. Entire industries have emerged to help people gain the appearance of being qualified. But as I have pointed out in this blog, what appears on paper does not necessarily reflect the complex realities of individuals.,A business undertaking might persist if the expectation of return is quite strong; conversely, it might not commence in the absence of such expectations. A labour of "creativity" can start or stop with the developer whose interests shift and sway over the course of his or her life. I think in general in relation to creative initiatives, organizations can't treat developers as they would products that can be bought off the shelf and returned. I hope my story of the Earthshield demonstrates the potentially awkward and meandering paths towards development. Certainly when countries direct public resources to prop up struggling companies and economies continue to be mired, one might question whether business normatives remain relevant. For once the world is hostile to both development and developers, forcing square pegs to fit into round holes, quite possibly at some point organizations will start to get only round pegs. In my blogs I often discuss issues from the perspective of organizations. Here, I chose to consider development as a "lived experience" - i.e. something highly personal in nature. In terms of the Earthshield, development halted many years ago: so using it to study potholes, retail demand, logistics, theft, vandalism, congestion, equities, or anything else is all in the past. Maybe round pegs don't develop Earthshields.
Are many analytics leaders stuck in stages 1 and 2 of leadership while the companies are trying to move towards being Analytical competitors?,Point to ponder. Do we need more leaders now to say ?€?why not?€? and less to question ?€?why?€??
When Edward De Bono spoke about the six thinking hats way back, he definitely did not visualize the emerging field of data science. The hats were meant to be an aid for lateral thinking and brainstorming. Over the years, the hats have seen many creative uses in the corporate world. 3M used the six thinking hats for new product development, a new type of duct tape. Boeing used the six thinking hats to resolve a management vs. union deadlock. And J. Walter Thompson is reported to have used the method to develop an ad campaign for the Ford Focus.,Can the 6 thinking hats really enable such lateral thinking or am I talking through my hat? And where do data scientists come into this argument?,An average data-scientist does have to wear multiple hats to do his job, from mining through data to delivering insights that make a difference. But the six thinking hats are not just tools for brainstorming, they are the tools for his job; hats she/he needs to juggle everyday, all 6 of them, to be precise, to be successful at work.,Here?€?s how:,??calls for information known or needed. "The facts, just the facts.",The data scientist needs: Data, just the data, in all its myriad shapes and sizes from historical customer transactions to syndicated research papers, from the unstructured text data in social media to the king of all data, Big Data. Data is his morning caffeine fix, the starting point for his work, his job, his life.,??used to manage the thinking process. It ensures that the 'Six Thinking Hats' guidelines are observed.,The data scientist needs: The blue hat to generate insights from the cartloads of data he sees every hour. The blue hat applies to the statistical algorithms he needs to use; shuffling from logistic regression one day to perfecting Natural Language processing the next; from analyzing normality one day to researching Black Swan events the next, as easily as if they were multi-colored candies to choose from a candy store.,is judgment, the devil's advocate or why something may not work.,The data scientist needs the black hat to develop hypothesis based analysis. The fanciest statistical model may not work if the assumptions behind it are not sound. Donning a black hat, the data scientist can play the roles of the lawyer and the accused, both at the same time and free himself from the paralysis that data can lead to without arguing the whys and hows of analysis at every step of the way.,??symbolizes brightness and optimism.,The data scientist needs large doses of the yellow hat, everyday. Just when the deadline approaches, the project scope gets changed; just when he thought his model was working perfectly, a new variable gets literally dropped out of a hat, not one of Bono?€?s 6. Yellow hat, is not just a hat, it?€?s a survival tool for the data scientist?€?s day in the highs and lows of the data valley.,??signifies feelings, hunches and intuition.,Intuitively, the data scientist does not depend on intuition, right? He has his data to back him. But then there are dark spaces that even the biggest of big data cannot reach, there are business rules that need to be applied for every statistical model; just that right way of looking at data that develops into the ?€?Erureka?€? beer and diaper insights moment every data scientist dreams of making on the road to creating new ?€?Moneyball?€? history. Call it trial and error, or simply, red hat.,: focuses on creativity: the possibilities, alternatives and new ideas.,The data scientist needs the green hat to think out of his self-made data box. In today?€?s data rich, insight poor (DRIP) world, the green hat helps the data scientist to think of new innovative techniques, use visualization and infographic tools and make his data and insights show the real business value to busy executives who need something that stands out from the data clutter in their constant effort to hang on to their hats and take decisions that spell ROI and business impact.,There it is. 6 thinking hats in the life of the data scientist. Does it make you want to toss your hat into the ring? Juggling six hats all day long to get an ounce of insight from an ocean of data?,Pablo Picasso once said, ?€?To draw, you must close your eyes and sing.?€? But juggling six hats? That might deserve a resounding hats off, all 6 of them.

The perspective of the word , has changed drastically over the decades and more so in recent years. The practice of collecting data in yore for mere bookkeeping has today become a matter of wise investment to create a gold mine for future. Hence every sector like government, corporations and academia are investing heavily on getting their Enterprise Information Management Architectures in place.,The prospect of tapping into the DATA generated in an enterprise backyard or on the World-Wide-Web, has suddenly spurred the need of unique talent. The name given to that talent ranges from the fancy ones like ,/, to a little humble ones like ,/,/, to that of an old school ones like ,/,. Currently these designations are more or less interchangeably and ambiguously used in the industry for work which fundamentally involves , to essentially evaluate a problem or an opportunity and build a use case around it.,The reason these designations are sometimes fuzzy to their actual work definition is because the gap between the Information Management and Decision Science teams is narrowing. As the data driven business strategies evolve gradually, these roles are carving their distinct responsibilities and eligibility criteria so as not to be confused or overlapped with their actual strengths and expectations. But in that list, the , role stands out as ,. And because of the rareness and rising demand of this interdisciplinary talent, it is labeled as the , by Harvard. Today when everybody is waking up to the call of DATA driven fortune making, corporate houses are desperately seeking this , called Data Scientist, who can visualize and execute an end-to-end data driven strategy to solve complex business problems and tap on growth opportunities.,Now, with almost a decade long tryst with ?€?data?€? driven development, I am just adding my own thoughts to give a broader scope of what Data Science is and what it takes to get started on the path of becoming a Data Scientist. , Each of those intermediate stages requires knowledge of tools techniques and domain which can sometimes be very much diversified. Some of those broader topics which are used in those stages and which are a must for an aspiring Data Scientist are listed below.,The above list is just for the mindset benchmarking and gives a broad overview of what one minimally needs to get started on core Data Science road map. The list can get quite exhaustive with specifics but that is not the intent of this article. There are many Data Scientists and evangelists who have taken a dig at this and few of them whom I personally love are , and ,.,Hope this adds some value to your own thoughts. Feel free to comment as even I am learner and would love to know your views.,Originally posted on my website ,4-September-2014,Author: Sapan Patel, Data Engineer @ Amazon || Sapanpatel.in - ?? 2014, all rights reserved
The winner for our second data science competition is Tom De Smedt, biostatistician completing a Ph.D program at University of Leuven, Belgium. His special interests are in spatial statistics, environmental epidemiology, novel regression techniques and data visualization.,The competition consisted of simulating data and testing the??,, on correlated features or variables. The technique provides an approximation to standard regression, but is far more robust and deemed suitable for automated or black-box data science. The easiest version consists of pretending that variables are uncorrelated, to very quickly obtain robust regression coefficients that are easy to interpret. This is the version that Tom has been working on.,Initial findings suggest, as expected, that the Jackknife approach provides a rough approximation in the context of predictive modeling, although parameter estimates are quite different from standard regression in this test. It is also faster than ,??when the number of variables is very large (> 10,000), though we are still investigating this issue. The standard regression algorithm might be very efficiently implemented in R, while our Jackknife regression does not benefit yet from the same amount of code optimization.,We would like to test the Jackknife regression when applying the clustering step, to group variables into 2, 3 or 4 subsets, to see the improvement in the context of predictive modeling. This is described??,.??This has not tested yet so far.,In particular, we would like to see the improvement when we have a million variables (thus 0.5 trillion correlations) and use sampling techniques to pick up 10 million correlations (using ,) out of these 0.5 trillion, grouping variables ,.,So, instead of using a 1 million by 1 million correlation table (for the similarity matrix), we would use an hash table of size 10 million, where each entry consists of a pair-value $hash{Var A | Var B}=Corr(A,B). This is 50,000 times more compact than using the full matrix, and nicely exploits sparsity in the data. Then we would like to measure the loss of accuracy by using a sample 50,000 times smaller than the (highly-redundant) full correlation matrix.,More on this soon. We plan on publishing the full results and source code in the next two months. The award for this competition was $1,000. Congratualations Tom!,You can check our first competition, and the winner,??,.
 , , 

 , , , , , 
This blog discusses the applications of Monte-Carlo simulation methods by modeling real-world situations, explaining those using well known and often researched statistical distributions such as the Poisson distribution and then applying optimization models to solve a variety of business problems thus enabling managers to take decisions by moving beyond the usual methods and what-if scenario analysis.,Very often business managers are faced with dealing with uncertainty in the real world, while they already know the decisions they need to make, provided such situations were a certainly. Such situations are a plenty ?€? For instance: How many agents do we need in a call center? How much should I stock??a particular product? How many doctors do we need in a hospital? Where should I invest money based on project ROIs? In all these situations, the answer in a ?€?certain?€? world could be simple. If only call volume to a call center was fixed and steady through days, the number of patients visiting a hospital was the same every day and hour, we could forecast demand accurately or we know that the chances of project success was a certainty!,However, in the real world, situations are dynamic, consumer behavior changes rapidly and business managers are fraught with uncertainly. Managers usually use the term and the concept of ?€?What-if?€? analysis to overcome such challenges posed by an uncertain world by drawing a finite set of possibilities and estimating the outcomes of that set, they usually understand the best and the worst case scenarios and thereby make their decisions accounting for such outcomes.,However, there are 2 disadvantages in this approach:,Recent advances in computers and the use of random numbers in what is often referred to as ?€?Monte-Carlo simulation?€? methods, together with ?€?Optimization?€? have made it possible to overcome these limitations and offer??a high degree of accurate and fast outcomes by accounting for situations that couldn?€?t have been possible in other methods.,Monte-Carlo simulation methods are a broad class of computational algorithms that rely on repeated random sampling to mimic real-world occurrences of various possible situations and their outcomes. They have been used widely in physics and engineering where closed form of expressions are difficult to arrive at and in business to predict situations and take decisions based on historical events. The real trick is to model the real world as a ?€?System?€?, to understand the various uncontrollable inputs to this system and by understanding historical patterns of these uncontrollable inputs, simulate the future and thereby the decisions that need to be taken.,A lot of empirical work done and theoretical premise in which the inputs are uncertain helps us benefit from such simulation as well. For instance:,Similarly, in cases where there is no known premise or cannot be trusted, historical data can also be used to find if there is any pattern or any distribution that the data tends to follow. Once this is estimated, a computer can be programmed to generate random numbers that follow such estimated or theorized patterns , the outcomes generated for each situation and such situations replicated multiple times to model the real-world.,Simulation methods are good to model or mimic real world situations, but often fail standalone for their lack of ability to deal with business constraints. As an example, it may not be practically possible to have an infinite number of call center agents or doctors on call during rush hours, we may not be able to stock??very little or very large quantities of a product due to existing contracts or manufacturing limitations or have the risk appetite to put all your money into a single venture. Here is where optimization helps ?€? to deal with constrains imposed by a real world system which can be simulated and this combination can prove to be extremely effective.,Different types of optimization methods exist to solve Linear, Non Linear or Mixed integer problems along with special applications such as transportation problems, travelling salesman problems, assignment & scheduling problems etc. The idea behind all these problems though remain the same ?€? optimizing resources based on real world operating constrains. Tools such as SAS, R, MATLAB, GLPK etc. have enormous algorithms that can be used to solve such problems and with computational power increasing, in shorter and shorter times.,As I have pointed out before both simulation and optimization methods have applications across domains. They have both been used widely in engineering but the applications of these in business are practically limitless. We at BRIDGEi2i have been extremely fortunate to apply some of these techniques across a wide variety of areas. Let me illustrate a few areas where we have used of simulation & optimization (or both),While there are many tools to solve simulation and optimization problems, we have extensively used SAS, R and EXCEL to solve them. The primary reason for choice of these tools apart from our obvious comfort levels are that a) Analysis ready enterprise data usually exists in the form of SAS datasets or Excel spreadsheets b) Wider range of problems that can be solved using such tools making integration easier and c) Familiarity of end users with tools such as excel making deployment easier.,??To calculate the vendor managed flexible inventory for a portfolio of over 1500 products, a fully SAS automated process for the same and a excel based front-end for disseminating the insights and for scenario & cost analysis,??To build a staffing solution for a healthcare clinic for a multi-shift scenario for a single department based on historical patient volumes.,??

The month of August saw a plethora of data science-related news and announcements, including a Wired feature on Kaggle founder Jeremy Howard?€?s quest to improve health care, GE?€?s success in bringing analytics-derived insights to its products for the railroad, airline, hospital, and utility industries, a provocative interview with OKCupid founder Christian Rudder, and much more. Here?€?s our roundup of the major data science news of the month.
 , , , , 
With all of the discussion about Big Data these days, there is frequest reference to the 3 V?€?s that represent the top big data challenges: ,. These 3 V?€?s generally refer to the size of the dataset (Volume), the rate at which data is flowing into (or out of) your systems (Velocity), and the complexity (dimensionality) of the data (Variety).?? Most practitioners agree that big data volume is indeed huge, but that is not necessarily big data?€?s biggest challenge, at least not in terms of data storage capacities, which are growing rapidly also and keeping pace with data volume.?? The velocity of big data is also a very big challenge, though primarily for applications and use cases that specifically demand near-real-time analysis and response to dynamic data streams.?? However, unlike volume and velocity, most will agree that the??, (complexity) of the data is truly big data?€?s biggest mega-challenge at all scales and in most applications.?? Consequently, any dataset (whether large or small) that has hundreds or thousands (or more) dimensions per data item is difficult to explore, mine, and interpret.?? So, when you find a data tool that helps in the analysis of high-dimensional data, you stop and take a look.?? I did that recently with the ,.,First, note that this tool is not explicitly for big data, though it is certainly useful for small subsets of big data: that is, small data!?? The focus is therefore on scientific discovery from small data.?? This is the style of data science that nearly every scientist needs to carry out on a routine basis, since data from daily experiments are rarely in the rarified realm of big data, but modern scientific instruments often do generate large numbers of measured parameters per data object. ??,??enables the discovery, exploration, and visualization of correlations in high-dimensional data from such experiments ?€???, Exploratory Data Analysis (EDA).??,One of the most sensible characteristics of AutoDiscovery is that it does not try to be the ?€?one tool?€? for all possible statistical analyses. There are other statistical software packages that already do that, and there is no need to compete with giants like R, SAS, or SPSS.?? Consequently, AutoDiscovery aims to satisfy a very particular scientific discovery requirement: correlation discovery in the high-dimensional parameter spaces of complex (high-variety) data.???? It is a complement to those other (more comprehensive) statistical packages, not a competitor.,Correlation discovery alone may seem relatively simple and thus a specialized tool for it seems unnecessary.?? However, several proprietary features within ,??can more than justify its use.?? The top 10 features of AutoDiscovery for exploring complex relationships in data for scientific discovery are:?? (1) simplified, visual integration of data from multiple sources (including ?€?primary key?€? discovery across multiple data tables); (2) the streamlined easy-to-use EDA visual environment for data selection, filtering, and exploration; (3) rapid discovery of interesting findings that can confirm (or deny) initial hypotheses, inform further experimentation and experimental design, and generate multiple additional testable hypotheses; (4) automatic search for significant correlations across the full set of pairwise parameter combinations in your dataset; (5) automatic search for significant correlations between virtual parameters (i.e., the ratios of the original input parameters); (6) quantitative assessment and evaluation of the value of each finding; (7) automatic sorting of results, including deprecation of weak and insignificant correlations, placing them lower in the output listings, though still searchable if wanted; (8) optional correlation analyses within multiple sub-segments of each parameter?€?s range of possible values (thereby enabling discovery of changes in the parameter correlations across these limited ranges of the data values, which is a reality often observed in complex scientific experiments); (9) visual tools that present the linked network of the most significant pair-wise correlations among scientific parameters; and (10) correlation analysis outputs (tables, visualizations, and the ability to export the correlation tables) that enable efficient and effective browsing, exploration, and navigation of causal connections (and the causal direction) in correlated data items.,For scientists, the use of EDA for initial exploratory studies is crucial in the early stages of an experiment ?€? both exploratory and confirmatory analyses enable discovery, hypothesis testing, and refinement of scientific hypotheses.?? More detailed analysis would follow from initial discoveries of interesting and significant parameter correlations within complex high-dimensional data. An article was recently published in , on ?€?Statistical Errors ?€? p Values, the Gold Standard of Statistical Validity, Are Not as Reliable as Many Scientists Assume?€? (by Regina Nuzzo, ,). In this article, Columbia University statistician Andrew Gelman states that instead of doing multiple separate small studies, ,?? In other words, a disciplined scientific methodology that includes both exploratory and confirmatory analyses can be documented within an open science framework (, ,) to demonstrate repeatability and reproducibility in scientific experiments. This would break down the walls of ?€?black box?€? software that hide the complex analyses that are being applied to complex data. The ability of the scientist and her/his peers to reproduce an experiment?€?s rationale as well as its results will yield greater transparency in scientific research.?? AutoDiscovery is a tool that can further the Open Science cause.,??objectively discovers interesting findings in the early stages of research. This provides four additional benefits to the scientist in the EDA stage of research: (a) informs improvements in the experimental design; (b) validates and substantiates , hypotheses; (c) generates multiple new testable hypotheses; and (d) reveals promising ?€?hot spots?€? in the data that require deeper statistical analysis. ??The latter capability is quite exciting ?€? , ?€? , finding the unexpected, unusual, ?€?interesting?€? regions and features within your data?€?s multi-dimensional parameter space!?? Especially with complex data, the combined sum of these capabilities empowers the data scientist to tell the ?€?data story?€? in the full dimensionality of the dataset, not just in a few limited 2-D or 3-D projections.?? Consquently, AutoDiscovery is an objective quantifiable feature-discovery tool that presents the most interesting correlations to end-users for efficient and effective EDA: , in the sense that automatic discovery of the most interesting data correlations for deeper analysis avoids lots of useless searches and manual manipulations of the data collection; and , in the sense that novel discoveries (beyond known correlations and expected relationships) are made possible.,The discovery of more complex relationships (, multi-valued or non-monotonic data patterns) in multi-dimensional data requires specialized tools and transformations that are currently beyond the scope of AutoDiscovery (or of any other readily accessible tool), though discovery of these types of patterns may be enabled in future releases of EDA tools.?? An example of a multi-valued data relationship is the S-shaped 2D surface embedded in a 3D space (shown ,) ?€? discovery of such hypersurfaces requires special algorithms (such as ,) that are not available in off-the-shelf EDA packages.?? An example of a non-monotonic data relationship is revealed in the solution to the ?€?island of games?€? puzzle (the ,; and the ,).?? Monotonic relationships typically underlie cause-effect studies in science, and that's why EDA software (such as AutoDiscovery) currently targets the discovery of those types of data relationships.,The , reports a case study in which neuroscientists in the Laboratory of Adult Neurogenesis at Cajal Institute (CSIC, Madrid) used AutoDiscovery to discover correlations between neuron properties and behavior patterns, and the effects of stress and anxiety on learning and memory capacity.?? They describe the results this way: , That is precisely the type of efficiency amplifier that I can use in my research, and I believe that other scientists will experience similar accelerations of their discovery science.,Read more about AutoDiscovery, download a free trial, request a demo, and begin discovering the most interesting features in your ocean of complex data today at ,.?? A new release (ver. 2.0) of AutoDiscovery is now available for all scientists and data explorers to begin exploring the complex relationships within their data for scientific discovery. ??The development team at Butler Scientifics is ready to support users of their ,??tool and to provide licensing terms that can fit any budget (for individuals, or small research teams, or entire research institutions).??,Follow??,??on Twitter at??,????
 , , , , 
 , The ratings for each category seem to follow a uniform distribution. For example, here is a histogram for the Chess ratings:, , , , ,The next three charts confirm this lack of correlation. The first chart compares the ratings for Chess and Checkers, and includes a linear regression:, , ,Here is Chess vs Rubik?€?s Cube:, ,And Checkers vs Rubik?€?s Cube:, ,The raw data and some statistical analysis can be found ,. All of this looks like pure noise. But there is a hidden structure. Can you find it?, , Copyright 2013. All Rights Reserved.
 , , , 

Gamification is a buzzword that has crept into all industry verticals in the recent years ?€? despite criticism. Gamification, in simple terms, can be defined as application of game mechanics and game thinking to real world contexts to encourage user engagement with a product or service. It utilizes psychological inclination of human minds towards games. It boosts user engagement, loyalty and monetization through the application of core principles of games viz. play, transparency, design, challenge, achievement and recognition - into business strategies. The innovative designs and rewards keep the consumers active and coming back for more. Loyalty programs by retailers, for example, have been promising enough in the past which elucidates that the concept is not new. However, advancement of big data analytics and social media marketing in the recent past has made gamification more affordable and easily scalable to suit every industry need.,Gamification uses an empathy based approach to improve both external and internal interactions in the business world.,??,Despite the implementation of gaming techniques in designing the applications, the program can still collapse if there is no learning from the results. Hence, quantification of the results and outcomes combined with the application of analytics, automation and behavior modification can allow gamification to be systematized into organizations at scale. Analytics provide the apparatus and technology to study the changing trends and throws light on the areas of improvement. Furthermore, it provides deeper insights into the detractor?€?s views and thus helps in making the right marketing choice.,Gamification has been gathering more steam over the years. Forbes predicts that 40% of Fortune 1,000 organizations will have gamified business operations by 2015. EngineYard predicts that by 2016, 70% of Fortune 2,000 companies will have atleast one gamified application. It is believed to further grow up to be one among the giants like Facebook and amazon. However, the concept as a whole is still dawning, thus, adopting a few strategies would help to overcome the hype that calls gamification as ?€?just a fad?€?. In this regard, organizations need to recognize the fields to be gamified. They should begin with a clear objective, defining the target group and the behavior desired out of them. The second step would be to design the rewards which will encourage this behavior. Without washing the hands-off at this stage, the results should be regularly monitored right from the time the application goes live - including the discernment of the data to be collected, their measurement and analysis along with the identification of correct KPIs. The process should be constantly iterated using the new data-driven insights. Finally, depending on the success of small initiatives, businesses can spread gamification at large scale enthralling life in every arena. Employing analytics is a value add-on as it enables a ?€?test and learn?€? outlook. Further, businesses have the rare opportunity to evolve continuously, learning with every test, embracing the required refinements and thus reserve the best place in the market, always!,??,Gamification assures the best performance of every individual and thus achieves an elusive engagement of the audience by masking work with play and fun. A quote by Heraclitus, a pre-Socratic Greek philosopher, is worth reminding - ?€?Man is most nearly himself when he achieves the seriousness of a child at play?€?. Despite the hype created by Forbes, Gartner and the like, will gamification break open its cocoon of infancy and emerge to be a great player in the market? It?€?s a question that can be answered only with time.
For explanations about the methodology, including source code and possible improvements,??,. It also provides links to our other three listings.
The challenges in implementing linked data technologies in enterprises are not limited to technical issues only. Projects like these deal also with organisational hurdles to be crossed, for instance the development of employee skills in the area of knowledge modelling and the implementation of a linked data strategy which foresees a cost-effective and sustainable infrastructure of high-quality and linked knowledge graphs. SKOS is able to play a key role in enterprise linked data strategies due to its relative simplicity in parallel with its ability to be mapped and extended by other controlled vocabularies, ontologies, entity extraction services and linked open data.
??, , , , , ,??
The new European laws about "the right to be forgotten", however absurd they might be, is a new government threat for data engines.,First these laws are absurd because, , Google makes money in Europe and cares about complying with EU laws (up to some point) to preserve revenue streams in Europe.??But what about a little guy outside Europe, who will never make money from European web traffic, and who cares about free speech, but not about European regulations?, , This indeed creates an opportunity.It can be a big traffic generator for someone not subject to EU laws. And there's someone already testing the waters: ,. Some data entrepreneurs will probably make money out of this in one way or another (e.g. re-selling censored data, or generating profit via ad revenue), and eventually defeat these laws, without using any lawyers.??
The industrial revolution of the 1800s established the building blocks of Manufacturing as we know it today. Man, Machine, Material and Method were connected together to form an intricate system on which manufacturing processes and its operational dynamics were based. The resulting complexity of such a system however, has resulted in ineptitudes which have become difficult to circumvent.?? The significance of each element of the 4M?€?s and the consequent manner in which they mesh together generates a multifarious effect in daily industrial operations. The effect as such can be amounted to the ?€?Cascade Effect?€? phenomenon, racking through operations and leaving colossal ineffectiveness if left unchecked.,Examples of such inadequacies are:,The organizational scrutiny today rests in better demand forecasting and inventory management measures. However from an industrial viewpoint these solutions appear irrelevant and a greater onus is warranted on the problems at hand. A deeper analysis of process functioning in a factory appears necessary to uncover the underlying sources for the inefficiencies.?? Reinforcing design of appropriate metrics from first principles and then utilizing them to gain insight on system dynamics remains a priority. ??,Addressing the ?€?5, M of Manufacturing?€? or ?€?METRIC?€? design is what manufacturers today are turning toward to optimizing manufacturing process and ROI. The design of appropriate Metrics is an important superimposing factor to the 4M?€?s enabling manufacturers to draw the comparison to KPI standards. Today?€?s technological prowess ensures the systems and software are in place to capture detailed metrics in real-time.,Factory metrics may involve:,The use of metrics enables the establishment of quality benchmarking, and other protocols such as , and predictive analytics, root cause analysis and process optimization whilst identifying inefficient driving factors and their remedial measures. Analytics practices such as predictive modeling and simulation, are being extensively used today to target potential scenarios in which inefficient drivers appear. These analyses employ the right analytical approach consisting of maths, business and technology to isolate the most relevant drivers in a manufacturer?€?s factory operations.,When institutionalized across the factory and manufacturing facilities, data-driven decision making using factory specific data, facilitates several benefits:- operational cost reduction opportunities, inventory level optimization avenues, efficient resource management and overall improvements in the manufacturing process, allowing companies to seize the highest levels of manufacturing perfection.,??


How will Advanced & Predictive Analytics (APA) affect you? Our inaugural Advanced and Predictive Analytics report can help answer that question! APA is growing and developing a plan for your organization now is critical!,The 2014 Wisdom of Crowds?? Advanced and Predictive Analytics market study contains everything you need to assess this dynamic market phenomenon with 70 pages of in-depth market analysis, over 50 charts and tables, 18 vendor rankings AND a buyers' guide comparing and contrasting BI vendor APA capabilities - including core features, data preparation, usability and scalability.,Published by the ,. Purchase @??
These profiles are randomly selected among our active and new members. To be selected, you need to have an profile with short bio. It helps to post a blog, or share / comment contributions from other members.
In this blog, I will explain how an approach to handle small amounts data can be reconstructed to handle much larger amounts. This reconstruction is the product of an anomalous perspective or mutation relating to the attribution of performance.,Fig. 1 - 6-fingered handprint spotted near my truck,Many businesses share certain common features. Assuming that operations are not fully paperless, an office might maintain supplies to handle the physical artefacts of data: staples, paperclips, pens, highlighters, post-its, elastic bands, and binders. Companies generally have comparable computers and operating systems to deal with all sorts of information. There are also policies and practices that share much in common; these help to shape what gets recorded, when, how, and for what purpose. Professionals from human resources and accounting can sometimes be moved between companies without much preamble. This is because professions represent pools of core competencies: their perspectives over facts and figures are actually prescribed by the profession itself. Methodologies to measure performance also tend to be shared: it is possible to find examples in textbooks explaining how to apply industry-recognized methods. There might even be supporting software to carry out evaluations - thus confirming a market of similar users. So the business landscape contains players sharing numerous traits in relation to their data. In a manner of speaking, this represents a kind of organizational "species." So when I raise the issue of mutation, I mean a change that is relevant in relation the entire species: the emergence of something distinctive and structurally persistent. In relation to a biological species, mutation affects genetic details. In an organization, data represents the primary means of conveyance; the data helps an organization decide how to structurally adapt to its environment. Consequently, I would argue that for many companies, the issue of mutation actually involves its data system.,In order to bring the nature of the mutation to light, I must first introduce what is being mutated. I have worked in a number of different types of companies that seem to follow the same basic pattern or methodology to gather data and make it accessible for decision-making purposes. I consider the following to be common steps useful for assessing operational performance: 1) determining the key indicators of performance relevant to the operation; 2) ascertaining the presence of these key indicators in workflows; 3) routinely tabulating the number of events or incidents; 4) retaining an historical record of the periodic totals; 5) maintaining summary statistics for individual agents, project teams, functional groups, and strategic divisions; and 6) presenting the numbers in a manner that people can understand and to meet specific organizational needs. The exact details vary of course. A factory might count skids sent to shipping while a call centre would likely focus on the number of accounts opened and orders processed. I remember once working for a coffee factory. I was exposed to a great many things counted for quality and production statistics: number of canisters packed; amount of coffee per canister; bags of coffee beans opened; barrels of beans emptied; skids loaded with boxes of product; and trucks loaded with skids. There was a lot of counting because there was a great desire for control. The act of counting and the data collected from it represent control over critical processes.,I once received comments indicating the following: the market suffers from a documentation deficit in relation data gathering for quality and productivity. I'm not actually sure this is true, but I certainly need to offer readers a basic framework at least to help explain the mutation. In the illustration to follow (Fig. 2), the preceding six points have been expressed as basic questions that a person might ask in order to facilitate data-collection. The business of gathering information and reporting the individual production contributions of workers and factory departments can be facilitated using old technology: just sheets of paper and pencils would suffice in a pinch. Given good bookkeeping habits and adequate time to complete daily tasks, businesses can probably get along fine using archaic methods to assess quality and performance. This is assuming that the desired outcomes serve a purely prescriptive purpose - to confirm that particular tasks are carried out in specific ways within certain time periods. In other words, if the purpose of data is simply to ensure conformance behaviours, little sophistication is required in the data system. Conventional approaches serve prescriptive requirements well. I consider it a major accomplishment in record-keeping to be able to support management efforts to assess quality and performance. At the same time, I am certain that almost every company that has ever gone bankrupt did so under some kind of management; however, this isn't the focus of the blog.,Fig. 2 - Main Procedural Points,The so-called "clipboard" in point #1 represents the formal criteria or standards against which events within the workplace are to be assessed: for example, since it would be illogical to apply investment criteria to automotive quality, clearly the choice of clipboards is rather critical. The clipboard might be in the form of a checklist. The criteria used to generate compliance-type data often leverages on the expertise of those doing the checking: a check-box such as "gas pressure regulators working properly" might require some training to properly interpret. In terms of points #2 and #3, there is a difference between assessing the amount of work that has been completed and gathering those specific aspects that are relevant to the criteria. Over the course of producing 50 pallets or skids of merchandise, it is necessary to determine what aspects of production are being managed and must therefore be set to data. So I am saying that there is a clear delineation between the data gathered for central control purposes and that used for decentralized monitoring of routine events. Points #4 and #5 both relate to record-keeping, the idea being to maintain data in accessible formats that can later be used to generate desirable reports; this requires an interpretation not just of the original criteria but also the intent. The only part of the process that might receive any significant recognition occurs at point #6 involving data presentation; as such, it stands to legitimize or delegitimize everything else - perhaps unfairly so.,Ideally by going through the above steps, an organization can ascertain the contribution levels for each worker (in relation to behaviours considered relevant by the central authority to production). This is the idea, anyways. It isn't necessary to characterize production in relation to workers. For instance, factory operations can be broken up into a number of functional parts or specializations. Rather than collect data attributable to specific employees, it is possible by applying the same methodology to compile statistics for the organizational parts. In other words, the parts can be treated as employees. (Conversely, it seems to have become fashionable to treat employees as factory parts. It is difficult to say whether this perspective is any more or less correct.) Statistics can be maintained for each important aspect of production to support comparisons over different time periods. If small changes are implemented in one section, then the results can be examined against similar operations in other sections.,The listing of points that I just provided doesn't have to involve a process that already exists. The methodology can be used to gather data that has never existed before in order to support decisions that are completely new to an organization. Within the framework of my graduate studies, I examined one pilot project intended to determine the feasibility of centralizing employee counselling services. The purpose of the pilot was to collect data since so little of it existed at the time. A number of facilities were chosen to participate in the experiment. Organizations that have never had a coherent system of data collection, tabulation, and presentation would likely experience some difficulty coping with more sophisticated approaches. I suspect that the leap to big data would be quite challenging indeed without exposure to small data. Nor would it be necessary. As I mentioned earlier, computers aren't required to support the traditional method. However, as I will discuss in my next blog a few weeks from now (covering the Push Principle), just a slight twist in how processing occurs from the mutation can radically alter the meaning of the data. For now I just what to provide I guess some reassurance that intellectual capital conforming to the points I described earlier can "evolve" into something quite unique and promising. The question of whether or not the community has a place for it remains to be determined.,I use the term "TIME" to describe a special data-rich environment containing organizational contexts and data events; there are also algorithms designed to characterize the level of relevance between these contexts and events. "TIME methodology" refers to the approach I use to create the environment. I will be providing some structural details after describing what brought on the methodology. The mutation giving rise to the methodology and therefore a higher level of sophistication to access data involves a slight aberration in the interpretation of performance. Under the old methodology as I noted earlier, it is possible to obtain the contribution levels for individual employees in a department or organization; this is presented below on the left-hand side of the illustration (Fig. 3). The use of employee metrics to examine performance constitutes only a single perspective or context. While managers might show great interest in this particular context, this should not prevent data scientists from exploring other types of organizational contexts. Rather than associate events with people, it is possible to associate events with different contexts including those that form performance rankings as shown on the right-hand side of the same illustration. Instead of maintaining a single theme such as employee performance, an organization can systematically deal with many hundreds or perhaps even thousands of contexts. The objective of the TIME methodology is to create and sustain a data environment that preserves these important associations.,Fig. 3 - Structurally Identical Performance Gradients,Central to the aberration is the deconstruction of assignment operations. Applying a rather conventional interpretation of assignment, one might say that Paul is responsible for 2 + 1 + 3 +2 + 2 = 10 units of production where the level of production 10 is simplistically assigned to Paul. A rather faulty perspective that is probably pervasive is to say that Paul equals or can be equated to 10 units of product as if Paul's reason for existence is to produce units. Assignment in relation to the TIME methodology is more like immersion. For example, all of the data events that occurred in the City of Toronto during an ice-storm could be assigned to Terrible. I realize that it might not be obvious how a person goes about performing such a curious assignment, but the barrier is more due to lack of familiarity than technology. First of all, let us accept that an ice storm is indeed terrible, just to get over the initial hurdle. Further, I would say that a sunny cheerful day in May is rather Terrific. Further yet, I underline how the types of events found in the data of an ice storm differ from the data of a sunny cheerful day. Finally, performance metrics need not be limited to units of product. (What does the 3 next to "Terrible" mean? That means there were 3 applicable days. Sorry for leaving that out.) So let's examine the slight-of-hand closely. We would normally have said that Paul is responsible for 10 units. I'm saying that the direction can be the exact opposite: furniture, pants, wrist watch, wallet, spouse, children, education can all be assigned to Paul. An unlimited amount of data can be assigned from the right to the left; and doing so makes sense per my ice storm, sunny day analogy.,Technology speaking, "big data assignment" is nothing difficult. If a handful of data events can be assigned to Paul, then an astoundingly massive number of data events can be assigned to Terrible. Just do it - obviously not using conventional software. It might not be possible to handle the data as a primitive quantity given that it is a complex object having no fixed structure or size limit. Consider setting aside such apparent obstacles to focus on the beauty and simplicity of massive assignment. Traditional assignment served to limit the meaning of data and to set boundaries. Massive assignment helps to expand the meaning and broaden our horizons - but only if we let the technology take care of the details. Big data requires computers. The more power - the better. How much data should be used? Use as much data as necessary and more if desired. Assignment or immersion doesn't mean equality in an algebraic sense. Assignment is only part of the process. It is still necessary to determine the transpositional relationship between the data and the contexts; this is done through the use of algorithms as noted earlier. I refer to these algorithms as "relevancies." A relevancy that I frequently mention in my blogs is called the Crosswave Differential. (I have other types planned.),I believe that much of how we handle data relates to its simple origins. The development of society has become increasingly dependent on the development of methods to handle its data. Expressed differently, the level of complexity that society can attain cannot go beyond the level of complexity possible in its data. This conceptualization of society suggests that social problems might depend on how a society chooses to handle its information. So if we got rid of computers, society as we know it would likely have to revert to a simpler form. Moreover, if we eliminate books and stop teaching basic writing skills, society would simplify even further. If we lose the gift of language completely, civilization as we know it would probably collapse. I suppose that the data-complexity argument is debatable in relation to society as a whole; but to me its relevance in organizational settings is difficult to dispute. I want to emphasize that in the TIME methodology, current methods are not being replaced. Everything that currently exists can continue to do so; but another aspect is added in parallel. The data environment resulting from the TIME methodology extends from current resources in several ways. I will spend a few moments explaining the three major components of the methodology as indicated in the illustration below (Fig. 4): 1) the data itself; 2) its intended organizational context; and 3) an algorithmic assertion of the relationship between the data and the context. Really, only the last item is new. The first two items already exist although perhaps not articulated as part of a formal methodology.,Fig. 4 - Major Components of TIME,In relation to big data, I believe there is a bit of a psychological preoccupation questioning whether or not an organization should collect so much information - as if big data were an issue of self-control, choice, or restraint. If an online retailer simply decided one day to stop collecting large amounts of data, it wouldn't be able to operate. Much of the data gathered is needed to support operations: e.g. the identity of the person placing an order; the merchandise included in the order; the person responsible for filling the order and by what time; the destination of the order; and how the order should get there. I describe this type of data as the Data of Direction. The business model of an organization "directs" its operations either to collect or make use of the data, and there is really little choice in the matter. However, there has been some conflation between this type of data and another type that I call the Data of Articulation. I believe that a fairly common type of articulation shared by many organizations is present in marketing data. Articulation exists in relation to consumer feedback and after-sales support. A rather big problem involving big data might not be the amount of information but rather its increasingly eclectic nature as more data is collected. Standardization of diverse facts can render data more cohesive but less meaningful. TIME can accommodate any type of data, keeping in mind that all of the data is destined to be contextualized,The preceding two types of data can sometimes be confused with another type that I call the Data of Projection. Management criteria results in projection data. This is quite different from direction and articulation data. In certain respects, direction data emerges as a consequence of managerial efforts to bring about the business of the organization. Articulation data generally exists irrespective of what management might say or do although its recognition probably depends on managers. Projection data on the other hand is specifically the result of the application of metrics by management as a part of a control or regulation process. This body of data is conceptually smaller than articulation data, which can be of infinite size. One would also hope that it is smaller than direction data. While projection doesn't create a great deal of data in relative terms, the metrics of criteria is an important concept; unlike the other forms of data, it supports deliberate intervention and change management. The outcomes of projection can be examined using existing tools such as statistics. I believe that this creates a much needed bridge between old and new methods of analysis while also helping to extend the role of management into big data.,A relevancy is an algorithm that provides portrayals of relevance. Managers can therefore become aware how particular data might be relevant to important organizational contexts; this provides guidance in relation to intervention efforts. Perhaps more importantly as it relates to big data, a relevancy can systematically sift through all of the data available and provide managers with a listing of the data that seems most relevant . . . albeit without rational explanation. While poor design remains possible when using relevancies, at least the designers gain the means to deliberately minimize the presence of subjective inclinations. Relevancies provide logic to accompany the data that can be reproduced decades later, applied to other environments, and openly shared and discussed in teams, meetings, and academic papers. So although mistakes might still be made, the setting can be preserved, critically examined, and used to improve future decision-making.,Recall how Paul was changed to Terrible earlier in the blog using the same structure of performance. This reinterpretation of the gradient demonstrates how structure might change on the outside in relation to or as a consequence of a mutation: I explained the three major components - data, context, and relevancy. I also said that a fundamental aspect of the change occurs at a deeper level in relation to the issue of assignment or immersion. I would say that in most organizational settings, only a limited number of data events would ever be associated with Paul. The formal role of any employee in an organization - the aspect that necessitates compensation through payroll - is fairy constrained and theoretically possible to summarize in a job posting. Since there might only be a few dozen or perhaps hundreds of reoccurring events, there is little need for contextual multiplicity in performance assessments. It might make little sense to go through an elaborate algorithmic process to attach relevancy to the data since the relevance is predefined as part of the job. In other words, the simplicity of the relationship makes it possible to apply simple data methods. However, if we hope to handle much more diverse data, it is probably necessary to make drastic changes.,In this blog, I have described the structural implications of mutation allowing for the strategic assignment of highly complex data to the contextual multiplicities of organizations; this brings organizational development within the realm of information systems. I suggest that more traditional methods were used during simpler times. The times were simple because the world was full of resources and ripe with opportunities. All sorts of approaches sufficed not so much because they worked well but because the companies tended to operate regardless of the approach. Times were good. Today, simple companies can be replaced by foreign imports. I'm a bit nervous to use the term "low-brain economies" since it might become popular and later attributed to me, possibly triggering comparisons to "high-brain economies." Well, competition has become fierce, and it is important to consider the evolutionary nature of data systems in the equation. I have detailed a way to build on existing knowledge while moving forward. This level of discourse will divide people. It will surely delineate between the casualties and survivors of technological change. The division are caused by fear and ambivalence rather than barriers posed by the technology itself. In terms of the technology, the time for change is upon us, and the means to do so is at hand.
In the drive towards the semantic web, mailing lists are ripe, low hanging fruit. They are full of wisdom totally inaccessible to the casual user. To unlock this wealth of knowledge for our apps, we need it in a format like the , , This data dump format is to Stack Exchange what JSON is to JavaScript: an exchange format that spawns growth in ecosystems separate from the context it was born from. Virtually every language now supports JSON, and in the future, every web service will be able to consume these data dumps - just like our Wikipedia widgets today.,?????????????????????????????? JSON : JavaScript, stackexchange.com.7z : Stack Exchange, The Stack Exchange data dump format isn't exactly appropriate for a mailing list, but it's a great starting point - both for mailing lists 3.0 and your data science career. Bringing the gems of wisdom from the old archives to this machine-readable format is called data curation. Basically, this means , It's an exercise in ETL with data cleaning. In buzzword speak:,Mailing Lists 3.0 = Web 2.5 APIs + data_dump.7z + crowdsourcing,Data curation isn't glamorous and hardly anybody will notice or care about the hard work you've done. It's tedious work to clean, format and verify data against a published schema. It's also ,. The longer you babysit an archive, the better it looks in your portfolio. These formats are a work in progress, so you will have to actively maintain an archive even if the mailing list is dead. With thousands of mailing lists in the public domain, there's a LOT of data to be curated., , Given that the new digital "generation C" are so called for their habits of Creation, Curation, Connection and Community, your open data curation project is merely keeping up with the times to extend your shelf-life and delay the day the kids replace you. , , Instructions for Extract Clean Transform Load (ECTL):, , STEP 0: Choose your target, Look for a small, quiet mailing list used by specialists in a profession that compliments your CV. Steer clear of for-profit services like craigslist and google that may feel threatened by data aggregators. , , STEP 1: Extract, Write a spider to crawl the archive and get the title, message, timestamp, and sender of each post. Read my??, for resources to learn how., , STEP 2: Store, Use SQL, Mongo, or whatever you are most comfortable with. I chose Elasticsearch for text search so i could do some basic word counts and figure out what tags to use. A Bitnami image had me up and running in no time., , STEP 3: Clean, Tag and categorize every message in the archive. This tedious stage is why you should target a very small list. Scripts can automate some of the work, but each entry needs to verified manually. , , STEP 4: Transform, Write a script to query your database and save it in the proper format. Download a sample archive from archive.org and study the format. The closest thing to an official specification of the schema is here:, , , , To be worth your salt, you need to be able to figure this one out for yourself. ,. Someone asked for an XSD in the comments and was told "That's what separates the men from the boys. If one can't work out what the data types are in the dump with the below description, then maybe one is not worthy and should stick to the happy world of copy-paste JavaScript." , , STEP 5: Load (Verify), Import the data dump into a fresh Askbot, Stack Exchange Data Explorer, or Bitnami OSQA. Surf the Q&A site a bit. Anonymize the names of the users for the sake of privacy., , STEP 6: Publish, Save the data dump on your cloud storage account. Send a message to the mailing list with a link to the archive and your intended maintenance schedule. Offer the user identity keys to the admin if s/he wants it. Community members may want to connect author attribution to social media accounts. Push your scripts to github., , STEP 7: Add "Open Data Curator" to your CV and be patient., "Data curation" is still a new concept to a lot of people. Enjoy the short-lived feeling of being special before recruiters expect it from all of their candidates., , Next months' post is on visualization. Coincidentally,??, has just ended its tease to open to the public and disrupt our UX mind space.

TL;DR: A Data Scientist is a data pipeline plumber. Analytics are icing, not cake.,This article is written specifically for unemployed and underemployed graduates of math intensive subjects like physics and statistics. Others may have more to prove.,After writing my introductory reviews of??, and ,, I was going to write something about algorithms and analysis. Then it dawned on me: beyond proving that I'm not completely brain dead, my math skills NEVER helped me get a job. Also, I wrote a quick summary of bread-and-butter algorithms in a ,.,Don't Drink The Kool-Aid,Predictive analytics is supposed to be the heart of a Data Scientist. It's a lie. Numbers are a figment of our imagination and math is a powerless spectre. , Computer Scientists are good at math too, so unless you have domain specific expertise, your so-called 'analytics' contribution to the team are questionable at best. Computer time is money, which means if your MATLAB code and R scripts may need to be re-written in a language that can be optimized to run 10-1000x faster ,.,Diminishing Returns in a Juiceless Lemon,In building the analytics section of your portfolio, the bare minimum is enough. You only need to show basic competency with the most popular models. Placing dead last in a Kaggle competition shows awareness, initiative, and basic competency. Trying out different types of algorithms yields diminishing returns. ,. If you think that you can beat Ivy League researchers at their own game, by all means, set your eyes on the prize. Otherwise, be happy to see your name at the bottom of the Kaggle standings and spend your time learning ETL instead.,Script Kiddies > Actuarial Scientists,The Data Scientist who covets math skills as a jewel puts themselves in a precarious position: , As github's code grows to cover more situations, employment opportunities shrink. Years from now, script aggregating tools will be so sophisticated that actuaries will find themselves competing against kids that scorn certification and degrees. Excel jockeys will be put out to pasture if they are JavaScript illiterate. Don't get caught on the wrong side of the fence when Data Science dies. As Miko Matsumura says, ",",Context to The Rescue,As I've said before, one of the beautiful things about working with data is that it provides concrete context. An employee that intimately understands the context of the company's data is indispensable. They are able to take one glimpse at a report and say "something's wrong here", cutting through hours/weeks of an analyst's work. Statistical models are built on pyramids of assumptions, and assumptions are famously brittle. ,.,The Map Is Not The Territory; There's No Such Thing As Raw Data.,The "concrete context" is of the domain the data sits in. "Raw data" has the closest connection to real devices taking real measurements, but they aren't really raw - they're numbers, not things. Numbers aren't real. ,Asset or Liability?,Those that are unable to compensate for each assumption as it shifts or breaks down quickly become a liability. The annals of Wall Street are full of stories like Merton and Scholes' LTCM. The data they were analyzing was information about information, reports about reports. They didn't realize their models and abstractions pushed them too far out of context to make sound decisions. High on ego, they put total faith in their formula and doubled down on debt when they should have hedged with something more stable. A novice mortgage broker could have seen their insanity., - Merton and Scholes invented the Nobel-prize winning formula their company was exploiting. , Financial analysts are unable to understand the assumptions that are baked into the many formulas used to build Wall Street's house of cards.,Plumbers, Not Tinkerers,. Most of our intricate problems of analysis have been solved by a myriad of open source software and commodity hardware. Certainly within the 80/20 margins. As niche markets for specialists shrink, a wise tinkerer will round out his skill set and become a pipeline plumber to stay relevant.,Three Pronged Portfolio,To summarize, there are 3 major components of a comprehensive Data Science portfolio. Here is an example that should be received with serious consideration by any Big Data company.,Transform: GitHub scripts for open data curation.,.,Visualize: Examples of the canonical plots from the ,. Matlplotlib is just fine.,Model: Kaggle competition. Bottom of the standings.
??,As CFO, Controller, Auditor, or other finance professional busy with the financial controls of your company you have probably heard about Big Data and Predictive Analytics.?? In preparing for this informational series we thought it would be valuable to simply inventory and briefly describe all of the possible applications.?? So we searched the literature and talked to our informed sources to deliver this comprehensive list of 20 different areas in which Big Data can or should impact your professional life.?? In future articles in this series we?€?ll deep dive this list to give you even more insight.,What we found is that the list can be neatly separated into three components.?? 1.) The risk and business control components that define the traditional role of the finance and audit professional.?? 2.) Enhancements to internal and external audit.?? These are so significant they deserve their own category.?? 3.) Applications of Big Data and predictive analytics that promise to support finance professionals as the new pro-active stewards of better corporate performance., Predictive analytics makes forecasting, rolling forecasts, and data-driven budgeting easier and more accurate with forecasts down to the business unit or product and service level for greater granularity and ease of roll-up.?? Big data adds new data sources and increased detail and makes the forecasts fast to update.?? Improvements to predictive analytics make them more accurate., Advanced data visualization techniques make financial dashboards and score-carding easy to understand for all levels of users.?? Big data adds depth to the detail, speed to the updates, and the opportunity to incorporate new meaningful data types both internal to the company and from outside sources.?? Dashboards can track both leading and lagging indicators to give a more accurate view of the future, not just the past., ??Big data technologies can allow incorporation of both new and traditional data types at very high levels of detail and allow the data to be retrieved and processed in a fraction of the time previously required in traditional RDBMS systems.?? Statutory reporting now frequently incorporates the requirement for data from outside the company that needs to be integrated with internal data.?? New Big Data technologies can achieve this integration cost effectively.?? Predictive analytics and visualizations make all this new data easier to understand for quick action.,?? The amount of risk, time, and resource drain from these activities can be substantially reduced through application of both Big Data technologies and predictive analytics.?? As just one example, integrating new data sources from mergers and acquisitions, new business areas, or data from suppliers and partners now required for compliance can be much more easily and cost effectively integrated using new Big Data capture, store, and retrieval technologies.,?? Whether credit scoring business customers, evaluating the risks from complex financial transactions, or utilizing the same predictive tools used by large insurance companies to cost-out risk, predictive analytics offers the power and Big Data technologies offers the granularity and speed to make these decisions more detailed and more accurate.,?? Using the greater variety and detail of data available through Big Data to power predictive analytics to highlight potential fraud increases the likelihood of detection.?? It also decreases the incidence of ?€?false positives?€? resulting in better, faster detection and protection, and less cost in human resource.,?? Responding to disclosure requests or preparing for litigation has traditionally required massive amounts of man power to laboriously comb through mountains of unstructured text, email, instant messages, and other forms of non-automated communications materials.?? That cost can be dramatically reduced and more relevant sources identified through the automated text analytics available now for the first time through Big Data and analytic technologies.,?? Detection of duplicate invoices and duplicate payments is one area where many CFOs start with Big Data.?? It is now practical to use automated analytic sorting to evaluate years of detailed invoice and payments to identify and recover duplicates thanks to low cost, high speed Hadoop-based Big Data storage and retrieval systems.,?? Using predictive analytics, CFOs can score potential collection claims to determine which are most likely to be recovered with least effort.?? In the past, companies with large volumes of smaller outstanding collections weren?€?t able to justify the labor cost of pursuing these.?? Now, predictive scoring allows us to identify those most likely to pay and to easily identify the cost-effective cutoff point at which additional labor costs would be greater than returns from the effort.,?? In a previous article we spoke of the end of sampling.?? This can quite literally be true.?? The new Big Data technologies essentially remove the requirement for sampling by handling massive amounts of data and many different varieties of data at a cost, and in a retrieval and analysis time frame that was previously impossible.?? On the operational side, it is this ability to look literally at everything that now makes it possible for technology to enforce compliance in real time.,?? In the past, integrated and free standing IT systems needed to be evaluated individually and together by technology experts to ensure consistency and accuracy.?? New Big Data tools and analytics have largely automated these tests allowing continuous monitoring without the need for an increase in IT audit human resources.?? These new analytic tools can deliver results up to 90% faster than traditional tools.?? Particularly when paired with visualization, items needing scrutiny can be quickly identified., ??Even the most complex financial transactions can be modeled and examined with the new analytic tools shortening audit cycles, lowering cost, and allowing deeper audits.,?? Data reconciliation across disparate systems has always been an enormously time consuming and important task of audit.?? Big Data analytics tests and compares transactions across multiple systems and simultaneously tests for compliance to policy and procedure.,?? Analytics can compare the accuracy of reports prepared by separate systems to test and ensure consistency and accuracy., ??Based on the analytics ability to model and replicate complex transactions and postings, testing, reproducing, and confirming the accuracy of transactions under multiple scenarios is easy and quick.,?? Companies increasingly compete based on process excellence and customer intimacy.?? More than ever it falls to the CFO to help the company navigate the rapidly changing external environment by constantly evaluating and recommending the right strategies and executing those with excellence and a focus on the customer.?? Big Data technologies and analytics empower CFOs to constantly monitor and recalibrate predictions based on the enhanced ability to understand and analyze internal processes, competitive pressures, and customer response.?? Quickly finding the answers to specific problems and modifying forward forecasts in near real time allows financial professionals to play an influential role in steering their companies.,?? The same Big Data and analytic capabilities that evaluate efficiency also offer up suggestions for growth opportunities in adjacent markets or complimentary products and services.?? Big Data enables deeper and broader insights through analytics that can be the difference between successful and unsuccessful acquisition and business development strategies.,?? Traditional valuation techniques based on discounted free cash flows can be greatly enhanced with a clear understanding of customer lifetime value.?? Big Data and analytics offers a clear path to better understanding customer behavior, buying patterns, and wallet share which can be aggregated up to Customer Equity measures., ??The flip side of customer equity is customer risk.?? Big data and analytics offers a path to better understanding of the big three questions: why they come, why they stay, and why they go.?? Achieving and monitoring deeper insights into customer behaviors, loyalties, and attitude toward the company?€?s products and services protects the company?€?s financial interests.?? Big data and analytics allows this with new depth and accuracy and CFOs can then correlate these measures with cash flows, profits, and other financial measures to guide strategic decisions.,?? A close parallel to customer risk is reputational risk.?? Reputational risk can arise from customers, non-customers and even employees.?? It is transmitted with the speed of the internet across the many channels of social media that can make one bad incident into a viral catastrophe.?? CFOs and audit professionals have both recognized the need to constantly monitor and respond to threats from social media which may be customer comments about products, service, price, or quality, but may also be inappropriate leaks of sensitive or incorrect private financial or transactional plans and data.?? Big data and analytics now offers a way for financial professionals to develop and maintain a listening post to spot and defend against these risks which have the potential to materially impact financial results.,??,Bill Vorhies, President & COO ?€? Data-Magnum - ?? 2013, all rights reserved.
??is the largest and only vendor-neutral analytics conference for government, highlighting case studies of how government agencies are using big data and predictive analytics to solve real world problems. Connect with top predictive analytics experts. Explore high-level analytics for Big Data. September 15-18 in Washington, DC at the Grand Hyatt Washington Hotel.
I will update this article regularly. An old version can be found ,??and has many interesting links.??All the material presented here is not in the old version. This article is divided into 11 sections.,A laptop is the ideal device. I've been using Windows laptops for years, and I always installed a Linux layer (acting as an operating system on top of Windows), known as Cygwin. This way, you get the benefits of having Windows (Excel, Word, compatibility with clients and employers, many apps such as FileZilla) together with the flexibility and pleasure of working with Linux. Note that Linux is a particular version of UNIX. So the first recommended step (to start your data science journey) is to get a modern Windows laptop (under $1,000) and ,.,Even if you work heavily on the cloud (AWS, or in my case, access to a few remote servers mostly to store data, receive data from clients and backups), your laptop is you core device to connect to all external services (via the Internet). Don't forget to do regular backups of important files, using serives such as DropBox.,Once you installed Cygwin, you can type commands or execute programs in the Cygwin console. Here's how the console looks like on my laptop:,You can open multuple Cygwin windows on your screen(s).,To connect to an external server for file transfers, I use the Windows ,??freeware rather than the command-line ftp offered by Cygwin. If you need full privileges on the remote machine, use ,??instead (for Telnet/SSH sessions).,You can run commands in the background using the & operator. For instance,,will launch Notepad (the standard Windows text editor) from the Cygwin console, into another window, and open the file VR3.txt located in your local directory (if this file exists in that directory). Note the ,??symbol preceding any command (see Figure 1). In addition, the console also displays the username (Vincent@Inspiron-Switch in my case) as well as the directory I'm in (/cygdrive/c/vincentg/ in Linux, corresponding to the c://vincentg/ pathname under windows). Basic operations:,So far you've learned the following Linux concepts: , and??, $ (sometimes replaced by > depending on the Linux version), , & (for background processing), ,, , ??cd pwd, and ls, , (-l for ls) and , (. and .. for the cd command).,Files have an extension that indicates what kind of file it is (text, image,spreadsheet) and what sofware can open and process them. In Figure 1, VR3.txt has the .txt extension, meaning it's a text file - the most widespread type of data file. There are two types of files: binary (used by various programs; compressed/encrypted format) and text (can be processed easily by any program or editor). It is important to know the distinction when doing FTP file transfers (FTP clients allow you to specify the type of file, though it's automated and transparent to the user with FileZilla).,Other extensions include,Files are not stored exactly the same way in Windows and UNIX. Also, some systems use UNICODE for file encoding, which takes much more space but allow you e.g. to work with Chinese characters (stored using two bytes per character). When processing such a file (they are rather rare fortunately), you'll first need to clean it and standardize it to traditional ASCII (one byte = one character).??,Finally, the best text format that you can use is tab separated: each column or field is separated by a TAB, an invisible char represented by , in some programming languages. The reason is that some fields contain commas, and thus using csv (comma-separated text files) results in broken fields and data that looks like garbage, and is hard to process (requiring a laborious cleaning step first, or talking to your client to receive tab-separated format instead).,When processing data, the first step is to produce a ,. It is easily done using a scripting language (see section 4).,Filenames should be designed carefully (no space or special char in a filename), especially when you have thousands or millions of files across thousands of directories and sub-directories, and across dozens of servers (the cloud). One of the two core components of Hadoop is actually its file management system, known as HDFS (the other component being the distributed , architecture to process tasks).,It's always a good idea to always have a time stamp embedded into the file name, representing the creation date. Note that in Figure 1, the files all start with VR, an abbreviation for Vertical Response, as these files are coming or related to our email service provider, called Vertical Response. File names should be very detailed: keep in mind that sooner rather than later, you might run scripts to process millions of them. Without proper naming conventions, this task will be impossible.??,A final word, if you look at Figure 1, the first column indicates who can read (r), re-write (w) or execute (x) these files, besides me. It's never been an issue on Windows for me, but on a true UNIX operating system (not Cygwin), you might want to set the right protections: for example Perl scripts (despite being text) must be set to Executable, with the UNIX command ,, where filename.pl is your Perl script. File protections (and locks) are important for organisations where files can be shared by many users, sometimes simultaneously.,You don't need to spend hours learning UNIX and buy 800-page books on the subject. The following commands will get you started, once you have your Cygwin console:,Operators include , (to save output to a new file), , to append output to an existing file, , (the pipe operator, see examples), , (see section 2, used for background or batch mode when executing a command), , (see examples) and , (see example)??,??for these commands (exact syntax and options).,??(or batch files) are small programs that execute a list of commands, and can be run in batch mode. For ,, see section 4.,You can get started in data science wth just a few Unix commands, a tool for statistical analyses such as R (unless you write your own algorithms to get more robust and simple tools) and a scripting programming language such as Perl or Python. Python (together with Pandas libraries) is the most popular language for data science. Python and machine learning resources are provided later in this article. ,??is a good introduction on Python for data science. ,??has tons of resources about Python for data science.,Here I describe fundamental features of Perl, but they apply to all scripting languages. You can download Perl from ,. Numerous short programs (Perl, but also R), easy to read and understand, can be found ,. Perl scripts are text files with a .pl extension (say myprogram.pl) that you can execute in the Cygwin console with the command line ,In short, it's a great language for learning data science, though not so great if you work in a big team and have to share, integrate and update various pieces of Perl code from various coders. Nevertheless, Perl is very powerful, can be blended with other languages (all languages can nowadays), and I still perform all my consulting with Perl. Perl used to be the only language with great string processing functions, and able to handle regular expressions easily - an advantage over other languages, for text processing or text mining. However, other languages have caught up, and Python is now just as good.,Perl is an interpreted langage, which means that you don't need to compile Perl programs. This can potentially slow down executation a little bit, but in my experience, most of what I developed in Perl runs 10 to 100 times faster (without loss of accuracy) than what I've seen in the corporate world, mostly thanks to developing better algorithms and using fewer (but better, more predictive) metrics, and fewer observations (samples). These algorithms are listed at the bottom of this article - an example (in the context of feature selection) is ,, using smaller samples thanks to better use of data science.,Some basic stuff that is used in pretty much any programs include,The easiest way to learn how to code is to look at simple, well written sample programs of increasing complexity, and become an expert in Google search to find solutions to coding questions - many answers can be found on ,. I have learned R, SQL C, C++ and Perl that way, without attending any training. If you need training, read the section on training in chapter 2 ,??or check out this ,. The following are good examples of code to get you started.,Here are some sample code. The more stars, the more difficult.,Below is a simple script that performs automated dns lookups to extract domain names associated with IP addresses. The input file is a list of IP addresses (ips.txt) and the output file is a text file outip.txt with two fields, tab-separated: IP address and domain name. A tempory file titi.txt is created each time we call the external Cygwin command 'nslookup'. Note that $ is used for variables. There's some basic string processing here, for instance: ??, , , , , , , , , , , , , , , , , , , , , , ,Now, you can download big logfiles for free (see section 10), extract IP addresses and traffic statistics per IP address, and run the above script (using a distributed architecture, with 20 copies of your script running on your laptop) to extract domain names attached to IP addresses. Then you can write a program to map each IP address to an IP category using the technique described in my article??,. And finally, sell or license the final data to clients.,Three concepts:,Write a Perl script that accesses all the text files on your laptop using two steps:,Then count the number of occurrences for each word (broken down per file creation year) across these files, using a hash table. Purpose: identify keyword trends in your data.,R is a very popular language to perform textbook statistical analyses or nice graphics. I would not use it for black-box applications. These black-box routines must rely on robust techniques such as ,??or ,. Large applications such as ,??involving 20 million keywords, are performed in Python or Perl, known as scripting languages. ,??for data analysis and machine learning are widely available and discussed in a few O'Reilly books: they offer an alternative to R, for big data processing. Note that R does have an extensive collection of sophisticated ,, too many in my opinion. Finally, R is currently used for exploratory data analysis rather than production-mode development. I am not sure how R compares with Python, in terms of speed. For more info, read ,??or ,.,You can download the open-source R package from ,. Installation and running R programs via the GUI, on a Windows laptop, is straightforward. Memory limitations can be bypassed using multiple copies of R on multiples machines, some R packages, or using ,??(R + Hadoop). R programs are text files with a .r extension.??,Also,??,.,We have ,??about Python for data science, the preferred full-fledged programming language for data scientists.. ,??for a starting point.,Also check??,, many are about R or Python, including analytic libraries such as Pandas (Python).??,Hadoop is a file management system used to perform tasks in a distributed environment, across multiple servers if necessary, by spitting files into sub-files, performing the analysis on each sub-file separately, and summarizing the results (by collecting the various outputs associated with each file, and putting it together). This environment uses redundancy to easily and transparently recover from server crashes. It works well for web crawling projects, even for sorting, but not so much for graph databases or real time data (except as a back-end platform).,The following articles are starting point to understand Hadoop:,Finally, don't forget that SQL is still a widely used language that we should all know: at least the basics, up to joining multiple tables efficiently, and playing with indexes and keys.,This section is under construction. It will focus on some advanced Excel functions such as Linest (linear regression), Vlookup, quantiles, ranks, random numbers and some data science applications that can easily be performed with Excel, for instance, the following analyses (offered with nice Excel spreadsheet)??,Also, a list of articles about data science with Excel ,.,Many visualizations can be performed with R (see section on R in this article), Excel, or Python or Perl libraries. Specific types of charts (graphs or nicely representing decision trees) requires special software. The most popular software is Tableau. Birt (by Accenture) is popular for dashboards, and Visio (Excel product) for diagrams (patents).,To understand the difference between machine learning and data science, ,. A large list of machine learning references ,. It covers the following domains:,Also check out our list of ,: many are considered to be machine learning applications. Finally, a number of additional articles are found in our ,??(constantly updated), as well as in our ,??(constantly updated).,We offer both research and applied projects for potential data scientists. Research projects involve working with simulated data, while applied projects involve working on real data. Both simulated or real data sets can be quite large. In addition, we offer interesting data science challenges, our most recent one ,??(time series and spatial processes). The previous one was on ,.,Here's our list of projects, as of today.,Here are some data sets to get you started. Some are internal to us, but freely available to the public. Each of these articles has a link the related data set, or the data set is available as an attachment in some cases.,Here is another list featuring??,. KDNuggets also maintains a??,.??,We will add more content to this section. In particular, about SQL/NewSQL, SAS, Hadoop, Scala, Julia, machine learning libraries, Mahout and other items.
As a compliment to my??, blog I've just started a , covering everything you want to know about being an analyst, business analyst, data scientist, and any other data-using role in business today.,I'm doing this because I want to give back a little to the analytic community that has given me such a great career. In short, each video will give you (hopefully useful) tips from a 25 year veteran of the analytic profession. I have done a fair bit in my career from programming to executive roles in corporations around the world. The common theme (if there is any) is analytics across 11 countries and working for the likes of Storebrand (Norway and Iceland), Citigroup (UK, Turkey and Greece), ABN Amro (Netherlands and China), The Theseus Institute (France),??NatWest (UK), Information Innovation (Netherlands, Canada and USA), then IAG, AMP, and ING (Australia).,I have just left my job as the head of analytics at News Corp to found a startup called??, - a text analytics play launching an app that automatically evaluates, summarizes and links your documents to relevant knowledge on the internet.,My hope with the YouTube Channel is that people will find it a useful way to spend a couple of minutes each week.,I may live to regret the name 'The Naked Analyst' but hopefully most viewers will notice that I am, in fact, not broadcasting naked and will move on to more fertile pastures! I chose the name hoping that this will indicate my 'unaligned' status. Unaligned in that I am talking about analytics without any particular bias or influence from the myriad of vendors and consulting companies.,So no hidden agendas and no marketing push to buy the lastest tool from a vendor promising to solve all of the problems in your organisation. Not that this ever happens ...,Here are the first 2 videos. The first is a quick introduction:,This second video contains some tips for people who would like to get their first job as an analyst:,I've also put together??a related??, of a few videos that include those made by others on similar topics that I have found on the net. 30 minutes of watching this whole list should give you some insight into how you can go about finding that first job. Good luck!

Confidence interval is abbreviated as CI. In this new article (part of our series on robust techniques for automated data science) we describe an implementation both in Excel and Perl, and discuss our popular model-free confidence interval technique introduced??,, as part of our (open source) intellectual property sharing.??This technique has the following advantages:,This is part of our series on data science techniques suitable for automation, usable by non-experts. The next one to be detailed (with source code) will be our ,.,Figure 1 is based on simulated data that does not follow a normal distribution : see section 2 and Figure 2 in this article. Classical CI's are just based on 2 parameters: mean and variance. With the classical model, all data sets with same mean and same variance have same CI's. To the contrary, our CI's are based on k parameters - average values computed on k different bins - see next section for details. In short, they are better predictive indicators when your data is not normal. Yet they are so easy to understand and compute, you don't even need to understand probability 101 to get started. The attached spreadsheet and Perl scripts have all computations done for you.,We assume that we have n observations from a continuous or discrete variable. We randomly assign a bin number to each observation: we create k bins (1 ??? k????? n)??that have similar or identical sizes. We compute the average value in each bin, then we sort these averages. Let p(m) be the m-th lowest average (1????? m ??? k/2, with p(1) being the minimum average). Then our CI is defined as follows:,The confidence level represents the probability that a new observation (from the same data set) will be between the lower and upper bounds of the CI. Note that this method produces asymetrical CI's. It is equivalent to designing percentile-based confidence intervals on aggregated data. In practice, k is chosen much smaller than n, say k = SQRT(n). Also m is chosen to that 1 - 2m/(k+1) is as close as possible to a pre-specified confidence level, for instance 0.95. Note that the higher m, the more robust (outlier-nonsensitive) your CI.,If you can't find m and k to satisfy level = 0.95 (say), then compute a few CI's (with different values of m), with confidence level close to 0.95. Then ,??or , the lower and upper bounds to get a CI with 0.95 confidence level. The concept is easy to visualize if you look at Figure 1. Also, do proper cross-validation: split your data in two; compute CI's using the first half, and test them on the other half, to see if they still continue to have sense (same confidence level, etc.)??,CI's are extensively used in quality control, to check if a batch of new products (say, batteries) have failure rates, lifetime or other performance metrics that are within reason, that are acceptable. Or if wine advertised with 12.5% alcohol content has an actual alcohol content reasonably close to 12.5% in each batch, year after year. By "acceptable" or "reasonable", we mean between the upper and lower bound of a CI with pre-specified confidence level. CI are also used in scoring algorithms, to provide CI to each score.The CI provides an indication about how accurate the score is. Very small confidence levels (that is, narrow CI's) corresponds to data well understood, with all sources of variances perfectly explained. Converserly, large CI's mean lot's of noise and high individual variance in the data. Finally, if your data is stratified in multiple heterogeneous segments, compute separate CI's for each strata.,That's it, no need to know even rudimentary statistical science to understand this CI concept, as well as the concept of hypothesis testing (derived from CI) explained below in section 3.??,If you look closely at Figure 1, it's clear that you can't compute accurate CI's with a high (above 0.99) level, with just a small sample and (say) k=100 bins. The higher the level, the more volatile the CI. Typically, an 0.999 level requires 10,000 or more observations to get something stable. These high-level CI's are needed especially in the context of assessing failure rates, food quality, fraud detection or sound statistical litigation. There are ways to work with much smaller samples by combining 2 tests, see section 3.,An advantage of big data is that you can create many different combinations of k bins (that is, test many values of m and k) to look at how the confidence bands in Figure 1 change depending on the bin selection - even allowing you to create CI's for these confidence bands, just like you could do with Bayesian models.,The first step is to ,??to make sure that your observations are in perfect random order: read ,??section??, for an explanation why reshuffling is necessary (look at the second theorem). In short, you want to create bins that have the same mix of values: if the first half of your data set consisted of negative values, and the second half of positive values, you might end up with bins either filled with positive or negative values. You don't want??that; you want each bin to be well balanced.,Unless you know that your data is in an arbitrary order (this is the case most frequently), reshuffling is recommended. Reshuffling can easily be performed as follows:,Note that we use??100,000 + INT(10,000*RAND()) rather than just simply RAND() to make sure that all random numbers are integers with the same number of digits. This way, whether you sort alphabetically or numerically, the result will be identical, and correct. Sorting numbers of variable length alphabetically (without knowing it) is a source of many bugs in software engineering. This little trick helps you avoid this problem.,If the order in your data set is very important, just add a column that has the original rank attached to each observation (in your initial data set), and keep it through the res-shuffling process (after each observation has been assigned to a bin), so that you can always recover the original order if necessary, by sorting back according to this extra column.,Download the ,. Figures 1 and 2 are in the spreadsheet, as well as all CI computations, and more.??The spreadsheet illustrates many not so well known but useful analytic Excel functions, such as: FREQUENCY, PERCENTILE, CONFIDENCE.NORM, RAND, AVERAGEIF, MOD (for bin creations) and RANK.??The CI computations are in cells O2:Q27 in the Confidence Intervals tab. You can modify the data in column B, and all CI's will automatically be re-computed. Beware if you change the number of bins (cell F2): this can screw up the RANK function in column J (some ranks will be missing) and then screw up the CI's.??,For other examples of great spreadsheet (from a tutorial point of view), check the Excel section in our ,.????,The simulated data in our??, (see the , tab), represents a mixture of two uniform distributions, driven by the parameters in the orange cells F2, F3 and H2. The 1,000 original simulated values (see Figure 2) were stored in column D, and were subsequently hard-copied into column B in the ,??(results) tab (they still reside there), because otherwise, each time you modify the spreadsheet, new deviates produced by the RAND Excel function are automatically updated, changing everything and making our experiment non-reproducible. This is a drawback of Excel, thought I've heard that it is possible to freeze numbers produced by the function RAND. The simulated data is remarkably non-Gaussian, see Figure 2. It provides a great example of data that causes big problems with traditional statistical science, as described in our following subsection.,In any case, this is an interesting tutorial on how to generate simulated data in Excel. Other examples can be found in our ,??(see Excel section).,We provide a comparison with standard CI's (available in all statistical packages) in Figure 1, and in our spreadsheet. There are a few ways to compute traditional CI's:,As you can see in Figure 1, traditional CI's are very narrow. Note that inflating the traditional CI's by a factor SQRT(k), that is, replacing $F$6+R3 by??$F$6+SQRT($F$2)*R3 in cell S3 in our spreadsheet (and similar adjustments in all cells in columns S and T), leads to similar CI's. Indeed, traditional CI's have been designed for the mean, while ours are designed for bin averages (that is, batch averages in quality control), or even individual values (when n=k).This explains most of the discrepancy. Finally, our methodology is better when n (the number of observations) is small (n < 100), or for high confidence levels (> 0.98) or when your data has outliers.,Here's some simple source code to compute CI for given m and k:, , , , , , , , , , , , , , , , , , , , , , , , , , , ,: write the code in R or Python.,Rather than using ,-values and other ,??(about to become extinct) that nobody but statisticians understand, here is an easy way to perform statistical tests. The method below is part of what we call ,.,Let's say that you want to test, with 99.5% confidence (level = 0.995), whether or not a wine manufacturer consistently produces a specific wine that has a 12.5% alcohol content. Maybe you are a lawyer, and the wine manufacturer is accused of lying on the bottle labels (claiming that alcohol content is 12.5% when indeed it is 13%), maybe to save some money. The test to perform is as follows: check out 100 bottles from various batches, and compute an 0.995-level CI for alcohol content. Is 12.5% between the upper and lower bounds? Note that you might not be able to get an exact 0.995-level CI if your sample size n is too small (say n=100), you will have to ,??from lower level CI's, but the reason here to use a high confidence level is to give the defendant the ,??rather than wrongly accusing him based on a too small ,. If 12.5% is found inside even a small 0.50-level CI (which will be the case if the wine is truly 12.5% alcohol), then a fortiori it will be inside an 0.995-level CI, because these CI's are , (see Figure 1 to understand these ideas). Likewise, if the wine truly has a 13% alcohol content, a tiny 0.03-level CI containing the value 13% will be enough to prove it.??,One way to better answer these statistical tests (when your high-level CI's don't provide an answer) is to produce 2 or 3 tests (but no more,??,). Test whether the alcohol rate is,We include two figures in this section. The first one is about the data used in our test and Excel spreadsheet, to produce our confidence intervals. And the other figure shows the theorem that justifies the construction of our confidence intervals.
What is an employer?€?s most business-critical corporate process? At or near the top of this list has to be hiring employees that deliver more value to their role and company than they cost to their employer. Employees bring in revenue, rescue a customer, make your products, deliver goods, and sustain your profitability going forward. Identifying the right people, and avoiding the wrong ones, is an imperative to business sustainability.,But how is??,candidate screening handled at your organization? Many employers take an approach that isn?€?t at all what we expected.,Talent Analytics uses predictive modeling to help organizations reduce attrition and increase on the job performance with in high volume job roles. Our projects give us insight into the entire hiring process.,We?€?ve discovered that a surprising number of companies, typically midsize and large enterprises who screen a large volume of candidates, often relegate this single most critical task to individuals far removed from the line of business.?? At times this task is given to contractors, interns, temps or external part-time employees.,What a huge point of failure leading to massive hiring errors and missed opportunities.,Organizations need to replace this manual, error-prone screening process with predictive analytics processes and technologies that are unbiased, trained to look for a specific combination of predictive factors.?? Predictive models don?€?t get tired; they learn and get better over time; they equally weight candidate.,We?€?ve uncovered two truths when recommending hiring processes including talent analytics. First, some people seem to inherently distrust the analytics-based process. And second, those who distrust it are often the same people who consign candidate screening to under qualified screeners.,We understand hesitation. Including predictive modeling in the hiring cycle is new to many organizations. It can be difficult for many people to get their head around the models and what they?€?re doing.,But while skepticism of new approaches may be logical, blind reliance on old, ineffective methods is not logical and is not fair. And that?€?s certainly the case when depending on part-timers or interns to screen job candidates.,These screeners typically need to review a large number of??,s in a short period of time for a wide variety of roles. Even if they?€?ve had weeks of intense training on how to screen candidates?€?which is almost never the case?€?it?€?s impossible for a single individual to keep all relevant variables in mind as they scan??,s or conduct 10-minute screening phone calls. As a consequence, the initial decision about one of your most important corporate processes is made in what?€?s clearly an error-prone manner.,A far more effective method is to include talent analytics to ensure your hiring processes are based on relevant data. Analytics technology can process high volumes of data, without bias, without excluding important variables, without growing tired?€?and with consistency, from the first candidate to the last.,The result is a hiring process that?€?s accurate, that?€?s repeatable and that?€?s far more likely to surface the best candidates and eliminate those you want to avoid.,So there doesn?€?t have to be a battle between traditional screening processes and talent analytics. Just be sure you?€?re applying the two approaches with the right balance that will deliver the results you want: a workforce that truly supports your business objectives.
I am seeking input from industry professionals on whether I have the requisite experience and skills to be a candidate for a data analyst position in the life sciences industry.,My background in brief: I am an IT Director at a major pharmaceutical company who is exploring a career change into data analytics. I have 25+ years of varied IT experience including leading teams of database administrators, BI analysts/developers and master data management specialists. My hands-on skills are advanced Excel, basic SQL and R, along with strong analytical and communication/presentation abilities. I have a good understanding of statistics -- as a university adjunct professor I taught several courses, including one in data analytics to MBA candidates -- but no work experience in data analysis.,My motivation for exploring a change is simply that I enjoy data analysis and statistics. My financial situation will allow me to take a job at a data analyst level salary and progress from there, but first I wonder whether my background would qualify me for a position.,Does anyone have thoughts to share on this?,Thank you??
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,These profiles are randomly selected among our active and new members. To be selected, you need to have an interesting profile with picture; it helps to post a blog, or share / comment contributions from other members.
Below are 30 of the top stats that can help you either sell social media to the higher ups in your company or help you in securing additional resources for your company?€?s social program.,.
It?€?s a frequently asked question. You can read several discussions on the topic??,,,, and??,. So far, the best solution is to process data to a smaller dataset. Then use D3.js to visualize.,With carefully crafted data processing, we can get decent story from data. But this solution doesn?€?t provide a lot of flexibility to experiment with data on the fly. We need a more streamlined workflow. Less friction can spark interesting data innovation.,Google BigQuery a great tool to handle big dataset. It?€?s definitely going to help us handle big dataset for D3.js.,I will use New York Taxi dataset hosted on Google BigQuery. It is 4+ GB and has more than 350 million rows in 2 tables. In this article, I want to show you how to query it on the fly. Then use D3.js to create a line chart of total trip amount over time. You can explore the dataset here:,(You?€?ll need to setup BigQuery account with one project to see public table),Then you can use Javascript client library to query for your dataset on the fly. See full article and code here:,Finally, you'll have a visualization that gets data directly from Google BigQuery.,Phuoc Do
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.
Here are 10 rules to design great, simple, efficient, adaptive dashboards.
 , , , , , , , , 
 , , , , , , 
Or to put it differently, when your metrics lie to you: how to find out, and what should you do???,The purpose of this article is to let Google aware of the problem, and fix their Google Analytics reports (filtering out the fake traffic). This scheme also impacts??,. Tons of websites now have their traffic stats wrong, and should consider using home-made solutions, to filter out fake traffic undetected by vendors.??,I used just a tiny bit of data science and tiny data (from Google Analytics??real time stats - number of users, top locations, pages visited)??to unearth the following:,The following is an updated analysis as of October 22. 2014 at 11 pm PST, as the pattern has evolved since yesterday.,Evidently, these patterns will change over time.,It could be,The scheme is (for now at least) extremely easy to detect. It will take a few days (my guess: less than one week, let's see) before Google and other companies find a fix.,Showing artificial traffic (robot) in a separate box or tab. I guess robot traffic is filtered out by Google,??,??(see section 3 after clicking on the link), but it would be useful if Google Analytics reports this automated traffic separately. Note that the bot in question triggered the Google Analytics JavaScript tracking code, quite unusual, but nevertheless this bot was easy to detect, and should not have been counted in normal traffic. Empirical evidence suggests that this bot and other bots also inflate traffic stats from other websites, based on measurements from various web traffic measurement companies.??
Interesting PDF document featuring 15 data scientists (mostly co-founders of various start-ups or well known data science websites), with an average of 9 pages per interview. Posted on , by Hannah Brook.??Here's the list:,.??
 *
This article is co-authored with Dr. Livan??,. The two charts below are based on a number of Twitter followers and their growth rate, ,. Our next step is to identify ,, that are themselves followed by many other ,: in our opinion, this is one of the best metrics to measure real popularity. ??,For some, the number of Twitter followers is a must to make a real impact. Tweets that are re-tweeted can indirectly generate ,. Here you can find the number of followers that some data science influencers are getting per day.,To attract more Twitter followers:
There are many lists of top data scientists on Twitter. Here we mention and comment one of them, published on BigData-MadeSimple; most other lists have similar drawbacks. It has been argued that the overlap between my ,??and the ,??is small, because there are LinkedIn versus Twitter data scientist users (and maybe Quora or Google+ data scientists).,But is this the real explanation? It turns out that the Twitter lists typically suffer from big gaps (my LinkedIn list has big gaps too, but at least I mention the gaps and biases). This Twitter list misses people that should be in the top three.,So how do you create a good list (with no big gaps)? The answer is simple: use data science! More explicitly, here are 7 tips to make a good list (and we'll share our list with you when we are done),One of the people not listed in the initial list of 33 data scientists (below) is ,;??it has more followers (22,900) than pretty much all the people listed below, and is growing faster than many, currently at a rate faster than (say) @kdnuggets (both accounts reached 20,000 followers almost the same week, a few weeks ago). I'm sure many are missing for the same reason: they don't use their name in their profile; @analyticbridge uses 'big data science' rather than his name. Of course, you need to filter out commercial accounts owned by corporations when considering these types of accounts, but that should be easy, using white lists of commercial Twitter accounts.,So who's @analyticbridge? Who else is missing? I'll leave it to you to find out, but we hope to provide a list of our own soon, to fill the gap. Of course, @analyticbridge must be someone with a small ego, as he's not interested in having his name published. In an era of privacy scares, not using your real name could be a good strategy.,They added @analyticbridge in position #34 after I mentioned the issue. The first number after the handler is the number of followers as of today; @analyticbridge (not in the original list) ??has 22,900.

The role of statistics in data science is often debated. Despite rapid developments in technology giving access to algorithmically sophisticated approaches, I feel that statistics can still provide many worthwhile insights. If I have a database of sales figures spanning many years, I feel that I can become more aware of historic trends and seasonal patterns through the use of statistics. Statistics offers a sense of state, direction, pace, and progress. Statistics can also enable estimation of future levels. What statistics doesn't do in my opinion is address questions like the following: "How did we get here?" "How do we get ourselves elsewhere?" In this blog, I will be arguing that statistics should not be expected to guide intervention since the "object of intervention" (the thing receiving the attention) hardly "participates" in statistical data. I define participation in data as the extent to which underlying phenomena are exposed to the risks and benefits of intervention directed at the data. These dynamics will become clearer after some explanation, I hope. We should never dismiss the importance of studying state and comparing changes over time. However, making a comparison to describe how something developed is quite different from explaining the reasons why those developments occurred. The academic performance of students can be determined using their grades. This tells us nothing about the situations confronting students that affect their performance. If the objective of an educational system were to improve grades, it would be necessary to gather much more data than just exam and essay scores. Grades reflect how a student interacts with external criteria. The data is not an intrinsic part of the person. If participation exists, its parameters are set by the users of the data rather than those being evaluated.,I believe that most organizations recognize the need to incorporate as much data as possible into decision-making. Whether or not these decisions lead up to the intended outcomes is a separate issue. It is possible for managers to make decisions without using any data or even giving their circumstances much thought. Financial commentators periodically mention how random selection can sometimes outperform professional investment managers. The fact that people make use of data does not necessarily mean that the data will be used effectively. It is necessary to distinguish between how managers respond to their data and the outcomes of intervention. There is a need to confirm whether decisions truly contribute positively to organizations. I would argue that the efficacy of decisions depends on the level of participation of the underlying phenomena in data. In relation to political discourse when discussing "participation," public activism and community involvement probably come to mind. In my own research, I found that people can be involved in institutionally mandated processes without necessarily having much real impact. They can be counted and at the same time ignored. Participation need not be effective.,This brings me to my observation that people can be part of a process and yet not actually participate. A company can maintain sales statistics and yet not have the foggiest idea why people purchase their products; for the statistics say nothing about the reasons. I can count a basket of apples and in the end have or gain little understanding of this fruit; for the purpose of counting is not to understand but only to confirm the number. Although the underlying phenomena can participate in data, this participation doesn't necessarily have to be significant. A metric that is extremely prevalent in Western society is cost. I guess because there is so much money to take into account, almost every situation can be reduced to its cost relevance. It is an alienation of things to characterize existence purely in relation to costs. It can be said that in a data-rich environment that is highly cost-oriented - e.g. in an accounting department - the participation of the deeper phenomena might be minimal. Many workers appear at their workplaces every day as confirmed by their attendance history although their employers might have little understanding of them as individuals. In the image below, I formally separate the object phenomena from its data proxy or its representation in data. Data is a proxy for phenomena. I describe the conceptual overlap as participation.,Figure 1 - Data-Object,Data is merely a representation of the object and not the object itself. This is an important distinction. It is possible to have a lot of worthless information; consequently, an organization might improperly value its intellectual capital and therefore misjudge the likely return on investment. I believe that objects participate in data in different contexts and at many levels. Demographic data collected during the census contain little in terms of object participation. The use of demographic data is defined externally. Those that participate in the census have minimal say over how the information collected should be interpreted and applied to decision-making. Participation of the object in data is important from the standpoint of providing services and to support intervention. If an object is quite far away from its representation in data, there is a limit to how much the needs of the object can be addressed through the use of the data - for instance, for allocating resources. These dynamics are not exclusive to service providers. In a discussion of performance evaluation, scorecards and rankings can be deployed that are quite sophisticated and yet a great distance from the objects being examined. Marx wrote about a form of alienation called reification. It is a routine occurrence in our society for metrics to be elevated irrespective of the distance from underlying phenomena; perhaps in many respects, reification has become structuralized and normalized through our theocratic faith in technology and neoliberal preconceptions of development. Having big data is not the same as having great solutions. Tools don't make a craftsman. A craftsman makes things using tools.,I describe the user of data as the subject. This is the person or organization that puts forth the need for data and therefore criteria to enable fulfillment. In data, the interests of both subject and object come into play; it is important to recognize this pervasive and fundamental power struggle at the atomic level of our social reality. Earlier, I said that the object doesn't necessarily have to participate much in data. But actually, the object doesn't have to participate at all. The recognition of data is often determined by the subject. While it hardly seems business-like to do so, those attending a board meeting can propose a course of action lacking substantiating data; this can occur if there are pressing demands for a response. The absence of information does not negate the need to render decisions. Decisions can be made through the use of presumptions, assertions, assumptions, philosophies and ideals, entrenched patterns of reasoning and behaviour, and of course gut instincts. The subject decides how far away the data can be - the distance of the data from the socially constructed forces that define and impose meaning irrespective of underlying phenomena. Given the context of discourse surrounding public participation, I decided to use the term "regulation" to describe its subjective counterpart, as shown on the image below. Therefore, in data there can be participation of the object; and there can also be participation of the subject. I describe the latter as regulation. I think that it is fair to say, when we talk about data, the object-subject distances don't normally enter the picture. I believe this is because data is frequently not interpreted in critical or complex terms.,Figure 2 - Subject-Data-Object,The duality of capitalism is sometimes discussed in environmental philosophy. On one hand, it is possible to gain benefits from capitalism; on the other, there can be environmental destruction and harm to people. I know these polemics might make me seem rather radical. I'm actually big on free enterprise. Consider duality as it relates to companies selling products that people don't want to buy; managers making decisions disassociated from the underlying problems they hope to tackle; and marketing departments collecting large amounts of superfluous information. Little of the surrounding reality might participate in the data being collected. The data merely becomes part of the illusion. If we assume a type of expanding recursive behaviour in data, hazards exists in the expansion of organizations as distances increase in the subject-data-object construct. Structural expansion might bring about a need to increase control patterns - philosophies, policies, practices, protocols, and regulations - that lead to an imbalance. So great has been our tendency to be controlled, the participation of objects in data has not kept pace. In my blog post on Transpositional Geography, I explain how the relevance of data to different contexts can be used to map out non-spatial events. I would say that over time through systematization - literally through the expansion of control systems - the transpositional plane has leaned in favour of regulation by the subject; against participation by the object; insulating capital from its environment.,Figure 3 - Expansion and Transposition,Also in an earlier blog, I describe three types of data flows in an organization: projection, direction, and articulation. These data flows, indicated in the illustration above, provide analogs to explain the subject-data-object construct in systemic terms: regulation expands to become projection; participation to become articulation; while the body of data becomes the multifarious events related to processes of production. On an organizational level, I believe that true adaptation is becoming increasingly difficult to achieve due to declining levels of participation in the data. It is quite challenging to adapt while being insulated from the environment within the organization (through lack of internal articulation) and outside it (through lack of external articulation). Handling the subject-data-object construct can affect the functionality of large structures and systems. I hope this perspective provides useful insights in terms of the pathological condition of structural insulation: e.g. how phenomena can be substantively alienated from data systems thereby making successful intervention less likely.,Before proceeding with some case examples, I just want to emphasize the rather routine nature of non-participatory data. The fact that an organization has a plan and closely follows it does not mean that things will improve. Governments by nature deliver institutional responses to problems; this is not necessarily related to problem mitigation or resolution. For instance, it might make perfect sense from a bureaucratic standpoint to promote improved insulation in homes in order to save energy. In the past this has led to class-action lawsuits: the use of urea-formaldehyde comes to mind along with a few other incarnations of energy-saving insulation. Companies sometimes find it necessary to implement restructuring. I ask readers to reflect on how often, how well, and for how long restructured companies actually survive. We need to distinguish between the plan or course of action and the actual outcomes - the proxy and the phenomena. Although intervention might indeed cause data to behave as expected, this does not mean that the reality will do likewise if the level of participation between reality and the data is quite poor. Nobody has ever really questioned the ability of organizations to carry out plans. But whether the successful implementation of such plans is good for society or the organizations themselves is a separate issue. Companies seeking the highest profits possible have suffered against foreign competition: for in a war of minimalism, somebody else can often produce the same products far faster and better. Thus, many organizations have deliberately adopted plans likely to lead to their market termination. So if companies fail, it is not necessarily because their plans fail but rather as a consequence of total success; it just so happens, quasi-intellectualism is often a poor substitute for effective intervention.,My undergraduate thesis dealt with the effectiveness of public participation in local planning; this is quite similar to the topic of this blog. Although my research was specifically about community participation in municipal planning, I feel that subject-data-object dynamics inspired by my undergraduate research are broadly applicable to other areas of concern. Keeping this portability in mind, I would now like to offer some real-life examples from my own data. I have been arguing that effective intervention is quite difficult if there is lack of participation in data. This is a good principle, but there are also practical reasons to regard data more inclusively. For instance, if a person is unaware of what data contributed to the metrics, it might be unclear how to intervene in order to influence the metrics. Another rather routine problem involves the question of how to respond if in fact the data collected seems irrelevant to the metrics: that is to say, intervention was attempted, but the data used in the intervention did not lead to the desired outcomes. One has to determine if the intervention was faulty, or perhaps the data chosen reflects the underlying phenomena poorly. About a year ago, I started collecting a large amount of personal health data. Rather early in the development of the alpha, I decided that I would eventually use the data to support intervention. So I needed the system to provide specific insights. It is inadequate to have charts showing trends - i.e. statistical charts - due to their lack of tangible guidance.,When I originally thought about "improving" my health (through algorithmic intervention), the mission seemed simple. I had to gather data, and then from this data I would respond accordingly. This was the plan - and it made sense at the time. But how does a person respond accordingly to unfamiliar data? Not so much due to effectiveness but rather ease of intervention, I initially experimented with my choice of nutritional supplements. During much of the trial year, I had minimal understanding of intervention. I discovered over the course of these experiments that (1) a person can accidentally discover useful things and be more aware of their usefulness simply by collecting and frequently studying the data. It isn't always necessary to have an elaborate plan of action. I also noticed how (2) the underlying phenomena might not fall fully under the scope of the data being examined. Therefore there is a constant search for different types of data to characterize the phenomena. I then reasoned that (3) the data collected involves my interaction with the phenomena; therefore, the resulting metrics reflect not so much the phenomena itself but its relationship or response to something else including but not limited to my intervention. This has given rise to my assertion that the data represents a "stress response." The algorithm that I use to determine the relevance between the data and metrics is premised on stress dynamics. (During my graduate studies, I examined how structural transformations contribute to stress. I might write more about this in the future.),The image that follows shows my "sleep perceptions": the gradient reflects a combination of factors affecting sleep including heart regularity and depth of restfulness at steady-state conditions. There is a trend line on the graph indicating gradual improvement despite rather choppy day-to-day fluctuations. The data giving rise to the gradient levels represents not sleep itself but rather my perceptions of it. I monitor the relevance of many hundreds of data events in relation to the gradient. But the events are only useful to the extent that they proxy or allow for the participation of the phenomena of sleep. Participation is much less rigid than the idea of causality: to understand a person, it is not necessary to know the cause; to be part of an ecological system is to exist as a person and not as the outcome of something else. Figure 4 confirms that there has been some improvement in sleep perceptions through intervention - much of it recent, after I gained a better understanding of how to intervene. I use a linear trend line although polynomials would certainly contribute to different perspectives. While it is arguable whether or not a person should deliberately attempt to intervene to achieve improved sleep perceptions, I had no problems halting the consumption of things that seemed to interfere with sleep. For instance a popular remedy said to be worthwhile for arthritis and another for prostate functionality seemed to have adverse consequences on my sleep perceptions. (I want to emphasize that the perceptions are mine only. Others should not infer guidance since none is intended.), ,Figure 4 - Sleep Perceptions,Between day 46 and 76 on the x-axis, there is a spike suggesting really superb sleep perceptions. I haven't been able to reach such heights for a long time. But because the system is so effective in its ability to give details, I believe there is one particular supplement that I was taking just at that time. It is a supplement that is being examined for its contribution to cardio-vascular health. Sadly, my supply actually ran out on day 76. I adapted to the situation, of course. In order to achieve gains without the supplement, I turned to particular types of rhythmic exercises. This is not to say that all forms of exercise help me sleep. Only specific types seem to provide improvement. Fortunately a few weeks ago, I found the supplement once again at a local grocery store. The nature of this supplement indicates that dietary changes might have positive consequences , to eat a lot of leafy green vegetables. A problem that has emerged in relation algorithmic intervention is lack of rational explanation: I might get both the question and answer without necessarily understanding the underlying problem. Intervention then becomes an exercise in risk management: I justify my response from the indicators available without completely understanding the situation. For instance in relation to treatment specifics such as dosages, I cannot internalize the logic ,. I rely on algorithmic tools that are more responsive to the surrounding circumstances than me. Nonetheless, it goes without saying, forming reasons to try to explain reality has become a preoccupation. I have found that lower internal body temperature has been associated with improved sleep perceptions; it is all extremely interesting.,Although I can tolerate lack of sleep it sometimes seems indefinitely, the same cannot be said in relation to poor breathing. Breathing problems justify immediate and aggressive intervention. I have reason to believe that for me breathing perceptions are sometimes related to environmental conditions. One day almost by chance, I decided to view the textiles from my blankets under a microscope. I found that some of my bedding materials were full of loose fabrics. I felt that these fabrics could be easily inhaled during sleep. So now I frequently change bedding. I choose fabrics that are unlikely to loosen. I have also found certain supplements that for me seem to improve breathing - one specifically that my family doctors prescribed many years ago albeit not for breathing. I normally just visit the vitamin counter rather than a pharmacist for my supply. I find that the supplement helps with breathing on the day it is taken without cumulative benefit. I maintain two different contexts for breathing: one for my morning breathing perceptions (Fig. 5); and the other for the evening (Fig. 6). Similar improvements are indicated on both illustrations.,Figure 5 - Beginning-of-Day Breathing Perceptions,Figure 6 - End-of-Day Breathing Perceptions,I once continued having breathing problems many months after recovering from pneumonia sometime in the mid-90s. (By the way, we can blame my ability to program in several contemporary computer languages on these breathing problems. The coughing made it quite difficult to maintain a regular job. One of the few things I could still do effectively was program.) I visited a specialist for advice. He said after examining my x-rays, "I don't know if anyone has ever mentioned this to you, but you have chronic sinusitis." It only affected one of my nasal passages. I didn't bother looking up the meaning of this medical term for the blog. However, I can plainly say that my nose has tended to be stuffed up. I haven't deliberately tried any kind of intervention focused on nasal performance. But it is reasonable to expect some mitigation as a result of recent efforts to improve my breathing. I started collecting data to get a better sense of the situation. The gradient for nasal perceptions is also linked to sound - that is, the sounds that I hear while attempting to breathe hard through my nose. The most recent chart including its trend line is shown below (Fig. 7). My sense of smell has been much better recently - better than I ever remember it being. It has been like gaining a completely kind of nose, which I must say has been rather mood altering., ,Figure 7 - Beginning-of-Day Nasal Breathing Perceptions,We sometimes hear stories about people following their GPS devices regardless of what they see in front of them. An organization might create a plan and follow it regardless of the ensuing events - as if the point of the exercise were to follow a plan rather than achieve improvement. In such scenarios, the surrounding reality cannot be said to participate in decision-making. Not all forms of participation are configured in a manner that resembles democratic involvement. Whether or not people vote or assert themselves during public meetings, their interests can be taken into account. This is the essence of participation. Data is much more useful when it is connected to the lives and lived experiences of people. Patients in a hospital can participate in decision-making by having their needs considered; they don't have to be involved in the day-to-day administration of hospital services. Similarly, it is possible for emerging consumer needs to be integrated into product development. Participation does not necessarily manifest itself as democratic representation. Conversely, even if people are involved by their physical presence in participatory processes, this does not necessarily mean that their interests are taken into account.,So I have offered some real-life examples of intervention. I actually had many more, but I deleted some material to control the size of the blog. I don't know how many people would be fascinated by my internal body temperature; if I can turn things to ice through touch, then that's sort of interesting. Unlike algorithmic processes leading to intervention decisions, the outcomes of intervention can be examined using relatively traditional methods: e.g. technical analysis including moving averages; trends patterns for example using linear regression; and in relation to subjective targets established strategically. Those accustomed to examining metrics of performance through statistics can continue doing so I think rather effectively. A statistical evaluation of superficial metrics remains perfectly reasonable. I have simply argued in this blog, traditional strategies don't necessarily support intervention particularly well. Overall, algorithmically-inspired intervention can have beneficial impacts as confirmed by the preceding images. Intervention depends not just on the ability to detect a problem but also confirm the effectiveness of mitigation; the latter is only made possible if there is participation in the data. The frontier of intervention involves a type of math focused on the relevance of data to its underlying phenomena (the object) rather than the user (the subject). In contrast, statistical evaluation has mostly been about conforming to external needs of the user. Statistics can tell us the likelihood of a newborn baby becoming a lawyer or doctor - which is useful if we would like to know the likelihood - since the information is otherwise quite pointless from the standpoint of intervention.
What do Target and the NSA have in common?,They both??used Big Data and analytics in a way that inspired a major slap on the hand. Target was predicting which of their customers were pregnant and sending them targeted coupons for baby products, which prompted the ire of a father whose teenage daughter hadn?€?t told him yet. The NSA incurred the wrath of world??leaders, the US Congress, and even a good portion of the American public when??,the extent of their data gathering??methods were revealed by a former contractor.,It reminds me of this bit of wisdom:,The lack of an ethical standard for uses of data is causing conflict between users of data and the people those data describe.??Organizations that wait until the courts weigh in on the matter may find themselves being the subject of precedent-setting cases. It would be far better to adhere to a code of ethics concerning data than have a lackadaisical??attitude??exposed in court.,So how do we decide whether a potential use of data is ethical? I propose that we ask ourselves the following questions before beginning any project:,This list is by no means exhaustive;??this is just the start. My hope is that others will see this and add in their own questions and guidelines, and we can begin the process of building the ethical framework that is so desperately needed.,I?€?m sure there are some decision makers??that will not welcome these questions, but, as is frequently the case, we can choose to regulate ourselves in this matter or we can have regulations forced upon us. In some places the government is already getting involved, such as the legislation proposed by Senators John McCain and John Kerry in the United States. The European Union considers data privacy such an issue they formed the Article 29 Working Party way back in 1996, and that group regularly weighs in on proposed legislation. Governments tend to move upon??signs of public outrage, and we are dangerously close to that point with some uses of data.,So I ask you, what are you going to do with that data about me? Would I like it?
Companies spend $35 billion on so-called SQL databases to store and retrieve their data. Goliaths Oracle, IBM ans Microsoft??all sell such databases. But a 50-employee, Cambridge, Mass.-based startup claims to be winning business from their enterprise customers. Should investors in those Goliaths be worried?,The startup in question is NuoDB and it sells a??distributed database management system (DBMS) to companies that use SQL databases, operate their data centers in the cloud and want guarantees of great service.,NuoDB started selling its product in January 2013 and its customers include Autozone??and Dassault Syst??mes. NuoDB was one of 19 vendors in the Gartner??Magic Quadrant for Operational DBMS.,To understand why NuoDB could threaten Oracle and its peers, it is useful to explain the distinction between ?€?scale up?€? and ?€?scale out.?€? According to my July 8 interview with NuoDB investor Mitchell Kertzman, managing director of Hummer Windblad and former Sybase CEO, ?€?If you are a company that wants to expand its database capacity and buy from Oracle, you need to scale up. This means that you have to buy expensive hardware and software that is engineered for peak demand. With NuoDB, a company can scale out ?€? meaning that it can inexpensively expand and contract its processing power through the cloud and have much less waste.?€?
Foreign exchange market is world's largest and least regulated financial market. Its estimated daily turnover is $5.35 trillion, according to the Bank for International Settlements?€? triennial survey of 2013. Speculative trading dominates commercial transactions in the forex market, as the constant fluctuation of currency rates makes it an ideal venue for institutional players with deep pockets ?€? such as large banks and hedge funds ?€? to generate profits through speculative currency trading.,??,Though other financial markets are not as big as Foreign exchange market, their daily turnover will be in multi billion dollars.,??,??,Let?€?s say a trader at the London branch of a large bank receives an order at 3:45 pm from a U.S. multinational to sell 1 billion euros in exchange for dollars at the 4 pm fix. The exchange rate at 3:45 p.m. is EUR 1 = USD 1.4000.,??,As an order of that size could well move the market and put downward pressure on the euro, the trader can ?€?front run?€? this trade and use the information to his own advantage. He therefore establishes a sizeable trading position of 250 million euros, which he sells at an exchange rate of EUR 1 = USD 1.3995.,??,Since the trader now has a short on euro and long on dollar position, it is in his interest to ensure that the euro moves lower, so that he can close out his short position at a cheaper price and pocket the difference. He therefore spreads the word to other traders through emails, chat rooms & messages, that he has a large client order to sell euros, the implication being that he will be attempting to force the euro lower. Thus, he sets an platform to fix the euro & dollar exchange rate.,??,At 30 seconds to 4 p.m., the trader and his/her counterparts at other banks - who presumably have also stockpiled their ?€?sell euro?€? client orders - unleash a wave of selling in the euro, which results in the benchmark rate being set at EUR 1 = 1.3975. The trader closes out his/her trading position by buying back euros at 1.3975, netting a cool $500,000 in the process. Not bad for a few minutes work!,??,The U.S. multinational that had put in the initial order loses out by getting a lower price for its euros than it would have if there had been no collusion. Let?€?s say for the sake of argument that the ?€?fix?€? - if set fairly and not artificially - would have been at a level of EUR 1 = USD1.3990. As each move of one ?€?pip?€? translates to $100,000 for an order of this size, that 15-pip adverse move in the euro (i.e. 1.3975, rather than 1.3990), ended up costing the U.S. company $1.5 million.,??,The modus operand is similar for the other fixing scandals such as benchmark rate fixing (example LIBOR), interest rate fixing, derivatives price fixing and etc,.,??,Banks do have policies, procedures and surveillance systems in place that are supposed to stop these kinds of collusions. How ever, they are clearly ineffective when dealing with employees who have the skills to manipulate the system.,Also, some times the employees are driven by the greed of top executives and are forced to rig the benchmark rate to gain undue advantage and to make huge profit for the company at the cost of their clients.,Hence it is imperative to imbibe an ethical culture deep root in the organisation.,??,Banks trading platform including forex operations involve thousands of email communications, chat room conversations & instant messages. Hence it is essential for banks to deploy a smart system that is capable of integrating various structured, semi structured & unstructured data, ability to mine voluminous & variety of data and possibly with machine learning capability to filter potential alerts in a real time basis. These alerts can be investigated further by any case management system to identify any potential fraud and avoid them proactively,??,It is impossible to rig the rate without collusion with other like minded traders.,For example, Forex social networks are a fast emerging trend in the world of FX trading. They allow their members to see what other traders are doing, individually and en masse, in order to gain a broader knowledge of the market and the trading strategies of others. They are similar forums for many other types of such financial instrument trading. Banks should adopt a technical solution to closely track & monitor suspicious employees, by analysing his & his associate?€?s financial transactions, trading transactions, his internal communications and more importantly his social network communications. It will be possible to integrate these data, analyse them and to detect and classify potentially fraudulent employees.,??

 , , , , , , , , 
From what you hear from all the latest technology news, Bring Your Own Device (BYOD) is the end-all, be-all of technology policies, guaranteed to make businesses run smoother, employees feel happier, and cash pile up like never before. BYOD can certainly be helpful, but it?€?s far from the solution to all of a business?€?s problems, mainly because there are many pitfalls companies can easily fall into. These mistakes are easy for anyone to make, which is why knowing them before implementing a ,is essential if you want to see success.,One of the main draws of Bring Your Own Device is the potential to cut costs. The basic idea is that if employees use their own devices, they?€?ll have to deal with the cost of upkeep and upgrading while the company won?€?t have to purchase items of their own. So it might surprise you to find out that a poorly structured BYOD program may end up costing a company more than they save. Think of it this way: if a company picks up the tab for a monthly smartphone bill, the end result may be the company paying for expensive data plans the employee never would have gotten had he or she been in charge of all expenses. Plus there?€?s always the risk an employee could use the phone for overseas calls, again costing the company some money better spent elsewhere.,BYOD policies may also lead to less productivity among the workforce. Yes, the goal is that BYOD programs will make employees more productive since they?€?ll be familiar with what their own devices can and can?€?t do, but think about all the ways an employee can become distracted. Whether they?€?re checking the latest updates on Facebook or spending their time knocking out pigs on Angry Birds, every minute spent playing a game or checking social media is a minute where no actual work is getting done. Less productivity means less money for the company.,For all the advantages and disadvantages of BYOD programs, perhaps the most serious drawback is the added security risks. Let?€?s face it--even when dealing with their own expensive devices, people can get pretty careless and lose them. In most cases, it might mean losing some contact information, but in the case of misplacing a smartphone or tablet with valuable company data on it, the consequences can be severe. There are ways around this issue, like full device swipes, but that?€?s only effective if employees immediately alert IT personnel about the issue. Many don?€?t do that right away, making the risk of losing vital data that much greater. BYOD also introduces a number of other security risks such as viruses and malware that can steal data and new avenues like text messaging where employees can share company information with little chance for detection.,BYOD might be useful, but it could end up creating conflicts between employees and management. Take the earlier example of workers spending too much time on distracting apps while on their own devices. While management might respond to this problem by blocking those apps deemed to be a waste of time or segregating the work part of the device from the personal part, some people don?€?t respond well to being told how to operate stuff they already own. Since it?€?s their property, they may feel added rules are unnecessary, and they may argue against it. Needless to say, the stricter the BYOD policy, the less risk of data and productivity loss, but that also costs workers the freedom they thought they would have under the program. Employees also have to be willing to trust management with their personal info, which might be a stretch for some people.,These pitfalls might seem like surefire reasons to stay as far away from BYOD as possible, but that doesn?€?t mean a thoughtful, clearly communicated BYOD program can?€?t greatly benefit a company. The important thing for every business leader to do before implementing BYOD is to know the risks involved and to develop strategies for dealing with these realities. A good plan can help minimize risk and magnify the upside of BYOD.
Imagine government and other web sites answering an open ended collection of English questions, and also explaining the answers in English.?? Imagine people socially networking, Wikipedia-style, to continually expand the range of questions that can be answered.,The approach starts from the observation that data by itself is necessary, but not enough, for many practical uses of an intranet or the Web.,What's also needed is knowledge about how to use the data to answer an ever increasing number of questions -- such as, "How much could the US save through energy independence?" , "Does a high level of national debt stifle economic growth?",There is?? technology online that can leverage social networking for the significant task of acquiring and curating the necessary knowledge -- in the form of Executable English.,Please see ,???????????????????????????????????????????? 
I've always been interested in data, how it's interpretted and the different ways it can be sliced. However, I've always considered statstics itself to be a math that I didn't like. As "data science" and "big data" became more popular, however, I started to look into ways to learn more about it and possibly use it as a entry into another act of my career.,The advent of MOOCs has opened up the possibility of learning new subjects and subject-focused websites like Data Science Central here make diving into new subjects more accessible than in the past. I wanted to detail the start of my journey here - in a couple of months, I'll update where I am on my path and what else I've learned along the way.,The hardest part of a learning journey is where to start. If you don't know anything, there's so much to learn that it's difficult to know what order to take things in. Google sent me down a few directions with a solid ,??page, following prolific data scientists on Twitter that I got off a Hilary Mason ,??(Hilary herself being a prolific user of Twitter) and just general hunting around.,However, again, that gave me lots of places to start but not any notion as to what the right start was. As I was reading, though, it became more and more apparant that no data science language, technique or tool was going to be useful unless I had an understanding of basic statistics. This led me back to Coursera, where I had dabbled in classes but never dove in. Picking through the catalog led me to Dr. Mine ??etinkaya-Rundel's Duke course on ,. Dr. ??etinkaya-Rundel's approach was comprehensive yet approachable. Sometimes in the college courses, the professors bury you in mathematical theory and you get so busy memorizing Greek letters that you get frustrated. The approach here was to keep the theory light by explaining it through application. I took the class with no issues and am no longer intimidated by statistics.,In the meantime, Udacity was pushing its ,??specialization and Coursera had published ,??through John Hopkins. Both had a free and paid component but the course list looked a bit tighter and better integrated on Coursera, even though I chose the free option. I also reviewed Dr. Granville's ,??here. I eventually ruled out the apprenticeship, for now, because I felt I had more learning to do.,Right now, I'm in the second of three sections in the Johns Hopkins track. Those will wrap up by July. I'm also signed up for the ,??course that may have started all this, at least in the MOOC space, from Stanford. The Johns Hopkins track has a Machine Learning course but I expect it to be a lighter treatment than the Stanford one and a good grounding in Machine Learning is necessary to play in that space as well.,I don't know where this journey will take me yet but it's been a great learning experience so far. I look forward to continuing and then maybe jumping in on the apprenticeship at some later date!
You like working with data. You?€?ve completed a few data science courses and enjoyed them. Now what?,I second??, enthusiasm for course sequences, specialization tracks, and certification offerings. Whether through traditional brick-and-mortar schools, on-line offerings, or bootcamps, the carefully-planned curricula and (depending on the program) personal instruction can be very helpful.,Here are several additional recommendations from Month 15 on the path?€?,We?€?re learning, organizations are evolving; technology is evolving.?? Exciting times!
 , , , , , ,: Yes, I know that Anametrix has a strong solution in this new paradigm of discovery analytics. A traditional dashboard ??? a business intelligence dashboard ??? is already pre-programmed to show you certain metrics, certain features in the data. These are metrics that are being tracked on a regular basis. They can be helpful, but basically you?€?ve already decided what you?€?re going to look for and find. These are likely to be in the category of known ?€?unknowns.?€? But the biggest potential in business is to find the unknown ?€?unknowns.?€? That?€?s a new paradigm in discovery analytics as you have at Anametrix. We can predefine the metrics, but you can also explore the data in many different dimensions, in multiple correlations and do discoveries that can lead to new areas of knowledge. , , , , , , Exactly. In my early days at NASA, I worked on the Hubble Space Telescope and its data archive. We were proud of that archive because scientists could make any query, not just the ones that were pre-planned and pre-programmed into the interface. That was quite noble at that time. You could scan the top 5 percent of the database, find the most interesting information and drill into it, instead of trying to do the whole all at once. In business these same capabilities enable you to go deeper and ask much more interesting questions, particularly if you can get to those questions faster. So speed is important, given competition. But so is the complexity of the data in volume and multi-dimensionality. I?€?m excited about being able to ask deeper and deeper questions. The computational capacity is hundreds of times faster today than even 10 years ago. But the volume of data also is growing at the same rate as the computational power. We?€?re always going to need these , incremental queries to guide us through the avalanche of data we?€?re looking at. , , , , , , , , The three I came up with may not be the ones a business manager would, but here?€?s my take and what I?€?m hearing from a lot of sources. The first is , where you are basically looking at ways of improving business decisions. One of the values of analytics applied to big data is that you can learn the decision rules. This is almost like a decision tree model or a business rule model. The model identifies decision points within your data stream. What kinds of actions work best? What kinds of actions are needed under certain circumstances? Let?€?s say for example, your BI dashboard lets you establish an action to be taken when a metric exceeds a certain threshold. The business process execution can then become more and more automated through the application of the big data analytics. , , The second area where business can benefit is , ,, the ability to actually discover and engage customers across many different channels. So it?€?s no longer just the vertical channel. A customer visits my website, and I can track clicks, and then they leave, and that?€?s all I know about that person. With cross-channel analytics and discovery, you can identify your customer when they visit other sites, either based on some kind of web beacon or user name identity. So I?€?m a good example. I log into different sites using my Facebook and Twitter accounts. You get to that place where it asks for a user name, password or log in using your social media. So once you do that, then the analytics tools recognize it?€?s you on those different channels. So cross-channel customer engagement becomes a reality. It builds customer loyalty and actually enables you to discover new customers. , , The third one is ,. That?€?s a broad term but useful to talk about in specific contexts. One of those is in human resources or talent acquisition and retention. So businesses are looking more and more at the application of multiple data sources to identify top talent and top performers in their company and how to determine the best ways to retain those people. But there are other organizational functions that can benefit from using data, including risk analysis, fraud detection or business optimization. So in these ways the business can use data to improve efficiency, performance and the bottom line. , , 
There really is no set amount of data that prompts the need for data warehousing. Some may become overwhelmed by the volume of data their company is producing, while other may become overwhelmed by the amount of data sources they need to integrate. To decide if you?€?re close to needing a warehouse, you can start by first gathering together your different data sources.,??,What really indicates you need some form of streamlined solution is when you must pull from these multiple data sources every time you need to answer basic business questions. To determine the ROI of your advertising, for instance, you may need to turn to several different spreadsheets and databases. You would then refer to yet another set of spreadsheets and databases to determine if your customers?€? lifetime value has been affected in any way by your service initiatives.,??,If you?€?re at the point where such reports and analytics are not being run due to the complexity and time involved in cross referencing different sources, it?€?s probably time to start seriously considering data warehousing. Here are five other signs that you might need a consolidated solution.??,If you need to gather data from various sources, only to combine it into one spreadsheet for consumption by others, you?€?re probably at the end of your rope. In this scenario, you?€?re essentially creating manual data reports. That?€?s fine on a small scale, but once creating such reports is getting in the way of actually acting on the findings, you need to figure out a better solution.,Even after all that information is in a central spreadsheet, you?€?ll still have to refer back to all your original data sources to provide accurate updates. Have you ever found yourself estimating or even rounding up in order to avoid tracking down another database to get the real statistics?,The desire to estimate or cut corners could result in conflicting reports from various departments. Sometimes those reporting are just so overwhelmed with the process they don?€?t even realize they?€?ve entered incorrect information. Even worse, these departments could have their own resources for data that others don?€?t access. How can you make sure everyone?€?s using the same numbers at all times?,It is possible to overwhelm your programs with too much data. If you get to the point where no amount of freezing panes, pivot tables, charts, and pages will help you locate your data even within an Excel spreadsheet, it?€?s time to consider another solution.,Without a central data repository that?€?s automatically updated, you could spend more time waiting on access to data or data from coworkers, than actually using that information. If you?€?re a manager, such inefficiencies will not only slow down your ability to monitor your team, but the team?€?s ability to work under tight deadlines.,Once you implement a ,, you can eliminate such problems. Your new solution will allow you analyze and store enormous amounts of information without slowing down to generate your reports. Even better, your security will be top notch because you won?€?t need to spread your encryption efforts over several different programs.,??,So, what do you think? Is it time to implement data warehousing with your organization?
Here I am talking about small countries with 10 million or less inhabitants, with good universities and high standard of living: places such as Switzerland, Singapore, Ireland, Belgium, Greece, Netherlands etc.,At least, some people start to notice that a number of data science thought leaders are coming from small countries, and feel very encouraged by this. For instance, when 'small country' people learn that I also come from a small country, they get all excited and optimistic, and start doing things that they otherwise would not do, such as creating a very popular blog.??We should also create an organization such as ,. Together we would be bigger., In some countries (South part of Belgium where I come from), there is a very strong anti-entrepreneur spirit, and any new business is taxed heavily; doing business is considered evil (all entrepreneurs are leaving as soon as they can).,Changing this mentality might be impossible and a lost battle, but you have several options:
Above is a distribution of price differentials for the Dow Jones Industrial Average from the 1930s. The image was generated by one of my programs called Storm. I posted a few images from the same application in other blogs. If I recall correctly, the more volatile differentials (closer to the action) are at top; the more stable differentials (further from the action) are along the bottom. I describe this distribution scheme as a "hyperspatial image." The idea behind hyperspace according to my childhood exposure to several arcade games is to leap across distances instantly. It occurred to me one day to distribute a huge number of differential combinations across a thematic lattice, resulting in the rather surprising image above. The image is fairly persistent regardless of the type of data. On Storm, the image can be played like a view clip; so it is possible to see the migration of events across these mathematical rings. I believe that most people would say that the pattern hardly resembles the Dow Jones Industrial Average at all. Perhaps many would as I do suggest that it looks rather like the surface of water. The image has made me question the spatial nature of data and the issue of geography in data. In this blog, I will discuss how geography exists through conceptual delineations in data. I will then describe how a type of data-oriented geography might be used to map out the internal logic of certain phenomena.,From a coding standpoint, I find it difficult to distinguish geographic from any other type of data. I monitor the outcomes of my commuting decisions using a computer application. It is true that the routes contain geographic details; however on a coding level, there is no geography. Computer simulations routinely cause people to interact with geographic terrain that is purely fictional. Yet perhaps few would say that the terrain is any less geographic for existing only in a simulation. Moreover, the absence of "terrain" is not enough to prevent data from being described and handled in geographic terms. For instance, patrol reports from different police divisions might contain data characterized geographically but existing only in tables that can later be portrayed on a map. However, the data is no less geographic if it is never portrayed on a map. I suggest that the geography is not specifically about the terrain itself or even any data about the terrain; but rather, it is an aspect of how humans interact with the data. The fact that the data involves terrain is not necessarily important. There doesn't have to be any geography (in a material sense) in order for data to be interpreted geographically (in a logical sense); this distinction raises the question of what it means for something to be "geographic." I offer an interpretation where geography is mostly about the organization of data.,Consider the problem of schoolyard bullying. For to me, this is an issue that is geographically or at least spatially relevant. The same can be said in regards to incidents of sexual assault on campus. However, data regarding these types of incidents seem suspiciously deprived of any geographic context. Geography is arguably not an issue in relation to bullying given that online bullying is so common; and of course the online world is notable for its lack of anything resembling geography. The assertion is perhaps harder to make in relation to sexual assault given how easy it is to find secluded areas of poor lighting where predators might hide; still, I believe that many would say, sexual assault can likewise occur online without any apparent geography or spatiality. Rather than argue whether or not online events can be characterized geographically given lack of physicality, instead, I challenge the premise that geography must always be tangible or material. People certainly encounter delineations online. Homogeneity is difficult to find. Online, people are separated by language, their interests, social objectives, philosophical leanings, and their professions. I find it difficult to accept that the absence of a tangible world renders such delineations non-geographic.,I was born on a complex arrangement of tropical islands containing three distinctive features: land, water, and volcanoes. I recall an old photograph of me near a small enclosure I believe designed to smoke fish; the water stood just a short distance away. I don?€?t remember posing for the photograph. But I recall the volcano at the other side of the water. Many might describe geography as my placement in relation to the land. But consider instead the placement of the land in relation to my perception of myself. My recollection of the scene is indeed geographic; however, the images and feelings are entirely from "data" - if memories can be described in this manner. This is not just any data like the length of car: it is embodied data conveyed though lived experience. Perceptual context can be connected to massive amounts of data. The big data of the discursive is the biggest data of all; there are no boundaries either to discourse or self-placement. The geography of data as I envision it is about self-placement in the expanse of data; more specifically, it is about the relevance of how data is arranged or structured in relation to the person or organization.,Self-placement in data is an important concept. I have a couple of illustrations to help with the conceptualization. The first image is called "ME Attribution," which stands for mind-environment attribution. When we have a piece of data, it is often worthwhile to ascertain whether it is connected with the mind (M), environment (E), or some aspect of both (ME). The more environmental the data, the easier it is connect the data to physical landmarks. For instance, a phone-booth, lamp-post, and birch tree are objects that are usually physically connected to the material world; it is therefore possible to create a map showing the geographic distribution. My perceptions of the same items can be found my mind. The geography that these items occupy exists within me instead of the material reality. In fact, I assert that such things would have no place within me were it not for the geography already existing inside.,The second illustration is called "BME Attribution" for body-mind-environment attribution. In my ME example, I was describing the placement of data at polar opposites: the psychological versus the material. Data can often be connected to the body (B) or the physical. Given that the mind and body are so close together, and since one occupies the other, at times it might not be possible to make clear separations. Injury to the body frequently leads to mental distress. Mental strain can erode the body. Since the body occupies an environment and the mind lives in the body, it makes sense for the mind to feel stress upon the destruction of a habitat for which its body is optimized to occupy. The placement of a person in unfamiliar physical environments such as a busy office or noisy factory might contribute to stress. Similarly, I suggest that self-placement in a confusing and hostile data environment can also cause stress. (Self-placement in this sense does not mean that the person takes occupancy; but rather he or she finds the self placed in the environment.) The geography of data serves not just to provide access to data but distance from it; make it possible to place data in front and behind other data; create meandering routes and straight paths. Body, mind, and environment represent reference points to help us understand the placement of the person in relation to the data.,Sometimes a problem might be attributed to the body while it is actually environmental in nature. Consider for instance the concept of a revenue stream: increases in revenue tend to be associated with growth and expansion. However, such revenue streams might not reflect prudent decision-making specifically. The financial data could merely indicate that there is market growth: a company that operates during good times would tend to have good numbers. Revenue data offering metrics of the market can instead be interpreted as metrics of the market participants. Conversely, data pointing to poor performance might reflect the market more than the participants that occupy it. Consider for example the plight of a technician that services air-conditioning systems. I sometimes give HVAC examples in my blogs perhaps due to some vocational training as an HVAC technician. The HVAC technician scorns warm winters and cool summers. It?€?s not due to lack of skill that prevents him or her from putting food on the table but rather moderate temperatures.,I believe that studying the geography of data has some useful applications. It is possible to "map out" the geography of non-spatial environments. The mapping of non-spatial spaces can be achieved through a process that I call "tracing." When events are grouped together as a class, interesting insights can be gained such as the types of future events that might be thrown in order to detect particular aspects phenomena. I introduced the ME and BME attribution models not just to give an explanation of placement but also how events to detect phenomena can be distributed. For instance, taking a pill in order to become healthier is a concept entirely focused on the body. After some period of use, a person might discover that food supplements have few positive or negative impacts on health. However, exercise could prove to be much more relevant than food supplements. Exercise is "less" body-oriented than taking pills since it requires space outside the body to perform: e.g. jogging, swimming, and even weight lifting. Classes can be filled with events to delineate divisions within exercise; and perhaps at some point, it would become apparent how some forms of exercise are better than others. So there can be a perpetual process of both of expansion and precision. I?€?m uncertain if a term already exists to describe this pattern of discovery. For the sake of argument, consider "dendritic epistemology." Once again I am thwarted by a college dictionary short on scientific terms. Anyways, the roots of a tree expand not through random firings but rather progressive extension. Perhaps a biologist or botanist can pass along the term to me.,So the geography of data is about self-placement within data, which means finding the relevance of events in relation to the person or organization. I actually have a more formal term for this type of geography. I call it "Transpositional Geography." This geography can be "reconstructive" as in the case of computer simulations. I know that geography can also be "deconstructive," and I don?€?t dispute how I seem to be describing an approach that could be used deconstructively. As far as data science goes, I think the market is in reconstruction - helping organizations "realign." A spatial scene can be reconstructed from data assuming that the data is suitably configured; for me, the same approach can be applied in relation to scenes that are non-spatial. For me, Transpositional Geography is focused on the data as an area of study in itself particularly for the purpose of mapping non-spatial events. This should be a topic of some interest to data scientists that regard the internet, cloud systems, and networks as important sources of data. I would argue that the evolution of the internet has been about finding and placing self in the limitless expanse. Many of us experience a sense of belonging to things in the material world; in the absence of physicality, for instance on a website for an online retailer, the idea of belonging should be a concern given how easily people can come and go as they please.,Data does not have to be interpreted geographically - that is to say, transpositionally ?€? or, dare I say transpositively. In fact, some data might leave little room for self; but then nobody would care about it. I remember during Sunday service one day being fascinated by the layout, signs, and symbols. Although this was a service involving physical bodies in a place of worship, the scene and setting seemed intended to trigger much deeper connections. The non-spatial component of the service could take me places beyond the confines of physicality. The service leader helped people organize the placement of things inside themselves. He brought some objects closer while taking other objects farther away; put some items in front and others at the back; he forced people to retrace their steps. ??So the ceremonial discourse seemed highly geographic in nature; everything seemed to matter much more.,Now, I was inside something like a mechanical warrior on an alien planet. This was a simulation of course. It had nothing to do with Sunday service by the way. There was enormous landscape all around me and big sky high above. Mechanical warrior planes were flying overhead and shooting. They were on my radar. I was shooting back although not too well. It became apparent to me that space was not behaving properly on this planet. I could reach destinations rather quickly. Space appeared a bit warped; or my ability to move through it was accelerated. The scenery was present to add realism; but since my main objective was to engage in combat rather than move through scenery, the scenery allowed for abnormally rapid passage. I therefore argue that the existence of spatiality in this simulation was merely to support transposition. I further contend that when people surround themselves with facts and figures, they do so not merely for utility but also transposition.,I would expect Transpositional Geography to be useful in relation to investments. There is often a need to map out the internal logic of the market. In the day-to-day price fluctuations of a particular stock, information about the underlying company tends to be released relatively infrequently. It is therefore reasonable to assume that the dynamics outside the body (B-company) that can be responsible for sifts in valuation. Consequently, spending a great deal of time and effort on the internal events of the company, for instance focusing on the valuation of assets and revenue streams, is problematic given the influence of the environment. At the same time, an entirely technical approach making use of market trading data seems insulated from information about reality. I would suggest that a more geographic strategy considers the issue of market self-placement: this is the setting in which information about the company comes to light and also details about competing needs for capital. I find it simplistic to interpret pricing strictly in relation to the "investment" and never the "investor" (MBE-market). Sometimes investors sell not because of the investment but rather the need for funds, perceived risks to themselves, and lack of connection to "the dream." Nonetheless, part of the joy of a geographic approach is to survey the internal logic of something in a literal sense rather than having somebody like me defining it. "They are selling because they think they are made of blue cheese." Ah, that assertion seems inconsistent with the non-spatial delineation of relevance.,Another useful application that comes to mind would be in situations of sick-building syndrome. In some of these cases, employees might be blamed for poor performance and lack or attention to detail when in fact the ventilation might be poor, or parts of the building are toxic. I encountered a similar problem in my own research around the time when computers were becoming prevalent in office environments. Almost all metrics of performance were criteria-based, as they still are. In such a paradigm, it would seem that improving performance would simply be a matter of raising the bar and adjusting behaviours. However, I found some evidence showing that about half of insurance disability claims were related to musculoskeletal and circulatory problems; these indicate BE, BME, ME, and E factors. Therefore, one would think that attitudinal metrics might be less relevant than environmental considerations. I am not dismissing the practice of providing psychological counselling and support except that it might sometimes be completely unrelated to the underlying problem and therefore unlikely to lead to improvements in performance. Stated differently, in some cases it might be a waste of time and money. Certainly, providing counselling but not protective equipment is rather negligent in relation to asbestos and exposure to radiation. It is important to delineate data in a transpositional manner rather than expect it to satisfy professional preconceptions.,Transpositional Geography can help us understand the relevance of things to an organization. We can study the data present in a department for indications of perceived value. It is also possible to assess the relevance of the data to actual contexts for instance using a tracing technique. I feel it is possible to "explore" data through tracing in order to thoughtfully arrive at relevant events and contexts. This is not to say that the data collected would necessarily be "small." However, finding worthwhile data would be a much more deliberate process with coherent objectives compared to something like data-mining. Transposition allows for the mapping of "local" perspectives. It is not an instrument that imposes meaning over the data; but rather it is a method of exploration. By local, I mean that transposition supports local sensitivity, the decentralization of information, and processes involved in the articulation of outcomes and consequences. In my blogs, I discussed the topic of "data embodiment." The geography of data is about self-placement and the organization of data around the self, which as I said might have nothing to do with the material world. The self can exist online and in the cloud. While people still migrate across oceans and over landscapes, these days they move effortlessly through seas of electronic data; venture over convective gales of information; not lost but aware of their own placement and the things important to them.
Interested article published by , in I09. I would add credit card transaction scoring to detect fraud, and scoring in general such as FICO or Experian scores. What would you add to the list below?,There was a time not too long ago when search engines battled it out for Internet supremacy. But along came Google and its innovative PageRank algorithm.,Today,,, followed by Microsoft (18.1%), Yahoo (11.2%), Ask (2.6%), and AOL (1.4%). Google now dominates the market to the point where we don't even question it anymore; for many of us, it's our primary avenue of entry into the Internet.,PageRank works in conjunction with automated programs called spiders or crawlers, and a large index of keywords and their locations. The algorithm works by evaluating the number and quality of links to a page to get a rough estimate of how important the website is. The basic idea is that the more important or worthwhile websites are likely to receive more links from other websites. It's basically a popularity contest. In addition to this, the PageRank algorithm considers the frequency and location of keywords within a web page and how long the web page has existed.,As much as we may be loathe to admit it, the Facebook News Feed is where many of us love to waste our time. And unless your preferences are set show,the activities and updates of,your friends in chronological order, you're viewing a pre-determined selection of items that Facebook's algorithms have chosen just for you.,To calculate which content is most interesting, it considers several factors, such as the number of comments, who posted the story (yes, there's an internal ranking of "popular" people and those with whom you interact with the most), and what type of post it is (e.g. photo, video, status, update, etc.).

The descriptions below are from Wikipedia.,is a,designed to address the requirements of high-performance numerical and,while also being effective for general purpose programming.,Unusual aspects of Julia's design include having a type system with,in a fully,and adopting,as its core,. It allows for,and,; and,of,and,libraries without a compiler without glue code and includes best-of-breed libraries for,,,,,,,, and,matching.,Julia's core is implemented in,and,, its parser in,, and the,compiler framework is used for,generation of machine code. The standard library is implemented in Julia itself, using the,library for efficient,,. The most notable aspect of Julia's implementation is its speed, which is often within a factor of two of fully optimized C code.,Development of Julia began in 2009 and an,version was publicized in February 2012.,(,) is an,and,for general,. Scala has full support for,(including,,,,,,,,,,,, etc.) and a very strong,. This allows programs written in Scala to be very concise and thus smaller in size than most,. Many of Scala's design decisions were inspired by,over the shortcomings of,.,Scala source code is intended to be compiled to,, so that the resulting executable code runs on a,. Java libraries may be used directly in Scala code, and vice versa. Like Java, Scala is,and,, and uses a curly-brace syntax reminiscent of the,. Unlike Java, Scala has many features of,languages like,,,and,, including,,,,,, and,. Scala also has extensive language and library support for avoiding,,,,,,,, and,. Scala has a "unified type system", meaning that all types (including primitive types like,and,) are subclasses of the type,. This is similar to,but unlike Java. Scala likewise has other features present in C# but not Java, including,,,, optional parameters,,,,, and no,.,The name Scala is a,of "scalable" and "language", signifying that it is designed to grow with the demands of its users.,(,) is a general-purpose,,,and,system. The sequential subset of Erlang is a,, with,,,, and,. It was designed by,to support distributed,,,,, non-stop applications. It supports,, so that code can be changed without stopping a system.,While,require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using,instead of shared variables, which removes the need for explicit,(a locking scheme is still used internally by the,).,The first version was developed by Joe Armstrong in 1986.,It was originally a proprietary language within Ericsson, but was released as,in 1998.
Interesting article posted on NoSQL-database.org, listing 150 databases. Here are some highlights.
This list is a bit old (I think 2011), but it features a bunch of very interesting people, true data scientists who can't afford wasting their time to post on Twitter - unlike other similar lists published by journalists.
 
There isn?€?t any specific standard to model data warehouse. It can be built either using the ?€?dimensional?€? model or the ?€?normalised?€? model methodologies. Normalised model normalises the data into third normal form (3NF) whereas dimensional model collects the transactional data in the form of facts and dimensions. Normalised model is easy to use as we can add related topics without affecting the existing data. But one must have good knowledge of how data is associated before performing specific query, so it might be difficult to handle. Reporting queries may not execute as well because massive numbers of tables may be involved in each query. Dimensional model is very efficient to use for non experts and performs pretty well as data is classified in a logical way and similar types of data are stored together. But while adding new topics whole warehouse must be reprocessed (Jones and Johnson, 2010).,Dimensional model is designed to optimise decision support query function in relational databases, where as normalised model is designed to eliminate redundancy problem of the data model, retrieving data which contains identifiers and therefore optimise online transaction processing (OLTP) performance (Firestone, 1998). Therefore it can be said dimensional model is the best modelling method in data warehousing.??,OLAP stores data in arrays which are the logical presentation of business dimensions. Multidimensional array represents intelligence of data elements relationships because analyst?€?s views depend on it. This multidimensional array data model is called ?€?Data Cube?€? (Kirkgiize ,, 1997). It consists of facts and dimensions instead of rows and columns as in relational data model. Facts are the accurate or numeric data of business activity and dimensions are the sets of attributes that put facts into context (Wang, Chen, Chiu, 2005). Dimensions are interconnected in hierarchies, for example, city, state, region, country and continent. Figure 1 shows three dimensional views data cube of sales data (Kirkgiize ,, 1997).??,In data cube each cell contains one or several values called measures or metrics. It helps to analyse aggregated facts and the level of detail is directly proportional to number of dimensions in the cube. Here axes represents dimension and the space represents facts (Wang ,, 2005). Above shown figure of data cube contains three dimensions namely Time, Geography and Product. Each cell consists of (T, P, G) and business measure is the total sales. Here the amount of product P along with total sales sold in geography G in time period T is stored. Figure 2 shows hierarchy of dimensions.??,The user can generate the exact view of the data required with the help of data cube. This process is called slice and dice because the data is evaluated according to a subset alignment. The subset can also be rotated and figured as a parent one. For example, to find out the sales in a particular place in the specific time, slice and dice operation can be operated in the cube. This will help to choose arrangement along each dimension, it may be time, geography or product according to user needs (Kirkgiize ,, 1997).,Data cube may contain hierarchical dimensions. This hierarchical dimension provides data manipulation and detailed analysis for various levels of the dimensions. Moving up and down in a hierarchy is known as roll up and drill-down respectively. After choosing the level of dimension, it can be sliced and diced. According to user need, they can drill-down and roll-up to view the data either in region wise, in product wise or in city wise level (Kirkgiize ,, 1997).,References:,1)??, ,2) Jones, J. and Johnson, E. (2010). Beyond the data model: designing the data warehouse. CA Erwin, pp. 1-9. [Online] Available from: ,3)??, , , , , 
The??,??is always published Monday.??Starred articles are new additions, posted between Thursday and Sunday,These profiles are randomly selected among our active members. To be selected, you need to have an interesting profile with picture; it helps to post a blog, or share / comment / like contributions from other members.

It is the most difficult to digest and comprehend book to date out of all I have recently read. At the same time it was pure fun.,Why so hard, I guess I need to blame myself because this book unexpectedly turned out to be more from the Academia world where my skills in Algebra and Statistics faded out over time than from the practical world where I spend my productive time. At the same time it was pleasant to have a feeling of being a student again.,Nevertheless, the book offers a ton of insight, and how-to?€?s for the in ?€?the trenches?€? practitioners. This book is full of external reference and facts, it sure took a while for the authors to assemble it.,From my observations, the knowledge of the R language is necessary before starting reading this book, sadly, even if a program code is provided in the book there is no sample output.,The book is written so it has chapters by guest authors, this makes sense as a data project is rarely comprised of one kind of a professional, this nuance is also covered in the book by the way.,These guest authors are top notch professionals that would write a complete book on their own subject matter of expertise. But because they are the ?€?top guns?€? in their corresponding field each managed to cover a lot of grounds just within a dedicated single chapter.,So, in short, the best thing about this book is that in one single investment you get a comprehensive coverage for life on what approach or algorithm to use against a given data science task at hand. You must feel more secure after reading this book and as a result be more eager and ready to embark on any data science project.,This book can also be used to expand your knowledge further, in many directions.,Five out of five stars.,Disclaimer: I received this book for free as part of , program.
 , , , , 

Data Science Central launched this week the ,, a public online resource for practitioners to read, download or publish high quality papers. If your article is accepted, you are entitled to a signed, free copy of ,, may have your profile featured on DSC on the ,, and be featured in the , section in our ,.,The Research Center is the place to post research papers, preprints and non commercial white papers discussing data science / big data new techniques, principles and methodology. We invite both academics, post-doc and PhD candidates as well as scientists from corporate, private or government research labs, to post their papers, to,Currently, the 11 articles listed are internal to our ,. Also, all articles published in the Research Center are part of our ,??and may be distributed on partner websites, thus increasing your reach. To become a partner and use our feeds, ,??or contact us at vincentg@datasciencecentral.com. There is no cost.,??to add your article to the Research Center, and use the , button. All articles are subject to (quick) review process.
With loads of content and hype about Data Science, Analytics and Big Data coming up every day, I felt compelled to share my journey and experiences in exploring this space. Here, I intend to begin a series of posts on the subject stripped it down to it's core (as I understand it) and than building upon it.,While there are a whole bunch of directions these subjects could be approached from, I present here the path that I found most convenient to tread. I would like to call it the path of the 'Data Shinobi' (another word for the 'Ninja') where you understand and gain expertise in the techniques just enough and relevant to the task at hand unlike a seasoned 'Data Scientist' who has mastered most of these techniques over many years. It must also be noted that when I mention the 'Shinobi' it's in the context of the training that shinobi's underwent rather than their expertise and mastery over skills.??,?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ??,??,??- In most cases, you have an amazing approach and models to get your solution up and running. However, in most cases you have to wait for what feels like ages to get your data ready. Usually you could spend about 70-90% of your time reshaping your data to be processed.,??,??- 'A picture is worth a 1000 words' still works here. So, building a great model or rebuilding the data architecture to speed up analytics needs to be 1000 times more amazing if it isn't backed up by a stunning visualization. I believe visualizations need more effort from the other half of the brain and sometimes people who have no idea about quantitative modeling could suggest or build really beautiful visualizations that make you think "It makes perfect when you look at it like that..",??,One, and probably the older, is??,??that has evolved from traditional statistics and works on error minimization through generalization techniques. The other is??,??which is probably the way a Computer scientist would approach a problem. Machine learning techniques utilize error minimization by recursively training the 'machine' to make better predictions. The line between the two is getting thinner as they seem to borrow some concepts from each other time-to-time.,??,Traditional buckets under which techniques or approaches were classified were - Regression, Classification, Clustering and so on...(this is almost always the 1st chapter in most books on Data Science). However, some techniques such as LDA for Text Analytics might lie outside these buckets. Similarly SVD and ALS used for recommenders are quite different techniques.,??,Even though you master some techniques and have a good grip on R/ SAS, there are always new challenges you face which makes Data Science interesting. Additionally, there might be a complete field where a domain expert might know a lot more than you do. For Example, Supply Chain experts understand a lot about optimization and forecasting techniques. It's hard for a Data Scientist without Supply Chain techniques knowledge to apply Stat learning techniques to problems in this field. In the end continuous improvement (or 'Kaizen') is what would work well here as well.,??
Many of those who call themselves statisticians just won't admit that data science heavily relies on and uses (heretical, rule-breaking) statistical science, or they don't recognize the true statistical nature of these data science techniques (some are 15-year old), or are opposed to the modernization of their statistical arsenal. They already missed the train when machine learning became a popular discipline (also heavily based on statistics) more than 15 years ago. Now machine learning professionals, who are statistical practitioners working on problems such as clustering, far outnumber statisticians.,Many times, I have interacted with statisticians who think that anyone not calling himself statistician, knows nothing or little about statistics; see my recent bio ,, or visit the LinkedIn profiles of many data scientists, to debunk this myth.,Any statistical technique that is not in their old books are considered heretical at best, or non-statistic at worst, or most of the time, not understood. New statistics or??,??textbooks are published??,??but with the exact same technical content: KNN clustering, logistic regression, naive Bayes, decision and boosted trees, SVM, Bayesian statistics, centroid clustering, linear discrimination - as in the early eighties, applied to tiny data such as ,. Rarely do they include anything new (that is, less than 10 years old) such as ensemble methods, Lasso and ridge regression or Bayesian networks. Some new statistics textbooks include analyses of small Twitter data using R or sometimes Python, or talk about association rules or recommendation engines, but are still far away from real applied statistical data science.,If you compare traffic statistics (Alexa rank) from top traditional statistics websites, with data science websites, the contrast is surprising. In the table below, the lower the rank, the higher the traffic volume, with a rank of 50,000 getting about twice as much traffic as a rank of 100,000:,Popular statistics websites:,Popular data science websites,Google ,??tells the same story.,These numbers are based on Alexa rankings, which are notoriously inaccurate, though over time, they have improved their statistical science to measure and filter Internet traffic, and the numbers that I quote here have been stable recently, showing the same trend for months, and subject to a small 30% error rate (compared to 100% error rate a few years ago, based on comparing Alexa variances over time for multiple websites that we own and for which we know exact traffic stats after filtering out robots). These numbers are for US traffic only, which represents between 25% to 55% of the traffic on these websites, but the US traffic is the only one that can be easily and well monetized - thus representing 90% of all business value. TrafficEstimate.com is another free website that you can use to compare traffic statistics, and then there are third party vendors such as compete.com that provide additional information for a very expensive fee.,Despite being a dissident and disruptor in the statistical community, we attract more and more statisticians (you are welcome to ,), ??as well as mathematicians (many in the defense or financial industry), or operations research practitioners, and even clinical trial experts. Many people don't really care about their job title: there are many ,. For instance, my primary job titles are CFO, owner, entrepreneur, co-founder, ??business hacker, and my secondary job titles are data scientist and consultant. Of course, I am a lean business data scientist, thus being business hacker is actually part of being a data scientist for me, as well as for many other business data scientists. It explains why we are growing much faster than competitors that have far more employees and overhead: we ??leverage unusual (heretical) applied statistical techniques such as ISP segmentation to optimize eBlasts, RSS feed optimization and automated syndication??(to optimize reach - a growth hacking technique), content mix optimization, reverse word of mouth advertising, computational marketing, mathematical optimization of spending (revenue spending, and value added to clients to increase revenue and re-invest in our business without external funding), and various ,, to boost our growth.,Modern statistical data science techniques are far more robust than traditional statistics, and designed for big data. I have implemented many over the last 15 years with success stories to tell (see section 4 below), and have reached a point where I feel confident about , data science, ,??and statistical science. I'm preparing a book on data science automation.,Here are a few of these techniques:,Still, there are a number of old-fashioned techniques and principles that are here to stay, and even experience growth, such as experimental design, sampling, or Monte Carlo simulations. Some, such as ,-value and traditional hypothesis testing ,, to be replaced by ,??that everyone can easily understand and apply.??Some ,??and need to be significantly improved to make them robust and suitable for black-box, automated algorithms. I'm working on this, for instance transforming traditional regression into what I call??,, to make it usable by non-experts.,A number of great old principles, here to stay, are ,, and include:,I would also add: always try to explain and reduce sources of variance, without over-fitting, using sound cross-validatation and sensitivity analyses. Though I even questioned the traditional definition of variance, and have proposed a few alternatives, including L1 and ,??(also see ,) to be used when scale does not matter - but considered by many statisticians as??a taboo.,My ,??methodology (an hybrid approximate, very robust constrained logistic regression blended with a few hundred small, stable decision trees, relying on??, to optimize a newly created metric called predictive power - see ,??for details) has been used to process and score billions of clicks, IP addresses (sometimes in real time) , keywords and detect some of the largest Botnets impacting the digital advertising industry. It is heavily statistical in nature, uses model-free, data-driven confidence intervals, and pretty much none of the statistical techniques described in any textbooks other than mine. It is indeed data science. Most recently, it was used to detect large-scale criminal activity hiding behind Amazon AWS (click??, and also??,??for references) and not detected by Amazon, by analyzing ad network data and data from Spamhaus, Barracuda, ProjectHoneyPot, StopForumSpam, Adometry and some other sources including social network data. Interestingly, it was not based on Amazon data, but instead, leveraged external data sources exclusively.,When I posted this fact in reaction to SimplyStatistics.org's article??,, my comment was quickly removed. It makes you wonder whether they try ro obfuscate the use of statistics in data science, or want to stick to old statistics by fear??of progress, rigid thinking (tunnel vision) ??or inability to adapt. Anyway, many of the statistical techniques that I used to turn big data into value, are described in the newly created??,, but you will never see them described nor even mentioned in traditional statistical publications (except in my ,).??

Should you use linear or logistic regression? In what contexts? There are hundreds of types of regressions. Here is an overview for data scientists and other analytic practitioners, to help you decide on what regression to use depending on your context. Many of the referenced articles are much better written (fully edited) in my ,.,Note: Jackknife regression has nothing to do with Bradley Efron's Jackknife, bootstrap and other re-sampling techniques published in 1982; indeed it has nothing to do with re-sampling techniques.,Before working on any project, ,.
Here are , that you should check out before your next data science interview. Read these documents thoroughly to get prepared, impress your interviewer, boost your chances to be hired, and get bigger paycheck if hired:
Here I described how I identified a root cause, and provided a dual solution for a problem impacting LinkedIn and its members. For your reference, I also included links to many other solutions, coming from our data science research labs, applied to a bunch of companies and business problems.,: LinkedIn email blasts deployed to group members are working less and less, many times not at all. I stopped receiving most of them two weeks ago without any action on my part - though I like them as it helps me identify ,. Posting an article on LinkedIn is becoming incredibly more difficult as popular bloggers get flagged as spammers, as a result of this email spike featuring the same postings over and over.,: LinkedIn recently started to send email digests from all groups, to all members, too frequently (or maybe they improved email delivery and these messages started to pop up like mushrooms after the rain, in Inbox rather than Spambox). In these digests, the most popular articles appear at the top. Great bloggers posting top quality content get featured over and over - the same articles from the same guys appearing in multiple blogs (because of cross-posting) week after week (because of the popularity of the postings in question). Eventually, the popular blogger gets flagged, is unable to post, and lower quality content eventually shows up as the replacement.,: If you are a popular blogger, stop posting on LinkedIn, and instead post on your website. Content posted on LinkedIn ,, and if your content no longer shows up on LinkedIn, people will eventually have to subscribe to your website - long term this is a winning strategy for you, backed by data science evidence. You can hire a few interns or create a few profiles to still continue to post on LinkedIn and comment existing posts, but your strategy should be to stop relying on LinkedIn. For LinkedIn, the obvious solution is to NOT featuring over and over the same great posts across multiple groups in these group eBlasts that feature popular articles, and limit cross-posting to 2 or 3 groups maximum. Don't do a group blast for groups with no NEW great content since last week, that's one of the main sources of same content being hammered week after week to many users. Don't rely exclusively on your "popularity score" algorithm, apply filters, instead design a content diversification algorithm as described below. This will solve all the issues.,: I suggest LinkedIn data scientists to work on this new algorithm ??asap.??In short, you need to design an algorithm that detects when an article has been over-featured, over-posted, over-promoted, and stop promoting it. It involves collecting and analyzing data from different data silos, to make sure that ??the proportion of people seing the same blog post more than 3 times a week in their Inbox, is minimized. Such an algorithm would ultimately deliver rich, varied content to all users, a win-win solution both for users and LinkedIn.
As IOT Data proliferates one of the game changing use cases which it enables is dynamic pricing. As assets get instrumented one can have usage based pricing of assets on lease. We are already seeing disruptions in pricing model in the automotive industry where sensor data which is a proxy for driving habits is fuelling usage based insurance premiums.,Flutura has been working with Utility companies which is one of the industries in Industrial Internet/IOT category undergoing fundamental shifts because of deregulation and increased instrumentation of the grid. Pricing in the utility industry is a very crucial lever and there is a lot of headroom for Utility companies to innovate on their pricing levers using data science. In this blog Flutura outlines 9 components in the pricing framework?? out of which 5 analytical constructs can be used to dig deeper into pricing models. Dig deeper into the science of pricing using big data analytics can impacts multiple Utility outcomes like Profitability, Customer churn (now that Utility is getting deregulated) and Peak grid stress (influencing customers behaviour at peak time using pricing),Based on the maturity of the market, the intended business outcome and the amount of deregulation in the distribution of energy a number of pricing models can be adopted. One could adopt TOU (Time of use) pricing which charges a premium for energy consumed at peak time. One could adopt UBP (Usage based pricing) model where based on the deviance from the median usage a customer could be charged a premium for the differential energy consumed. The construct to operationalize will have to be carefully chosen based on the factors outlined above. Some of the specific signals governing the choice of model can be the sensitivity of the neighbourhood to price changes, the sentiment of the end consumer to marginal price changes, the kind of segment the the customer belongs to and past knowledge bank of what stimuli worked for the consumer segment. Each of these are elaborated below,Price elasticity process essentially answers a simple question ?€? Is the neighbourhood responsive to price changes? Is Houston more sensitive to the$ 0.25 cent increase in unit energy pricing than Dallas? What is the % change in peak energy consumed in Palo Alto and its neighbourhood when there is a change in peak price? Do households in Austin change their energy consumption pattern in response to time of use pricing (TOU) or Usage based pricing or Location based pricing framework? Depending on the observed behaviour we can be tag that Austin as price sensitive neighbourhood using markers. Also if 2 neighbourhoods have similar characteristics, one can do what if scenario analysis to understand the potential reduction in energy consumed in that neighbourhood as a function of changed pricing,??(the 5 constructs)
In this post, we will look at the various 'Shrines' and 'Giants' on whose shoulders most modern Data Scientists stand. I am often daunted by the Job Descriptions people come up with for Data Scientists these days.??,Well, having a person to do the entire analytics stack is akin to having one person who can build a complete car from scratch. Thankfully, we are seeing a lot of sub-areas diverging under Data Science and slowly people are starting to understand that all cannot be placed in one bucket anymore.,This blog is an attempt at listing down the various areas, pioneers and some resources to delve deeper into these areas. Each of these areas has a lot of depth and takes time to even digest, leave alone master. Most of the details below, in conformity to this series, are based on my perception and my discussions with other Data Scientists.
I have a contrarian view, here are my answer to several issues??,, regarding being a start-up CEO:,Use profit sharing as funding. You must be profitable on day one, but many business models allow for that: for instance being digital publisher, if you have a niche market that you control, and are well connected with your potential advertisers (or use AdSense!), and great at growth hacking. This was and still is my case.,You'll waste months of valuable time chasing investors with no guarantee of success, while competitors like me work from day one on producing value and revenue rather than funding. The only "investor" is me, so it's not really an issue. I was born in poverty, and made all my money out of re-investing profit sharing. I came to US in 1996 ,, I was not even able to buy or rent a car to drive to work at that time, I had to ask an advance on salary from my employer - NISS. Just like football players, very few make billions of dollars, a few are quite wealthy, most are poor yet happy. Same with Start-up people; you are more likely to become relatively wealthy if you don't talk to investors and are just self-funded - but you will not become a billionnaire. If a well capitalized start-up starts competiting against you (after successfully getting big funding from a VC firm), use business hacking and sheer brain power (creativity, vision) to kill them - it can be done, really, if you are a real entrepreneur!??,I live in a house much cheaper than I can afford, I drive the cheapest car in my neighborhood though I am the wealthiest guy, but I do indulge in great restaurants and vacations, on a regular basis (I can manage my business from any place with an Internet connection - that includes a nice beach). Finally I love playing with finances - even tax optimization, actually my title is more like CFO.,Agreed.,Hire a great sales guy with proven performance, offer ownership and commissions, make him a co-founder (it will be easier for him to sell with a co-founder title, than say 'sales executive' job title) and get him to work and own other projects, so that he becomes a true partner. Of course you need to have a product that can be sold right away, otherwise no one will want to partner with you.,Agreed. I like it, if you don't, you are not an entrepreneur.,Our sales guy greatly helped with this. And I found engineers in Romania thanks to our nanny.,I find it glamourous, though I don't care about glamour. I'm in business to make money, not to generate glamour. I enjoy living in a beauftiful house in the foothills of the Cascade Mountains, I take vacations at the Four Seasons in Santa Barbara or in the Caribbean, though most recently (and despite increased revenue), I'm just happy with the local deserts, beaches and mountains if there is great food, and in particular, lot's of relatively inexpensive short vacations. That's more important (to me, at least) than glamour.??,I've been waiting 15 years for that day to happen, it never did with the start-ups that I created ,, and thanks to carefully planned diversification and vision, it is very unlikely to ever happen. In fact, I've never felt so secure since I left cubicle world for good, in 2011.,I don't recruit (actually, barely), I automate and outsouce (to vendors), instead.,I share everything openly with the entire world - I call it open intellectual property (IP) or open patents; thanks to sharing on a large scale, I get feedback from people to improve my IP, and since I dominate my niche market, no one can steal my public IP to successfully compete against me, or to sue me over IP issues.
This was the subject of a popular article on Slate, following Obama's idea to bail out people with student loan problems. Decision makers, lawyers, politicians (right or left) or journalists often make recommendations that do not make sense, that are bound to fail, because they lack analytic judgment. This case epitomizes this lack of analytic acumen, and I provide the correct solution if you want to make someone pay (other than the tax payer) for these defaults.,First, you should only penalize colleges (specific training programs or degrees) if there is a causal link to defaults - for instance, curricula that automatically leads to unemployment. But a proportion of these student loan defaults are caused by,Only when the proportion of default is unusually high, for a specific college and degree, should there be a penalty. In short, Obama needs data scientists, not lawyers to address the student debt problem, because it is very much a statistical problem, at least in the way it is currently approached.,Also, there should be no penalty for debts caused by failure to complete the degree (contrarily to the suggestion in the Slate article, suggestion based on the real fact that a non-completed degree leads to far higher risk of unemployment), except again if there is an unusual high proportion of abandonment in a specific college/degree combination. Otherwise, this might encourage students to drop out, to not repay the loan.,Also, why not penalize the lender, rather than the college? Isn't this an issue of reckless lending, after all? And anyway, a school can increase tuition fees to cover costs of litigation and penalties resulting from this new proposed law. These are all issues that any data scientist would have thought about, even before collecting any data or doing any experiment.,For the future college student, a solution might be to look at programs offered outside traditional colleges, to reduce costs, such as (for data scientists) Coursera, or the ,. Some university programs offer great value despite very high costs, ,.,??,...
A few semesters into my undergraduate studies, I decided to move away from campus in order to escape the incessant party atmosphere. I chose to rent an old but roomy apartment in Kitchener, Ontario. I read somewhere that this city was formerly called New Munich. One day in the dead of morning, I heard loud banging and smashing downstairs. It seemed that neither I nor my cat could sleep, so I went to investigate. I saw the owner of the building at the front entrance of the apartment holding a bat and blocking the doors. I heard him say, "I'm not looking for any trouble. I'm just trying to run a business." Hiding behind him was a female tenant of the building, apparently the target of threatening comments. Slowly from the shadows, there emerged four young men of remarkably similar appearance - attire, physique, and cleanliness. These rather well-groomed men weren't hurling sexual comments at the lady but rather ethnic. I heard one of the young men say that the lady certainly looks like a Jew. I was in my pyjamas and slippers, and I seemed to be the only non-white person downstairs. At the time it seemed perfectly normal that I ask these men what they expected to accomplish by smashing garbage containers and breaking bottles in the early morning. I said that it takes a great deal of effort to create such a disturbance; and, regardless of their sentiments towards the lady, such harassment seems to burden them more than anybody else. One of the young men, apparently the leader, thought over this comment. He admitted that he agreed with me. The group went their way although with some additional smashing as they left. I thought that these men were probably Nazis although not necessarily by political membership. Now many years later, I share this experience as an entry point to discuss certain pathological aspects of criteria-driven metrics. By pathological, I mean that the use of metrics - the methodology and resulting data - seems structurally deficient.,I routinely come across articles emphasizing the importance of examining causality rather than association. The usual criticism is as follows: perhaps the most common mistake a researcher can make is to confuse association with causality. I think this criticism is important - although perhaps more as an academic concept than actionable insight. In real life, I suppose that something can contribute to or cause another thing; but the likelihood of nothing else being causally relevant seems remote. For instance, what causes children to be intelligent? If we could clearly define the meaning of intelligence - and I doubt that we can - the issue might be examined purely in terms of demographics. Belonging to a certain ethnic background might be said to contribute to greater intelligence. However, intelligence is a complicated issue. A person that succeeds in one particular environment (such the world of finance) might do miserably in another setting (such as academics). Intelligence is likely related to access to education. There are also broader social contexts: for example, discrimination can make some individuals seem intellectually weak although there might simply be systemic barriers at play. Causalities derived from outside a laboratory are debatable; indeed, even the causalities discovered from within a laboratory might persist poorly under more realistic conditions. Yet data is accumulated often with the intent of establishing causality; or its inference might be necessary in order to make use of the data in the intended manner. I suggest that in many routine situations, data can become insulated from reality; this occurs as complex and dynamic causalities are gradually replaced by simple and static assertions. Alienation can become entrenched or systematized in data systems, which then furthers its spread. We lose the ability to detect the imposition of disabling environments.,I want to develop my arguments further by continuing to discuss Nazis, which to me seem like interesting agents of social construction. I refer to Nazis found both in science fiction and in history. Many of us have been exposed to Nazis informally through stories - such as my own at the opening of this blog. Maybe some readers have never heard of Nazis while others have encountered the more militant versions in real life. The National Socialist Party of Germany was led by Adolf Hitler. I suspect that the term "Nazi" is shortened from "National Socialist" (Nationalsozialistische). ??The relevance of the term in films and books is more extraordinary than its origins. I find in the movement a tremendous emphasis on central control and prescriptive criteria; search for social conformance and adherence to rules; preoccupation with the separation of superior from inferior; and belief in movement towards of perfection. For Nazis, causality seems to extend from their perceptions and preconceptions of reality. This is precisely my point in regards to causality more generally. More often than not, except in the most controlled laboratory environments and simplest dynamics, causality is the product of social construction. Why would this be an important concern in terms of data? Both the data system and the data it contains can be regarded as instruments of inculcation and promulgation - to control the expansion of knowledge and therefore power in society. In an organizational setting, the data can serve to commandeer structural capital and impair decision-making.,I was not entirely surprised to discover in the Clone Wars that Darth Vader's storm troopers are actually clones. While I am no expert in genetics, I believe that I can make an assertion about clones that most people would readily accept: ,. Imagine the benefits of having a "one size fits all" situation: the same uniform can fit everybody; all of the chairs and tables can be the same size; there only has to be a single educational system since everybody learns exactly the same way and at the same pace. The storm troopers in real life - in Nazi Germany - were not actually clones although many themes seem consistent. We find a move towards ideal fit or minimal differentiation. Nazis regarded in more conceptual terms would likely believe in improved productivity through greater uniformity and compliance. They would tend to promote viral behavioural normatives for the purpose of achieving specific outcome. I know there is a perception that Nazis lost the war; but I feel that they persist in the vernacular through management practices and quasi-intellectual organizational theories. It is difficult to ignore the sameness prevalent in many organizations and the viral nature of their conformity. Their data systems are so similar as to be purchased off the shelf, indicating a comparable if not uniform treatment of data. The data gathered tends to bind a company to its chosen identity, which upon closer analysis reveals many similarities to other players in the market. I suggest therefore that it is not so much self-identity that controls an organization. The data system is evidence of incursion by an external power. The causalities extracted serve as cogs to support and reinforce prescriptive decision-making. What is this external power? It must be similar to gravity. It draws people near. It causes them to fall, slip, and trip. It is theocratic more than scientific - a belief in "progress." For there to be progress, there must also be direction.,In environmental studies, the idea of sameness can be found in monocultures. A monoculture might be a species of plant found to have superior properties; so rather than allow a natural diversity to exist, large fields might be deliberately set aside for a limited number of varieties. There might be increased yield on one hand; on the downside, any weakness affects the entire population. For example, a particular type of wheat might be susceptible to rotting. If the wheat planted all come from the same strain, the entire population can be wiped out by a single disease. Sameness can be found in organizations among decision-makers. A colourful expression from a human resources instructor that I once had is "corporate inbreeds." Executives might hire people of similar persuasion not necessarily because it is in the best interest of the company but rather for the sake of organizational congruence - the idea being that sameness is superior. In the recruitment of workers, "similar-to-me" hiring seems to occur: in this case, the selection process might favour people that are similar to the person doing the selecting. So although a large amount of data might be compiled during screening, the selection process might nonetheless disassociate an organization from its underlying needs. I have spent a number of blogs exploring differences in data: 1) the metrics of criteria resulting in data to ensure that operations conform; and 2) the metrics of phenomena helping to highlight the realities that face an organization. In an organizational setting, the need to control is supported by the metrics of criteria; and the need to understand by the metrics of phenomena.,Sameness is a rather "industrial" concept that helped us to transform and modernize production; to usher in an age of progress. I offer an analogy. One of the important developments of the industrial revolution was the introduction of standardized machinery in production environments; this created a need for standard parts that could then be replaced in the event of failure. Sameness enables replacement. It is important to recognize how this ideal can influence our perceptions of causality. Janice's inability to stay at her seat "caused" the office to fall behind schedule. The office's poor ventilation caused Janice to leave her seat for fresh air more often. The employer's unwillingness to perform regular maintenance caused the ventilation to operate poorly. Lack of revenues contributed to cut-backs that caused less maintenance to be performed on the ventilation system. Consequently, lack of revenues among other things caused the office to fall behind schedule. So what is the true causality? I believe that most organizations would have little difficulty making Janice the scapegoat. If Janice where a machine or part of a machine, she could be blamed for her adaptive failures and then replaced. Replacement is made possible through the commodification of labour where large pools of individuals can offer the same work at similar levels of quality. The market has moved in the direction of commodified labour not by accident. In the metrics that determine causality, we set aside the interconnectedness of things and elevate only particular aspects of the data that fulfill our instrumental needs. While causality is something extremely complex, instrumentalism poses it in the most simplistic terms; this simplicity is actually parts of the production system. If the case were different, there wouldn't be such extensive cloning of systems, labour, management, and even products. We have come to expect uniformity not just in what we do but also how we think about production.,I originally had the heading "Elements of Nazism in Scientific Management," but I was reluctant to draw a connection. Similarly, because Frederick Taylor has been regarded by some as the father of scientific management, I find it difficult to write about him in critical terms. However, consider Taylorism more as an abstract concept. In Taylor's testimony before the Special House Committee, he said that there can only be one right way to shovel coal. Taylor had some success and also failures applying a highly scientific approach to improve the productivity of workers. He studied how workers shovel coal and gravel. Equipped with a stopwatch, notebook, and it seems a measuring stick to obtain exact shovel dimensions, he arrived at specific ways to shovel that seemed to improve output. Since labourers come in all different shapes and abilities, I find myself questioning his findings or at least doubting the usefulness of his conclusions. Nonetheless, Taylor's influence remains felt even today. Big data for some people might mean heading down the same road paved by Taylor's early investigations. If a person had no choice but to shovel coal, the idea of using a scientific approach to evaluate repetitive movements probably seems like a reasonable route. Computing technology didn't exist during Taylor's time. Perhaps, he would have been an avid supporter given his tendency to accumulate and evaluate large amounts of data.,I believe that Taylor was what many people today would describe as a "data scientist." He was a scientist in how he arrived at his conclusions. But whether or not the conclusions really mattered was a separate issue. An astute company could have ignored his finding pertaining to manual labour, turning instead to heavy machinery. Machine labour increases production, the safety of workers, profitability, while improving delivery times. Taylor's basic assertion as indicated by the literature was that workers often wasted time by standing around. I believe that Taylor used the term "soldiering." He felt that soldiering contributed to poor production. In response, he aspired to find ideals ?€? that is to say, perfect ways to get the job done. If everybody performs their duties in the perfect way, production would be at an optimal level. His assertions were prescriptive in nature; the metrics were entirely criteria driven. Personally, I find myself more focused on how the workers must have felt being labelled as lazy by an outsider; being told in the smallest detail how to bend, dig, and lift; being judged by metrics of performance perhaps unrelated to effort and commitment. A person who works every day might not appear as hard-working as a person who works a few days a week. The portrayal of reality in the metrics is highly constrained. There are recent cases where testing standards have been found to be dissociated from the reality of the work - literally that the metrics did not reasonably mean what the proponents claimed.,It is possible to give the illusion of progress perhaps by paying some employees to dig holes and others to fill holes. It is possible to schedule high-level meetings; having meetings to schedule meeting; meetings to discuss meeting. The fact there is movement does not necessarily mean that it leads to production. Conversely, lack of movement or the metrics-evasiveness of behaviours should not lead us to believe that the behaviours as necessarily non-productive. I spend a lot of time programming and writing; it is often difficult to quantify the results. ??I wrote some of the software that I now use routinely more than 20 years ago. ??I guess it would be easy to dismiss my efforts as non-productive. In any case, I believe that the Nazis embraced aspects of Taylorism. While I "question" the causalities asserted by Taylor - e.g. that following prescriptive criteria is the best route to improvements - the Nazis made assertions on an entirely different level. The Nazis didn't pose their concerns in relation to laziness but rather racial superiority and inferiority. They seemed less concerned about telling workers how to shovel coal and more interested in imposing dominance and oppressing other groups. Nonetheless, I would suggest that the differences are not in substance but rather magnitude and texture. The Nazis were probably applying production management techniques on a broad social rather than factory level. While Taylor gathered production data, the Nazis conducted questionable research to support their movement. It seems that Nazis found for instance that many races were inferior to Germans. I am reminded of an article that I read during my graduate studies showing different rankings of mental incapacity. The rankings seemed to be used substantively to divide people and perhaps forcibly impose institutional care. I believe similar principles can be applied in a setting of mass production to oppress people.,Why have I bothered making the connection between Nazism and Taylorism in this blog? In an age of big data, I consider it tempting to slip into one of these powerful mindsets: Taylorism in relation to production environments and Nazism in broader social settings. Recently there has been an explosion of surveillance tactics both by companies and governments on a global level. I believe it would be fair to say that Taylor and also the Nazis spent a lot of time gathering data about people and placing their findings into structures determined not by the data itself but their preconceptions of reality. Michel Foucault in "Surveiller et punir" describes in rather graphic detail the rise of the penitentiary system in Europe. I believe it would be fair to say from the French text, punishment (from "punir" to punish) is a horrifically embodied experience. The other component of the title resembling "surveillance" (from "surveiller" to watch) denotes a disembodied condition. It is possible to monitor distant areas of concern remotely from security and intelligence offices. Data-loggers such as black-box devices can compile information without human involvement. We therefore have the disembodied and systematized imposition of monitoring versus the embodied and lived experiences of those being monitored. I would describe surveillance as a type of punishment in itself. Most of us have a reasonable expectation of privacy. Privacy is evidence of autonomy and freedom. Therefore the presence of vast amounts of data regarding our day-to-day lives causes us to question our autonomy. Why would this necessarily be so? As I have already pointed out, data in the context of Taylorism supports a paradigm of external control over our smallest behaviours. In terms of Nazism, data seems to bring about social delineations apparently to distance some segments of society from the benefits of membership. So while having massive amounts of data might not in itself mean anything, it can easily mean the wrong thing.,Foucault in his book mentions J. Bentham's Panopticon. A Panopticon is a distinctive type of prison facility that allows for quick viewing of prisoners. I would describe a Panopticon as a turret surrounded by holding cells: from the middle the building, a guard can see the prisoners sprawled all around. Moreover, the prisoners are all able to see each other. Those detained in such a jail have little privacy. Humiliation is a collective experience. The idea of a "jail" is really quite a concept: in order to keep somebody behind bars for long periods of time, there has to be some cohesive rationale. One interesting reason that I picked up during my graduate studies involved a belief in the following: people tend to conform and do what is expected of them if they are made aware that they are being watched. Jails can therefore serve as a place and form of behavioural conditioning. These days, although they don't appear to be in jail, everyday citizens are constantly reminded that they are being watched - that enormous amounts of data are being gathered about them all the time. Recall that Taylor had declared war on laziness; to me, having such an individual nearby with a stopwatch watching every move is certainly comparable to how a prisoner might be treated in a penitentiary. The move towards higher levels of surveillance represents a social trend. There must be a presumption of "criminality" to justify the collection of data. The crimes are not crimes in a legal sense but rather in a Nazi sense: failure to conform to predefined normative behaviours makes a person a criminal. The intent of surveillance is not necessarily to prosecute members of the general public but instill within them a fear of non-conformity.,The idea that we manage what we measure implies that everything we measure is destined to be managed, which is not necessarily the case at all. However, the current context in which we collect data almost seems to necessitate control over the underlying phenomena giving rise to the data. What are we comparing the metrics against? In order to assess something as erroneous or inferior, it is necessary to have a basis of comparison. Here I borrow from my background in environmental studies by raising the concept of a "climax": e.g. a climax forest. The argument in support of a climax is as follows: many things including human society naturally gravitate towards a superior form. It is part of our evolutionary development. In this context, there can indeed be only one right way to do things or to exist - all else being inferior or simply transient states en route to perfection. We hear the climax expressed in comments such as, "The children are getting taller and taller every day!" and "They are teaching calculus to kids at a younger age each year!" The presence of criteria presumes an ultimate superior form or condition. From a point of perfection, diversity would be illogical. It only makes sense to have clones and monolithic approaches.,The accumulation of massive amounts of data can support subjective and potentially destructive processes of organizational and social engineering. Interpreting data from a Nazi standpoint (although not a "Nazi's standpoint"), any data collected serves to satisfy the metrics of criteria. Since it is impossible to find fault in perfection, there is never any need to adapt an organization to changes in the environment. One only has to ensure conformity and compliance. The data therefore serves to demonstrate level of conformity; it becomes a tool to project ideals and identify distance from ideals. Wrongness from the standpoint of perfection can be measured by evaluating those things that exhibit non-conformity. I don't claim to be an expert in Nazis by the way, so I apologize if my portrayal misrepresents Nazis in real life . . . in case there are any Nazis reading this blog. "What caused John's poor performance? Since John belongs to an inferior demographic group, poor performance is likely due to his connection to that group." "What caused the death of this patient? The surgeon that performed the operation failed to follow rules of professional conduct; this non-professionalism caused the death of the patient." Lack of sanitary tools and supplies had nothing to do with the death. Unrealistic schedules and deadlines imposed by the hospital on the surgical team did not kill the patient. The problem with the Nazi perspective is that it might be entirely wrong; yet there is no means of recovery from bad decisions. The causality has been placed on a pedestal; and it is difficult to question perfection. Data can be and has been used to define people and impose behavioural requirements. I would say that an important role of causality has been to colonize and subjugate indigenous populations, making them primitive by labelling them as such. ??Causality can be used to make people inferior; establish criminality; and silence reality.,"Anybody who drinks alcohol is a criminal." To me the statement tends to reflect a Nazi mindset. In Canada, a federal minister once said something to the effect, "If you don't allow us to openly access your data, you support child molestation." (He didn't use these exact words. Many people interpreted his comments to mean that they were personally being accused of child molestation.) A logical extension of this assertion is that people concerned about big government and loss of personal privacy are more likely to be child molesters. Thus we encounter a legitimization of a surveillance society not to understand more about people but rather to ensure their control, adherence to rules, and conformance to standards of conduct. I feel that Nazism has been revived in broader social settings by our technological capabilities - particularly the ability to accumulate massive amounts of information. Unlike the Nazis, I don't know what the final form of our society should be. Unlike Taylor, I don't believe there is one right way to perform industrial processes. However, I do want to take a moment to examine the non-adaptive nature of the metrics of criteria. When some segments of society start slanderously and neurotically labelling others to put them in their place, criminalizing them almost to push them off their place, we really start to encounter the structural entrenchment of control. ??We witness the use of surveillance and data to justify modes of incursion and social disablement.,To suggest that people should not confuse association with causality assumes that understanding exists ,: i.e. we already know when a relationship is nothing more than an association, and when within this association there are no aspects of causality. A follower of Taylor might say, "You know, there is only one right way to shovel coal. You aren't doing it right. This is why you aren't getting much work done." So this is a person that professes to have an authoritative understanding. But is it really necessary to dismiss an association just because it doesn't quite fall within our expectations of substantive determinants - i.e. fit our view of how things should be controlled? For example, we are now starting to encounter drug-resistant bacteria. If prescriptions stop working, it becomes necessary to seek options outside the existing framework. I question how far the search can go when information only flows in one direction - from the epistemological authority to everybody else. Contextual associations are not necessarily without value. While substantive analysis might legitimize the use of particular therapies, this is not to say that same analysis delegitimizes contextual associations. The fact that I can take a pill to relieve a headache does not diminish the relevance of life events associated with headaches - such as lack of sleep, stress in the workplace, and nutrition. So even if a pill stops working, there are other options to deal with the pain. There is no single right way to deal with the problem.,I have encountered comments to the effect that companies are not using their existing data resources well or effectively; so expansion into big data might not lead to particularly worthwhile outcomes. Why might the use of data not led to clear returns? What does it mean to make effective use of a resource? I choose to blame the problem on clones, yes indeed. It has become evident to me that business as we know it is built around conformity. Companies hardly ever develop their own systems. They customarily import or acquire applications as opposed to developing their own. The strategy is similar in terms of making use of people with similar competencies, management methodologies, metrics, and data systems. It has become common to acquire and replace people and resources to achieve the "best fit" rather than adapt to changing circumstances. I point to how similar most office environments are along with how authority is delegated. I suggest that organizations are so similar, in fact, that their internal structures are forced to handle data in a manner that might not be ideal in order to conform to industry practice. The instigation of change occurs through the metrics of criteria irrespective of environmental circumstances. By this I mean that the process of change is more a matter of form than substance - projection of design rather than the articulation of contextual relevance. It is easy to produce a product that nobody wants because this is merely a matter of design; the metrics of criteria mostly serve the cause of conformance. By the way, I am certainly not expressing any problems with the sale of commercial software and systems: I am more focused on the resulting lack of adaptation and the potential limits imposed by standardized solutions and methods. ??I consider the involvement of data scientists part of a creative process to help organizations recover from evolutionary dead-ends.,Even a company that adheres to rules faithfully can nonetheless fail. Ford for example was surprised to discover that consumers had become disinterested in the Model T. The company could have continued producing the same vehicle regardless of what the market wanted. At some point, Ford embraced the need to start producing different types of cars. Consumers came to expect diversity and change. It is the company that had to conform to the market rather than the other way around. There is no such thing as a "perfect" car. If there were a perfect car, and a factory could produce it, there would never be a need to retool facilities to build other types of vehicles. Imagine being a Nazi, perhaps not necessarily insisting on a single perfect vision of reality but certainly insulating one's sense of reason from the consequences of one's actions. This is something an alcoholic might do. Organizations can become structurally pathological in a similar manner: they might decide to use the data to support convoluted perceptions. This is not to say that the data is "wrong," but rather its presence might be part of the illness. To wish to become all alike, this is how a disease spreads from one company to another.,A data system does not affect data merely by the design of the system but also by its interaction with other parts of the organization especially people. For instance, when a system is predisposed to reductionism, this diminishes the need for employees feeding the system with data to acquire complicated information. Yet it is in this complexity where answers to troubling questions might be found - such as where a company should be headed. The decisions rendered from the data likewise become shaped not so much by the underlying phenomena of the data but the instrumental aspects of the data that conform to the system. The metrics of criteria aren't meant to guide change at all but simply ascertain the extent of compliance - in a manner of speaking, how much things remain on track even if an organization is completely off track. The search for the climax or ideal has caused organizations to import solutions and methodologies, and make decisions that are increasingly disassociated from their business settings. Therefore, I would expect the only stable path towards greater use of big data to incorporate both the metrics of criteria and phenomena. I would expect the truth of relationships to be fashioned more by the underlying phenomena of the data itself rather than the abstract disembodied philosophies and assertions of inbreeds.
These profiles are randomly selected among our active and new members. To be selected, you need to have an interesting profile with picture; it helps to post a blog, or share / comment contributions from other members.
All creatures have the ability to sense the surrounding world, but in various ways and degrees. You might envy the bloodhound?€?s exceptional nose, but humans possess visual prowess that (although it doesn?€?t match the eagle?€?s eye in distance) is unsurpassed in the ability to detect and make sense of patterns. Our eyes and brains work as a team to discover meaningful patterns that help us make sense of the world [1].,Digital computers take input in direct quantitative form constructed from digits. Human extract most of quantitative information from 3D visual environment: distances between observable objects, sizes of objects, colors intensity and hue, proximity, similarity, symmetry ?€? ?€?A striking fact about human cognition is that we like to process quantitative information in graphic form?€? [2].,A pattern recognition (sense-making) stage comes after low-level extraction stage of human visual perception.,A relatively recent in human history process of visual symbolic information reading (e.g. letters & numbers) and their derivatives (text, tables, etc.) severely limit the amount of quantitative information extracted from this visual input. Therefore we disable substantial part of information directed to pattern recognition (sense-making) stage.,This is why Graphs (e.g. line chart, bar chart, etc.) are very powerful tools for understanding digital data: they mimic human visual environment by encoding digital data as locations, distances, sizes, colors, etc., therefore enabling power of human standard pattern recognition (sense-making) process. In short, the following information conversions sequence is taking place:,Do the same by using derivatives of symbolic representation (e.g. tables, text) makes sense-making difficult and sometimes impossible (cases of medium to large amount of symbolic data) due to limitation of human cognitive capabilities to memorize symbolic information. This is why presenting information in graphic form or information visualization is so important for its understanding.,The exponential technological development created overwhelming amounts of disparate, conflicting, and dynamic information and, therefore, huge needs to analyze and understand information. Despite vast amount of newly created effective digital algorithms for information analysis, attempts to completely remove human from a decision loop been unsuccessful. Altogether overwhelming analyzing needs, deficiency of automated algorithms and previous visualization methods created enormous demands for information visualization and visual analytics.,You can find short review of information visualization and visual analytics in attached review [4], MILESTONES part (page 3) and in research agenda [3]. Different Charts or visual representations correspond to different data types and designed to solve specific problems. But how to combine vast amounts of disparate data types together in unified visual representation suitable for discovery and satisfying visual information seeking mantra?,?€?The holy grail of information visualization is for users to gain insights. In general, the notion of insight is broadly defined, including unexpected discoveries, a deepened understanding, a new way of thinking, eureka-like experiences, and other intellectual breakthroughs?€?[4]. To make search for insight feasible between other requirements, we must have:,Data transition B) to VUDR cannot be done without moving to high level of abstraction. It will compact the data and positively affect A). Then interaction C) will be done on high level of abstraction. It will let to have synchronous high capacity multi-level information presentation from high abstraction level to connected low levels including raw data.,Therefore the critical to the above is to find a solution of challenge B) or to find unified visual data representation (VUDR).,Interestingly VUDR design can be prompted by the process of finding insight. Let?€?s consider the following: the burst of recognition often happened then one arrives at insights by linking previously unconnected thoughts. The theory is computational and it is possible to formulate the search for insights as a problem of searching for the potential linkage between even the most unthinkable relations. Initial studies of transformative discoveries such as Nobel Prize winning discoveries are particularly promising. This approach is particularly relevant to visual analytics and insight-based evaluative studies because they can characterize insightful patterns in terms of structural and temporal properties [4][5].,[1] Stephen Few: ?€?Visual Pattern Recognition?€?, COGNOS, Innovation Center, 2006., [2]Pinker, S.:?€?A theory of graph comprehension?€?; In R. Freedle (Ed.) Artificial intelligence and the future of testing, (pp. 73?€?126). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc., 1990., [3] ?€?Illuminating the Path: The Research and Development Agenda for Visual Analytic?€?. January 1, 2005. James J. Thomas (editor), Kristin A. Cook (editor).??, [4] Chaomei Chen: ?€?Information visualization?€?, 2010 John Wiley & Sons, Inc. WIREs Comp, Stat 2010 2 387?€?403.??, [5]Chen, C. | Chen, Y. | Horowitz, M. | Hou, H. | Liu, Z. | Pellegrino, D : ?€?Towards an explanatory and computational theory of scientific discovery?€? ; Journal of Informetrics, Volume 3, Issue 3, July 2009, Pages 191-209.
Author: by Edward Tufte, Publisher: Graphics Press, 1983, Pages: 197,A modern classic. Tufte teaches the fundamentals of graphics, charts, maps and tables. "A visual Strunk and White" (The Boston Globe). Includes 250 delightfullly entertaining illustrations, all beautifully printed.,Author: Nathan Yau, Publisher: John Wiley & Sons, 2011, Pages: 384,Practical data design tips from a data visualization expert of the modern age. Presents a unique approach to visualizing and telling stories with data, from a data visualization expert and the creator of flowingdata.com, Nathan Yau. Offers step-by-step tutorials and practical design tips for creating statistical graphics, geographical maps, and information design to find meaning in the numbers. Details tools that can be used to visualize data-native graphics for the Web, such as ActionScript, Flash libraries, PHP, and JavaScript and tools to design graphics for print, such as R and Illustrator. Contains numerous examples and descriptions of patterns and outliers and explains how to show them.,Author: Jacques Bertin, Publisher: Esri Press, 2010), Pages: 456,Originally published in French in 1967, Semiology of Graphics holds a significant place in the theory of information design. Founded on Jacques Bertin?€?s practical experience as a cartographer, Part One of this work is an unprecedented attempt to synthesize principles of graphic communication with the logic of standard rules applied to writing and topography. Part Two brings Bertin?€?s theory to life, presenting a close study of graphic techniques including shape, orientation, color, texture, volume, and size in an array of more than 1,000 maps and diagrams.,Author: Colin Ware, Publisher: Morgan Kaufmann Publishers In, 2008, Pages: 256,In Visual Thinking for Design, Colin Ware takes what we now know about perception, cognition, and attention and transforms it into concrete advice that designers can directly apply. He demonstrates how designs can be considered as tools for cognition - extensions of the viewer's brain in much the same way that a hammer is an extension of the user's hand. Experienced professional designers and students alike will learn how to maximize the power of the information tools they design for the people who use them.,Author: Colin Ware, Publisher: Morgan Kaufmann, 2004, Pages: 486,Most designers know that yellow text presented against a blue background reads clearly and easily, but how many can explain why, and what really are the best ways to help others and ourselves clearly see key patterns in a bunch of data? This book explores the art and science of why we see objects the way we do. Based on the science of perception and vision, the author presents the key principles at work for a wide range of applications--resulting in visualization of improved clarity, utility, and persuasiveness.,Author: Stephen Few, Publisher: Analytics Press, 2013, Pages: 260,A leader in the field of data visualization, Stephen Few exposes the common problems in dashboard design and describes its best practices in great detail and with a multitude of examples in this updated second edition. According to the author, dashboards have become a popular means to present critical information at a glance, yet few do so effectively. He purports that when designed well, dashboards engage the power of visual perception to communicate a dense collection of information efficiently and with exceptional clarity and that visual design skills that address the unique challenges of dashboards are not intuitive but rather learned. The book not only teaches how to design dashboards but also gives a deep understanding of the concepts rooted in brain science that explain the why behind the how.,Author: Andy Kirk, Publisher: Packt Publishing Limited, 2012, Pages: 206,The author explains A structured design approach to equip you with the knowledge of how to successfully accomplish any data visualization challenge efficiently and effectively. He explains a portable, versatile and flexible data visualization design approach that will help you navigate the complex path towards success. Explains the many different reasons for creating visualizations and identifies the key parameters which lead to very different design options.,Author: Phil Simon, Publisher: Wiley, 2014, Pages: 240,In The Visual Organization, award-winning author and technology expert Phil Simon looks at how an increasingly number of organizations are embracing new dataviz tools and, more important, a new mind-set based upon data discovery and exploration. Simon adroitly shows how Amazon, Apple, Facebook, Google, Twitter, and other tech heavyweights use powerful data visualization tools to garner fascinating insights into their businesses. But make no mistake: these companies are hardly alone. Organizations of all types, industries, sizes are representing their data in new and amazing ways. As a result, they are asking better questions and making better business decisions.,Author: Alberto Cairo, Publisher: New Riders, 2012, Pages: 384,The first book to offer a broad, hands-on introduction to information graphics and visualization, The Functional Art reveals: Why data visualization should be thought of as ?€?functional art?€? rather than fine art; how to use color, type, and other graphic tools to make your information graphics more effective, not just better looking; the science of how our brains perceive and remember information; best practices for creating interactive information graphics; a comprehensive look at the creative process behind successful information graphics; an extensive gallery of inspirational work from the world?€?s top designers and visual artists.,Author: Ben Fry, Publisher: O'Reilly Media, 2008, Pages: 384,With Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you: the seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interact; how all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous details; several example projects with the code to make them work; positive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set.,.
I received this morning, like everyone, an email from LinkedIn about ,. In short, it represents about 0.5% of my 10,000+ connections, and I decided to share with you those people listed at the top - the top 100, not just the top 59. This is based on profile views over the last 7 days, so it will change week after week. Also, I removed recruiters, generic VC or angel investors, and non data scientists in general (based on skill mix): it narrowed down to 25 profiles, listed below.,Interestingly, very few are females (I think only one, Monica), and of course quite a few are employed by linkedIn. Some famous people like Gregory from KDNuggets, Nate Silver or Prof. Davenport are missing. But at least, it shows a very different picture, compared with the traditional "top data scientists" lists published by journalists, and based on some mysterious mix of questionable Twitter metrics, and unfiltered data. In short, the list below provides a fresh perspective about top data science thought leaders and practitioners.
Purchasing new business intelligence (BI) software is a difficult process. There are dozens of vendors with different specialties and price ranges. For many looking into BI for the first time, part of the process is simply??,., , , , , 
These profiles are randomly selected among our active and new members. To be selected, you need to have an interesting profile with picture; it helps to post a blog, or share / comment contributions from other members.


Somebody once mentioned to me that there is a need for a standard method of performance evaluation that can be applied to all employees regardless of their exact duties: e.g. to compare a janitor to an accountant. In my jurisdiction, there is a regulatory requirement for "equal pay for work of equal value" that can affect companies with government contracts. I consider the concept of "equal value" complicated due to its subjective nature. Certainly two people handling exactly the same work might be compensated similarly. But we also have to consider level of risk; level of responsibility; hours of work; and of course actual performance level. A related discussion surrounds the exact meaning of the term "work," which itself is a rather deep issue. Taking into account all of the productive resources of an organization, I believe that the debate should consider the relative merits of different assets; but this assertion adds to the complexity of the performance evaluation. It is a Holy Grail of sorts being able to determine how different programs, projects, policies, practices, behaviours, and capital resources contribute to particular outcomes. I find that the following metrics share some structural similarities: 1) effectiveness for instance in relation to marketing campaigns; 2) performance in terms of human resources; 3) quality, reliability, and conformity for products and services; and 4) change and adaptation strategies leading to restructuring and organizational development. Any method that applies to one area might also be applicable to another, offering many business benefits once fully articulated. How then might it be possible to determine any kind of standard performance? I will be using some simulated data and a process that I call the TIME methodology to examine this question.,The simulation that I prepared for this blog is meant to be simple enough for readers to anticipate the outcomes. The performance of workers in this simulation is determined by their levels of production. Each worker contributes to the production of a single thing in the same manner and nothing else. So already this simulation is unrealistic since everybody has the exact same duties. Such a scenario is unlikely to occur in real life. Nonetheless, perhaps this is how comparisons are sometimes done between workers: an objectively verifiable metric is used as the basis to compare productivity. Consequently, Kathleen at reception cannot be compared to Janice in production using such an approach; attempting to do would create severe distortions in the evaluation. There are 11 agents in total: agents 0 to 10. Each cycle in the simulation contains a workgroup of 5 agents chosen randomly from the 11 available. I arbitrarily set the contribution levels for each agent as follows: agent 0 contributes 0 units; agent 1 contributes 1 unit; agent 2 contributes 2 units and so forth. The testing environment does not know the individual contribution levels: it only knows the total for each workgroup. The question then is whether performance can be determined based on the involvement of agents irrespective how much they produce individually (i.e. their amplitudes). However, keep in mind that in this simulation - just coincidentally to simplify the analysis - performance is the same as amplitude. This is a type of performance evaluation problem that any organization might face perhaps to determine bonuses, salary adjustments, and for management purposes. Some readers might complain that there are already ways to determine individual contribution levels. The simulation is meant to demonstrate how the TIME methodology handles performance. I will describe some additional benefits a bit later.,The TIME methodology is all about moving around information in a particular manner; this is more of an algorithmic rather than statistical process. Since the movement can involve large amounts of data, I find it reassuring to deposit controls in the data to assess the reliability of operations. I feel that the idea of Big Data is problematic from a practical standpoint in that small errors might be magnified through algorithmic recursion and repetition; this can lead to big mistakes. In response, I make use of diagnostics tests: I add control data to the field data. More or less, the image below shows the normal outcome of diagnostics testing. How events get distributed depends mostly on the construction of contextual relevance. A production simulation can have as its contextual relevance the number of units produced: the more units produced, the greater the performance. Therefore, production in this case is relevant to the context of performance. Although using units might help us to understand how the distribution relates to production, it is not necessary to rely on units of production using the TIME methodology. It is possible to make people busy in order to give the appearance of productivity; such forms of churning can lead to the exhaustion of scarce resources. So on the issue of contextual relevance, relying strictly on units of production is extremely foolish; it is also unnecessary given a more sophisticated path made accessible through the use of the TIME methodology.,Since there are 5 agents per group, I would expect the lowest score to be 0 + 1 + 2 + 3 + 4 = 10 units; the highest score should be 10 + 9 + 8 + 7 + 6 = 40 units. Accordingly, the distribution of events occurs between 10 and 40 as indicated on the diagnostics image above. Can the performance of agents be determined without knowing their individual contribution levels but by having workgroup production data? By and large, yes. As the next illustration shows, it is possible to determine relative performance by inference using workgroup participation rather than individual production amplitudes. I say that performance is relative since the actual contribution levels are not known; or at least they shouldn't be known. Agent 10 is ranked higher than agent 9; agent 9 higher than agent 8; agent 8 higher than agent 7; and so forth. Yes, I notice the distorted readings for agents 6, 3, and 0. Keep in mind that the relative differences in contribution were determined inferentially from random groupings. Just by chance, agents might find themselves in strong or weak groups that make them seem like better or worse performers than they actually are. A great performer might be matched up with terrible groups. A terrible performer might be assigned to great groups. Therefore, distortion can occur in the case of random groupings; perhaps these dynamics describe the situation even in real life.,It might be tempting to use an algebraic approach to determine contribution levels. Algebra delivers misleading clarity. In real life when people are placed into groups, performance depends on the group. This is the whole idea of having groups. If the total were equal to the sum of the parts, there would hardly be a business need to have groups. In reality, a worker might perform really well in one group and poorly in another. More importantly, a group might perform better with particular workers and worse with others. The reality of performance created by and within a group might not exist once that group is dismantled and its resources redistributed. Nor in practice is every person in a group responsible for production-related activities. An organization taken as a whole cannot operate properly if it has only production personnel. The contribution of each member might not be apparent all the time or within every context. Nonetheless from this simulation where we coincidentally know the individual contribution levels in advance, it seems that the TIME methodology provides a reasonably good ranking.,There is a saying that we shouldn't compare apples to oranges. I believe that it would be more accurate to say, we shouldn't compare a quantity of apples to a quantity or oranges. The numerical relevance of one does not translate in the same manner to the other. Of course we can compare fruit. We just cannot say that 1 mango = 1 banana or a sum mangos mean the same thing as a sum of bananas since there is no equivalency in amplitudes. Numbers only allow for numerical comparison. Bananas and mangos are still comparable but just not mathematically. In the simulation, I make use of units of production; this provides for a comparable basis. The use of comparable amplitudes creates a barrier preventing the integration of mixed data. For example, a person might ask, how can market reports, absenteeism records, and production statistics be used together? The use of these data resources is instrumentally defined by the contexts in which they were produced. Once the data is disembodied or disassociated from their realms of discourse, it becomes necessary to recontextualize in order to attach relevance. I hardly ever use amplitudes since doing so demands a paradigm of sameness that distorts meaning.,Although the theory surrounding the TIME methodology might seem complicated, the conceptual dynamics are straightforward. Accordingly, I will put aside theory for now and just explain some basic concepts. TIME is an environment intended to support the deployment of algorithms. These algorithms take advantage of the fact that all of the data accumulated using the methodology has been contextualized. The algorithms determine the relationship between the data and the contexts. I have two additional images to share from the simulation. The first image is for agent 0. Recall that this individual literally contributes nothing to production. The second image is for agent 10 having the highest contribution level. I already describe the Crosswave Differential (XD) in earlier blog posts; so I won't really elaborate on its construction here. However, put simply, the XD is an algorithm that separates the contribution of an agent from the ambient level that exists without the agent in relation to established contexts. Agent 0 does not actually result in a contribution level of 0; but whenever agent 0 is included in a group, the overall the performance of the group declines. Of course, it would be reasonable to confirm agent 0's performance through conventional means assuming he or she is a production worker. When agent 0 participates in groups, the groups produce about 23 units. When agent 0 is absent from groups, production increases to about 30 units. (I am reading directly from the image.) The dark arrow shows the crosswave for agent 0. The light arrow indicates the crosswave when agent 0 is not participating in groups.,In the second image, the ambient crosswave pattern under the light arrow is closer to the centre while the pattern for agent 10 under the dark arrow is closer to the right. When agent 10 is part of a group, production is at about 31 units. In the absence of agent 10, ambient performance slips to about 24 units. So this is a fairly simple albeit rather instrumental explanation of performance. I suggest that a superficial use such illustrations is probably inappropriate for reviewing performance in real life. The context is just so estranged from the day-to-day realities of life in a workplace. Approach the use of algorithmic representation carefully since the contexts defined by management for assessment purposes might not lead to long-term business gains. A person in a group might not actually be responsible for producing units of tangible output. For instance, a manager might not be involved in production in a literal sense. Nonetheless, if it seems that groups produce more when agent 0 is replaced by somebody else, sadly from the standpoint of the contextual relevance set by management itself, the stars seem hostile to this individual. As I will explain shortly, the TIME methodology is designed to promote an inductive management environment that takes into account any number of possible considerations. There is no need to take a simplistic approach.,I use the term "symbolic aggregation" in other posts to describe how events can give rise to contexts. Dozens or perhaps even hundreds of events can lead to a context or contexts; and this can then be used as an event to combine with other events leading to more contexts. As such, there isn't a limit to the number of events giving rise to any context; and there is no limit to the number of contexts giving rise to even more events. I certainly hope this is easier to understand than to write. If there is a limit to how much data can be processed, it is not due to the methodology per se but rather the current state of an organization's processing technology. If there were a computer system powerful enough, all of the events that might occur on a transit system or highway could be assigned to a context such as X. An event-to-context relevancy algorithm such as the Crosswave Differential (a specific type of "algorelevancy") could then measure the extent to which data is relevant to its asserted context. Symbols can naturally "fall out of context" depending on the exact nature and effectiveness of the algorithm.,The simulation compares apples to apples as one might expect in a comparison of performance between similar types of employees. But employees are not necessarily all that similar. For instance, there might be skilled trades-people along with administrative staff. The fact that we try to reach unified approaches from diverse human and capital resources might lead us to question why performance should be limited to people: why not be really eclectic and mix dissimilar productive elements? Through the TIME methodology, it is possible to incorporate all sorts of data into the analysis. Not only different types of work can be taken into account but also workplace conditions and settings. I might for instance combine employees, operating systems, hours of work, office lighting, and ventilation levels. A truly large amount of exceptionally diverse data can be included. The methodology involves relatively little overhead if one chooses to be open to the possibilities. Since the possibilities are limitless, I suppose that even a little overhead can overtax computer resources eventually. I recall myself wondering whether a head of cabbage can perform better than a CEO whose company is responsible for widespread environmental destruction. I'm not naming names, British Petroleum. I honestly don't know the guy's name, come to think of it. Using the TIME methodology, we can actually determine whether a chair or clock contributes more to performance than a CEO, depending of course on the construction of relevancy.,If an employee named Katie contributes strongly when she is not using a particular computer, this is useful information from a management standpoint. Performance evaluation can therefore be used to improve the workplace rather than simply labeling some workers as superior and others as unsatisfactory. It is possible to examine specific performance settings that contribute to particular types of sales. Some workers might do really well selling certain products from home while others might function better in a traditional office environment. The transpositional strategy becomes important as one reaches out to include different types of data. For instance, if monthly sales reports are available for a particular industry, it takes some thought to incorporate data from these reports into the broader analysis. The problem isn't the availability of data but rather its transpositional application to different organizational contexts. Nor is this an issue of prediction, but rather it poses a challenge of phenomenal description. A report is nothing if not its contextual relevance.,TIME is a type of data-rich environment. The TIME methodology represents a way of forming this environment so that the data resources can be accessed in particular ways. Algorelevancies are used to measure the "distance" between elements or objects of data within the environment albeit not in a spatial sense. Algorelevance is a transpositional concern. Some readers might want to read my blog on the Geography of Data, which I also describe as Transpositional Geography. I currently have one algorelevancy that I use routinely and another that is under development. The relevancy of data to performance can be characterized either as stress or shock, which are terms to describe the nature rather than direction of relevance. Neither stress nor shock is meant to imply something bad. Instead, stress is something that tends to build up and be persistent. For example, Kurt Lewin's Force Field analysis to me describes a stress situation where the interaction of opposing forces lead to equilibrium conditions along a conceptual edge. Shock involves a sudden or acute reaction. I have been mostly focused on the stress response, which is probably the most prevalent in business settings. Shock might be more relevant in terms of the response of financial markets to shifting global conditions; acts of terrorism and rioting; fear and panic over fuel and food shortages; and maximizing the impact of military campaigns. Conditions of shock can occur infrequently and rapidly, making it difficult to examine data in relation to stress.,My father was a maintenance mechanic. I have a fancy citation issued to him by a former employer that says, "For devotion to duty during typhoon 'Welming'." My very young father at the time, rather than let the facility fall during the typhoon, stayed to secure the building. I suppose he helped to save the plant and protect the livelihoods of many people. Such an act of heroism has tangible capital value that is difficult to measure over the course of day-to-day operations. The context of performance might not reveal the contribution of members in every potential setting especially shocking conditions. Certainly only the most superficial management regimes would choose to perceive performance in purely instrumental terms and in a nominal sense. I consider the XD poorly suited for situations of shock. But I firmly believe that sometimes, companies sink or swim based on the uncommon valour of its employees. When organizations have similar processes, systems, and workers, even the smallest differences under the rarest conditions might affect the order of things. So shock analysis might be important strategically, and I hope to contribute a bit to this discussion in the future.,The conceptual framework described in this blog is all about the stress response making use of the Crosswave Differential algorelevancy. If anybody is interested in shock, my tentative name for the shock equivalent is the Shockwave Differential algorelevancy. When I use the term "algorelevancy," I am usually referring to the transpositional approach. When I use the term differential, I mean the actual algorithmic measurement. However, when I use Crosswave Differential, I might mean either the approach or the measurement. I therefore invented the term algorelevancy to prevent confusion and to be more specific. I know this is all probably just a blur except for those passionate or perhaps obsessed about partitioning knowledge. My main objective is just to distinguish between the data environment and the algorithms used to make sense of the data. I would cringe if I heard a comment like, "The TIME methodology just doesn't work." The methodology only creates the setting for the algorithms. I firmly believe in the value of contextualized data. It is perhaps the best way to go if a person hopes to make effective use of extremely large amounts of data. The TIME methodology enables contextualization. But the assertion of transpositional relevance is a separate concern.,The TIME methodology is a way of organizing data. But before this organization takes place, there must be data to organize. Before data collection occurs, there has to be an understanding of what data to gather. A reference library is full of data; but a person still has to know what data should be gathered. Without this knowledge, it might not be apparent even after a person finds the data. Over the course of this search, it is possible to accumulate important events leading to different types of decisions. In the context of these decisions, the relevancy of the data might is affected. The determination of relevancy (therefore also the construction of algorelevancies) is a management concern. Managers might question the relevance of decisions in relation to profits. These days being exposed to so much data, it can be temping to ignore how the setting of data can influence decision-making. What data might seem promising at first could later prove pointless. What data seems irrelevant at the outset might later be found instrumental. Therefore, in attempting to manage data resources directly, there is great likelihood of exhaustive effort accompanied by high risk of squandered resources. The TIME methodology reduces the burden of managers by allowing them to concentrate on the contexts in which data might be relevant.,"Transpositional" relates to the management of contexts in which events occur. The impact of data on performance or any other context is determined through relevancy design (the transpositional construction of relevance) and how data should be placed within this structure. I have found it difficult to describe the algorithmic process; and perhaps the code is rather complicated. But essentially, it is necessary to determine the placement and properties of elements of relevance through contextual construction. Although I don't want to complicate matters, algorelevancy describes not just an algorithm but an algorithmic process. Obviously this function is best performed by a computer. However, contextual construction is performed by managers. Algorelevancies must operate within the parameters of contextual constructs made possible using the TIME methodology. Although to me the idea of contextual relevance is fairly tangible, it might be a foreign concept to some readers; this could be because data is sometimes gathered in a state that lacks any kind of contextual connection. Using the TIME methodology, data never exists without a context. Data is never disembodied. Moreover, any piece of data can be associated with an unlimited number of contexts. I therefore distinguish between the data itself and the organizational contexts that support assertions of relevance for any data collected. The methodology allows for active managerial involvement without necessarily requiring their intervention in daily processes. The methodology can accommodate massive amounts of data, thereby bringing the scope of management more in line with what the new technology has to offer.
AnyLogic Modeling and Simulation software is utilized by globally recognized organizations to ensure better, more profitable decision making. Annually, these organizations gather to showcase their accomplishments at The AnyLogic Conference, which will be held November 12th and 13th, 2014 in San Francisco, CA, USA. Presenting organizations include GE, Intel, Pricewaterhouse Coopers, CSX, Recargo, Deloitte and more, covering topics such as supply chain & logistics, healthcare, marketing, rail and pedestrian flow. Find more information and register at ,.
The tale of Dorothy and her friends as they journeyed to the Land of Oz; each to find answers to the problems they were facing is one many of us would have loved reading as children.,Fast forward 200 years. Visualize Dorothy in the new world of Research and Analytics. Meet the new cast of characters and imagine their story:,Dorothy represents the growing world of Analytics that is re-shaping traditional research by accessing multiple sources of data and shifting the focus from descriptive data towards predictive analytics. She knows not just how to understand customer behaviour but how to use analytical algorithms to design personalized experiences and drive meaningful insights for the right customer micro-segments.,Her friends, the lion, the tinman and the scarecrow represent the research everywomen who are trying find their hearts, brain and courage in the world of shifting CXO crowns and priorities as traditional research increasingly gets sidelined.,The story starts as Dorothy?€?s old home, blown away in the cyclone of business change, lands on the Wicked Witch of the East, the world of disparate offline survey and bulky research reports that Dorothy?€?s analytics superpowers accidentally overpowers. The land of munchkins where she lands is the world of endless stock reports and survey codes.,As a reward for her good deed, the Good Witch of the North grants Dorothy a magic toolkit powered by social and digital media that changes her traditional research toolkit.,She also gives Dorothy the silver slippers of visualization and gamification that give her new speed and power. Visualization changes the way she can create drill-down, online, automated reports to re-design her old reports for the business-users. And gamification gives her new ways to engage with her end-clients and design surveys that seek to engage rather than just collect information.,But Dorothy and her friends are still not out of danger. The yellow brick lane represents the pathway to analytics led business transformation and the magical land of the Emerald City signifies the land where the data meets insights and impact; where the Chief Marketing Officers, Chief Strategy Officers and Chief Data Officers use analytics to drive organizational wide transformation.,And there resides the Wicked Witch of the West who represents the world of shrinking marketing budgets and profits and commands the winged monkeys of ROI to keep catching those who cannot show her the money. She asks tough questions such as ?€?How can I measure my Customer Experience across multiple channels??€?, ?€?Can you marry all sources of data and give me one view of my customer??€?, ?€?Can you tell me what products I can sell to a customer to increase retention and stop attrition??€? Her questions are relentless. Dorothy needs to draw on all her strengths, use her new toolkit to silence the increasing doubts. Maybe the Wizard can help her, she thinks.,But the Wizard of Big Data is just a confused soul trying to pretend that he knows it all. The myth about his powers far exceeds his real powers.,Dorothy and her friends are brave and persistent. With her magic powers and her silver slippers, she proves that Research powered by Analytics is the way to drive real impact for the CXOs' business problems. And she doesn?€?t really need the wizard to solve her problems. Fast forward 200 years later? Well, that could be a new story.,But for now, all ends well. Just as in the old story, she says goodbye to her friends and returns home, stronger, braver and more powerful after her eventful journey and experience. A fairytale? Or the reality today?
 , , , 
Interesting comparison table and comments, regarding the following statistical packages: R, MATLAB, SAS, STATA and SPSS. I wish Statistica would be included. The table tells you which statistical methods are available in each package. The list of statistical methods is itself impressive. Note that Jackknife (a resampling method in the table below) has nothing to do with??,??(new technique not implemented in any package yet, though Dr. Granville has promised to provide the source code).,Also statistical libraries are available in most programming languages, for instance Pandas in Python. Here are five interesting articles:,The table (below) and additional information about the various packages ,.
Many years ago, I attended a vocational college to learn skilled trade. I was taught about the behaviour of systems. I learned that after renovations to a house, the furnace might cycle on and off more frequently; this can leave some parts of the house too cold. A wood-burning stove or fireplace should be treated as a part of a system. Open doors and windows in the dwelling can cause exhaust from such appliances to enter living spaces. I realize that these particular examples of systems might evade those without an HVAC background. Well, long before I went to college for retraining, I was working in the back-office of a financial company. In those days, printers often used ink-ribbon cartridges. In one workplace, there was a particularly large printer shared by several departments; this device required frequent cartridge replacement. One day, somebody decided to save money by purchasing recycled cartridges. These cheaper substitutes often caused the printer to freeze up. Consequently, this minor cost-cut that seemed reasonable on surface contributed to extensive delays. Many departments were dependent on the printer for reports that had to be generated each day. I want to crystallize the nature of the problem: 1) on one hand, there was financial benefit from reduced expenditure on cartridges; and 2) on the other hand, there were systemic losses greatly exceeding those benefits. In the Push Principle that I will be discussing shortly, I introduce an approach involving key data events to influence an entire system rather than its individual parts. The implications of such an approach are far-reaching: it highlights the importance of creatively and strategically handling data in order to target critical areas of concern affecting the entire body. This is because specific actions can translate into impacts that ripple across physiological, social, and production ecologies.,Why would anybody want to deal with problems in relation broader systems? The main reason is to achieve the greatest amount of benefit to the entire system using what scarce resources are available. Attempting to deal with a single problem or a small aspect of a large picture can have undesirable consequences to other parts of the system. It is financially and logistically justifiable to examine ways of determining the systemic impacts of both problems and possible remedial actions. Apart from the general rationale, I point out the usefulness of systems in relation to software development. To some extent, systems contain their own internal logic or intelligence. I could for instance attempt to understand factory operations by studying all of the existing behaviours occurring within a facility in isolation. This tells me what people are doing. It doesn't show what they are supposed to be doing or might do differently; so I cannot ascertain the difference between normative and organizational behaviours. When people and capital are involved in processes in certain ways to achieve particular ends, this "organization" exists to achieve productive outcomes: this is an issue of functionality instigated by the objective. But "how" people go about achieving productive outcomes is socially constructed: this is an issue of instrumentality initiated by the proponents. I am suggesting that a systemic approach has the potential to trigger intelligence that neither I nor anybody else can anticipate in advance; therefore we cannot fully incorporate such intelligent design into processes of production. The attempt to initiate or dictate development is inherently non-systemic; the intelligence is external to the system and therefore quite difficult to package such that it is adequate to the system. This blog post is about moving towards internally-guided rather than externally-driven intelligence.,I need to review some older blog material before proceeding to discuss how I personally would approach the issue of internal intelligence. A few months ago, I introduced a special object called a slider as shown on the image below. A slider represents a conceptual distribution of events over a dividing line called an edge. Expressed simply, events appearing on the left of the slider seem negatively associated with particular phenomena; items on the right positively; and then there are the events surrounding the edge that seem prone to philosophical speculation. A slider helps the user by implementing "descriptors": these are details that accompany the data explaining the nature of the data, which can then be used for sorting purposes and to create distribution profiles. In the image to follow, I use the descriptor "exercise" to sort events. Many exercise events appearing to the right of the slider might share certain features; these commonalities can point to new directions to further one's knowledge of the phenomena. I have a research prototype called Tendril that can generate sliders and the other data objects that are mentioned in this blog.,I feel that a slider provides a worthwhile perspective if one hopes to determine how to meaningfully expand the body of events. Sliding offers guidance if a person lacks knowledge of the underlying phenomena. Making use of such an object is actually a rather philosophical stance in relation to data. "I don't know very much about this at all. It's not that I need a starting point. I don't even know what a starting point is in relation to this data." An alternate philosophical stance asserts an understanding of phenomena a priori. Although information is absent, thus justifying its further collection, ironically we might already have a perception of how things generally fit together. This perspective might be valid if, despite our incomplete knowledge of the dynamics, we have a fairly coherent understanding of ourselves in relation to our surroundings. Furthering this perspective, I will now discuss something that I call a "risk object." In the case of risk, arguably one already knows what risk is. A person basically hopes to ascertain where an event falls within his or her perception of risk.,Popping is a style of dance. It's not a brand of breakfast cereal or an ingredient. In popping, the body follows a series of smooth motions and abrupt shifts, juxtaposed perhaps to imitate a robot snapping and whirling around swivel points. I personally call this a type of exercise. I'm no expert in kinesiology. However, I would say that popping is probably good for the circulation; it is perhaps a bit stressful on the joints. For me in terms of data collection, popping represents a type of reoccurring event that affects many different types of contexts. If I extract the slider details for an event such as popping over different contexts, I become aware of its systemic impacts. I do not necessarily know the impacts of popping in advance. I do not wish merely to satisfy my preconception. But I do know what an "impact" is in relation to me; and I want to know if popping has an impact. (If I do not know what an impact is, then I cannot say if popping has one.) The question is no longer whether popping is "good" or "bad." Popping is both good and bad depending on the context, intensity, duration, and other details. Dealing with the event on a systemic level is a matter of risk management involving complex choices: there is 1) an absence of complete knowledge; 2) some perception of contextual benefits and drawbacks. It is necessary to navigate or negotiate a path in the face of these conflicting and at times competing realities. I present the risk object for popping below based on real personal data.,I pop to industrial music and electronica, a style of music that arguably induces popping. To dance like a robot, it is necessary to play the preferred music for robots. Perhaps the human body can do robot-like moves that no robot would be able to imitate; this is especially true once the beat starts to fall apart. I am left wondering why we bother mentioning the "robot" at all except perhaps to simplify the explanation. Popping requires a fair amount of endurance and willingness to move not-like-a-human - at least not all of the time. For me, popping represents a kind of a Post-Romantic alienated expressionism performed by the human body. A person could imitate a car crash (a collision). This is a little bit beyond ordinary popping of course. I imagine that a forklift is easier to imitate than a car crash. Due to my age, I'm unsure how long I can keep up my popping. I might die popping. I consider popping a rather enjoyable activity. However, I chose to focus on it specifically because it can be reasonably expected to have both positive and negative physical impacts; and this is precisely what the contextual results seem to show on the following illustration. Just as a brief explanation for those unacquainted with my sliding terminology, when popping appears under "under" and "less," this means that it is adverse in relation to the context; under "more" and "over," this means that popping is positive.,Popping for me registers as "under" under the contexts "pain" and "joints." Joints involve things such as fingers, knees, ankles, and wrists. I usually place an "x" at the front of a context to show that it is a lagging context: e.g. xjoints. A lag means there is some sort of time delay between the event and the contextual impact. Popping seems to be associated with adverse perceptions in relation to the stomach, feet, and frame (bone context). I actually don't feel much discomfort when I pop (at the actual time of popping) except periodically at the knees and feet. If I do quite a lot of popping, I sometimes get cramps around the kidney area (pain context); again, the body is forced to move in unnatural ways. I normally update the database later in the evening often many hours after events take place. Perhaps most of the other contexts are fairly self-explanatory. My point here, as the illustration above should show, is that popping or any other type of event can have both positive and negative consequences. It is therefore illogical to deal with events that occur within a system in isolation as if the other parts of the system don't exist or matter. It is particularly hazardous to attempt to gain benefits from certain events while failing to take into account the adverse impacts apparently associated with those benefits. Conversely, in order to achieve certain outcomes, a person might attempt to exploit events that have multiple impacts. In relation to me, since I only have data on myself, taking a balanced omega formulation seems to have a number of beneficial impacts and not many negative consequences.,The placement or relevance of an event in relation to a particular context is determined by an algorithm. I describe this algorithm as the "relevancy." There might be different types, but the one that I use throughout this blog is called the "crosswave differential." Recall that a "system" is usually made up of different contexts. My research prototype goes through all of the contexts assigned to a particular system to calculate the relevance of individual events. Similarly, a relevancy algorithm can be used to determine the relevance of a context to a system; of a system to a supersystem and so forth. However, in terms of the immediate matter at hand, there are often many different systems full of contexts containing a myriad of multifarious events. I usually generate an intermediate object or protoform called a "protosystem" that contains all of the event results for all of the contexts (resulting in quite a large data object). Different systems would then extract from the protosystem. Forming an intermediate object is slower than simply generating the results for a single system; but in the end it prevents repetitive processing. For example, if one intends to make systemic comparisons, invoking many different systems perhaps constructed fractally or thematically, it would be time consuming to recalculate relevancy values for the same event-to-context combinations. Making use of an intermediary also allows me to let my computer run for long periods of time without supervision. I usually have dinner, watch television, and get some exercise during the lengthy process. I wasn't able to conceptualize an image for the protosystem object to share; so I simply pasted a portion of the object file below.,The risk object is fairly complex from an algorithmic standpoint. It is also a bit bulky: it is necessary to skim through the different contexts and make assessments on this basis. A person might ask, would it be possible - taking into account the risk object for each event - to create a type of high-level "superobject" showing the apparent systemic value for everything? Inherent in this question is the idea that risk management can be automated. The processing environment is given the duty of attaching value to consequences - a job that might normally go to risk managers. I'm uncertain if this question can be easily addressed because the mathematics of confirming systemic effectiveness is perhaps rather subjective. We all have a different sense of how contexts should be prioritized and weighted. Each perspective on the future affects prioritization of the present. For example, I routinely take my blood pressure and check my eye-sight; conceivably, an event might help one but harm the other thereby forcing important choices to be made. (I am using an extreme example. The whole idea is really to select events that help both. But sometimes due to lack of resources and harsh environmental conditions, it might be necessary to consider difficult tradeoffs. So I'm saying that prioritization has been in the hands of people making choice rather than algorithms.) Nonetheless, I believe that a superobject can serve as a useful reference point to assist in decision-making, keeping competing transpositionalism in mind. I call the superobject a "grail." This object is extracted from a protosystem. A grail object is not much more difficult to form than a risk object.,In the illustration below, I used a fairly arbitrary schema to determine the distribution of the grail overthe y-axis, which represents the algorithmic gradient. The relevancy determined the event placement on the y-axis simply by deducting 1 for each incident of "under" (U) and "less" (L); adding 1 for each "more" (M) and "above" (A). The event fields were then set side-by-side from lowest to highest along the x-axis. (I would expect more realistic applications to make use of weighted ULMA values.) Although the data used in the illustration is real, I chose to highlight some interesting events that probably make the data seem a bit fabricated. It would be easier to explain an event position if similar contexts were included in the grail. At the moment, I have no idea why Copper Wristband appears near the middle along with Full Moon; however, there are so few incidents of either, I wouldn't say that the data necessarily means much. The juxtaposition is due to the contextually diverse nature of this particular grail. Perhaps the placement is purely incidental. Yet even incidental placement can be relevant. I would expect "surprises" by nature to emerge where there is the least amount of attention. Perhaps the middle of the grail forms a conceptual edge sometimes occupied by peripheral concerns: it is the most likely place to find neglected, ignored, and overlooked events. So the placement of an event should not be regarded in purely linear terms but rather transpositional.,Probably not obvious in the illustration above is how I deliberately "manipulate" the distribution through intervention. I usually go through many non-successful data events in order to find those that seem reasonably beneficial. I don't necessarily avoid events that appear on the left; but I definitely try to repeat events occupying the right. So we find above a sea of relatively fruitless data events followed by something resembling a pointy mountain. Among events to the left not labelled on the illustration are the following: nightmares, a particular brand of mouthwash, pickles, and hotdogs. (For me, fast food in general appears on the left.) This is not to say that any of these things are "bad" for people since I have no data on other people. I suppose a reasonable question to ask is why I bother keeping such detailed data about myself. Well, the research prototype is not designed to hold data specifically about me; and when I started collecting information about a year ago, it was to determine if the system could handle diverse personal data. In other words, it was an experiment. I never actually meant to continue personal data collection for much longer than a few months. I have since embraced the idea of being a pioneer: so apart from any other future applications, I intend to keep collecting personal data.,Since certain events can have multiple negative and positive consequences to an organization, it stands to reason that intervention should to the greatest extent possible attempt to reduce the number of multiple negative events and increase the number of multiple positive events. I describe this as the "Push Principle." This strategy is quite different from maximizing or minimizing particular outcomes in isolation. Push occurs through the strategic use of grail objects to achieve multifaceted outcomes. Change is not induced or prevented through the use of a single force focused on a particular type of event - a "poke." Rather, less force is applied over an array of events - a "push." I am actually describing a management philosophy or perspective on the allocation or distribution resources. However, I also have a data-science-oriented application. Premised on the idea that a certain number of key events may have contributed to systemic decline, I recently tried to determine the likeliest cause of a condition resembling food poisoning - meaning that it might not be food poisoning - which I personally had to face. Am I making this stuff up just so I can blog about it? Not at all, this is merely curious happenstance in the life of person with liberal eating habits. Although it was a terrible experience, I had enormous interest in the resulting data.,So how does one go about pinpointing the culprits to something like food poisoning? I have a methodology to share. We can thank my food poisoning for the development of this methodology since it did not exist prior. I call it "differential shift analysis" (DSA) - a rather literal term. It represents a way of identifying recent events that may have contributed to systemic impacts. It might be described as a type of shock analysis. (By the way, I'm still working on a relevancy algorithm to characterize shock in a non-systemic context.) The methodology for DSA isn't complicated: it is only necessary to "subtract" an older grail object (scenario normal) from its more recent grail counterpart (scenario abnormal); this results in a differential shift profile that I call the "displacement pattern." I set the events side-by-side from lowest to highest in order to arrive at the pattern. Although I am describing a case of apparent food poisoning, I feel that this very same methodology can be applied to other types of events and phenomena. The illustration below shows what I consider to be a "standard" shift profile (the "dispatch sequence"): if the interval between objects is quite short, there should hardly be any change, resulting in a dispatch where most of the events are near the zero. We are then left with the dip and spike associated with the particular shock incident at the outskirts of the sequence.,The illustration above shows that, at time of posting, I maintained about 800 events (across the x-axis). A relatively small number of these events (dipping below 0 on the y-axis) seem related to the systemic decline that I experienced. Moreover, some events are particularly unrelated to the decline; at least, this is one way to interpret the distribution to the right. My memory in relation to the things that I eat and take on a daily basis is rather unreliable. So I was surprised to find that "battered shrimp" seems particularly unrelated my food poisoning. The same can be said of cheese doodles, milkfish, and oil supplements. The placement of some items is the result of "algorithmic drift"; these are events that satisfy the algorithm in a purely mathematical sense but which don't really fit my actual requirements. Among those items that seem most associated with the decline are the following: a type of frozen dessert; a particular processed meat (indicated by the algorithm to be exceptionally likely); some watermelon; cheddar cheese; and a particular brand-name burger. In the list of new things added to the database recently are mustard and a type of rhythmic exercise meant to stretch the abdominal area. So there are a number of suspicious characters in this line-up.,I was inclined to cancel out the burger since the incubation period for pathogenic bacteria would likely have affected me sooner. Also, the production of a "brand-name" burger involves many levels of quality control. The frozen dessert is something I have had a number of times in the past, and the package is nowhere near the expiration date. If I had to guess, although I can't be certain of course, I would say that the symptoms resembling food poisoning were related to the mustard, processed meat, watermelon, or the abdominal exercise; this is complicated by triple the usual amount of popping on the evening of the illness. Using an approach like this, it almost seems like I have abandoned all reasoning. Actually, reasoning is inescapable. I merely search for signals in the data. The next image involves what I would describe as the "recovery period" after the illness. As a preventative measure, I was very interested in distributing events to help explain my situation. During this time, as indicated on the x-axis, the number of events increased significantly. I placed a separate data series on the chart to show the events that contributed positively to systems both before and after. (The location on the gradient for this second series is based on the "after" figures.) Some events contributed positively before but negatively after the illness. There are other types of patterns such as those events that although positive after the illness nonetheless declined. I leave out these variations for the sake of brevity.,Conceptually speaking, only the contributors around the middle of the displacement pattern are consistently worthwhile. That is to say, the placement is near zero because the contribution to the system was comparable both before and after the illness. However, the contributors to the left and right perhaps owe their placement to some extent on the illness itself. For instance, the displacement would not now be higher on the right had the contribution not been lower prior to the illness. Similarly, the displacement would not now be lower on the left had the contribution not been higher prior to the illness. So the many zones of the displacement pattern offer different insights. Although theory might not exist to explain phenomena prior to its detection, one should be inspired to develop theory at some point thereafter: e.g. perhaps certain events - such as those related to the application of particular therapies - are best associated with recovery rather than maintenance or the treatment of something chronic.,I hope the usefulness of the Push Principle is apparent in the displacement pattern. It is possible to identify systemic shifts premised on the disproportionately significant impacts of relatively few events. Further, if one attempts to analyze the descriptors of the events, theoretically it should be possible to determine the nature of events that seem particularly related to systemic shifts. This methodology creates investigative openings. If something being investigated involves a coherent system - e.g. forensics in relation to a transit system - differential displacement might provide objective guidance supported by the data available. The extent to which particular activities leverage on a system, it might be possible to detect those activities using the same system; this is my general idea on the matter anyways. For example, if a system is designed to behave in particular ways in relation to specific events, then the relevance of these same events to those specific behaviours should be evident using differential displacement. However, in a large organization, there are so many different types of events occurring routinely, detection can be quite challenging using a clean statistical approach. Reality exists in the ghost patterns. I believe that tracking requires an approach attuned to massive amounts of data that possibly contain only loose or garbled systemic connections.,I hope that smart devices containing adaptive algorithms will one day accompany people throughout their lives, helping them to cope with hostile environments and circumstances. I periodically come across articles and blogs that almost seem to suggest, collecting a lot of data is a fruitless endeavour. I actually think that our survival as a species will require us to overcome major hurdles in the future; and our ability to rapidly make effective use of data will emerge as an increasingly central theme. I'm uncertain if my food poisoning example pointed out the following feature about the research prototype: in order to arrive at a grail object, it is necessary for the processing environment to "automatically" sift through systemic constructs, contexts, and data events. (There are just too many different things to handle manually.) I feel that it should be possible to create pre-packaged processing environments for users. I would like these intelligent systems to eventually change along with the bodies of users, their financial resources, lifestyles, relationships, workplace demands, and surrounding risks. I envision "partners for life" perhaps only periodically augmented and refined by diagnostic supercomputers.,In this blog, I pointed out how a system can be used to examine the impact of events and, conversely, how key events can be identified to achieve systemic outcomes. In the example, I make use of differential displacement to support detection of short-term and infrequent phenomena. While I find "systems" mentioned in the literature in a purely conceptual sense, I believe that the potential of systems remains to be fully realized through the use of algorithmic approaches particularly in relation to large amounts of loosely connected data. Algorithmic methodologies involving systems raise the possibility of intelligence, automation, and adaptation over the course of routine data collection. I discussed Push as a means of leveraging on the system to achieve desirable outcomes; this contrasts with the idea of localized intervention oblivious to systemic consequences. I never really explained the title during the main body of the blog, so it is probably ironic that I should do so now near the end. All of this is merely preamble or transition. In about a month, I will be building on some of the ideas introduced here. In particular, I will be discussing the process of contextual selection in the development of systems. I will also be offering what I consider to be a unique data model premised on the persistent interrelationship between participation and disablement, which I believe to be inherent in many data structures. However, in my post in a couple of weeks, I will be writing about software development focused on data science from an entrepreneurial perspective.
This article presents various ways of measuring the popularity or market share of software for analytics including: Alpine, Alteryx, Angoss, C / C++ / C#, BMDP, FICO, IBM SPSS Statistics, IBM SPSS Modeler, InfoCentricity Xeno, Java, JMP, KNIME, Lavastorm, Mathworks?€? MATLAB, Megaputer?€?s PolyAnalyst, Minitab, NCSS, Python, R, RapidMiner, SAS, SAS Enterprise Miner, Salford Predictive Modeler (SPM) etc., SAP KXEN, TIBCO Spotfire, Stata, Statistica, Systat, WEKA / Pentaho.??,I believe that adding new methods in statistical packages, to the point that each package now offers hundreds of functions (dozens of regressions, dozens of classifiers, dozens of time series methods and so on), is a bad idea. Most of these functions are never used. It only confuses the high-level user, and makes these packages not suitable for automated or black-box data science by non-statisticians (engineers, economists). If you really need that level of sophistication and fine-tuning, you are better off writing your own code in Perl, Python, or R or some other programming language.,??is currently working on ,. It consists of producing very few, global methods with few parameters (one method per core problem, e.g. one generic clustering technique, one generic regression technique etc.) with focus on automation (algorithms run in batch mode and/or automatically scheduled), streaming data, black-box data processing by non-statisticians, and ability to process large data while avoiding ,??at the same time. These methods ??are designed for robustness, simplicity and scalability, with minimum accuracy loss over traditional methods, and can be integrated as modules in existing production-mode machine learning applications, large and small. The new methods, initially designed in ,, are in the process of being made easy to implement, with code and explanations provided. It started with ,,??including hypothesis testing. This week it will be about predictive power (a metric for feature selection), followed in september by Hidden Decision Trees blended with Jackknife regression. The results will be presented in an upcoming book, ,.,For another article about software comparison, ,.,This is a very long article written (I believe in 2013) by Robert Muenchen. Read the full version,??,.??The following metrcis are used for software comparison, in Robert's article.

These are the most critical rules that digital publishers should follow, based on our data science studies.,1. ,??,The amount of Google organic traffic that you will get is proportional to the amount of non-Google organic traffic that you get - this is the best Google organic traffic predictor: increase your email newsletter frequency, or membership base, or email channels, or use better segmentation: make sure your email database is segmented by ISP, traffic source and sign-up recency, for email campaign optimization. Do not spam or you will get burned - there's an optimum point; email campaigns is a great way to drive traffic if you don't abuse it or fatigue your mailing lists (make sure you get more quality subscribers than you lose, each month; do some survival/churn analysis; identify poor non-opening subscribers and eliminate them).??If you use Google Analytics, then Google knows how much non-Google organic traffic you get, and Google will deliver free Google searches to your website accordingly: expect Google organic traffic volume to follow your non-Google organic traffic closely with a one week time lag, and to be much smoother and predictible than your non-Google organic traffic. Avoid black-hat SEO, this will eventually gets you penalized by Google.,Another way to boost your share of free Google traffic is writing great content that get re-posted, shared or re-tweeted, ,. Or hire interns or design detection-proof robots (create a great profile for them, get them connected to many influential people) to re-post your great articles on social networks.,You can get much better traffic from Google Adwords (paid keywords) than from Google organic searches , if you do your homework and purchase tons of relevant keywords (,), and break these keywords down in multiple campaigns and ad groups. In particular, with AdWords, you can get,To get decent traffic, you need to purchase keywords that, according to Google, are relevant to your domain. We tried to create a landing page on our website, and Google campaigns, ??to drive traffic for our advertisers that don't do well with our internal non-Google campaigns, but it's not working: Google sends us very little paid traffic, as if they figure out that this traffic is not a great fit for us. There are alternatives such as Facebook advertising, to eliminate this problem.,: I have no connections with Google and I have not been paid by Google to write this article.
This indeed applies to all industries and all products. In short, how do you detect new uses of a declining product (cigars in this case) that could turn them into a good product and revive the industry in question? The tobacco industry has tried to sell outside US (especially Asia) to make up for declining sales in US. They came with e-cigarettes. But they never thought of a tobacco use in a context not associated with addiction.,Here's one: sell cigars that people (non-smokers) will smell rather than smoke. Based on a small experiment, on average women don't like the smell, men do. Smelling a cigar provides an hedonist experience very similar to finishing a great diner with a cognac or (if you are a smoker) a smoked cigar. It comes with several benefits: no smoking, replacing hard liquor (cognac) with an harmless product, and the ability to taste very expensive cigars since they last for a few weeks (as you never intend to smoke them - the cigar will keep its taste for a few weeks). And cigar producers or retailers can now target non-smokers and ex-smokers, a much bigger market. Pipe tobacco (with a sweet smell) could also be used in a similar way, that is, smokelessly.,So how does big data helps discover such new marketing opportunities? Maybe big data is not big enough yet and it's not a data issue. Or maybe if you analyze tweets and posts that talk about cigars, you will find opportunities like this one. Or maybe you can do a survey to identify potential new uses of your product by asking people. After all, this is something done all the time with pharmaceutical drugs: a drug initially designed to cure disease A, but later found very valuable to cure another disease B.??
A case study??,??was attempted by Brohman, 2000 to dig into the process of data warehouse practice and how it shapes the organisation. According to the qualitative analysis from the case study, a model was illustrated named Business Intelligence??,??as shown in Figure 2.,In above shown figure, business intelligence and business value are the main concepts of the model. Business intelligence is the package for??,??whereas business value is the outcome from data warehouse development and practice.??,References:,1)??, , , ,2) , , , 
INFORMS Continuing Education program is designed for analytics professionals who need to solve real-world issues. INFORMS courses provide real take-away value; the skills, tools and methods you can implement immediately at work. ,.,: The full version is always published the following Monday (,). Starred articles are the most recent.
Posted by , in AnalyticsWeek.,Read entire list at 
From BigData-Startups. The infographics below is just one of them.,Here's the list:,View infographics at 
Sometime ago I came across an Excel based dashboard with around 45 tabs, and every tab had around 10 charts/ graphs /tables. That is roughly 450 figures and tables. I could only sympathize with the end audience of that dashboard. Unfortunately, almost every organization is deluged with meaningless reports and dashboards of this kind. I immediately knew I had to write this article on YOU CANalytics to save the end users from the torture these reports can cause. Luckily nobody is reading these reports and for a good reason.,The problem is not with the Excel based dashboards but the clouded thinking on the part of the designers and analysts. Trust me no fancy software (Tableau, Business Objects or QlikView) can rectify the problem with a muddled brain. Hence let me suggest a few strategies to create winning reports and dashboards. The strategies have been framed as titles.,Before I tell you more about the above strategies, let me share with you a piece of memory from my childhood that will serve us well in this journey.,Let us go some 25 years back in time when India was not bombarded 24/7 with more than 500 channels on television. Then it was just one channel ?€? Doordarshan ?€? with less than 8 hours of broadcasting time in a day. Television was a lot calmer then. Doordarshan used to have a short clip about Mahatma Gandhi as filler. The clip was not more than a couple of minutes long with brush strokes leaving a white canvas with the doodle similar to the one shown adjacent. In the background a man?€?s calm voice recites the following ?€? , That is when I learned that one could depict the Mahatma with just a couple of brush strokes. I believe we need to discover simplicity within us and around to create relevant and winning dashboards. The following are my 7 tips for the same.,?€?[It's] the economy, stupid?€? was the slogan Bill Clinton effectively used in the US presidential election in 1992 against??George H. W. Bush (Dubya Bush?€?s father). Bush?€?s campaign was focusing on the Iraq war and Saddam while Clinton hit the bulls eye by focusing on the right issue and won the election by a huge margin. I will modify the phrase a bit for creating winning reports by focusing on the single most important factor ?€? ?€?it?€?s the question, stupid?€?.,For me charts, graphs, analysis or even data come much later. I think the foremost thing of importance is the question these tools are trying to answer. Every dashboard, report, presentation, analysis or model has a business question which it tries to answer. I would recommend that you write down the question(s) in plain English before starting with analysis without assuming that everybody knows the question(s). After that everything is just an attempt to answer the question using the best possible medium / tools.,Okay if you don?€?t know them already let me introduce the end user of your dashboard, analysis or presentation to you. They are overloaded with information and a plethora of pending issues. To make things worse they are perpetually distracted with the emails on their smartphones. They are looking for simplicity but find it difficult to think straight within the chaos. Your analysis, report or dashboard could bring some simplicity in their work life, but they will find it difficult to see it at first. This will make your job really tough. But you cannot afford to get flustered by the surrounding, otherwise you will just add to yet another useless analysis or dashboard.,Now that you know your end audience better, you will also appreciate that they may at times ask questions that reflect their confusion during the analysis phase. This is when you need to guide them, question their thought process and if you disagree, say no to their muddled ideas. Be polite but firm while doing so.,We all love stories. Presentations, reports and dashboards are just visual stories. The opening chapter of this story is of course the question(s). Once the questions are framed clearly, a good practice is to think of answering the questions like a story. The graphs, tables and charts are different characters in this story and they will help in moving the story forward. A story is nothing but a the logical progression of thought. For instance, a guest-appearance by Jabba-the-Hutt in a Shakespearean tragedy is unacceptable, similarly an unwanted chart or graph is enough to confuse your audience and make them disengaged. Use your characters wisely and arrange them in the logical order of the story.,The great movies cannot be made without great editing. You need to be completely dispassionate and ruthless while editing your reports and dashboards. Remember your audience is not looking for a 3+ hours long ?€?Schindler?€?s List?€? movie but a??succinct trailer. This is your last chance to remove those unwanted characters (figures and tables).?? The hardest part is when you have to cut the role of your favorite actor ?€? the analysis you have spent several sleepless nights working on but it does not fit well with your story. Keep your scissors sharp as you will need them.,Mark Twain, one of the greatest writers once said: , The idea captures the gist of one of the essential principles of designing dashboards and reports ?€? less is often more!,I am a huge fan of Japanese minimalist art and design. The primary philosophy behind minimalism is to say more with fewer elements ?€? like the depiction of Mahatma Gandhi displayed above or the adjacent picture. Similarly I believe for dashboards the graphics and charts should be simple and to the point. Simplicity goes a long way when it comes to designing dashboard. Though trust me, this is a really difficult and effort-intensive path.,The following is a case from the Emergency Room (ER) of Cook County Hospital in Chicago from the mid-1990s. Being a public hospital they serve poor people without health insurance.?? The ER gets a quarter million emergency patients per year ?€? this ensures a perpetual chaos-like situation for the doctors and medical staff to handle. The excessive inpatient rate also guarantees a consistent shortage in number of hospital beds. A patient walking in with chest pain could be a case of impending heart-attack or (not so dangerous) indigestion. This first case needs serious medical attention and the second a bit of reassurance and a bus ticket back home. The problem statement for the doctors and medical staff at Cook County hospital was simple: ,They were not looking for a fancy dashboard or sleek reports but a simple rule to help them make this decision fast. A competent bunch of statisticians (physicists actually) helped the hospital solve this problem. A predictive model suggested answering the following 3 ?€?urgent risk factors?€? along with the ECG report:,What transpires was a simple decision tree that anyone could use to direct the patient to the appropriate treatment. Insights that facilitate action or actionable insight is exactly what the decision makers are looking for. Any dashboard, report or analysis that delivers this will never go unnoticed and is a winner.,I have done my bit by sharing the tips I feel will make people take dashboards more seriously. I believe, business metrics and KPIs for your dashboard are going to pop out naturally once you follow the above steps. Let me know if you have some suggestions for the above list, post your comments right below this.
I tend to examine the different roles played by data. For instance, when I work on computer code, I often ask myself what the presence of data is meant to accomplish. Sometimes the analysis is not at all straightforward or simple. In society and organizations, people exist and persist in the records as data. The data survives even as employees come and go. I therefore consider it important to regard the data and its environment as a system in itself, something that has a life all of its own. The data exists beyond the individual sentiments of managers, administrators, and customers. Data contributes to the process of reasoning and the expansion of knowledge. Consequently, I find it a bit disturbing to read about ?€?big data?€? often in rather vague terms. I notice that discussions tend to lack a theoretical basis. In this blog, I will attempt to provide a conceptual foundation for big data as it relates to organizations. However, the underlying purpose of the blog is really just to stimulate discussion. I expect that some readers will immediately disagree with some of my points and offer their own perspectives. In the end, I am sure that the foundation for any field of study develops over time through collective and collaborative efforts.,Perhaps when the boiler was first marketed, some companies looked at the technology and thought, why would any organization need one of those things? Can the ability to heat fluid and move it to different places be useful? Think about the amazing feats of production made possible by being able to have many people comfortably occupying a shared space full of capital resources regardless of the time of year. Discussions about big data tend to suffer from the irony of being marketed in small terms; this makes it possible to dismiss a rather big idea by its apparent lack of congruence to isolated applications. Yet I notice no shortage of vague generalities. ?€?You say this boiler will change the way we do business, but I don?€?t understand how it might help me sell more hogs.?€? I consider the following a worthwhile approach: ??start with a conceptual foundation and then build up a case. Trying to sell something complicated without any firm reference points can be problematic: generality plus peculiarity equals implausibility.,In order for me to offer a setting for big data, it is first necessary to examine the role of data in organizations. I have chosen to approach the challenge of establishing placement through systems theory. But rather than chop up the usual liver, I will go ahead and add my spin. Although the systems model seems to explain the flow of materials through organizations, it is also possible to interpret the model in relation to the flow of instructions, information, and data. In this blog, I will formally separate the structural complexities of an organization from its informational complexities. Many people are familiar with the basic components of systems theory. I want to point out that the theory holds the idea of progression or direction: the movement is from inputs to outputs and not the reverse. Direction and flow are particularly evident in organizations once the delegation of authority is assumed: organizational control tends to emanate from ?€?the top.?€? Just to make my point, I made a minor addition to the systems model diagram.,The so-called ?€?top?€? in relation to my flow diagram is at the left. I call the emanation of authority ?€?projection.?€? Projection creates data by imposing specific requirements that can then be used to determine degrees of adherence. I call these requirements the metrics of criteria. There is a saying that goes, ?€?What gets measured gets managed.?€? I suggest the truth is slightly different: ?€?Managers measure what they want to manage.?€? My sentence is probably rather awkward to say; but anyways this is not a blog on English grammar. A long time ago in simple work environments, oversight and control were probably more direct. Control meant giving instructions and remaining present long enough to ensure adherence. In most contemporary work environments, there are still work orders and instructions. But the evaluation of work occurs through the use of criteria. The implementation of criteria offers the workplace tests or standards that give power to authority. Consequently, ensuring compliance is a matter of applying standards of conduct or behaviour and then generating data, assuming that the organization hopes to eventually manage what it measures. Data is the product of projection - among other things.,Criteria can be found in dashboards, scorecards, and many forms of performance evaluations; these may or may not be well-aligned with the broader business objectives of the organization. I hope most readers would recognize that although there is quite a bit of data involved in management regimes, in fact the need for ?€?big data?€? is fairly limited. It should come as no surprise how big data might seem baffling and be met with considerable skepticism in particular organizational settings. Specifically, operations where there is little employee participation in decision-making processes do not require sophisticated management approaches. I would go so far as to say that these types of organizations are sometimes specifically designed to minimize the number of managers. Procedures and processes might be rather entrenched. So there is little need for intervention or creative problem-solving. It is not my intention to make any kind of criticism but simply to highlight the limited data requirements of more rigid organizational constructs. When conditions are static, there are maintenance rather than growth opportunities in relation to data products and services. Organizations would probably need protected markets to persist for long in such circumstances.,Although it is probably not an enlightened management strategy, it is possible for an organization to project authority irrespective of circumstances or consequences. For instance, a company can manufacture products that nobody will buy. Managers can order employees to flap their arms like chickens, which I think would be dreadfully comical. Why flap arms? In Canada, a particular province required everybody in its fire department to achieve specific aerobic outcomes as a condition of employment. I am not saying that this is the same as imitating chickens. I am focused on how the criteria can be disconnected from reality. The Supreme Court found the logic of such criteria questionable for somebody maybe working in dispatch. A nation can, as part of a back-to-work program with good intentions, hire people to dig holes and others to fill the same holes, the idea being to man-handle the data to agree with the criteria for progress. We still encounter situations like coal mining accidents and factory fires perhaps in the textile industry when workers remain at their duties despite extremely hazardous conditions; the performance criteria is not necessarily connected to worker safety. Suffice it to say, projection can occur in an entirely desensitized organization; and companies routinely find themselves sliding into competitive oblivion due to their impairments as I will explain in greater detail shortly.,I added some megaphones to the diagram. While the identities of the other parties are probably debatable, I would suggest that at the far left we have ?€?the market?€? (those that might buy); at the far right ?€?the clients?€? (those that have bought or tend to buy). This is something of an oversimplification. The market occupies both sides. Those that have bought can easily influence those that might buy. However, those that are most likely to complain have already bought; this is why their megaphone is pointing back at the company. In traditional systems theory, the idea of ?€?feedback?€? might be compared to a response by the market to the behaviour of an organization. Government regulators can often be found at either side of the organization: at the left to ensure administrative compliance to work standards; at the right to protect consumers and citizens once something unacceptable happens. These are fairly hostile conditions for an organization that has projection as its main operational strategy: thus the ?€?!?!?€?,Except in the case of a tight military operation with certain specific objectives, projection irrespective of consequence would probably contribute to disappointing results. Also from the standpoint of data, projection offers just a certain type of data. The only real measurement is level of compliance or adherence to expectations. It is not necessary for the organization to understand anything about its environment since there is no need to adapt beyond the original design parameters. I call this a ?€?survivorship tactic?€? not to be confused with a survival tactic. If we throw seeds over different areas of land, there is limited adaptation to the circumstances. The seeds will grow on fertile soil and exhibit different degrees of success on less fertile soil. That is all they do. It is a ?€?live or die?€? strategy. Similarly, an organization with a mission to make pencils might do well in a place where pencils are quite scarce but there is an abundance of paper. So there is an information ceiling in this setting that greatly reduces the need for big data. One does not need massive amounts of data to ensure that people follow specific instructions. Nor would following the instructions necessarily lead to organizational success. Even in situations of total compliance, there might be absolute failure. Again, this is a ?€?live or die?€? setting - natural selection rather than internal adaptation.,I covered projection. I actually have two more major data flows to cover. But I am going to skip the one in the middle and return to it later because it?€?s easier to do so in terms of the diagrams. I apologize for the confusion. So we return to the organization in its hostile operating environment, generating data through its metrics of criteria. We hear wailing and gnashing of teeth. ?€?This is ridiculous. I need some kind of process to satisfy the market, deal with consumer complaints, the regulators, activists, our suppliers, wholesalers, and retailer distributors.?€? It is actually common in stable systems for another player to be added to the organizational setting. The term ?€?player?€? is rather embodied, suggesting perhaps that I actually mean a body; but actually I am referring to the part or role played. I describe playing this role as ?€?articulation.?€? Specifically, ?€?internal articulation?€? is where the system attempts to self-regulate or adjust in order to prevent aspects of insulation or isolation (desensitization). Some organizations will attempt to anticipate problems before they occur; or some effort will be made to incorporate experiences into future decision-making.,A simple example of internal articulation can be found in heating systems. A furnace can heat a house even in the summer during the hottest days. A furnace makes use of a special device to ensure comfortable temperatures: a thermostat is an important part of a heating system although it usually generates no heat itself. A thermostat does not ?€?control?€? a heating system the same way a home owner controls a thermostat; but rather the thermostat fulfills a function that is an integral part of the system. In a production environment, which might be able to produce continuously regardless of demand or what comes out of the system, the role of articulation is sometimes played by those in quality control and assurance; customer service; marketing; and logistics. The extent of articulation depends on the intended scope. If a company tries to sell a product that nobody wants; that is deficient in some manner; that cannot be feasibly brought to its market; then projection actually becomes almost pathological in an organizational sense. I believe that articulators play an important role in adaptation. Articulation can generate much more data than projection because of the different types of metrics that are often needed to make an organization sensitive to its environment.,The data generated by articulation results from the metrics of phenomena. In my own programming projects, these metrics are conveyed through complex data structures. Theoretically as I already pointed out, it is possible for an organization to operate using only methods of projection. However, once sales start to decline, the data system might provide few if any insights on the reasons behind the slide, assuming that the system is not meant to articulate information about the environment. If an egg producer cannot successfully and consistently bring products to market, retailers might find a more predictable and reliable supplier. The data system can give many indications of problems before radical decisions are necessary: for instance, unusually long delivery times using particular routes under different road conditions. These are useful metrics. I believe that articulation is poorly developed from a technological standpoint because it is so much more complicated and difficult to achieve than projection. Simple and perhaps even the simplest organizations can implement a rigid delegation model. Complex enabling structures on the other hand require a high level of organizational sophistication. I feel that articulation offers the best strategic placement for big data.,There are certain common terms that are tossed about in the community, and perhaps I have been responsible for some of the tossing. For the sake of consistency in my own posts, I want to explain that projection and the metrics of criteria are part of the ?€?prescriptive?€? regime of organizations. In contrast, articulation and the metrics of phenomena help form the ?€?descriptive?€? regime. Description is ironic by the simplicity that it implies; it suggests an almost mechanical process of describing things. I don?€?t really cover the ?€?predictive?€? aspects of organizations; this is to say, I leave out the term in my diagrams. So my colleagues are ahead of me given my ambivalence to prediction. Further, I immediately place myself at a disadvantage by approaching as my main area of discourse something that many people dismiss as rather straightforward. It?€?s been said that Eden is protected by a burning sword, without which paradise would become quite a tourism destination or condo-development project. However, it would be far too easy to locate Eden if this were true given the rarity of burning swords. ??Some people would go just to see this peculiar burning sword. ??I therefore study obvious things that are too perplexing to justify further consideration.,I am only just now starting to write about data-embodiment, which is my personal approach to articulation. In criteria, it would be fair to say that measurements tend to be rather specific. It is easy to determine what belongs and doesn't belong. Absenteeism is about lack of presence. So a person missing from the workplace could be recorded as absent. The exchange of goods for money leads to sales figures. In phenomena using data-embodiment, the measurements don?€?t behave in this manner at all. Everything is related to sales albeit at different levels of relevance. In other words, everything is related and connected but not necessarily well. There never has to be a clear separation. A data-embodiment approach tends to make use of large amounts of data - potentially all of it - all of the data - and possibly more massive amounts of assertions of data. For instance, PERSON and BRIEFCASE might represent specific isolated events. But it is possible to assert the following given the types of people that carry briefcases: SUSPECTED_SALESPERSON or SUSPECTED_TERRORIST. Temporal and spatial issues create challenges because the act of description might be for transient phenomena. Interconnections determine the reasonableness of recognizing things as existent. I have described data-embodiment as an ?€?ecosystemic approach?€? since no effort is made to examine things in isolation; it is necessary to handle the data in an interconnected and complex state. This is different from focusing on specific aspects of isolated things in standard ways: e.g. monthly sales.,The final informational flow is practically a side-order of fries compared to projection and articulation. I call the third flow ?€?direction.?€? Projection is an aspect of design. Articulation is an aspect of self-regulation. Direction reflects the intended routine of an organization; it likewise generates data. Good examples of direction can be found in the financial industry: large amounts of data are produced over the course of operations as an aspect of the day-to-day business. So if the megaphones are removed from the left and right of the organizational construct, business can still take place as usual for a period of time under normal conditions. Direction generates accounts, transactions, logs, registries, and records. I feel that when most people discuss big data, they actually mean data from how organizations direct their day-to-day routines. Big data would therefore be big in a literal sense due to the amount of data; but beyond quantity, the data itself creates no unusual demands on existing systems. The illustration below contains all of the points raised so far plus a few that must be set aside. I will try to discuss organizational domains and pathologies in later blogs.,I feel that organizations that are destined to do well with big data are those that direct significant resources at articulation. ??This is something of a polemic statement from a management perspective: ??it tends to support the rise of complex operations, which creates a need for sophisticated management approaches and therefore a need for managers. ??Nonetheless, it hardly makes sense to collect much data if there are few plans to ever use it. ??Big data requires complexity. ??So do organizations that employ adaptive strategies. ??I agree that simple and efficient operations sound lovely as far as frilly preconceptions go, except that other companies will always be able to offer the same products and services faster and less expensively. ??Efficiency is no substitute for growth. ??Even in completely protected markets, at some point saturation will be reached; beyond this, expansion will depend greatly on environmental sensitivity. ??Articulation supports adaptation by making individual organizations more connected to their unique environments and specific circumstances.
This edition features more than 15 resources and articles from various outlets.
The prime benefit of data warehousing is simplicity. The presentation of data in data warehousing is a single image. This single image is made by collecting data from different department of the organisation. Due to this, time for production and operation of data reduces and thus simplifies the decision making as well. This reduction of time to access data also leads to increase in production and effectiveness. Data warehouse will also help to enhance the function of operational systems. It means that there won?€?t be any hamper in the regular process of the system even if user will operate any other operation rather than normal one. As data warehouse has technical infrastructure, it is flexible and scalable in nature and supports for both technical and business changeable environment. It gathers raw data as well as manages the flow of information which helps the user to make decisions because of its appropriate information (Furlow, 2001).,Business intelligence is responsible for transcribing raw data into information and knowledge. This will create an effective decision making and strategic process. For any business, value of business intelligence is primarily discussed as an information process which provides some basis for making some basic changes in a specific venture such as introducing new collaboration, making new marketplace, acquiring new clients, offering new products in the market (Olszak and Ziemba, 2007).,References:,1)??Furlow, G. (2001).The Case for Building a Data Warehouse. Data Warehousing.?? , pp. 31-34.,2)??, , 
????,alks about a framework to google analytics to make a strategy case.,[,],??[??, ],Data science central is almost a repository of all business analytics/big data blogs. This has its own pros and cons. If you have plenty of time, you can rest assured that you will find something you will really like to read. But, you really have to search for the right articles. I personally am a big fan of the main author Vincent Granville's articles. I always take a glance on the weekly digest of this blog and go into article if it's heading excites me.,Even though I am a content editor of Analytics Vidhya, I still love reading to some very insightful articles written by Kunal Jain on the blog. The focus of the website is on development of analytical skills and collecting analytic industry best practices using ??case studies (,)??, enlightening disucssion on some specific modelling techniques and inspiring interview from CEO of Fractal analytics Mr. Srikanth (,) and top Kaggler Mr. Steve(,) .,FT does not directly links to business analytics. But to be a true analyst you need to reflexive on the latest market trends and business scenarios. FT regularly covers the impact big data has on business and how it shapes up some of the key business strategies. One the the recent article publish??,??demonstrates the same.,Reading blogs not only makes you aware of the latest trends and techniques of the industry but also gives you a new perspective to think over popular issues. Do you agree with this statement? We will like to know other blogs which can be useful to analytics professionals.
Very interesting article published by the ,. The picture below compares computer science with statistical science - before (I guess the early nineties) versus now. The column labeled CS3 (CS for Computer Science) represents modern computer science, actually this is data science. What's left in statistics is for the reader to guess, I suppose.
The purpose of this article is to demonstrate how the practical Data Scientist can implement??a??Locality Sensitive Hashing??system from start to finish in order to drastically reduce the search time typically required in high dimensional spaces when finding similar items. ??Locality Sensitive Hashing accomplishes this efficiency by exponentially reducing the amount of data required for storage when collecting features for comparison between similar item sets. ??In other words, Locality Sensitive Hashing successfully reduces a high dimensional feature space while still retaining a random permutation of relevant features which research has shown can be used between data sets to determine an accurate approximation of Jaccard similarity [2,3]., The concept of Locality Sensitive Hashing has been around for some time now with publications dating back as far as 1999 [1] exploring its use for breaking the curse of??dimensionality in nearest neighbor query problems. ??Since this time various applications of Locality Sensitive Hashing have been making appearances in academic publications all over the world. ??Even very successful technology companies like Google have published improved LSH algorithms [2]??using a consistent weighted sampling method??"where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity" [2]. ??In fact, college textbooks produced from some of America's most prestigious universities like Stanford now include entire chapters dedicated to finding similar items using Locality Sensitive Hashing [3].,However, well over ten years after the first publication, Data Scientists are still hard pressed to find practical implementations of Locality Sensitive Hashing systems which demonstrate why this concept is??useful in "big data" applications, or how one would go about creating such a system for finding similar items., The exponential growth of data over the past twenty years has now created??many instances where collecting and retaining all of the relevant information??for finding similar items is not feasible. ??First, consider indexing all the web pages in existence for the purpose of creating a new webpage search engine. ??Making a copy of every single web page's displayed text would not be practical for a number of reasons. ??The disk space alone required for such a task would be gigantic, and searches against such a large amount of data would prove very inefficient when considering the expectations of the current web user.,Next, we will consider one additional??form of unstructured data which is not quite as obvious... ??Let us now consider a gene sequence. ??Imagine breaking apart a gene sequence's text into smaller chunks for the purposes of machine learning and finding other similar sequences. ??Perhaps you are trying to identify the unknown??taxonomy hierarchy of a ??gene sequence, or you are looking at partitions of an entire human genome to rapidly find segments which may contain high similarity to a known genetic mutation. ??Even if we created "smaller chunks" of only 100 characters in length, starting at each position within the entire gene sequence's text, a gene sequence containing only 4 unique characters A, C, T, and G could??generate 10^80 possible unique values. ??On the surface, this may not sound like a "big data" problem. ??However, we could all agree that even a reduced version of such a model would not fit on a lap top for use in a remote village in Africa.,Locality Sensitive Hashing can be used to address both of the challenges described above. ??It is a technique for fitting very big feature spaces into unusually small places. ??Likewise even smaller feature spaces can also benefit from the use of Locality Sensitive Hashing by drastically reducing required search times and disk space requirements. Instead of storing and searching against all available raw data or even random samples of all raw data, we can use LSH techniques to create very compact signatures which replace storing all of the features typically required for such searches. ??For example, in the case of displayed webpage text, all displayed text tokens sampled from a document now become a small collection of ??integers which will be stored and used for all subsequent similar webpage??comparisons.,Using the signatures produced by??Locality Sensitive Hashing exponentially reduces both storage space and processing time requirements for similar item searches., A form of??Locality Sensitive Hashing called Minhashing reduces feature space size using a family of random hashing functions to hash each individual piece of raw input data retaining only the minimum values produced by each unique hashing function.,Wait a minute...? ??What did he just say...? ??What does this actually mean...?,Let us start with a very practical example. ??I am using this next example for simplicity only. ??I do not recommend that it is the best way to go about comparing text documents for similarity... ??I have written one article with text tokenization examples using ,??[4] style processing and another using??,??[5] which provide additional details on this topic. ??For this example however, imagine that we are tokenizing documents into individual words or other token forms by whatever process we use. ??If we wanted to compare our documents for similarity using this strategy, we would have to maintain a collection of all words produced during tokenization by document, and the frequency that each word occurred as well (for frequency weighted calculations). ??This "word" collection would??quickly grow in size as the number of documents increased. ??In addition, the average length of documents would also impact the size of this collection. ??In this example our "feature space" size is dictated by the number of unique words we encounter, the number of unique documents we process, and the average length of each document. ??Using the Minhashing process, the "feature space" described above could be drastically reduced both shrinking the size of our unique "word" collection and the time required to perform document similarity searches against it., What this means is that given a collection of 300 unique hashing functions, a 2500 word document now becomes a minimum hash signature containing only 300 integer values which represent a random permutation of all the 2500 words which were originally contained within the document. ??Integer values are not only typically smaller than words, but the??total number of items representing the document is now??magnitudes of order smaller than its original form., The Jaccard similarity between two sets S and T is calculated by dividing the intersection of the sets S and T by the union of sets S and T:,However, the??Jaccard similarity , can simply be approximated??by intersecting the values between two minhash signatures [3]. ??For additional explanation and reading on this topic see section 3.3.3 pg. 80-81 in [3]. ??To state this in layman's terms, two minhash signatures including 300 minimum hash values from 300 unique hashing functions with 300 matching values between both signatures would be 100% similar while two sets with 150 matching out of 300 total values would be only 50% similar., Creating a family of ,unique hashing functions may sound complicated. ??However, I was able to create the following function in C# using only a couple of lines of code. ??John Skeet's post [6] explains the logic behind this approach in more detail, although my version has been slightly modified to seed each hash function call with two random numbers. ??Using two random numbers in this way decreases the overall chance of collision and also ensures that the chances of generating two identical random hashing functions would be vary rare. ??In fact, potential for this event can be eliminated altogether by using a hashset to ensure that only unique random number seeds are selected. ??It is also important to note that any data type could be used for??hashing purposes with the addition of a C# generic input type for the inputData input parameter.,The hashing method used above is an ,??[7] which uses XOR bit shifting to create the seeded hash values. ??This hashing method has??a very low chance of collision. ??While it works exceptionally well for demonstration purposes, it may be possible to achieve more accurate LSH search results by choosing a hashing method which has a higher probability for producing collisions between??tokens with similar values. ??In practice, this is sometimes accomplished by creating a thin layer process directly before hash execution which intentionally converts similar tokens within a given threshold to the same value prior to hashing. ??This approach is left as an independent exercise for the reader.,Using the LSHHash() function above, any number of unique hashing functions can be created by simply selecting and saving random number pairs to be used as seed values. ??For example, to create a family of 300 unique random hashing functions, the code below can be used to select 300 random number seed pairs. ??The 300 seed pairs are always used in the exact same order to produce minhash signature values during the minhashing process. ??For high performance applications, random number seed hash codes could also be saved in order to avoid calculating the seed's hash code using the GetHashCode() method each time the function is called. ??In this scenario, the actual hash code value for the seedOne and seedTwo input parameters would be passed into the function instead of the random seed's original value.,Each pair of hash seeds above is saved in a minhash array. ??The length of this array is always equal to the minhash signature's length. ??For instance, when hashing function number one is needed, the seed pair from position zero within the minhashes array is provided as input to the LSHHash() function described above., Once a family of random hashing functions has been created, minhash signatures can quickly be generated using any collection of input data. ??The following function demonstrates a minhash signature being created using a collection of integers. ??However, since a hash code can be generated using??any data type, the input data could just as easily be a collection of strings, objects, or even binary data containing videos or images. ??This particular characteristic makes minhashing useful for application in a number of different feature reduction scenarios., ,The getMinHashSignature() function takes a collection of any number of integers as input, and then each integer??within the collection is hashed by each unique hashing function used to create the minhash signature. ??Duplicate integer values are also skipped as we can be certain that they will not result in the production of a new minimum hash value. ?? Once again, this collection of integers could just as easily be words pulled from a webpage, gene sequence tokens, objects, or any other type of input data. ??It is also important to notice that this is where the feature space reduction takes place., In the example application for this article, we generate 10 collections containing up to 100,000 integers within each collection. ??The example application creates a single integer collection called "the query" which is then compared to each of the 10 integer??collections previously mentioned which we will now call "the documents". ??The length of individual collections have??been randomly determined to better mimic real world similar item searches. ??First, the Jaccard similarity is calculated between "the query" and "the documents" collections??as a similarity benchmark. ??Next, "the query" and "the document" collections are all minhashed to create one minhash signature??for each??collection. ??After minhashing occurs, only 400??minimum hash values remain within??each??collection of integers resulting in an exponential feature space reduction. ?? Finally, the Jaccard similarity is calculated once again for comparison. ??The results are shown in the figure below.,When using a minimum hash signature including 400 unique hashing functions, the Locality Sensitive Hashing process retains both the highest and lowest scoring document rankings. ?? Nine similarity values are less than 3% in difference after minhashing occurs with a single document's score??(#8) showing a 5% difference. ??Increasing the number of hashing functions used will also create more accurate minhash similarity approximations, if required. ??The total number of collection items??prior to minhashing includes 67,925 integers. ??After minhashing, this count is reduced to only 4,400 items which includes 400 integer values for each of the 11 total collections which were minhashed. ??Since??the example application generates new collections using random numbers, similar results can consistently be seen when executing the program repeatedly against the different random number collections created., Locality Sensitive Hashing offers the opportunity for exponential feature space reduction when creating a similar item search system. ??Both large and small systems can benefit from LSH processing since reduced feature spaces offer dramatic reductions in overall search times and disk space requirements. ??When reviewing the empirical benchmarks presented above, two feature spaces can be seen producing comparable Jaccard similarity results while the minhashed feature space represents only around 6% of the original feature space's total size.
?€?If it ain?€?t broke don?€?t fix it.?€?,Were that remark directed at government at any level for any function the response would be predictable - could anything be more broke than government. Probably the f-uped conjunction would work its way into most responses. It?€?s hard to believe that anyone within or associated with government could react differently, even if their outward response were subdued.,Just experiment with it. Think of any of several governmental functions such as military ventures, Social Security, Health, Medicare and Medicaid, mass transportation, highway infrastructure, Education, campaign financing, economic policy or census taking. If you think one step beyond any of these titles you will be inextricably drawn toward bizarre examples of government policy.,Clearly, government can be improved upon and yet government does not know how to use data. Rarely, will you find government employees applying their own data to every day government functions. I make an exception for data collecting and distribution which are not analyses toward policy ends. (O.K., the CPI and other indices are analyses for a purpose. How are the public releases going?) The present state of affairs is even more ironic. I refer, of course, to government?€?s use of contractors to analyze their data. Every such attempt leads to unsatisfactory results, unintended consequences and often to the abridgement of constitutional rights.,Government isn?€?t stupid. They respond in a reasonable fashion ?€? don?€?t touch it, leave data alone. Please correct me if I?€?m wrong. Only persons collecting a government pension outside of academia need respond. Do any of you apply supervised machine learning in your present duties? I?€?m not surprised; but why not? I guess I know the answer.
As the size of the database grows database performance becomes critical. Automation is a growing focus for data center operators facing increasingly complex environments. Database administration is complex, repetitive and time consuming. DBAs have to work long hours during off hours downtime. The outage of database costs heavily to the companies and affect their repute.,Shopping engines and online shopping places are highly dependent on database performance. Slower application performance hugely impacts customers. On the internet a visitor doesn?€?t wait for the page to be opened and buy the item, instead he continuously visits web pages and leaves the slower pages. Slower pages annoyed the visitor and the visitor goes to competitor page and buys the same item at same or even at a fewer price.,Research shows that an average number of hosts that an internet browser has to call in order to fully load a webpage is around 10. Anyone of them, at any point in time, in any geography may become slower enough that a visitor decides to leave the page. This in turn, negatively impacts sales. In addition, it is the website owner that takes the hit to brand and reputation.?? ??,As a solution to the problem companies have application performance management tools in place. Some database comes with their own database performance management tools however companies are focusing on GUI based performance management tools it is easy to work with these tools without DBA involvement. These tools are designed to be used on servers that are running the most critical applications that need high level visibility into performance metrics. ????,Some external tools (for instance High Load Capture) help perform root cause analysis of Oracle, DB2, SQL server, MySQL, Sybase and MongoDB database centric multi-tiered applications.??
I was often the lone wolf among my peers in university because I supported a prominent place in society for corporations and an important social role for capital. I questioned whether the directors and executives of companies entered into boardrooms really intending to ?€?oppress?€? people such as minorities and people with disabilities. Did they deliberately make bathrooms inaccessible to people in wheelchairs perhaps to advance their preconceptions of who gets to go to the bathroom, I pondered aloud. Well, there are certainly individuals in companies with attitudes that might be blamed for certain unfortunate events. There might also be a corporate culture of insensitivity. However, I emphasize the importance of data systems in decision-making processes. When decisions seem to be disassociated from reality, I consider it worthwhile to examine the data system, environment, and organizations setting. I find that disassociated decision-making is a routine occurrence. A company will produce something that people will stop buying or not buy. I propose here that ?€?control?€? even among companies is something evasive due to the absence of sophistication in their data systems. In this blog, the system that I will be addressing is the data itself, which might not appear to be much of a system on the surface. This is the whole point. The data can serve as much more of system to enable greater levels of control. It must have the ability to capture reality in a greater number of dimensions relevant to organizations.,The insulation of reasoning from the outside world is comparable to alcoholism or drug impairment; this provides a synthetic or artificial form of control. It is a worthwhile struggle to overcome such adverse conditions. However, I would suggest that it is actually technically difficult or challenging to arrive at options and alternatives; to give hope of efficacy in terms of different courses of action. So I would like to share some of my own efforts and offer different ideas to others in the community. I don?€?t approach this as an authority but just somebody who has walked down nearby paths in somewhat different directions. About a year ago, I wrote a short program to add up events and provide me with daily running balances. The development of this program has been rapid and unexpected, certainly beyond the scope of my original plans. Rather than write about the program itself, I intend to cover the some of the underlying concepts behind it. Still, I will present some files from the prototype to substantiate and reinforce the ideas. My main focus in this blog is data-embodiment. I have used this term in the past albeit without design information. I consider the conceptual structures behind the coding broadly applicable for those developing their own systems; as such, an examination not so much of coding but design might help others on their journey. My prototype is called ?€?Tendril.?€? At this time it is written only in the Java programming language. I will only mention it a few times in this blog.,An ?€?event?€? is something relevant that is worth recording. For instance, a charity might consider donations important: along with the amount, it would be useful to know when donations were made, by whom, from where, and perhaps the fund-raising circumstances. Usually when we consider data, we do not distinguish between what we can control from what we want to control; instead, we retain considerable amounts of information on details that we cannot control. However, even in the case of a charity where control over operational outcomes might not be the main consideration, it can still be useful to distinguish between the level of giving (the context) and the setting in which those contributions occur (the events). It is simply a bit confusing delineating between events and context. I?€?m going to set aside the question of separation since this is really a matter of strategy and judgement; moreover, just to immediately confuse the issue, a context can be made into an event; of course an event can be made into a context; and all sorts of non-control factors can be distributed as events.,In the real-life example from Tendril to be covered shortly, the ?€?events?€? include my choice of driving routes, weather, and road conditions. The ?€?context?€? is how quickly I reach my destination. By and large, there is good conceptual separation between the events that can be controlled (the choices made during the drive) and the context in which control occurs (the need to minimize the duration of the drive). The least logical choice for a context would be weather since there is no hope of me influencing it (that is to say, avoiding it or making choices to alter it). However, a person can make driving choices in light of weather in order to shorten a drive. The diagram to follow shows the connection between events and context. Another term that I will be using for events is the ?€?body of data?€? or just ?€?body?€?; ?€?head of data?€? or ?€?head?€? for the context. I will be lopping off terms such as decapitated, headless, and disembodied data. In other blogs, I already made use of the term ?€?data-embodiment,?€? by which I mean the formation of complex structures that include events and their different contexts. Data-embodiment is about the development of informational constructs.,I can?€?t take credit for structure shown above, only perhaps for its specific portrayal. The structure already exists in the data science community: ?€?Are you satisfied with this product? Please help us understand why by checking the points that apply.?€? Customer satisfaction can be regarded as a type of context: organizations hope to make customers satisfied through particular events. The ?€?base-structure?€? for embodiment can therefore be found in customer satisfaction surveys. It can also be found in checklists: for the achievement of an acceptable standard (the context), it might be necessary for specific conditions (the events) to exist. My conceptualization of embodiment is meant to accommodate massive amounts of data; however, much less data can be handled and interpreted in a similar fashion. It is possible to have just a single event and context, making conflation likely for those easily tempted by simplicity. It is easy to confuse the event of social membership with context of contributing to society; driving a car with getting to work; having friends to partying. I actually recommend that people practice bifurcation to assist in the development of critical thinking; it might also contribute to effective base-structuring.,In the next illustration, I set a number of contexts side-by-side. The purpose of doing so is to establish progression. I find that the idea of ?€?progression?€? (not to be confused with progress) often requires some thought. A simple example of algorithmic progression is duration: in such a scenario, different time ranges can serve as contexts. Some might say, defining time ranges is hardly algorithmic, which is quite true. The definition of time ranges is not progression. Having a contextual array in order to distribute events is progression. Compiling these events through contextual gradients is algorithmic. In a complex organization with competing needs and objectives, progression is probably less straightforward. However, notice how the level of discourse has become substantial: rather than simply discussing something flat and mono-dimensional such as sales, the conversation is about contributing events and perceptions of progression that might go well beyond the physical sale of merchandise. The idea is that we cannot control sales per se but rather the events; we cannot perceive the satisfaction of human desire except through elaborate instruments such as progression. I know that some might suggest, a perceptible increase indicates an appeasement of human desire. Sales don?€?t measure appeasement per se. A person can buy something purely on expectation and preconception. ??Disembodied metrics provide little explanation of underlying phenomena.,There can be an unlimited number of ?€?events.?€? ??How exactly these events should be distributed given different circumstances is a topic that I will leave for some other blog. ??The programming term that I use to describe a context is a ?€?counter.?€? ??A group of counters associated in a contextual gradient or flow pattern is a ?€?monitor.?€? ??In practice, I tend to have many monitors under a particular ?€?program.?€? ??In the next image, I try to convey the idea that our perceptions of phenomena can be supported by multiple monitors giving rise to contextual multiplicity. ??In another blog, I identified the different conceptual flows of information in an organization: ??projection, direction, and articulation. ??I said that big data might find a good market in internal articulation due to the need to make organizational sense of the environment. ??I added that in articulation, the ?€?metrics of phenomena?€? are most relevant. ??I know this probably sounds purely philosophical, but actually I was giving design guidance. ??There is no limit to contextual multiplicity. ??This is the nature of the metrics of phenomena. ??Sensitization and pathological desensitization are worthwhile considerations affecting the metrics, I guess also to be discussed on some future blog. ??I just want to point out that the presence of a conceptual construct for big data does not necessarily mean that it will work well: ??good design is still important.,I would now like to introduce my real-life example. I think of all the different blog topics a person might write about, computer code is perhaps the driest. Fortunately in terms of the prototype, the supporting concepts are fairly coherent; or at least I hope others find it so. I will give my example in relation to a special box. In trying to name this box, I could only think of Pandora?€?s Box. Apart from the name already being in use in folklore, I thought it might also be the name of a retail outlet or product that holds jewelry. So I decided to call my box "Pandora?€?s Unified Epistemology Box." Being a rather lengthy name for a box, I will simply refer to it as "the box" with my apologies for having such poor marketing skills.,I chose to present my commuting database since it is fairly new; consequently, its design remains rudimentary. It contains information about my choice of roads during the commute to work and home. To the left of the illustration to follow, I present the events or body of data for a single day. I distributed these events on [tuesday] [may] [day6]. The commute was about [ride75] minutes in duration (rounded to the nearest 5 minutes). It was rather [sunny] and [dry] outside. I left work at [1641] and got home at [1755]. There was quite a lot of roadwork taking place at the time: [desurfacelots]. There can be an unlimited number of events. To the right, I show three different counters or heads of data. Each counter is meant to be used in the context of a 75-minute drive home. I find that my driving times vary a lot. At the moment, factors such as Wednesdays, non-dry road conditions, and dim lighting measurably lengthen my drive. I maintain contextual multiplicities for different road conditions and routes. A common change in route occurs when I stop to get gasoline. There is also a route that I call ?€?evade?€? meant for situations involving blocked roads. The question of what constitutes a ?€?route?€? is actually a strategic design decision that is not necessarily straightforward.,The decision-making and situational construct can be characterized as a 3-dimensional box as shown above. Driving time is apparent only on the surface of the box. Now, it goes without saying that another person might record his or her daily driving time and arrive at a similar sort of arrow without using anything resembling my box. Similarly, a person can record sales data without any understanding of the contributing factors. The box is designed to support the embodiment of data. This embodied form is persistent: it can exist indefinitely in the data system to explain the surface metrics. Although it is hardly unusual to have surface metrics, imagine the benefits of maintaining data in its embodied form particularly in an historical context or in relation to the intellectual capital of organizations. Once the metrics are disembodied (or perhaps they were never embodied in the first place), the facts are free-floating in space wandering intangibly like ghosts. I have made it my calling to track these ghosts.,I do not consider myself a professional programmer. I actually have a certain level of mistrust towards computer code. So I frequently find myself incorporating diagnostic checks into my work. In the case of my commuting database, apart from having commuting time as the context, I also distribute events of commuting times, which I know must seem rather redundant. Call me paranoid, but the events of commuting times should follow the contextual gradient rather closely; this is confirmed below with a correlation of -0.996. (It?€?s negative because I prefer having higher scores for short commutes.) The contextual gradient is a work in progress: that is to say, I am still considering different ways of measuring the extent to which events contribute to contexts. Currently, I use a measurement called the crosswave differential. Using this approach, I can say that the ride home on Monday has been faster than Tuesday; dry road conditions are extremely important; and sunny days actually seem to slow people down. I am not making general statements about human behaviour but rather the dynamics as I encounter them given my driving style and the routes that I take going eastbound on the 401, one of the largest highways in North America.,Above is my ?€?testing 1, 2, 3; testing-testing?€? pattern. I am confident in my work to the extent I can determine when it isn?€?t working. The reason I find myself with a ranking of events is because the data is embodied by events. Without the body of data, there would be no data to provide a ranking. What we describe as data can be handled as a complex structure for the purpose of determining the contribution of events. I indicated in an earlier illustration that it is possible to have many different groups of contexts (contextual multiplicities or ?€?monitors?€?) to help describe phenomena. These provide the metrics of phenomena. The monitors help us perceive the reality of the phenomena. However, there is no limit to the number of monitors; and it is often worthwhile to have at least several. While certain events might contribute positively on some monitors, the same events might register negatively on others. In other words, there might be competing risk-reward dynamics. I consider event-distribution and -recognition to be fairly detailed topics; so I will not cover it here at this point. Perhaps I will elaborate on some other blog.,I said that events tend to reflect the things that we can control while contexts tend to be things we would like to control. This is a general rule of thumb that is subject to creative discretion. Having placed commuting times into the event distribution for diagnostic purposes, it seems reasonable in practice to incorporate various metrics in the distribution to determine the contextual association. A person could for instance distribute stock prices, incidents of vandalism, diesel consumption, and other metrics as events, leaving the choice of contextual realities to influence the portrayal of how the events fit. However, something such as vandalism is hardly a simple event. I describe it as a symbolic aggregate; that is to say, there are probably many events already embedded within it interacting with each other. The results are particularly intriguing if the events are expressed as a coherent gradient: ??it them becomes possible to speculate on apparent causality between the pattern of events and the contextual gradient. I have an interesting simulation where I use Tendril to guess how much of certain key gourmet coffee ingredients customers prefer given a stream of simple scores: e.g. 1 terrible up to 20 fantastic. I personally find the program effective. It is possible to determine which specific product characteristics allow for the highest prices. Only a fairly sophisticated ?€?detection?€? system (event-distribution strategy) can record the elements that go into an order. It would be necessary to ?€?check all that apply,?€? bringing us back to the base-structure for data embodiment.,Organizations tempted by simplicity will likely be disadvantaged in the emerging complex data paradigm. In this discussion of adding dimensions to data, I hope it is apparent that the data itself is becoming more complicated in a structural sense. Therefore ?€?the data?€? requires some thought in terms of how to address real-life problems. The structural challenges concerning the data are real. I feel it represents a field of study all on this own. I haven?€?t really invoked statistics except on the diagram for the Event-Context Efficacy Test. I consider the event interactions statistically evasive although I don?€?t dismiss the idea of trying to make use of statistics. Statistics just seem a bit out of its element. ?€?I stopped for gasoline on Wednesday afternoon. I decided to take the collector lanes on my way home from work. It was raining. There were tractor-trailers all over the place. Drivers were weaving in and out. Some of them looked like they going to snap. I kind of wondered when I would get home.?€? I consider this is a pretty hostile setting for statistics. Still, given advances technology, it is hardly my place to dispute how statistics might be used to sift through event data. In terms of how to make estimates in an embodied data environment, I am still working on this; but I admit that I do it all the time using the event gradients as proximity references or ?€?landmarks.?€? ?€?I think I can pick you up at around 6 PM today.?€? I didn?€?t snatch that estimate from outer space.,So I don?€?t rely on statistics for handling embodied data. I don?€?t consider the crosswave differential particularly statistical in nature; it is a tool more to differentiate between events and ambient conditions. I guess the point is debatable. There must certainly be a means to confirm whether or not the structural design seems to be delivering noticeable improvement in real life and for diagnostic purposes; I think statistics has an important role in this respect. How I design the system of distributing events and assign contexts might be completely different from how another person decides to approach the same problem. Despite the added dimensions to the data, there is frequently a need ?€?confirm performance?€?; I have described this imposition on data as the ?€?metrics of criteria.?€? There is a balance between criteria and phenomena because in the human world, many things that exist for humans do so for a purpose. It is not enough just to have embodied data. There must be systemic embodiment. The data is part of a human system such as production. Nonetheless, I would say that for the most part, the balance has not existed. The role of data has been largely instrumental and criteria-driven. So I hope that others might consider embodiment in the articulation of operational and environmental conditions.
According to the Asghar ,. (2009), Business Intelligence (BI) is divided into two main parts: (a) BI dimension and (b) BI process. Knowledge, functionality, technology, business and organisation are categorised under BI dimension. The performance of data sources, data warehousing, ETL, OLAPS and other related tools are categorised under BI process. Basically, dimensions and processes are interrelated to form a complete life cycle of BI system development.,Knowledge is a prime thing for business process. It is also a requisite for creating an innovative idea about new products and technologies, volume of scales, making new clients and keeping relations with clients. The main sources of knowledge in an organisation are clients, suppliers, information systems, news, internal and external data, website, commercial information, inner credentials, media press, workers and stakeholders.?? On the other hand, knowledge has been categorised as a practical, expressive, semantic, periodic, unambiguous and implicit. So it can be said that knowledge plays a vital role in any organisation to fulfil the demand of BI (Olszak and Ziemba, 2003; Simmers, 2004)., Olszak and Ziemba (2003) signify that the BI benchmark must be taken in consideration to analyse all the benefits that will be generated in an organisation. They are:,Strategic planning for development of an organisation, relationship between enterprise strategy, mission, goals and tasks, identifying problems to be undertaken and providing information for different activities involved in an organisation.,Customer relationship improvement,According to Olszak and Ziemba (2003), business intelligence includes the knowledge creation, data source and information technology. To achieve this, today there are different platforms which integrate data sources, advanced databases and analysis server with user friendly front-end application. This has made easy for all level of the business staff to analyse the business performance. Some of the business solutions are Oracle databases, IBM DB2, Microsoft SQL Server, Microsoft SQL Server Business Intelligence Studio, Oracle Analytical work manager, Oracle BIEE ( Business Intelligence Enterprise Edition), NCR teradata warehouse, Hyperion, SAS, Cognos, Business Objects, Open source project Pentaho and MS Miner. To choose proper business solution is a difficult task. It can be chosen on the basis of function, architecture design, compatibility and business operation and requirement (Asghar ,, 2009).,Simmers (2004) identifies that the increasing practices of business is directly proportional to high demand of information. These demands are fulfilled by BI through various internal and external surroundings.,It is very important to know about the organisation and its goals before going through the BI solution. The present and future demands for knowledge in an enterprise are based on these goals (Olszak and Ziemba, 2003).,References:,1) Asghar, S., Fong, S., Hussain, T. (2009). Business Intelligence Modeling: A Case Study of Disaster Management Organization in Pakistan. , pp. 673-679.,2)??, , , , 
"Researchers at , may be on to something big when it comes to manipulating Big Data. ??Jeffrey Rzeszotarski, a Ph.D. student in the??,??and Aniket Kittur, an assistant professor at the Institute took a look at how to use new touch-screen technologies like the iPad or Galaxy Tabs to better manipulate data found in spreadsheets and the like.,The proof of concept idea is called "","" and it's inspiring to see how cool data manipulation can be. Data points are rendered as colored dots. Using standard touchscreen hand gestures, users can sort, filter and pull data points to form new views of data ties.,The Kinetica app can load any Excel XLS, CSV, and TSV table of data. Users can mix and match techniques to investigate more than five dimensions on one iPad screen.,Here's a short video that gives you a feel for the project.,
"
Interesting article about the history of the Internet, with some really cool maps. Our upcoming "picture of the week" ??will come from this article. Check out??,??to discover our previous ,.,The internet increasingly pervades our lives, delivering information to us no matter where we are. It takes a complex system of cables, servers, towers, and other infrastructure, developed over decades, to allow us to stay in touch with our friends and family so effortlessly. Here are 40 maps that will help you better understand the internet ?€? where it came from, how it works, and how it's used by people around the world.
It is important big data science in multiple ways. First, data security and encryption relies on algorithms that typically use an encryption key: the key - at the very core of these algorithms - is essentially the product of two very large prime numbers. While there has been new developments to produce different algorithms ,, large prime numbers are still central to most encryption schemes. Recent security breaches (eBay, Twitter, Target) show how critical data security is. Anyone who can come up with an algorithm that can easily ,, could jeopardize the security of all financial transactions, electricity networks, personal data, military and corporate secrets, aerial navigation, and much more. Pretty much all of us have indeed spent countless hours trying to solve this very old problem, to no avail.,In this article, I describe a methodology that might lead to a solution, or if not, one that leads to very interesting results and potentially applicable to other problems such as random number generation. And it definitely illustrates how chaos is far less chaotic and much more organized than most people think!,The other reason why this is an interesting data science problem is because the methodology proposed here involves creating and processing very big data to find patterns and insights that would help solve our problem: factoring a product of two large prime numbers. It is actually a very good application for data scientist candidates, to learn how to massage truly big data and find patterns. Note that in many ,, there is often an intensive data processing step, as well as theoretical investigations, to come up with a solution. In this article, we keep mathematics to a very elementary level, so that all of you can understand and participate. This has been added as an open project for candidates participating in our data science apprenticeship, called ,. You'll have to wear your pirate hat this time, to work on this project, as this is a kind of reverse-engineering adventure. We initially thought to include this in our ,??series, but the scope goes far beyond that of a ??weekly challenge.,We assume that z = x*y is the integer that we want to factorize, with x and y being the two factors that we try to identify, typically two large prime numbers, which makes it a very hard problem from a computational complexity point of view.,We will use this generic iterative algorithm to factorize z:,Algorithm:,1. Start with ,, where a and b are two parameters.,2. Then proceed iteratively as follows:,Where c, d are two parameters, and f, g are integer-valued functions satisfying,Two of the most basic functions satisfying the previous conditions are,where c and d are large enough integers, larger than the expected factors x and y (solution of?? z = x*y). We will use these two functions here.,The integer sequence (x(n), y(n)) with n = 0, 1, ... either converges to the solution (x, y), or becomes periodic for n large enough. This is a straightforward application of the fixed point theorem.,Indeed, this sequence converges very rarely, and most of the time, creates what seems like a chaotic series of numbers - so chaotic that such equations have been used to produce (poor) simulated random numbers - the opposite of what we are trying to do here: finding a strong pattern and fast convergence to (x, y). Yet, there are sets of parameters (a, b, c, d) that, given ??z = x*y, result in very fast convergence to the solution (x, y), solving the factorization problem and identifying the large prime numbers x and y in as few as 2 iterations.,The problem, is of course, how to identify these parameters (a, b, c, d) that lead to fast convergence, or at least convergence that is fast enough to make it faster than the best factorization algorithms currently known. A solution consists of generating a very large number of (a, b, c, d) combinations for a few different z (product of two primes x, y), check out the few (a, b, c, d) that lead to fast convergence and see if some patterns emerge among these combinations.,We started to look into this, and we run some experiments on small z (we know big z's behave very differently from small z's, just like big data behaves very different from small data), but we did identify a few interesting facts:,Just curious, does anyone know about an initiative to factorize products of large primes using Hadoop? I'm sure this must even been tried, and probably long ago. Hadoop would work well for this kind of problems. Note that checking whether a number is prime or not is a much simpler problem, if you use ,.??,Finally, another approach to factoring z into x*y, is to replace x, y, z by three matrices with integer coefficients, X, Y, Z with det(X) = x, det(Y) = y, and det(Z) = z. Here??'det' means determinant, and we use the fact that det(X*Y) = det(X) * det(Y) = x*y, solving Z = X*Y rather than z = x*y. When X, Y, and Z are 2x2 matrices, a particular case is equivalent to??working in??the complex plane.

From??,Check out ,.
"
"

There's no shortage of great talent for companies such as Apple, Google, Intel, Facebook, Wikipedia and some exciting startups. But what if you are not one of these?,I received the following job ad in my mailbox (,), from a third-party recruiter, and it's probably for a data science position at Nike near Portland, Oregon (my guess). Basically, it's a 6-month gig to build an A/B platform.,This skills mix is not found in typical employees. Granted, this is a consulting job, but you should then advertise it as a consulting opportunity. You would have to offer $300/hr to motivate the few guys that have this rare mix of talent and knowledge. There are consultants or small consulting companies (with this mix) willing to do the job for $300/hour (motivation is even bigger if you reimburse their weekly hotel/travel expenses, and consulting firms are better equipped than individual consultants as they can use a mix of cheap, junior with expensive, senior consultants to lower the cost). Or do you really need to build such a platform in the first place? Why not get one from a vendor instead?,You mention that machine learning / statistics is a plus (not a requirement), yet the core of the job is developing an A/B testing platform. Something where deep statistical knowledge is most critical.,For a regular employee, you would want an engineer with some statistical knowledge, train him so that he becomes an expert in experimental design (aka A/B testing) - at least in digital experimental design. The guy needs to acquire a deep knowledge of Internet/server architecture, traffic flows and web metrics, to understand source of biases in this type of A/B testing. Yet he needs to be very knowledgeable about statistical theory and non-parametric, robust confidence intervals in the context of big data and most importantly - in the context of doing TONS of A/B tests, whether the data is big or small.,In short, you need someone with some very specialized, narrow (not comprehensive) statistical knowledge (not an expert in logistic regression) and very specialized (but not comprehensive) knowledge about Internet architecture. These people exist, they are domain experts. Would a guy like me, who developed his own home-made Map-Reduce / Hadoop environment, qualify? No, I would surely be turned down for not having Hive/Pig etc. And that's a mistake.,In addition, you want the person to also be a developer. Here of course, I would not qualify: I'm a web developer, I developed this very website for instance as well as analytic API's, I managed developers (consultants), and haved used scripting languages for a long time, but I'm not the developer with the kind of team experience that you really need; companies made the mistake in the past to believe that I am a developer -- don't assume that you can turn a creative, independent guy like me into a useful team developer, it just does not happen, what I offer to a team is different. And while I'm great at designing the architecture of simple and useful dashboards (and I love it), I do not "code" them - yet I'd still be a good asset for the design part of the dashboard. Maybe dashboard creation could be done (at a low cost) using an automated tool (vendor) or templates requiring little to no coding, rather than having a consultant do it. Then you can remove "dashboard creation" from the list of tasks, broadening the pool of applicants.,My recommendation: hire two consultants - a domain expert like me for a limited number of hours, maybe 200 - (or better, one that has worked with true Hadoop if you can find one). And a developer who will closely work with the domain expert - at least in the initial step when the architecture / design of the platform is discussed, as well as during final steps when you test it on real data. Maybe hire a business analyst for the dashboard creation, or use one from the employer, or purchase/customize a product.,:
Map and Reduce functions can be traced all the way back to functional programming languages such as ,??and its Polymorphic Map function known as ,. ??Even before fmap there was the Haskell??,??command used primarily for processing against lists. ??I am sure there are experts out there on the very long history of ,??who could provide all sorts of interesting information on that topic and the influences of both , and , functions in programming. ??However, the purpose of this article is??to discuss effective strategies for performing highly parallel map reductions in the C# programming language. ??There are many large-scale packages out there for performing map reductions of just about any size. ??Google's , and Apache's ,??platforms are two of the most well known. ??However, there are many competitors in this space. ??, are just a few??references. ??MapReduce concepts are claimed to be around 25 years old by??,. ??Strangely enough, the , is currently held by Google and was only issued during 2004. ??Google , that MapReduce was developed for "processing large amounts of raw data, for example, crawled documents or web request logs".,In more complex forms, map reduction jobs are broken into individual, independent units of work and spread across many servers, typically??,??units, in order to transform a very large and complicated processing task into something that is much less complicated and easily managed by many computers connected together in a ,. ??In layman's terms, when the task at hand is too big for one person, then a crew needs to be called in to complete the work. ??Typically, a map reduction "crew" would consist of one or more multi-processor nodes (computers) and some type of master node or program that manages the effort of dividing up the work between nodes (mapping) and the??aggregation??of the final results across all the worker nodes (reduction). ??The master node or program could be??considered the map reduction crew's foreman.?? In actuality, this explanation is an over-simplification of most large map reduction systems. ??In these larger systems, many additional indexing, i/o, and other data management layers could be required depending on individual project requirements. ??However, the benefits of map reduction can also be realized on a single multi-processor computer for smaller projects.,The primary benefits of any map reduction system come from dividing up work across many processors and keeping as much data in memory as??possible during processing. ??Elimination??of disk i/o (reading data from and writing data to disk) represents the greatest opportunity for performance gains in most typical systems. ??Commodity hardware machines each provide additional processors and memory for data processing when they are used together in a map reduction cluster. ??When a cluster is deployed however, additional programming complexity is introduced. ??Input data must be divided up (mapped) across the cluster's nodes (computers) in an equal manner that still produces accurate results and easily lends itself to the??aggregation of the final results (reduction). ??The mapping of input data to specific cluster nodes is in addition to the mapping of individual units of input data work to individual processors within a single node. ??Reduction across multiple cluster nodes also requires additional programming complexity. ??In all map??reduction??systems, some form of ,??must be deployed when multiple processors are used. ??Since parallel processing is always involved during map reduction, thread??safety is a primary concern for any system. ??Input data must be divided up into individual independent units of work that can be processed by any worker thread at any time during the various stages of both mapping and reduction. ??Sometimes this requires??substantial thought during the design stages since the input data is not??necessarily processed in a linear fashion. ??When processing text data for instance, the last sentence of a document could be processed before the first sentence of a document since multiple worker threads simultaneously work on all parts of the input data.,The following figure illustrates a map reduction process running on a single multi-processor computer. ??During this process, multiple worker threads are simultaneously mapped to various portions of the input data placing the mapping results into a??centralized location for further downstream processing by other reduction worker threads. ??Since this process occurs on a single machine, mapping is less complex??because??the input data is only divided between worker threads and processors that all reside on the same computer and typically within the same data store.,When multiple computers are used in a ??map reduction cluster, additional complexity is introduced into the process. ??Input data must be divided between each node (computer) within the cluster by a master node or program during processing. ??In addition, reduction results are typically divided across nodes and indexed in some fashion so mapping results can quickly be routed to the correct reduction node during processing. ??The need for clustering typically occurs when input data, mapping results, reduction results, or all three are too large to fit into the memory of a single computer. ??Once any part of the map reduction process requires disk i/o??(reading data from and writing data to disk), a huge performance hit occurs. ??It is very important to stress that this performance hit is??exponential and deadly. ??If you are still skeptical, please stop reading and take a quick lesson from the famous Admiral Grace Hopper ,. ??Obviously, some form of disk i/o is required to permanently save results from any program. ??In a typical map reduction system however, disk i/o should be minimized or totally removed from all mapping and reduction processing and used only to persist or save the final results to disk when needed.,The following figure illustrates a map reduction cluster running on four machines. ??In this??scenario, one master node is used to divide up (map) input data between three data processing nodes for eventual reduction. ??One common challenge when designing clusters is that not all of the reduction data can reside in memory on one physical machine. ??In an example map reduction system that processes text data as input and counts unique words, all words beginning with A-I might be stored in node 1, J-R in node 2, and S-Z in node 3. ??This means that additional routing logic must be used to get each word to the correct node for a final reduction based on each word's first letter.,When reduction or mapping results are located on multiple clustered machines, additional programming logic must be added to access and aggregate results from each machine as needed. ??In addition, units of work must be allocated (mapped) to these machines in a manner that does not impact the final results. ??During the identification of phrases for instance, one sentence should not be split across multiple nodes since this could causes phrases to be split across nodes and subsequently missed during phrase identification as a result.,Map reduction systems can range in size from one computer to literally thousands of clustered computers in some enterprise level processes. ??The C# programming language provides a suite of thread-safe objects that can easily and quickly be used to create map reduction style programs. ??The following sections describe some of these objects and show examples of how to implement robust parallel map reduction processes using them.,The C# programming language provides many features and classes that can be used to successfully perform map reduction processing as described in the sections above. ??In fact, certain forms of parallel map reduction in C# can be performed by individuals having a minimal knowledge of ,??or hardware specific thread management practices. ??Other lower level tools however, require a great knowledge of both. ??Regardless of the tools chosen, great care must be taken to avoid , when parallel programs are deployed within a map reduction system. ??This means that the designer must be very familiar with best demonstrated practices for both??locking??and multi-threaded programming when creating the portions of mapping and reduction programs that will be executed in parallel. ??For those who need assistance in this area, a detailed article on threading in C# can be located ,.,One of the most important things to remember is that just because a particular C# object is considered "thread safe", the commands used to calculate a value that is passed to the "thread safe" object or the commands passed within a??, to the "thread safe" object are not??necessarily "thread safe" themselves. ??IF a particular variable's ,??extends outside the , of parallel execution, then some form of locking strategy must be deployed during updates to avoid race conditions. ??One of the easiest ways to test for race conditions or threading errors in a map reduction program is to simply execute the program using the same set of input data multiple times. ??Typically, the programs results will vary when a race condition or threading error is present. ??However, the error might not present itself after only a few executions. ??It is important to exhaustively test the program??using many different sets of test data as input, and then execute the program??many times against each input data set checking for output data variations each time.,The specific C# classes??described??later in this document do not represent the only alternatives for performing parallel map reductions in the language. ??The selected classes merely represent a few of the viable techniques worth consideration. ??For instance, one available approach that is not covered in this document is , which is C#'s parallel implementation of LINQ to Objects. ??Numerous other C# tools are available as well. ??It is important to mention that the map reduction patterns described above are sometimes referred to / very similar to what is known as ??producer / consumer ,. ??Many great articles can be located on the internet when producer / consumer??pipelines and C# are used together as search terms.,Using the pattern described earlier, several basic C# components can be repeatedly used (and sometimes extended) to create a map reduction system of virtually any size. ??The following high level C# components and classes will be used as the "nuts and bolts" of this particular system:,?? ????,One of the easiest ways to implement a parallel map function in C# is by using the ,??class. ??Specifically Parallel., or Parallel.,??can be used to quickly map (in parallel)??independent units of work into a??centralized, thread-safe collection (we will get to thread-safe collections in a second) for further downstream processing. ??In addition, the class can perform parallel mappings with no lower level thread management programming required. ??The Parallel class is hardware intelligent and scales threads based on the current platform it is executing on. ??However, it also has the??, option for those who want more control over how many threads a particular Parallel class process is using. ??The primary purpose of the map function is to break apart input data producing one or more key-value pairs that require reduction processing by other downstream worker threads. ??While mapping worker threads are producing these key-value pairs as output, reduction worker threads are simultaneously consuming (reducing) them. ??Depending on the size of the map reduction process, other??intermediary processes such as a partitioning process might occur between a mapping and its final reduction.,In a C# example application that counts unique words from large amounts of input text, one or more stages of mapping could be used to produce the final mapping key-value pair data output. ??In order for both mapping and reduction worker threads to work in tandem, all phases of the mapping process must be executed??asynchronously. ??This can be accomplished by using either??background threads, some form of , processing, or both.,The following code demonstrates the use of Yield Return processing to break up input text into blocks of 250 characters or less using the space character as a word delimiter:,As each 250 character (or less) block of text is identified, the "yield return" command causes the process to "yield" and immediately return the identified text block to the calling process. ??Under normal circumstances, all identified blocks of text would be returned at one time when the entire process was completed. ??This would also mean that other downstream worker threads could not begin work on mapping the blocks of text into individual words until all text block identification was complete. ??The delay would slow down the entire process greatly. ??A yield return method for producing text blocks is not??necessarily??required for counting unique words in a map reduction system. ??However, this code will be used to demonstrate how yield return can be used and??subsequently called using Parallel.ForEach to complete the mapping of text to individual words.,When all stages of mapping are completed, the final results are added to a mapping results Blocking Collection. ??Using our C# example application that counts unique words from large amounts of input text, a mapping results Blocking Collection is created called "wordChunks". ??This particular Blocking Collection uses a , as its base collection. ??Since words are added to and removed from the ConcurrentBag in no particular order, using a "bag" yields performance gains over a "stack" or "queue" which must internally keep track of processing order. ??The following code shows how the "wordChunks"??Blocking Collection is created:,Technically a mapping function's output should be a key-value pair. ??The key-value pair typically contains some form of key and associated numeric value that are both used during the reduction process. ??In many cases, the key will only be contained one time within the final reduction results. ??The values for each duplicate key encountered during mapping will be "reduced" by either summation or another??mathematical??calculation??that results in one final reduction number value that is representative of the single key contained in the final key-value pair reduction results. ??In our example word counting map??reduction, a key-value pair is not even required for the mapping stage. The wordChunks bag can contain any number of words (given your current memory constraint). ??These words can also be duplicates. ??Since we are only counting the occurrence of words, each word in our bag is considered to have a default frequency of 1. ??However, the ConcurrentBag could have just as easily been created as a collection of key-value pairs (ConcurrentBag<KeyValuePair<string,int>>), if needed.,The next program demonstrates a Parallel.ForEach mapping function using the Yield Return method created before. ??This process uses multiple worker threads to identify and clean words from the blocks of input text provided by the Yield Return method. ?? The Parallel.ForEach mapping process begins as soon as the first block of text is identified since Yield Return is being used.,The program above uses Parallel.ForEach to call the text block production program "produceWordBlocks". ??This program immediately yield returns blocks of text less than 250 characters in length and delimited by spaces as they are identified. ??Parallel.ForEach worker threads simultaneously process these text blocks identifying individual words which are also delimited by spaces. ??The program also removes any whitespace, punctuation, or control characters located within the words. ??Obviously, this is an example program and many other character filtering or inclusion enhancements could made depending on your particular requirements. ??In an alternative implementation, the Yield Return method could be removed entirely and its functionality included into a single Parallel.ForEach mapping program. ??This may or may not produce better performance results depending on your code, the input data, and the requirements of your system.,Once all individual words have been identified from all word blocks, the wordChunks Blocking Collection is notified that no more words will be added to the collection. ??This notification is very important for any downstream worker threads that are simultaneously reducing / removing words from the collection. ??If the Blocking Collection becomes empty during processing, collection consumers will continue to "block" or "wait" until either the , method is called or additional items are added into the collection. ??The Blocking Collection is able to successfully manage differences in the object??production and consumption speeds of various worker threads using this approach. ??In addition, a Blocking Collection??, can be added to ensure that no more than a maximum number of objects can be added to the collection at any given time.,The process of parallel reduction is very similar to mapping with regards to the use of Parallel.ForEach to facilitate its process in our example application. ??Where reduction differs however, is in its use of one or more data storage components that provide very fast access to a particular set of reduction key-value pairs. ??When all data storage components are combined, the reduction key-value pairs eventually become the final output for map reduction once all input data has been processed. In a system where multiple map reduction computers are used together in a map reduction cluster, multiple data storage components could be used to store a very large number of key-value pairs across several computers. ??In the example word counting map reduction process, the reduction key-value pairs consist of a unique list of words which act as keys for the??reduction key-value pairs. ??Each word key contains a value that represents the total frequency of??occurrences for that particular word within the input text. ??The final result becomes a mapping of all words contained in the input data to a reduced list of unique words and the frequency that each unique word occurs within the input data.,The C# ,??is basically a thread safe hash table that is well suited for acting as the data store component within the example application. ??The ConcurrentDictionary holds key-value pairs in memory until there is either no room left in memory or the dictionary already contains the , number of elements. ??Since the ConcurrentDictionary takes a 32-bit hash of each key, the maximum number of elements is the same as the Int32.MaxValue or??2,147,483,647. ??Most computers and processes will run out of memory prior to triggering the ConcurrentDictionary's overflow exception due to exceeding the maximum number of elements. ??In situations where very large amounts of data are being map reduced, the map reduction data components (in this case Concurrent Dictionaries) can be ,??across several computers within a cluster. ??However, sharding requires a slightly more complex map reduction process since key partitioning logic must be developed to manage the associated sharding challenges such as??what nodes(computers) will contain what keys, node load balancing, node additions, node removals, and node failures.,It is obvious that some readers may be asking why not use a database at this point, and this is a very valid question. ??The short answer is that most any data management solution could be used. ??Databases, NoSQL databases, In-memory databases, or some form of key-value datastore could be implemented. ??The most important thing to consider however, is that most relational databases will rely heavily on i/o to complete the work. ??Any form of i/o during map reduction processing will also most??likely??defeat the purpose of map reduction??altogether. ??So whatever data management solution is chosen, just make sure that your data is being stored in memory. ??Even most relational databases now have some form of in-memory tables and clustering abilities.,During reduction processing Parallel.ForEach is used once again to create multiple worker threads that simultaneously consume mapping results as they are being created and added to the "wordChunks" Blocking Collection. ??Worker threads reduce all mapping results in the example application by looking up each mapped word within the reduction data store component (one or more ConcurrentDictionaries in this case). ??If a word already exists in the data store, then the mapping word is reduced by incrementing the existing reduction word's key-value pair value by 1. ??Otherwise, the mapping word is reduced by creating a new key-value pair entry in the reduction data store with a starting value of 1. ??The following code demonstrates how this process works:,A ConcurrentDictionary was created to hold our final reduction processing results. ??Furthermore, the ,() method was taken advantage of during Parallel.ForEach processing. ??It is important to mention once again that??delegates??provided to a thread safe object are not??necessarily thread safe themselves. ??In this case, the AddOrUpdate method accepts a delegate to provide update commands to execute when a key is a present in the wordStore dictionary. ??To ensure that the update is performed in a thread safe manner, ,??is used to increment existing values by 1 as an??, each time they are encountered. ??The Parallel.ForEach process executes against the wordChunks Blocking Collection removing mapping results (words) from the collection until all words have been processed. ??The Blocking Collection will also cause to the Parallel.ForEach reduction process to "block" or wait for additional words when the collection becomes empty and the , method has not yet been called by the producer (the mapWords method in our example program). ??Using the Blocking Collection's ,??method in the Parallel.ForEach loop is one way to trigger the blocking behavior.,The previous figure of a??map reduction process running on a single multi-processor computer can now be updated to reflect the C# objects and classes discussed in our example application. ??Using only a few key C# components, parallel map reduction can be preformed with minimal effort when compared to creating parallel map reduction processes in other languages. ??The figure below represents a??map reduction process written in C# and running on a single multi-processor computer:,Once all components of the map reduction process have been created, a small mapReduce method is written to bring the entire process altogether. ??One of the most important parts of the mapReduce method is to create a ,??for execution of the mapping function. ??While the mapping function populates the Blocking Collection with mapping results in the background, the reduction function simultaneously removes / reduces the mapping results into the reduction data store. ??Since all of this processing occurs in memory, the mapReduce method is extremely fast. ??The following mapReduce program ties together the entire map reduction process for our example application:,Printing the results of your map reduction is also as simple as printing the contents of your wordStore dictionary:,Map reduction processing provides an innovative approach to the rapid consumption of very large and complex data processing tasks. ??The C# ??language is also very well suited for map reduction processing. ??This type of processing is described by some in the C# community as a more complex form of producer / consumer pipelines. Typically, the largest??potential??constraint for any map reduction system is i/o. ??During map reduction processing, i/o should be avoided at all costs and used only for saving final map reduction results, if possible. ??When the processes described in this document are combined with data store sharding and map reduction partitioning processes, data processing tasks of most any size can be??accommodated. ??If you actually read this far, I'm impressed. ??Thank you!
According to ,, a data scientist should:,Paco Nathan,??replied and wrote:??There are ,. In my case, as an entrepreneur managing a company on auto-pilot (no employee, 7-digits yearly revenue with 80% margins, with significant outsourcing to vendors), none of the above test questions apply, I'd probably fail most of them, but I am a data scientist nevertheless (,), as well as business / growth / data hacker.??

UCLA is having a big data conference on Thursday and Friday Mar 27, 28 2014.?? The conference is organized by four computer science and genomic biology types. Speakers cluster [one of the rare appropriate uses of cluster analysis I know of] into three types of folks. Big biologists [they must be big, they're doing big data] doing big data, genomic stuff; computer scientists doing topic models, and a few math modelers who as far as I know, don't usually look at data at all.,The conference [well, it's called a workshop] is idiosyncratic [unique] in that it has no statisticians involved. The four organizers hail from Radiological Sciences, Physiology, Microbiology and Computer Science. If I read and recall their backgrounds correctly, two come from Comp Sci, two from Biology backgrounds, three are working in genomics of some sort and one in imaging. It seems probable that all are doing statistics, likely very complex statistics and that's fine; there aren't enough statisticians around, there never has been. Scientists have always had to do their own data analysis. ,It is a bit hard to imagine who might be interested in the entire conference other than me and perhaps a few friends of mine. The genomicists may be interested in other genomicists but not in the math modelers. The comp sci folks may be interested in other topic modelers [a new class of perfectly good models] but will they be interested in genomics? And I haven't a clue what other talks the math modelers will want to hear.,My main interest as a statistician is perhaps to hear about the dynamic fringes of statistical application. The edges of statistical applications are where a lot of the fun stuff is going to be happening. And to defend a bit of statistics/biostatistics turf. I certainly want to hear about the topic models and from the topic modelers. I want to hear from the math modelers in case they've stumbled into some data; I happen to know a bit about how to combine data and math models and I care about the problem too. But I can't make progress without help. I'm not so interested in the genomics; I'm not not interested in the genomics mind you, but the problem is too much data, not enough time to investigate it. ,Another interest of mine is getting to know who the players are. The big data movement is going to get political shortly on campus and you can't tell the players without a scorecard.
This is a classic. A guy who correctly predicted election results in all 50 states, and many other correct predictions, now fails., ,First, Nate is well known not because of his previous correct predictions, but because he got hired by the Times magazine where he contributed as a writer. Many other people got the election results correct for the 50 states, but only Nate is known for his performance. It's actually an easy prediction as most states have well known political climates (CA is liberal, TX republican). Only 3 or 4 states are difficult to predict, and you can get them right with a bit of chance (say 1 in 4 chances to get them all right)., , Here's an analogy. Let's say that 8 million stock traders make totally random trades, with 50% chances to win or lose on each trade. About 8 traders will make correct trades 20 times in a row (and that's a challenge far more complicated than predicting state elections). Guess what? 4 of the 8 successful traders (out of 8 million) will get their trade wrong on day 21, and only 1 will still be correct on day 23. By day 24, it is guaranteed (by the law of probabilities) that even the "best" traders will now perform miserably (that is, no better than the "worst" traders from the first 20 days who got all their trades wrong), moving forward., , I'm sure Nate made enough money that he does not care, and he will sure very easily find new assignments. What happened to all Princeton math PhD's who run Wall Street to nearly into the ground in 2008? I'm sure most of them are still doing very well, though I've heard many switched from doing stock market arbitrage to advertising arbitrage (both require good data science expertize), not because they were fired for incompetence, but because the industry (Wall Street finances) shrunk significantly.,For those interested ,, Silver's team wrote, about global warming, that "
Since they made marijuana legal in Washington state, I went to ,, and discovered that they now use marijuana as an ingredient in their famous recipes, in particular in their smoky macarons.,It has absolutely no effect (mind altering) on me, but it also reminded me when I was young and tried a few drugs: none produced anything other than mechanical effect (increased heart beat) on me. Even alcohol does not produce anything unless I drink more than one bottle of champagne within a 4-hour time period.,Maybe I'm permanently and naturally high all the time without using any drugs (I have some reasons to believe in this) explaining why drugs don't have any effects on my brain, maybe I am an outlier, but maybe it's something different. Maybe these drugs work only for few people, and real drug addicts are the few people truly sensitive to drugs.,A few questions:,This is just food for thoughts. I would love to read your opinion (I'm not conservative nor liberal nor anything else). How do we gather good data on this, to measure the proportion of people not reacting to a normal dose of marijuana, broken down by political opinion? How do we even set up some sort of clinical trial to test my hypothesis that many people (how many?) have no reaction to marijuana, especially if eaten rather than smoked? And how do we make laws based on data rather than on arbitrary arguments lacking statistical significance? Also, how can we guarantee that whoever agrees to do this study, is truly neutral?
Here I focus on LinkedIn and how they can monetize their groups via charging a fee for email blasts, but the same applies to Google+, Twitter, Facebook etc. In short, LinkedIn alone could generate an extra $50 million per year, thought the best implementation would probably involve LinkedIn outsourcing email blasts to a vendor such as MailChimp or VerticalResponse: it would probably mean that LinkedIn would earn only $25 million a year, the vendor would earn $25 million a year, but for LinkedIn, it would mean no more spam issues (and no more technical support, complaints), and email blasts totally outsourced and automated.,LinkedIn has ,. Most are very small, some are large (> 100,000 members), some are very large. On average, based on our estimates (we checked 20 randomly selected LinkedIn members to see how many groups they belong to - the average was 24), each LinkedIn member belongs to more than 20 groups. Right now, there are about 300 million members. Let's say, conservatively, that about 100 million members are active and accept to receive email blasts, about once a week from each group. In short, it means that LinkedIn could potentially monetize 100 million members x 20 groups per year. Assuming only one out of 10 groups would accept to pay a fee to LinkedIn (or its vendor) for email blasts, we are at 100 million users x 2 groups/user, in terms of monetizing capability.,We pay $15,000 a year to our vendor (VerticalResponse) to send 20 million messages to 100,000 subscribers each year. Since we are a rather large client, let's assume that the aggregated fee for 100 small clients reaching out to 100,000 users (when combined together), would be like $25,000 a year.,From this analysis, LinkedIn could generate $25,000 x 100,000,000 (users) x 2 (two groups out of twenty per user, that accept to pay a fee) divided by 100,000 (our VerticalResponse subscriber base). That's $50 million a year. My guess is that LinkedIn would have to share 50% with their vendor (VerticalResponse or MailChimp) unless they want to manage this whole thing themselves. The same applies to Google+, Facebook, Twitter and other social networks, thus my $200 million figure in the subject line.,Interestingly, LinkedIn is about 1,000 times bigger than we are (in terms of members). Their revenue is about $2 billion per year (2,000 times our revenue). Our community is a leading niche network that should command much higher ad revenue per pageview than LinkedIn (not sure what LinkedIn's margin is, ours is 80%). My guess is that LinkedIn is significantly overpriced (or we are underpriced, or a combination of both).,Another potential source of revenue would be for LinkedIn to share the email addresses of members (of the groups that you own on LinkedIn) with group owners only, for a fee - except those from members who explicitly refuse to have their emaill address shared with third parties. Actually, they did that in the past, but it was free. It is clear that LinkedIn attaches a lot of value to email address of its members, ,??to prevent thieves from stealing these email addresses and reselling them..
Interesting interactive timeline featuring a number of "big data" milestones since 1932. There's way too much emphasis on BI, ERP and SAP, but still, it contains lots of interesting history when you filter out these references.??,Big data, back in 1940,:,.
We created a ,??in 2012, and we are still adding keywords. It is also in our ,??(better English, recent update). Here we share with you another similar dictionary, from BigDataProjects.org.,Here are the first few enties, from the Techniques sections (there are two sections: techniques and technologies):,A technique in which a control group is compared with a variety of test groups in order to determine what treatments (i.e., changes) will improve a given objective variable, e.g., marketing response rate. This technique is also known as split testing or bucket testing. An example application is determining what copy text, layouts, images, or colors will improve conversion rates on an e-commerce Web site.,Big data enables huge numbers of tests to be executed and analyzed, ensuring that groups are of sufficient size to detect meaningful (i.e., statistically significant) differences between the control?? and treatment groups?? When more than one variable is simultaneously manipulated in the treatment, the multivariate generalization of this technique, which applies statistical modeling, is often called ?€?A/B/N?€? testing.,A set of techniques for discovering interesting relationships, i.e., ?€?association rules,?€? among variables in large databases. These techniques consist of a variety of algorithms to generate and test possible rules.,One application is market basket analysis, in which a retailer can determine which products are frequently bought together and use this information for marketing (a commonly cited example is the discovery that many supermarket shoppers who buy diapers also tend to buy beer). Used for data mining.,A set of techniques to identify the categories in which new data points belong, based on a training set containing data points that have already been categorized. One application is the prediction of segment-specific customer behavior (e.g., buying decisions, churn rate, consumption rate) where there is a clear hypothesis or objective outcome.,These techniques are often described as supervised learning because of the existence of a training set; they stand in contrast to cluster analysis, a type of unsupervised learning. Used for data mining.,A statistical method for classifying objects that splits a diverse group into smaller groups of similar objects, whose characteristics of similarity are not known in advance. An example of cluster analysis is segmenting consumers into self-similar groups for targeted marketing.,This is a type of unsupervised learning because training data are not used. This technique is in contrast to classification, a type of supervised learning. Used for data mining.,A technique for collecting data submitted by a large group of people or community (i.e., the ?€?crowd?€?) through an open call, usually through networked media such as the Web. This is a type of mass collaboration and an instance of using Web 2.0.,A set of techniques to extract patterns from large datasets by combining methods from statistics and machine learning with database management. These techniques include association rule learning, cluster analysis, classification, and regression. Applications include mining customer data to determine segments most likely to respond to an offer, mining human,.

Anyone knows the reference for this diagram? Looks like it might have been published first by Gartner. Domain expertise, the #1 success factor, is still missing in this diagram. I invite you to read my articles on "10 types of data scientists" and other references below to learn more about my unusual point of view on data science.

You can call it , or ,, but the idea is to use analytic intelligence to reverse-engineer algorithms, transform, manipulate and modify data in external databases, without even accessing the databases in questions, for your business advantage.,:, Should data scientists learn these , techniques? I believe so, in order to outsmart data hackers.

This is a recent, very popular article published in the ,. A similar one -??,??- was ,. In both cases, this is an attack against big data. It features Google's failure in its flu prediction for 2013, also claiming that algorithms not trying to identify causes (but focused on correlations only) are doomed to fail.,:
My selection of articles and resources recently posted in various news outlets - mostly from specialized publishers dealing with big data, machine learning, visualization and related topics. The picture below is from the first article.,(*) I disagree with this Harvard Business Review author. Senior data scientists work on high level data from various sources, use automated processes for EDA (exploratory analysis) and spend little to no time in tedious, routine, mundane tasks (less than 5% of my time, in my case). I also use robust techniques that work well on relatively dirty data, and ... I create and design the data myself in many cases.
Here we go. This a a follow up to our previous??,. The map below illustrates the data science (route optimization) problem described in the UPS Truck article.
I've heard from Wiley that our ,??had already 4,133 pre-ordered copies, which is (according to Wiley) a great start. It was published last Monday.,I invite you to check the ,??or check out the book on ,.,I'll try my best to answer all the questions asked by readers, in a timely manner. I am also in the process of writing new data science articles to hopefully write a second book, with additional data science topics. And Wiley is going to publish an interview with the author, more on this later.,I also invite you to ,??on our websites: ,, ,, ,??and ,. Top contributors will received a free copy of the book, signed by ,.
Should you hire someone who knows all the most recent flavors of logistic regression? Or an Hadoop developer?,In my opinion, this is the wrong strategy. These employees are very expensive (at least $120k per year), and they might not bring the ROI that you expect. At least, if going in that direction, hire someone favoring simple, scalable, robust, automated solutions over anything else. To automate, you need someone great at developing ,??that can handle faulty data, or processes that occasionally crash. These techniques will be described in our next book. People good at that usually hate mundane tasks so much that they always automate or outsource them - maybe you can start your hiring interview discussing this topic, asking what the candidate successfully automated, and how.,I believe you should hire someone with deep domain expertize, and strong analytic acumen. A guy who,Another solution is to outsource to vendors, and that's what we've been doing successfully at Data Science Central for a few years - all our data sources come from external vendors and data production / cleaning / summarization is automated. Then you can either hire a data scientist to process the data in question, or use robust data science tools (assuming you have someone in your team who can decide which analytic vendors are best for you - it's good to have two vendors, not just one), or have an executive with strong analytic acumen spend 5-10% of his time analysing these data summaries: that's what I do as the co-founder and CFO at Data Science Central, because I also have the whole financial silo-free picture, and that helps a lot when blending everything to make decisions; it really helps to have great interactive dashboards to access the data, usually your vendors provide these dashboards and you can access them as web apps anywhere in a plane or in your office. Sometimes the greatest insights do not show up in dashboards, so you need some vision to "see" what nobody sees, then get the dashboard improved. In my case, in one example, it involved creating a segmentation by ISP rather than by traditional customer segments to identify failing ISP having truly, significant negative impacts on marketing campaigns. This also illustrates the fact that one trustworthy person who knows all aspects of the business can sometimes be better than an entire team, as in 1 + 1 + 1 < 1 (such people are not that rare, but very few of them are geeks or classical data scientists).,So where do you find this kind of data science talent? The analytic acumen is a gift that at least 10% of the population has - though most who possess this gift (not something you can easily learn at school) are not data scientists; some are lawyers, psychologists, geographers, doctors, and it might be worth interviewing some of these people. Management consultants are also a goo bet. I will soon provide tests to assess true analytic acumen - though some questions are??, already, and it's NOT the little traditional tricks such as the famous Microsoft interview question involving elevators (succeeding in these tests is not correlated with success at work, Google agrees with me on this).,Also, I believe it is a mistake to look for the top guru who knows many data mining techniques in details. ,??is better, as you will be dealing with a versatile employee (great for small companies). A candidate suspicious about predictive models, metric choices, and data quality, is worth interviewing - unless he/she can't bring change in a smooth way, or has no alternative to traditional predictive modeling. You need to find someone who can learn any useful techniques in a few hours, without his employer or anybody else having to spend money on expensive training - and test his/her Google search / information discovery skills during the job interview. You want to look for people who can self-learn the right stuff easily. The good news is that these people are far more numerous than data scientists, and less expensive. The purpose of my ,??and ,??is actually to create such people. You can also identify these potential hires by looking at what they publish in their blogs. So first, you need to identify these blogs. I will post a list of these blogs in the next 30 days.??

 , 
Once the embedding of Big Data researchers and Education researchers, this marriage will generate results, which will actually be considered as the golden needle found from the hay stack of Big Data. A child, who does not know how to write or read is smarter enough to use a computer/mouse/iPad etc, without any instruction set provided. This is learning by watching someone, in this case an adult's behavior of utilizing the smart device. In case, if an eigth grader give a visual??programming??task of a lego robot, it is easy for this 8th grader to arrange the command set of the graphical user interface of a programming tool to write a code to program the brick of lego robot with a processor attached to make this lego robot to??maneuver??on its own within a specified domain by this 8th grader.,The Set theory applies to Biology, Chemistry, Mathematics and rest all subjects that we teach our children in our traditional school system. This theory is clear about the use of UNION, in general life a marriage is considered a union a combining of two life partners, and this is part of the Set theory, This article is not to introduce Set??theory?? it is utilized to??exemplify??that we use in our daily life numbers and more, which is based on some form of Mathematics,,It will be great for someone to take leadership and start to create a curriculum after reading this article, that combines the research of STEM in education and the proper use of Big Data to assist this research work by getting embedded and produce or find the golden needle in the most important field of a combined education for all to benefit from.

??,Are you looking for an exciting career opportunity that is just as paying as it is desirable? , calls Data Scientists are the sexiest jobs of the 21st century. Data Scientist term coined when two people, DJ Patil and Jeff Hammerbacher, were trying to name their data team working on big data and did not want to limit their functions with just any job title like business analyst or research scientist. And ever since, the title has become extremely popular.??Data scientists are someone who is inquisitive and who can stare at data and spot trends. Individuals in this particular role don?€?t just sift through and organize piles of information for companies; they are part of a cross-functional team within an organization who provide various departments with pertinent information to facilitate growth and innovation. Now that you know what a data scientist is, let?€?s discuss how you can be on your way to be an effective Data Scientist.??,??, , ?€? a good Data Scientist is handy with a collection of open-source tools ?€? Hadoop, Java, Python, among others. Knowing when to use those tools, and how to code, are prerequisites. To be a Data Scientist, you should have your hands on a number of tools and technologies, especially open source ones, such as Hadoop, Java, Python, C++, ECL, etc. Besides, having good understanding of database technologies, such as NoSQL database like HBase, CouchDB, etc. is an add-on.,??, , ?€? The second skill, as you might expect, is a base in statistics, algorithms, machine learning, and mathematics. Conventional computer science degrees no longer satisfy the quest of a data scientist. The job requires someone who on the one hand understands large-scale machine learning algorithms and programming and on the other is a statistician. So, the profile is better suited for experts in other scientific and mathematical disciplines, apart from computer science.,??, , ?€? As data scientists wear multiple hats, they need to have strong business skills. A data scientist has to communicate with diverse people in an organization that includes communicating and understanding business requirements, application requirements and interpret the patterns and relationships mined from data to people in marketing group, product development teams, and corporate executives. And all this requires good business skills, to get the things done right.??,??, , ?€? The fourth set of skills focus on making products real and making data available to users. In other words, this one?€?s a combination of coding skills, an ability to see where data can add value, and collaborating with teams to make these products a reality. You may be able to mine and model data, but are you able to visualize it? Well if not, mind that you should be able to work with some, at least a few of the data visualization tools. Some of these include Tableau, Flare, D3.js, Processing, Google Visualization API, and Raphael.js.,??, , ?€? You don?€?t just have to look around and do with data. You got to think creative, and innovate. A data scientist should be eager to learn more, be curious to find new things, and think out of the box. They should be focused on making products real and making perfectly done data available to users. They should be able to see where data can add value, and how it can brings better results., , This may seem obvious, of course, because data science is all about solving problems. But a good data scientist must take the time to learn what problem needs to be solved, how the solution will deliver value, and how it'll be used and by whom.,??, , ?€? Communication is the key to work with various cross-functional team members and present analytics in a compelling and effective manner to the leadership and customers. In other words, you may be brilliant in your rarefied field, but you're not going to be a really good data scientist if you can't communicate with the common folk.,??,Everywhere around us, data are being collected at unprecedented speed and scale ?€? from online social networks and ecommerce sites to sensors in laboratories and smart utility meters. In order to unlock the powerful potential of this big data ------------the world needs Data Scientists and if you have the above mentioned 7 skill sets, you are all set for the exciting journey.
Here are some of my favorite things about big data and data science, from A to Z (actually, ZZ):,A ?€? ,B ?€? ,C ?€? ,D ?€? ,E ?€? ,F ?€? ,G ?€? ,H ?€? ,I ?€? ,JJ ?€? ,K ?€? ,L ?€? , (LLE),M ?€? ,N ?€? ,O ?€? ,P ?€? , (data profiling),Q ?€? ,R ?€? ,S ?€? , (SVM),T ?€? ,U ?€? ,V ?€? ,W ?€? , (Waikato Environment for Knowledge Analysis),X ?€? XML (specifically ,),Y ?€? ,ZZ ?€? ,The extended (fully annotated) version of this list, containing additional explanatory detail for each item, can be found at:??,??,These are just a few of my favorite things in big data science. Please add your own favorites in the comments below.,Follow ,??on Twitter at??,??


This article describes methods for machine learning using bootstrap samples and parallel processing to model very large volumes of data in short periods of time. The R programming language includes many packages for machine learning different types of data. Three of these packages include Support Vector Machines (SVM) [1], Generalized Linear Models (GLM) [2], and Adaptive Boosting (AdaBoost) [3]. While all three packages can be highly accurate for various types of classification problems, each package performs very differently when modeling (i.e. learning) different volumes of input data. In particular, model fitting for Generalized Linear Models execute in much shorter periods of time than either Support Vector Machines or Adaptive Boosting. In instances where very large datasets must be fitted, learning times for both ADA and SVM appear to grow exponentially rather than in a linear fashion. However, when using a combination of parallel processing and bootstrap sampling all three machine leaning methods can be tuned for optimal performance on both the Linux and Windows platforms.,After looking at the general algorithms supporting each of these models, it quickly becomes obvious why the Generalized Linear Models perform so much faster when training on larger volumes of data. While GLM performs a more simplistic linear regression using a set of observed values to predict its response variable, the algorithms for both SVM and ADA deploy much more sophisticated techniques. Support Vector Machines are a form of non-probabilistic binary linear classifiers [4] which fit a ?€?maximum-margin hyperplane in a transformed feature space?€? [4]. The creators of SVM, Cortes &Vapnik (1995) [1,5], describe this process as ?€?class separation?€? which means ?€?looking for the optimal separating hyper-plane between two classes by maximizing the margin between the classes?€? closest points?€?. [1] When using the popular Radial Basis Function kernel, the corresponding feature space is ?€?a Hilbert space of infinite dimensions?€?. [4] Adaptive Boosting works by taking misclassified examples from previous learning iterations and assigning them higher weights in subsequent iterations. [6] This process is called ?€?iterative re-weighting?€?, and it is crucial to the success of Adaptive Boosting. [6],The process of bootstrap sampling allows estimation of a data population?€?s variance by ?€?measuring those properties when sampling from an approximating distribution?€?. [10] In simplistic terms this means that just because you have collected or randomly sampled some data, your particular data?€?s unique characteristics do not necessarily represent all of the unique characteristics that could be observed when examining all possible instances of the type of data which you have collected. For example, lets imagine that I have borrowed five different randomly chosen Mercedes Benz model C250 vehicles from the Mercedes dealership for observation (lucky me!). It is obvious that I would be missing many colors and options from the entire C250 lineup. Furthermore, if I randomly sampled 20 vehicles from the same dealership?€?s lot at different times of the year, I may draw very different conclusions about the most common colors, options, and vehicle models offered by that particular dealer. Bootstrapping is a technique which can assist with smoothing out some of these unintended variances in your data bringing greater stability to any conclusions drawn.,In machine learning, Bootstrapping is a great technique to avoid over-fitting your model to a particular set of training data. A modeling process which is trained on or utilizes the results from multiple bootstrap samples will typically ?€?generalize?€? better making more accurate predictions when classifying against many different sets of input data. When referring to Figure 1, imagine that the lines drawn for any given machine learning method displayed are very specific to the particular set of data provided during training. Now, imagine that the actual line for all possible instances of this type of data looks similar but varies at different points on the curve. This means that the model will be over-fitted and not generalize very well when making classifications. Bootstrapping can be thought of in a manner similar to the Adaptive Boosting iteration images in that the different bootstrap samples can be used as yet another method to appropriately adjust the classification line for more accurate results.,One of the most powerful features of Bootstrapping during machine learning is that some given populations of training data are simply too large for more complex machine learning methods such as SVM and AdaBoost. In these situations, multiple bootstrap samples can be taken to build a highly accurate and responsive modeling process using much smaller randomly sampled subsets of the input data. Furthermore, the optional prediction probabilities returned by most machine learning packages in R (including GLM, SVM, and AdaBoost) can be used to rank individual bootstrap predictions so that multiple model results can be combined for a prediction consensus between any number of bootstrap samples and even different machine learning packages. Creating , bootstrap samples for the purposes of machine learning is a very simple process which typically comprises taking , equally sized random samples with replacement from the input dataset which are then subsequently used for training purposes during machine learning., , ,The R Programming Language offers multiple solutions for symmetric multiprocessing (SMP). SMP is a form of parallel processing using multiple processors which all connect to a single shared main memory. Parallel processing for both Linux and Windows platforms can easily be supported in R with very minor program modifications. The doMC [11] and doSNOW [12] packages in R offer a very simple loop construct ?€?foreach?€? which executes in parallel when the ?€?%dopar%?€? directive is used. Furthermore, the ?€?foreach?€? construct is near identical in both packages requiring only minor setup changes when porting code between Linux and Windows. Although some machine learning algorithms such as SVM are not easily implemented in parallel, multiple SVM training processes against bootstrap samples can easily be executed in parallel drastically reducing overall training time requirements., , ,It is important to mention that ?€?parallel?€? is not always better. In many cases, the overhead and time required for breaking a task into individual, independent units of parallel work and creating threads or forking processes is much greater than just simply executing the task on a single processor. Furthermore, parallel processing can quickly become very complex and error prone introducing deadlocks and race conditions into code that previously executed bug free in a single processor environment. However, some tasks show huge parallel performance gains when implemented correctly . Figure 6 below documents the Generalized Linear Model under-performing substantially with processing time increasing by over 100% when executed in parallel. However, both SVM and AdaBoost post huge performance gains when running in parallel with SVM training almost 4 times faster.,The University of California at Irvine maintains a , of machine learning databases which are useful for testing purposes. The following practical example demonstrates using the UCI , with the GLM, SVM, and AdaBoost machine learning packages in R. Multiple bootstrap samples are created, and then each bootstrap is trained and tested in parallel using each machine learning package. Test results demonstrate that comparable accuracy results can be achieved using as few as 8 random bootstrap samples of only 20% (with replacement). In addition, utilizing the bootstrap samples allows model training to occur in parallel and produces large performance gains in most cases. It is important to mention, as we will soon find out, that bootstrapping does not always produce optimal results for both speed and accuracy. When possible bench-marking should be performed to determine the best implementation method., , , ,When using bootstrap samples, making blended class predictions can be accomplished by simply counting each bootstrap model?€?s predicted class for a given input and determining the final blended class assignment using a voting scheme between bootstrap models. For example, if 4 models predict True and 1 model predicts False, the blended prediction is True by a vote of 4 to 1. Furthermore, blended confidence for a particular class can be determined by using a ranking procedure where all bootstrap predictions are ranked based on the probability of a given class. Once ranks have been assigned, they can easily be added together to compute a final ranking for each prediction?€?s likelihood of belonging to a specific class. This is useful when it is important to understand the confidence for each membership prediction compared to all other predictions., , ,The final results demonstrate that comparable accuracy can be achieved using as few as 8 random bootstrap samples of only 20%. Furthermore, both the SVM and ADA models boast very large performance gains during training by taking advantage of parallel processing against smaller volumes of input data. It is important to mention that while all of the elapsed execution times are very close to (or less than) one minute using the Magic Gamma Dataset, the exponential growth of training performance times for both SVM and ADA make these performance gains even more substantial when fitting larger volumes of data. Likewise, it is important to recognize that the parallel processing performance break-even point for the GLM model is much higher. No parallel processing training benefits are gained in the case of GLM when using this dataset.,One small drawback related to bootstrapping is that prediction processing inputs increase by a factor of 8 in this example, since 8 bootstraps were used. Each classification must be made using each of the 8 bootstrap models. Elapsed time differences for predictions are minimized by using parallel processing. However, classification times for the bootstrapping process are slightly slower. Fortunately, prediction processing times do not appear to grow exponentially based upon input data volumes like the ADA and SVM training times do. In the case of ADA and SVM, attempted training against as few as 250,000 records may cause the training process to never complete. At this point, bootstrapping is not only optimal, but it is the only option described in article that will still work.,Further gains in accuracy can also be achieved when looking at a particular class of predictions. When predictions are ranked and sorted by the probability of a given class, higher accuracy thresholds can be set by only selecting higher ranking subsets of a particular class. For example, selecting only the top 175 ranking predictions for the class ?€?g?€? yields a prediction accuracy of 100% for both SVM and ADA. Selecting the top 1500 highest ranking predictions for the class ?€?g?€? increases accuracy by an average of 9.3% across all models.,Using the blended bootstrap model rankings above, additional increases in classification accuracy and class prediction volumes can still be achieved by stacking the results from each machine learning package. Since GLM, SVM, and ADA each use different techniques for classification, top rankings will not be identical across all three prediction populations. For instance, SVM achieves the highest accuracy score of all three machine learning packages at 93.4% when looking at the top 2,000 highest ranking predictions. Furthermore, the ADA and GLM packages score above 93.4% accuracy when looking at the top 1,500 and 100 predictions respectively., , , , , 
This article discusses a far more general version of the technique described in our article ,. Here we adapt our methodology so that it applies to data sets with a more complex structure, in particular with highly correlated independent variables.,Our goal is to produce a regression tool that can be used as a black box, be very robust and parameter-free, and usable and easy-to-interpret by non-statisticians. It is part of a bigger project: automating many fundamental data science tasks, to make it easy, scalable and cheap for data consumers, not just for data experts. Our previous attempts at automation include,Readers are invited to further formalize the technology outlined here, and challenge my proposed methodology. A recent application to salary estimation??,. Other examples are discussed below.,As in our previous paper, without loss of generality, we focus on linear regression with centered variables (with zero mean), and no intercept. Generalization to logistic or non-centered variables is straightforward.,Thus we are still dealing with the following regression framework:,Y = a_1 * X_1 + ... + a_n * X_n + noise,Remember that the solution proposed in our previous paper was,For convenience, we define W (the estimated or predicted value) as,W = b,When cov(X_i, X_j) = 0 for i < j, my regression and the classical regression produce identical regression coefficients, and M = 1.,You should add an intercept to the model, and the final estimate becomes W* = c + W, where c = Mean(Y - W). This way, you remove the bias: Mean(W*) = Mean(Y). The term c is called the intercept.,Terminology: Z is the noise, Y is the (observed) response, the a_i's are the regression coefficients, and and S = a_1 * X_1 + ... + a_n * X_n is the estimated or predicted response. The X_i's are the independent variables or features.,I have added more cross-correlations to the ,??consisting of 4 independent variables, still denoted as x, y, z, u in the new, ,.??Now corr(x, y) = 0.99.,The model proposed in our previous article still works relatively well, despite the enormous correlation between x and y. In short, the correlation between the observed and estimated response is 0.66, down from 0.69. If you use the classical regression, the correlation is 0.72. The loss in accuracy (from 0.72 to 0.66) is small, but the gain in robustness is big. If you bin my b_i's to 0/1/-1, you gain even more robustness, and the correlation still stays around 0.70. The variance reduction in all cases is about a factor 2.,With the new data set, the classical regression parameters are now very volatile and sensitive to extra noise (injected in the data to test sensitivity): a_1 = 4.243, a_2 = -3.816, a_3 = 0.078, a_4 = -2.015.,Note that if we compute the a_i's using my methodology and only the first 100 observations (instead of all the 10,000 observations), we lose a bit more accuracy: ??the correlation between observed and estimated response drops from 0.72 to 0.62. In the next section, we describe how to get a great estimate that is still very accurate (minimum loss compared to classical regression) yet far more robust, even if we compute the a_i's using only the first 100 observations (1% of the data set).,As described in section 5 in our previous article, we can improve the estimates by considering a model with two M's, namely M and M', where M applies to a subset of variables, and M' to the remaining variables. Now the estimated response is,S = M * SUM(b_i * X_i; i ,??I) + M' * SUM(b_j * X_j; j ,??J),where I and J constitute a partition of {1, ... , n}. In short we are clustering the variables into two clusters. Again, the goal is to minimize var(Z) = var(Y - S), with respect to M, M', I and J. There are 2^n possible partitions (I, J), so we can loop over all these partitions, and for each partition, find the M' M' that minimizes var(Y-S). Then identify the partition with absolute minimum var(Y-S).,The optimum partition will put highly correlated variables into a same cluster. In my example, since x and y are highly correlated by construction, one would hope that the optimum partition will be {x, y} forming one cluster of variables, and {z, u} forming the second cluster. So I manually picked up this particular partition ({x, y}, {z, u}) as good enough for our test. I then tried a few values of M, M' for this particular partition, and settled with M = 0.1 and M'= 1.0. Clearly, there is no overfitting here.,Interestingly, and despite the fact that I computed the a_i's using only 100 observations (1% of the observations), I ended up with a correlation of 0.70 between observed and estimated response (up from 0.62 if using only one M; the absolute maximum attained with classical regression being 0.72). Also, the variance reduction is as great as with classical regression. In the ,, my hand-picked parameters M and M' are located in cells P19 and R19 respectively, in the data & results tab.,In short, we have achieved the same accuracy as classical regression, but with far more robustness: our estimated a_i's are a_1 = 0.122, a_2 = 0.123, a_3 = -0.464, a_4 = -1.870 and are smaller (in absolute value) and more robust than the classical regression coefficients listed in section 2.,Even better: although x an y (the first two variables) are almost identical, their classical regression coefficients are of opposite sign:??4.243 and -3.816 respectively. Ours, as one would expect, are almost identical:??0.122, and 0.123 respectively.,Of course if you have many variables (n is large) then you might need more than two M and M'. But I'd try to always use no more than 4 M's for robustness. Note that 4 M's require visiting 4^n partitions to identify the optimum one. This is too large a number, so in practice, I recommend to visit only 1,000 partitions out of 4^n, and choose the best one among these 1,000. To make the algorithm run much faster, you can do your computations using just 1% of the data set (but no less than 100 observations). Now you have a robust algorithm with a computational complexity that does not depend on the number of observations (if your computations are based on a sample of 100 observations), nor on the number of variables. Pretty amazing! Thus you don't need a Map-Reduce framework to solve the problem.,Note that in the case where we use two M's, namely M and M', given a partition (I,J), it is straightforward to compute the optimum M, M' depending on (I,J). Let's use the following notation:,Then S = M * S_I + M' * S_J, Var(Y-S) = ||Y-S||^2, and the optimum is obtained by differentiating Var(Y-S) = var(Y - M * S_I - M' * S_J) with respect to M and M'. This leads to a straightforward system of 2 linear equations with 2 unknowns M and M'. You just need to solve this system to find M and M'. If you work with three M's, you'd have to solve a similar system, but this time with 3 unknowns M, M', and M''.,Just like we've clustered variables that were similar, we can apply the same concept to cluster observations into two (or more) groups, using a different M for each group. Or to cluster both variables and observations simultaneously. That's why I called my technique ,, because it's a simple, easy-to-use tool that can do a lots of things, including clustering variables and clustering observations - the latter one being sometimes referred to as profiling or segmentation.,However, in practice, if observations are too disparate for regression to make sense, I suggest using other techniques to cluster the observations. Adding one or two carefully crafted new variables can help solve the problem. Another approach is to apply ,??to bin the observations in hundreds or thousands of data buckets (each with at least 100 observations if possible), and apply a specific regression (that is, specifics M, M') to each bucket. This works well with big data. You could even choose M and M' so that they are similar to those computed on parent nodes, to preserve some kind of general pattern (and robustness) in the whole structure that you created: an hybrid HDT / jackknife regression classifier and predictor.,This article describes basic concepts of jackknife regression, a new tool for robust regression, prediction and clustering, designed to be implemented and used as a black box. The next steps consist of,Finally, we offer a $1,000 award to the successful candidate who,This project must be completed by August 31, 2014. You will be authorized to publish a paper featuring your research results, and ??your results will also be published on Data Science Central, and seen by dozens of thousands of practitioners. Your article must meet professional quality standards similar to those required by leading peer-reviewed statistical journals. Payment will be sent after completion of the project. Depending on the success of this initiative, and the quality of participants, we might offer more than one $1,000 award. ,??to find out about our previous similar initiative.,Ideally, we'd like you to also investigate the L-1 approach discussed in section 5, and compare it with L-2, especially in the presence of (simulated) outliers in the data. Though this could be the subject of a subsequent paper.,Email us at vincentg@datasciencecentral.com for details.
In connection with our proposed methodology to create a black-box, automated, easy-to-interpret, sample-based, robust technique called ,, to be used in small and big data environments by non-statisticians, We offer an award and massive promotion to the successful candidate who,This project must be completed by August 31, 2014. You will be authorized to publish a paper featuring your research results (with your name as main or only author), and your results will also be published on Data Science Central, and seen by dozens of thousands of practitioners. Your article must meet professional quality standards similar to those required by leading peer-reviewed statistical journals. Payment will be sent after completion of the project. Depending on the success of this initiative, and the quality of participants, we might offer more than one award.,????(see section 5)

Data Scientists Salary Survey shows that industry data scientists are in a sweet spot, especially in US, Canada, and Australia, with average salary $135K. European and Asian data scientists salaries are significantly lower.
The field of data science continues to grow, and with it come thought leaders who contribute to the industry through outreach and education. Many of the data science professors teaching today are leaders in the big-data field, speaking at conferences, writing books, and even creating groundbreaking big-data developments themselves. Find out which schools boast the most influential leaders in the data science industry.

This is a compilation has everything you need to jumpstart your skills in the core tasks of data transformation, modeling, and visualization.,tl;dr: Coursera and John Hopkins have a new course called The Data Scientist's Toolbox. ,MODELING,Below is a list of popular analysis from Rexer's 2013 survey. The table is biased towards customer transaction, text, and social media data. The average Rexer Survey respondent reports using 12 algorithms. This the industry's algorithmic bread and butter.,Analysis | Typical Use ,Regression. Simple trend lines.,Decision trees. Simple decision analysis.,Cluster analysis. Classification.,Time series. Stock tickers.,Text mining. Sentiment analysis.,Ensemble models. Bootstrap aggregating.,Factor analysis. Latent variable exploration.,Neural nets. Function approximation.,Random forests. Classification.,Association rules. Customer behaviour.,Bayesian. Pattern recognition.,Support vector machines (SVM). Classification.,Social network analysis. Marketing and advertising.,Uplift modeling. CRM up-selling.,Survival analysis. Reliability and risk management.,Link analysis. Knowledge discovery.,Genetic algorithms. Phylogenetics.,Splines (MARS). Regression. ,CRAN has pages dedicated to each typical task of statistical computing,Python has several packages tailored for statistical analysis including Pandas, Orange, PyBrain and Scikit-learn,TRANSFORMATION,OpenRefine is designed to help journalists and other non technical people organize incomplete data from different sources. ,A decent Scrapy presentation by David McLean. He shares his opinion about the evils of scraping.,David Beazley's Learn Python Through Public Data Hacking. Dabeaz gold.,Wes McKinney, creator of Pandas gave a 3 hour tutorial on data analysis,Feel more confident with your Python skills with Raymond Hettinger's Transforming Code into Beautiful Idiomatic Python.,VISUALIZATION (a.k.a. Web publishing),The best way to dive into visualization is by perusing the canonical examples in the d3.js gallery. Spend some time playing with the demos and ask yourself "How can this become more useful?" Indeed, most of the gallery is frivolous, but the potential is undeniable.,D3 tutorials have come a long way in a short time thanks to people like Scott Murray. He did a great O'Reilly webcast ,There's also youtube videos with Mike Dewar and Malcom Maclean has published D3 Tips and Tricks with Leanpub. ,Other noteworthy visualization platforms are DataWrapper, Flot, Highcharts, TheJit, Arbor.js, and Kartograph. These libraries depend on JavaScript, SVG and CSS, which is why I consider data visualization synonymous with web publishing. ,John Lindquist's Egghead.io covers the details of AngularJS ,Paul Irish is a star evangelist for HTML5 workflows. He uses a lot of tools in front-end dev.,Getting Started with Django by Matt Love is most excellent, incompleteness notwithstanding. ,Code Academy has interactive tutorials for Python, JavaScript and web fundamentals ,Professional trainers are slower paced and cover material that isn't in MOOCs or youtube videos:,Lynda.com - R stats - ,Tutsplus - Say Yo to Yeoman - ,Pluralsight - Twitter Bootstrap 3 - ,Codeschool - NodeJS - ,CBT Nuggets - AWS - ,You now have everything you need to make the rubber hit the road. Go find some cool data and get to work!
Published in ,. It shows the difference in cost-of-living between 2003 and 2013. However, I see two issues:,: , 
Data empowers business: it gives us the information we need to make the decisions that drive enterprises, industries, and economies. Big Data enables us to collect a massive amount of information (that we can store, search, share, and analyze) to bring us closer to the goal of finding trends that lead to smarter business decisions. ??Big data has big impact on businesses, governments, and societies, and its impact is continuously growing.?? And it gets even bigger than that.,Big Data has long been used by governments to prevent fraud in healthcare, secure global supply chains, alleviate traffic congestion, and much more., But Big Data?€?s benefits extend also to energy management.?? Big Data can help us reduce energy consumption and drive down energy costs, increase energy efficiencies and optimize processes, and make big progress toward our global goal of sustainability.,Big time.,Big data is used in energy management to gain transparency, identify trends and opportunities for savings, and anticipate (and prevent) problems before they occur.,As it relates to energy costs: it can and it does.??,In fact, excess energy consumption, though costly, is not only a financial problem.?? Wasted power is also wasting away our planet. ??,For example: the real estate sector.?? Real estate managers at large enterprises and government agencies already use big data for security applications, site selection, tracking costs, and managing projects. They can (and should) also effectively leverage Big Data for big energy reductions., Facility managers can use big data to benchmark a building?€?s energy consumption against the data on their entire property portfolio, they can find opportunities to save energy through operational efficiencies and equipment maintenance. ??,What kind of big savings are possible??? The state of Missouri uses big data in their building management to save over two percent annually for 10 years!,??,Big data solutions mitigate wasteful energy consumption by lifting the opaque veil of obscurity and providing transparency into system-level energy usage.?? Beyond increasing awareness into consumption, these , provide analyses that reveal patterns and trends, compare outlets, and identify maintenance issues and retrofitting necessary to avoid major equipment failures. ??,Enterprises and governments leverage the actionable insights these systems produce to optimize their energy use and reduce consumption throughout their operations, thereby reducing their energy bills by 30% or more.??,Big data of energy management leads to big savings of energy.??,Financial savings, of course, should be enough to motivate any organization to adopt energy efficiency solutions.?? But ?€?flexing your power?€? is about more than just dollars and cents.?? Environmentalism and capitalism are no longer mutually exclusive: they are now mutually profitable.??,Optimizing energy consumption saves money. Profits increase.??,Integrating corporate or government sustainability objectives into marketing strategies builds brand reputation; evangelizing it internally builds employee loyalty. Enhanced brand reputation and increased employee loyalty increase sales. Profits increase further.??,Selling sustainability is a two-fold proposition. Leading the effort requires success on both fronts. The benefits of a sustainability program must be sold internally to company stakeholders and externally to customers.??,Big data findings, once again, to the rescue.,To sell sustainability inside an organization is not just to sell it to the C-suite, thought their buy-in is critical to get the program started. Once goals are in place, sustainability requires the power of the masses. ,, VP of Sustainability & Philanthropy at AT&T, explains, "I think of myself and my team as chameleons. Being able to think and communicate in the same fashion as a business unit we're working with is, for me, the most important skill set needed to be successful." By using the information gathered by big data, statistics can empower the people.,Selling sustainability to customers is about branding philosophy. Here, too, the insights of big data deliver big results. ?€?Our company reduced its carbon footprint by 5% last year?€? builds a brand that people can trust . . . one they happily invest in and do business with.??,Imagining an espresso-drinking, designer-jeans-wearing, sports-car-driving techie getting together with a Chai-sipping, hemp-clad, bird-watching environmentalist seems comical, but that is precisely what big data has done.?? By simultaneously forwarding the goals of sustainability and profitability, big data has matched the unmatchable and achieved sustainable profits and profitable sustainability.



Msg 3101, Level 16, State 1, Line 1,Exclusive access could not be obtained because the database is in use.,Msg 3013, Level 16, State 1, Line 1,RESTORE DATABASE is terminating abnormally.,?€??€??€??€??€??€??€??€??€?-,:,First bring database offline to close opened connections, and than again bring online second you can continue to restore the backups for the database.


 This article provides a full demo application using??both the C# and R programming languages interchangeably to rapidly identify and cluster similar images. ?? The demo application includes a directory with 687 screenshots of webpages. ??Many of these images are very similar with different domain names but near identical content. ??Some images are only slightly similar with the sites using the same general??layouts but different colors and different images on certain portions of the page. ??The demo application can create a pairwise distance matrix including measures of dissimilarity between all 687 websites in around 67 seconds on my laptop. ??Next, the R programming language is used to perform Hierarchical Agglomerative Clustering grouping all similar images together in the same clusters. ??The application demonstrates how to create very tight, highly similar clusters or larger, very loose, slightly dissimilar clusters by adjusting the clustering cut height. ??After reviewing this article and the included demo project code, the reader should be able to identify similar images and weave together high-performance processing pipelines using inputs and outputs from both the C# and R programming languages., , While the C# programming language is "a??,??encompassing??,,??,,??,,??,,??,,??,??(,-based), and??,??programming disciplines", [1] R is an open source functional programming language [2] which has become very popular for more complex statistical data analysis. ??Typically, these two programming languages are seldom??used together. ??There have been attempts to successfully integrate R and C# within the same process using ,.??[3] ??However, this project is relatively??unused with around 93 downloads on its homepage as of today.,One of the primary advantages for using R is the large number of "packages" contributed by users across the globe which can be quickly integrated into any R project for added programming features and functionality. ??C# on the other hand has some extensive built in libraries for rapid parallel programming development which are simply hard to surpass. ??I was recently excited to discover that the standard R installation includes a program called Rscript.exe which can be used from the command line to execute R programming code contained within a file. ??After a little experimenting, I was able to write a small class within C# which will execute any R program file using Rscript.exe. ??In fact, you can even pass variables back and forth between the two languages interchangeably within a single processing pipeline., This may sound like a lot of material to cover. ??However, all of the code and clustering results can be reviewed using the fully functional demo application and sample images which are included in the Resources section below. ??The demo application was written in C# using Visual Studio 2013 and requires a full installation of the R programming language to work properly., The , class executes R programs from C# using the Rscript.exe program which comes with the standard R installation. ??This class simply starts Rscript.exe in the background and then passes it any number of command line arguments which can be accessed and used within the R program.,The only cumbersome part of using the??,'s??RunFromCmd() function??is making sure that you provide the correct location for the Rscript.exe program. ??On my machine, this was as simple as providing the value "Rscript.exe". ??However, on other computers??where Rscript.exe is not recognized as a valid command line command (i.e. a proper path / environment variable was not set), this argument may require a fully qualified path for the "Rscript.exe" program. ??My best advice would be to figure out what is required to enter in the command line window to start Rscript.exe and then use that value.,Prior to execution of the??RscriptRunner.RunFromCmd() function, the three variables workingDirectory, clusteringOutput, and cutHeight are separated by spaces and passed in as arguments to the function. ??Once these arguments are passed to Rscript.exe, they can be accessed directly from within the ImageClustering.r program.,Since the output file location for the??ImageClustering.r program was actually provided by C# as the variable "clusteringOutput" in Figure 2, the C# program can easily begin to process the output created by the R clustering program when the RscriptRunner.RunFromCmd() function call completes. ??The last foreach() loop in Figure 2 demonstrates C# reading in the clustering assignments created by the ImageClustering.r program., "Luminance is a photometric measure of the luminous intensity per unit area of light" which??"describes the amount of light that passes through or is emitted from a particular area, and falls within a given solid angle." [4] ??Relative luminance follows this definition, but it normalizes its values to 1 or 100 for a reference white. [5] ??When separated into RGB components, a luminosity histogram acts as a very powerful machine learning fingerprint for an image. ??Since these type of histograms??only evaluate the distribution and occurrence of luminosity color information, they can handle affine transformations quite well. [6] ??In simple terms, it very easy to separate the RGB components of an image using C#. ??In fact, using the "unsafe" keyword you can very, very rapidly calculate the relative luminance of each pixel within an image. ??I am certainly not the first person to think of doing this in C# either. ??There are several C# Computer Vision pioneers who deserve some credit in this regard. [7,8] ??For this project, I was able to quickly adapt the luminosity histogram feature extraction program contained within the , library.[8],Once an image has been processed, both vertical and horizontal??luminosity histograms are extracted and retained for similarity calculations between images. ??Figure 4 shows each pixel's red, green, and blue channels being isolated for use within the luminosity calculation. ??Images can quickly be compared for similarity using the similarity function shown in Figure 5 which calculates the deviation between two histograms using the weighted mean. ??Once both the vertical and horizontal similarity have been calculated, the maximum or average vertical / horizontal similarity is retained for each image pair to create the pairwise image distance matrix.,Keep in mind that there are many ways to extract features from images??and create image fingerprints. ??Many people recommend other approaches such as taking 2d Harr Wavelets of each image. [9] ??In addition, higher performance options such as uisng minhashing with tf-idf weighting have also been implemented.[10] ??Harr features have been successfully used for more complex Computer Vision tasks such as detecting faces within an image. [11] However, the speed and performance of luminosity histograms are hard to ignore, if the recognition quality meets your requirements. ??Other image feature engine implementations in C# such as CEDD: Color and Edge Directivity Descriptors [12], FCTH: Fuzzy Color??and Texture Histograms [13], a hybrid scheme using??NMRR and ANMRR values [14], and LIRe: Lucene Image Retrieval [15] can be located within the C#??application provided ,. [7], A parallel pipeline within the demo project is used to "map" images into luminosity histograms while simultaneously "reducing"????luminosity histograms into a pairwise distance matrix. ??For those of you wondering,??a pairwise distance matrix is simply a matrix that contains a measure of dissimilarity for all possible pairs of images considered. ??Dissimilarity for a pair of images is calculated as 1 - ??the Similarity between both images. ??The distance matrix will always contain (n^2 - n ) / 2 meaningful entries where n equals the total number of unique items in the matrix. ??This is due to the fact that all items will intersect with themselves down the center diagonal, and all values on either side of the diagonal are duplicated and only need to be stored one time., ,The pairwise distance matrix is required as input when using the ??Hierarchical Agglomerative Clustering function within??R. ??In order to create this matrix in parallel, a three stage parallel pipeline is used. ??First, pairwise image matches are created using a C# yield return enumeration. ??Each time the generateMatches() function in Figure 7 produces a pairwise match, processing stops and "yield returns" each match to the??createPairwiseMatches() function's Parallel.ForEach loop.,The??createPairwiseMatches() function shown in Figure 7 above, extracts features in parallel mapping images to vertical and horizontal luminosity histograms. ??Furthermore, the histograms for each image are saved in a hash table for quick reference since each image's features will be repeatedly matched to other images. ??Once the match features are extracted, the match is immediately placed in a thread safe blocking collection for further downstream reduction processing. ??While the mapping functions shown in Figure 7 are executing in a background thread, parallel reduce functions simultaneously execute processing each completed match produced to calculate the similarity between the match images.,Once this process has completed, a pairwise distance matrix is saved to disk which can be used as input into the R program's??Hierarchical Agglomerative Clustering engine. ??Similar to Figure 6 above, the final distance matrix contains pairwise distances for all images in the input directory., ,??, While there are multitudes of packages and options for clustering data within R, the base language provides functions for simple HAC clustering. ??The purpose of this article is not explain in too much detail how HAC clustering works. ??Rather, a demonstration of how HAC clustering can be used to identify similar images is provided.,The purpose of clustering is to divide a collection of items into groups based on their similarity to each other. ??Very simplistically... ??The HAC clustering of images works by comparing the pairwise distances of all images and then grouping them into a structure called a dendrogram. ??The "dendrogram" is a map of all possible clustering assignments??at various dissimilarity thresholds. ??The dissimilarity threshold dictates the maximum amount two images (or two clusters of images) are allowed to be dissimilar and still end up being merged??into the same cluster. ?? Once the dendrogram has been created, all images can quickly be assigned to clusters using any dissimilarity threshold value, which is referred to as the "cut height". ??The cut height is typically provided by the user. ??This process can also occur in reverse with the user requesting a particular total number of clusters at which point the algorithm calculates the best cut height to achieve the requested result. ??While a small cut height will produce smaller clusters with highly similar??images, a large cut height will create larger clusters containing??more dissimilar images.,What this all means is that the dendrogram maps out all possible clustering memberships based on each image's dissimilarity to the cluster as a group. ??This can be done using each cluster's minimum, maximum, average, or centroid distances. ??Depending on what measure you choose to use, the clustering type is referred to by "nerds" as either single-linkage (minimum), complete-linkage (maximum), UPGMA (Unweighted Pair Group Method with Arithmetic Mean) (average), or centroid-based (centroid) clustering. ??While there are many types of clustering??methods, these seem to be the most common.,The clustering algorithms typically work by starting out with all images in their own individual cluster of 1, and then successively combining the clusters which are in closest distance proximity based on the distance metrics described above. ??The clusters are combined until no more clusters can be joined without violating the provided cut height threshold. ??While this description is a slight over-simplification, additional research regarding this topic is left up to the reader., ,Looking at the dendrogram in Figure 10 above, it is easy to see that a cut height of 0.60 would produce only two clusters containing all 40 images. ??In this case, the two clusters are very large and likely contain many dissimilar images since the cut height threshold allows images with a distance of up to 0.60 to be included within the same cluster. ??In the other extreme, a cut height of 0.10 places all but 2 images into singleton clusters containing only one image each. ??This is due to the fact that images must be at least 90% similar to be included within the same cluster. ??Using the demo application, the cut height can be adjusted to explore the impact on clustering similar images.,Figure 11 shows a 10% reduction in cut height forcing the third image out of the cluster. ??Since two of the images are highly similar, they remain in a cluster of 2 once the cut height dissimilarity threshold is reduced to 0.25. ??It is important to understand that the optimal cut height for image clustering will vary greatly depending on the types of images you are trying to cluster and the image features used to create the pairwise image distance matrix. ??Even within the sample images provided, strong arguments can made for adjustments in the cut height depending on individual goals.,Conversely, if one were trying to identify websites made from the same template, all of the images above would be clustered acceptably. ??In fact, you would even want these images to be included in the same cluster, if they had different color schemes. ??In this instance, a more generous cut height might be applied, and in some cases, different features might be required for the image matching exercise at hand.,Once the image distance matrix is saved to disk using C#, the ImageClustering.r program reads in the file and converts it to an R distance matrix (dist) object. ??Next, the function hclust() creates the dendrogram mapping using the UPGMA or "average" distance method. ??The cutree() function then cuts the dendrogram to the "cutHeight" which is specified by the user. ??It is important to note that this value was actually specified by the user on the demo application's form. ??This value can be seen being passed as an argument from C# to the RscriptRunner's??RunFromCmd() function in Figure 2. ??It is also seen being captured within the ImageClustering.r program in Figure 3. ??The R clustering program is very efficient clustering the 687 images in just over 1 second on my machine. ??Finally, the file names are sorted by cluster number and written back to disk., This article successfully demonstrates that the C# and R programming languages can be combined to create powerful parallel processing pipelines using MapReduce style programming??and harnessing the analytical powers of R as needed. ??Information produced in both R and C# can also easily be exchanged and combined across programs when necessary. ??The demo application provided in the Resources section below uses the powerful parallel programming libraries in C# to rapidly extract and compare luminosity histograms from images creating a pairwise distance matrix for all images contained in a directory folder. ??Next, C# uses R to perform HAC clustering to combine similar images and then display??the output from R on demo application's form. ??The demo application gives the user a thumbnail preview of the currently selected image row and also the next 3 images below the currently selected image. ??This allows the user to change the clustering cut height and quickly re-run the R clustering program until they are satisfied with the image clustering results.







This permanent experimental design setting allow you to learn, participate or check out the results at any time, as data is gathered and reported in real time. This article illustrates a few concepts:,You can actively participate and earn a reward (up to $1,000) or passively look at how the test progresses day after day (or even second after second), and offer your explanation to the problem we try to solve, by analyzing the data available to anyone.,When we send an email to more than 100,000 subscribers, we want to check the performance of the campaign. We measure the performance using various metrics, such as clicks on links, open rate, etc. Some email blasts are text only (for instance LinkedIn), and the only way to track performance is using links that are redirects. We could create our own redirect system for better monitoring, but so far we have relied on bit.ly and goo.gl URL shorterners, which also provide traffic monitoring in real time. These statistics are sometimes used for traffic attribution.,We suspect that the numbers reported by bit.ly are wrong, sometimes inflated by a factor 3, sometimes by a factor 10. We suspect (based on gut feelings) that a glitch in Outlook is responsible for discrepancies, causing clicks to sometimes be double or triple counted, if not worse. This problem appears with user clicking from an email message, not from a click on a web page.,The question is: how do we assess the bias? How can we do a test to measure the discrepancy?,We proceed here with classic experimental design. We observed the problem with bit.ly, and we assume that Google's shorterner (goo.gl) is more accurate. Thus we want to compare bit.ly (test) numbers with goo.gl (control).,We have created 2 test messages A and B (see section 5), each featuring 4 links:,The first message contains the following links:,The second message contains the following links:,All links point to the exact same page, but we used different tags to create 8 distinct URL's. This allows us to track the traffic separately for each link. In particular, it helps identify the effect of link position in the message, on the number of clicks. Note that bit.ly and goo.gl links are interlaced, with goo.gl in first position in first message, and bit.ly in first position in second message. This is how experimental design works, to eliminate the effect of external variables such as link position. Ideally, you'd like to do a test with more than two messages, especially if you want to assess variance in your estimates.,The traffic statistics (page views), broken down per day, are available at the following URL's, in real-time (each new click appears instantly in the traffic report, you can check it out yourself):,First message:,Second message:,You can use these links to collect data. In addition, a total page view count, called Ning count, is available (also updated in real-time) ,, although most of the traffic reaching this page does not come from our test.,A potential issue is the fact that both goo.gl and bit.ly are not different, but both victim of the same glitch, thus both numbers would be inflated. Of course, the first thing to look at is whether these numbers are similar.,A big improvement to this test consists in building two artificial (test) pages rather than using a single, popular page: one page where bit.ly traffic is directed, and another one where goo.gl is directed. These two pages would only be accessed through the test, so that the Ning count should be equal to the total bit.ly clicks count for the first page, and to the total goo.gl click count for the second page. But it's not a perfect fix: there is no way to guarantee that traffic would come only from our test, plus Ning numbers might be wrong - too low or too high. Actually, we also use two additional mechanism to monitor traffic: Google analytics, and stats provided by our newsletter management vendors.,Other potential issues:,There are different ways to participate.,You can share this experiment with your students, and they can actively participate.,You can gather the data, analyze it, assess if there is a discrepancy between bit.ly and goo.gl (an even look at daily numbers from the Ning count if it helps), and post the results of your investigation in the comment section below. If you find an explanation to the discrepancy, that would be even better - it would prove (assuming your conclusion is different from mine) that data analysis can beat gut feelings.,Another way to participate, and win an award, is to contact me at vincentg@datasciencecentral.com, at least 24 hours in advance, to let me know that you are going to email or post copies of , and/or , (see below) on a specific date, for your own testing purposes. You can modify the messages, as long as the 4 URL's are unchanged. If more than 100 clicks are generated across the 8 target URLs, you will be paid $1 per click (capped at $1,000) for the traffic generated that day - the whole traffic, not just yours - as reported by bit.ly and goo.gl. You must use a corporate email address to participate (to contact me), or else have a profile on Data Science Central, with a link to your LinkedIn profile. Fake clicks do not count, and we have sophisticated mechanisms to detect them.
The commodification of labour coincides with technological advancements in production: it is perhaps most noticeable in relation to factories. ??Factory processes replaced the labour once done by skilled tradespeople. It might not be obvious how this trend has continued to this day and is now affecting professionals in complex fields including those in the data sectors. I am talking about the "made to order" and "off the shelf" acquisition of labour commodities. What I describe as commodities enter the production system as contract and temporary employees hired to handle specific jobs and projects beyond which their services might no longer be needed. Although this is the general idea behind labour commodification, in practice there can be some important structural risks.,I will momentarily reflect on my father's periodically visceral stories of meat packing facilities. He was a mechanic for much of his life. One job that he used to do was dismantle, clean, and reassemble meat-cutting equipment. He said that as he was trained, he also trained others, and this was how people learned to perform the job properly; in effect, he was posing the job of cleaning the equipment as a skilled trade. Many years after he left the industry, I recall extensive coverage in the local news of listeria outbreaks in meat packing plants. Now, for a mechanic actually examining the equipment every day, the risks posed by improperly trained staff or perhaps no staff at all are fairly apparent. The same can't be said when human resource decisions are made in settings disassociated from production realities.,Fields involving extensive amounts of data - I include programming among them - can also suffer from being disassociated from decision-making within the context of commodified labour. On one hand, it might seem logical to sporadically bring in individuals only as required - or to have them do particular jobs only when needed - based on perceptions of efficient production. I have chosen actually not to dispute the argument of "efficiency" since the metrics are plainly evident; business decisions can and perhaps should be made to achieve specific measurable objectives. I will counter this argument by posing some of the problems faced by skilled tradespeople. Not everything can be purchased off the shelf. Useful activities such as training do not always register on the metrics except perhaps as labour costs.,The idea behind creating a profession is to bring together a body of people with certain shared competencies. This might not seem like the commodification of labour, but actually it is definitely. Once pooled with comparable abilities within a particular field, it is possible to replace one lawyer with another; go to a different family doctor to get the same or similar quality of care; hire a qualified accountant from a market containing many. So it can be tempting to say, if this model works in different fields, the results should be similar in relation to those in the data sectors. However, consider the structural implications of commodification: change would likely be driven by extraordinary events through the executive rather than by routine interaction through quotidian processes. Change would tend to occur after problems happen; also, the changes might only overcome the specific problems at hand. Perhaps the most critical structural impact would be the insulation of the organization from learning.,Particularly in relation to data - something that carries attributes of a commodity - I believe we have this focus on the data itself with little emphasis on the weight of acquisition and analysis. There is an enormous amount of learning or structural capital that goes towards the processes and systems in place to collect and make use of data. I want to distinguish between the "learning" and the "systems" associated with data management and analysis. It is possible to have a system that nobody knows how to use. Under normal conditions, the system extends from or emerges as a consequence of the learning. Under the commodification model, I suggest that the situation is rather reversed: there is systems acquisition followed by numerous glorious battles and hell-fire in the ensuing attempts to learn. This is an unstable model.,The commodification of labour can be easily detected in job listings that ask for a number of unusually specialized skillsets; all of these tend to suggest an inability or lost ability to provide training. When I say "inability," this does not mean the organization cannot do it but simply that its business model might not allow for it. Considering the situation in more structural terms, such an organization has placed its operations and perhaps its future on the commodification model. What risk does this pose? Well, if it is true that an organization can set up shop locally and acquire labour commodities to perform specific jobs for production, then it must also be true except in protected markets that any organization can set up shop anywhere to produce the same products or offer similar services. The winner will be the organization that can provide the cheapest prices. So if the worker can be easily replaced under a particular production model, so can the company.,We therefore have this situation where a company that is highly information-oriented might find itself in an inferior competitive position on one hand and also unable to naturally change its internal circumstances. There is a disconnect between what it does and what it has learned to do. I am describing termination by adaptive failure - a problem that I feel is often pronounced during periods of austerity. I suggest therefore that neither complex fields nor complex professions can be easily sustained in times of constrained resources; in fact, reductions in complexity will probably be a persistent aspect of our time. We will see a deterioration in benefits from complexity in the years and possibly decades ahead together with sharp increases in organizational risks. ??We confront this volatile situation applying a relic of production systems - labour commodification - to the data sectors. ??Since commodification is difficult apply, and has been an instrumental in the decline of manufacturing, it seems all but certain to fail our needs in the age of abundant data.
Source: U.S. News,See 
Responsiveness and clarity, perhaps more than in any other industry are crucial to Telecommunications.,Challenged by the advancing communications demands of a ?€?smartphone generation?€? over the last few years, the role of communications service providers (CSP?€?s) and the data they offer is increasingly valuable, owing to the sheer quantity and quality of the unstructured data they produce.,Think about it. From mobile network to Internet providers and more, CSP?€?s are uniquely placed, sitting right in the driving seat of big data generation. But just because they?€?re creating data, doesn?€?t mean they?€?re using it to its full potential.,So how can the telecommunications industry take advantage of its perfect data position and insight, to deliver better service and enhance their understanding of customer needs and behaviour? How can it transform its data from a ?€?difficult-to-process-and-monetise?€? by-product of digital communication into a valuable, optimised asset?,Big data has changed the way CSP?€?s, and indeed the way most organisations think. Facilitating the ability to be reactive and responsive in real-time, big data allows telcos to better themselves from the core ?€? from advancing competitively, to increasing profits, generating new revenue and tailoring services and product to suit demand.,To achieve this, the data focus should be on using consumer insight to become as consumer-centric as possible; adopting a personalised, consumer focus in marketing, product development and interaction efforts.,Allowing each aspect of an organisation to co-operate efficiently, effective big data allows telcos to:,These data uses for telecommunications companies are by no means comprehensive, but are a small insight into how big data solutions can be used to enhance communications and business within a communications industry. Collecting vast amounts of data everyday from routers, access points, social media and more, CSP?€?s are of course also in a position of responsibility - the information they gather is of monetisable value to other industries as well as to themselves.,Because of the sheer volume, variability and velocity of data, a common issue for CSP?€?s is the difficulty in analysing information in, or close to real time. Cooperation with big data providers resolves this, as new technologies and analytical frameworks like Hadoop can help ?€? enhancing the data to clarify the??,??as well as the??,. In addition, regulatory data and stringent privacy policy compliance is also ensured.,An example of how clearly managed data can uncover the ?€?what?€? and ?€?why; a CSP client, collaborating with data providers noticed a correlation between calls to premium rate numbers and customer churn. When customer profile data was examined to determine why, it became clear that the cost of the calls was not the issue, but that the customers themselves were financially vulnerable. The numbers they were calling were financial service companies, such as credit agencies.,Data providers are therefore key to helping organisations capitalise on previously unmanageable, unclear data, facilitating the reduction of costs, and the increase of profit and loyalty retention, while maintaining consumer trust.

This is a great introductory resource for those interested in R. but have not got a simplified version??yet with strong????programming functions of R.????It makes statistical concepts easy to understand and visualize ,complex statistical concepts.
There's a lot of confusing jargon and buzzwords in this new field. It helps to know who some of the major players are and what services they offer. This list is a mild introduction and far from exhaustive.,Amazon Web Services: Infrastructure as a service (IaaS). EC2 virtual servers, S3 storage, Mechanical Turk, analytics, and more.,Yandex: Russian competitor for google. Recently launched Cocaine server based on Docker.,Salesforce: Customer Relationship Management (CRM). Acquired Heroku in 2010.,Heroku: Platform as a service (PaaS) for hosting Ruby on Rails, NodeJS, Java, Python Django, and more.,EMC: Enterprise content management (ECM) and big data analytics.,Netflix: An AWS success story. Have implemented several improvements based on Kaggle competitions.,Kaggle: Bounty engineering competitions used by NASA and Wikipedia. Famous for 3M$ Heritage Health Prize.,Zementis: Predictive analytics for big data and real-time scoring.,SAP: Enterprise resource planning (ERP). Enterprise system oriented architecture (SOA).,Crowdflower: Crowdsourcing leaders. Categorization, content generation, image moderation, sentiment analysis, transcription, and more.,Rexer Analytics?€?s Annual Data Miner Survey: A must-read for newcomers.,For small time data slingers, "the cloud" has a simple interpretation: we can rent AWS EC2 instances by the hour and S3 storage cost pennies a month per gig. No need to buy a new MacBook Pro - whenever a project needs more than your old pc can handle, "move it to the cloud." Vagrant, Docker and Ansible take away a lot of the hurry-up-and-wait IT takes to configure a machine.,"Scalability" is an overloaded term, which usually leads to premature optimization. Don't get distracted by buzzwords. Twitter, Netflix, and Oracle IT managers worry about the problems of data volume, velocity and variety. We're not building Netflix.. but we might work for them one day. The goal is to learn the popular tools that Big Data start-ups use without going into the fine details of robust deployment.,A great talk on optimization and scalability of analytics is David Schachter's ,How to Speed up a Python Program 114,000 times,Schachter starts with low hanging fruit, re-writing the analyst's poorly written code, and stops when his optimization gets tied to hardware. He also explains why Hadoop isn't viable for his purposes and makes fun of Twitter engineers for using LAMPP for mobile communication. ,I stick to Python as much as possible. I also only want to learn tools that promise longevity. In this battle between old comfort and new features, JavaScript often beats Python. NodeJS MV* web frameworks are increasingly popular, d3 is written in JavaScript, HTML5 apps are replacing desktop programs, and the language is a prime candidate for architects who are trying to "move code to the data." JavaScript for longevity is a safe bet.,My favourite resource for learning JavaScript was the epic series Crockford on JavaScript , ??,Crockford recently published a talk on concurrency called Monads and Gonads, which helps to understand promises ,But I'm a Data Scientist, not a web developer!,If you are in favour of specialization, then you already have an excellent career. The aspiring Data Scientist that can build a website and scalable REST services is much more likely to get hired on to a Big Data start-up. Nobody can know it all, but understanding context of the tools makes your abilities fit well in the team.,Back in 2006, Tim Berners-Lee described the Semantic Web as:,..an overlay of scalable vector graphics ?€? everything rippling and folding and looking misty ...integrated across a huge space of data..,Web 3.0 has been elusive so far, but we're almost there. Mobile web apps are becoming increasingly more data intensive, while Data Scientists are learning how to build REST services and MV* websites. Both industries are starting to use the same tools and skill sets are overlapping. The semantic web is waiting on that "huge space" of integrated data, which will emerge as the two industries mature enough to converge as one.,My corollary to Berners-Lee's quote is:,Web 3.0 => The Data Science Toolkit == The Future Web Toolkit,Next month: My Boot Camp Curriculum

All of us at some point in the process of examining data, check for correlations among different variables in the data especially pair-wise correlations.,Among a large chunk of business analysts in industry, there exists a notion of??,??being the only criterion for pair-wise correlation and hence at the maximum a??,??is run in SAS to check for the same.?? This ??will of course be useful for finding out correlations,. However it more often than not,which frequently contains,etc.,Read the full article and get the solution here: ,Let us know your thoughts.,Edvancer is an online analytics training institute and offers courses across the field of analytics.
We are all increasingly active in the digital space. 70%+ of people in the EU, and growing, use the Internet*, all contributing towards more data generation. But public misperceptions and perspectives on data and how it?€?s used for marketing, threaten to limit data?€?s potential curtailing marketers?€? abilities to provide personalised services. Careful data usage greatly enhances our lives. Unfortunately, fear or irresponsible use (a thankfully rare occurrence), along with some sensationalist journalism has led to a generalized perspective of data as ?€?bad?€? or ?€?creepy?€?.,??,Misuse has serious consequences. But data is neither ?€?good?€? nor ?€?bad?€?. It is merely a tool, facilitating an action. Transparency of use and benefit ?€? keeping consumers informed - is the way to build trust, allowing both sides to take a balanced, informed perspective on data use.,??,Simply being able to access the right customer data and use it for your customer?€?s benefit is not necessarily straightforward. It requires customers to willingly share their data, trust, and understand why they should do so. Legislation and regulation must also be appropriate and fair to allow a balanced approach to data.,??,??,When it comes to data use, most consumers fall into one of three categories: The unconcerned (16%), pragmatists (53%), and fundamentalists (31%).** (source DMA, Future Foundation 2012),??,Those who are unconcerned with how their data is used, or what for, tend to be male and under 25. Pragmatists are likely to be under 25, female and accepting of data use, as long as it is responsible. The most resistant segment, the fundamentalists, tend not to use social media or the Internet in general, and mistrust data use - they would like to see increased structure and regulation.,??,Of course customer data must be used responsibly, kept secure, and shared appropriately ?€? and legislation, regulation and reputation are there to ensure this. These are the ?€?table stakes?€? for ??trust - the number one reason people will agree to share data. However, 65% of consumers now expect organisations to use data to deliver better services and marketing, so after ensuring security, marketers must build on that trust and fulfil those expectations ?€? turning the initial trust into personalised service and products of value which in turn leads to more trust and a virtuous circle.,??,??,As data generation increases, perspectives on data usage evolve. 70% of people believe that attitudes towards privacy are changing, with 80% believing that disclosing data is just a normal part of life today. But while your customers may have particular expectations when it comes to data use, trust and sharing, how much do people really understand about data use, and its implications?,??,Do customers know??,??they?€?re trusting data use and what they expect as a result from it? Do you?,??,When putting data to monetisable use, the potential possibilities are infinite. But??,??means keeping the balance between too much, and too little. Your customer data will only aid your consumers if used in the right way.,??,Freedom of data use is a constricted thing ?€? if we allowed a world where data is used too freely, we might see the healthcare and financial industries (health insurance for example) accessing our food histories, seeing if we spend on alcohol, linking with gym membership data (or lack of), and deciding what to charge us for services. A fundamentalist?€?s worst nightmare. But in a world without any data freedom at all, we would be unable to personalise anything ?€? marketing and advertising would be unbiased, and importantly irrelevant and untargeted. Those interested in extreme sportscould receive adverts for retirement homes ?€? and vice versa. Indeed a serious scenario for consumers is if draconian data restrictions lead to broken advertising. If this happens, then the providers of free digital services such as search and social sites, will consider charging for their services as advertising revenue will dramatically fall.,??,??,Personal data does not have a one-use-fits-all approach. Responsible use is there to balance data in the middle, away from the extremes of the spectrum, and is something consumers should be aware of.,??,With a balanced approach, we can access the right, specific consumer data and ensure it is appropriately used - targeted to best serve the relevant consumers. Basically, data puts the consumer at the heart of an organisation ?€? which is the key to securing long term loyalty, much wanted and better services and products, advanced customer experiences and increased revenue.,??,*Information throughout, courtesy of Acxiom UK?€?s whitepaper ?€???,2013

On 19,Feb, 2014, Facebook announced the acquisition of Whatsapp, a mobile messaging service with,of cash and Facebook shares in total. This figure seems insane for a company which charges some of its users US$ 0.99 every year for its services and reportedly had,last year.,Considering that,, it cannot just plonk down US$ 19 bn on the table without being able to show its investors & shareholders,. So, how is Facebook going to generate returns on this large investment??? Surely, any increase in Whatsapp?€?s revenue is going to be too low to generate any significant return. Also, as per the stated intention of the founders of Whatsapp, they are likely to,.,On the other hand more than,on the network. To improve its capabilities to be one of the best digital advertising platforms and to beat Google is key for Facebook?€?s future growth and monetization of its user base.,For those who have used Facebook Ads, you would have noticed the fantastic targeting capabilities that it provides given the vast information that it has collected. It also provides an even better option called,. To explain Custom Audiences in short, if a marketer has collected,of its prospective or existing customers it can upload the same to Facebook, which will match them with its users and,. ??This is one of the,that are provided today in the digital medium.,So the question to ask is apart from a large user base and fast growth (Whatsapp is adding 1mn users a day),,that Facebook doesn?€?t or couldn?€?t get its hands on organically?,Read the complete article here: ,Edvancer is an online analytics training venture and offers courses across the field of analytics. Learn more at 
This weekend sees Super Bowl XLVIII come to New York (yes, we're well aware that the stadium is technically in New Jersey). Earlier this week one of our data scientists noticed the Empire State Building lights putting on quite a show. A quick search revealed that the iconic building's lights are being used as a giant 'sentiment meter' to show the results of a Twitter war between Broncos and Seahawks fans. Fans were instructed to tweet responses including the hashtag #WhosGonnaWin to a series of questions about the upcoming game, with??,.


The mantra is famous in Hollywood history: ,?? It brought fear to young viewers everywhere. But, as the story goes, it was soon obvious that there was nothing to fear.??,With so much Big Data noise, and hype, and pressure (oh my!) pressing in on us from all sides, there is understandable fear and loathing around the concept of Big Data. In my opinion, the way to relieve any such concern is to address it objectively. And the way to do that is through a scientific approach -- , through Data Science and the scientific application of Analytics.,With that in mind, and as a resource for anyone interested in further reading, I recently??posted here 2 lists of books:,There is one more book that deserves special attention (to readers here), and that is Vincent Granville's upcoming Data Science book -- check out the awesome content here:??,.??,The ,??gives this additional information:,The applications are endless and varied: automatically detecting spam and plagiarism, optimizing bid prices in keyword advertising, identifying new molecules to fight cancer, assessing the risk of meteorite impact. Complete with case studies, this book is a must, whether you're looking to become a data scientist or to hire one.,We are looking forward to the publication and release of this book in March 2014. It will provide a thorough resource for Big Data practitioners seeking information on??Data Science, Analytics, and Machine Learning. Oh my!
Consumers are demanding greater financial flexibility and experiences. 2013 was the first time in the history of banking where the numbers of transactions from on-line banking trumped the number of retail banking transactions by 6:1. The younger generation prefers on-line banking vs. retail banking, and the older generation is adopting technology faster than ever before.,New sources of data are emerging from businesses, public data, phones, cars, social media, location data, health data and iBeacons.??,The older generation is very keen on retail banking and they are also demanding targeted experiences in the branch.,Consumers are becoming increasingly mobile, dynamic and are demanding their financial services organizations be woven into their lives in meaningful ways.,Mobile banking, and mobile payment options are disrupting banking operations and introducing competitive threats.??,Banks are struggling to find new sources of revenue with the above trends, regulations are becoming tighter and many new competitors are emerging providing greater experiences (on-line and mobile) than the classic financial institutions.,????,Banking is on the verge of major transformation.?? The new sources of data, competitive pressure, and the rise of on-line banking combined with the transformation of retail experiences and demand for segment of one services will provide the best possible consumer experience.??,Today retail banking employs multi-decade old practices for consumer experiences.?? Existing clients of a bank walk up to a teller and the experience of the customers are very simple: balance inquiry, withdrawal, deposit, referral to a lender, move your account to a fee charging account and/or to a certificate of deposit.,The analytics which tellers, branch manager, customer service call center representatives, wealth managers, brokers and analysts have access to are extremely limited resulting in poor consumer experiences and revenue declines.,The consumer on-line experience, although built in the last decade, is purely a transactional experience with little value beyond convenience.,Banks need to focus on solutions that provide retail branch tellers, retail branch managers, banking call centers, and wealth managers with micro (hyper)-contextual segmentation analytics and real time sentiment insights.,These solutions must include, but should not be limited to the following data sources:,??,The solutions should include a business intelligence visualization, and exploratory dashboard, and recommend next best action tools. These solutions should be utilized by retail branch managers, retail branch tellers, banking call centers and wealth managers, not just be built for analysts or data scientists.,To effectively create such dynamic access to data, and enable it to learn ongoing these solutions must support, but not be limited to, natural language processing as one of many inputs.,Intelligent digital agents (AI) should also be deployed that can simulate massive amounts of models simultaneously.?? These agents will validate semantically both structured, unstructured and perpetually connected consumer data by the feedback provided from a front line employee.,The insights should be presented in a digestible form providing:,??,The focus on a big data initiative for banks looking to improve the customer experience should be centered on understanding what consumers?€? value, and orienting products, services, and segmentation around those values.,Today the consumer is shaped around the data, to effectively achieve a true customer-centric experience the banks need to turn the model upside down, and begin to shape products, offerings, and resource planning around the consumer.,Knowing that 70% of the calls going into a call center comes from customers that love sports, could make it easier to plan hiring, and training beyond the basic product, and service skills.??,People like to do business with people they like.,Banks who focus on understanding, and shaping their business around the consumer and shaping its offerings, and resources accordingly will become more likable.??,More likeable means more business.??,It?€?s not a complex formula!,??

??,Now-a-days, Banking Industry is facing many challenges - rapidly changing consumer environment, rigorous regulatory guidelines, highly competitive environment, emergence of new channels to name a few.,??,With these great challenges come great rewards?€?Banks have the opportunity to pull ahead of the curve or completely fall behind. Accurately applying Data Science will separate the leaders from the followers.,??,??,??,??, Some banks have pioneered integrating Data Science in their decisioning process and leveraged the massive volume of available data to their benefit. For example - Several large U.S. banks are using big data to understand how customers use their different channels, such as branches, online, mobile, call centers and ATMs. Analytics has been instrumental to Capital One, which has grown earnings per share by more than 20% each year since it went public in 1994, and has grown to be the third-largest provider of credit cards in the U.S. ??Citi, for its part, is experimenting with new ways of offering commercial customers transactional data aggregated from its global customer base, which clients can use to identify new trade patterns. Bank of America has been able to shift the bulk of technology spending from activities around consolidation to investing in more innovative technologies, such as Big Data. BOA can now run BankAmeriDeals with cash-back offers to holders of credit and debit cards based on analyses of where they have made payments in the past. KeyBank created a central analytics organization and had direction from the CEO down to break through fiefdoms.,??,However, many banks have not yet been able to generate actionable insights from available data or effectively integrate data science the way they think and operate. The key is to start small and take manageable steps toward incorporating Big Data analytics into their operating models. It is crucial for all banks to realize the importance of Data Science and act on it ---- or they can ignore this at their own peril!!!!
Interview with

Many, many moons ago I began my first data science project. It involved a search algorithm for selecting the most probable mode of inheritance for a congenital anomaly given 10,000 two-generation pedigrees. Given the complexity of the algorithm I had to do my own coding: in FORTRAN ! It was tedious to say the least but I felt that I thoroughly knew the ins and outs of the methodology as a consequence writing code.,After that project I discovered a new analytical tool that promised to alleviate most of the??required coding. It was called SAS (at the time the User Guide was a single book), and it took care of my needing to write much code. As SAS proliferated so did my use of it. Soon I was exclusively using SAS for all of my analytics.,Then a paradigm shift occurred. Machine learning offered a new way to approach previously intractable problems. Off the shelf niche programs were initially used but eventually got supplanted by offerings from SAS and other analytic titans. I was so hooked on writing minimal code that I continued using canned programs even though they left a lot to be desired.,Eventually I felt so stymied by canned code that I began to look at MATLAB. True, that product could also be used through minimal coding, but just learning a bit of MATLAB programming dramatically increased its capabilities.So out went the Stanley toolbox software, replaced by MATLAB. Unfortunately, MATLAB's neural network and optimization toolboxes were prohibitively expensive. I wrote a genetic algorithm in MATLAB but it felt "coarse" and not very extensible.,I had an epiphany: what if I went back to doing most of the coding myself. I could construct algorithms exactly the way I wanted them along??with a nice GUI. There was a buzz about a language called Python that seemed to be gaining traction in the data science community. Fast forwarding, I am where I was many, many moons ago; writing the majority of the code (in Python, not FORTRAN) for my analytic methods. Yes??computers are incredibly faster than when I started out and more data can be stored on a flash drive than on a??hard drive of yesteryear. But at the bottom line is the fact that if you want to do serious data mining, then rolling up one's sleeves and writing code is unavoidable.,??,??
Here is a nifty IPython. Check out the graphs:
Can data mining techniques accurately predict the medal counts at the Olympics??? This question came into mind four years ago while I was watching the Winter Games in Vancouver.?? I knew that a predictive model could give us an estimate of the number of medals each nation might win; but how close could we get to the actual outcomes? It was a tantalizing project ?€?.???????????? 
When I stumbled upon the phrase "Data Scientist" 3 years ago, I immediately recognized it as my best prospect for a productive career. How to start? What are the tools of the trade? , , This is the blog post I wish I could have read back then. , Many of the things I list here didn't exist or were unstable until recently., , I discovered the "predictive analytics" rabbit hole and started to read and watch whatever I could find on the subject. Upon watching the Gigaohm interview with Flip Kromer (,), the rabbit holes multiplied. I watched the interview several times to make sure I had everything in his data management laundry list:, , PostgreSQL, MongoDB, HBase, Cassandra, ElasticSearch, Redis, Tokyo Tyrant, , Chef/Puppet/Ansible, , Every aspiring "Big Data" worker should watch his interview., , Which of these databases will I need for my DS career? As I learned the different flavours of NOSQL, it became clear that I need to learn ALL of them - different nails need different hammers., , A Flip Kromer quote driving my current project is "We need a Mechanical Turk that slides up the talent scale." Mechanical Turk for specialists is a major frontier of Data Science, and one I'm most interested in. My favourite example is FoldIt , . I call these "Data Workbenches": the turning point towards visualizations and browsers doing more than just browsing., , Visualizing data is the most glamorous of the DS skills, and most of us are dazzled with d3.js and feel love at first sight. Falling in love with d3.js brings a new set of rabbit holes: HTML5, JavaScript, CSS, SVG, and NodeJS. , , Learning NodeJS (closures, promises..) dumps even more tools on the list and my learning curve is turning into a wall. , Can somebody please help me put all of this in context?!?, , , Thank you Hadley Wickham! The 3 main competencies of a Data Scientist is TRANSFORM, MODEL, and VISUALIZE., , TRANSFORM, Python: Pandas data.frames, R: plyr and dplyr, , MODEL, Statistical programming becomes straightforward when you know what stats are needed for your data., Take a look at your data, learn some stats, and then go shopping for modules., Python: numpy & scipy modules, R:, , VISUALIZE, Python: matplotlib, R: ggplot2, HTML5: d3.js, , Wait! That's not it. We still don't have any data! We need to get some and make it machine readable., , GET DATA, Scrapy web scraper (python libxml2 and libxslt), PLY: Python Lex and Yacc, , There's a few tools missing that we need to set up an environment, Vagrant, Docker, Git, , Does a Data Scientist really need to learn all of these tools? Yes. These, and more still. Serving data will be discussed in my next post., , Enterprise people might recognize that I haven't mentioned Hadoop. This is because I believe that people are trying to use MapReduce in situations where it causes more problems than it solves. I also don't like Java, and can't wait for an alternative to the HDFS. Travis Oliphant has a great presentation about fighting against the tide of Hadoop in the enterprise , , I almost forgot the MOOCs - Massive Open Online Courses. Coursera.org has offerings for almost every subject you need to learn about. My personal favourite MOOC was Machine Learning with Andrew Ng - go watch it on youtube and do the exercises too?? ,I enjoyed videos on:, , Introduction to Data Science, Scientific Computing, Computational Methods for Data Analysis, Startup Engineering, Statistics: Making Sense of Data, Big Data in Education, Mathematical Biostatistics Boot Camp, Information Theory, Probabilistic Graphical Models, Social and Economic Networks: Models and Analysis, Markets with Frictions, Maps and the Geospatial Revolution, Computational Finance and Financial Econometrics, Principles of Reactive Programming, Analytic Combinatorics I & II,If you're an aspiring Data Scientist, welcome to a career of lifetime learning. Good luck and have fun :),Next post: web stacks, cloud services, converging trends, and why I say "Future Web == Big Data"
Much more devices held, much more messages sent, much more data up in the air.,Upon the arrival of IOT (Internet of Things) era, number of connected devices grows rapidly, the signals of which stack up mines containing valuable, hidden insights.,We used to analyze log files for risk management, spotting the anomalies and exceptions based on outlined records. Via the ever-richer meta data and context, we are entitled to weave more story from the strings now. By union or intersecting user data and log records, we can segment user groups by distinct behaviors and find their particular patterns. By comparing the information of each webpage we can further find the association among each genre of websites.,The idea of log mining is not disruptive; it inherits the long-lasting data mining methodology. Making log files more productive, however, the workload can be quite challenging. Using traditional database technologies, with no doubt, can jeopardize the system stability due to its capacity limit while using distributed approach the performance still cannot improve for the heavy shuffling among data components.,BigObject, designed to compute up to a few billions data records under an extended relational model, works ideally for this use case. We worked with a service provider who offers portal for educational institutes. It tries to identify what types of user role dominate in each campus. There are on average over 66 million records per month, with BigObject the multidimensional calculations can be done in 15 seconds. Simply embedding BigObject with it web-service, each website administrator can better monitor the service utilization now.??,BigObject delivers 2 to 3 orders of magnitude improvement in speed. Only when the analysis can be done quickly enough, the information can be truly actionable. Make your analysis actionable, visit ,.
In my latest ,??(6.25 minutes), I provide an overview of ,, an extremely useful branch of statistics. ??,Topics I cover are:,* What is Survival Analysis,* Objectives of Survival Analysis,*??Models available to analyze the relationship between a set of predictor variables and the survival time,* Examples of where survival analysis can be used.,You can access the video ,. ??If you enjoy the video, please leave a comment on my blog or by following me on Twitter @VRaoRao.,Thank you!
??,Recently Mark Ginsburg, a Director and senior data scientist with our firm, wrote a historical perspective on the importance of simplicity and transparency in conducting data analytics. The push for transparency and free/open standards has long been a key component of modern computing. Equally, within data science we find that clients increasingly demand openness and transparency in watching the transition from data into insights. However, we still see vendors in the marketplace pushing their 'proprietary' and 'black box' approaches to analytics. Mark offers a historical perspective why taking this route is almost always a really bad idea.,Below is an excerpt from Mark's piece:,However this is not to say that the Unix shell, as it evolved in the 1970s and onward, was feeble. ??It was actually a rich environment since developers could combine the terse shell commands with pipes (the vertical ?€?|?€? symbol) which can stack commands and redirections such as ?€?>?€? and ?€?<?€? to, for example, read in files or send output to files.,For example, this command,executes the ?€?ls ?€?al?€? command on the left to list files and directories then pipes in the output of that command (see the left-most ?€?|?€? pipe symbol) as the input to the grep command to the first pipe?€?s right. The grep pattern ?€?^?€? character is part of the regular expression notation meaning the start of the line. Thus, since directories have the letter ?€?d?€? starting the listing, this command picks up directories only. The second pipe sends the output to the ?€?head?€? command that has a ?€?-5?€? parameter. Finally, the top five directories are then redirected, using the ?€?>?€? symbol, to the file named on the right.,If we had to describe the programming and functional environment, we could use words like ?€?minimalistic?€? and ?€?flexible?€? and ?€?functional?€?.,The Unix shell and the protocols such as ?€?FTP?€? (File Transfer Protocol) and ?€?rsh?€? (remote shell) and more recently, ?€?ssh?€? (Secure Shell) and ?€?HTTP?€? (Hypertext Transport Protocol, the famous Web protocol which gained traction in the 1990s) gained in importance as the early Internet added compute nodes and interconnections. ??However, there was friction between the scientific community that used the nodes for research and nascent commercial interests. ??In fact, circa 1981 the National Science Foundation (NSF) enacted an ?€?Acceptable Use Policy?€? (AUP) on its nationwide backbone to ban activities not in support of research or education. ?? So, for a while, (although in 1995 the NSFNET Backbone was defunded) there were relatively pure research and education projects flowing on the various interconnections. ??The scientific community used the flexible and minimalistic shell environment to piece together a wide array of software, ranging from astrophysics to chemistry to biology and all areas in between.,As software and hardware became more sophisticated, research communities and their laboratories could accomplish more and more with computing power. ??However in the laboratory too there was friction between scientific inquiry and commercial motives. ??A very well known example was when a printer vendor no longer supplied the source code to Richard Stallman?€?s MIT lab. ??This meant Stallman and his peers could not modify the printer to do what they needed any longer. ??This motivated Stallman to launch an initiative of worldwide important, the Free Software movement.??,From Stallman?€?s GNU (launched in 1983) operating system project page, we read these important principles about free software.,Notable early GNU General Public License (GPL) successes include Linux (actually only GPL?€?d in 1992) and the Debian Linux distribution (1993), explicitly committed to the Free Software Foundation (FSF) principles and the Apache HTTP server. ?? More recently the MySQL database and PHP Web scripting language have been added to this list.,In any large enterprise, there are also conflicts between scientific inquiry and commercial interests. ??In the Big Data space, commercial interests can introduce to the unwary??
With a wealth of publically available customer feedback?€?such as the CFPB complaint database and social media?€?mortgage lenders can leverage , to identify pain points and opportunities to strengthen customer relationships. , feedback throughout the borrowing process can help lenders solve customer experience issues, and prevent those issues going forward.,Remember that money is personal, and customers expect the personal touch. Going the extra mile?€?responding directly, sending a gift, or reaching out in other ways?€?can ,. By personalizing the customer experience through the lending experience, financial institutions are more likely to earn the crucial trust and confidence that translates into repeat business and referrals.,For more insight into customer experience management in mortgage lending, read our 
n 1917, Darcy W. Thompson,??a pioneering mathematical biologist wrote a beautiful and exceptional demonstration about the relation between morphology and biology. Indeed, Thompson introduces its famous and elegant form of varying the Cartesian space as a tool to show the analogies between species. The central thesis of Thompson was that biologists of his day overemphasized the role of evolution, and under-emphasized the roles of physics and mechanics as determinants of the form and structure of living organisms.,??,?????? In his works, Thompson establishes the seminal "theory of transformations?€? that one species evolves into another not by successive minor changes in individual body parts but by large-scale transformations involving the body as a whole.,??,???? Thompson's work is a great example of scientific data visualization to show correlations between apparently different structural phenomena.,Applications are all around, for instance, in 2005 (,), I applied this idea to modulate the search space of an optimization problem attacked with a Genetic algorithm (GA). The approach was to allow the GA to find the right transformation of the search space in order to improve the search mechanism by incorporating the parameters that transform the search space inside the features to be evolved.,For those who haven't read the book yet, I strongly recommend it for having a good weekend,(Thompson, D W., 
.

??In a recent interview with , magazine, Beyond the Arc CEO, Steven Ramirez shared his insights on the evolution of Big Data analytics in 2013, and what we can expect in 2014.
Machine learning is becoming a buzzword, everybody talks aboit it and few seem to be interested in the math underneath (I find statements like "I wanted to know more but all sources were too statistical/mathematical and I wanted more practical stuff").,Let me tell you something: You can't really use Machine Learning if you don't know the statistical/mathematical basis. I am really upset when I see a Youtube video of some guy in T-Shirt probably working at a large organization ranting about Machine Learning and Data Science, telling programmers that maths is easy to grasp. Everybody knows how to press a button or, if you force me, almost everybody knows how to fix something in their Windows control panel, but that does not mean we can trust them when talking about building a secure payment system, Everybody can use Mahout or the like but that does not mean he knows jack about what he is doing using Naive Bayes to predict the class from thre variables (x, y, z) where z=x^2 and x belongs to the range [-1,1].,Machine Learning is just a fancy word for the statistical/mathematical tools lying underneath, whose objective is to extract something that we may loosely call knowledge (or something that we understand) from data (or something chaotic that we do not understand), so that computers may take action based on the inferred knowledge. An example of this would be a robot arm/humanoid: without programming actions on direction/velocity/acceleration vectors based on an established model, we may put sensonrs on a subject's articulations, and from these datapoints learn a regression model on the manifold of natural movements. Another example is in Business Intelligente: We may learn groups of customers (market segmentation) so that we may engage several groups with specific policies or offers target at them.,Maching Learning is applied Statistics/Mathematics. Is very little and very unpractical without Optimization/Operations Research, from the algorithmic and practical/scalable point of view.,I've come to the conclusion that there exists two large main approaches to ML, disregarding the specific technique we are dealing with and its target (i.e., supervised or unsupervised), plus one in the middleway:,In the functional approach, one uses the theory of Hilbert spaces (and therefore of differential equations and linear algebra). The goal is to find a set of transformations of the data so as to best perform in a score for the task (called functional). These transformations come from a pre-defined set which is not related to the data in any way and are a combination of possibly orthogonal basis of a space of functions (transformation) defined on the domain of the original data. Examples of this are: Linear/Ridge/Sparse Regression (linear or identity transformation for regression), SVM (non-linear using the kernel trick), PCA (SVD/eig computation) and KPCA, Matrix factorizations (for signal separation/clusering...), K-means, Projection pursuit... The basic idea is:,In the probabilistic approach, the prior knowledge is an assupmtion of the prior probability and the likelihood, and works towards obtaining a posterior probability (that the outcome is a given choice given the data just seen). Examples are: Logistic Regression (simple non-linear transformation for classification), Naive Bayes (classification), ICA, SNE, Gaussian Processes (and Probabilistic Graphical Models)... The general idea is,Neural networks and deep learning are a different story. I consider them to be their own field, drawing tools from the above. They are neither functional because they are not dealing directly with functions (transformations) in the functional analytic setting, and they are not probabilistic for obvious reasons, but use any probability or information theoretic tool as needed. The fact that they connect output of transformations to inputs makes this field related to the first approach indeed, seen as a chain of transformations (spaces defined on the image of their predecessors), but the focus here is obviously the algorithms and that changes things.,Reinforment learning is nowadays just a fancy word for techniques widely known, studied and used in Stochastic Processes, HMM (Hidden Markov Models) being the only exception. Sometimes they call it Sequential Learning, but it is not widely considered Machine Learning, neither scholarly nor popularly.,So, as you can see, there is nothing new (not at least in the sense of the discovery of the fundamental theorem of calculus, or of quantum mechanics). One should just know the math to see what we are talking about.,Regarding novelty, I am annoyed each time I read about ML techniques from the big data guys, who are normally programmers starting to do something more that data aggregation and querying. It seems that there is something both new and exotic in what they are saying, and it it mostly well known techniques from statistics, such as this ,, or the beginning of ,.,Now, the practical side of things require that ML scales to Big Data. That limits the applicability of matrices and non-linear transformations to adapt linear methods, so let's see what the next breakthrough is.
Most MDM implementations will be based on a commercial software product.?? By relying on a packaged solution, the architectural considerations have been addressed by the software vendor, allowing the project team to focus on the data management considerations. ??However, there are cases when a custom MDM solution may be needed.?? When proceeding down the path of custom development for MDM, we have some advice for you: avoid these common mistakes.,The most common data domains addressed by MDM solutions include customer, financial, products, partners, employees and locations.?? Most organizations begin with a single data domain, typically focused on customer. Often additional data domains such as product or partners are created as separate MDM instances, managed by separate departments. As the customer data matures, it becomes apparent that an integration or federation between the customer and the product domain is necessary to get an accurate, consistent and complete picture of what each customer has purchased or considered. ??The same relationships may be needed between product and partner.?? The existence of multiple single data domain MDM solutions within an organization creates siloes of data that complicate the governance process and present challenges for data analytics and data quality. ??Many organizations realize they need a multi-data-domain approach to master data management.,For these organizations, there are two options: extend a software product or custom develop a solution. ??Many software vendors offer specialized solutions focused on a single data domain.?? Extending a software product that was designed for a specific single data domain into a multi-data-domain model can be risky, complicated and expensive.?? This is a complex decision and every organization has to decide which option is best for their unique requirements.?? Many organizations build a custom solution to address multi-data-domain master data management requirements.?? At Sullexis, we want to share several common mistakes that you should avoid when building a custom MDM solution.,Separating master data from transactional data should be a simple process.?? But it often isn?€?t.,Our guiding rule is that master data is data that does not change frequently and is at the core of the business. Transactional data is more dynamic, changing frequently in status and content; it is something that happens at a certain point in time.,Is Contract data dynamic? Well, it depends. Massive retail companies have a high volume of contracts that are constantly changing. Because of the number and volatility, it would be incredibly difficult to manage this data within a master data domain.?? We would consider these contracts transactional data, and not recommend inclusion in a master data domain.,Alternatively, for an organization such as a reinsurance company (the insurer of insurance companies), the contract (or policy) is a core component of the business and is fairly static, at least during the term of the contract. For reinsurers, these contracts are a good candidate for being treated as master data.,Purchase Orders are transactional in nature and not usually considered master data. However, in some organizations, particularly in the software industry, it is common to issue blanket POs to major vendors.?? These POs can span a long time period, be static in nature, be central to the operation of the business and represent significant financial commitments.?? In fact, they are similar to a contract.?? These organizations may need to consider these POs as master data.,Cities, States and Countries information almost never changes, is generally important for a business and should be centrally maintained. Should it be master data? In our experience, this data is most often treated as reference data rather than master data.?? Reference data is static data that is infrequently modified and is governed by standards bodies. For example, the list of countries is an ISO standard (3166-1).?? Data that is managed by external standards bodies typically does not need to go through the governance process of other master data.,However, Cities, States and Countries??,if the core information is enriched with dynamic data created within the organization such as alternate names, synonyms or custom classifications.?? Alternatively, if the organization operates in zones of the world with more dynamic changes due to geo-political conflicts, this data may need to be treated as master data.,Including transactional data in the MDM realm can cause many problems.,While we have shared guidelines based on our experience and best practices, it is necessary for you to work within your organization to decide how best to classify your data. ??Avoid making assumptions based on what you have read or your experiences on prior projects. Discuss every single data element with the data stewards and business owners within your organization. Challenge each other?€?s point of view until everyone agrees which data is deemed worthy to be classified as master data.,, Director, Sullexis
While Banks are getting more and more pressure from customer?€?s increasing demand, highly competitive market and strict regulations ?€? in the current environment, understanding customer behavior, attitudes and requirements is more vital than ever for banks?€? strategic thinking, operational planning and day-to-day customer treatment, according to Ernst & Young.,??,??,??,??,At one West Coast bank, customers can spread yoga mat and strike a tree pose or grab a free beer during an Oktoberfest-style celebration. Another national bank offers macchiato at its coffee bar and couches designed for lounging. Locally, banks are adopting Apple Store-like designs, adding concierges, and setting aside space for community groups to meet. Bank of America, for example, recently launched what it calls a flagship office in Boston?€?s Back Bay, housing in one place the variety of financial services offered by the banking giant. Teller services are squeezed behind a column, while private offices where more lucrative business is conducted dominate the floor space. Capital One Financial Corp., well known for its credit cards, is breaking into the Boston banking market with six new marketing offices called ?€?cafes.?€? Scheduled to open over the next several months, they will resemble coffee shops more than banks. Employees will serve up java and sandwiches, along with advice on how to set up online accounts and access other services. Modern and innovatively designed branches will drive customer engagement and is the key to branch long-term success.,Rather than decrying customer?€?s increasing demand and competitive market, if bankers can embrace the new reality ?€? and focus on innovation, disruptive technology and automation--- the profit will follow!!!!


In my latest video blog, I provide an overview of a simple framework that I developed to discuss data. ??You can view it here:
To be short, in-memory computing takes advantage of physical memory, which is expected to process data much faster than disk. In-place, on the other hand, fully utilizes the address space of 64bit architecture. Both are gifts from the modern computer science; both are essences of the BigObject.??,In-place computing only becomes possible upon the introduction of 64bit architecture, whose address space is big enough to hold the entire data set for most of cases we are dealing with today. In that case, we are able to trade space for time and thus make real-time big data analysis possible. As data is preloaded to the memory space under in-memory approach, it still hits the limit when data size is larger than the swap space - and the performance drops drastically. This fortunately doesn't happen to in-place computing.??,The mission of BigObject is to offer affordable computing power for everyone can fulfill big data applications. For the record, we used??a laptop with 8G memory to compute 100 millions of data, and it took only 5 seconds. As a result, the tremendous investment in hardware equipment shouldn't hold you back from diving in to big data analysis anymore. As long as you possess data, you can build model, then you are entitled to reveal the insight.??,??,More information and free trial please visit the??
As said by great man Zig, "If you can dream it, you can achieve it. You will get all you want in life if you help enough other people get what they want." -Zig Ziglar ...,The following is an example, how I think, behave and perform, I hope this information helps:,A Director position holder must be:,~~~ A strategic thinker who provides thought leadership to projects and products,--- I always believed in "Strategic Thinking", it is a powerful and invaluable skill, one that leads to greater chances of success in projects one is involved in. It involves:,?????????? ** Setting goals, ,?????????? ** Developing long-range plans, ,?????????? ** Anticipating the unexpected, ,?????????? ** Continuous analyzing of one's environment, and ,?????????? ** Cooperating with One's competitors to have an added advantage.,~~~ Data driven planning, specifically in the world of Big Data and decision making,--- It is a known fact, that the decisions to any new product desgin, develop, test and launch are purely based on Data, I always have presented my research with data to my management and my recommendations to the entire teams I worked with for all os us come to a consencous and make informed decisions.,~~~ Strong motivator and team builder,--- I like not to take any thing personally, I like to work with anyone at any time basis, I am a student of life and share my positive experiences with life to motivate others, how to handle conflicting situations and more.,~~~ High degree of customer orientation,--- We are consumers first, if we think like this and walk in the shoes of consumers a Mile before making any choice, we are at added advantage, this is what I always do.,~~~ Must be an advocate of lean processes,--- I strongly beleive in the best use of available resources, as it is the key to success. It is vital to identify the value of the project/product, stream line all values, create a work flow, by establishing a pull to complete project/product, try to get as much perfection as possible, and restart this process...This was majorly my process impromenet cycles. ,~~~ Be committed to organizational development and operational excellence,--- There is a big saying, I absolutly beleive in "The food container you eat from, do not poke holes in it, rather take care of it".,~~~ Hands on experience in building and delivering enterprise class software products,--- Absolutely, this is what I do. I worked for Mitsubishi Motors Supply Chain Management, Texas Oil's Oil Products Manufacturing Management are few of to mention.,If this article have intrigued you, you are most welcome to contact me via my linkedin profile at:
We are witnessing a paradigm shift in Data Environment. In recent years, Big Data has risen on the technology horizons and is under the aspect of efficient and cost effective management and analysis of vast amounts of data for both public and private organizations. There are several organizations, which are trying to harness this continuing data stream, and in 2014, several of these organizations will go about making this data available in real time .,Any organization, that want to take advantage, no matter they are automotive, financial or healthcare concerns, the latest developments in the field of in-memory data management are blurring between, computing algorithms over operational data and analytical data between datacenters and data warehouses, there will be any of the medium term otherwise will more likely be eliminated entirely .,Famous Extract, Transform and Load to or from some data warehouse ?€? usually we otherwise call it ETL , Batch or any designed aggregation mechanisms, using Hadoop/MapReduce or YARN are now found in resulting time delay and need to be replaced in 2014 by realtime dashboards. Consider we are driving a car, our physical dashboard with odo meter, gas gauge, heat indicator, cd/aux/radio/FM player are with us no matter, how advancement we have seen in last several decades in our automotive industry. This physical dashboard has a significance, as long as we are driving humanly, we will need it, and you will agree here that we are processing data as our data environment (physical location of vehicle) is changing every moment, as long it is in motion, and our decisions are accordingly to stop at red light and be aware of a deer or a moose or any other animal/human crossing the road not hit it.,For more, kindly click at:
 , , , , , , , , 
As an individual user, we are no longer living the world of one computer, rather we are living with living and distributed smart devices. Let us first explore a Single-User/Single-Server Architecture that we used in those times, which we survived, without of much hassle either, we still use this architecture most of the time.,A Single user still use to approach a network via a web application with a local front cache, which then allows user to access requested material as per the business logic (security/privacy is checked before data release) using local business hash-maps and locks (which data needs to be unlocked), next comes Data Tier with some framework having 2nd local level cache, which leads the request of the user to a single source of the data, it might be a database or a Hadoop Cluster to serve the user?€?s needs. In case user?€?s need expand and more data requests start coming, this means one server is not enough.,At the time, when local server is not enough to provide complete data set that is requested by the user(s), we need to scale this requesting application to multiple servers, in other words horizontally, using AWS, Rackspace or inter-organization, local area networks. This raises an interesting question, how do we do horizontal scaling of an egress request From:,User ------> Cloud -----> Application,To:,User ------> Cloud ------> Cluster [ Server 1 + Server 2 + Server 3 + Server 4],This can be a local area network (LAN), if it is within an organization. Here we have seen a paradigm shift from a local architecture of egress request of data into a Clustered Architecture. This extension of cluster data access by a single application is beyond local JVM (Java Virtual Machine). We have to take care of several aspects, when we enhance the application to several servers horizontally. When we add more servers to a cluster to have more load, it is called horizontal scalability. We have to make sure that we look into having the following attributes achieved in some way:,??? Reliability,??? Horizontal Scalability,??? Consistency,??? Load Balancing,??? State Sharing,??? Risk and Failure Tolerance,??? Now in the world of Big Data ?€? Sharding,More to come later...


During winter, most of the utilities, energy marketers , generators and commerical enterprises ??want to maximize their supply for heating and other applications.??,so which is most cost effective supply of natural gas . natural gas is available in plenty but always comes at a cost.,check out our post on ; figuring out the most cost effective & optimized supply of natural gas to the city of Boston
??on??making interactive Plotly graphs with multiple y axis scales, with examples in R.
The term "Data Science" has been evolving not only as a niche skill but as a niche process as well. It is interesting to study "how" the Big data analytics/Data Science/Analytics can be efficiently implemented into the enterprise. So, along with my typical study of analytics viz. Big data analytics I have been also exploring the methodologies to bring the term "Data Science" into mainstream of existing enterprise data analysis, which we conventionally know as "Datawarehouse & BI". This excerpt is just a study of Data Science workflow with respect to enterprise and??,??for discussion on ??Operational Data Science" (I am just tossing this term "Operational Data Science", it can be named better!). Meanwhile, I must mention the articles those I have followed during my whole course of learning on the operational side of Data Science??. Both the articles mentioned below are super write ups written by Data scientists during their research work and they can prove to be a valuable gift for enterprises.,While article #1 ??gives fair idea of complete data science workflow, it can be very well understood by article #2??with nice explanation and challenges mentioned. Idea is to understand them together.,Operationally, these 5 areas can be efficiently covered if data scientist can rightly collaborate with ??Data Engineers, Datawarehouse Architects & Data Analyst. It is the??responsibility??of a Data Scientist to run the show from data discovery to communicating predictions to the business. I certainly don't intend to define the role of a data scientist here (In fact i am not even??eligible for this). My aim is to sum up the skill sets and identify operational aspect of it. One of the imprtant point to be discussed here is 'Diversity of skills'.,It needs diversity of the skill i.e. "Business understanding ??on data (data discovery)" to "Programming hand on the scripting or SQL" to "communicate effectively through right visualization tool". It is difficult for one person to diversify in all these areas and same time specialize in one. In such complex environment ??we should look at the opportunity to bring Datawarehouse + Unstructured Data analysis + Predictive Analytics together.??This opportunity is well detailed in the article#2.,Most of the organisations work in silos on their data and in??absence??of ??effective communication channel between Datawarehouse and Analytics team the whole effort of effective analysis goes??awry. 80% organisations divide the effort of Datawarehouse, Advance analytics & ??Statistical analysis into different teams and these teams not only address the different business problems but they also aligned with different architects. In my opinion this could be the main reason that kills the flavor of Data Science. Interestingly during one of my assignments in the field of retail data analysis, I observed that they had developed their datawarehouse team only at the??maturity??level of summarization and aggregation. I realized that this datawarehouse or Data store world would end after delivering bunch of reports and some dashboards. Eventually, they would be more interested in archiving the data thereafter. That's the Bottleneck !,To overcome this bottleneck we need to bring analytics either into mainstream of data processing layer or we should develop parallel workflow for this, and article #1 articulates the same and proposes the flow mentioned below. If you believe this figure (from article #1) ??is a data science workflow then you need to have diverse skilled engineers working on common goal to deliver this workflow unlike conventional data analysis. Observe it closely and figure out the business oriented engineering team.,Team with diversity can work together for an enterprise to deliver this workflow.,Interestingly, there is no right and specific order of delivery from these people. Having said that the person who has strong business background can work at both the ends of a shore i.e. in data discovery as well as in communicating the final result (either in terms of prediction or pattern or summarization). However, programmers can pretty much independently work in all areas of data preparation, ??data analysis and scripting to build datasets for modeling (In fact this is hardest area read the article #2). In a same way, statistician can very much communicate the business result and reflection. Now after all these efforts what left is just a game of effective collaboration. That is easily visible in the figure mentioned below.,Moreover, along with the right collaboration channel there should be a Data scientist(s) who can watch over and architect the whole work flow and should always be ready to design+code+test the prototype of the end product. So, this whole Operation Data Science need a collaborative team and an architect(s) with diverse skills who should be ready to phrase the below statement.,Original post:??

By: Nicholas Hartman, Director at CKM Advisors,Whenever I introduce the data analytics we do here at CKM, an ever increasing percentage of people will respond along the lines of ?€?So like Hadoop / NoSQL / [insert generic ?€?big data?€? term]??€? Many are surprised when I respond saying not always.,The hype around so called ?€?big data?€? seems to have convinced many that unless the data and analytics are ?€?big?€? it won?€?t have a big impact. In reality, for many organizations there?€?s still tons of value to be generated from smarter use of ?€?small?€? and ?€?medium?€? data. The missing gap is often data science skill, not big data technologies.??,For example, a process may have multiple enterprise systems that store important transactional data in separate silos. A tremendous amount of value can be derived from having a data science team integrate this process data across silos and identify problems/root causes that span these silos. The quantity of data in these cases may be less than 1 TB, sometimes much less. However, a good data science team could still use that information to completely transform a company?€?s operations. In many cases that team may need nothing more than a simple server and a few open source tools.??????,At the end of the day, we?€?re less concerned if the data is small / medium / big or ginormous than we are with the problem we?€?re trying to solve. With that foundation, we?€?ll then tap into any of our suite of available tools to best implement the algorithms developed by our data science teams.,Hadoop, NoSQL and other technologies are fantastic, we just don?€?t need them to solve every data analytics challenge we face. In some cases, these technologies would actually make it more difficult to solve the problem. If we have a 500 GB dataset of relational datasets then a well tuned MySQL / MS-SQL server coupled with a single linux box for running analytics code may be all we need. If we want to conduct lanaguge analysis on 500 GB of free-text then yes we might farm that out to a Hadoop cluster.??,Next time I'll discuss our response to another broad question we get all the time: ?€?Should we invest in big data??€???,??
??, ,??
By: Nicholas Hartman, Director at CKM Advisors,Today we'd like to share with you some fun charts that have come out of our internal linguistics research efforts. Specifically, studying weather events by analyzing social media traffic from Twitter.??,We do not specialize in social media and most of our data analytics work focuses on the internal operations of leading organizations. Why then would we bother playing around with Twitter data? In short, because it's good practice. Twitter data mimics a lot of the challenges we face when analyzing the free text streams generated by complex processes. Specifically:,In this exercise, tweets from Twitter's streaming API JSON stream were scanned in near real-time for their ability to 1) be pinpointed to a specific location and 2) provide potential details on local weather conditions. The vast majority of tweets passing through our code failed to meet both of these conditions. The tweets that remained were analyzed to determine the type of precipitation being discussed.

The recent studies done by the U.S Bureau of Labor Statistics shows that people learn only 70 percent of what they know about their jobs informally. When talking about the IT health trainers then what possibly could be the stats? Are they efficient enough in delivering their best? What things they need to keep in mind in order to move with these changing learning trends?,Well, here are discussed those 3 tips, which if followed seriously, can bring a massive improvement in their way of training. Take a glance:,So, every??, must keep these above discussed tips in mind and enhance his way of training in a better way.
Disruptive technologies may be seen as an opportunity or a threat, depending on one?€?s role within an enterprise; fortunately, the positivity of perspective easily becomes a choice when CIOs and CMOs are able to step back to take a look at the bigger picture and determine to collaborate in the interest of the organization as a whole.??,The term ?€?disruptive?€? is rather loaded, however, and may be counterintuitive to those who do not understand its full implications. By definition, a ?€?disruptive?€? trend or technology is a product, service or technology that becomes the preferred or optimal option for customers because of its modified and improved value offering; in actuality, it may be beneficial to think of the term ?€?disruptive technology?€? as an ?€?emerging opportunity?€? instead. ??Regardless of the term used, it is clear that disruptive trends such as cloud computing, big data, and mobile technology can be leveraged for the benefit of any organization or enterprise.??,What is critical is that both IT and business professionals understand the full implications of these trends, their adoption stages and their benefits, and leverage them appropriately.,When the C-Suite is able to comprehend the emerging, differentiating and business value stages of each disruptive trend or technology, they can use it to its fullest advantage for business model transformation, competitive advantage and optimal scale.??,?€?Nexus of Forces?€?, a term coined by the ,, describes the junction and corroboration of social media, mobile technology, cloud computing and information that motivates new business developments. Gartner posits that despite the fact that these separate entities are innovative and disruptive alone, they are, in conjunction, revolutionizing business and society. As a result, antiquated business models are on their way out and new leaders are emerging. In fact, IDC (International Data Corporation) has predicted that these entities combined will be the driving force behind IT market growth over the next six years and they have termed this phenomenon the ?€?third platform?€?.,As these emerging disruptive technologies make way and change the IT infrastructure and its relationship to the architecture of the enterprise, the relationship between the business, operations, marketing and IT departments is dramatically impacted.,As a result, professionals across board must learn to shape and combine those emerging opportunities to meet the objectives of the organization. IT professionals must adapt to support those users who are taking advantage of these new technologies, and business and marketing professionals must be mindful of the value of IT when it comes to innovation, integration and security, which is at increasing risk given the more valuable and vulnerable nature of big data.,Perhaps the biggest learning curve in this situation lies in unification and cooperation across the C-Suite, thus allowing the benefits of disruptive technologies (opportunities) to maximize throughout their lifecycle as they grow and change over time, but equally as important is the organizational wide collaboration of various departments in order to position IT as an enabler and a booster of business performance.,Many have suggested that success in the future of the enterprise lies in cross training and hiring; we will begin to see more IT professionals who have more ,??about marketing, and more marketing pros who are tech savvy. Some think that by 2020 the two roles will have merged, or at least become less distinct, and that we will see an emergence of new roles such as Chief Experience Officer, as well as directors of content and social media. Nonetheless, IT concerns such as security and compliance will continue to be vital to every organization, and that is not necessarily something the CMO can handle.
Since the inception of human being information in any shape is utilized by us.For quite a while organizations have been using the data to make informed decisions. In a sense Analytics is just a buzz word that is taken as the application to this decision making process. let's suppose you're in the professional education profession, you might be an instructor with a Big Data solution provider, internally, or you might be an instructor to teach client organizations staff. You might be a Big Data or Cloud administrator. So what are some objectives you have? Well one objective is to make the staff you are training, more ready for their jobs. This means you will be using data of these training participant's assigned work's results to improve their performance. You might work for a not for profit company that's goal is to raise funding for helping some causes, which means that you probably would like to help your organization in these ventures. So how can we use data to help our organization to raise a higher funding? ,It is a fact that when we even speak with each other, whether we notice or not there is the talk that we are doing?? and we are always getting noise, that might someone speaking near use over the phone or cars driving by, what we have to worry about there is, is there noise in the data. As this noise is the distraction that we can hear with our ears, how computer or an algorithm will know, what is noise, what is data to process with. We use several algorithms for extracting meaningful data for us from the noise. How we decompose the signal to noise is really an area for Data Analysts. Our computer program should understand what was signal and what was noise. ,In the contemporary world today we have much more data available, which we also know as Big Data. Every product that is available anywhere, electronically or in a physical store, the composition of the product, the price of the product, the benefits and more factors are available to use, who are consumers to make informed decisions. The major factor that is now emphasizing on Big Data is the importance of analytics is, that smart devices and computers are getting faster, and faster. These are the areas, where Big Data Analytics are playing and will play significant paradigm shift for all of us in 2014 and beyond.
 




This article outlines the differences between Agile and Traditional Project Management. APM vs TPM, where APM stands for Agile Project Management and TPM stands for Traditional Project Management. APM focuses on customer interaction and satisfaction, where as TPM focuses on artifacts and Plans and adherence to the Plan. APM responds to and change by using adaptive actions, where as TPM adheres to Change Management and tight control by using corrective actions.??,APM deals with evolution in Requirements on everyday basis, where as TPM deals with all requirements in the initialization of the project. The teams of an APM process are committed to a feature completion using Feature Breakdown Structure,??and are empowered to make decisions, where as TPM uses WBS or Work Breakdown Structure to convert into the plan of action to complete the project, and the teams are controlled using Top-Down command structure.,Agile processed are generative according to requirement change adaptation to make feature better for the end-product, where as in TPM the processes are prescriptive and are detailed. APM focuses on the customer value or is value driven, and TPM deals with non-value adding controls on team to produce end-product.,??
 






Hi all Greetings from Prajan,,??I am Prajan(Pandiyarajan), Working As a Perl Developer in Sciera Solution.I have 2+ year in perl and Big Data .,??I got a difficult??task from my manager ,that was i need to Crawl 200 million URL in same Domain with in 20-1 month time ,i have tried in Perl i got Maximum 250 hits only in 60 seconds.I heard will make this using Hadoop But i don't know hadoop ,Any one can You give a solution for my task with detail then how can i reach This Task.
In the era of Big Data, it is getting obvious for the healthcare market to expand its horizons to absorb technologies to achieve greater efficiencies by leveraging the Service Oriented Cloud tools and technologies, which will lower costs for hospitals and deliver higher quality patient care on simultaneous basis, There is a huge influx impulse of healthcare market drivers is generating Big Data every minute.,??,Once a patient is in some healthcare facility in any condition, we are to use latest technologies by leveraging the devices available to use by utilizing bedside vitals by gathering data using intelligent pumps, wired ventilators, this data stream is instant. As per the aspect of patient?€?s information security this data can be masked and provided to any practicing medicine expert in terms of discrete clinical metrics.?? Whether it is a patient or a nurse or a physician or an accountant working with the domain of Healthcare market, even an informed healthcare consumer is able to generate and request to see the results of new data elements through increased cellular data communication instantaneous message passing communication.??,??,As per CMS.GOV ?€?On October 1, 2014, the ICD-9 code sets used to report medical diagnoses and inpatient procedures will be replaced by ICD-10 code sets.?€? This switchover to ICD-10 will be playing a game changer part in the generation of Big Data.?? It is no hallucination that any organization dealing with healthcare in any aspect should be capturing all data associated to healthcare market need to make sure that the data is collected on accurate basis, while making sure that Data Governance is properly taken care of, and this ?€?Big Data?€? is protected with some security measures at the same it the confidentiality is persevered as well.,For more details, kindly click on the given link:
Let us start with some jargons and distill these into some meaningful things to be understood by us the only intelligent autonomous system in other words humans.,??,SaaS ?€? Applications focused on end-users, used internet as a medium, e-mail, salesforce etc.,PaaS ?€? Set of tools focused on developers, such as Ruby on Rails, Python, Eclipse, REST, SOAP, Oracle.,IaaS ?€? Complete software and hardware solutions, VMware, Amazon EC2, Rackspace Cloud, Google Compute Engine,??,All of these deals with ?€?Data?€? focused on end-users, stored using IaaS, and extracted using PaaS, processed using SaaS. It does not matter, whether you are on-Premises, or have to run an Infrastructure, or you are to use some designated or recommended platform, you have to deal with ?€?Data?€?, and this data is now actually ?€?Big Data?€?.,??,It does not matter, if you are using a Cloud, as public, private or community Cloud and last not least the Hybrid Cloud. The security can be the main area of interest for us all, to safeguard our interests. In case, if we are using a private cloud, we might be in extreme tight secured gateways, however once the data that we are processing or generating hits the cloud waves, we have no idea, what and who is accessing it, when we say what, it can be web crawler or some software spiders from some organization are gathering information, we cannot predict these external stakes.,??,There are several Cloud Computing platforms working towards standards, however there are two major working groups in Cloud Computing worth looking into are:,??,-?????????????????? IEEE P2301 Working Group (Cloud Profiles),-?????????????????? IEEE P2302 Working Group (Inter Cloud),??,The utilization of Cloud is to have reduced IT Costs, it provides reliable access to institutional information, with reliable backup services alongside disaster recovery and surge capacity and can still be used to produce some money by selling server space or SaaS.,For more kindly click at the link given below:
Hello, friends!,In the following entry of my own blog I describe how to take advantage of one of the features of QlikView, a business intelligence tool, to perform quick searches across a data set.,I hope it to be interesting and practical.,QlikView is an option to take into account by a data scientist, given the fact that the desktop application is free for individual use and some of its features can help in different scenarios.
You could take a look at some traditional stat anslyses, like Cluster Analysis, and for a visual representation, ??try Multidimensional Scaling. These would save you a lot of time. There is no reason to reinvent the wheel, but Big Data seems intent upon ignoring the vast historical knowkedge regarding statistics, ??psychometrics, and Measurement Theory. Believe me- if you have a question on how to proceed with an analysis, multivariate statistics is likely to have several accaeptable answers. When I see people "discovering" a "new" analysis that was thoroughly described a half a century ago, it makes me moan in despair. Look at Factor Analysis Structural Equation Modeling as well.
With immense growth in technology, financial institutes, skyscrapers and malls, one can find that the number of??consumers have increased exponentially in the Middle East over the past few years. Reports suggests that 91% of the population comprises of expats in UAE enjoying high salaries and tax free benefits.??,Moreover, the options to spend your money have gotten bigger and better??with UAE working towards the initiative to build Smart Cities setting strong infrastructure fundamentals in future. The trend to market anything and everything to consumers has gradually shifted to attract customers with personalized services. In order to achieve that, businesses need to collate, maintain and manage the??,??volumes of??,??to provide efficient and effective service and engage them further to boost up revenues.,With each company wanting to cater real time promotions, it?€?s challenging to get such large volume of data in a structured way , able to read and analyze it??and finally use it to implement new marketing strategies, product development or for better customer service.,So, the next question that arises is how do these companies always come up with exactly what you want?,The answer being??,. Big Data comes from your mobile phones, Facebook likes, You Tube sharing, emails, pdf files, white paper downloads, sensor chips embedded in your car and much more. The insights gleaned by big data can create BIG opportunities deepening customer engagement, optimize operations, mitigate risks, and capitalize new sources of revenues. Big Data is slowly gathering its hold in Middle East with IT companies providing solutions in retail, banking, hospitality and oil and gas industry improving business performance, underpinning new opportunities for growth. Penetration reached through analyzing transaction data, social sites, foot traffic and in-store checkout wait times have led to shifts in decision making and in-store tactics.,Consumers will be using 15 times more data by 2017 than they do today. There will be 3 billion??smartphone??subscriptions, 5 billion mobile broadband subscriptions and 9 billion mobile subscriptions. By 2020 there will be 50 billion connected devices worldwide according to one research done by Ericsson earlier this year.,??,Big players like IBM, EMC etc have already recognized UAE as a potential market for Big Data and are trying to penetrate further with better technology and services. Research done by EMC stated that companies in UAE are increasingly seeing the transformative benefits that are achievable with Big Data analytics technology.,Focusing on such knowledge sharing and networking, Dubai will be hosting??,on 19,??and 20,??May, 2014. Connecting senior executives and decision-makers from medium to large organizations across the Middle East region, Smart Data Summit are for those dedicated to making the most out of their valuable data assets leading to better management decisions.??
A prominent discrimination case in Canada involves a firefighter named Tawney , Meiorin had successfully performed her duties as a firefighter for many years.?? She lost her job after the introduction of mandatory testing to determine her fitness for the position.?? The testing measured aerobic capacity, and it was developed in a manner that many would regard as ,; that is to say, it used a highly quantitative and analytic approach.?? However, the Supreme Court of Canada used a critical lens to interpret the data.?? It found that the tests results did not seem related to Meiorin's actual ability to perform her job.?? Moreover, it seemed that most women were unable to pass the test.?? What I describe here as , arises from the actions of institutions - done to a large number of people by a large number of people - using ,.?? I would argue that in the Meiorin case, a key normative is that only healthy young men should be firefighters.?? The imposition of scientific testing and dismissal of those that failed reflect the use of institutional data; the facts and figures existed to achieve specific collective objectives.?? Data in this context can regarded as an instrument of control.?? In this blog post, I discuss the dangers of making use of data without considering its institutional intent and collective consequences.,Normally after that type of introduction, I would probably be expected to delve deeper into the human rights component of the Meiorin case.?? Actually I want to underline the adverse business impacts of using data to usher in strategies that are disassociated from the surrounding realities of the market.?? We have a scenario where there was a great deal of data both during the development and application of testing.?? However, the data served a purely instrumental purpose.?? It defined rather than explained reality.?? In past blogs, I have used the example of sales figures.?? Taken simplistically, a measurement of sales provides almost no explanation about the underlying contributing factors.?? So if sales start to decline - as they surely must at some point in a competitive environment - an organization finds itself with little guidance.?? We have to recognize the institutional nature of sales:?? it can provide a basis of comparison between the performance of different companies; it might help an organization determine its rate of growth; and it could offer some indication of future inventory requirements.?? It might also be completely wrong.?? This is not to say that the quantification is necessarily faulty, but the organizational inferences might be poor or inaccurate due to the deeper nature of the data.?? A measurement of sales is likely relevant only in specific contexts to particular parts of an organization.?? Consumers could care less about the numbers; nor is the data all that connected to the underlying needs, desires, hopes, and aspirations of consumers.,A large US retailer recently started operations in nearby Canada.?? Canadian sales have declined , in a single year.?? Of course during early planning, it seems unlikely that any analyst would factor in a drop of almost half of revenues after the first year of operations.?? The institutional data likely provided a , growth profile thereby legitimizing entry into the Canadian market.?? I was listening to the CBC on my way home, and I was left with the impression that the retailer expects Canadians to adapt to its business model.?? So organizational decision-making was quite insulated from the consequences of its actions.?? Organizational pathology has tended to indicate to me the presence of disassociation between perception and phenomena in the data itself.?? The context is sometimes imposed by an institution over its data such that there is reinforcement of illusion.?? A powerful force can emerge to maintain behaviours that are possibly irrelevant or counter to the underlying organizational needs.,I have used special terms in the past to describe the underlying themes in this blog, and for the sake of consistency I will mention some of them here.?? Data can be used to serve the , objectives of an organization.?? For instance, data can be collected simply to confirm the extent to which particular processes have occurred.?? (Whether those processes lead to the intended outcomes might fall outside the metrics.)?? The , support a regime of compliance through the use ,.?? This is data that informs us only of a specific contextual reality.?? The measurement of sales is reductive as it tells us nothing about why sales increase or decline.?? We simply gain an indicator of impacts to revenue.?? Whether there is a small amount of data or an abundance, the existence of that data is within the context of control dynamics that define the meaning of the data.?? Organizational perceptions can become separated from the underlying phenomena of the data including outcomes and consequences.?? Many philosophies and behaviours can emerge during good times that some might claim to be related to sales; but when sales decline during bad times, the irrelevance of the reductive data becomes more apparent along with its disassociation to the surrounding realities of the market.,When an organization collects data about customers and future demand for its products and services, there is always a danger of the data concealing rather than describing reality.?? So it is important to be critical - to question the underlying meaning of the data.?? An organization can pathologically insulate itself from reality - and later express great surprise about its unexpected journey.?? It is possible to have a great deal of data that, rather than causing an organization to engage reality, leads to its disengagement, detachment, and alienation.?? When I use , in an organizational sense, it is usually self-imposed.?? If we manage what we can measure, then the construction of data can effectively disable managerial efforts.?? Not only this, but the institutionalization of data fosters a decision-making environment in which only certain managerial efforts take pace.?? I find that organizations actually share many parallels with humans.?? Drinking and driving come to mind - the so-called , related to the chemically induced separation of reason from reality.?? As we come to terms with our ability to collect and make use of large amounts of data, I think it is important to sit back and question the , of the data.?? The institutional response to a situation is not about the search for meaning but rather its imposition.?? When we are unable to understand the underlying phenomenon, applying an institutional response does not help us deal with the problem, but rather it insulates our perceptions and the actions we take.



New! Plotly lets you style interactive graphs in IPython. Then, you can share your Notebook or your Plotly graph. It's like having the NYTimes graphics department inside your IPython.,You can also get these Notebooks on the??,.??Visit Plot.ly to see more??,.??,Here's a preview of how it looks to have your code, data, and graph all interactively available. See the ,.

??,Belgacom Group is the world 8th Telecom operator, it is 1st in Belgium for Mobile phone and landline services (e.g. 4,6 million mobile phone customers). The group employs 17 000 people, is owned (53, 5%) by the Belgium state and develop landline services under the brand names of Belgacom, Telindus, Skynet. Belgacom also develop a very specific approach for its strategic positioning by balancing the product leadership, prices competitiveness and the knowledge of its clients.,??,The Belgacom decision processes lean on several Teradata solutions that represent about 90To and have seen four major evolutions since 1997:,??,Simultaneously of the decision system development, the user/IT relationships have also seen an evolution with highs and lows in collaboration. During the implementation of the first data warehouse, the dedicated team obtained excellent results concerning the users and the implementation of new applications (like in 1998 with the implementation of billing, database and SAS reporting). The users found the help they looked for to satisfy their needs at that time (highly competitive national and international market as well as the emergence of internet).,??,With the years, the IT department got overwhelmed by the daily decision system management already set up and by the lack of resources when it had to answer to business user requests (e.g. complex queries development). In this context, users and particularly those coming from the marketing and sales departments, have tried to find more independence and they created teams of ghost developers in their own departments to help them realizing an always bigger number of analysis, reports and requests. This approach has been very creative and allowed them to cover short terms needs but it came with a high increase of the costs and an alteration of the products quality. Actually, at that time they could have had many different answers to a single request i.e. there was no longer any ?€?company data integrity?€?.,??,In a third time, the IT department manages to dedicate more resources to the development of application for other departments. The IT particularly used Business Object to develop parallel universes and new dashboards. However, users kept their independence from the data warehouse. They even kept on uploading their data on their own to develop data mining activities, modelling and they also start to set up an operational decision support. The number data quality issues has kept on increasing, many contradictory reports and departments conflicts appeared to finally thwart many departments that were not able to develop what they needed but of the lack of resources.,??,Recently a new approach has been proposed by the IT department. It includes the creation of a Business Intelligence Competency Centre (BICC) and investments on new technologies (Ab Initio, Informatica, Microstrategy, the Teradata Logical Data Model (LDM)) The BICC is composed of 3 Centres of Expertise (CoE).,??,The BICC therefore gathers Business, IT and analysis expertise that necessary to build and use an active data warehouse. It allowed dissolving and integrating the ghost developer groups, to increase the level of expertise of all participants and to improve the business knowledge of engineers and architects. At last, even though the BICC implementation was globally a success, some resistant extremists had to be managed with more or less diplomacy.,Finally:,For further information, please visit:,??
In the late 1980s and early 1990s, the call center market was booming and it was truly the only way to give consumers access to service at their convenience while providing a vehicle for companies to up-sell and cross-sell products.?? Before the Internet of things that was a very effective vehicle and to some degree, it still is.,Companies who were considered leaders in customer care were well known for setting the bar for customer experience pretty high.?? I was in the telecom business back then and distinctly remember sitting in meetings discussing how to improve the customer experience.??,At the time MCI (now part of Verizon) did not compare itself to its rivals in terms of measuring customer satisfaction, expectations, and service experience. MCI compared itself to American Express who was the leader in customer service experience at the call center level.??,The focus was not so much on how can we provide a better service than AT&T, Sprint, or Verizon; rather the focus was on how do we extend the same experience customers are expecting because the bar has been set high by someone in another industry, and how do we set the bar higher?,The Internet of things introduced a major disruption to customer-care by effectively turning every customer into an employee.?? The service is always good, because I, as the employee and customer am the one who sets the bar. However, there are some clear customer experience leaders in the market.,Amazon.com is able to match product with interests better than most companies. When a consumer is shopping on the site, the expectation on the experience that consumer has with any other company is set rather high.,When I shop on Amazon.com my experience as a consumer is rather exceptional, they align what I need predictively and they get better and better at it each time. They follow through with fast shipping, easy returns, and give me access to customer reviews so I can make an informed decision.,??,Based, on my interactions with Amazon, The customer experience bar is set at a high standard for me. When I go into my bank my experience is totally different, they don?€?t know me well at all.,Strangely enough, they know more about me than anyone in the world.?? They know who I do business with, they know where I travel, what I buy, what I earn, what I save, they know so much it?€?s scary ?€? yet my experience at the touch point bluntly put?€? sucks ass!,The branch representative presents me products I don't care about, the ATM machine shows me videos I don?€?t care about, and when I go online it feels rather cold and very impersonal. The crazy part is that banks are spending billions of dollars per year wasting my time with these needless products.,When I want to expand my relationship with a bank the experience isn?€?t one that is a bolt on, rather I start all over again with little continuity from one product to the next. And worse, no one has a clear view of the entire scope of my relationship with the bank.,The bank?€?s products may share the same name, but I am doing business with 4-6 different companies in reality and they do a great job in making sure I experience that lack of connectivity with my experience.????,I go back on Amazon.com, and feel refreshed.?? Why can?€?t I feel refreshed when I go into my bank??? Hello??? I need to buy stuff, you are my commerce partner, I pay you to help me do commerce, can you start treating me like a partner would??? Is that too much to ask?,??,Let?€?s face it, why do I shop on Amazon.com??? They seem to know who I am, and what I value and I am in and out fast while getting the outcome I desire.?? To me they don?€?t sell stuff, they enable an outcome I value.?? I don?€?t think of Amazon as a seller, I think of them as my partner in enabling experiences I value.,Can I say the same about a bank today??? No.?? I am either a risk to them, or a revenue opportunity. Today a bank isn?€?t my commerce partner finding ways to support my family?€?s future and me by aligning offers and products that are relevant to me.?? The grade I would give a bank in terms of customer experience is an F.,Banking executives will often use the excuse that they are limited by regulation, which is just an excuse for not innovating around the customer.?? Maybe if banks started to think of themselves as commerce partners and consumer advocates they would not need such stringent regulations. The regulations are to protect the consumer.,Let?€?s call it what it is today. It?€?s a contentious relationship.?? I need them, they need me, but we are not friends looking out for each other.?? That?€?s the sad truth. We would both be served best by changing that.,??,Today?€?s successful manager understands that quantitative methods can be powerful agents for solving the problems of human institutions, and in some cases human beings.,In capitalist economies such as that of the United States, Canada, and the Western European countries, managers of firms are continuously faced with numerous choices.,Managers of firms are assumed to have certain objectives, such as the maximization of profits of shareholder wealth, or the minimization of the cost of producing a given level of output.,Because managers and consumers are pursuing their own private interests and decisions are made in a decentralized manner, rather than by a central planner, a very important question concerning the coordination of economic activities arises.,This long debated problem has been solved today with the explosion of access to real time data that can be derived out of social media, and big-data. Yet there are a few ways to manage this further. One is microeconomics and social media sentiment which allows us to tie real time consumer opinion.,The other way to manage economics is macroeconomics. This approach is concerned with the issue of how the quantity and price of output of individual firms or industries is determined. In contrast, macroeconomics addresses the determination of the entire economy or aggregate output and price.,In the case of social media it only showcases the consumer's current state of being, but it does not help predict market conditions that will influence that consumer derived from micro or macro-economic concerns.,Big-data micro-contextual segmentation on the other hand, can triangulate all these moving pieces with the right analytics and decision algorithms which would solve the biggest question companies always face: What to sell now and tomorrow to maintain consistent growth, and profits.??,In the final analysis, only the customer can decide whether the company has created value and whether it will survive in the hypercompetitive global marketplace; more reason to tune into what consumers want and value. Listening to the consumer is not enough, tracking behaviors is not enough, enabling the consumer to have a voice is not enough, and enabling the consumer to be at the helm is not enough.?? All the above working together in a customer engagement collaboration solution might just be the ticket.,??,Predictive modeling of trends among social groups to identify life values, and developing the business intelligence approach to integrating such data across all business functions is critical.?? Banks will be able to properly develop, market, and retain relationships as a result of this marriage that?€?s missing today.????,Big-Data is nothing without new algorithms needed to match consumers to values. Forget psychographic, demographic, and life stage data - that's not enough. ??Those models work in a verticalized market and were part of the industrial revolution assembly line methodology of marketing. ??,What's needed today is to identify consumer values, as in what matters to that individual at the core of who they are contextually - only through social media, and big-data tied into a contextual segmentation engine can that be accomplished.,It?€?s clear there are too many moving pieces for all the decisions to sit with either the CIO, or the CMO. Both need to work together to properly leverage all the components that make up today, and tomorrow?€?s ever-unpredictable consumer.??,What will such data reveal once properly analyzed, synthesized, and put into semantic context?,1) How we track data must change. We are too verticalized to properly make sense of it.????,2) Education system was built on the assembly line concept, to support verticalization, and this will not serve the needs of the future brand ?€? it?€?s too limiting.,3) Companies who are not engaging with consumers at all will cease to exit within 10 years, no matter how big they are today.,4) Branding, marketing, and consumer engagement will need to be realigned around values.,??,Start focusing on understanding consumer values. Yes values, as in understanding that a group of consumers may value connecting with other people, more than a group of consumers who values escaping from reality by being entertained.?? A consumer who values connecting with others will want you to be able to provide him/her all that goes into that??as in telecom, transportation, social networking, and events.??,??,These groups of consumers who value connecting with others is being served by multiple brands today, which use multiple strategies, which are all competing for a voice with them.?? Banks who understand consumer values can align themselves as enablers, and commerce partners to help consumers achieve experiences they care about.,In order to effectively accomplish this, the first and most important step will be a marriage between the CMO and CIO, and a closely sponsored relationship from the CEO.??,Secondly, goals will need to align around the consumer and provide values to the consumer not by vertical, but by segmentation around consumer context, specifically values based on data broken into a segmentation of one at the touch point when it matters most to know what to offer, for the right reasons.,??
Healthcare Information Management Systems need updates and changes as other industries. Agile Project Management that can be utilized to manage and implement these small or large healthcare informatics projects in small chunks of work.??Agile projects deliver value to the hospital/clinic management in frequent small deliveries of product in terms of software updates, addition of new modules, and update of existing modules which can also be called features.??We use the traditional Waterfall Methodology, in the beginning to collect the requirements??for the project, as it is vital for any project to have documentation to make the project HIPPA compliant.??,??,Once the requirements are collected then the design of the whole solution gets started and would be completed in specified time, as mentioned in requirements, when this (these) features/modules need to be available for the hospital staff to start work with.??Next phases are followed by the,??,-- Development,,-- Testing, and,-- Implementation of the product.??,??,If this whole process takes a year to complete as traditional Waterfall has the tendency to lengthen the project, the business does??not see any concrete significance until the very end of that project.,??,When we decide to use Agile Project Management, these features/modules are produced via diminutive??logical chunks of smaller tasks called iterations or sprints.??Usually the business needs in Healthcare industry do not change that frequently, however Agile is a great technique to use when, any of the changes are needed to be embodied in the current system or any alterations are required due to a merger etc.??,??,Every single iteration can be easily accommodated when we use Agile Project Management, and the project associated staff can focus on what are the business needs now, rather, what these needs will be in distant future.??,For more, kindly visit:


An important question in the contemporary world of social media and the business ventures using Cloud seems like hype. Organizations, academics are trying to introduce Big Data analytic curriculum to solve associated analysis problems, which seems not to be handled by the graduates and experts of Statistics, Mathematics, Biologists, Chemists and also Sociologists, we do not even discuss rest for the sake of readers?€? time.??,Big Data as we have so far discovered by the research, print (e-print more of in blogs, on videos, in conferences etc) media for any business, a research concern is available and accumulating in any type of formats, whether it is a report (on-going sales report, retail B2B report) or a qualitative research conducted by a market research firm, it can also be in the form of logical analysis of some situations to draw upon some continuous data, analysis of signals generated by the sensors, in some sort of work with audio visual forms of data.,The curriculum established for several decades or in last few centuries of Mathematics and Statistics have several ways to overcome these analytical needs with defined algorithms, in case, if we are not that deep in any of these mentioned fields, we do posses many experts, who can help us learn and understand these methods, for example the data dispersion can be found out of a huge data sample with some confidence using Standard Deviation. It will definitely help to solve or partially provide us with results one might be looking for.,For more details, you can read the book "Big Data, NoSQL, Cloud A Paradigm Shift", which is available at??,.
Hey Data Scientists,
We are in fall and days are getting colder. soon winter will be on us and all our homes will be heated. ??check out Natural gas flow dynamics for New England region during the month of October 2013.??,We have used Algonquin pipeline flow data and natural gas price curves from ICE ( Intercontinental exchange).,??
In this blog, I share some images from an application called Storm.?? I wrote the program many years ago.?? Storm has the ability to generate 3-dimensional plumes from a stream of data.?? It also has an unusual feature that allows the user to trade based on the kinetics - effectively eliminating the need to know about pricing.?? At this time, I would like to draw a clear distinction between , and ,.?? I should also point out that I used Storm for recreational purposes so I could - in a literal sense - "play" the market usually during interesting times in history such as WWII, Vietnam, and the Tech Bubble.?? When I was developing the program, it occurred to me that probably some gifted individuals - maybe even children - could trade by kinetics much better than me.?? I also used Storm to examine precipitation and hurricane data, electrocardiogram signals, earthquake data, and other lengthy streams of data that conventional software seemed poorly designed to present graphically.?? However, the main purpose of this blog is to share details about the plume structure, which might assist in the visualization of dynamic data more generally.,I present three types of plumes below:?? Paranoid, Reluctant, and Reactionary.?? Each plume is presented in three different formats:?? surface image (left); depth image (middle); and transitional (right).?? In a transitional image, the surface data is algorithmically squeezed into fixed boundaries.?? I generally used the transitional image for simulated trading purposes.?? My basic rationale behind these plumes for trading purposes is as follows:?? ,.?? However, incidents of repetitiveness tend to be distributed in an evasive manner.?? Repetitiveness should also not be confused with repetition particularly as it relates to stocks.?? The reductive nature of the index - the pricing - tells us almost nothing about the underlying investment or the activities of stakeholders:?? this makes it unclear whether at any given time pricing represents fair or flawed valuation.?? I found the usefulness of kinetic representation in how it allows for sense of tempo and rhythm.,In order to generate these plumes, it is necessary to load the data into stacks:?? (1) can hold the price for today; (2) the price for yesterday; (3) the price from 2 days ago; and so on.?? Therefore, (1)-(3) represents the price for today less the price from 2 days ago.?? Sometimes rather than hold the actual price, I find it helpful to load the moving average - averaging from today into the past - maybe over 10 days.?? So these are ,A Paranoid differential plume is jumpy.?? A person could logically expect it to be jumpy because the plume structure is based on the following differential pattern:?? (1)-(2) (1)-(3) (1)-(4) (1)-(5) (1)-(6) (1)-(7) and so forth from left to right across the surface - the differential forming the depth of the plume.?? I found it difficult to fit a Paranoid into fixed boundaries.?? It routinely hits a boundary and keeps banging against it.?? The surface appears muddy or blurry.?? I have tried playing the transitional image, but it is fairly fast-paced and stressful.,In a Reluctant differential plume, it is assumed that the underlying numbers have certain convective qualities - that pricing flows gradually.?? In order to capture this phenomenon, the following differential pattern is used:?? (1)-(2) (2)-(3) (3)-(4) (4)-(5) (5)-(6) (6)-(7) and so force across the surface.?? The surface appears icy.?? The kinetics will tend to be contained in a tight centre - pulling to the boundaries only under exceptional circumstances.?? The transitional image is easier for people like me to play - that is to say, mature people that to enjoy the scenery.?? However, a lot of potential gains can be lost by the time a boundary is reached.?? (The push is at the Paranoid.),The surface appears bumpy on the plume.?? It is a cross between a Paranoid and a Reluctant differential plume.?? Here is the pattern - adjusted slightly to clarify the sequencing:?? (1)-(2) (2)-(4) (3)-(6) (4)-(8) (5)-(10) (6)-(12) (7)-(14) (8)-(16) (9)-(18).?? There is an assumption that both convection and abruptness are relevant.?? I can usually play the transitional image of this plume comfortably.,In order to fit the differential plume into fixed boundaries, I exploited the cyclical patterns on the surface images - indicating movement from one extreme to another - e.g. from all turquoise to all gold.?? I eliminated the amplitudes and kept the polarities such that all turquoise , created one boundary (-1 to the left) and all gold the other boundary (+1 to the right).?? (The colour scheme for the transitional is different.)?? Due to limited computer processing capabilities at the time, I could not flip between different types of plumes from this aerial perspective.?? I usually played the head or face of the transitional image as shown below.?? I often combined all three of transitional wave patterns.?? The face is the front of the transitional image - the left being the , part of the plume and the right being its , - the top conceptually representing , and bottom ,.?? So the objective of the game is to catch the wave at the bottom and sell at the top.?? In Paranoid play, the wave might reach the bottom, bounce around, and then dive.?? So I quickly discovered that tempo was an important issue - and no particular stock necessarily gave a good sense of it.,Through this approach I found it possible to do simulated trading without knowing the price.?? Moreover, the visualization provided a great deal more information than the price on its own.?? I also found the games fairly entertaining.?? I wondered if it were possible to maybe to test for different aspects of cognitive problems using Storm.?? I imagined some brilliant young person perhaps with a musical or dance background being able to anticipate hurricanes and earthquakes.

Like ?€?Big Data?€?, the term ?€?Cloud Computing?€? has given rise to a number of misconceptions concerning what it is and what it does. While many of these delusions have been sufficiently debunked, a number of mistaken beliefs about the cloud have managed to keep hanging around---attaining mythical status despite being laid bare on more than a few blogs designed to dismiss them once and for all. And so, culled from the blogosphere for your consideration comes this non-biased compilation of the top 5 cloud computing myths?€?that just won?€?t die.,This myth---the notion that there is only one cloud: the public cloud---has made the rounds on a large number of cloud myth busting blogs. Of the various IT experts who weighed in on the subject, the general consensus was that, although there is greater awareness of the public cloud, the private cloud is not only a reality, it is quickly gaining headway in the corporate landscape. Offered as evidence to refute the ?€?one true cloud?€? mythology, reference was made to a ,??poll, wherein 9 out of 10 respondents stated that they were either in the planning stage, the implementation stage, or that their organizations already had a private cloud up and running.,Another myth that seems to be widespread among enterprise decision-makers is that cloud use is an all-or-nothing proposition. You?€?re either in or you?€?re out, with no options in-between. In dispelling this major misconception, a number of articles discussed the reality and practicality of a ,. Through the integration of public and private cloud applications, the hybrid cloud allows enterprises to customize and scale cloud use according to their specific needs.,The idea that transitioning to a ,??is a long and laborious process is a common misconception that is holding many companies back. While the thought of shifting from a traditional network to a cloud-based infrastructure can be daunting for any enterprise, the transition is typically easier and faster than expected. A strong case in point is found within the Federal Government. Recently the Department of the Interior (DOI) contracted cloud services with IQ Business Group to utilize its SaaS platform to capture, classify and store a whopping 75 million emails per month. The time it took IQ Business Group to get the DOI?€?s Enterprise Records and Document Management System up and running? A mere 45 days. Contracting with the right cloud services provider is the key to a quick and smooth transition.,Both of these misperceptions regarding the costs of cloud computing are prevalent on the web, along with seemingly sound arguments supporting each viewpoint. In weighing both arguments, the truth lies somewhere in the middle. Whether cloud computing is costly or inexpensive depends upon how enterprises go about it. For example, public clouds with pay-per-use features seem to be very economical for applications that are short-lived or those that have highly variable capacity requirements. However, for applications with long lifespans that have fairly constant capacity needs, fixed monthly or yearly costs appear more economical. Hybrid clouds could also be more affordable, depending on the needs of the enterprise. Although a switch to the cloud will necessitate up front costs, savings down the line should offset those costs. A nearly unanimous opinion on the subject of cloud costs was that cost-savings should never be the primary motivator for going to the cloud.,Even staunch supporters of a cloud-based infrastructure admitted that the idea of storing and processing sensitive data off-site warrants a discussion of cloud safety. The private cloud is thought to be more secure than the public cloud, as the former is sequestered behind the firewall of the enterprise. However, the actual level of ,??is dependent upon the security resources and practices of the corporate data center. Enterprise-grade public clouds have seriously upped security by employing cloud security experts, staying fully compliant with regulatory and industry standards, conducting regular third-party security audits and conducting automatic hardware and software updates. Industry experts caution that enterprises need to understand their cloud provider?€?s security practices in order to access potential threats to security.,Myths, by definition, seem to have a life of their own, and the workings of the web can keep myths circulating indefinitely. Therefore, it?€?s essential for any enterprise considering cloud technology to practice due diligence, rather than being swayed one way or another by the myths and misconceptions perpetuated in the blogosphere.


Romance and big data have a lot more in common than you might think. Though the world of tech and data may seem an odd place to uncover love, both online dating and big data work to personalise what a person or brand has to offer, matching and targeting it uniquely to appeal to that one special individual who'll want exactly what?€?s being advertised.,In many cases for both singles on the dating market and brands on the commercial one, achieving success ?€? using big data to successfully reach the right individual or unique prospect - is the start of (hopefully) lifelong, trusted and better matched relationships, and could be the future for love and online dating.,A facilitator of the modern way of life, big data helps us on a daily basis in all kinds of areas, from retail to healthcare, finance and more. Data is not only relevant to advertisers and marketers however. With infinite uses, targeted information helps us live safer, healthier, more personalised lives - including our love lives - matching the perfect partner to the perfect person at the perfect time. After all, better optimised and analysed data means one in six of all US marriages now occur as a result of online dating.*??,Defining exactly what makes data ?€?big?€? data can be confusing.,For example if people simply visit dating sites and input information, this does not constitute big data. Big data is defined by the conjunction of three things; volume, velocity and variability.,From the original computer punch card dating-match systems which first came out in the 60?€?s,** to the comprehensive online dating sites we have today, the process of using data to pair people has become increasingly streamlined and effective. Higher percentages of people are generating data of volume, variety and velocity online; on dating and social sites and through apps (according to the article??,??dating app use is growing faster than all other apps combined). As a result, the data created is allowing matchmaking services to target more accurately than ever before.,Regardless of platform, big data infrastructures already support dating sites and apps**, and make the analysis and management of voluminous data sets (,??according to eHarmony) possible. In the future, big data will likely play a more active role towards enhancing match accuracy right from the start of data collation, just as in marketing; using real time information about people, their unique background, hobbies and more from a variety of sources in addition to self-inputted information, social profile and mobile app data currently used.,The ability to use data from a varied reach of sources as dating service consumers opt-in, will likely be key in furthering the relationship between big data and dating.,On dating sites users currently input the explicit features they prefer and state what they want to know when looking for a partner, say; commitment views, age, hobbies etc. (Profile creation works on the same principal. But often, our actions don?€?t quite match our words. For example you may have stated (and think) you?€?re a fan of pure reggae, but your iTunes history may speak differently. What if in future, purchasing data, on an opt-in basis, could be used to match you more accurately with others with similar tastes?,Analytics, as an informational facilitator already currently uses behavioural matching to similarly enhance accuracy. Users may have stated, and??,??they prefer brunettes but might??,??click on more profiles of redheads without realising. It?€?s here that data really advances the way online dating works; analysing what you?€?re??,looking at, and what you're??,looking for best match accuracy.,Ultimately, analysing just what makes people fall for each other is not an exact science, and compatibility on meeting is still the decider for many relationships. So while right now, data can?€?t quite predict results 100% accurately, it can give romance a helping hand. But give it time ?€? and in a few years data may even be able to solve that.,??,Other references??

 , , , , , , , , , , , , , , , , , 

Practicing Data science indeed a long term effort than a learning handful of skills. ??We ought to be academically good enough to take up this challenge. However, if you think you came a long way from your academic rebuilding, ??but you still have that zeal & passion to take the oil from the data and fill the skill gap of data science then here is the,??tips. Below points must??,before jumping into any data science & data mining problems:,Not all datasets are in the form of a data matrix. For instance, more complex??datasets can be in the form of sequences, text, time-series,??images, audio, video, and so on, which may need special techniques for analysis.??However, in many cases even if the raw data is not a data matrix it can usually be??transformed into that form via feature extraction. A practical example of feature example is explained in my last post on,library.,: ??The algebraic, geometric & probabilistic viewpoints of data play a key role in??data mining. You should exercise them beforehand. So??you can easily sail though your boat in Data Science !,Original post: 
I am a student currently pursuing a course on ?? analytics ??from Ivy Professional School. I am curious to know what are the future prospects in this field & jobs available for freshers who does not bear a stats background.,I would be highly obliged if professionals & recruiters could throw some insight on the above mentioned topic

Our modern lives are driven by computers. Machines grade your child?€?s standardized tests and identify potentially fraudulent credit card purchases. Behind the scenes of modern society even more machines are creating staggering amounts of unstructured data. Much of it is stored, but almost none of it is analyzed. Keeping these networks running requires that analysts not only be able to understand the streaming network data, but that they understand it as it happens.,Machine log data is generated by servers, routers, and other networked devices as they interact and manage processes and commands. Network security analysts are intimately familiar with the concept of machine log data, as the core of their job description is monitoring machine generated data for time-critical threats and machine failures.,The analysts tasked with monitoring streaming machine log data for network intrusion and fault indicators have seas of information flowing at them at once, and the tools available to detect and act on threats are manually intensive. Network analysts currently rely on querying sections of historical data to identify anomalous patterns similar to past downtime events. Meanwhile, threats change and evolve as fast as they come. Manual querying and the establishing of quickly outdated rules leaves holes in the ability to detect and act on threats as they happen.,Analysts??,??in their protective arsenal. These tools??can pair with other network monitoring tools to continuously oversee network data, rank and list the most surprising patterns detected, and eliminate the noise that might mislead analysts. Automated pattern detection processes data in real time and stores historical data so that streaming data can be compared over time.,With automated pattern detection at their disposal, analysts are presented with a ranked list of the important and relevant events that demand attention instead of digging for them hopelessly. They can detect threats as they happen, preserving a potentially fragile balance between offensive and defensive measures to protect your network investments. Users can even prioritize events and set alerts to stay on top of the most critical threats to network performance.
While discussing the value of??,??in my last , article, ,commented succinctly the need for a shift from macro (market) trend analysis to micro (or individual) trend analysis.,Engaging with customers, partners and each other as individuals has fundamentally changed and the power of decisions is the individual. Before the consumer revolution and expansion of social media platforms and communities, we did not have the ability to connect and analyze at this level.,Personal relationships with brands and products existed, but were not visible to others - as is now possible with the digital economy and social media. While it is possible today to extract value from the opportunity it is critical to have both a business strategy (CMO) and big data analytic solutions (CIO) working together to support growing the business - one person at a time.,Banks have been leaders in analytics for decades, yet they have not fully realized the benefits ?€? until now.?? What?€?s different now?,Customers are expecting a more personalized service across all industries, and banks are not immune.?? Regulators are instituting more intrusive and granular requirements while the world?€?s data output from perpetually connected consumers are creating major challenges in satisfying customers and regulators. All these factors are making it increasingly more difficult for banks to stay relevant and turn a profit,Financial Services Industry (banking) CIOs and CMOs are under incredible pressures by business leaders wanting more consistency in information and by regulator?€?s references of data integrity with each new requirement.?? Unlocking the contextual insights in the data to better understand customers represents a significant opportunity to gain a competitive advantage, and fundamentally change the way decisions are made for commercial gain.,Predicting how customers and competitors?€? customers will behave and how those behaviors will change is critical to tailoring fees for timely and relevant consumer offers.?? Big Data should be about changing the way you do business to harness the real value in your data, re-shape the interaction with the market, and increase the relationship value with your customers. Therefore, which data is required to achieve these objectives, who needs it, and how often, are key big data decisions to consider, especially when multiple data sources, coupled with geo-spatial data, social media, emails, call center transcripts, and other unstructured data all play a part in knowing the customer today, and tomorrow.??,Both internal and external data, structured and unstructured should enable financial services firms to personalize their products to each customer and tie in enhancements needed across the organization to support better customer-centric performance.,In resources planning and alignment, big data solutions for financial services is not only about customer segmentation of one, but also about leveraging existing assets in such a way to reduce costs of infrastructure deployment.??,These can either be shared or optimized better to align people?€?s skills around the right initiatives that align with what will add the most value to the bottom-line at any given moment in time.??,Financial services companies are using big data today to focus on operational issues ?€? risk, efficiency, compliance, security and better decision making, however there is a growing need to identify how big data is going to be used for innovative profit growth.?? The value of big data is the ability to triangulate all these disparate activities into a more holistic approach to managing customer relationships.,If you have a set of customers that??you've??missed communicating proper disclosures to, you have a possible Dodd-Frank compliance issue, right??? But, you could also have a relationship issue with that customer who?€?s discovered outside your walls, either from your competitors or other consumers that they have an alternative service they could change into.,Offering this consumer a new product, might not be such a good idea until you resolve the compliance, or expectations issue.??,Conversely you may have customers who are on your A + list for compliance.?? These customers tend to be very supportive of companies they do business with because they tweet about them and mention them on Facebook.?? They are champions for brands they love, yet if you don?€?t know this you may be missing the opportunity to increase revenues from your loyal supporters.,The problem isn't , knowing.?? Banks are great at postmortem and analytics ?€???they've??been doing it for years.?? The problem is what to do when the touch point happens and having a real time view into the customer dossier at that moment in time to determine the timing of the up-sell or damage control, keeping in mind that the dossier will tell a different story each time ?€? it?€?s never static.??,This is exactly what gets banks into trouble - assuming consumer values, behaviors, and needs are static ?€? they no longer are.,They can change several times in a single day, which means your 9:30am call to your call center will produce a certain contextual insight about your customer, which is different than the branch visit at 2:45pm on the same day ?€? each touch point is a unique opportunity and needs to be treated as such.,The inability to connect data across organizational and department silos has been a business intelligence challenge for years, especially in banks where mergers and acquisitions have created countless and costly silos of data. Sourcing and analyzing internal data isn?€?t difficult, discovering value still locked away in these internal systems is another story. Financial services firms lag behind their cross-industry peers in using more varied data types within their big data pilots and implementations The reason for such a lack of focus on the unstructured data is due to the ongoing struggle to integrate the organization?€?s structured data. The real problem is that banks are looking at it mostly from a technology?€?s perspective ?€? it?€?s a business application that needs to be introduced, not just the tech.,Financial services further lags behind in core capabilities of text analytic in its natural language state, such as the transcripts of call center conversations. These analytics include the ability to interpret and understand nuances such as sentiment, and intentions, and can often be used to understand behavior and references to improve the overall customer experience.,To compete in a consumer empowered economy it is increasingly clear that financial services firms must leverage their information assets to gain a comprehensive understanding of markets, customers, channels, products, regulations, competitors, supplies, employees and investors. Realize value by effectively managing and analyzing the rapidly increasing volume, velocity, and variety of new and existing data, and putting the right skills and tools in place to better understand their operations, customers, and the marketplace as a whole.,Customers interact from a number of touch points within the financial services walls, or outside on the social media sphere. All connection points currently lead to a data repository, be it a social media monitoring effort that is feeding into a CRM platform, a customer service center call recording and transcript, an online transaction, an ATM deposit, or an in person transaction; all these touch points currently collect data in silos.,Solutions such as what SegOne Inc. is developing, apply semantic indexing algorithms to make all the data sources more insightful in order to present to the customer in real time, the right service/product offerings that meet the customer?€?s needs at that particular moment in time. Contextual analytics is the soul of your data ?€? without a soul you are a corpse or a robot ?€? not able to add any value at all.,With real time micro-contextual analytics, when the touch point with the customer is happening, the bank will be able to provide:,??,At the same time provide feedback to R&D, Product, Marketing, IT, and the business line executives on:,??,When you take millions of touch-points daily, constant regulatory changes, fierce competition, unstable, volatile markets worldwide, perpetually connected consumer data which reveals behavioral, and interest patterns, it becomes really challenging for banks to have an effective big data initiative.??,Furthermore the need to keep the customer facing staff up to date on everything, in order to make the best recommendation to customers, based on what the customer interests are, and making sure they communicate within compliance requirements, is almost impossible.,Add keeping up with the assets requirements across the enterprise to keep up with all the movements, come up with a targeted marketing campaign to attract, retain, and expand relationships, then take the time it takes to develop, deploy, and prove out a big data initiative, add seven figure consulting engagements to help you triangulate it all, add the cost of transforming to a centralized data warehouse, add the time it takes to put a solution that is useful in place --- and you know exactly why banks are lagging behind in big data adoption.??,Banks need big data business applications to help them solve many of the issues they are facing.,Developing a big data strategy for banking should not be a technology-focused exercise.??,Fundamentally, banks need to be able to transform their business models into customer-centric models and transform their environments into customer-advocate centric environments based on all the regulatory requirements.?? Realistically banks need to become enablers of consumer commerce.,Banks need to know who the customers are in terms of what they value, how they behave, how they interact with competitors, other companies, and other consumers. In other words, know who needs to be handled with kid gloves from day one, and who is a brand champion.,Banks should not assume that a quarterly, or even monthly segmentation exercise will cut it, or that they can segment all customers into a few manageable buckets ?€? that does not work anymore!?? For a long time segmentation of one, was considered impossible, not scalable, and unmanageable ?€? for financial services it?€?s a fundamental ?€?must have?€? in order to manage risk across 100% of its customer base, and in order to drive fees and transaction based revenues from 100% of its customer base.,If your big data initiative does not create a way to automate managing each customer, individually, while turning compliance requirements into strategic advantages, it?€?s not a big data initiative you should bother with.?? It?€?s simply another version of the same old, same old ?€? that?€?s perhaps faster.,A solution that enables banks to have micro-contextual analytics for each customer will empower banks to get ahead of risk, and proactively drive profits from the most valuable customers.
 , , 
Here is a refresher from the last post:,?€?I went on a job board and searched for the number of job postings that listed Big Data tools as part of the requirements.?€?,I created a comprehensive list for , in?? response to the feedback that I received for the last post.,As you may recall I cited three categories for job creation in my previous post: Data Management, Data Infrastructure, and Data Presentation. The emphasis here is on Data Management, although I have included some of the ingredients for building Cloud and Mobile applications as well.,I discarded the tools that scored low.,See 

CHeck out my blog post at: ,??,??
Hi all,,This is my first post here. I'm glad to introduce this newly launched big data analytic engine, the BigObject. In the past 2 years we have been working on an optimal approach to handle big data for analytic purposes and challenging the existed models, some assumptions of which are no longer valid. For example, as the data size grows so rapidly, is it still practical that we stick to the relational models neglecting the time spending in data retrievals? What impact did the advent of 64bit processor cause to change the way we treat data and code? The approach we adopt, in-place computing, roots from the concept "computations take place where data are stored." We believe that making data components ready for CPU to compute instead of moving them around is the most efficient way.,Moreover, we manage another "V" factor that is ignored, the Valence, the degree of inter-dependency among each data components. The data with high valence causes the data shuffling among the computing nodes and hence slows down the computations. The in-place computing approach can avoid this problem.,Another important thing to mention is, the BigObject is dedicated to deliver affordable computing power. It fully utilizes the virtually infinite address space of 64bit architecture and trades space for time. For the records,??
I have so far encountered two general types of data . . .,This is data that conforms to prescribed criteria.?? I sometimes describe it has the metrics of criteria or measurements of conformity.?? For instance, an organization might want to measure something potentially obscure like "efficiency."?? It therefore becomes necessary to establish under what conditions or criteria something is efficient.?? I describe "sales" as reductive because it literally just tells us about sales either in unitary or dollar terms.?? If sales start to decline, we have essentially no information why or how to ameliorate conditions since such details are absent.?? The focus is on function, process, and performance - details that have little value when an organization intends to abandon a declining market.?? Some forms include orders, sales, and inventory.,This is data that describes or informs us of phenomena.?? I sometimes call it the metrics of phenomena.?? There is conveyance of embodiment so often missing from reductive data.?? We gain details of the underlying reality behind the management numbers.?? For instance, an organization with a large number of disability claims might insulate itself from the reality of workplace conditions - such as employees being stationary and physically inactive for long periods of time.?? Aspects of phenomena might include geography (spatial characteristics), scheduling (temporal considerations), and presence (physicality).?? The focus is on setting, participation, and consequence - things that give guidance under deterministic conditions.?? Some applications might include transit, siting, intelligence, and surveillance.,When something is perceived, it can be interpreted both in reductive and expansive terms.?? However, it is important to note the power dynamics.?? To identify something as inefficient - to add weight to this label - is to reduce it of attributes and characteristics that might create competing interpretations.?? If this occurs on a pathological level, I call it instrumentalism.?? From a management standpoint, a business case might be easier to establish if environmental destruction were dismissed as non-substantive.?? It might make perfect sense - under an insulated and inebriated paradigm - to exploit uneducated female garment workers and their children in a foreign country to supply us with cheap clothing locally.,The power dynamics are systemically persistent.?? The dichotomous metrics of criteria and phenomena affect the use and usefulness of organizational data.?? The nature of the data is actually different from a structural standpoint.?? There tends to be a fixed and relatively small number of reductive data fields.?? Expansive data literally expands (possibly using special software) carrying not just the body of phenomena but the language of meaning that extends from the phenomena.,Any and all sorts of comments are warmly invited.
I have always found the task of converting qualitative data into something quantifiable a bit challenging.?? A common route might be as follows:?? assemble all of the resources containing qualitative information (e.g. questionnaires containing open-ended questions); seek out apparent themes in the responses; and quantify how frequently these themes are mentioned or raised.?? This methodology leaves open the question of when something is or isn't a theme, and whether something must be a theme in order to be relevant.,Facing several dozen human rights tribunal cases in order to complete a paper on disability law, I decided to design a questionnaire that I could complete based on the qualitative details of each case.?? Whereas an ordinary questionnaire often contains only a handful of open-ended questions, limiting the amount of information that must be handled, a tribunal case might be 10 to 60 pages in length.?? A thematic comparison seemed pointless to me since the same themes occurred in each case.?? (This is because I obtained the cases by using a search engine that screened for particular themes of interest.)?? So I decided to develop a conceptual framework for the data.?? Perhaps having a survey without a guiding concept would be a bit like open-pit data-mining - gathering facts in the hopes of finding something valuable at a later time.?? Nonetheless, I still remember telling myself, my conceptually-driven approach seemed rather foolish since I had to know what I was looking for in advance; there was also a danger of going down a completely wrong path.,I decided to use something rather elaborate derived in part from a number of different behavioural models.?? Not that the exact details are too important at this point, but I called it the ACTOR definition protocol.?? For each case, I noted certain aspects of how the adjudicator made comments on the following:?? Attraction - the perceptions of the businesses involved; Conduct - the extent to which the businesses took others into account; Tenacity - the amount of effort exhibited by the businesses; Organization - the systems and structures in place in achieve certain outcomes; and Responsibility - how the businesses recognized their roles.?? I found that adjudicators tended to confirm when performance was inadequate, adequate, or more than adequate.?? This meticulous delineation of discourse took some practice, but I eventually created a conceptual portrayal for each case that could then be quantified.,I want to point out that although conceptualization might reduce the amount of information, the resulting data object is not particularly reductive.?? In fact, the ontological exercise was extreme by how it forced me to determine the placement of facts that seemed to have infinite shades of grey.?? I came to realize, ACTOR is merely a starting point.?? Many subdivisions could exist for each letter.?? I was accessing the reality of the case only through this structural context in order to extract numbers, but the reality is much bigger than the quantification.?? For instance, consider just the concept of Conduct: taking others into account - during the planning of policies - during the training of staff - while determining seating arrangements for patrons - to ensure accessibility - during the delivery of service - to manage competing interests - listen to complaints - and handle complaints.?? There are so many shades of Conduct.?? It seemed almost ruthless to assign a value of -1, 0, or 1 to the qualitative assertions.,I can add a small twist to this discussion.?? Let us say that rather than do tribunal cases, I have a steady stream of client surveys.?? Because the stream is perpetual, it seems impossible to know in advance the nature of all future responses.?? There is a bit of a technical hurdle that I think interferes with the continuous integration of qualitative data.?? Moreover, I might not know exactly how to make use of the data given that business needs are likely to change.?? So it would certainly be tempting to force the data into its simplest form to respond to my immediate interests today irrespective of the changing reality behind the data.?? I don't know if anybody has had the experience of posing questions that seemed worthwhile during development but were found to be unhelpful after distribution.?? It is necessary to have systems and methodologies to accommodate emerging realities - particularly when an organization finds itself confronting unfamiliar surroundings.?? Making effective use of qualitative resources might become increasingly important to help organizations adapt to change.
In this post, I discuss the basic characteristics of code that I have personally used to extract online data - in a process these days often called data-mining.?? I intend to cover some general features.?? Those that wish to do so can also compile the coding samples.,Over the years, I have programmed in a number of computer programming languages including Visual Basic, Perl, Python, and LISP (AutoLISP).?? The coding samples on this blog are written in Java, my language of preference.?? I never received any formal programming instruction beyond high school except for a short course at Sun Educational Systems.?? Nor do I consider myself a programmer.?? I think that the ability to handle code is simply something that some people should be prepared to acquire in order to achieve certain desirable outcomes.?? Java has a well-developed GUI or windowing environment.?? Also, I personally prefer an object-oriented approach.?? So this explains my choice of Java as a programming language.?? However, I certainly the support the use of other languages.,When crawling over the internet, I tend to come across the same nagging problem repeatedly:?? I never know exactly what to expect.?? Styles, formats, structures, and scripting can differ between sites, and these things can also change over time.?? So code written today might become obsolete in the near future.?? However, there are also basic issues to deal with such as how information flows over the net:?? that is to say, slowly and unreliably compared to a hard-drive.?? On a hard-drive when a person accesses a file, the process is almost immediate, and it rarely fails.?? Online, the user cannot be certain whether or when data will be downloaded.?? Then there is the question of what to do if the files are not forthcoming:?? how to backtrack for retrieval later; when to try again; and when to give up since the files associated with some links might simply be missing.,In order to deal with the uncertainty, I generally make use of a thread for the page or file that must be downloaded as shown below under WebFileLoader.java.?? Among other reasons to use a thread is to be able to halt the program in a controlled manner.?? Once on a thread, the program can wait for the data to load or instructions from the user or some other program to halt the load.?? Loading a file from the internet is similar to loading it from a hard-drive except that the file must clearly be from an online resource.?? In order to show that the resource is online, a URL is used to create an InputStream as shown under WebInputStream.java.?? The URL class can be found in java.net while InputStream is from java.io.,Those unfamiliar with Java will note how the program throws events that often must be caught - the most common in this application being IOException.?? The program can also be designed to throw user-defined events - for example, the recognition of non-conformance scenarios.?? I personally consider this facility useful for recording operational analytics:?? for instance, from 2,000 download attempts, there might be 1677 files downloaded, 201 faulty links, and 122 download failures.?? These are reductive statistics that only confirm whether or not I successfully did the download.?? If a download failure made the program record the path of the file, type of file, and time of download, it might be apparent that the server could not access specific types of files from particular locations during a certain time period.?? If the size in bytes is recorded, a file-size limit might be evident.,A programming problem I faced many years ago was to get the downloading thread to confirm that it has finished without necessarily being limited to one particular order source.?? For instance, if the program is downloading all at the same time a webpage, a binary executable, and an image, at some point it might have to say, "The person who ordered this image should please pick it up."?? When food is ordered from a fast-food outlet, it would be unproductive for the server to be limited to a single customer.?? So the basic idea is to allow for the gradual accumulation of data - and for different parts of the program to place orders and be notified as files are downloaded.?? The facility that makes this notification process possible is shown below under Forward.java, which I admit is a peculiar class for most programmers to use.,With the data flowing at different rates possibly from many locations, a person must then determine what to do with it.?? Generally speaking, I download pages and files in bulk and then apply additional processing to extract analytics.?? It simply takes too long to examine a lot of data directly over the internet.?? There is a concern about how far or deep to search for documents since links can open up links to infinity.?? Well, for sure I wouldn't want to download a document from the same location more than once, and I wouldn't want to count a link to that document from a particular page more than once.?? Once the files are on a local hard-drive, it is necessary to confront some the deeper questions such as what now??? Myself, I almost always have a specific objective.,It appears fashionable for people to download everything and then look for everything.?? I think there is a bit of a misunderstanding in terms of how data-mining works in a practical sense.?? Really, everything gives us nothing.?? If I look at a picture of a forest, and I ask children to look for things in the picture, a proposal from one child to look at everything might cause me to say, "Such as . . . ?"?? "Let's just systemically go through everything!"?? "For example . . . ?"?? Using the analogy, even a mining company has some idea what it hopes to find.?? What it wants to find legitimizes the cost and effort of finding it.?? A company doesn't simply buy parcels of land and start combing through it for anything.,The coding below was part of a larger program that I used to scan through the website of a product testing agency.?? I wanted to compile a listing of products that conformed to a particular safety standard.?? The data could then be sorted by manufacturer, year of production, and other specific characteristics.?? I immediately received interest by those that install the product to provide the listing, and I suppose had I remained in that industry I would have made every effort to do so.?? Stepping back from the listing, it is possible to gain an historic profile of products that changed as consumer expectations and desires changed, giving us a kind of product morphology related to external events such as fuel shortages, income levels, economic conditions, and social aesthetics.?? Happy data-mining., , , , , , , , , , , , , , , , , , , , , , , , , , , , ,??, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,??, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,??,??
Above - During a Session at the Archives of Ontario in 2011,In this blog post, I describe my early experiences leading me to conclude, data as we know it tends to be "disembodied" - that is to say, often lacking any kind of connection to different types of bodies.?? When we talk about things being disembodied, I suspect some form of decapitation is involved.?? However, I am referring to the disembodiment of information.?? There are some consequences to disembodiment - such as causal disassociation - that can reduce the usefulness of data-mining.,The image above shows my surroundings while reviewing the contents of about a dozen boxes containing management records.?? I was accessing data from an organization that no longer exists although it provided service for about 30 years.?? It operated before the advent of desktop computers; and it was closed down soon after computers became common in offices.?? A contemporary human resources department operates with a lot of more data these days; so some might argue that my observations are a bit dated.?? However, perhaps current conditions can be best explained by examining the early contributing factors.,The organization dealt with a lot of workers.?? When it was first conceived in the 1960s, there was great interest on employees as individuals - specifically those that had drinking problems.?? The organization later became responsible for all sorts of counselling needs.?? In the boxes, I found photo-albums.?? When a person has quite a lot of data, it is sometimes easy to lose sight of the underlying reality behind the data - namely, the people.?? I consider it important to be aware of the reified nature of the portrayal.?? I believe "reification" is a term that has been attributed to Karl Marx.?? I think reification is supposed to give rise to a type of alienation.?? I found it difficult while staring at these people with hair and clothes from the 70s and 80s to disassociate their bodies from their metrics.,A profile of insurance claims that I found in one of the boxes suggests that about half of the claims related to muscloskeletal and cardiovascular problems.?? I mention these two claim categories specifically because the proposed cost-cutting solution was unlikely to lead to a reduction in claims:?? decentralization.?? Not better chairs and desks.?? Not adjustable monitor stands.?? Not more opportunities to step outside and get air.?? ,, a non-health approach, was regarded as a way to tackle rising disability claims.?? I am not saying that decentralization is necessarily an inappropriate path in terms of administrative developments for an organization; but it might be unrelated to the underlying issues.?? If a person had an addiction to alcohol, and somebody suggested that decentralization could help, I would certainly express some concerns about the reasoning.,I have noted therefore that an abundance of data does not necessarily equate to a comparable amount of useful guidance.?? Today we have an increasingly abundant supply of data.?? The use of data-mining techniques can potentially create a new age of data for analysts and researchers.?? Collecting this data represents the beginning of a process rather than its end since , - decapitated early in life and cursed to wander aimlessly in the digital labyrinths of hard-drives.?? Neither reality nor interpretation resides in data.?? In my example, data provided a bridge for decision-makers to come to conclusions that were disassociated from the lived truths behind the numbers.?? Can any instrument of conveyance provide anything more than an assertion of reality?,I actually support the data-mining movement.?? Apart from the meaning of the data under examination, I am actually interested just in data itself as a subject.?? Nonetheless, there is social obligation.?? A person has to be committed to truth and beneficial outcome.?? The issue of poor or faulty embodiment should be important since actual bodies are involved.?? These bodies might be natural habitats, municipal infrastructure, students, and health-care recipients.?? Lack of embodiment in data can result in inappropriate conclusions, uninspired ideas, and unproductive approaches.,Any and all comments are warmly invited.
Big Data is big in nature, however Education data is not that big yet compared to Big Data. Quantitative analysis, Audio-Video recorded data of Education related research and general research work conducted by the academic institutions is steadily increasing. The data change is started within our education system as our students are taking exams, courses, conducting their research using computerized means. This means that the searching behaviors of students or whoever associated with education can be collected and trends can be drawn for the fields, which are under study by certain set of students to draw a trend means to predict, what might lay ahead for these students and where these students can get their research related material or help. This also means that these sets of data are now scaling and can be utilized for any such analytic needs.,Education Data Mining can be extremely helpful in doing inferences, predictions and more to establish students behavior and attitude as well as concentration to his/her education. The results found in this sort of analytics can help better the educational system as all stakeholders can look into the trends found once analytic reasoning is applied on the data of student related parameters. Usually we use regression techniques to analyze data, When we unitize the data into statistical numbers for analytic reasons, usually the produced results can be plotted on a graphs and trends can be found in terms of lines or combination of several data points as a concentration of some student behavior to learning or researching or any such related activity.,Classification is another method that we can use to analyze data, no matter this data is of researchers in a lab, or students in a university department or a product launched for consumers or a service provided to organizations in terms of B2B e-Commerce or supply chain needs. Let us take an example of a service analytic reporting to related stakeholders. Service utilization by general consumers, survey filled by the (many/few) consumers about, how they felt about the service, internal stakeholders' review of service, testing done on the service by quality assurance stakeholders can be considered as few parameters to be converted into labels. We can also consider a variable, whether the provided service is good or bad or there are other opinions, which can be taken into consideration by the designers for the release of next version of service to improve results.,We can use several algorithms for analytic needs, few can be, such as "Step Regression, Logistic Regression, Decision Trees or some sort of Classifier Algorithm". If we look into Step Regression, gives us the results in the terms of 0 or 1, if we look into the given service analysis example, this means, whether consumers used service or did not use service at all. These techniques are equally applicable to the field of Education to find out the ratio or some other trend, how students are performing of a certain school/district/college/university of one state or a country or can be expanded to the available Big Data for a certain area for research purposes.



The ,pivot role of a wild savant woman in the midst of revolutions and enlightenment: Emilie du Ch??telet from my summer book Passionate Minds of David Bodanis, Three Rivers Press N.Y, 2006. I am impressed by Emilie ?€? insights permeated into the scientific mainstream with her unconventional but logic thoughts, so original that her male scientists recycled her ideas even forgot who had originated them. Emilie du Ch??telet contributed the square to Einstein concept of energy E=mc2, how she revealed to Newton the gravitation hidden truth; (check Wikipedia for the long list of her contribution to sciences) not to mention??her audacious relations with Voltaire and their border escapes out of Versailles calumnies.,This is probably old story for you, still I think it fits with the current science transformation period we are in.??,Pondering her high horse spirit, I wonder how Emilie ought to think and react at the dawn of scientific transformation of the modern day? What else women avant garde nowadays did not do what Emilie did? One thing for sure, Emilie did not have to transgender to express herself. What else Emilie did not do as post-modern women did? Emilie was raised in a catholic noble French family. What would she do if she were born as sub-minority? how would she manage to express herself in a "predestined" social concept? what would Emilie do dealing with predictive programs? Centuries followed, countless of intelligent women continue to inspire the society in every aspects of life, to name some, Freud's Mistress Minna, the women in??Red movies (1&2), and quote,The Speilrein hype is just hype. , This novel attempts to capture the psyche of the early 20th Century, the wishful thinking of modern Feminism, projecting current neuroses and "triumphs" onto unsuspecting protagonists that never in this world existed.,??Hilda Pierce female intuition balance between spirit of principle and the spirit of greater good . Hilda Pierce in Foyle?€?s War, projects female intuition balance between spirit of principle and the spirit of greater good. You can see the recurring character Hilda Pierce, played by , (from the episodes , and ,).,In modern days, practical thinking wisdom is to keep the enemy closer...as a woman, I consider predictive machine and quantum leap as enemies to humanity, figurative speaking. Which explains my passion (love hate relationship)??for big data. I invite you to do the same, feminist or not!,Currently there are a handful of people who know how to program a real quantum computer. By giving??a free public access, the Bristol U. hope to generate more qubits codes! Quantum processor will be made accessible in this coming Sept 20. ,The??online Qcloud link means that this quantum processor can be remotely controlled by anyone anywhere in the world.?? This will allow the next generation of engineers, mathematicians, scientists and entrepreneurs to run an experiment, and test real experimental data against their simulations, making the resources for quantum computing available for everybody...


Extracting meaningful insights from data to address business needs has benefited immensely from the availability of data visualization ??tools that have ??data more approachable. Today the proliferation of off-the-shelf tools, which are easy to learn and are web enabled, have democratized the way data is presented and consumed. Tools like Spotfire, Tableau, Qikview have helped breathe life into data. They provide a professional look and feel and give an inherent feel of fidelity of the data that is being visualized, more than when data is simply presented as text .,Well designed and deployed Data visualization many a time could lead the user to a question which could spark the need for deeper insights and which possibly cannot be answered by the visualization software. But before we delve into this problem let us understand the primary goals of data visualization.,Essentially , Data Visualization is expected to??,Recently we had an opportunity to work on a project with the objective to understand and analyze the failures in a telecom network, and the related quality issues which could have direct bearing on customer churn apart from the repair and service costs. The goal was to enhance the quality of service (QoS) of the telecom network provider using predictive analytics. The added expectation was to enable the service manager to take proactive decisions on repairs using machine learning models.,We developed dashboards for the equipment maintenance manager, using a well regarded ??commercial off-the-shelf visualization tool. We also developed the advanced failure prediction model using the input data as the standard telecom equipment log files and used techniques such as ??pattern mining and event sequence analysis to predict equipment failure. ????R open source programming language was used to create this model.,We had ??two options to present this data. In one scenario the maintenance manager used the visualization tool to drill into the failures to locate the regions or equipment models with high failure. This information was then shared with the engineering team for root cause analysis who used our R based model to predict failure. The visualized data provided information to act on, but was it intelligent enough to bring in preventive maintenance?,The manager had a number of questions ??during such slice and dice analysis, namely - Why is a region doing better than others? Why are some failures more common in one model and a particular region and not the others? Unfortunately, simple data visualization cannot provide answers to these questions and they get stifled. Consequently the service manager hopes that his engineering team will come up with the right answers. Many a time, the well represented and slickly visualised data is ?€?counter-productive?€? by making the user numb to the questions which could get triggered.,Our approach was to integrate the relevant prebuilt sequence mining models in R and integrate it with the off-the shelf visualization tool.?? This approach immediately gave the manager the freedom to ask even deeper questions about the state of equipment he managed.,In the new approach the manager did not send the information to his engineering team for analysis but felt empowered to do the same . ??,Once the region associated in the problem got identified, next , running the advanced sequence mining algorithms, identification of the frequently occurring patterns in the repair history were ??carried?? out.. The patterns in data showed that two components were failing in tandem. While the short term measures would be generally to replace the defective parts, but more importantly the findings were passed on to product engineering team to redesign the part which would be more fault tolerant.,Here is an example of using the power of R aided by the inherent strengths of good visualization tool to build intelligent and ??actionable data visualization.The service manager did not have to leave his data visualization?? environment nor wait for the engineering team to do background analysis.,I would like to conclude that in data visualization, there is more to the choice of representation of data only. Data visualization should not make the user numb with its slick representation, but should help?? in revealing those elusive insights by goading the user to ask questions which he would have ??never asked.?? ??,by Somjit Amrit,Somjit is the Chief Business Officer of Technosoft Corporation, an IT Outsourcing Services provider.,He can be reached at somjit.amrit@technosoftcorp.com
Google recently replaced its AdWords MySql Database with a Database that they built in-house namely F1 Database. AdWords serves thousand of users, " which all share a database over 100TB serving up hundreds of thousands of requests per second, and runs SQL queries that scan tens of trillions of data rows per day," Google said.,After reading Google's paper on its F1 Database (not open source), I started thinking about its ramifications for Databases in general and , in particular. , paper might trigger new initiatives that eventuate in materializing the phantom (next paragraph). The paper mentions few challenges with F1 DB that need to be addressed. I came away with two lingering issues. First, there is no mention of security. Secondly, it states, "Hide RPC latency, Buffer writes in client, send as one RPC". What will happen if the network connection between the client and the Database goes down? Will the data be lost? This is a serious problem for operations that need to commit as fast as possible; Airline reservation is one.?? I probably misunderstood.,The system resembles a hybrid between Relational and Hierarchical (think mainframe) Databases. What is the Holy Grail?? in the Database world??? Relational Databases (RDBMS) are like high-rises comprising many apartments.?? What if there are no vacancies and people have lined up to rent from us. The way RDBMS has handled the demand is by adding more floors on top of the high-rise. It is expensive and slows down the day-to-day operations. A new technology (NoSql) emerged a few years ago and solved the space allocation problem. Instead of building new floors we place the tenants in inexpensive houses. Once we run out of vacant houses we give the tenants new houses. The downside? It makes managing the place more difficult and?? we might unwittingly?? reserve the same house for two different individuals. There are ways to prevent that, but it's a perplexing task and it places a lot of pressure on the engineers who design the housing complex. The Holy Grail is to discover a method by which we ??can combine the best of both worlds and remove the negative.,Following Google's invaluable tips in the paper, no doubt some engineers are working hard to figure out how to build an F1++ Database. What if they succeed? What will happen to NoSql and NewSql if they produce an open source Database System? The confluence of several forces that are currently shaping open source, Big Data, Mobile, and Cloud technologies might in time make NoSql and the existing NewSql irrelevant-- flash-aware applications, shared-nothing architecture, Mapreduce methods, software-defined storage, in-memory computing, shared virtual storage array networks, new compression algorithms, atomic writes, horizontal scalability, software-defined networking, columnar technology,?? progress in fault tolerance, database sharding, and solid state drives.,There is one very powerful force that in my view will keep NoSql alive and well for years to come and that is the power of developers. The genie is out of the bottle and all the nuclear fusion combined in the world cannot put it back in there. Speaking from personal experience as a Developer/DBA, I know that developers hate roadblocks. Once they start on something they like to continue working. To get them away from what they are deeply involved in is like taking a pacifier from a baby. For the first time in history, they can get on their generally free and open source bikes and run without the hassle of calling the DBA's to open the gates for them every 40 miles. NoSql pushed the Database inside the developers' world and they love it! Is it good for the industry? Perhaps not, but it might just create millions of programming jobs. After all, somebody has to untangle the convoluted code (not to the fault of developers) left behind. Separation of Database and code, as painful as it might be for developers is a necessity. It establishes checks and balances. According to Google?€?s paper, they have taken those factors into account. ,. Hopefully the trend will continue.,See 
Bootstraps, Permutation Tests, and Sampling Orders of Magnitude Faster Using SAS, Computational Statistics-WIREs, Vol. 5, Issue 5, 391-405.?? Download @ ,While permutation tests and bootstraps have very wide-ranging application, both share a common potential drawback: as data-intensive resampling methods, both can be runtime prohibitive when applied to large or even medium-sized data samples drawn from large datasets.?? The data explosion over the past few decades has made this a common occurrence, and it highlights the increasing need for faster, and more efficient and scalable, permutation test and bootstrap algorithms.??,Seven bootstrap and six permutation test algorithms coded in SAS (the largest privately owned software firm globally) are compared.?? The fastest algorithms (?€?OPDY?€? for the bootstrap, ?€?OPDN?€? for permutation tests) are new, use no modules beyond Base SAS, and achieve speed increases orders of magnitude faster than the relevant ?€?built-in?€? SAS procedures (OPDY is over 200x faster than Proc SurveySelect; OPDN is over 240x faster than Proc SurveySelect, over 350x faster than NPAR1WAY (which crashes on datasets less than a tenth the size OPDN can handle), and over 720x faster than Proc Multtest).?? OPDY also is much faster than hashing, which crashes on datasets smaller ?€? sometimes by orders of magnitude ?€? than OPDY can handle.?? OPDY is easily generalizable to multivariate regression models, and OPDN, which uses an extremely efficient draw-by-draw random-sampling-without-replacement algorithm, can use virtually any permutation statistic, so both have a very wide range of application.?? And the time complexity of both OPDY and OPDN is sub-linear, making them not only the fastest, but also the only truly scalable bootstrap and permutation test algorithms, respectively, in SAS.??,Keywords: ??Bootstrap, Permutation, SAS, Scalable, Hashing, With Replacement, Without Replacement, Sampling,JEL Classifications:?? C12, C13, C14, C15, C63, C88,Mathematics Subject Classifications:?? 62F40, 62G09, 62G10??????,* J.D. Opdyke is Senior Managing Director, DataMineit, LLC, a consultancy specializing in advanced statistical and econometric modeling, risk analytics, and algorithm development for the banking, finance, and consulting sectors.?? J.D. has been a SAS user for over 20 years and routinely writes SAS code faster (often orders of magnitude faster) than SAS Procs (including but not limited to Proc Logistic, Proc MultTest, Proc Summary, Proc NPAR1WAY, Proc Freq, Proc Plan, and Proc SurveySelect).?? He earned his undergraduate degree with honors from Yale University, his graduate degree from Harvard University where he was both a Kennedy Fellow and a Social Policy Research Fellow, and he has completed post-graduate work as an ASP Fellow in the graduate mathematics department at MIT.?? His peer reviewed publications span number theory/combinatorics, robust statistics and high-convexity VaR modeling for regulatory and economic capital estimation, statistical finance, statistical computation, applied econometrics, and hypothesis testing for statistical quality control.?? Most are available upon request from J.D. at ,.,??,SAS and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute Inc. in the USA and other countries.?? Other brand and product names are trademarks of their respective companies.

Over the last several months, as I looked at addressing the business needs across various industries as someone leading a team of?? Data Scientists, the question of domain expertise invariably cropped up.,Attending one meeting with a Pharmaceutical company, I was posed with the question of, "Have you done work in the areas of Rare Signal detection?" In a similar vein, while preparing for a meeting with an Auto finance major, the question was in the area of using Auto telemetry data and deploying it to work on fraud detection in auto-insurance claims.,Multiply the business problems with the numerous industries and the enormity of the challenge becomes apparent. More so since it may not be possible to be a domain expert in every possible industry. Which begs the question, is there a line between domain knowledge and domain expertise? ,While domain knowledge would refer to the appreciation of the industry, its business processes and challenges, domain expertise is expected to be much deeper. It would be addressing meaningfully, through the effective use of technology, a problem or two which a particular industry is grappling with.,Is the expectation then from the prospective customer 'You need to understand my industry language (i.e. domain knowledge)' or is it 'You need the expertise to solve an industry specific problem (i.e. domain expertise)'?,How then does one manage the expectations here? A typical Data Scientist needs to be adept in understanding the business problem, have a good handle of the data on hand, and have a grasp of the algorithms which would aid him/her in the journey of?? discovery, design, deployment and ultimately delivering?? the results .,These would come in various shades across Data Scientists. As the area becomes mainstream at a furious pace, primarily driven by storage and accessibility costs, the need to balance out the three, otherwise known as the ?€?Triangle of Intelligence", namely business (knowledge or expertise), data (content) and algorithm (thinking)?? will possibly decide the difference between resounding success and abysmal failure, while addressing a business problem.,I came across an interesting note by Caitlin Garrett in the ,Caitlin rightly mentions that as a practitioner of Data Science, it is mandatory to have analytical thinking, mathematical/statistical ability, a knack for communicating results to non-data people, and creativity. Her blog however does not make a reference to the business (knowledge or expertise).,Can we surmise then, that the ability to articulate and appreciate the business problem and marrying the expected ways to get this addressed through analytical skills could be a good starting point to provide that confidence to the business user that?? the?? problem at?? hand can be addressed? ,The combination of business acumen and technical skill isn?€?t easy to come by.,David Logan in his interesting blog:, ,has mentioned about the "Purple People", folks who are blessed with the business acumen and the analytical abilities. Purple is the blend of Red (Business acumen) and Blue (Analytical abilities) and we would by now know that this would be a very hard to get profile.?? ,However as this area matures and moves beyond the hype cycle, we may have the luxury of seeing experts who cover both areas well.,The question still remains. The profile as described is rare but the expectations of the prospective customers most of the time is?? "Can you solve the point problem which I have been grappling with for so long?" for which domain expertise and NOT knowledge would be required.,The only way to address this is to borrow the domain expertise from the prospective customer and build that from the base of domain knowledge one is expected to have and move forward. ,But as one is trying to demonstrate credibility to the prospect , this may be easier said than done. I look forward to hearing what my colleagues think about this conundrum. ,Posted by,Chief Business Officer,Technosoft Corporation,email : 

People are more interested in umbrellas when it is raining, bathing suits when it is warm, and jackets when it is cold. The importance of timing and seasonality for these purchases is obvious. However, far more than just scarves and sunscreen are seasonal. Even interest in one of the biggest purchases people make, their house, appears to follow a repeating annual pattern.,We can estimate interest in a particular topic by analyzing search volume data from sites like Google. Because Google rescales the data before releasing it, the individual units of this data aren't particularly informative. However, the relative changes tell an intriguing story.,We'll limit the search data we consider to searches from the United States. This targets the northern hemisphere so that the data is effected by only one season at a time. We have many options for search terms: "House for Sale", "Houses for Sale", "Homes for Sale"; you get the picture. Happily, they all show pretty much the same seasonal pattern. We'll use the term "Homes for Sale", because it has a little larger overall search volume.,The seasonality of people's interest in homes for sale should also be reflected in the prices they are willing to pay for a home. To investigate this connection, let's also look at home price data collected by the US Federal Housing Finance Agency.,With the help of R and some time series analysis, we can decompose the observed search volume into three parts. A smoothed trend line showing long term change, a repetitive seasonality, and some leftover that we can call "random.",There is a clear repeating seasonal pattern in the search volume of "homes for sale." Here is an estimate of one cycle of this pattern.,This estimate shows a high plateau of interest in "homes for sale" in the spring and summer, low interest in November and December, and a rather steep decline in the fall. Let's see if home prices show an analogous seasonality. Using a similar analysis, here is an estimate of one cycle of the seasonality for home prices.,Purchase prices of homes seems to be higher in the summer and lower in the winter.* This corresponds pretty well to the seasonality of interest in homes for sale that we see in the search volume. As one would expect, larger amounts of interest seem to correspond to higher prices. However, the interest seems to come first. The lag in prices might be explained by the time it takes to actually go through the process of finding and buying a house.,As to why there is seasonality in interest and prices, I have no idea. It could have something to do with school schedules. However, my personal just-so-story involves trying to carry a desk across an icy parking lot. My personal desire to move is highly seasonal.,Houses and circumstances are often unique. This broad national data can't tell us the best time to sell a given house. However, all things being equal, we can make a reasonable guess when the probability of fetching a high price will be greatest.,The seasonality we see in interest in homes for sale suggests that the spring is a good time to list a house for sale. The seasonality in home prices suggests that the summer is a good time to sell. Because of the decline of interest in homes for sale in the fall and the lower prices in the winter, one might want to reconsider listing in the fall and selling in the winter.,With these two housing data sets we can also estimate what time of year one has a higher probability of buying at a good price. As a buyer, one would want to go against the cycle; start looking in the fall, buy in the winter. If the prices work out, moving in the winter might even be worth the trouble.
The Balanced Scorecard (BSC) is a new buzz-word that stands for the performance management magic pill. Many books on management praise this business concept and report??an impressive??,??by Fortune 1000 companies. When it comes to practice it appears that only top management in the company knows about the Balanced Scorecard and it is used at a minimum of its potential.,Here are a few words about the Balanced Scorecard. It was introduced in the 90s by Drs. Kaplan and Norton,??and has suffered 3 major upgrades. Now the Balanced Scorecard is a buzz-word for anything from KPIs to a plain strategy map. I personally like it in all of its??reincarnations, especially when it helps real businesses improve their business performance.??,I have never seen??a ,??Balanced Scorecard working in the company, whether it is a Balanced Scorecard for an international retailer like Tesco (the??,??published yearly)??or the BSC for a small business - all professionals can adopt it to their own needs by introducing modifications, so that in the end you cannot call it a Balanced Scorecard, but who cares when it helps.,??,Let's review Balanced Scorecard design process part by part. Take from this story something that you like and then use it in your business today to see if it works for you.,Sometimes the only strategy you have is to try something out and then wait and see what will happen.??Most start-ups follow this idea and call it a strategy.??Don't worry, this is a strategy as well, and a Balanced Scorecard??still??will be useful.??,??,Do you know why there is a word "Balanced" in the name of the concept? It's because it is supposed to give a "balanced" top-view of the business. If you are a VP of Sales, check out your dashboard or strategy map right now.??,That's the key idea about the balanced approach. Don't just take KPIs from industry leaders, which are normally financial ones. Instead develop your strategy first and then add KPIs that will make sense to your business. ??Make sure that there is a 360 degree view of your business, not just a narrow scope on finance.,??,The next step is explaining how you are going to achieve strategic objectives and how will you know if you are on the right track. You will need an action plan and KPI (Key Performance Indicator).,A piece of advice about KPIs: use descriptive names. Ethan Rasiel in "The McKinsey Way" described some best practices for presentation design, such as naming the chart in a more descriptive, action-oriented way. For example, instead of "Slide 1. Sales in China," it is better to use "5% sales growth in China gives us opportunity to sell more of product A." Apply the same idea to a KPI, it will work only when the name of a KPI is descriptive and anyone in your company can understand what it is about.,Another typical pit fall for the KPI is that it is not easy to measure. If it takes few hours to calculate the value of the KPI, believe me, in few months you will stop using it. Consider using several simple KPIs and their combinations instead using one complex KPI.,??,The question is how is the Balanced Scorecard supposed to appear on the desktop of the line-level sales manager? The Balanced Scorecard was invented as a tool for top managers. If only top managers have access to the Balanced Scorecard then it is questionable if it might be useful for line level employees.,The solution to this problem is??,. Your sales department won't be able to use a copy of BSC for top managers. The strategic objectives from the top management's scorecard should be translated to the lower level, giving employees of the department level their part of the business opportunities and problems. Normally, a local version of the Balanced Scorecard is created, and that feeds its values to the top level scorecard.,??,Now ask someone in your department why he or she is doing what he is doing now? If the person fails to give a good answer consider this as a sign that your Balanced Scorecard still doesn't work.,Ideally, each employee should understand what the purpose of the job is and how his or her efforts will help to achieve the ultimate goals of the company.,??,I saw many attempts to link compensations and rewards to KPIs of the Balanced Scorecard.??Unfortunately, it is still not??recommendable??for the general practice because of employees who are smart and like to play with KPIs.,There are also??,??about linking??compensation plans for top managers to BSC's KPIs. In this case a reward scheme is always complex one, it takes in account all aspects of the business and the recommended mix is 70-80% of financial KPIs with 30-20% of non-financial indicators.,??,How will you know that your Balanced Scorecard works for you? I believe that the best thing you might see is the improvement in??,. Employees like it when the task and progress evaluation method is clear.??You will see that people will start to help to achieve??your business goals, instead of just doing their job.??The most important thing is that you will see that the management process becomes more transparent and controllable.??,??,In the beginning of the article I've mentioned Balanced Scorecard to be a buzz-word. In a few years the buzz-word might become less popular, but I believe that key ideas of this business concept are worth trying in any business.,Most Balanced Scorecard implementations that I saw don't follow the classical framework, but these companies take some important ideas that help to engage employees more and to see things from a strategic point of view. Try it for your business and see if you can benefit from it.,??,??is a founder of AKS-Labs, vendor of BSC Designer software and tools for software engineers. His areas of expertise are remote team management,??Balanced Scorecard, KPIs, business performance management, general info-business development and marketing. Aleksey is the author of a number of articles and books on Balanced Scorecard. He runs Balanced Scorecard seminars in the Moscow Business School (MBS).




 , , , , , , , , , , , , , , , , 
I just created a cloud at home. Suppose I start on a project aiming to create a computer game. I purchase 4 servers and some software. After a couple of weeks I realize that in order to complete the project I'll need 6 more servers, but I have run out of money. I decide to write an operating system that connects the 4 servers and creates a single virtual platform-- simplified version of visualization software such as VMware. I won't get into the nuts and bolts, suffice it to say that I can now create as many as 12 virtual servers be it Windows, Linux, or UNIX.?? Note that the underlying hardware hasn't changed; rather, I am making?? efficient use of them. The cloud masks the infrastructure beneath it. I am going to proceed with my example on the premise that I'll remain on private cloud (not sharing resources with other systems in a data center),I complete my video game project and subsequently form a small business offering the game online. A year has gone by and my business is booming but I no longer have the time or the bandwidth to do all the work myself. I weigh the pros and cons, and I decide to move my application to Amazon and let them run it (Software as a Service). Although I have to pay for the service, it is less expensive than running it in-house. Besides, since I don't have to worry about my application, I can focus on marketing. I'll be able to cast a wide net and get more subscribers. My investment might pay dividends later.,A couple of years down the road my online game is phenomenally successful. I have over 1000 new subscribers each day and the daily data generated by my application is over 100 TB. This confirms that I made the right decision by choosing cloud services because my internal IT infrastructure couldn't scale to handle the volume of data. Moreover, the costs could spiral out of control amid the need for rapid expansion and the expenses which accompanies the chaos and the confusion that are inherent in attempting to grow IT in a short period of time. Accommodating 100 TB of data each day will require steady and sustainable resource allocation as well as an elastic infrastructure. Cloud is a suitable solution.,Fast forward to the present time. Bad news! My subscribers are leaving and my business takes a nosedive. I hire a game guru who knows all the tricks of the trade to find out which way the wind is blowing. He tells me that my subscribers are switching to a competitor. My competitor has borrowed ideas from my game and it has produced a smash hit. I go online and take a look at their game. I am at a loss and cannot figure out the underlying cause.,Shortly thereafter, I am told that the competitor has a way of predicting when players might be getting too flustered with the game and call it quits. Their application is designed to utilize nuggets of intelligence to automatically ease the player's frustration by subtly providing clues at the right time when the player is in a tight corner. Although these cases may be few and far between within each session, providing more opportunities to make headway helps turn the corner. That prevents the user from throwing in the towel.,How is that possible? Well, that's the job of ,. The competitor is running SAP HANA's in-memory Predictive Analytics software on IBM Smartcloud. HANA invokes a function in the game's application before the barometer reaches the saturation point. We know that all else being equal the longer a game can keep a player engaged, the less likely that he/she will switch to a different game. Engagement is the key.?? HANA communicates with the game by sending it signals which modifies the game's behavior in real-time. In short, it strikes the right balance between hardship and sail-through.,The following includes the list of companies that in my view will in time dominate the , ,.,Amazon, Google, Microsoft, IBM, Oracle, Pivotal (VMware/EMC),See 
Text (word) analysis and??tokenized text modeling always give a chill air around ears, specially when you are new to machine learning. Thanks to Python and its extended libraries for its warm support around text analytics and machine learning. Scikit-learn is a savior and excellent support in text processing when you also understand some of the concept like "Bag of word", "Clustering" and "vectorization". Vectorization is ??must-to-know technique for all machine leaning learners, text miner and algorithm implementor. I personally consider it as a revolution in the analytical calculations. Read one of my earlier post about ,. Let's look at the implementors of vectorization and try to zero down the process of text analysis.,Fundamentally, before we start any text analysis we need to first tokenize every word in a given text, so we can apply mathematical model on these words. When we actually tokenize the text, it can be??transform??into , model of document classification. This , model is used as a feature to train classifiers. We'll observe in code how the , and , term can be explored and implemented using Scikit-learn. But before that let us explore how to tokenize and bring the text into a Vector shape. So the ,????representation will go with 3 step process:?? tokenizing, counting and finally normalizing the vector.,Scikit's functions and classes are imported via the??sklearn package as follows:,Here we do not have to write a custom code for counting words and representing those??counts as a vector. Scikit's CountVectorizer does the job very efficiently. It also??has a very convenient interface. The parameter min_df determines how CountVectorizer treats words that are not??used frequently (minimum document frequency). If it is set to an integer, all words??occurring less than that value will be dropped. If it is a fraction, all words that occur??less than that fraction of the overall dataset will be dropped. The parameter ,??works in a similar manner. Once we vectorize the posts using feature vector??functionality??we'll have 2 simple vector. We can then simply calculate the Euclidean distance ??between these two vector and calculate the nearest one to identify ,. This is nothing but step towards clustering/classification of , posts.,Hold-on we haven't reached to the phase of implementing clustering algorithms. We need to cautiously move with below steps towards bringing our raw text to a more meaningful ,. We also try to correlate some of the technical terms in , with every steps:,With this process, we'll able to convert??a bunch of noisy text into a concise representation of feature values. Hopefully, you're??familiar??with the term ,. If not, then below explanation will help to build understanding around TF-IDF:,So, continue to the previous code where we have imported CountVectorizer library to vectorize and??tokenized??the text and in below example we are going to compare "Big Data Hype" term with 2 different posts published about "Hype" of "Big Data". To do this we first need to vectorized the posts in question (new post) and then get the third post vectorized using the same method of scikit. Once we have vectors then we can calculate the distance of the new post. This code snippet , covers vectorizing and tokenizing the text.,I would highly recommend the book??"Building machine learning system with python"??on , or on??,Original post:
A new 191-page PDF eBook published by the National Academies of Sciences Press is available, "Frontiers in Massive Data Analysis," and can be downloaded for free (after free website registration):,The first 9 of the 10 chapters offer a comprehensive survey of state-of-the-art big data architectures, machine learning, and analysis techniques.,Chapter 10 really shines as it offers a new framework for evaluating systems and techniques intended to conduct massive data analysis. Called the "seven giants," it's patterned after the "seven dwarfs" of evaluating high-performance computing systems. The seven giants are:,For each of these problem domains, the chapter describes what it is, and what the challenges and examples of notable approaches are.

I'd add an 11th one as well: you check data science sites before you check news sites in the morning!,10. You think ?€? ?€?So much data, so littl?€??€?,9. You know what heteroscedasticity is.,8. Your best pick-up lines all include the word ?€?moneyball.?€?,7. You look at your grocery bill and try to predict what you will buy next.,6. You think data scientists are cool?€? and you have a theorem to prove it.,5. The numbers talk back.,4. Counting sheep keeps you up at night.,3.??You know where all the insights are buried.,2. You have a Kolmogorov-Smirnov hangover.,And?€?,1. You crunch numbers for breakfast.

The term "critical thinking" is often found in job postings.?? Some would argue that this essentially means, "Thinking outside the box."?? Karl Marx, who asserted that labourers represent a class of people, has been described as a critical thinker.?? Regardless of how a person feels about Marx, it goes without saying that the phenomena of social classes is well-established.?? Politicians for instance fight for the support of the "middle class."?? How precisely does such an observation by this sociologist make him a critical thinker??? Why would it be important at all from a business standpoint and especially in relation to data??? Well, organizations greatly depend on the market demand for its products and services.?? It can be tempting to insulate the organization from emerging external conditions.?? For instance, Ford was surprised when the demand for Model Ts started to decline.?? Kodak was reluctant to enter the digital camera market.?? I hesitate to use more recent examples as it might seem like I am being critical about the organizations specifically.?? Actually I am focused on the nature of data.,Using the systems model in its linear format as a convenient template (input - process - output), consider an organization from left (input) to right (process) to right (output) to right (impacts) to right (consequences) to right (entrenchment).?? Here is a possible profile for an organization where terrible things have happened: strategic planning; operational planning; design; development; materials; assurance; production; quality testing; distribution; storage; sales; shipping; delivery; support; product failure; product dissatisfaction; injury and liability; regulatory action; enforcement of remedial standards.?? Of course in practice there are frequently both parallel and serial flows.?? There are also counter flows, which I will discuss a bit later.?? Consider how the nature of the data changes from left to right.?? To the left, the data is perceptual, intangible, and generally disembodied (free of form).?? As one moves towards production, distribution, and sales, physicality and embodiment become more common.?? Going further right, entering areas of impact and consequences, the information tends to indicate how the bodies have been adversely affected.?? "Your product has ruined my business."?? "Your product has harmed our family."?? "Your practices are socially reprehensible."?? In short, the nature of data changes contextually - from perceptual, to productive, and then consequential.,I also want to emphasize the prescriptive nature of data.?? For instance, along the perceptual domains where planning and design occur, there are also formal assertions - e.g. of what constitutes good products, bad products, successes, failures, good employees, and bad employees.?? There is definition of when things become recognized as particular events such as mistakes.?? I mean, most of us probably take it for granted that a mistake is a mistake.?? I have had the experience of contributing to criteria that establish the exact parameters leading up to the recognition of mistakes.?? If the criteria are changed, the underlying phenomena might not count as a mistake.?? Thus, to "count" something within the context of a prescriptive regime is to make use of the metrics of criteria; the outcome is highly reductive data - that is to say, counts and characterizations primarily serve to convey events within the prescriptive context.?? Given this context, it might be difficult for the data to properly reflect social consequences and perhaps, a bit closer to production, even consumer dissatisfaction.?? The metrics of criteria are pervasive, ensuring production irrespective of, say, changing consumer preferences.?? Sales tell us when people are buying less but not necessarily why and how to adapt to the situation.,At the moment, I have not found evidence of "critical data" in the business environment.?? But having started with Marx, I want to point out at this point that being critical usually means focusing on the consequences particularly over long periods of time.?? There is intent.?? There is action.?? There there are consequences.?? Whereas decision-making in an organization is somewhat confined, consequences expand:?? there are immediate outcomes, impacts, and then consequences that reach out like rings from a heavy object dropped into a lake.?? These developments can affect society for long periods of time.?? I am not saying that consequences are necessarily negative.?? However, when politicians start talking about "giving the middle class a raise," they actually mean far-reaching consequences from lengthy interactions between labour and production.?? What does this have to do with producing, say, widgets??? Using prescriptive data from the production of widgets, actually there would appear to be minimal social impact.?? Similarly, when a company "discovers" that people have stopped buying its products - that preferences have turned to different types of products - the underlying truth of the production system might not be apparent.?? The invisible truth goes a bit like this:?? organizations have to find a place within the changing lives of people.?? It is highly instrumental to suggest just the opposite - that people exist to power the business model.?? Companies produce.?? People buy.?? That is the deal, right?,As a closing point to this blog, I also want to share certain dynamics that I have personally noticed.?? In relation to the data, the further the reach to the right, the further back it can be thrown.?? For example, talking about these widgets again, it is quite easy to notice defects near the production process.?? That short reach means that the problem can be thrown back just a bit for correction purposes perhaps to the people near the point of production.?? If one reaches further right, talking about a pattern or history of mistakes, the throw has more energy.?? If the objective is to completely realign thinking near the top, regulators and stakeholders might reach quite far right of the organizational construct by raising human rights violations and social justice issues.?? Many might point out, that sort of stuff in the consequential domain is hardly recognizable as data:?? complaints full of feeling and emotion, court cases, critical discourse, angry boycotts, move to competing products - generally things that appear non-substantive from a production standpoint.?? In summary, I would say that data can insulated from consequences, causing the organizational construct to be deprived of worthwhile information.


Numbers are the fundamental language of business. The bottom-line on the income statement is a??number.??The business plan is expressed specifically as numbers on the operating budget, numbers that may derive largely from??statistical projections of revenues and costs.,Decisions to invest in assets that can accelerate the growth of the business are usually based on numbers that reflect the expected profits and risks of each alternative use of invested funds.,Success or failure of the business or any of its parts typically comes down to numbers.??It has been well established that quality is the key to long-run growth in revenues. However measuring quality is not enough.,Controlling the quality of productions in a manufacturing plant or the quality of customer service by inspecting and measuring goods and customer satisfaction does not eliminate the need for commitment-to-excellence programs, thorough training of production and service personnel, and preventive maintenance of equipment.,Regression analyses and moving average methods of time series analysis are two of the most commonly applied forecasting tools used in business, largely because they are robust yet easy to use.?? Other forecasting techniques range from qualitative approaches, such as juries of expert opinion, and subjective estimates of the sales staff, to highly sophisticate statistical methods of time series analysis, such as the box-Jenkins and spectral analysis method.,They are important in strategic planning to project consumer demographics that can prove critical in your ability to anticipate future consumption patterns.,They are useful in marketing to estimate the effects of changes in pricing policy on sales volume and market share.,No matter how you look at it, effective management is much more than just a matter of working with numbers.,The successful manager relies on common sense and intuition; sensitivity to human factors that defy quantification, and creativity that transcends the numbers.,When the numbers send up a red flag, the successful manager looks beneath them to find out what is going on.,Most successful managers also know that the business cannot thrive without close attention to the numbers, and that tools designed to work with the numbers can be indispensable.,??,Today there are other numbers to consider derived by social media reach, influence, sentiments and tone; all which need to be measured, and put into the call for action operating model. Today?€?s successful manager understands that quantitative methods can be powerful agents for solving the problems of human institutions, and in some cases human beings.,In capitalist economies such as that of the United States, Canada, and the Western European countries, managers of firms are continuously faced with numerous choices.,Managers of firms are assumed to have certain objectives, such as the maximization of profits of shareholder wealth, or the minimization of the cost of producing a given level of output.,Because managers and consumers are pursuing their own private interests and decisions are made in a decentralized manner, rather than by a central planner, a very important question concerning the coordination of economic activities arises.,This long debated problem has been solved today with the explosion of access to real time data that can be derived out of social media, and big-data. Yet there are a few ways to manage this further, one is microeconomics, which seeks to provide a general theory to explain how the quantities and prices of individual commodities are determined.,The development of such a theory will enable us to predict the effects of various events, such as industry deregulation and oil price shocks, on the quantity and price of output.,One of the most important properties of the competitive market equilibrium is that the quantity produced is the socially efficient quantity. The cost of producing the last unit of output just equals consumers?€? marginal willingness to pay for it.,The supply-and-demand framework enables us to analyze or predict the effects of various events and government policy changes on the price and quantity of goods.,Social media sentiment allows us to tie into it, real time consumer opinion. The other way to manage economics, is macroeconomics, this approach is concerned with the issue of how the quantity and price of output of individual firms or industries is determined. In contrast, macroeconomics addresses the determination of the entire economy or aggregate output and price.,The most widely used measure of aggregate output is gross national product (GNP Index); the market value of all final goods and services produced in an economy within a given time period.,One of the aspects of macroeconomics is the fluctuation of the existence of lack thereof, of trade-off between unemployment and inflation.?? However in the 1990s the trade and investment flows between the US and other economies increased, there is another variable of great concern to business people and policymakers: the exchange rate.,This can change substantially by a few percent in a single day.,Then there are interest rates; one important practical implication of the interest rate parity equation is that increases in the US interest rate cause the dollar to appreciate.,US macroeconomic experience of the early 1980s provides a graphic, though somewhat painful, illustration of the link between interest rates and exchange rates.?? On the other hand, under fixed exchange rate regimes, one country usually assumes the role of lead country, and the other countries act as followers.,The ability of two countries to chart their own courses, or to pursue distinct, possibly even contradictory, macroeconomic goals is much lower when those countries attempt to maintain a fixed exchange rate.,This may be a blessing or a curse.,When exchange rates are fixed, and investors expect them to remain so, and interest parity conditions retract, interest rates must be equal in the two countries.,The central bank of the follower country relinquishes its ability to control interest rates and thereby achieve macroeconomic objectives such as reducing unemployment.,In the case of social media, it only showcases the consumer's state of being right now, but it does not help predict market conditions which will influence that consumer, derived from micro or macro-economic concerns as described above. ??Big-data on the other hand, can triangulate all these moving pieces, and with the right analytics, and decision algorythms could solve the biggest question companies always face: What to sell now, and tomorrow to maintain consistent growth, and profits.,????,Before the development of the marketing concept as a management philosophy in the 1950s, marketing was defined essentially as selling.,The traditional view of marketing up to that time was that marketing was responsible for creating demand for what farms; factories, forests, fishing and mines could produce.,Marketing has also been viewed in the past as the function responsible for creating a satisfied customer and for keeping the entire organization focused on the customer.,Marketing is one of the functions that must be performed by the management of any organization, amongst other functions such as manufacturing, finance, purchasing, human resources, sales, R&D and accounting.,The most effective marketing concept considers more carefully how the company can match up its distinctive competence?€?s with a relatively undeserved set of customer needs and offer superior value to those customers.,Market segmentation, market targeting and positioning, ideas that were developed as part of the original marketing concept, become even more important strategically under the new concept.,The value proposition, matching up customer needs and wants with company capabilities, becomes the central communication device both for customers and for all members of the organization.?? This is all possible today due to social media, and big-data.,Focusing attention on the company?€?s strategy for delivery of superior value to customers is crucial.,Superior marketing defined as customer-focused problem solving and the delivery of superior value to customers is a more sustainable source of competitive advantage than product technology per se in the global markets of the 1990s.,Marketing is not a separate management function; rather it is the process of focusing every company activity on the overriding objective of delivering superior value to customers.?? It is more than a philosophy; it is a way of doing business.,In the final analysis, only the customer can decide whether the company has created value and whether it will survive in the hypercompetitive global marketplace; more reason to tune into what consumers want and value, a social media enabled company, ??along with the adoption of big-data can do that very effectively today.,Listening to the consumer is not enough, tracking behaviors is not enough, enabling the consumer to have a voice is not enough, enabling the consumer to be at the helm is not enough.?? Predictive modeling of trends among social groups to identify life values, and developing the business intelligence approach to integrating such data across all business functions to properly develop, market, and retain relationships will be the marriage that?€?s missing today.,Big-Data is nothing without new algorithms needed to match consumers to values. Forget psychographic, demographic, and life stage data - that's not enough. ??Those models work in a verticalized market, and were part of the industrial revolution assembly line methodology. ??What's needed is to identify consumer values, as in what matters to that individual at the core of who they are - only through social media, and big-data tied into a business intelligence engine, can that be accomplished.,??,During the past two decades, the general view of the role of IT in business has shifted significantly from its traditional back office functional focus toward one that fundamentally pervades and influences the core business of an organization.,However, many managers entered the 1990s with a high level of skepticism regarding the actual benefits from IT. The productivity gains from IT investments have been disappointing, hence why the CIO is constantly tasked with cutting costs, vs. driving innovation. This is because IT was primarily expected to enhance operational efficiency (blue-collar) and administrative efficiency (white-collar).,More recently, the dominant business competence appears to be business flexibility with significant competence brought together within a flexible business network of inter-organizational arrangements such as: joint ventures, alliances and business partners, long term contracts, technology licenses and marketing agreements; this is why the cloud is growing so fast, because it allows companies to be nimble in changing the moving parts sort of speak in an efficient way, and with agility.,Undoubtedly, IT functionality will have a more profound impact on businesses than its effect this far, when it begins to focus on market convergence. Nevertheless, successful businesses will not treat IT as either a driver or the magic bullet for providing distinctive strategic advantage, until a marriage can occur.?? This marriage is between the CIO and the CMO, its?€? "imagination tied to sustainability".?? CMOs are often looking for better ways to position the brand?€?s viability in the market, and CIOs are always tasked with making sure the approaches are sustainable, scalable, and won?€?t break the bank.,In many ways these two CXOs have conflicting agendas, until now.?? With big data, it is becoming more realistic for this partnership to work, and CEO?€?s not driving it, are doing their companies a huge injustice. CEOs who don't understand social media, and big data will become extinct within 10-15 years.,The management challenge is to continually adapt the organizational and technological capabilities to be a dynamic alignment with the chosen business vision, and more importantly with the consumer at the helm.,Hence for strategists, IT is not simply a utility like power or telephone, but rather a fundamental source of business scope reconfigurations to redefine the rules of the game through restructured business networks. When efficiency-enhancing business process redesign is pursued, the boundary conditions specified by the current strategy are considered fixed and given.,The most challenging thing is for managers to implement the strategy for business network redesign in a coordinated way. However using IT applications for enhanced coordination and control is both efficient, and effective for carrying out the business processes.,This challenge is difficult because the choices involved in exploiting the present and building the future confront managers with complex trade-off.,The conflict between the demands of the present and the requirements of the future lies at the heart of strategic management for at least three reasons:,1) ??The environment is which tomorrow?€?s success will be earned is likely to be quite different from the environment that confronts the organization today;,2) ??To succeed in the new environment of tomorrow, the organization itself must undergo significant and sometimes radical change;,3) ??Adapting to change in and around the marketplace during a time of significant internal change places and extremely heavy burden on the leaders of any organization.,The choices made in business scope, and competitive postures, are made to achieve purposes or goals. There are two central questions that need to be answered:,1) ??What does the organization want to achieve in the marketplace?,2) ??What returns or rewards does it wish to attain for its various stakeholders, stockholders, employees, customers, suppliers, and the community at large?,It is no accident that some organizations successfully adapt to an environment and initiate new ventures in a number of related product areas while others never seem able to repeat a single success. In short, what takes place within the organization makes a difference.,Winning in the marketplace is heavily influenced by how well the organization makes and executes its choices of where and how to compete.,It has become commonplace to note that one of the hallmarks of today is change. It is our constant. Good management and the management of change is the same thing; how to make sure that what you have in place today will meet the challenges you will face tomorrow.,Flexibility and quickness will count as much as vision and patience.,As economies mote to an information age complex technologies heavily influence by social media, big-data, global markets, intense competitions, and turbulent constant change, managers everywhere are struggling to cope with failing organizations.,Recently, the rise in environmental complexity has accelerated with revolutionary advances in computerization, with the introduction of social computing.,An explosion of knowledge, a unified global economy, the ecological crisis mounting social diversity, and other global trends, are almost certain to blossom into a far more complex world.,Major corporations comprise economic systems that are as large as some national economies, yet most executives and scholars think of them as firms to be managed with centralized controls. Moving resources about like a portfolio of investments, dictating which units should sell which product at which prices and setting financial goals.,Today?€?s and tomorrow?€?s corporations are becoming more and more automated and mobilized. Rather than the traditional organization of permanent employees working 9-5 within the fixed confines of some building, the virtual organization is a changing assembly of temporary alliances among entrepreneurs who work together from anywhere using the worldwide grid of global information networks, and social media.,Interface between organization structure and IT systems has become one of the most crucial issues in management, yet it is so poorly understood that we usually allow the inexorable force of IT to ramble through organizations unguided, with powerful unintended consequences.,It is almost as if robust ivy were growing over a building, destroying its aging mortar and old bricks, and leaving only the vine as a supporting structure.,Business will not be able to use IT effectively without a sound working model of the modern organization, and that model seems to be the market paradigm, or lines of business approach aligned around consumer values.,IT is the major reason for the replacement of hierarchies to enterprise models in today and tomorrow?€?s corporations. The challenge is enormous but the stakes are also enormous.,Managers can best prepare for this coming upheaval now by learning to make a mental shift from hierarchy to enterprise. We are witnessing not only a dramatic increase in the need for leadership but also a transformation in what we call leadership. This turbulence is in turn changing where leadership is practiced.,For example, hierarchies collapsing into flatter pyramids to respond to faster-paced markets are pushing leadership further and further down into the organization. Today?€?s flatter organizations mean that most of us will have to manage across more functions and be sitting on more project teams throughout our management careers.,??,It?€?s clear there are too many moving pieces for all the decisions to sit with either the CIO, or the CMO. Both need to work together to properly leverage all the components that make up today, and tomorrow?€?s ever unpredictable consumer.??,What will such data reveal, once properly analyzed, synthesized, and put into a decision engine?,1) ??First of all how we track data must change, we are too verticalized to properly make sense of it,2) ??Second, the education system was built on the assembly line concept, to support verticalization, this will not serve the needs of the future brand ?€? it?€?s too limiting,3) ??Third, companies who are not engaging with consumers at all ?€? will seize to exit within 10 years, no matter how big they are today,4) ??Branding, marketing, and consumer engagement will need to be realigned around values,Values?,Yes, values, as in understanding that a group of consumers may value connecting with other people, more than a group of consumers who values escaping from reality by being entertained.?? A consumer who values connecting with others will want you to be able to provide him/her all that goes into that??,, as in telecom, transportation, social networking, and events.??,These groups of consumers, who value connecting with others, are being served by multiple brands today, which use multiple strategies, which are all competing for a voice with them.,In order to effectively accomplish this, the first and foremost step will be a marriage between the CMO and CIO, and a closely sponsored relationship from the CEO, with goals that align around the consumer, and providing value to the consumer not by vertical, but by segmentation, specifically values based data segmentation.,In this new world of brands, the relationship of skilled workers will change also, you will see a huge spike in freelancers, and independent contractors doing projects, and a new respect for resources who can provide aligned values, without the concerns of long term contracts, or employment issues related to outdated skills.?? In markets like Europe, organizations that can provide for hire staff will see a huge win due to the restrictive labor laws in those markets.,The biggest winner??? You and me, where our own individual values will be served with some sense of uniformity, and cohesiveness, from possibly one brand that we will build relationships with, because they serve our needs better than those talking at us, as it?€?s done today.??,Tomorrow?€?s brand will not need to market, sell, or advertise, tomorrow?€?s brand will be a data science integrated company who?€?s approach to market will be so refreshingly simple with the consumer at the helm, that cross selling will simply be the norm, not a major effort as it is today. ??Tomorrow's brand will not be limited to verticals, or specialization, but rather provide consumers solutions based on their values. Take a company like AT&T, in this new world, they would be in the telecom, transportation, PR, advertising, and social networking business serving the consumer values of "people who like connecting with other people", and as markets evolve that will take shape into whichever way those consumers wish to connect, perhaps even teleportation (some day). Either way the brand of the future will shape its strategy around data science, vs. trying to fit data science into its current strategy.


Join us for a Webinar on September 10. Space is limited. ,., , , , , 

I am no expert in market analysis of any kind, but my personal Big Data Analytics embedded in my small brains which I inherited from my cave-dwelling ancestors tells me that the industry is anticipating huge profits from Big Data.,Big Data vendors and consulting companies are filling their gaps amid growing competition to provide complete solutions. What is interesting is the global reach ??for finding good fits: ??Yahoo Ztelic-China, Cloudera Myrrix-UK, EMC ScaleIO-Israel, SAP Hybris-Switzerland. Those were the recent accusations by Big Data Mega Stars.,CSC grabbed Infochimps for its analytics capabilities. Cisco brought home Sourcefire to beef up its Cyber Security. Yahoo acquired Zteli ??specializing in social-network data. Cloudera took Myrrix to augment its arsenal with machine learning capacity. Raytheon purchased Visual Analytics Inc for its visualization tools. EMC nabbed ScaleIO to bolster its high performance storage offerings. Tibco bought StreamBase to provide real-time analytics. SAP acquired the ecommerce software maker hybris. Cisco landed Composite Software to boost its ?€?Master Data Management?€?.,See 
I am starting my masters in the field in predictive analytics ??and searching feverishly for scholarships. If anyone is aware of scholarship opportunities for students within the industry of data I would greatly appreciate it!??,Thank you,,Brian


Big data applications in the CRM space is a no brainer -- so why are there so many laggards? Is is because of the investment required? I discuss this in my??,??and which refers back to a CMO.com article I contributed and was published this week.
In 2009 when I completed my first book??,??social media was just starting to take off, and I made some predictions about the impact it would have on business, and the role both big data, and social media would have on the future of brand strategies.??,Last year at a Web Conference in Parma Italy, I boldly stated on a national TV interview that companies who would not embrace social media and big data would no longer be in business inside 20 years. What you are about to read is a re-write of chapters from my book completed in 2009, with some reminders and hopefully innovative ideas that you can use today.,??,Social Media enable digital communication and collaboration which are increasingly spewing over onto the corporate world, but what distinguishes social media is best described not as viral but epidemic. Social media?€?s effect on a brand and corporate reputation can be instantaneous and far-reaching, therefor companies are discovering not only the need to monitor, but to develop metrics ad measure this growing space. The long term impact of social media will change how companies deal with product decisions, inventory alignment to various market demands, market positioning based on demand or lack thereof due to sentiment in particular markets, investments for growth and expansion in specific geographies or markets, suppliers and employee relations too.??,IT?€?s role is changing because all the current systems to manage the day to day business were built mostly to be linear, and cloud computing is enabling more agile scalable systems but are yet to be?? ?€?real time?€? enabled around data coming in as it is happening, vs. after it happened. Companies that don?€?t see the connection between consumer alignment and real time supply chain management will not only have difficulty competing as data science and social data grows, but won?€?t survive against new companies being built around real time market demands.,Anyone who thinks that?€?s a far reaching statement should review the impact of the web on business systems in the past 15 years. Web 2.0 technologies ?€? with their emphasis on collaboration and sharing ?€? continue to accelerate, but the social enabled web is changing the face of business forever. We?€?re reading almost daily, how your customers and prospect are communicating online using some of these tools, and making business decisions more quickly and collaboratively as a result. Why would a company not participate is beyond comprehension, and soon it will mean corporate suicide.,Mainstream media, whose platforms like radio, television and newspapers, are often one way in communication offer up static messages that do not foster immediate feedback from the audience. From the 20,000-foot view, social media enables individuals to connect to each other and then share using easy-to-publish tools, today creating a blog from scratch can take less time than creating a cheese sandwich. The visible discussions that result can include customer experiences, both negative and positive, about any particular brand, because social media truly leverages the Web?€?s massive scale to carry information ?€? fact or opinion ?€? globally and instantly. Data on consumer behaviors, both online, and offline is readily available today - in fact there is too much data which will require??algorithms??to make sense of it, however by tying in the social sphere the data can be synthesized to a clear signal that closely aligns to what consumers value right now.,When you find yourself questioning whether to allow employees access to social networks while at work, think back to the days the same questions came up related to allowing use of the web. It sounds silly to even think of not giving internet access to employees, it will sound even sillier 10 years from now related to social media participation.,Furthermore, with the financial collapse of recent years, and much public disgust with CEOs of large corporations, and distrust in general ?€? companies willing to create more transparency will be rewarded with loyalty and grow exponentially. Before the collapse of recent years, the Edelman Trust Barometer, commissioned by the global public relations firm of the same name, indicated in its 2007 study that only 22 percent of survey respondents in the U.S. trust CEOs, and that percentage is even lower in a combined audience of U.K., French and German respondents (18%).,??,In both the U.S and Europe, ?€?rank-and-file?€? employees are more trusted than CEOs, because of the advent and adoption of social media tools, individuals can easily and quickly find these trusted peers, share opinions and learn about their experiences with products and services purchased from previously detached corporations. This is a disruption to traditional corporate communication practices, to say the least, but it?€?s also a great opportunity for CEOs to connect with customers and change distrust into loyalty.,The corporate world knows social media is having an effect on business but has not yet grasped the full and complete implications, in some cases because there is much to hide, and in other cases because there isn?€?t much to share?€? either way, that?€?s one of the many reasons social media is so important to consumers, they want everything in the open?€? that?€?s how they are, why should companies act any different?,For those companies that are attempting to understand social media, the next logical step is to figure out how to measure those implications. There are many reasons to do so, among:,Listening to the conversation is not as simple as it might seem, whether that listening is part of a preliminary process to understand the environment or an active monitoring program to measure the impact of some event on a corporation?€?s reputation or strategy. In fact merely making the decision to engage in, and measure, consumer-generated media is just the first step, given that the influence of social media itself is often difficult to pin down due to lack of matured processes around it.,??,The promise of all the measurability and metrics that originally attracted a lot of companies to initially go out onto the Web is only becoming possible today because of social media. To some extent, the Web does live up to its promise of providing metrics.,Advertisers know exactly how many people open pages with their ads and how many people clicked on them, however they still know little about how many people saw their billboards or newspaper ads ?€? and even less about whether anyone cared ?€? social media is enabling better control and understanding.,For corporations serious about tracking their ?€?return on influence?€? ?€? that is, not just standard ?€?ROI,?€? but a broader more long-term, long-lasting return ?€? in social media and the blogosphere, being able to measure, track and compare the results is a requirement for determining next steps and strategy. For organizations that want to join the conversation using social media tools, and realize that traditional Web analytics alone are not sufficient, the next question is crucial: Which attributes should be measured?,??,Before going further it would be helpful to define two key terms in this discussion, metrics and measurement.?? While they often seem interchangeable and are often misused, they are linked. Metrics, as it does in other areas of business, simply refers to the terms or parameters themselves that an organization wishes to understand. In social media circles, the word ?€?attributes?€? is used as well, but the key concept is the determination that any organization needs to make about which factors are important to understand.,Measurement, on the other hand, is the next step ?€? the process of taking these metrics or attributes and determining how they are impacting (or are being impacted by) the actions of an organization, such as a marketing campaign (internal) or a crisis (external).,It is one thing to measure with a great deal of confidence a market leader?€?s changing market share from year to year, but quite another to determine the extent of damage to a company?€?s reputation after its CEO uses company funds to purchase solid gold shower curtain rings for his home. What are the parameters, and what are the units of measure?,In some cases, blog posts, podcasts and video are designed to entice a given individual to learn more, tell others, or perhaps embed a video player on his or her own blog posts. The ability to measure such an activity is necessary to know the answers to some key questions:,The questions to measure can go on and on, but they always need to be centered on the consumer, because consumers used to be people who consumed stuff, people who crave good experiences; these people are now empowered to influence.,??,Web 3.0 is about enabling semantic data from social media, to make sense of the big data not just have access to it, but at the core it?€?s about people actively participating and driving the data. Most people don?€?t like unpleasant experiences, and since today people and brands have influence together and people?€?s voices are the new media, brands relying on old media to build consumer relationships, are on the Titanic to say it bluntly.,Mining the social media conversation is not enough. Brands need to focus on listening in order to facilitate conversations between them and customers, employees, suppliers, and investors.?? It used to be that having a solid website with a good SEM/SEO strategy was the way you engaged outside the firewall.?? This is no longer the case, in fact everything that has ever been done, before social media, has simply been an internal view of the outside world?€?a limited perception of reality often supplemented by very expensive research, advisors, and so on to make sense of what ?€?customers might be thinking?€?.?? We all know that everything comes down to leveraging relationships and networks, but why have companies not done so with the customer? It was not possible before social media, and big data. The closest to engaging with the customer, was a survey, or when the customer called in with an issue, and most of these were not designed to foster honest conversations, they were mostly manipulated questions to drive a better score for the company.?? Today the power of social media networks offer up raw and open feedback, real valuable data, and brands can make truly intelligent decisions around.?? But how do you measure the success of social enabling your businesses?,What you want to measure are two basic elements of social media, and these are the core metrics:,??,~??The Economist, March 11, 2006.,The key is to be proactive, to listen and to participate in the conversation in a real way as a means to bringing the conversation closer to the brand. If a company is not participating, the conversation is taking place away from the brand, and this can lead to tragic consequences or fantastic results?€? either way it?€?s a missed opportunity. It?€?s not just about monitoring, but it is about knowing when to converge in the conversation.,Brands want to be on the lookout for problems that can be solved proactively, and identify opportunities. Here are a few tips:,Everyone has a voice, and that voice can be translated into data that can drive innovation, improve time to market, inventory management, sales and marketing decisions, collaboration between employees, suppliers, clients, investors and companies.,People?€?s needs are basically still the same; technology doesn?€?t make people social, but it influences behaviors.?? How has behavior been influenced?,People stopped consuming media, and started producing it. People are sharing ideas openly and giving away secrets and in doing so they are building their personal brands, and becoming self-made celebrities, while celebrities become more like regular people.?? People?€?s social capital expands rapidly today with less degrees of separation. People are streaming their lives, and experience life collectively. The global village is real.,??,People?€?s communications are becoming effortless and instantaneous and nearly anyone can influence, especially within their niche; the social web thrives. Social economics have equity, connections are currency, and currency gets spent finding support in networks for: philanthropy, jobs, public relations, endorsements, complaints, services, products, ideas, political campaigns and Super Bowl advertising strategies.,People used to rely on institutions for information and answers, but now people rely on the individual institution, on each other?€?s individual network as human filters from the noise out on the cloud.??We are moving toward a people powered web, the web now has a soul and it?€?s uniting us all.,Within technology powered by social media, the human factor is not optional. Insights into behaviors lead to innovations, and innovations help people. Social Media humanizes big companies by giving the CEO a channel that talks directly to the customer and listens all at the same time?€? People value doing business with people, not brands.,??,Creating more social or collaborative relationships is the next wave of thinking in customer-centric business management that has evolved over the past two decades. In the 1990s, Customer Relationship Management (CRM) was mostly about managing customer information; a very company-centric view of the relationship, but there?€?s more to a customer relationship than data management and process automation.,The limitations of CRM?€?s internal orientation and technology-obsession led to the rise of Customer Experience Management (CEM), which is focused on designing and delivering loyalty-building experiences with integrated Business Intelligence (BI). CEM is not just about using technology. All these systems are necessary, but lack an important element in today's social world; they lack logic, emotions, and reasons behind intent.,We are entering the era of Customer Collaboration Management (CCM), which is about engaging with customers, employees, investors and suppliers in a real dialog, real time. CCM changes the role of IT from Information Technology (IT) to (IM) Innovation Management, with people at the helm, not technology; more importantly it is the customer who ultimately should drive IT?€?s strategy and the overall company?€?s direction?€? if the company is smart enough to listen and participate in the conversation.,If you?€?re still not convinced that the revolution in social customer service is coming, take a close look at your customers?€? complete service experience; you may find that it starts in the social web outside your official service processes and firewalls?€?when your customer searches on Google or interacts on a Facebook group. The service experience could also end outside your organization?€?when the customer Tweets about you; its common practice today to post a question on the social sphere for help on a number of topics, vs. picking up the phone and calling a call center representative. When was the last time you picked up the phone to find out how to increase the battery life of your iPhone? ??You can search for that online and find a number of answers and solutions; customer service is being provided among consumers - it's happening outside your company walls.,The next big opportunity for executives will be engaging with customers, employees, suppliers, and investors in the world of social media, and transforming the function of Information Technology to customer centric Innovation Management. Your Customers are Already There!,Individuals are becoming brands, ?€? but what?€?s really happening is that individuals want brands to be on equal footing, consumers want brands to become a part of their network, and to talk with them, not at them, and they want their voices heard and acted upon.?? Social media powered business is not just a new frontier of how business gets?€? done, it is the oldest form of business dynamics, based on people talking to people. The right social media strategy will enable companies to harness the power of the relationships with customers, employees, suppliers and investors to drive market innovation, improve profits and reduce costs ?€? REAL TIME.,It?€?s a business to consumer and business to business highly collaborative live network to help drive:,??,When you move IT into the customer value chain, IT becomes more about innovation, less about managing information, IT becomes the intelligence behind the execution; based on the feedback loop that social media and bigdata creates into back office environment already in place.,The next generation of IT services, powered by social media, will be dynamic and agile; capable of responding to the needs of a company real time, allowing the company to save money by not introducing products or services of no value, and allowing the company to make money by providing value to customers based on real time feedback and demand.,The dynamics of markets ever changing consumer behavior is adding complexity and costs to IT departments; IT simply can?€?t do it all anymore within its traditional mode of operation. Decisions have to be made faster ?€? from years/months to days/minutes, none of the environments in place today, not even cloud computing can enable that without adding consumer convergence data. You need access to what the people have to say real time.,??,Technology has become democratized and it?€?s harder to control user behaviors when they can just go to the cloud for answers or alternative solutions. Social computing is changing user expectations as consumers and employees expect faster responses, more transparency and action on complaints right away. The experience needs to be consistent inside the brand?€?s firewall, as they are outside. Without participation in social media a brand simply cannot deliver this and this will result in frustrated customers.?? It?€?s inevitable, social media powered business is not a nice to have, it is a must have to not only thrive in the future, but survive.,Encourage everyone in the company to converge; connecting with real and potential customers is not just one department's job, it?€?s the role of the entire company. Establish a social media policy, and launch a communication and training plan for at least six months, or until it has become second nature.,Dialogue about what you are hearing and learning, but remember it is not a spectator sport. Share the feedback internally and to the networks, and capture the data to improve the flow of information within the company. You can gain real time access to make inventory, supply chain, customer care, sales and marketing, and product decisions. You can gain insight into managing employee relations better, improving investor sentiment, and creating improved transparency with suppliers and vendors, which allows them to plan in advance to best support your needs.?? The more you know about your audience, the less it will cost you to keep them happy, the more your audience knows you and have a relationship with you on equal footing, the more profitable you can become.,A social media powered business 3.0 is all about people.?? Create a program that empowers your key people to become the brand, to become the voice of the business, engage on a one to one basis, the key is to align with the values of the consumer, the more you know what the consumer values, the better you can follow them.?? That last statement is the key here.,So much effort is put into getting consumers to follow and like you as a brand ?€? the key is to flip this strategy around, as in following the consumer, and having the technology platforms with the intelligence to respond to the consumer?€?s needs, real time.?? The company of the future won?€?t need to drive innovation; it will discover it from those who truly drive it ?€? people who crave good experiences.,Today IT manages information; in the future IT will manage the innovation coming in from consumers, and integrate it into appropriate response systems to help the company be aligned real time with market demands.,??
For one : ,Another use of bigdata in ionospheric calculation:,Be kind to visit my hut on divine communication, a validation to brain waves, esp, and all the weird things!
 , , predictive models into our repeatable processes to yield desired outcomes. An automated risk reduction system based on real-time data received from the sensors in a factory would be a good example of its use case., , may receive the data from hospitals and doctor offices in real-time and Data Analytics Software that sits on the top of Big Data computer system could generate actionable items that can give the Government the agility it needs in times of crises.,See 
Data analysis echo system has grown all the way from SQL's to NoSQL and from Excel analysis to Visualization. Today, we are in scarceness of the resources to process??,??(You better understand what i mean by??,) kind of data that is coming to enterprise. Data goes through profiling, formatting, munging or cleansing, pruning, transformation steps to analytics and predictive modeling. Interestingly, there is no one tool proved to be an effective solution to run all these operations { Don't forget the cost factor here :) }. ??Things become challenging when we mature from aggregated/summarized analysis to Data mining, mathematical modeling, statistical modeling and predictive modeling. Pinch of complication will be added by??Agile implementation.,Enterprises have to work out the solution: "Which help to build the data analysis (rather analytics) to go in Agile way to all complex data structure in either of the way of SQL or NoSQL, and in support of data mining activities" .,So, let's look at the??,??(I would prefer to call Python libraries as echo system) and how it can cover up enterprise's a*s for data analysis.,: functional object orientated programming language, most importantly super easy to learn. Any home grown programmer with less or minor knowledge on programming fundamentals can start anytime on python programming. ??Python has rich library framework. Even the old guy can dare to??,.??Following data structure and functions can be explored for implementing various mathematical algorithms like recommendation engine, collobrative filtering, K-means, Clustering and Support Vector Machine.,:,Let's begin with sourcing data, bringing into dataset format and shaping mechanism.,{??,: Data loading, Cleansing, Summarization, Joining, Time Series Analysis },: Data analysis covered up in python libraries. It has most of the things which you look out to run quick analysis. Data Frames, Join, Merge, Group By are the in-builds which are available to run SQL like analysis on the data coming in CSV files (read CSV function). To install Pandas you need to have NumPy installed first.,{??,: Data Array, Vectorization, matrix and Linear algebra operations i.e. mathematical modeling },: Rich set of functions for array, matrix and Vector operations. Indexing, Slicing and Stacking are??prominent functionality of NumPy.,{??,:????Mean, variance, skewness, kurtosis },: SciPy to run scientific analysis on the data. However,??statistics functions can be located in the sub-package??,{??,: Graph, histograms, power spectra, bar charts, errorcharts, scatterplots },:??2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.,Moreover, how can we second python support to Big data Analytics and Machine Learning. ??Below resources can be utilize for various big data applications:,Having said that, Python is capable enough to give a way out to implement data analysis algorithms and hence to build your own??,.,Watch out this space for implementations of various algorithms in Python under one umbrella i.e. ??,.

 , 

"Understanding how data converts from data to information to intelligence can be seen in the flow across the databases in an enterprise. What is a data warehouse? How do we get Business Intelligence? In this video I explain the high level view of how large corporations like Dell evolve their data to the front-end BI applications.,#iwork4Dell,
,??"



??
 , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 
"Debugging Hadoop jobs can be a huge pain. ??The cycle time is slow, and error messages are often uninformative --- especially if you're using Hadoop streaming, or working on EMR., ,
I once found myself trying to debug a job that took a full six hours to fail. ??It took more than a week -- a whole week! -- to find and fix the problem. ??Of course, I was doing other things at the same time, but the need to constantly check up on the status of the job was a huge drain on my energy and productivity. ??It was a Very Bad Week.,
,
"
It was not easy to select a few out of many Open Source projects. My objective was to choose the ones that fit Big Data?€?s needs most. What has changed in the world of Open Source is that the big players have become stakeholders; IBM?€?s alliance with Cloud Foundry, Microsoft providing a development platform for Hadoop, Dell?€?s Open Stack-Powered Cloud Solution, VMware and EMC partnering on Cloud, Oracle releasing its NoSql database as Open Source.,?€?If you can?€?t beat them, join them?€?. ,.,Hortonworks,Cloud Foundry -- By VMware,OpenStack -- Worldwide participation and well-known companies,fusion-io -- Not open source, but very supportive of Open Source projects; Flash-aware applications.,REEF -- Microsoft's Hadoop development platform,Lingual -- By Concurrent,Pattern -- By Concurrent,Python -- Awesome programming language,Mahout -- Machine learning programming language,Impala -- Cloudera,R -- MVP among statistical tools,Storm -- Stream processing by Twitter,LucidWorks -- Search, based on Apache Solr,Giraph -- Graph processing by Facebook,MongoDB, Cassandra, Hbase,MySql -- Belongs to Oracle,MariaDB -- Partnered with SkySql,PostgreSQL -- Object Relational Database,TokuDB -- Improves RDBMS performance,Red Hat -- The defacto OS for Hadoop Servers,Talend,Pentaho,Jaspersoft

Recent revelations regarding the National Security Agency's (NSA) extensive data interception and monitoring practices (aka PRISM) have brought a branch of "Big Data's" research into the broader public light. The basic premise of such work is that computer??algorithms can study vast quantities of digitized communication interactions to identify potential activities and persons of interest for national security purposes.???? ??,A few days ago we wondered what could be found by applying such Big Data monitoring of communications to track the conversational impact of the NSA story on broader discussions about Big Data. This brief technical note highlights some of our most basic findings.,Our communication analytics work is usually directed at??,. However, in this case we??applied??some of the most basic components of our analytics tools towards public social media conversations,??provides real-time access to public tweets. The two graphs below show the relative volume of communications collected for conversations related to Big Data (between 30 May - 12 July 2013). The rate of communications collected was highest on weekdays and during working hours (in US timezones). This observation is consistent with the expectation that most of the Big Data conversation takes place in the context of business activities and the companies applying Big Data to business problems. Conversations on other topics (e.g., leisure activities) would have a very different profile.
Good day folks,,I'm conducting a data analytics project for a small supermarket and the IT guy sent me a file (.db0 extension) of their transaction details, but I'm unable to read it. ??Apparently it comes from the??IBM 4690 Supermarket Application controller. ??I've done internet searches and nothing helpful comes up. ??The guy that sent it to me said he has never had to convert or read it into CSV or any other file format, so he can't provide any assistance. ??Can anyone help? ??I've attached an example of the file.,Urgent Help needed.,BERT,Here is an example of the file! ,>>
Source: ,.
As the internet of things explodes, the building management industry is ripe for disruption from sensor data.,Please click??,??for details about each use case



Contact us: daniela@infinityce.com.br,Phone: 55 (11) 3132-7250,??
These are not ordinary times for CIO's. The news about Netflix's Big Data success, and Big Data's involvement in predicting the last election results might have created a buzz among the IT workers, but it was the NSA's surveillance news that established a beachhead for Big Data in the public arena. Business managers might become nervous about what the competition is doing with Data Analytics and they may turn to CIO's for help. As more and more success stories bubble up to the surface, it will become harder and harder for CIO's to sit on the fence. Although this is not the first time that a groundbreaking technology is putting pressure on IT to deliver, it is probably the biggest rock to lift to date. Reasons being, the dizzying array of choices available, the complexity of cost benefit analysis, fear of data security problems, lack of skillful workforce, perplexities of on-premise implementations, reservations about the maturity of the technology, expensive services from BI Megavendors, and Cloud's slow file transfer.,The CIO?€?s may not give into pressure, lest they find themselves caught in a ,. On the other hand, waiting too long could have far-reaching negative ramifications for the company. The IT industry has gone through a similar experience while implementing Data Warehousing. According to Gartner, an approximate sixty percent of Data Warehousing projects have failed. The decisions made by the CIO will have a direct impact on IT. IT managers who maintain large volumes of data are already grappling with day-to-day problems that are caused by the sheer size of the data they store. The last thing they need is diving into a Big Data project. Therefore, ,. What do we currently have? Where do we want to go? Do we adopt a Cloud solution, Data Analytics as a service, or on-premise implementation? What about a hybrid Sql/Nosql environment? May be an Open Source strategy? Which Hadoop distribution? Are the BI Megavendors affordable? Can we benefit from integrating social media data? Why not employ a NewDB?,If you want to go towards Predictive Analytics, you need to consider Cloud services. Cloud has a few advantages not the least of witch is scalability. The CIA signed a $600 million contract with Amazon a few months ago. Although most BI Megavendors offer Cloud Services, Amazon and Google are your best choices since they both have been in the Cloud business longer than others and they offer Analytics. Amazon hosts some of the BI Megavendors' applications as well. However, bear in mind that Cloud data transfer is slow. Another consideration is Cloud security. The startup Attunity is an Israeli company that has made significant progress in speeding up the Cloud data transfer. Another startup, Seculert is a strong player in Cloud security space.,You might want to consider a private Cloud solution and many companies are already building their applications on off-premise data centers. The startup Engine Yard provides a Cloud application development platform.,Open source is probably the least expensive but the most difficult to implement. Working with Hadoop is not easy and there is a lack of seasoned Hadoop technology workers. Nevertheless, once you have installed your commodity servers, and Hadoop is running and it is stable, the hard part is over and you can focus on installing your Analytics software. Hortonworks Hadoop is 100% open source. Jaspersoft and Pentaho offer Open Source Data Analytics software. You can save a ton of money if you succeed!,To ease the pain and speed up your implementation, you can go with a Hadoop distribution that has builtin API's to work with other software that you might need. MapR and Cloudera Hadoop distributions fall in that category. IBM and Intel also have their own Hadoop distributions. Pivotal HD (Hadoop) is an alternative; the startup is the child of EMC and VMware. Xplenty startup has a different approach and??claims that it has a cost effective solution that eliminates the barriers to entry, making Hadoop accessible to everyone.,As for selecting a Predictive??Analytics tool, there are several choices: Tibco Spotfire, KXEN, StatSoft, SAS, Action, Amazon Analytics, Google BigQuery, Oracle, SAP,?? IBM, Microsoft, Microstrategy, and Teradata. There are also the following startups.??Tableau, Splunk, Datameer, LucidWorks, SpaceCurve, ParAccel, DataGravity, Altryx, SiSense, and ClearStoryData.,Finally, there are some startups that are doing interesting work.,Hadapt, Continuuity, and Cloudera (Impala), Concurrent (Lingual)??provide tools for Hadoop developers,Apache Sqoop connects Hadoop to traditional RDBMS,Skytree, Ayasdi, Splunk, Automated Insights, and Oxdata have machine learning tools,Import Io extracts data from Websites,VOLTDB and ParStream: Distributed, massively parallel processing columnar database based on a shared nothing architecture running on commodity servers,Rainstor NewDB on Hadoop utilizing Sql,Ayata offers??Prescriptive Analytics?? -- one of few,FoundationDB is working on a NoSql product that is ACID compliant-- best of both worlds (Sql, NoSql) type of effort,FairCom's c-TreeACE Database allows for processing both Relational and NoSql type records,Cloudant offers a distributed database as a service (DBaaS),ScaleBase dynamically scales out relational databases (MySql).,DeepDB provides simultaneous transactions and Analytics in the same data set, in real-time.,Platfora reads data directly from Hadoop without a middle layer and generates analytics reports.,DataStax delivers consulting, support and training for Cassandra.,MySql Refugees unite under SkySQL,*Note: Although Hadoop adoption is on the rise, its performance with real-time Analytics is not there yet.

Marketing is being redefined by the changing habits of consumers, by the limitless choice for placing ads and the access to customers through a variety of channels. As a result, many companies are changing their plan of action and the allocation of their budgets for the different channels, including the web, campaigns, including mobile, social media, etc.,Recent studies which have analyzed the course of online shoppers, indicate that traditional marketing campaigns on the web as the purchase of keywords to search engines, the proliferation of banners, the use of affiliate networks, and emailing campaigns, remain the preferred solutions to boost online sales, while investments in social media are still very low, used to buzz, and rarely associated with revenue generation.,The e-retailers problem is to determine the best approaches to attract web buyers all year long and in particular during certain key periods, such as Christmas and New Year for example. For that they have to understand what motivates consumers to buy? How do they find the offers? Do they research or not, thoroughly or not? What role social buzz has in the buying process?,Consumers, who buy online, have necessarily been influenced (a bit, much, not at all) by e-retailers marketing efforts on the web. While organic traffic (client direct accesses to the site) is the most profitable trade channel, it is also the least reliable because most buyers do not know most of the time the website address. Most buyers go through various stages before reaching the site and complete a transaction. In fact they have in 80% of cases researched, react to a promotional email, seen an ad or an article, or use a comparator. While the majority of buyers are touched by a marketing program before making their purchase, 45% are exposed to at least two marketing activities before finalizing their transaction. This shows the interest for retailers to be not satisfied with knowing only the last action taken by the client (the famous last click).,While marketers use a variety of tools, and develop a large number of promotional and communication, studies show that investments in search engines and e-mails are more effective in generating business turnover. Depending on the industry, these two actions alone generate 40 to 60% of sales. The impact of other activities, affiliation, banners ... etc.., is more difficult to assess, because it is often very early in the process of influence, and few studies have the necessary historical data to highlight their importance. As most retailers rely on analysis of the last click, the evaluation of the contributions of different types of actions is totally distorted.,??,More and more companies want to better understand the entire course of their clients before they purchase, as well as the actual impact of their marketing efforts. This level of knowledge required to understand more fully the relationship that the customer has had with the brand, and beyond simple statistics related to its last click. For that it is necessary to establish an evaluation of the multiple actions that affected the customer before he buys, based on a proper management and analysis of relevant Big Data.,To go further on this subject, how to assign to different marketing actions a fair contribution share to the results, you can usefully consult the following presentation, it does not aim to be exhaustive, but might give you some perspective:,??
??, Research&ItemID=11538&ResearchTitle=The Evolution of Manufacturing,A penny for my thought: An increasing complex value chain, local deliver, fast, hopefully cheap, question is, would it be tailored after individual needs (not only want) ???,...leads to the next thought: What kind of suit future men wear? The need to change and to adapt in the??era of??technology digital transformations, nano technology among others. Balance pros and cons ,(don't forget the graphene and its risks ,OKAY!,How does????3D printing, ??atomically precise manufacturing (APM)??and nano tech has any thing to do with Big Data? Here is my horizontal thinking: Operational. How human adapt to digital and tech rapid change?,The latest??book??Radical Abundance written??by , advocates for??the atomically precise manufacturing, He urges engineers??stay with "common sense solutions to engineering problems, by crafting solutions that are both consistent and efficient in the realms of science and engineering and yield truly useful products, not prototypes destined to languish almost forgotten in the technological research centers that?? conceived of them" (Amazon review),.,See more 
When so many thought leaders gather in one place, there is sure to be some highly insightful conversations and more importantly, some solid conclusions reached about the current climate of HR. ??Industry experts recently gathered in Chicago for the SHRM Annual Conference to discuss HR technology trends, among other topics. ??Their conclusions about the top 10 HR technology trends for 2014 give us a clear glimpse into what HR leaders are talking about and will continue to talk about during the coming year.,Here are a couple of those 10 top technology trends that we feel especially close to and why.,Of course it is! ??And we'll even go further to say that in some cases the cloud is??,secure than the on-site, on premise solutions that companies currently have. ??But not everyone is as willing to except this reality. ??Which is why our Pando solutions can be hosted in the cloud or live on-premise behind a firewall.??,Decision making is a collaborative process. ??How we collaborate with our colleagues is changing and advancing every day. ??If the tools we use to collaborate aren't really built for collaboration, then the decision making process is slow and unwieldy. Pando not only allows for collaboration, but encourages it! ??Facebook, LinkedIn and Twitter are popular because people are social beings. ??We have an innate need to share with others and converse. ??Check out??,??to see how Pando encourages collaboration through sharing and conversation.??,Correction, HR needs to collaborate with other departments to see what's working and to help achieve??,??goals.,No department within a company can just worry about themselves. ??If sales doesn't care what marketing does, and marketing doesn't care what HR does and HR doesn't care what operations does, then what kind of business decisions can really be made? Some think that they can just float along with an analytics tool that allows them to see their HR data and that's it. ??We believe in integrating HR data with sales, marketing, operations and other data so everyone can see the whole picture.,Some people feel productive sitting in front of an excel spreadsheet and spending hours typing in calculations and entering data. But that is not productive. ??Productive is making data-based business decisions that move the company forward. ??When you have a tool, like??,, that automatically pulls in the data from disparate systems, you can get real work done.??,And that's why we make sure all our solutions are available on any mobile platform. ??There's just something so cool about analyzing metrics and data on an iPad. And that's why we love it!,HR leaders need to do this and do it soon. ??But they also have to be careful about rushing in and not fully understanding which metrics they should be using and what data questions they should be asking. ??It's more than just purchasing analytics tools and plugging in. ??Companies need partners in ventures like this. ??Otherwise, it can turn out to be a complete waste of time. ??,??,We strongly believe in having international standards for HR. ??Our work on the ANSI HR Standards committee is devoted to establishing professional HR standards. ??To further the adoption, comparison and benchmarking capabilities of the ANSI Cost Per Hire standard we have created a central data-set where companies can submit their demographics and CPH data and compare with other companies. ??Check it out??,??When HR leaders can prove how their processes directly affect business outcomes, they'll get the recognition they deserve and be able to contribute to the important data-based business decisions.,For the full list of the Top 10 HR Technology Trends of 2014,??
Unless you are in voice recognition, machine learning, or artificial intelligence already Natural Language Processing (NLP) is not a mainstream technology many of us are familiar with.,See my post How to build better systems ?€? What is Natural Language Processing? at:, Applied Enterprise Architecture: , or, ,.




In this series I reveal and explain rules of intelligence contained within grammar, that can be utilized to unleash intelligence in software. These rules are extremely simple, but still undiscovered by scientists.,To be able to explain making assumptions, we need to understand (the difference with) drawing conclusions first:,?€? Conclusions are drawn straight ahead - top-down - like in: Given "," and ",". Generated conclusion: "," (see Part 2);,?€? Assumptions however, are made bottom-up, by which the validity is not secured. So, we should add a word that expresses the uncertainty of the assumption.,Under certain conditions, two types of assumptions can be generated autonomously:,?€? Given: ",",A rule of intelligence contained within grammar: Conjunction "," can be used to dissected the above sentence into two singular definitions:,?€? ",",?€? ",",Now take one of the dissected sentences and another sentence:,?€? ",";,?€? "," or ",",The common word in both sentences is ",". At this point we can draw a , (see Part 2) from the original sentence "," and the sentence, ",". We can now conclude - actually assume - that ",".,Below we will learn how the uncertainty of the assumption is expressed.,(More detailed conditions: ,),?€? Given: "," and ",";,The first sentence has a prior term of the possessive verb ",". And its conjunction "," - another rule of intelligence - indicates that both specifications "," and "," are necessary to validate the definition ",".,In the second sentence, we can read a relational specification: ",". However, the reverse relationship from Pete to John is unknown, in which case we may not make any conclusions about Pete, although we would like to claim that:,?€? ",",?€? ",",Both we call a ,.,(More detailed conditions: ,),?€? The "assumption distance" can be calculated by counting the number of bottom-up steps in the assumption;,?€? And we can translate this number of steps into a word with increasing uncertainty, like: 1 step = ",", 2 steps = ",", 3 steps = ",", etc;,?€? When involved knowledge has changed, the uncertainty level of an assumption should be recalculated., Above we have made the assumption "," from "," and ",". Jumping from the bottom word "," to upper word "," is only one step. So, we can express the uncertainty of this one step as: ","., An assumption might include another assumption. Then the level of uncertainty of the included assumption will add to the number of steps of the assumption to be generated. Known: "," and from the previous example: ",". Now an assumption can be generated autonomously: ",", whereas the word "," expresses the extra assumption step, a double level of uncertainty.,To download the open source implementation: 

New Genres of Captcha have arrived which truly unleash the power of Captchas. These Captchas enables powerful Advertising via 3D and Video Captchas. Also, Captchas can be used for analytics and sentiment Analysis in wide range of domains such as Advertisements, News, Art, Places, Movies and Sports. Measure the effectiveness of your ads via Sentiment Analysis to predict your advertising results! All that effort on branding, messages, and color schemes can finally be validated! These Captchas are highly enjoyable and impart useful knowledge and entertainment and serve as a complete entertainment pack, it is a must for a publisher who can also monetize their website with Advertisements which come in form of Captchas. These Captchas, won't only serve security purpose but increase traffic and interest to your content.,Checkout -
??,Generally, the larger your population size is, the most robust your model will be.??,What data preparation mistakes or advice would you add?

Hello, everybody,,?? ?? I am excited to find this nice website. I'm an engineer focusing on BigData ( Hadoop/Hive/HBase/Spark/Shark etc) . I found a lot of resource in this website. I'd like to learn more and share with others here.
??
 Overall, our ad hoc approach to the data scientist experience seemed to go over well, but there?€?s always room for improvement. I?€?m interested in any ideas or experiences you guys have might regarding young data scientists, and would love to hear about them in the comments below. In the meantime, if you?€?ve had a sneaking suspicion about a certain neighbor around a full moon, or just want to have a little fun, I?€?d recommend trying out your own version of the game.??,For more information, feel free to ,. I have downloads of the index card clues available for those interested in doing this themselves.
xcitement around Big Data has significantly increased the demand for data scientists. This hot position has been hailed as the "Sexiest Job of the 21st Century" by the Harvard Business Review.,When we started in the advanced analytics space we didn't use the term 'data scientist.' We were more focused on solving problems than defining a job description. Our initial teams??, 
 
using large datasets from PJM, R , javascipt , we have provided,a categorical data representation of the bidding pattern followed by wind power generators.,please provide feedback.,thanx,Parag Patil,co-founder,KW Energy??
Would be interested to know who and where the major research into applying Big Data to FI marketing and risk management is talking place. Thanks.


M2M + Big Data Analytics = Unlocking Blue Ocean Opportunities at the intersect,When human beings got connected, it unlocked a whole new set of possibilities and companies like facebook, linkedin etc came up with solutions which had never been there before.,What are some real life examples of M2M ?,M2M Big Data Analytics demystified at??
Recently, ,??at StatSlice Systems wrote an intriguing and thought-provoking whitepaper on information singularity??and the principles of the analytics rock star.??,Successful analytics professionals should follow a set of guiding principles which are very important and often missed by traditional methodologies. In the whitepaper, you'll learn about these principles that can make you an analytics rock star in your organization and increase your worth in the marketplace.What makes an analytics system successful? As a professional in this field, you must be able to answer this question thoroughly in order to form the foundation of a good implementation. ,??good enough for you; 




Bob Muenchen's very useful work on this topic, , sent me back to some 2012 work we did at , on the subject of what employers are looking for in the way of analytics skills.?? First, our main results:,1.?? Our numbers showed a much less SAS-dominant world: ??1.92 SAS jobs for every R job.?? Bob had found the ratio to be 11.06 for every R job.,2.?? Like Bob, we found that R was gaining relative to SAS - in our May scrape the SAS/R ratio was 2.44, while in December 2012 it had declined to 1.92.,About our methodology:?? Rather than using Indeed.com, a consolidator, we did web scrapes customized to Dice, Amazon, CareerBuilder, and Monster.??Indeed.com includes most of those sources, but web scrapes are difficult to configure for indeed.com, since it is a consolidator and the job postings can take different formats, depending on the original source.?? The Amazon scrape is for jobs at Amazon itself (Amazon does not run a job market); it was included not to be comprehensive but to have a representative entry from a large tech-oriented firm that does its own hiring.?? We searched for jobs posted in the previous 7 days and used the search terms:?? analytics or forecasting or statistician or "data mining" or PMML.?? PMML was an experimental inclusion and turned out not to be important.,The web scraper then returned all job listings it could find with those criteria.?? The search was not perfect, simply because some of the job listing websites were not in a format that fit the scraper.?? We then searched through each job listing for a lengthy set of keywords.?? SAS, R, SPSS, etc.??were included.?? Through trial and error several processing rules were developed to identify "R" correctly.??,Here are the results from December - each percentage is relative to SAS.?? In other words, the first line indicates that the term R appeared in 52% as many posting as SAS did.,We plan to run another scrape shortly - comments welcome!



There are two major perspectives of Data Science we can look at:,- Consumer/User Perspective,- Data Scientist?€?s Perspective,This article explores these two areas to ponder upon in little more depth.,- Consumer/User Perspective (User will not like ?€?noise?€?),A single user/consumer might need some analysis to either start some study or make some decisions. This single user might be a CIO/CTO or perhaps we can also say that this single user is a group of decision makers to decide some aspects. It means that they have some perspective object decision to make for some reason. This approach is also called Top-Down view of report generated for this user (group of users) by Data Scientist(s).,The report provided to this user is usually geared to the variables this user have provided and related variables on the parametric relationship are also provided to make the report as concrete as possible. For example, if a marketing executive is looking at a report provided, that report should have all aspects of market trends, where the comparison products are heading, who are the buyers, whether the trend is B2B or B2B2C or B2C or C2C. All of these parameters play important role for the user of this typical report to make a right choice among his/her decisions to launch their product. The graphical representation of trends play vital role for the decision makers.,Trend estimation [1] is a statistical technique to aid interpretation of data. When a series of measurements of a process are treated as a time series, trend estimation can be used to make and justify statements about tendencies in the data, by relating the measurements to the times at which they occurred. By using trend estimation it is possible to construct a model which is independent of anything known about the nature of the process of an incompletely understood system (for example, physical, economic, or other system). This model can then be used to describe the behaviour of the observed data.,Note:,Users like their reports ?€?noise?€? free, it is the most important factor one should know before presenting any analytical report to the user. Consider yourself walk in a room full of people speaking at one time (all of them), will you understand a single thing, no way. The question is why not, because there is noise factor. As a human, we need one person to speak first and other to listen and than other to respond back to carry on with the talk to make some meaningful conversation.,- Data Scientist?€?s Perspective,Kindly stay tuned for this portion of the article?€?
Nowadays we hear a lot about big data, cloud, or the big data analytics on cloud. One of the underlying needs here??is the data storage. It is stored as 0?€?s and 1?€?s in some datacenters. It isn?€?t cheap to maintain all of this data. Looking at the heat generated by these datacenters, it isn?€?t even environment friendly.,Can this data be really in the clouds? (The real clouds formed in the sky) :),In India we have heard about Rishi?€?s who used to sit under a tree and meditate for years and then attain enlightenment. I am sure that most of us would have heard about the geniuses connecting to some super consciousness. At moments these Rishi?€?s or the geniuses that we know of; would have made a quantum jump and their brain got filled with lot of information/data that they weren?€?t aware of previously.,Did Rishi?€?s and geniuses have a brain which had the capability to connect to data that is suspended in the universe?,If some of our brains can connect to the suspended data, would it be really possible to create a Technology that can pump information back and forth into the universe? I like to call it ?€?The suspended data grid?€?
I have two databases on Microsoft SQL Server (daily business activities performed) and also on peachtree and on orange human resources software. I want to build a data warehouse with this databases available. My questions are:,i. Where can I integrate all these databases together,ii. After I integrate, how can I mine this data?,iii. What is the best software to use and mine this data?,iv. Can combining all these databases produce insight for making business decisions after mining?,Please, really need your help and suggestions.,Thank you.

??,Please visit - ,??,Limited Slots !!!!,??,??
Relationships are built on trust and mutual understanding ?€? if a company wants customers to trust them with their??,, the company must foster that trust. Data governance and sensitivity about your customers?€? data must be enforced internally and communicated externally ?€? to your customers.,When aggregating sales and marketing??,, marketers, who want to leverage this information, should also be involved in the setting standards for data governance. For instance, insurance marketing data is often shared with the brokers and agents through various reports and tools. To prevent any external, non-discretionary use of customer data,,it is important to implement best practice for sharing. The table below provides a broad stroke of which data categories and suggests distribution policies for groups of users of the data.,Depending on the relationship the insurance carriers have with their brokers, the marketing team can predetermine buckets of CRM, Sensitive, or Predictive Scoring data so the brokers, for example, can analyze prospect and customers to determine correct coverage levels for the customer. Consider this scenario: for good reason, marketers will not share all of their predictive scoring and segmentation data with captive agents. However, if the marketing team has identified a group of customers who are categorized as frequently late in premium payments, that group of customers can be shared with captive agents so they can take action. Applicable actions in this situation can include sending friendly email reminders or providing payment options before the person is late. In other situations, marketers, with access to information that is not shared with agents or brokers, may be enabled to manually grant temporary, limited access to agents/brokers.,Of course all data should be considered sensitive and must be governed by a specialized team, as any misuse of sensitive data would likely be detrimental to the overall brand.?? However, if markets are part of the data governance teams, they will be both more responsible data users and data monitors.

Want to know what we think about how Big Data is going to affect HR and recruiting?,Aspen Advisors Founder, Andrew Gadomski shares his thoughts on Big Data. ??,This interview is the first of a monthly series of podcasts on Big Data and HR. ??Big Data Trends will be available on Recruiting Trends, TotalPicture Radio, iTunes, Stitcher Radio and SoundCloud. ??Andrew will host the series, inviting practitioners actively using big data in their organizations to share best practices.,Here's an exert from the podcast, "

With all the hype around Big Data, we?€?ve become extremely proficient at collecting data ?€? be it from enterprise systems, documents, social interactions, or e-mail and collaboration services. The expanding smorgasbord of data collection points are turning increasingly portable and personal, including mobile phones and wearable sensors, resulting in a data mining gold rush that will soon have companies and organizations accruing Yottabytes (10^24) of data.,To put things into perspective, 1 Exabyte (10^18) of data is created on the internet daily, amounting to roughly the equivalent of data in 250 million DVDs. Humankind produces in two days the same amount of data it took from the dawn of civilization until 2003 to generate, and as the Internet of Things become a reality and more physical objects become connected to the internet, we will enter the Brontobyte (10^27) Era.,So it?€?s all dandy that we?€?re getting better and better at sucking up every trace of potential information out there, but what do we do with these mountains of data? Move over Age of Content, enter the Age of Context.,??,Big Data has limited value if not paired with its younger and more intelligent sibling, Context. When looking at unstructured data, for instance, we may encounter the number ?€?31?€? and have no idea what that number means, whether it is the number of days in the month, the amount of dollars a stock increased over the past week, or the number of items sold today. Naked number ?€?31?€? could mean anything, without the layers of context that explain who stated the data, what type of data is it, when and where it was stated, what else was going on in the world when this data was stated, and so forth. Clearly, data and knowledge are not the same thing.,Take the example of a company that has invested heavily in business intelligence (BI) software that organizes internal data. In an increasingly connected world, this company has not leveraged its data to its potential. Why not? Because the company?€?s internal data is isolated from the rest of the data universe including news, social media, blogs, and other relevant sources. What if you could join the dots between all these data sources and surface hidden connections?,Using an application that, unlike the average BI product, pulls in information from the entire data universe would help the company answer questions like ?€?Why did our sales plummet last month??€? as opposed to just ?€?What happened to our sales figures last month??€?,The company could further question, ?€?Did our slip in sales have anything to do with the recent elections and uncertainty??€? and ?€?How can we make sure our sales do not slip next time there is a shift in the political landscape??€? Identifying potential causality is key in spotting patterns that enable prediction.,For organizations and businesses to survive today, they have to contextualize their data. Just as a doctor diagnosing a patient with diabetes based on body temperature alone is incorrect, so is making business decisions derived from data out of context. A doctor needs to know about the patient?€?s age, lifestyle, diet, weight, family history, and more in order to make a probable and guarded diagnosis and prognosis. Contextualization is crucial in transforming senseless data into real information ?€? information that can be used as actionable insights that enable intelligent corporate decision-making.,At the end of the day, our overworked minds want to be spoon-fed insights. We want key signals and tightly packaged summaries of relevant, intriguing information that will arm us with knowledge and augment our intelligence.,But how do we extract real intelligence from data?,??,Besides cross-referencing internal data with a plethora of other sources, we need algorithms to boil off the noise and extract the signals, or real human meaning, from the data. What do we mean? Let?€?s say you have a million tweets from New York City on the eve of the U.S. elections and rather than read them all, you want to know quickly how people are feeling based on these tweets.,To do this you must apply complex algorithms derived from machine learning, computational linguistics, and natural language processing to harvest the key words and corresponding emotions from the tweets. Then you get the key signals: Are people feeling anxious? Hopeful? Confident? Fearful? This is precisely what we do here at Augify: Paving the way for the future of understanding by surfacing signals from the noise in our cloud-based, algorithm-packed product. We wrap the signals in a dashboard of slickly designed, color-coded gauges and visualizations that enhance your understanding of key insights.,??,So it?€?s great that applications out there can gather and analyze data, detect human-based meaning from it, and visualize it all, but any application is limiting itself if it is only useful once you open the application and enter a query. We wanted to go beyond this. That?€?s why we decided to get contextual on you and increase our technology?€?s reach. We go beyond contextual data by developing contextual applications ?€? we wanted to put data to good use by applying it to real-life situations in our daily lives.,The Don of ubiquitous computing, Mark Weiser, stated in 1991 that ?€?the most profound technologies are those that disappear. They weave themselves into the fabric of our everyday life until they are indistinguishable from it,?€? much like the telephone or electricity. They have seeped into our surroundings, playing an integral role in our everyday lives.,Modern day examples include adaptive technologies such as Google Now, which tracks your online behavior and uses this data to predict the information that you will need, such as local traffic or weather updates. Similarly, ?€?learning thermostat?€? Nest self-adjusts your home?€?s temperature based on your activity, saving energy usage and bills. Pervasive technologies mean they are everywhere and nowhere at the same time ?€? an invisible layer listening to your actions in order to be more helpful.,Along the same vein, we believe in pervasive Augmented Intelligence, meaning that you can reap the benefits of Augify?€?s signal-detecting technology anywhere you go in the connected world. So even when you are working outside of the Augify application screen, be it writing an e-mail or reading a website, Augify?€?s algorithms will work silently and invisibly in the background, culling key insights from the text you are reading at that very moment and feeding them to you in a pop-up window.,??,Imagine being able to save time reading long text by getting a synopsis of the key themes, topics, emotions, people, and entities in a news article? By seeing the emotions, sentiments, intentions, and credibility in someone?€?s Twitter feed, and by spotting the key influencers in their network? What if, as you typed an e-mail to your boss about a new enterprise software, you received pop-up recommendations of credible reference articles related to said software?,In other words, what if you could detect the important signals in any web content in real-time? With Augify?€?s contextual application, this is truly possible, enabling you to augment your intelligence anywhere your online activity takes you.,??,The Age of Context demands that contextual data be applied to everyday situations in useful ways. How do we make use of this data? Since we?€?ve gotten good at collecting data, now it?€?s all about putting it into context and making sense out of it ?€? mining for the nuggets of insights that answer the ?€?So What??€? question. Data is meaningless and even cumbersome without context ?€? the key holistic and interpretive lens through which data is filtered and turned into real information.
Spark and Spark Streaming are two components of the "Berkeley Data Analytics Stack" (BDAS).?? Spark Streaming is one of the few open source options available for "Real-time Big Data".?? See my slides and 35-minute presentation from last night, which was part of Global Big Data Week:,??

The evolution of culture, people, and functionality within corporate America, combined with Internet driven commerce and social media that provides an abundance of data to drive strategy and measure results, has given marketers a chance to ?€?brightly market?€? to customers and lay claim for their success. ??The fact that research is demonstrating a strong merge between IT & marketing, with marketing taking the leading role in not surprising.,??,??,Cited:??


.
Recent posts by Larry Wasserman and Vincent Granville are fantastic additions to the data science conversation.

:,??,??,??,??,??

How can one differentiate the First Order and Second order effects during Data Analysis?
Here are some highlights from a few thought leaders and practitioners in the big data space that I found valuable enough to share ?€? to me, they're hitting on similar themes so worth summarizing.
 
Full Tilt Poker is the second largest online poker room; it belongs to Rational FT which is a computer services company that develops software, business computer maintenance and performs operations based digital marketing. , ??,Two fundamental points of running a poker room online are the integrity of the site and the trust of customers. Players must be completely sure that they are protected against fraud. Negative publicity on this subject has a direct and immediate impact on customer loyalty, the ability to attract new customers and sales. The difficulty here is the volume of data to process, the complexity of the analysis, the difficulty of detecting all forms of fraud and costs. Depending on the activity, the rapid increase in data volumes and analytical requirements may involve increased infrastructure costs and lower overall margin.,Frauds are taking place in the online poker rooms and are not detected due to low analytical capabilities. The magnitude of the problem is often unknown. It is difficult or impossible to identify many types of fraud, collusion between players, the practice of multi-accounts, the bypass of the system to prevent unwanted disconnects, the use of assistive software, the physical player changing (so changing the playing style), using a Trojan horse to see the hole cards of opponents, etc.. ...,??,Fraud detection process requires a lot of data and time; we must work per iteration of question and answer with very long cycles to complete. In the end it is difficult to track down sophisticated frauds and many fraudulent actions are not identified. However, it is possible to identify players who often play together and are likely to be conniving, who have already made disconnections, who have played more than 500 hands, etc.,Using an analytical Aster Data platform, Full Tilt Poker had significantly improved its fraud tracking, particularly in being able to take into account in the analyzes detailed hands played. Earlier treatment of these data were long, transfer from one server to another, decompression via batch processing and final analysis took seven days to analyze a week of frauds at the rate of 1200 hands per second. With the Aster Data solution, transfers are removed, the treatments are massively parallelized on a set of low-cost servers and analysis of seven days of data takes 15 minutes at a rate of 140,000 hands per second.,More, the Aster Data analytic platform is able to run sophisticated algorithms and detect fraudulent actions in very large sets of events. Full Tilt Poker has observed that queries that took 90 ' take now 90''. Queries that usually do send back any result are now producing valuable responses in less than 1 hour. These capabilities have helped to provide access to data, via SQL queries, to all the analysts of the society.,??,In summary, the Aster Big Data solution identifies forms of fraud that were previously undetected, significantly reduce the analysis cycle time, shows extraordinary performance especially for the treatment of hands, allows deeper analysis of sophisticated fraud, better meets money laundering tracking requirements, provides players with new customization services benefits, improves internal reporting on activity poker room , while reducing the TCO of the system (hardware, maintenance, SAN, personnel, network).
It's not that I am necessarily trying to coin a new "V" for big data, but rather highlight the importance of the scientific method and ultimate goal of big data, to bring real value.,Wired Magazine was kind enough to pick up my article and you can read it here: ??
.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.

??

Check out our ,.
This is the first of a series of blogs in which I will reveal and explain rules of intelligence contained within grammar, that can be utilized to unleash intelligence in software. These rules are extremely simple, but still undiscovered by scientists., , , Systems generating questions already exists. However, their questions are useless, because the original sentence - from which the question is derived - already holds to the answer., , A rule of intelligence contained within grammar:, Conjunction "," indicates a choice. It can be utilized to generate questions about gaps in the knowledge, by substitution of both sentences, and converting the result to a question:, , ?€? Given: "," and ",";, ?€? Autonomously generated question: ","., , , Of course, we assume John is a man. But if we feed the system a list of names with their gender, I wouldn't consider it intelligent, because it is not autonomous, while generating a question is autonomous behavior., , , By storing the knowledge, the system can find out if the question generated, is a gap in the knowledge, nor not., , To download the open source implementation: 

I am working for a clothing retailing company,we are launching a BI system.Could you give some advices about that,thanks!

Data is being collected everywhere around us. This data is analyzed and used to impact B2C and B2B markets in traditional and nontraditional sectors in ways we never imagined. Indeed Analytics has proven itself to be an essential tool for any organization, influencing every decision and impacting every stakeholder.,But what exactly does this big Analytics boom mean for India specifically? Well the outlook is extremely bright and here is just why:,The analytics industry worldwide is expected to grow annually at 31.7%, nearly 7 times more than the entire ICT market. In India, the industry is slated to grow 60 times in the next ten years. Surprisingly though, there is just not enough capacity yet to fulfill the demand for analytics talent. New research??by the McKinsey Global Institute forecasts a 50 to 60 % gap between the supply and demand of people with deep analytical talent.??In India for every 100 data scientists needed, there are only 10.,On the bright side, India has the potential to fill this gap. We are a country with thousands of technical colleges. Quality Education is our strength. By 2020 more than a 130 million will join the global workforce.?? India?€?s talent pool - well educated, smart and skilled individuals. They will be in high demand in the analytics industry because of their process expertise and English language proficiency. What big data analytics firms need are people with an optimal mix of quantitative, business and strategic thinking skills. With education institutes waking up to the reality of big data analytics, more and more of them are beginning to equip their students with these intrinsic analytics skills. A generation of visulaizers- not only will we be able narrow the analytics talent gap in India, but our talent will be imported elsewhere narrowing the gap worldwide.,India?€?s with it?€?s strong entrepreneurial culture and numerous start up success stories, backed up by our biggest success of all, our growth in the IT services industry over the last 20 years, gives analytics investors a firm sense of security. Already India already delivers to more than 70% of the global data analytics services market. Their share is expected to grow $ 1.2 billion by 2015. A significant part of the pie that is being invested into big data analytics has gone to firms in India. All this primarily because investors have seen the success of Indian leadership and witnessed the growth of the IT sector. They are willing to bet once again on a country that is a shining example world over.


 

The Big Data wave makes us take a new step in the information age in which we live. Data & information have taken a central place in all our activities; even war is concerned (information warfare, electronic warfare, for example). In the information age, the power of ideas plays a more and more prominent role, but as the principle of skepticism is largely widespread, our era is also the era of suspicion and mistrust. Normally there are always data that are transformed into information, which requires particular expertise, knowledge, ethics, and exposes us to many twists: secret (information enclosed) rumor (zero degree of information), propaganda, disinformation or deception.,For example, advertising is a form of propaganda that is accepted in our society, it is used to develop a "pleading" for a product or a brand. It is promoting a point of view in order to convince, but also to block the brain function; it is typically the role of suggestive advertising to prevent the brain works analytically and critically. Advertising, as all actions propaganda has individual or crowd conditioning goals. Propaganda tries to seduce, to rally, to convert the undecided camp, to strengthen supporter?€?s convictions, to shake the skeptics, but in some cases (excluding advertising) to demoralize, terrorize opponents. It should be noted here that terrorism is an act of propaganda which bases its power on the violence of the message.,Disinformation deliberately tries to distort the understanding of a situation. Disinformation provides false information to guide in the wrong direction. Often it presents in a biased way a fact, provides biased information, and covers their true information in an avalanche of comments and details. Disinformation champions are the politicians. The politically correct expressions are a form of disinformation, and the double talk another form. The big question here is whether disinformation could be morally honest, as it serves respectable objectives??? ,In handling information, the gold medal goes to deception actions. There are different forms of deception, gentle (hoaxes, April fool ?€?) and hard (example: commercial scams). The militaries are experts of this type of manipulation, already highly recommended five centuries before Christ by Sun Tzu who devotes a chapter of his famous book to deception. The references for this type of military action are many: Trojan horse, Horace and Curiace, EMS Dispatch, and during World War II, Pearl Harbor, Barbarosa, Mincemeat and Overlord. Deception actions systematically use stratagem, combination of false clues, luring sequence of actions, the use of defectors, spies and traitors in order to drive the enemy who distrusts to the wrong decision and to fatal behavior.,The world of Big Data is presented on a sympathetic angle, as for example the possibility to improve market understanding, but no doubt it will be a good source for powering many acts of propaganda, disinformation and deception.,To go further on the topic you can download the following presentation:?? ,??
.
Our company recently implemented a BI Reporting solution intended for a retail company with both, an offline retail chain and an online web-shop. This was a Demo project, so during the planning phase our team decided to use two different platforms, including different back-end and front-end software.,In general, we planned to compare the free and non-free software available for similar BI Reporting implementation.,We chose the following platforms for comparison:,These two platforms were chosen due to the following reasons:,To make this competition fair and comparable we used the same data sets on both DBs and were implementing the same reports with the same UI (or at least the most similar UI) on both front-end sides.,For data synchronization between the two DBs we used MS SSIS for ETL implementation (data was generated on Oracle and then moved to MS SQL).,After the decision of which two platforms are to be compared was made, the project was started.,??,At the very beginning the software engineers successfully installed all MS software (Server + SSIS + SSRS), connected all the parts, built an ETL, etc. In general it took about one work week (5 work days) for all these activities. First positive conclusion about MS was that they can have really good documentation with no issues occurring during setup.,For testing, we used MS SQL Server 2008 R2.,The second platform was not as easy to setup. Although Oracle was installed pretty fast and without tweaks, dealing with SpagoBI was pretty tough. It took four work weeks for complete installation, tuning up, and removing all issues and side effects.,The following were the most cumbersome issues:,Here we used Oracle XE 11g and SpagoBI 3.6.0.,SpagoBI installation and tuning would be much easier with manual in hands and support service from SpagoWorld, but we agreed to make the experiment fair and not pay any money in this case.,Although our team is already experienced with SpagoBI installation and next time it will take no more than one week to complete, for new users it may be challenging at first.,Nevertheless, all troubles were eventually eliminated and the team was ready to start on the next task.,??,The next stage of our project was all about data preparation. Here is what we did:,All generated data was carefully validated and a lot of improvements were made to make the data as close to real retail data as possible. We were using this ?€?fake?€? data to secure the real retail data and customer information.,The same data set was used to test both platforms.,??,This project phase was most enjoyable. In this phase a real ?€?battle?€? between SpagoBI and SSRS took place.,We implemented a set of different reports intended for company executives and the marketing department in order to track company performance using benchmarks and KPIs, review sales performance, analyze effectiveness of marketing campaigns, and analyze customer segments and their behavior.,In this task, both MS SSRS and SpagoBI proved to be good tools, providing good graphic capabilities with some differences in implementation of the UI.,Below you will find the most remarkable pros and cons of each tool.,Pros:,Cons:,Pros:,Cons:,Pros:,Cons:,??,In general, SSRS is a really nice and usable reporting solution.,We observed the following:,Pros:,Cons:,??,We?€?ve successfully developed BI Reporting, with both solutions currently live and fully functional. Some users of these reports prefer SSRS , others prefer SpagoBI reports. The users keep arguing which solution is better and what functionality is more convenient in the 1st and in the 2nd case.,In general, both solutions are usable and BI Reporting systems can be built using either of the platforms. The following statements provide deeper insight into our findings:,??,??,Do you have experience in using different BI software? ??Please, provide your comments if you do. We would very much appreciate if you share your own experience of using BI tools, e.g. which tools you were using and for which tasks, how do you rate these tools, and what difficulties you observed during development.
Some interesting insights have come out of the??,??recently held in London. ??But I?€?d be lying if I said that what was revealed was any shock to us.,Much of the talk of the conference was about the importance of predictive analysis as it applies to HR and data. People are finally starting to warm up to this idea. ??However, ??accessing your data is only half of the battle. ??The part that still seems to be left out of most conversations is the importance of knowing what to measure. ??And that?€?s where??,??comes in. ??Analyzing data is far more complicated than using Excel to build a chart or graph. ??Matthew Hanwell, a former Director of HR at Nokia, put it best when he explained that we need to think of data as more of an MRI scan. ??Its multidimensional.,Any company, large or small, may have a superb IT team and a superb HR team. ??That IT team may be experts in the installation and operation of a data analysis system, but that doesn?€?t mean they know what HR data is important to analyze. ??And that HR team can be trained until the cows come home on to use the new data system, but if they aren?€?t analyzing the right data and using that data to make the right decisions, then what good is it? ??This is exactly why when we introduce our business intelligence platform to clients, we stress that??,??is an important part of it?€?s use.,Google, that highly successful company that you may have heard of, seems to be on the ball for how they use data in the HR space. ??Not only are they tracking data to analyze performance, but they?€?re using that analysis to implement training and development based on what their research has revealed. ??This method of linking data and change sounds a lot like our??,. ??We determine what needs analysis, make the data visible, then use that data to implement change through our toolkits and assessments.,Reading through the many quotes and insights from the HR Tech Europe conference, it seems that Nick Holly, of the Henley Business School Centre for HR Excellence, put it best, ?€?Big data provides the greatest opportunity HR has had in years to become relevant by using the data to provide insights that make a difference to the business?€?it is also the biggest single threat to individuals i nthe function because a lot of HR people base their success on gut feeling and intuition and relationships, and proper data analytics can that irrelevant.?€?

??,Every minute, 48 hours of video are uploaded onto Youtube. 204 million e-mail messages are sent and 600 new websites generated. 600,000 pieces of content are shared on Facebook, and more than 100,000 tweets are sent. And that does not even begin to scratch the surface of data generation, which spans to sensors, medical records, corporate databases, and more.,As we record and generate a growing amount of data every millisecond, we also need to be able to understand this data just as quickly. From monitoring traffic to tracking epidemic spreads to trading stocks, time is of the essence. A few seconds?€? delay in understanding information could cost not only funds, but also lives.,Though ?€?Big Data?€? has been recently deemed an overhyped buzz word, it?€?s not going to go away any time soon. Information overload is a phenomenon and challenge we face now, and will inevitably continue to face, perhaps with increased severity, over the next decades. In fact, large-scale data analytics, predictive modeling, and visualization are increasingly crucial in order for companies in both high-tech and mainstream fields to survive. Big data capabilities are a need, not a want today.,?€?Big Data?€? is a broad term that encompasses a variety of angles. There are complex challenges within ?€?Big Data?€? that must be prioritized and addressed ?€? such as ?€?Fast Data?€? and ?€?Smart Data.?€?,?€?Smart Data?€? means information that actually makes sense. It is the difference between seeing a long list of numbers referring to weekly sales vs. identifying the peaks and troughs in sales volume over time. Algorithms turn meaningless numbers into actionable insights. Smart data is data from which signals and patterns have been extracted by intelligent algorithms. Collecting large amounts of statistics and numbers bring little benefit if there is no layer of added intelligence.,By ?€?Fast Data?€? we?€?re talking about as-it-happens information enabling real-time decision-making. A PR firm needs to know how people are talking about their clients?€? brands in real-time in order to mitigate bad messages by nipping them in the bud. A few minutes too late and viral messages might be uncontainable. A retail company needs to know how their latest collection is selling as soon as it is released. Public health workers need to understand disease outbreaks in the moment so they can take action to curb the spread. A bank needs to stay abreast of geo-political and socio-economic situations to make the best investment decisions with a global-macro strategy. A logistics company needs to know how a public disaster or road diversion is affecting transport infrastructure so that they can react accordingly. The list goes on, but one thing is clear: Fast Data is crucial for modern enterprises, and businesses are now catching onto the real need for such data capabilities.,Fast data means real-time information, or the ability to gain insights from data as it is generated. It?€?s literally as things happen. Why is streaming data so hot at the moment? Because time-to-insight is increasingly critical and often plays a large role in smart, informed decision making.,In addition to the obvious business edge that a company gains from having exclusive knowledge to information about the present or even future, streaming data also comes with an infrastructure advantage.,With big data comes technical aspects to address, one of which is the costly and complex issue of data storage. But data storage is only required in cases where the data must be archived historically. More recently, as more and more real-time data is recorded with the onset of sensors, mobile phones, and social media platforms, on-the-fly streaming analysis is sufficient, and storing all of that data is unnecessary.,Historical data is useful for retroactive pattern detection, however there are many cases in which in-the-moment data analyses are more useful. Examples include quality control detection in manufacturing plants, weather monitoring, the spread of epidemics, traffic control, and more. You need to act based on information coming in by the second. Re-directing traffic around a new construction project or a large storm requires that you know the current traffic and weather situation, for example, rendering last week?€?s information useless.,When the kind of data you are interested in does not require archiving, or only selective archiving, then it does not make sense to accommodate for data storage infrastructure that would store all the data historically.,Imagine that you wanted to listen for negative tweets about Justin Bieber. You would either store historical tweets about the pop star, or analyze streaming tweets about him. Recording the entire history of Twitter just for this purpose would cost tens of thousands of dollars in server cost, not to mention physical RAM requirements to process the algorithms through this massive store of information.,It is crucial to know what kind of data you have and what you want to analyze from it in order to pick a flexible data analytics solution to suite your needs. Sometimes data needs to be analyzed from the stream, not stored. Do we need such massive cloud infrastructure when we do not need persistent data? Perhaps we need more non-persistent data infrastructures that allow for data that does not to be stored eternally.,Data?€?s Time-To-Live (TTL) can be set so that it expires after a specific length of time, taking the burden off your data storage capabilities. For example, sales data on your company from two years ago might be irrelevant to predicting sales for your company today. And that irrelevant, outdated data should be laid to rest in a timely manner. As compulsive hoarding is unnecessary and often a hindrance to people?€?s lifestyles, so is mindless data storage.,Aside from determining data life cycles, it is also important to think about how the data should be processed. Let?€?s look at the options for data processing, and the type of data appropriate for each.,Batch processing: Batch processing means that a series of non-interactive jobs are executed by the computer all at once. When referring to batch processing for data analysis, this means that you have to manually feed the data to the computer and then issue a series of commands that the computer then executes all at once. There is no interaction with the computer while the tasks are being performed. If you have a large amount of data to analyze, for instance, you can order the tasks in the evening and the computer will analyze the data overnight, delivering the results to you the following morning. The results of the data analysis are static and will not change if the original data sets change ?€? that is unless a whole new series of commands for analysis are issued to the computer. An example is the way all credit card bills are processed by the credit card company at the end of each month.,Real-time data analytics: With real-time data analysis, you get updated results every time you query something. You get answers in near real-time with the most updated data up to the moment the query was sent out. Similar to batch processing, real-time analytics require that you send a ?€?query?€? command to the computer, but the task is executed much more quickly, and the data store is automatically updated as new data comes in.,Streaming analytics: Unlike batch and real-time analyses, stream analytics means the computer automatically updates results about the data analysis as new pieces of data flow into the system. Every time a new piece of information is added, the signals are updated to account for this new data. Streaming analytics automatically provides as-it-occurs signals from incoming data without the need to manually query for anything.,How can we process large amounts of real-time data in a seamless, secure, and reliable way?,One way to ensure reliability and reduce cost is with distributed computing. Instead of running algorithms on one machine, we run an algorithm across 30 to 50 machines. This distributes the processing power required and reduces the stress on each.,Fault-tolerant computing ensures that in a distributed network, should any of the computers fail, another computer will take over the botched computer?€?s job seamlessly and automatically. This guarantees that every piece of data is processed and analyzed, and that no information gets lost even in the case of a network or hardware break down.,In an age when time to insight is critical across diverse industries, we need to cut time to insight down from weeks to seconds.,Traditional, analog data-gathering took months. Traffic police or doctors would jot down information about patients?€? infections or drunk driving accidents, and these forms would then be mailed to a hub that would aggregate all this data. By the time all these details were put into one document, a month had passed since an outbreak of a new disease or a problem in driving behavior. Now that digital data is being rapidly aggregated, however, we are given the opportunity to make sense of this information just as quickly.,This requires analyzing millions of events per second against trained, learning algorithms that detect signals from large amounts of real, live data ?€? much like rapidly fishing for needles in a haystack. In fact, it is like finding the needles the moment they are dropped into the haystack.,How is real-time data analysis useful? Applications range from detecting faulty products in a manufacturing line to sales forecasting to traffic monitoring, among many others. These next years will hail a golden age not for any old data, but for fast, smart data. A golden age for as-it-happens actionable insights.,Original post:??

In this series I reveal rules of intelligence contained within grammar, and explain how they can be utilized to unleash intelligence in software. These rules are extremely simple, but still undiscovered by scientists.,Under certain conditions, three types of conclusions that can be generated autonomously:,?€? Given ",";,?€? Because of the common word ",", a conclusion can be drawn: "," by substitution of both sentences.,(More detailed conditions: ,),?€? Given ",", "," and ",";,?€? Because of the common words "," and ",", a conclusion can be drawn: ",", by substitution of those three sentences.,Another example:,?€? Given "," and ",";,?€? Conclusion: ",".,In this conclusion the conjunction "," is used, which indicates a choice (see Part 1). So, let's utilize this rule of intelligence, by converting the conclusion into a question, in order to stimulate the user to complement the knowledge: ",".,(More detailed conditions: ,),?€? Given ",";,?€? Obvious conclusion by reversing the sentence and changing the verb: ",".,(More detailed conditions: ,),To download the open source implementation: 



As Big Data circulates more often in the HR crowd, companies are looking for ways to jump on the bandwagon, pull their numbers and crunch away. ??But,??to make decisions is far more complex than having a shiny new system that spits out charts and graphs.,Big Data may start with that shiny new system, but it's only effective if it ends with a story you can tell to the decision makers. ??Being able to tell that story will provide the evidence to show that talent issues are directly connected to business success. ??Once HR leaders have the data and the story, they'll have to get comfortable with their new level of influence. ??They'll not only have a seat at the table, but have something to say that other leaders will listen to.,It's a new and powerful position for HR to be in, so having a??,??with those who know which Big Data story needs telling is important. ??Our goal in any partnership is to not only provide the solutions that each client needs, but to plan projects from start to finish that will empower HR leaders instead of just muddying the waters.
Relationships are built on trust and mutual understanding. Building a 360 degree customer data profile facilitates the marketers?€? understanding of their customers.?? This in turn, helps marketers provide their customers the best experience and service, target product offerings, and produce relevant communications. The key to building a 360-degree customer profile is aggregating as much relative data as possible. ??Centralizing the 360-degree profile in an easily accessible and robust analytical environment helps marketers optimize solutions and refine strategies.,.,It is important that your company has the capability to maintain a data integration system that automatically extracts, transforms, loads and houses customer data, in a platform that is robust and agile enough to handle complex queries, standard reporting and support business intelligence tools, thus, providing the marketer with easy access to a complete customer picture.?? This will allow the marketer to drill down and unearth unique solutions and opportunities.,Having a single source for all customer related attributes and activities will be a valuable asset for planning, strategizing, and understanding the customer?€?s value to the company. Questions presented below can provide a framework for companies to use to extract attitudinal, behavioral, and demographic behaviors about the customer:,Centralizing relevant data about customers in one location for marketers will ultimately increase ????overall efficiency and effectiveness, thus saving your company time and money over the long run. Our next segment will discuss how to share the 360 degree profile effectively to agents and brokers.,??

 , , 
 , , , , , , , , , , , , , , , , , , , , , , , , , , , 
Financial Institutions are focused on initiatives to survive in a world where regulatory pressure, risk mitigation and increasing volumes of data continue to pressure legacy infrastructures. Improved operational efficiency and revenue generation are at the forefront of the agenda.,Specific areas of concentration vary across regions of the world. Some common strategic initiatives in 2012 and 2013 include:,You can read the full article published in , here:



 , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 




??

Often we take good health for granted.?? However if you or a loved one has ever experienced a sever injury, disease, or other health event, it is often a life changing occurrence, which turns one?€?s life upside down.?? Today, there is more information available to patients to understand the symptoms, outline event progression, and summarize treatment options thanks to the Internet.?? However, there is even more information available in the rich volumes of data stored by hospitals, insurance companies, pharmaceutical companies, biotechnology, and other healthcare intuitions and organizations. The golden nuggets contained within these desperate repositories are invaluable for both macro health trending and micro segmentation based on known commonalities. Leveraging advanced data analysis techniques, there is greater promise in discovering unknown correlations that could provide understanding of our most pressing health issues. Unfortunately, the technology necessary to extract key insights from these disparate systems hasn?€?t been available?€?until now. The confluence of big data stores, distributed computing, and real-time, multidimensional analytics offers the potential to change every aspect of our healthcare system.?? Furthermore, the best practices and methodologies developed within the healthcare industry are also highly applicable to all businesses facing big data and analytics challenges.,??,After a life changing diagnosis twenty years ago, Laura Madsen, M.S. suffered from a chronic condition for years before someone could pinpoint what was actually wrong, as her paper medical records grew to rival Webster?€?s unabridged dictionary.?? Ironically, each new doctor/specialists would request the same information, which was a highly inefficient process.?? Laura eventually realized that most of the inefficiencies had nothing to do with the care team?€?s capabilities, but had everything to do with the sharing of patient information.,??,In 2006, her two-year-old nephew had been ill off an on for over a month, which isn?€?t uncommon for children that age.?? The pediatrician was concerned and sent her nephew to the Children?€?s hospital.?? The blood test revealed leukemia.?? After treatment, he nephew made a full recovery.?? However, the anxiety of associated with such an event was partially mitigated by the hospital?€?s use of technology to track patients, report patient status, and keep families informed about their loved ones.,??,After these two life changing events, Laura made another major life decision. It was to evangelize the critical value of technology?€?s importance in the healthcare industry to deliver efficiency, better outcomes, and improve transparency.?? With the aforementioned confluence, Laura was in the right place at the right time to make a difference, especially as increased regulations further accelerated healthcare data growth.,??,In her new book, ,, she offers her expertize in healthcare, data science, analytics, business intelligence, and big data best practices to deliver a invaluable roadmap for organizations to plan, deploy, and manage technology.?? Written with the business leader in mind, it is a detailed methodology for building intelligent business intelligent and information analytics capabilities to deliver sustained ??organizational value.,??,In a rare interview, Laura Madsen, M.S., will be discussing her new book and taking questions on her experiences in healthcare big data and how the healthcare learnings can be applied to other businesses.?? The interview webinar will be on February 20,, 2013 at 10:00 PT. It is open to all, however there is limited telephone space, so reserve your spot today ?€? at ,.

New York City Mayor Michael Bloomberg in a recent press conference detailed a new $15M partnership ??to tackle the increasing volume of big data??to increase New York?€?s capacity for applied sciences and potential for economic growth. ??The agreement will create 440,00 square feet of new space dedicated to big data education and training, as well as the addition of 75 big data research specialists.,Dr. Kathleen McKeown and Dr. Patricia Culligan were selected to lead the new initiative. The big data initiative will focus on research and technology relating to the development of smart cities, new media health, health analytics, financial analytics, and cybersecurity.,?€?How to best exploit big data for society?€?s advantage is an issue for many companies and research fields, and our Institute will focus on data science research across disciplines and the development of programs for training future data scientists much needed in today?€?s world,?€???said McKeown.,Read Article -??



.
" Here's the book in under five minutes. Props to my spectacular video guy, , of ,.,
,??,??,??, from , on ,."

This blog entry continues the topic of how a Data Scientist can convince colleagues to become more data driven.?? The previous blog covered office politics.?? This entry covers integration with the strategy and, more specifically, the process that creates the company's strategy.,Every company is unique and consequently so is its strategy process. At first glance, research on how companies develop strategies is complex and contradictory. There are simply too many ways to go about it: SWOT, Five Forces, Contingency Theory, Product Evaluation, etc... The problem here is that choosing one approach means neglecting other approaches and aspects of strategy. In 1999, Henry Mintzberg and Joseph Lampel performed a survey of scholarship in this field. These two researchers categorized their findings into 10 schools, which are:,?????????????????????????????? 1. Design School - sees a fit between environment, customers, and products; uses SWOT Analysis,?????????????????????????????? 2. Planning School - strategy exists in directives and tasks; uses objectives, budgets, and plans,?????????????????????????????? 3. Positioning School - sees an analytical bent to planning; uses game theory and analytics,?????????????????????????????? 4. Entrepreneurial School - sees leadership as critical to success; uses intuition and visioning to set objectives,?????????????????????????????? 5. Cognitive School - sees interpretation of events as input; depends on creativity; one tool of choice is thought maps,?????????????????????????????? 6. Learning School - sees retrospective sense-making as a first step; uses experimentation; depends on a scientific approach,?????????????????????????????? 7. Power School - sees influence within and without the organization as useful; uses bargaining, partnering, and networking; users could be labeled as Politicians,?????????????????????????????? 8. Cultural School - sees strategy as a social process; uses common interests and integration; users are typically people-oriented and value concepts like trust,?????????????????????????????? 9. Environmental School - sees the environment as the greatest constraint;?? uses other schools to develop plans; sometimes called Contingency Theory,?????????????????????????????? 10. Configuration School - sees transformations as key to successful strategies; uses concepts like "turn-around" and "dramatic change"; typically used by change-agents,??,Luckily for us, the researchers went one step further and laid out the ten schools into a ten-step process. This gives us a model on which to lay any given company's process. The ten steps are not performed in a linear manner. Environmental school is performed continuously. Several other schools, specifically Cultural, Cognitive, Learning, and Power, are performed simultaneously.,With this ten-step process, you can ask yourself which ones your company emphasizes, which ones are implemented weakly, and which ones are ignored. Knowing this allows you to determine where your project(s) fall along the 10-step strategy process. And...Knowing what occurred prior to your project and what could have occurred but did not, gives you ideas for enhancing your results for greater impact. Equally important, knowing what could happen after your project ends gives you insight in selling your project to stakeholders.,For example, let's say you determine your particular data mining project fits with the Cognitive School, but you feel that the Entrepreneurial School was skipped. Also, you feel that the Power School is important to your organization. Then your next step should be to use the Learning School, which logically follows as the next step, as a means for gathering support and political strength for your project. Stating the benefits to your projects to senior decision-makers and explicitly stating how to take advantage of your work grows your personal worth to the company.,Good luck.,Recommended Reading:,The original article by Mintzberg and Lampel is worth reading. They actually used the word Safari to describe their work. It provides expanded explanations of the ten schools and plenty of references to the scholarship used in their study.,Mintzberg, H., & Lampel, J. (1999). Reflecting on the Strategy Process. Sloan Management Review, 40(3), 21?€?30.??


Contrary to the prevailing sentiments, I counsel patience with regards to hiring a data scientist. Firstly, I mean no disrespect to my data science colleagues, and many will likely agree with me because no one likes to enter an environment where sub-optimal results or failure are probable. Yes, it?€?s likely you need to add one or even a few data scientists to the team, but not as your first step into the wide, wide world of Big Data. Give me a few minutes and hopefully you?€?ll see it my way.,Sure, Big Data is, well big. And it?€?s scary. Especially to those of you who have not been around large data sets most of your lives. So you need help understanding it. To put the problem in perspective let?€?s look at a medical analogy.?? If you have some discomfort in your chest, you don?€?t run to a cardiac surgeon and ask for bypass surgery. First you try to understand the nature of the problem and then go through protocols designed to make the discomfort go away. Even in the case of extreme circumstances, one goes through a diagnostic process, perhaps in an emergency room, to determine the best way to get ?€?better?€?. I would submit that if your business situation is extreme with respect to Big Data, there are probably bigger issues to deal with and you might need a different kind of specialist. A lot is going to depend on the size of your company or organization. If you are a medium to large global enterprise with lots of consumer-type customers the need for Big Data usage may be more acute than in smaller companies, and at some point you may find that you need data science expertise in house. Smaller companies need to think very hard about on staff data scientists as the costs may outweigh the benefits. In either case hiring decisions should follow strategy, planning, budgeting, etc. Knowing what you need to do ahead of time will help to frame implementation decisions down the road, including and especially hiring.,What if you don?€?t know enough to craft a strategy? In all fairness a data science perspective will be important in any well-conceived strategy or plan. Hire some consultants. (I know this seems self serving, but the right consultant will work themselves out of a job. Others, not so much.) The short term expense will be amortized many times over as you march down the road towards extracting maximum value from your Big Data investments. ??Not every consulting firm will have the expertise necessary, although many will profess they do. Like many other decisions we make: buyer beware. Network with colleagues, attend some industry conferences, find the right short term help and the rest will fall into place. When the time is right to hire, hold on to your chairs. According to the media, it looks like data scientists may be getting paid better than cardiac surgeons, so don?€?t expect to get any bargains.,There really isn?€?t any need to panic hire. Big Data is here to stay. If we have proven anything it?€?s that as an industry we are insatiable when it comes to capacity, speed, bandwidth consumption, etc. That means you have time to strategize and plan about how to make the most effective use of the Big Data that matters to you. ??Remember the old saw: proper planning prevents poor performance.?? Never has it been truer than with Big Data.


??,What is buzz marketing? In the strict sense of the term, buzz marketing is creating noise around a product, service, company or brand. For example you can recruit consumers, preferably proactive volunteers who influence their peers, and help them to try your products in good condition before pushing them to talk about their experience.,The buzz is one of the most powerful forces in the market, and knowing how to master this important marketing channel is critical. Word of mouth is more credible than most sincere seller; it affects more people, faster than advertising, direct mail or even a website.,It is this credibility which gives to the Word of mouth part of its power. But be careful, it also obtains its credibility because it can be negative, in which case the marketing finds it is not easy to control.,The buzz became a basic weapon in the marketing kit, and it is used more frequently. The best buzz relates to products or services, which consumers like to talk. Make available a great product, and your customers are happy to tell their friends, colleagues and family, and generate word of mouth you want. Buzz or rumors: what's the difference? The rumor is information with hidden or unknown origin that spreads widely without being checked. The buzz breaks out without any advertising investment. A rumor is a "subject", while the buzz is a "medium".,The buzz targets to capture the attention of customers and media, and ensures that speaking about your brand is fun, exciting and rewarding. This requires knowing how to launch and sustain conversations. As with any advertising campaign, the buzz campaign is based on a strong idea. This idea must meet a need expressed or unconscious, it must be attractive and original, catch the attention, generating a need or pleasure. To start a buzz we must follow the following steps. 1) Identify the key people (influential in their communities) may be vectors of the message. 2) Valuing these people through a personal experience that flatters their ego to make them eager to spread the message. 3) Encourage the message out by providing, information and means for feeding the buzz. Vectors can be a network, a group of related or maintained people.,In this type of approach it is necessary to identify and exploit different types of actors. Innovators, these people have an open mind to accept new ideas far away from traditional aspects of fashion. Marketing should not give them much attention because they are a minority. The "early adopters", they are always in search of novelty, they are attracted to risk. They adopt or create novelty and transmit to the bees (early majority). But beware, they are attracted by exclusive rights, specific offers, they like to feel among the privileged. Bees, they are the heart of the buzzing and they can supply large-scale chain information within the targeted community. We need to get the bees to talk about their experiences, and share their findings with others. Key people for exchanges are the "connectors" that have a complete address book or "mavens" who are experts in the field, and are opinion leaders. There is also the general public, which when touched can cause a snowball effect and finally the laggards that we have not to care of, because they are attached to traditional things and are not open to new ideas.,We should distinguish from the traditional buzz and buzz digital. Traditional buzz techniques are for example, product placement, distribution of samples, animation events of discovery, especially in the street and sponsorship. The key element in the traditional buzz is the contact and the relationship between vectors and products, so that the vector can observe and keep in touch with the people targeted. What?€?s the vector has to do, is talk about the product, idea, service: this is how to buzz. The digital buzz, also called viral marketing is a technique that allows using the internet very rapid dissemination of ideas, news and product information. Two methods can be used for example to spread an idea we can send a funny or surprising message and it will circulate rapidly among Internet actors, or offer to Internet actors via a good grip to visit an interesting site, and invited them to register to be part of a valuable information campaign / action.,To conduct buzz operations we should at least have operational tools (document repository, database, campaign management, survey?€?) and decision applications (customers profiling, segmentation, reporting on actions ...). If operations are developed on the web, especially via social media, Big Data analytical applications (clickstream analysis on the web, networks, texts, sentiment analysis...) can be very powerful to understand the market, identify the various stakeholders and to drive actions.,To go further on Teradata Aster Big Data solutions and references, in particular on the Buzz Marketing domain, you can usefully consult the following link: ,??
IT market studies expert indicate that the advanced analysis of various data and the management of an increasing volume of data, are among the five priorities large enterprises CIOs and companies at the forefront of the world of internet. Big Data is thus increasingly important, and a growing number of companies complete their decision-making infrastructure, with new analytical platforms to improve their efficiency and profitability.,First experiences Big Data observations show that many different technologies are used, even if an emerging technology based on the open source project Apache Hadoop, is often present in the infrastructure of the pioneers of Big Data. Hadoop's popularity seems to lie in its ability to handle large volumes of data with a standard low cost server infrastructure. But be careful all the experts say that there is no universal solutions for Big Data, users should be determined according to their needs, the appropriate technology mix to put in place, and set precisely where each (including Hadoop) can add value in their decision-making architecture.,First, consider the data that are to be processed. There are those in which the data model is established and is stable over time. Here we will find everything about the classic BI, reporting, financial analysis, automated decisions related to the operational and analysis of spatial data. There are those that can be stored in a more or less raw mode, which will be modeled in different ways according to the needs of iterative analyzes. This may involve, for example, the analysis of user clicks during their web browsing, data from sensors, CDR in telecommunications. There are those who are simply defined by a format. It is for example: images, videos, and audio recordings.,We must also consider what you want to do with the data. If the idea is to use structured data (reporting, BI, data mining), a classical appliance will be ideal, you can possibly complete with a cheap storage Hadoop solution appliance or with "the Teradata Extreme Data Appliance?€? for the data which present less interest in being included in the model of the warehouse business. For other data (web log, sensors, CDR, images, videos, ...) must be used according to the treatments described in the type Hadoop solutions and / or Teradata Aster, which can be implemented at low cost ( storage, application development, operation, integration with structured data warehouse) MapReduce programs.,Note that the Teradata Aster uses a patented technology named SQL-MapReduce that allows implementing MapReduce programs without having to learn a new programming language. This solution also offers the performance, scalability to handle large volumes of data, and process data with relational data pertaining to various formats. Compared with Hadoop this solution offers substantial benefits in terms of development costs and application response time.,Measured by an increase in revenue, market share gains and cost reduction, data analyses have always played a key role in business success. Today, the development of the Internet and automated business processes, makes crucial Big Data operations, and bring business leaders to rely more and more on their data analysis means. In this context, the teams are then conducted to supplement their existing infrastructure decision-making, with new solutions that implement complex algorithms. The pioneers who have already operated Big Data successfully, all say there is no magic bullet, even Hadoop, and it is for that Teradata offers different platforms implementing the Teradata database, Aster MapReduce and Hadoop solutions. ,To go further on the subject you can effectively discover the latest Teradata Aster Big Analytics Appliance that integrates in a single platform solution Hadoop and Aster (Hortonworks distribution):,??
As I build out Greenplum?€?s Data Science Services team, I?€?m in a very privileged position to witness, first-hand, the organizational transformations underway across nearly all sectors, private and public. Based on these observations as well as my own personal experience at Yahoo!, I believe that any enterprise initiative to move towards more pervasive utilization of data science is disruptive on multiple levels. Any CIO, CTO, or even CEO considering this strategic shift must place the transformation front and center at the C-level table, or risk significant erosion of comparative advantage to the first movers who are getting it right.,I am frequently asked by our Analytics Labs ,customers which levers they should control to drive towards the ?€?good outcome?€? as they embrace data science. The levers are numerous, and each is integral to the success of the effort. I?€?ve mentally cultivated my list over an extended period of time, based on my team?€?s data science work with our customers and prospects, my observations of the travails and successes of various Greenplum customers, and my pre-Greenplum days at Yahoo!, where I ran central Insights Services and led globalized data solutions during the company?€?s data ?€?glory days?€?.,In this post, I provide some high-level color on these levers, or ?€?transformation catalysts?€? as I call them. In subsequent posts I will cycle back to those warranting a deeper dive.
The growth of mobile has spurred a demand for mobile analytics, and cloud-based data storage/retrieval/processing has greatly simplified the process and lowered the price. Slowly, mobile app developers, interactive agencies, and market research companies are seeing the benefits of data analysis in evaluating mobile strategy and mobile projects.,Most mobile analytics companies are very similar: they require that you drop a small amount of their code into your app, which then allows a stream of granular data from every installed device to be captured. This data is then displayed on a web-based dashboard, which can help you monitor your users and slice/dice the data. This style of measurement is limited to only measuring??,??and not the entire device, reducing context of the app data. Examples of companies using this method: Localytics, Mixpanel, Flurry, PreEmptive, Countly, Appacts, Google Analytics, Bango.,Some analytics companies are aggregators?€?they capture IP traffic and then strip away web data and run analytics on the IP traffic coming from mobile devices. Lots of interesting datasets from this, but given the wide variety of native mobile app traffic, it?€?s a limited view. Companies capturing this kind of data are usually the big boys from the web, such as Google.,And then there are market research firms who buy data from lots of different sources in order to put all these data streams into context. Firms like Nielsen and GfK are massive aggregators and have lots of big name clients to show for it; one of the things we are doing here at ,??is culling unique data so that market research firms aren?€?t just looking at a stream of IP or in app behavior.,These solutions aren?€?t for everybody or certainly every budget, but the takeaway here is that you need to be doing research on your markets, audiences, and users. Research is a vital component of planning and development?€?what sort of analytics do YOU use?

Have you ever found yourself trying to convince a group of people to become more data driven and meeting a LOT of resistance?, , First, understand that this is normal.?? Second, understand that there are things you can do about it. This resistance exists because of factors at the personal and the organizational levels. People have existing plans based on their own assumptions.?? People, including you and me, make decisions on incomplete information.?? Without complete information, assumptions creep into our decisions.?? This means that as you, the analyst, help people make decisions on empirical data, you may be disrupting plans based on faulty assumptions. Not a good career move.?? , , Resistance also exists at the organizational level. Hierarchy and division of labor work to solve business problems in the most efficient way - with as few individuals as possible with the appropriate skills.?? This means everyone in an organization has different concerns with access to different resources.?? This naturally creates conflict, but this conflict actually ensures higher quality, efficiency, and different viewpoints being considered. The problem is that you, the data scientist or analyst, are responsible for bringing new insight into a decision-making process that is designed to create conflict. , , If you are starting out in an organization that has never used analytics or business intelligence, you will meet resistance. What do you do about it?, , First, get to know people.?? You may have a boss that controls information. If so, try to understand your boss's behavioral type (Myers-Briggs Type Indicator or DISC is effective here).?? Develop a plan on how to talk with your boss. Next, try to understand your boss's customers in order to better understand your boss's concerns.?? Build trust by adopting your boss's concerns.?? Eventually, you should be trusted enough to speak to people outside your office without your boss being present.?? Repeat this two-step process with everyone you meet., , Second, get to know the CEO's vision. You may be lucky to work in an organization where everyone understands the vision.?? This is called a shared vision. Most of us are not that lucky and, again, people with different roles and responsibilities interpret the vision differently. As you gain understanding in how this vision is interpreted in its various ways.?? You will be able to understand what data is important to any given business problem. You will be able to add reasoning to your reporting and discussions. You will be able to stay focused on fulfilling that vision and gain allies. , , Recommended reading: If you want to know more I recommend Jeffrey Pfeffer?€?s Power.?? This book explains office politics from the viewpoint that politics is just reality. Politics is nothing more than people trying to work together through existing social norms. Pfeffer does a wonderful job at bringing a nebulous topic down to real-world, concrete examples with corresponding real-world, concrete advise., , Good luck


Market research firm IDC predicts that all digital data created will reach 4 zettabytes in 2013. Gartner says that Big Data is moving from small, individual, and focused projects to an enterprise-wide architecture. All this requires a breakthrough when it comes to current approaches for leveraging Big Data.,We are clearly still in the earliest days of the Big Data movement. As such, companies are lacking the expertise and know-how to operationalize and monetize their data assets and are turning to services companies to help them bridge those gaps. Attempting to operationalize the assets can take months or even years and can be extremely expensive as well. Worst of all, the efforts may not help monetize Big Data directly. Thus, the current state of Big Data is great for service vendors, but not necessarily so great for Big Data buyers. Obviously, this is not a sustainable model, but it is the first step in the process.,Open source business intelligence firm Actuate recently announced the results of research that found 26 percent of companies with over $1b in revenue are working on Big Data projects today, with a further 34 percent in the evaluation and planning phase. This observation points to the fact that with the services model, Big Data is still a luxury exclusively for very big companies.,Beginning in 2013, the market will be looking for innovation. Product-centric companies will begin to disrupt the patchwork of services-centric solutions that currently exist. The product-centric companies will deliver the speed of data-to-insight conversion with a compelling economic business case. The business case will include the time-value of data and the measure of its useful lifespan, which gets smaller as the data gets bigger. Hence, there is a compelling need for automation and a product-centric approach to Big Data.??,Those businesses that start adopting the product-driven approach to Big Data early will have a significant advantage in their experience curve ?€? leading to more enterprise-wide analytics usage. This will lead to competitive advantages in both the short and long-terms.??,The biggest challenge will be to build a data-driven culture that will become a mandatory requirement for any business. The other challenge, for IT in particular, will be to keep up with the rapidly evolving suite of advanced processes to enable enterprise-wide utilization of data and collective insights.

Data is like horses; it can be untamed and unmanageable or it can be trained and useful. Taking data from the one state to the other can be routine and fit to the ken of any data scientist. But then, sometimes there are wild??Clydesdale data sets, big bucking blobs of inconsistency that require the skills of a Data Whisperer.,A Data Whisperer is someone who can see an analytics problem from the data's perspective. The DW understands where the data came from, how it has been used, what are its strengths and weaknesses, and what it needs in order to be reliable, safe, and useful.,Consider a large data set with a lot of missing values. ??Making sense out of the data that is there can depend greatly on what one does about the data that is not there. A tenderfoot might just it fence it off and work around it. A seasoned buckaroo might try and saddle it with formulaic proxy data. But, a Data Whisperer might use machine learning to predict the missing feature values from the ones that are present.,And, the Data Whisperer does , data science, studying the data in action in its natural habitat, observing its confirmation at rest and full gallop, how it is fed, housed, and cared for, and, most importantly, what it was bred to do. ??(No amount of whispering is going to turn a trotter into a thoroughbred or a reading into a tranaction.,Are you a Data Whisperer? ??When you start a project, what steps do you take to get to know the data on a deeper level? How do you use that knowledge to enrich the data and to guide your workflow? ??


I??found it odd there was no way to automatically deskew data in R, so I wrote a short little function to do it.?? It noticeably improves the peformance of linear models and linear support vector machines., 
 , , , 

Most of us have heard the term Big Data, but there is still a lot of confusion as to what the term actually means. In some spheres, the word has been defined as datasets so large that they are ?€?constantly moving.?€? In other words, collections of data that are too large to be measured with conventional software analysis tools. In others spheres, the word has been declared as a new class of economic asset, like gold or currency. This is data that can be utilized by businesses for its predictive power. Such data monitors and measures people and machines with clever algorithms and has the potential to predict behavior of all kinds.,Whichever definition you adhere to, the key to understanding Big Data is that it is a revolution in data measurement. It encompasses the advancing trends in technology for dealing with data of unmanageable proportions. , described this revolution similar to the impact of the microscope. ?€?Data measurement is the modern equivalent of the microscope?€?the microscope, invented four centuries ago, allowed people to see and measure things as never before ?€? at the cellular level.?€?,Can you imagine what the world would be like if our data could be seen in a new way, like the revelations of a scientist after his first look at an organism under a microscope? Unfortunately, becoming ?€?scientific?€? in data measurement is not an easy task. Big Data can be very bad for business. Let?€?s look at the top three challenges organizations are facing:,Companies that implement Big Data analytics platforms are often simply ?€?data hoarders.?€? This means they collect vast amounts of information, but are then unable to delete any data for fear that they can mine some sort of value out of it.,According to the , survey results, organizations on average need to archive about 2 to 3% of their data for legal hold, 5 to 10% to meet regulatory requirements, and 25% for business analysis and insights.?? This creates hoarding costs because businesses simply do not know what information to keep and what to trash, so they end up moving the data around, not solving their problems.,Keeping poor data around can cost businesses 20 to 35% of their operating revenue. What?€?s worse is that bad data or poor quality of data cost U.S. business ,.,To solve expenditure problems with Big Data, focus is being shifted from Big Data to Smart Data. This comes in the form of using visualization tools associated with the data, driving automation processes, or democratizing analytics tools so that managing the data is a collaborative process. In short, Smart Data is data that can be used to make decisions.,??,For example, connecting product lifecycle management (PLM), service lifecycle management (SLM), enterprise resource planning (ERP), and content management systems (CMS) can streamline delivery of up-to-the-minute data. With access to this information, users can make more informed decisions that minimize downtime and reduce the risk of error.,Another fear is that improperly managed data can offer false truths. Kate Crawford, a Microsoft researcher, noted that ,. ?€?In fact,?€? she stated, ?€?we may be getting drawn in to particular kinds of algorithmic illusions.?€?,Illusions arise when using sophisticated math models to assess high levels of data aggregation. Here small bits of essential information can be lost, thereby giving different meaning to any given analysis. More simply, we think Big Data offers a descriptive analysis of the statistical information being collected. The ?€?trap?€? occurs because we don?€?t realize that particularly large datasets often include an unintentional bias, thereby tabulating results that don?€?t accurately represent the full collection.,??,The misinterpretation of Big Data extends even further with the fact that data is always embedded in a context. Basically, numbers don?€?t speak for themselves; researchers, data scientists, and even employees must interpret unstructured data and place it in the right context for their given purpose or business decision. Thus, in order for Big Data to work effectively, the context must be realized along with its limitations and biases.,Eliminating ?€?blindness?€? in Big Data requires it to be modeled in parallel with small data in order to offer the required depth to eliminate illusions. But how can this be done?,One solution is to provide a single data aggregation system that can filter information delivery according to defined user roles. In this way, users are better able to organize, access, and view large datasets at both high and low levels. With multiple user role options, such as Viewer, Publisher, Admin, Reviewer, Librarian, and Commenter, a harmonious system of checks and balances is established.,Top-level user roles control the data at the highest levels while analysis of the data is broken down in specific steps. Individual roles, with distinct tasks and security settings, are limited to certain modifications or additions to the data. The ability to publish, create, search, filter, and edit the dataset is divvied out between users. The hierarchy creates an efficient, repeatable workflow that enables users to obtain the most relevant data available.,With so much emphasis centered on how we interpret data with depth and at a low cost, the world needs people who can understand data intelligently. The problem is that there is a large gap in the number of individuals who can perform this task with the demand for it. A , cited the workforce demand for an additional 140,000 to 190,000 professionals with deep analytical talent in the next five years alone. So how can organizations fill the need to analyze big data today - when the manpower simply isn?€?t available?,Until the backlog of data analysis professionals can be filled, organizations must rely on the development of innovative tools that provide them with access to the information that is necessary to make critical business decisions.,At TerraXML, we have clear understanding of how to turn Big Data into Smart Data. Our employees come from a variety of backgrounds rooted in mathematics, computer science, and enterprise consulting, however we are united in the ability to leverage data to help organizations make sense of their information. TerraXML partners with organizations to develop individual solutions that address specific business needs. Our solutions are designed to be customizable, allowing us to cost effectively address an almost limitless array of enterprise needs. This eliminates the need for solutions to be developed internally, which can come at high cost and with high potential for error.,For more information on improving information access and delivery, visit us at ,.,* Please note that this Blog post was originally authored by Michelle Hastings. The original blog post can be found on the TerraXML website at: 



 , 

The South by Southwest Interactive conference in Austin, Texas captivates our imaginations for a few days every year. How do they do it? The conference makes science and technology look both lucrative??,cool. Bright-eyed startup teams leave the conference with dreams of being the next Twitter??or Instagram.,We need to see more of this momentum, especially amongst Millennials. To generate interest in technology careers such as those created by the Big Data??wave, incentive structures in higher education must be examined and restructured.,A movement towards career-building incentives for Millennial students could revolutionize investment in higher education and lead to a workforce that is better equipped to solve business challenges like the growing need for experts in emerging technology fields such as Big Data.,How can a career services office accurately advise a graduating Mathematics major if their staff does not know what exciting jobs exist? What about an English major who builds computers on the side and wants to be a tech reporter?,??If a university does not track the metrics of job success by the industry and major of their graduates, they?€?re doing a disservice to the students investing money and time at their institution. Students deserve to have career advisers who are keeping abreast of business trends like big data analytics that are driving job creation.,The problem does not lie entirely with career services offices, though. Students generally understand that they should be demanding concrete metrics around employment. More often than not, though, they don?€?t know that they should care that they are missing out on jobs in industries that they never knew existed.,The Millennial student is wary of debt, cynical about their career options, hears many voices blaming them for their college outcomes, and sees little to no leadership??on fixing the problem. A way to rebuild a generation?€?s trust in higher education is to see universities tracking data around the success of professors and students alike, as well as noting what the next frontier in business is and how students can be guided to rewarding careers in those fields.,Millennials live and breathe information inundation, yet when looking to invest in higher education, the information is scarce regarding employment outcome. If we want to see our students succeed both before and after graduation, they need concrete examples of what inspiring things they could do with their education. Students need someone to spend some time educating them about modern industries, where the jobs are, and how their disparate interests can be applied to innovation.,Our higher education institutions want their students to tackle some of the biggest problems of our times. In order to do that, colleges and universities must better educate themselves and their students about where these challenges exist, and how a career can be built around tackling them.



Read article??






During analysis of movements of individuals in public places, there are only two dimensions that can represent movement of an individual,??shown via??data saved between starting and end point, even incorporating elevators and stairs to different (shop) levels). That is a multi-linear way of looking at movements of individuals in crowds in a specific environment. Most big shopping corporates use these kinds of analysis methods.,But what if (like in 3D environments) movement can be up and down as well, or even immediate? Or the environment can change from one moment to the other? This gives us??a literally new dimension in data about individual movements, meaning??person 'X' in environment 'A' at a certain time will react in a certain way. If that environment is different the next day, the individual has to readjust and react accordingly, as will his contacts.,Going a step further:,If an individual is allowed immediate entry to a certain location or product without having to move physically ("teleporting to a location"), be it 2 or 3 dimensional, how can you analyse the decisions a person will make? If I need three products, but need to walk through the whole shop (IKEA setup) to get them, that is not efficient for me as an individual, even if their statistics are showing that the majority may buy more that way. I do not like shopping in Ikea, I prefer walking 5 streets to visit three shops with humans who can advice me. But that is not analysable?,I think big data should be brought back to what they are valuable for: the individual.,Looking forward to reactions,,Emmanuel,PS: Another aspect of big data is future prognosis:,In the 80's, when I worked at Wang education, we had 15 years of data showing which users from which companies had followed which courses (a staggering amount of 500 MB of data ;o) They wanted to destroy that data as it was obsolete (courses we don't organise anymore).,Playing with it, I found out a certain customer had his staff trained on average every three years on new versions and/or platforms. The data??implied the customer was??ready for a new upgrade. When I asked the sales representative to contact them, two weeks later we had a new deal. Sounds fab, it is true, but sadly Wang doesn't exist anymore.,What does exist is how big data can predict, and using data on how people move around and communicate together in changing 3D environments is fascinating. I'd wish our servers could handle such big data.,??,Kind regards,,Emmanuel,??,??,??,??,??,??,??
 , 



Health care is ripe for Big data disruption because of 3 reasons

The ?€?Bell curve?€? or the ?€?Gaussian bell curve?€? is one of the fundamental concepts on which most of the statistical analysis is based. From social sciences to astronomy to financial services- most of the application of statistics in the real world relies on the assumption that the data being analysed is distributed in the shape of the bell curve.,The bell curve, named after Herr Professor Doktor Gauss, is a beautiful visual that depicts how data from any normal distribution would behave.,In simplest terms, the Gaussian bell curve reveals that in a normal distribution most observations hover around the medium or the mediocre, i.e. the average. And the odds of deviation from this average (or chances of a value being different from the medium)decline at an increasingly faster rate (exponentially) as we move away from the average.,Let us take a simple example* to understand this feature of a normal distribution.,??(* - This example has been taken from the book ?€?The Black Swan?€? by Nicholas Nassim Taleb. As a matter of fact, this whole article is inspired by Dr. Taleb?€?s brilliant writings on this subject.),Assume that the average height of humans is 167 cm or 5 feet, 7 inches. Also assume that the unit of deviation (generally taken as the standard deviation of the population) is 10 cm.,Now as per the rules of the bell curve or the feature of the normal distribution, if one were to look at a (large enough) randomly chosen sub-population of humans, one would find most people to be of a height close to the average ie. 167 cm.,Put another way, more people are likely to be of height 168 cm (1 cm away from the average) than say 178 cm (11 cm away from the average).,And the odds of finding someone much taller (or shorter) than the average decrease at a faster and faster rate.,The odds of finding someone more than 10 cm taller than the average i.e. taller than 177 cm is ,The odds of finding someone more than 20 cm taller than the average i.e. taller than 187 cm or 6 ft 2 inches is ,The odds of finding someone more than 60 cm taller than the average i.e. taller than 227 cm or 7 ft 5 inches is ,The odds of finding someone more than 70 cm taller than the average i.e. taller than 237 cm is ,The main point to understand here is the pace at which the odds decline as we look for more and more abnormal or unusual observations. For the 10 cm increase in height from 177 cm to 187 cm, the odds change from 1 in 6.3 to 1 in 44. But for a 10 cm increase in height from 227 cm to 237 cm the odds change from 1 in a billion to 1 in 780 billion.,This is the essential property of a bell curve. The odds of finding larger and larger observations become so small that the outliers or unusual occurrences become a very, very remote possibility and hence can be ignored for all practical purposes.,This is the boon of the bell curve. It allows us to focus on the mediocre or the ordinary, and ignore the?? rare or the barely possible.,This is why statisticians, academicians, analysts and all sorts of people love the bell curve. It allows them to focus on the usual, the frequent or the norm.,Statistical models, from simple regression models to complex ones like the Black Scholes model in finance, are based on this property of the bell curve.,It is this property that allows us to say that it is highly improbable to see someone who is over 8 feet tall. Or make even more precise predictions such as ?€? 68% of a large randomly selected population is going to be within 157 to 177 cm in height. And many such declarations that you regularly encounter in daily life ?€? from medical test results to exit polls.,So far we have talked about how the bell curve is a boon for statistical analysis ?€? it helps us simplify things and use rules to understand distributions. The curve?€?s symmetry and consistency make it ideal for making predictions. This is why it is such an important concept in business analytics.,In the next article we will actually address the main topic i.e. how these same qualities of the bell curve that make it so tempting and useful are also a curse. We will understand how uninformed application of the bell curve can lead to serious errors and can cause more harm than good. We will also see how mis-use of the bell curve is a lot more rampant than we think.,Gaurav Vohra is an alumnus of IIM Bangalore with over 10 years of experience in the field of analytics. Gaurav has been in the analytics industry from its initial days and his career has spanned companies like Capital One and Information resources Inc., recognized as thought-leaders in the analytics space.,Gaurav is now the co-founder of ,, a training institute that aims to meet the growing demand for talent in the field of analytics by providing industry-relevant training to develop business-ready professionals. You can visit Gaurav?€?s blog at?? 
Here?€?s a brilliant presentation from Mike Bowers, Principal Engineer at the Church of Jesus Christ of Latter Day Saints.?? It accomplishes two major objectives; Mike reviews the strengths and weaknesses of the five major classes of databases today (relational, dimensional, object, graph and document).?? He then dissects the major NoSQL databases on the market including MarkLogic, Mongo, Riak, Cloudant/Couch DB and Cassandra.?? How do they stack up? Are they enterprise ready??? If developer productivity, application performance and enterprise readiness are concerns that your company has, this video is a ?€?must see?€?.?? You can watch the video here:,With over 80 % of the data being created today is unstructured (probably better termed poly-structured), organizations need to store, search and analyze hundreds of different data formats at light speed.?? ??The ability to handle data variability, data variety and data relevance has jumped to the top of the agenda for both business and IT. But how can organizations discern meaning from this data????? How do they create context around unstructured data that exists in so many forms while also making it discoverable?????,Relational Models are not well suited to handle the problem since they were designed to organize your data in rows, columns and tables.?? The variety and complexity of unstructured data coupled with the overriding need to scale out on commodity hardware prevent them from leveraging over 80% of the data today.?? And there?€?s no end in sight.??,Mike shows a great example of how the document database (NoSQL database) takes unstructured data in the form of a story, identifies the data elements in the story (topic, location, author), semantically links these elements to show relationships between the elements and then identifies the hierarchy within the story (title, subtitle, body, etc?€?).???? Armed with all of this, the unstructured data lives with context.?? The original document persists but now all of the elements are discoverable in a variety of ways.????,Given the reality that unstructured data is growing so rapidly and needs to be integrated and analyzed alongside structured data to complete the picture, what does an application need from a NoSQL database??? Basically what every database needs - five core capabilities.?? The basic requirements are 1) inserts, updates and deletes 2) the ability to query the data 3) the ability to search the data 4) the ability to bulk process the data and 5) the ability to do all of this consistently.?? With extraordinary data volumes, this has to be done at scale in an affordable way.????,Mike evaluates search relevance, advanced search using facets, geo-spatial search, entity enrichment, data consistency, developer productivity using JAVA, the ability to retrieve multiple documents, integration with the BI stack using SQL, real time data ingestion, indexing and much more.?? ??Imagine if you had to ask your programmers to develop an application to handle data locks, threading bugs, serialization, dead locks and rare conditions????????? Imagine if you had to write the code to ensure all parts of your data transactions succeeded??? How would you ensure all of the data has been committed consistently? Do the commits meet all of your data rules??? ????How do you ensure your data survives system failures and can be recovered after an inadvertent deletion???????,Mike answers these questions and much more.????,Learn more at 








Excellent customer communications is now a recognized component to drive sales and give Insurance companies the competitive advantage. Here are some tips when utilizing analytics to solve??,??while keeping the customer in mind:,??,??,It is important to stay ahead of your competitors; our next few articles will each point in a more detailed light. Expanding into the growing popularity of customer-centric attention paired with analytics will effortlessly improve the overall scope of your company?€?s profits.


The ?€?Bell curve?€? or the ?€?Gaussian bell curve?€? is one of the fundamental concepts on which most of the statistical analysis is based. From social sciences to astronomy to financial services- most of the application of statistics in the real world relies on the assumption that the data being analysed is distributed in the shape of the bell curve.,In the last article we discussed the usefulness of the Bell curve. It helps us simplify things and use rules to understand distributions. The curve?€?s symmetry and consistency make it ideal for making predictions.,In this article, we will discuss how these same qualities of the bell curve that make it so tempting and useful can also be a curse.,There are many examples of normal (or approximately normal) distribution around us. The statistical concepts have been empirically tested and verified countless times.,Certain quantities in??physics??are distributed normally such as the velocities of the molecules in an??ideal gas. In??biology, the??,??of various variables such as the thickness of the tree bark or claws of a mammal tend to have a normal distribution. In Finance, changes in log of certain phenomenon such as exchange rates and price indices are assumed to be normal though this assumption is hotly contested by some. Bell curve grading??assigns relative grades based on a normal distribution of scores.,As Dr. Taleb says in his book, The Black Swan, we can make good use of the Gaussian approach (i.e. the bell curve) for variables for which there is a rational reason for the largest not to be too far from the average. If there is gravity pulling down numbers, or if there are physical limitations preventing very large observations (say, the length of the tail of a cat), we end up in mediocristan.,Mediocristan is a term coined by Dr. Taleb to denote situations where the Gaussian approach (normal, binomial, poisson etc.) will work.,The Curse of the Bell Curve, however comes from the fact that we often use the bell curve in situations that bear no resemblance to a normal distribution. Many real life phenomena do not follow the bell curve and yet we assume a normal distribution just because the simplicity of the bell curve is highly tempting. Let us examine some glaring examples here.,Most real life data does not exhibit normal distribution. A normal distribution is more of an exception than a rule. Real world data shows variations (high and low) that are far more frequent than what the bell curve predicts. Even data that seems to be normally distributed may seems so only because our observation period is not long enough.,This is an important lesson for any analyst dealing with real world data. Always check the data for normality. And always look for a rational explanation about why the data should be normal. Only if you are satisfied on both the counts, should you assume a normal distribution. And then also, proceed with caution.,The concept of The Bell Curve is a highly seductive one. Once it gets into your mind it is hard to get past it. Hence be careful about its use.,The bell curve has a lot of uses and it should not be discarded completely. But it should be used judiciously or the consequences can be disastrous.,Gaurav Vohra is an alumnus of IIM Bangalore with over 10 years of experience in the field of analytics. Gaurav has been in the analytics industry from its initial days and his career has spanned companies like Capital One and Information resources Inc., recognized as thought-leaders in the analytics space.,Gaurav is now the co-founder of ,, a training institute that aims to meet the growing demand for talent in the field of analytics by providing industry-relevant training to develop business-ready professionals. You can visit Gaurav?€?s blog at?? 



Cloud data, cubes, relational databases, flat files, unstructured data... the list goes on and on. Ten years ago, many would have said that the future of data and business intelligence lay in unified, cohesive data warehouses where organizations could store all of their datasources (after ETL, of course). The future is here, but many organizations are finding that their databases are maturing and growing faster than their ability to integrate them into their data warehouse.,This creates a serious problem. Businesses must deliver new data into their business intelligence and analytics tools, but oftentimes their data warehouse is the defined standard of the organization. Standards can be exceedingly difficult to change or modify, not just technically but politically as well.??, The solution may surprise you. It is not "the death of the data warehouse", but rather the acceptance of a multi-database ecosystem for analytics and reporting.,If an organization has relied on a data warehouse for transactional data, why change that? Most likely the system serves its function well and has already been paid for. The change comes when the organization adds a new data source. Let's use Salesforce as an example. Five years ago, the likely method for analyzing and using Salesforce data would have been to develop a process to bring Salesforce into the data warehouse and integrate it with all of the other data. Now with the advent of tools that can connect directly to Salesforce in the cloud, it makes much more sense to simply connect to Salesforce directly with your analytics or business intelligence tool. No ETL, no intermediary database, no extra time or expense., After bringing the data warehouse and Salesforce data together, the trick is being able to use them in conjunction with one another. One of the most interesting innovations introduced in the most recent generation of business intelligence tools has been data blending: combining two or more datasources together by defining a common key within the analytics tool. In other words, skipping the ETL process as well as the intermediary database and simply connecting directly to each data source and combining them within the analytics or BI tool.,The benefit of this technique is that it significantly simplifies the process of adding additional data and databases. No longer will there be arguments about adding new data because of the challenge of integrating it with the data warehouse. As database technologies come and go, they can enter and leave the analytical process (via the analytics software itself) with little or no negative impact. The best part is that business users can finally have the flexibility to easily add the small datasets that add context to databases (like Excel files and CSV's). Data blending enables a fluidity of data that is nowhere near possible with a traditional "make it all fit" data warehouse.,To use an analogy, yes, it would be easier if everyone in the world simply spoke one language (or all the data lived in one database). But, as the complexity, history and culture of each language adds deeper meaning to the world, the differences and benefits of each datasource can add insight to your organization., Ross Perez works at Tableau Software, a maker of??, and , software.??

A NoSQL database from MarkLogic provides the Royal Society of Chemistry (RSC) with the ability to unlock a treasure trove of assets.?? Now the RSC can publish three times as many journals and four times as many articles. It also gave the Society the ability to develop new educational applications to make chemistry accessible to a wider audience.,Modern approaches to information products replete with full text search have the power to transform your business.?? Built on an enterprise hardened NoSQL database that can ingest data in real time using a "schemaless" design, they provide a brilliant user experience displaying search results, allowing users to filter, save searches and more.,Unstructured data assets hidden in the recesses as dark data can be consolidated with new forms of data streaming from the internet.,This reminds me of another application built on MarkLogic called AuthorMapper from Springer Media. Using geospatial search, users can zoom into countries, identify articles of interest, read the abstracts, search by date range, apply full text search and more - all within a durable, reliable enterprise NoSQL platform that offers real time alerts in a scale out environment.?? Can MongoDB do this????? No they can't.,New Information products like these can be brought to market quickly since developers can built them with Java, Rest and other common languages.???? Extending them to include interactive analytics in the form of data visualization is built into MarkLogic.?? Don't be fooled by NoSQL pretenders only to discover you need to write hundreds of thousands of lines of code.??,You can watch a video of the RSC at , under the Customers tab.?? Click on videos and it?€?s in the lower right.?? You can experiment with the Springer application at 
LinkedIn was founded in 2003, is currently revenue 243 million and employs 1797 people. This is not what we call a large company. However, LinkedIn has 175 million members in 200 countries including 50% outside the U.S., two new members join the network every second, and analysts said that all "executive" of the Global 500 are members. Under these conditions, LinkedIn is facing a high volume of data to process. Indeed their information system must support 2 billion a year of research carried out by members, dealing daily data and 75 T0 10 billion lines.,By analyzing all data LinkedIn is able to establish for example the list of the words used by members to describe their capabilities, and these words differ from one country to another. United States and Canada are extended highlights of the experience, while in Italy, France or Germany we say innovative, as in Brazil and Spain they are dynamic and that in Great Britain they highlights their motivation.,LinkedIn is certainly one of the companies involved in the development of what is now known in the business world as the "Science of Data", which is based on know-how from computer science, mathematics, data analysis and business management. Specifically the process is to collect quickly raw data, explore and analyze, translate this data into actionable information, and therefore reduce the overall time between the discovery of relevant facts, the characterization of business opportunity and triggering actions. ,But what LinkedIn does with its data? The company classically realizes analysis to better understand and carry out its activities, but above all it creates products / services based on the information it generates, either at the global level as with most used words seen above, either at the individual level with systems recommendations (the people you may know, the jobs that ...). The data allow for example to identify people of influence, viral process and social trends, test new products / services, new sites to maximize the business impact of connection and use of the site by members, to understand service use over time based on subscription levels, the connection means (PC, mobile, ...), providing detailed reports of analysis of advertising revenue, to assess the impact of action of viral marketing, to optimize recommendation engine, to create specialized functions for services to business (marketing, recruitment, ...).,To obtain these interesting results of the operation of its data, LinkedIn had to develop its own management application?€?s data flow, storage, research, network analysis, etc., and of course their own dashboards. For that the company went to get on the market tools or solutions they need, and we can mainly list: Teradata Aster, Hadoop, Azkaban, Kafka, Project Voldemort, Pig, Pithon, Prefuse, Microstrategy, Tableau software,??,To go further about the LinkedIn case, you can usefully follow the below 50?€? video presentation, entitled "Data Science @ LinkedIn: Insight & Innovation at Scale", by Manu Sharma, Principal Research Scientist and Group Manager, Product Analytics at LinkedIn: ,??
A data scientist at Flutura has to wear multiple hats in order to deliver next generation analytical solutions in the sectors we operate in namely energy, telecom, digital and health care industry. In order to do that he/she has to ,-???????????????? The , ??hat,-???????????????? The , hat,-???????????????? The , hat,Most of the time it?€?s easy to fathom the depth of the data scientists math / algorithmic knowledge and the depth of his/her understanding on handling high velocity data and unstructured data points. But one area of weakness is the business dimension. , This blog talks about , Flutura executes to decode the business acumen of a data scientist,Human Beings are ,. Flutura data scientists were doing data forensics on mobile app funnel drop analysis for an online travel agency was able distil the quintessential essence of all essences - ,Therefore,-???????????????? Can the data scientist translate numbers into stories? This is a very important tool to build bridges with business. Else a data scientist has the risk of getting struck in the world of math and unable to make the connect.,Other 7 tests are??elaborated??at??,So in a nutshell here are 8 questions to ask,-???????????????? ?€?RESONANT STORY TELLING?€? TEST,-???????????????? ?€?STRING OF PEARLS?€? TEST,-???????????????? ?€?NEEDLE MOVEMENT?€? TEST,-???????????????? ?€?SNIFF THE DOMAIN OUT?€? TEST,-???????????????? ?€?ACTIONABILITY?€? TEST,-???????????????? ?€?USE CASE CURATION?€? TEST,-???????????????? THE ?€?NORTH POLE?€? TEST,-???????????????? THE ?€?WHAT DO YOU SEE?€? test,??,??These tests are by no way collectively exhaustive or perfect. But it serves as a reasonable starting point to get the right DNA of Data Scientists into the organisation. Else we run the risk of having people who just knows how to create a Hadoop cluster :) as being labelled a data scientist.,As the saying goes ?€?The real voyage of discovery consists not in seeking new landscapes but in having new eyes.?€?- Marcel Proust,Good luck with your efforts to recruit the rare species ?€? the holistic data scientist :) !!!,Other 7 tests are??elaborated??at??
There is little time, about 3 or 4 years, if you wanted to process a large amount of textual data or web logs, you need to mobilize large servers and implement consistent SQL programs, long to be developed long and long to give results. Fortunately requests were few and generally volumes were measured at most in terabytes. Now e-commerce and social media have been largely developed, and many companies see their customer relationships, and therefore their survival, entirely dependent on the ability of their computer resources to analyze web logs and text data. In addition, for many of them, the volume is now in the hundreds of terabytes or even in petabytes. ,Most young companies in the world of e-commerce or social media does not have the resources to implement the solutions mentioned above, they needed. Experts have sought other ways and developed new solutions more efficient and less expensive, including those based on distributed file systems (DFS) and MapReduce programs. In this context, the open source Hadoop implemented in Java was a great success. So now companies that want to deal with large volumes of textual data or web logs complete cost their decision information system with specialized analytical platform.,Some predict the demise of enterprise data warehouses as we know it today, especially as providers offer cloud solutions. This will not be undoubtedly the case even in the medium term, and we will see companies managing several specialized systems internal or external. However this is actually the end of the single centralized data warehouse that handles all corporate data that very few companies have actually implemented.,In fact pioneers certainly show the way forward, the solution is to deal with different specialized systems, new one for multi-structured data and traditional one for structured data, in private or public cloud model. In fact, solutions are now provided in three forms: software only, appliance or cloud, and pioneers opting for hybrid solutions. The choice between these options must be based on the specific requirements of each company: regulatory requirements, industry, business function, relationships with customers (privacy), available expertise, security, the impact of localization data, etc.,One of the major short-term difficulties faced by the pioneers is the lack of Big Data skills. The use of Big Data is what is called the Data Science, a discipline that combines math, programming and business acumen. To take advantage of Big Data it is necessary to invest in a team with such skills, and work closely with business and IT. Indeed, it is possible to find trends, patterns, segments, etc. we did not know, but these results do not change anything, we must transform these elements into business opportunities and ultimately into concrete actions on the market. Experts from the Data Science know pave the way, but cannot go alone to the end.,Among the pioneers we found companies of very different sizes from large groups like Wal-Mart, Wells Fargo, Boeing, and many web related companies like eBay, Google, Amazon, Yahoo, and many much smaller companies like Facebook (3000 people), LinkedIn (1700 people), etc.. To go further on this subject can usefully consult the following link: ,??
 
 , , 

In following the big data 'buzz' and trends, it appears that there is a disconnect between our analytical goals (i.e., the types of questions our customers are trying to answer) and the computational substrate on which we build in order to answer them.,NoSQL technologies, while being far more scaleable than relational databases, are fundamentally a 'data level' (DL) technology, that is they are at heart document based. ??Relational databases are an 'information level' (IL) technology capable of answering somewhat more complex questions. ??The problem is that most of the questions we now seek to answer are 'knowledge level' (KL), that is largely based on the myriad connections between people and things, and less on record content or text search. ??Relational databases are bad at handling such questions, NoSQL technologies are worse (see??,).,It seems that we should be moving upward in the knowledge pyramid (see??,) in terms of the technology underpinnings we use. ??We need to move not from an IL (i.e., relational) database model to a DL (i.e., document model) technology, but instead to a massively scaleable knowledge level (i.e., connection based) database.,I have been actively developing just such a system (see blog??,) for over twenty years, targeted at what we now call 'big data' problems. ??Over that time we built federated solutions to answer massively scaled knowledge level problems by combining relational and other IL/DL technologies. ??In the end, all such combinations failed to scale due to the shortcomings on the underlying technologies when applied to KL questions. ??It became necessary to re-examine the very underpinnings we use to build integrated systems. ??A fully integrated and uniform ontology-driven (see??,) KL substrate, all the way from the database to the GUI, was needed to achieve the simultaneous goals of scaleability, adaptability, and knowledge level operation.,Our software systems and the analytics they provide must in the end be designed to facilitation the organization decision cycle or OODA loop (see??,), if they do not, they will not provide the help needed in a timely manner. ??To close such a loop requires KL technology in the 'Orient' stage and Wisdom Level (WL) or human-in-the-loop in the 'Decide' stage. ??The image to the left shows the knowledge levels required at each stage from an integrated system designed to support any full organization OODA loop. ??Just as importantly the systems must be rapidly adaptive themselves in the face of inevitable change in the environment and in the data they contain (see discussion ,) otherwise they will clog up the organization OODA loop and thus fall into obsolescence.,It should be a fundamental first step in any software development to analyze the OODA loop it is designed to support and the knowledge level of the questions that will be asked at each step in the cycle(s). ??This in turn leads to an understanding of the capabilities of the underlying data technologies appropriate to build a solution. ??From what I can see, our data scientists are still not doing this. ??Analytics front ends with elaborate graphs and visualizers can got only so far to overcome the shortcomings of a limited data architecture.,I know this is heresy, but I've been in the 'big data' game for over two decades so I may know of what I speak. ??For more of my heretical thoughts on 'big data' and big data analytics, see: ??,.


When creating a predictive model, data miners need to ?€?tune?€? it to make the right kind of mistakes.?? Setting the cut-off point between ?€?promising?€? and ?€?unpromising?€? depends a lot on our client?€?s biggest concern -- missed opportunities or false alarms.,Data Mining Misconceptions #1: The 50/50 Problem,??[f1].,??,??


Why would you do a SAS course? A majority of us want to improve our understanding of getting things done on software used by the largest section of the analytics fraternity.,??,??Why not use the web and other free resources to do this? The biggest advantage of self ?€? study through online and offline resources ?€? is that it is FREE. No monetary outflow. And you decide the pace and quantum of learning.,??,The disadvantage is that you need to know what is important for effective functioning in the work environment and get material relevant to deliver at work. The dedication required to master a subject is immense. Regular hours devoted to self-study with a very clear objective in mind.,??,Thus, guidance from an expert who can decide/ suggest a course outline becomes of maximum importance. And if we get the relevant study material with doubt clearing sessions and practical examples, the effectiveness of study increases many-folds. And the time we will take to master the skill reduces considerably.,One the most frequent complaints against most courses are that they are too theoretical and there is not much connection to what is taught and the real life scenario. So the curriculum design becomes a very important factor in making a good SAS course. It should give you a lot of practise on things that matter most in work life,??,The course has to be designed and delivered by Industry experts, people who have worked on SAS and large data sets in real life scenario When a course is designed for a short duration with limited hours it become very important to have the quality of the content to be precise and of very high quality which means it has to be designed and delivered by seasoned professionals from the Industry who understand the dynamics inside out,??,Sometimes the difference between SAS courses offered by two different institutes may be the mentoring offered by the Institute. In most cases the SAS courses are taken up by professionals who want to enter the analytics industry and may be new to the nuances of how this industry operates. Hence it is a great help to have someone advice on the way forward and assist in landing that dream job for which you had decided to take the course in the first place,??,Many institutes now ensure that you get to be part of an alumni network of the institute which ensures that you remain abreast with the latest happenings in the industry and also network with other professionals from the same industry. This ensures that you are more visible to recruiters and Industry experts which enhance your chances of moving up the ladder by a great deal in the long run.,??,Do keep these in mind when you choose a SAS course. Doubts remain? Call up Gaurav at 9880544099 for free counselling !!,??,About the Author: -?? Subhashini?? is currently active in the Analytics Training (,)?? , Blogging and Consulting ??arena, and?? has a decade of experience across roles in Analytics in Retail Finance and Banking . These roles have been across Risk Management, Collections strategy, Fraud Control and Marketing. Her area of interest is the integration of results / outputs of Analytics with Business Decisions ?€? Tactics and Strategy.,(Link to profile -??,??),??
How to use s3 (s3 native) as input / output for hadoop MapReduce job. In this tutorial we will first try to understand what is s3, difference between s3 and s3n and how to set s3n as Input and output for hadoop map reduce job.??Configuring s3n as I/O may be useful for local map reduce jobs (ie MR run on local cluster), But It has significant importance when we run elastic map reduce job (ie when we run job on cloud). When we run job on cloud we need to specify storage location for input as well as output, which is available for storage as well as??retrieval.??In this tutorial we will learn how to specify s3 for input / output.

Big Data holds a big promise. But has that promise paid out already? Or are you heading for Big Dollar Disaster? Many take inventory of their data and find out they have terabytes of data lying around. Surely something should be done with that, so here?€?s how we see a lot of companies going about implementing ?€?something?€? for their Big Data.,Until they find out that the size of meaningful data within those terabytes is considerably smaller and there are other, cheaper alternatives available.,Don't get me wrong: it is great to work on exciting new technologies and get your hands dirty on innovative concepts. But is it delivering business value?,We like to compare working with (Big) Data to a digestive system. There are several stages the data flows through and some of these are getting too much attention while others need more., , The art of collecting data and storing it., , Digestion is processing your raw data into something that you can extract value from., , This stage is all about extracting insights from your data., , In the fourth stage you want to put the insights to action., , This fifth phase runs parallel to all others and is about getting rid of the unwanted, unclean, unnecessary parts of your data, invalid insights and predictions at every step of the process.,A lot of 'Big Data' attention is given to the first two stages: , and ,. This is where all the Hadoop, Elastic Cloud computing, Map Reduce talk is concentrated. A lot of VC money (round about $1 Billion!) is competing for a piece of that cake. But what you really need is a quick way to check if you can assimilate new insights and create the value that your business needs. That is the exciting part! So you really want to focus on , and ,, to taste, test and prove. Focus on the business value and work your way back to technology. How? In one of our next posts we'll guide you through some sensible steps to get started with your (big) data.,At BigML we take pride in building a smart, simple-to-use platform so you can create new and actionable insights in a few clicks. And it doesn't require your company's capital to invest in cutting edge technology. We've done that for you. We will enable you to find new insights and put them to action as soon as possible, maybe in a small corner of your business, to make sure you are doing the right things. From there you can expand, discover, make mistakes and find where the value is.,Let's say the word: we think capturing big data and big data processing is hyped. It's great technology, sure. But it needs to serve a purpose: creating new insights out of big data and putting them to action. Let's focus on that and the rest will follow as and when you actually need it.







 , , 

Big data is the most talked term these days in the analytics world. It will have big transformative impact on all the aspects of the business., Big data help make better decisions ?€? faster, more efficiently with higher quality.
Along with several others, Harvard Business Review has recently pointed out an area with significant job growth which is appropriate for individuals with a curious nature and an expertise in business analytics. But who will dominate this area? The Data Scientist- trending as ?€?,?€? in America, this role has a desirability that calls upon experts in analytics and in high ranking professions to make BIG discoveries within Big Data. What does the Data Scientist do? What are the types of skills does one need to fulfill this role?,The data scientist embodies the role of the analyst, the communicator, the trusted advisor, the programmer, and the consultant all in one. The data scientist has an expertise in statistics for extensive predictive modeling analytics. Most importantly, intense curiosities with the passion to investigate all angles and provide creative crafty solutions to companies. With all of these skillsets , can generate innovative ideas by analyzing the voluminous amounts of data that pertain to a company., identifies opportunities through mining of the data and delivers a list of hypotheses, theories, indicators, and questions to strategize improvements with company leaders. Otherwise stated, the data scientist connects the dots for a company by analyzing their constant flow of data and improves their business model by pinpointing the gaps. ??That is why it is important that data scientists are experienced professional that understands the business world and its?€? challenges with the seniority and credentials to engage executive colleagues and help monetize on these gaps.,One example of many is Amazon leveraging data scientists to figure out strategies to improve their bottom line. After much research, Amazon produced a way to use personalized analytics to recommend items to costumers based on what they and other customers with a similar profile have previously bought. Furthermore, many companies that are submerged online use personalized analytics to help consumers find new endeavors. Such as:,But is the Data Scientist really a new role in corporate America or is it just a renaissance for the Statisticians and PHDs who now have new tools that enable them to harness the wealth of data that is being collected under the ?€?Big Data?€? umbrella??? Marketers are coupling their analytics with the cyber tools that enable near real time interaction with the customer.?? In the old world ?€?data scientists?€? ??developed ?€?Cross-sell?€? and ?€?Up-Sell?€? models that drove direct mail and telemarketing efforts.?? In the new world data scientists are essentially doing the same thing, building models that can run real-time against more robust, richer datasets.

??,By Jeff Hasen, Chief Marketing Officer, Thought Leader , and author of ,I?€?m in the camp that says mobile?€?s promise lies in our ability to deliver ultra-personalized, contextual content to a wireless user who is increasingly expecting nothing less.,??,I?€?m less sure about how patient the mobile subscriber will be as we wrap our arms around the Big Data and all that it tells us ?€? if we take the time, and have the right resources, to listen.,??,Julie Ask, one of the smartest and most respected in the industry, is one of the biggest champions of the context concept.,??,???€?What will it mean in five years? Consumer desire for convenience will trump their need for privacy,?€? Ask, a vice president and principal analyst at Forrester Research, wrote for Forbes. , ?€?They will gradually allow access to this information residing on their phones to trusted partners in exchange for convenient services not unlike the use of credit cards today. Content and services will become highly personalized. The phone will be a both a hub collecting information from machines around us and a modem relaying it to applications or services that will leverage it to offer convenient services.,??,?€?The ability to deliver highly contextual experiences will evolve in sophistication with technology in the phone. Already, phones have GPS, accelerometers, gyroscopes, and magnetometers. Going forward, they will have barometers, chemical sensors, and microbolometers. They already have two cameras enabling 3D video capture and distance measurements.?€?,??,??,??,OK, that?€?s a look five years out. But marketers interviewed for my , said that mobile subscribers are punishing brands that fail to deliver a positive experience on mobile. Conversely, the users are rewarding companies that meet or exceed their expectations.,??,For years, we have heard marketers not only ask but demand data from mobile programs to make the efforts more successful and trackable. Few organizations, however, have the infrastructure, personnel, budget, or mindset to take in as much information as possible and to do something meaningful with it.,??,Still, the technology is advancing so rapidly that there are fewer and fewer inhibitors to getting the data.,??,I?€?ve heard Ask discuss the concept of presenting to the mobile user information or ads based on whether it?€?s warm or cold in the room that they are in.,??,Others also believe location is only part of the story.,??,?€?I see the importance of context growing more and more and beyond just location,?€? Michael Becker, the North American Managing Director for the Mobile Marketing Association, told me in Mobilized Marketing.,??,?€?Time will be the next access that will take a big role in our conversation,?€? Becker said. ?€?It?€?s not just a matter that I?€?m in Times Square but when am I in it, because the engagement around you is different if I?€?m standing in Times Square at 12 in the afternoon versus 12 at night. How do we play that role and have that level of context with consumers??€?,??,There is plenty of data to be gathered and analyzed there even before confirming that the mobile user is okay with being reached in such a personal way.,??,?€?We?€?re going to see the idea of permission marketing go beyond I got your opt in or opt it.,?€? Becker says. ?€?There are going to be layers of permission. When can you talk to me? On what subjects? And on what devices and mediums??€?,??,There is also a layering in of other factors, according to Hipcricket CTO Nathanial Bradley.,??,?€?If you look at the progression of that ambient targeting, it has to know whether the sun was shining when you invoked a mobile marketing campaign, the last time you bought donuts it was raining, whether the stock market was up or down or whether your sales were up or down during a particular marketing campaign or mobile delivery,?€? Bradley told me.,??,?€?All those ambient conditions contribute to a targeting that will become more and more enhanced. You can see in the future that if I picked up your cell phone by accident, it would be absolutely worthless to me because of the amount of targeting and the amount of customization of content that goes from device to device.?€?,Of course, the challenge will be taking in the data, analyzing it and responding appropriately. The mobile user is already expecting nothing less.,??,??Originally published on??

Peter Bruce, president of Statistics.com offers his comments:





My publisher (John Wiley & Sons) is allowing me to make the 28-page Introduction of , available for free download. The Introduction provides the structure for the rest of the book and details a few interesting ways in which organizations are utilizing Big Data.
??,A social network is a social structure that links actors (individuals and organizations), and highlights how actors are connected, from a simple relationship to family ties. We are all part of many networks that correspond to the dimensions of our life (family, study, work, leisure activities). Our membership, our activities, our place in these networks are for marketers a valuable source of information, knowledge and opportunities for actions to promote their offers, according to the principle that the behavior of individuals is in part related to the structures in which they operate.,The internet has facilitated the development, the life of social networks, and marketers have acquired the appropriate analytical techniques to exploit them. In fact, a network can be represented as a graph and be mathematically analyzed. In these approaches, the actors are nodes and relationships are links, forming a model where all significant relationships can be analyzed via the construction of a matrix to represent the network. We can then obtain a graph using mathematical treatments performed on matrices, and search the presence of a clique, a chain, a cycle that characterizes the network. Finally using algorithm it is possible to calculate the degrees of strength and the density between social entities, for example, determine the social capital of actors.,There are many measures of connections, distances, power, prestige: the number of nodes, number of links present versus the number of links possible, the sum of links to other members, the degree of density, degree of cohesion, betweeness, path length, the degree to which any member of the network can reach other members of the network, structural holes, etc.. Thus we can characterize both a network and each actor, for example, to identify key people who have an important role in communication and influence.,The analysis of social networks is valuable for example for controlling the flow of information, improve / enhance communication, improve the resilience of a network, find community, or to trust a community. For marketers this is an opportunity to better understand, target, approach clients, prospects, suspects, to sell them more, for better lead communities, to innovate, differentiate themselves from the competition and develop a competitive advantage.,Companies, such as Myspace, LinkedIn or Mzinga, have understood the importance of this type of approach and already widely practiced social network analysis, launching new products, improving the experiences of their customers and satisfying them better. Mzinga in particular, whose business is to provide a means for facilitating communities of clients, provides tools for network analysis. Thus the 14,000 communities, with 40 million people who are managed with Mzinga tools, can be analyzed by their leaders and allow them to optimize their operations.,But be careful, for analysing social networks you must use others solutions than traditional analytical approaches based on relational databases and BI tools. The companies mentioned above already practicing this type of analysis, have had to develop by their own their analytical applications. They use particularly new kind Hadoop solutions and / or Teradata Aster and implement MapReduce programs that involve specific facilities and specialists in this type of analysis. To go further on the experiences of the companies mentioned above, you can access useful information about them via the link below:,??
 , , 

 , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 
We have tried to synthesize the most disruptive big data use cases into a compact . 3 Minute video,at??,It covers 6 use cases , 4 healthcare data streams and hopefully sets the stage for curating more use cases in an area which truly needs a lot of healthy transformations !!!
As I mentioned in our post on??,, we've built many predictive models in the Rapid Insight office. During the predictive modeling process, there are many places where it's easy to make mistakes. Luckily, we've compiled a few here so you can learn from our mistakes and avoid them in your own analyses:




Next-best offer refers to the use of predictive analytics solutions to identify the products or services your customers are most likely to be interested in for their next purchase.,??,Facing this topic I have made a personal research, and realize a synthesis, which has helped me to clarify some ideas. The attached presentation does not intend to be exhaustive on the subject, but could perhaps bring you some useful insights: ,??
Hello, Imagine being able to do anything your mind can imagine, well now it can.. , The , , Form is not a TERM.. it is a GOAL.. the "holy Grail" if you will of data management and more importantly "Information" management.. Imagine being able to Store IDEAS as opposed to disconnected bits of data.. , A brief "INCORRECT" comment on WIKI.. "A relvar R [table] is in sixth , form (abbreviated 6NF) if and only if it satisfies no nontrivial join dependencies at all ?€? where, as before, a join dependency is trivial if and only if at least one of the projections (possibly U_projections) involved is taken over the set of all attributes of the relvar [table] concerned.[Date et al.][" , The TRUE definition of , , form is OBJECT Database.. where each and every piece of information/data is ATOMIC in nature and can be associated with any other piece of data/information, and thus NO restrictions.. NO constraints.. NO tables, NO Rows, NO VIEWS, NO CUBES... the correct term is "ASSOCIATIVE DATABASE" or Information system as it is in 3 dimensions by default.. and technically (N) dimensions , the advantages are thus: , 100x SQL/ROW/TABLE speed , 1/3 disk space , 1 EXABYTE capacity - Single instance storage.. (no piece of data is ever duplicated) , Security - Un-hackable - there is nothing to hack into , NO QUERIES - we use filtering , NO TABLES - thus no indexing to worry about , Automated data aggregation - as many sources as required.. , and that is just the start.. ;-) , let me know if you are interested in seeing it.. ? as a scientist. I think you would find it fascinating.. Data warehouses have met their match, send me your external email and I will send you more info if you like, and yes this is going to market as we speak .. , JM , 917-751-3131 , I have posted a video that explains & demos a great deal, but If you want more information, I can add you to my Dropbox.com shared folder ifyou would like more information and a video is available there as well??
 , 
My new blog post on what I coined as "sparkgrams".?? Included is an implementation in YUI3 for custom website presentations of data, but I wish R and iPython Notebook had similar functionality.



Fari Payandeh,Aug 12, 2013

Hi??,check out the demand response market profile at PJM??

??




 
I have an interest in optimization models as well as analytics.?? My son, who writes apps for the trucking industry, sent me a link to an article on how , may impact the "best" solution as opposed to the "optimal" solution to a class of problems.,So too our analysis may lead us in a certain direction but bring us to the wrong conclusion because important factors were not considered in the initial analysis.


Breaking new story. Those bad gurus are behind bars. The process was so simple that becomes genious:,The men used relatively a basic ?€?SQL injection attack?€? to crack company servers,??giving access to internal databases.,Once inside the network, they used malware (malicious code) to create a?? ?€?back door?€? that gave them return access, even after some companies identified breaches and thought they had fixed them. Then they installed ?€?sniffers,?€? or programs to identify, collect and steal vast amounts of personal financial data, individually known as dumps, that they secreted in a network of computers around the world.?€?Those dbs (databases) are hell big ?€??€?,Question: what are the security of NASDAQ doing? why was it not doing its job when the malware crack open the system?


A while back I was running a data mining project for a customer and made a conversational blunder. In one of the meetings, I mentioned seeing one interesting relationship in the data. Customers who purchased one particular product tended to buy and implement a second product at a later time. I did not realize that Everyone in the room INTUITIVELY knew that there is absolutely no relationship between the two products. A big blunder. After the meeting, two friends told me that my standing in the whole organization decreased because of that one statement.,Two lessons are worth mentioning here. First, know your audience. Second, and more importantly, know how to influence the amount of freedom in the room and increase it before making a statement like I made. My mistake was not in stating the relationship in the data; it was the timing of the statement. I could have done more to prepare the audience so that the statement would have been appreciated more. This preparation consists of increasing the amount of freedom of expression in the meeting.,Freedom of expression is critical to thinking creatively during an open discussion. How do we increase freedom of expression? We increase it in two ways that corresponds to the two forms of freedom: Positive and Negative. Both are good, and both are needed. Positive Freedom is the ability to change under your own power. For example, a car with an empty gas tank has low Positive Freedom. Negative Freedom is the lack of obstacles in your way. A car stuck in the mud, deep the woods, away from the road has almost no Negative Freedom. A car with gasoline and on the freeway has both forms of freedom.,Social discourse, which is critical to problem solving and innovation, needs both kinds of freedom, just like our example of the car. Positive Freedom comes from information; current, trusted, applicable information. This information is needed for the conversation to have importance. This potential impact is what makes Data Science important. Information is fuel to a conversation as gasoline is to a car. It creates Positive Freedom.,Positive Freedom alone is useless and can be damaging. I thought I was adding to the conversation when I made my blunder about the relationship in the data. I did not realize that the low amount of Negative Freedom in the room created a hostile environment. It was like driving a car into a tree: ample Positive Freedom but no Negative Freedom. I first needed to work on the Negative Freedom in the room, then work on the Positive Freedom.,When in a meeting, watch who speaks and when. Are only a few people allowed to ask questions? Is anyone ever discounted? Do you hear words and phrases like ?€?obviously?€?, ?€?we all know that...?€?, ?€?never?€?, and ?€?not possible?€?? When this happens, your job is to encourage others to speak out, to give credit to creative statements, and to acknowledge innovative ideas. Support others and acknowledge brave statements. Spend time removing obstacles, increasing Negative Freedom, and encouraging others to participate. Then, and only then, add fuel to the conversation.,By managing the amount of both Positive and Negative Freedom in the room, your chances of successfully influencing the discussion and resulting decisions from your work as a Data Scientist improves. Your impact on the organization on the decisions, as well as the process of decision-making, becomes positive and compounded.
There have been various attempts to integrate the D3.js visualization framework into iPython Notebook, in order to provide more visualization options than available with the standard Matplotlib. In my blog post today, I take one of the better integration attempts out there, port it from Windows to the Mac, and demonstrate:,1. Passing a Pandas Dataframe from iPython Notebook into the D3.js Javascript,2. Generating geo color maps in D3.js (not a built-in capability) by pulling geo shape data from Wikipedia on the fly.,3. The resulting visualization is inline in the Notebook (not a floating frame or window) and printable into PDF (with the limitation of max one visualization per PDF, and needing to use Firefox to do so).,First-class support for D3.js won't be coming until iPython Notebook 2.0, and 1.0 isn't even out yet after 10 years of development. In the meantime, it is necessary to use work-arounds such as the ones described in my blog post.
When was the last time your car made a funny noise or ran a little hotter than usual? Chances are, you quickly consulted a web browser for answers and found your way to a forum full of similar consumers and car enthusiasts, all eager to get answers to questions and argue over best solutions. A few pages through the conversation and you probably end up with a fairly educated guess as to whether a trip to a mechanic, or a car parts dealer, is in order.,Forums like the one mentioned are an invaluable resource for segments and communities of people across nearly every industry or interest. These focused conversational microcosms can be created for virtually any purpose or topic, and most usually allow concentrated attention on specific questions.,From businesses using forums internally, sharing common IT issues, to fans of a specific book or movie genre, these forums contain interesting conversational patterns that could enable their parent organizations to better understand customer and worker needs, motivations, and desires.,When you know how conversations are connected, the chance to better serve those involved in the forum emerges?€?as well as a unique opportunity to develop other forms of value from the conversations.,.

Yvonne Buysman, Sports & Fitness Fellow for The (ART+DATA) Institute helps us understand how the data helps design a better round of golf. Her writing reveals the effect of an instant feedback loop in golf . .,Data Analysis in golf : Designing peak performance,By Yvonne Buysman, Sports & Fitness Fellow,As a golfer on the eternal quest to improve my game, trialing new clubs and receiving instant feedback from cutting edge golf simulators trumps the Tom Hank?€?s FAO Schwarz toy piano scene in the movie ?€?Big?€?. Edwin Watts, in Orlando, is an example of a golf shop, supporting the club fitting process with an innovative simulator calculating swing angles and lofts. The swing, brand of clubs and courses left on the bucket list may top golf conversations, but numbers are the underlying factor. 18 holes, 9 holes, handicaps, and wedge degrees bring out competitive streaks and bottom line in golf ability. However with leading technology, numbers and data analysis in golf have created an entirely new and powerful language. Players, manufacturers, and coaches can start interpreting data for opportunity.,Martin Kaymer?€?s final efforts in precision at the 2012 Ryder Cup may have been the result of practicing with advanced data analysis equipment such as Trackman Pro. (,)The innovative tool delivers state-of ?€?the art data by displaying a golf shot?€?s trajectory in real time along with impact and ball flight information. Detailed data such as club speed and path, spin axis, smash factor, and hang time is being captured in every shot.,??????????????,As Tiger Woods fights the good fight to climb back towards his top ranking, he embraces data to help understand his standings. Attack angles, ?€?D-planes?€?, and how numbers relate to ball track and spin can help change a player?€?s game. PGA Professional, James Leitz, describes the D-Plane as a physics model that is simply ?€?The Descriptive Plane,?€? ?€?It illustrates why the golf ball flies the way it does because of impact.?€?In Tiger?€?s testimony of using innovative technology such as Trackman Pro, he admits every degree is critical to the universal law of numbers and makes a difference in the game.,Golf is about consistency and precision. Trackman?€?s philosophy is to support recording the rhythms and patterns in players swings, so changes can be made based on the quantitative analysis. When winners of the British Open such as Tim Clarke, or champions of the US Open such as Lucas Glover and Graeme McDowell, take action in using measuring equipment, the results speak for themselves. Data analysis tools such as Trackman provide proven change management to achieve performance and accurate swing and flight ball analysis.,To accommodate the mobile lifestyle, Trackman offers a version with full wireless control catering to tablets and smartphones via the WiFi enabled radar. Truly offering insight and a chance to witness current behaviors, built-in hi-speed camera and performance studio software allows the chance to integrate video with data. The PGA Teaching and Coaching Summit have included D-Plane and new flight laws to meeting agendas. Leitz is a firm believer in the benefits of technology and performance. ?€?Any Summit should always be cutting edge,?€? said Leitz.,????????,?€?Any teacher should continually seek better information, then filter it and present it to his or her students. I also have the distinct honor of presenting such information to enlighten my fellow professionals.,??????,The D-Plane concept was first written in the ?€?Physics of Golf?€? by Theo Jorgenson.He discusses mathematically what occurs and how the ball takes off after separation from the club face. Teachers have discussed the relationships between collision, friction, and energy. Trackman has helped verify best starting positions for ball face. New technology is supporting Ball Flight laws and playing a handshake in physics and mathematics. Again, golfers love talking numbers, and D-Plane analysis may make the conversation on the 19th hole especially interesting., The D-Plane concept has opened questions in need of 3 dimensional answers. The true path of a swing is a combination of up and down in motion with left and right.Hence a video?€?s 2-dimensional reflection cannot capture a 3-dimensional action. The development of revolutionary solutions such as Trackman and Flightscope(,) can provide true insight and the chance to impact swing. If Ben Hogan had the access to such modern inventions as Trackman, then he may have included it in his legendary golf book ?€?The Five Fundamentals?€? where he addresses grip, knees, arms and hip rotation contributing towards ball impact.,As a tour player desires the perfect club fit and an equipment manufacturer crafts to enhance performance and brand, new golf design language emerges. ?€?Degrees of adjustability?€?, ?€?deeper heal and toe?€?, ?€?lighter weight swing weight screw?€? and ?€?refined cut slot creates a higher launch angle?€? are just a few phrases pinpointing design tactics for precision. Often technology analysis drives the new design and goal for top performance.,Tracking and data analysis support is not only improving equipment in the world of golf, but also for major sports venues such as the creation of Adidas?€?s FIFA soccer ball for the World Cup 2010 in South Africa.,From its origins in 15th century Scotland, the game of golf has rested in principles of tradition. The laws of physics and math linking to D-plan philosophies and the developments of sophisticated data analysis equipment are welcome friends to the sport. The language of golf is taking the foundations of numbers and encouraging a fluency of terms in swing direction, spin rates and landing angles. When trying to fix your slice, fade, draw or hook, you may want to consider analyzing quantitative data on your swing made available by cutting edge technology and begin walking the new talk in golf and data.,????????????
For many years, manufacturers have been trying to outperform the competition by offering up to five warranties on a car, for example, overusing the warranty argument in their marketing. Moreover, warranty costs have gone through the roof and repeated callbacks have ended up tarnishing the image of many companies. Therefore, it is obvious that their favorite integrated management software is not going to solve the problem, as it requires the integration of data from the entire company and all third parties involved (suppliers, distributors, repair companies, etc.). A data warehouse is the only way to handle such integration and actively steer warranty and quality.,The stakes are high, as they represent more than 2% of the turnover of companies such as General Motors, Caterpillar and John Deere. In the past decade, the number of callbacks has gone up four-fold, with 2011 being an all-time record year, which in turn makes analyzing faults increasingly difficult. In the car manufacturing industry, the average number of defects is 133 for every 100 vehicles, while callback campaigns still last an average of 250 days. Finally, the reserves that companies need to protect themselves against the risks of a faulty product they sell are about ??? 300 per vehicle a year. The problem is the same in other industries, especially high technology suppliers.,Today, companies need to reduce related costs and warranty reserves, while improving their products and processes if they are to provide better service to their clients and indirectly protect their market share. In concrete terms, quality-related historical data should support the process of diagnosing recurring breakdowns (per part and manufacturing series) and costs should be passed on to the parties responsible (namely external suppliers). As well as best practices, low performance levels and training needs must be identified, while conclusions should be reached for designing and manufacturing new products.,As for the data warehouse, information from a wide variety of systems needs to be integrated in order to be detailed enough to support in-depth analysis of the various parties involved. Moreover, it should allow for quick analysis of a large volume of data and not be restricted when analyzing information from different functions. Implementing such an approach must be carried out progressively and, in the beginning, priority should be given to improving the search for the source of problems in order to speed up warning processes, restricting and targeting callbacks and improving the comprehension of incurred costs.,The problems of such a project are not technical in nature, since in this field it is important to make the right decision when preparing a company data warehouse in order to avoid the impasse of a data mart approach. Feeding, modeling and implementing various data mining and capturing tools are relatively simple projects that can easily be carried out and monitored. The real challenge lies in cross-company team-work where everyone needs to work together. This includes the brand image and communication unit, the different client-oriented services (before, during and after sales), buyers, manufacturers and financial services, which all need to define a company-wide warranty policy together and allow time for things to solidify.,Many of Teradata?€?s clients, such as Ford, Western Digital and Whirlpool are leaders in this approach and preliminary projects show that a first iteration can easily reduce warranty costs by 5%. Through contacts with leading clients, the Teradata manufacturing team has acquired solid experience and an excellent ability in supporting potential clients with their decisions, providing them with a well-adjusted, concrete offer which includes specific data modeling and analytical applications as well as our well-known infrastructure solutions (MPP server, Teradata database, etc.) Find more information on this topic on Teradata.com.


One of the weaknesses of marketing has always been overgeneralization. Marketers have forever relied on grouping people into categories based on common characteristics, like age, gender, or household income, and then treating all of those people as the same ?€? also known as stereotyping.,However usually they?€?re similar enough that you can get away with grouping them together with decent results.?? But what if you actually had the ability to sell to an individual person based on their exact circumstances, instead of treating them as just another member of a group??? That?€?s the promise of Big Data, and while we may never get to a scenario where everyone is sold to as their own individual identity, Big Data certainly brings us much closer to that ideal than the crude, broad-strokes marketing segments that have been used traditionally.,In his seminal book ,, Chris Anderson pointed out that people?€?s interests are actually far more diverse than marketers have wanted to admit.?? The reason that mass marketing worked in the past was because businesses had been dealing in physical goods, and because when you?€?re dealing in atoms there are necessary limits placed on distribution.?? At least in this universe, only one physical object can occupy a space at a time.?? Putting one item on a shelf means you can?€?t put any other items there.?? And since that shelf space also has a cost, like rent and labor to stock it, in order for a retailer to be profitable they only want to carry items that would bring in more revenue than it cost to own and manage that shelf space.,These economics of physical products led stores to only carry the items with the widest appeal, and therefore the best likelihood to sell enough to cover the cost of their shelf space.?? ,, but instead that they bought the best possible product for their needs , , Since there was necessarily a reduced selection, because we were dealing with physical goods, this meant that marketers could be successful selling mass products to broadly defined demographic groups.?? Those audiences , to buy from someone, since more specialized products didn?€?t exist because they couldn?€?t be distributed profitably, so marketers could get away with using generic audience segmentation.,We have moved into a world dominated by the digital.?? Digital products have completely different economics from physical products.?? With physical goods, if one person buys a particular box of cereal, no one else can own that same box.?? It?€?s a zero-sum scenario, whereas with digital goods if one person downloads an ebook, it doesn?€?t do anything to reduce the supply of that ebook.?? It can be downloaded as many more times as it needs to be, without ever reducing the availability.?? This means that there is in effect an infinite amount of shelf space in these online stores, and it has allowed the number of products offered by online business to exponentially increase.?? And what Chris Anderson found when researching , is that there , demand for these products, and when given the choice consumers are far more individualized than the old marketing models suggested.,Big Data is how successful companies are building more detailed models of consumer behavior.?? Instead of relying on the traditional demographic models that marketers used when we were operating in a mass consumption environment and had nothing better, Big Data capitalizes on developing market trends to allow businesses to become far more specific when segmenting their customers.,The digitalization of society means that there has been an almost obscene increase in the amount of data available to corporations, and they?€?re now struggling to make sense of it in order to turn a larger profit.?? For example, did you know that, if you add up the amount of information created by , from the beginning of time until 2003, it is , that we create , With this explosion of information, for the first time businesses are in a position to understand their customers in much greater detail.,Think about Amazon.?? Instead of defining customers simply by their demographics, they can now target them based on their behavior ?€? ,?? Amazon can see the products you browse, suggest similar products, show you what people who viewed the product you?€?re currently evaluating ultimately purchased, show you what products people usually purchase together, and if you don?€?t buy, they can send you an email with products they think you will purchase based on what you?€?ve recently looked at.,Think about how sophisticated that is compared to how traditional marketing worked.?? Amazon doesn?€?t care if you?€?re a black male in his eighties, a white female in her teens, or a Latin American female in her thirties.?? If your browsing and purchase behavior fit their predictive models for what they know will sell, then they can show you targeted communications that introduce you to those items.,So while you may still be treated as a predictive output of a Big Data customer segmentation algorithm, it?€?s a lot closer to being treated like an individual than consumers were before.,Originally posted on ,.
??,??,??,??
 H1b is a Visa program where people from outside countries are allowed to work with USA companies. The visa is valid for a 3 year period and is extensible for another 3 years.??, publishes quarterly data of H1-B and other visa applications. Below is an analysis and visualization of the same data.,The data is available as CSV files. Each file contains around 500k records. It contains details of each application whether it is approved, rejected or withdrawn. It has the name of the employer who is sponsoring the H1-B visa, salary range and location of the employer. It also contains the duration for which the employer intends to keep the h1-b visa candidate.,The salary/wage is not uniform. For most of the rows, the salary is listed as annual/yearly wage. But then there are applications where the wage is listed weekly, bi-weekly, monthly. So to analyze the data, we first need to normalize the salary. Another interesting information they have is the duration of each h1b visa application. So this becomes easy to normalize the salary by "Daily" and then if we multiply by number of valid days we get what the company will be spending on each candidate for the entire valid stay of the visa.,Once the data is available in SQL tables, it is now easy to generate city level aggregation. This data is then matched with zip codes to generate a county level aggregation.??,??,Higher the count of the h1b approved applications darker is the region mapped.,Next, State level metrics,We can now aggregate the metrics by state and generate the below visuals,Finally, the below visuals helps us understand the top 100 employers and job titles. It is interactive where you can switch between job titles and employer names.,Here is the list of top 100 companies that is hiring h1b candidates,- Infosys Limited [ Jobs=7766 , Pay = 1983.8 million],- Wipro Limited [ Jobs=2401 , Pay = 485.7 million],- Tata Consultancy Services Limited [ Jobs=1641 , Pay = 282.1 million],- Accenture Llp [ Jobs=1227 , Pay = 1305.6 million],- Intel Corporation [ Jobs=911 , Pay = 1358.2 million],- Cognizant Technology Solutions U.S. Corporation [ Jobs=698 , Pay = 124.4 million],- Hcl America, Inc. [ Jobs=694 , Pay = 150.1 million],- Igate Technologies Inc. [ Jobs=578 , Pay = 702.2 million],- Larsen & Toubro Limited [ Jobs=570 , Pay = 74.1 million],- Larsen & Toubro Infotech Limited [ Jobs=551 , Pay = 94.4 million],- Oracle America, Inc. [ Jobs=497 , Pay = 147.8 million],- Ibm India Private Limited [ Jobs=492 , Pay = 77.4 million],- Qualcomm Technologies, Inc. [ Jobs=407 , Pay = 112.8 million],- Hexaware Technologies, Inc. [ Jobs=385 , Pay = 68.5 million],- Satyam Computer Services Ltd [ Jobs=359 , Pay = 76.2 million],- Ust Global Inc. [ Jobs=308 , Pay = 647.9 million],- Apple Inc. [ Jobs=293 , Pay = 101.6 million],- Ust Global Inc [ Jobs=291 , Pay = 53.1 million],- Ibm Corporation [ Jobs=267 , Pay = 73.9 million],- Deloitte & Touche Llp [ Jobs=264 , Pay = 51.4 million],- Deloitte Consulting Llp [ Jobs=255 , Pay = 929.3 million],- Jpmorgan Chase & Co. [ Jobs=250 , Pay = 70.4 million],- Microsoft Corporation [ Jobs=224 , Pay = 77.7 million],- Broadcom Corporation [ Jobs=222 , Pay = 73.2 million],- Ntt Data, Inc. [ Jobs=219 , Pay = 49.4 million],- Persistent Systems, Inc. [ Jobs=209 , Pay = 42.8 million],- Compunnel Software Group, Inc. [ Jobs=205 , Pay = 38 million],- Mayo Clinic [ Jobs=201 , Pay = 55.7 million],- Ericsson Inc. [ Jobs=200 , Pay = 42.8 million],- Johns Hopkins University [ Jobs=199 , Pay = 23.4 million],- Avant Healthcare Professionals [ Jobs=189 , Pay = 48 million],- National Institutes Of Health, Hhs [ Jobs=177 , Pay = 20.6 million],- Yash Technologies, Inc. [ Jobs=176 , Pay = 24.4 million],- Hewlett-Packard Company [ Jobs=175 , Pay = 890.2 million],- Cummins Inc. [ Jobs=171 , Pay = 34.6 million],- Mphasis Corporation [ Jobs=170 , Pay = 32.1 million],- Satyam Computer Services Ltd. [ Jobs=169 , Pay = 38.3 million],- Amirit Technologies, Inc. [ Jobs=169 , Pay = 24.9 million],- Schlumberger Technology Corporation [ Jobs=165 , Pay = 36.6 million],- Marvell Semiconductor, Inc. [ Jobs=154 , Pay = 40.6 million],- Capgemini Financial Services Usa Inc [ Jobs=151 , Pay = 29.7 million],- Amazon Corporate Llc [ Jobs=149 , Pay = 41.3 million],- Satyam Computer Services Limited [ Jobs=147 , Pay = 30.3 million],- Bank Of America N.A. [ Jobs=146 , Pay = 41 million],- Sapient Corporation [ Jobs=145 , Pay = 39.7 million],- Csc Covansys Corporation [ Jobs=140 , Pay = 26.9 million],- University Of Michigan [ Jobs=139 , Pay = 25.9 million],- Cvs Rx Services, Inc. [ Jobs=138 , Pay = 66.8 million],- New York University Hospitals Center [ Jobs=137 , Pay = 18.1 million],- University Of Washington [ Jobs=136 , Pay = 16.1 million],- Ebay Inc. [ Jobs=135 , Pay = 47 million],- Populus Group [ Jobs=133 , Pay = 55.4 million],- Infotech Enterprises America, Inc. [ Jobs=132 , Pay = 26.2 million],- Natsoft Corporation [ Jobs=131 , Pay = 640.2 million],- Texas Instruments Incorporated [ Jobs=130 , Pay = 38.1 million],- Cloudeeva, Inc [ Jobs=126 , Pay = 369.3 million],- Orian Engineers Incorporated [ Jobs=126 , Pay = 24.9 million],- Globalfoundries U.S. Inc. [ Jobs=124 , Pay = 29.5 million],- Smartplay, Inc. [ Jobs=124 , Pay = 35 million],- Trustees Of The University Of Pennsylvania [ Jobs=123 , Pay = 20.8 million],- Wal-Mart Associates, Inc. [ Jobs=122 , Pay = 43.9 million],- Washington University In St. Louis [ Jobs=120 , Pay = 18.3 million],- Northstar Group Inc [ Jobs=119 , Pay = 22 million],- Baylor College Of Medicine [ Jobs=119 , Pay = 23.3 million],- Netapp, Inc. [ Jobs=119 , Pay = 38.4 million],- The Ohio State University [ Jobs=118 , Pay = 21.2 million],- Mindtree Limited [ Jobs=117 , Pay = 550.2 million],- Erp And Erp Corp [ Jobs=116 , Pay = 22.3 million],- Ntt Data, Inc. (Formerly Keane Inc.) [ Jobs=114 , Pay = 26.3 million],- Baltimore City Public Schools [ Jobs=114 , Pay = 13.8 million],- Yale University [ Jobs=113 , Pay = 15.7 million],- Google Inc. [ Jobs=113 , Pay = 40.3 million],- Emc Corporation [ Jobs=113 , Pay = 28.9 million],- Dotcom Team, Llc [ Jobs=113 , Pay = 19.5 million],- Cisco Systems, Inc. [ Jobs=112 , Pay = 31.4 million],- Duke University And Medical Center [ Jobs=111 , Pay = 10.4 million],- Emory University [ Jobs=111 , Pay = 16.6 million],- State University Of New York At Buffalo [ Jobs=111 , Pay = 10.3 million],- University Of Minnesota [ Jobs=111 , Pay = 23.7 million],- Kpit Infosystems, Inc. [ Jobs=110 , Pay = 19.6 million],- Perficient, Inc. [ Jobs=109 , Pay = 25.3 million],- The Board Of Trustees Of The Leland Stanford, Jr. [ Jobs=109 , Pay = 24 million],- Astir It Solutions Inc. [ Jobs=109 , Pay = 19.5 million],- Verinon Technology Solutions Ltd. [ Jobs=108 , Pay = 21 million],- Rite Aid Corp. [ Jobs=108 , Pay = 58.4 million],- Indus Valley Consultants, Inc. [ Jobs=107 , Pay = 19.7 million],- 3I Infotech, Inc [ Jobs=106 , Pay = 19 million],- Dgn Technologies Inc [ Jobs=106 , Pay = 24.8 million],- University Of Maryland College Park [ Jobs=105 , Pay = 14.5 million],- Ciber, Inc. [ Jobs=104 , Pay = 25.4 million],- University Of California, Davis [ Jobs=104 , Pay = 13.8 million],- Syntel Inc [ Jobs=103 , Pay = 17.2 million],- University Of California, Los Angeles [ Jobs=103 , Pay = 17.3 million],- Onward Technologies, Inc [ Jobs=103 , Pay = 19.3 million],- Delasoft, Inc. [ Jobs=102 , Pay = 17.4 million],- Nvidia Corporation [ Jobs=102 , Pay = 32 million],- Childrens Hospital Corporation [ Jobs=102 , Pay = 13.2 million],- Merrill Lynch [ Jobs=102 , Pay = 28.9 million],- Infoysys Limited [ Jobs=100 , Pay = 24 million],- Zensar Technologies Inc. [ Jobs=100 , Pay = 22.5 million],Summary: , A total of 300,000 candidates were approved for h1-b visa. Around 8000 applications were rejected, around 9000 applications were withdrawn and around 26000 applications were approved but later withdrawn.,Links to all three dashboard visualizations
check out the main consumers of Natural gas during summer and winter. check out how the pattern changes . i have used data from EIA, improved upon it by comparing it with other sources and presented in a very easy to see form,really interesting. with more of NG supply coming up
Although Mainframe Hierarchical Databases are very much alive today, The Relational Databases (RDBMS) (,) have dominated the Database market, and they have done a lot of good. The reason the money we deposit doesn?€?t go to someone else?€?s account, our airline reservation ensures that we have a seat on the plane, or we are not blamed for something we didn?€?t do, etc?€? RDBMS' data integrity is due to its adherence to ACID (atomicity, consistency, isolation, and durability) principles. RDBMS technology dates back to the 70's.,So what changed? Web technology started the revolution. Today, many people shop on Amazon. RDBMS was not designed to handle the number of transactions that take place on Amazon every second. The primary constraining factor was RDBMS?€? schema.,NoSql Databases offered an alternative by eliminating schemas at the expense of relaxing ACID principles. Some NoSql vendors have made great strides towards resolving the issue; the solution is called ,. As for NewSql, why not create a new RDBMS minus RDBMS?€? shortcomings utilizing modern programming languages and technology. That is how some of the NewSql vendors came to life.?? Other NewSql companies created augmented solutions for MySql.,Hadoop is a different animal altogether. It?€?s a file system and not a database. Hadoop?€?s roots are in?? internet search engines. Although , have turned it into a mighty database, Hadoop is a scalable, inexpensive distributed filesystem with fault tolerance. Hadoop?€?s specialty at this point in time is in batch processing, hence suitable for Data Analytics.,Now let?€?s start with our example: My imaginary video game company recently put our most popular game online after ten years of being in business, shipping our games to retailers around the globe. Our customer information is currently stored in a Sql Server Database , and we have been happy with it. However, since the players started playing the game online, the database is not able to keep up and the users are experiencing delays. As our user base grows rapidly, we spend money buying more and more Hardware/Software, but to no avail. Losing customers is our primary concern. Where do we go from here?,We decide to run our online game application in , simultaneously by segmenting our online user base. Our objective is to find the optimal solution. The IT department selects NoSql CouchBase (document oriented like MongoDB) and NewSql VoltDB.,Couchbase is open source, has an integrated caching mechanism, and it can automatically spread data across multiple nodes. VoltDB is an ACID compliant RDBMS, fault tolerant, scales horizontally, and possesses a shared-nothing & in-memory architecture. At the end, both systems are able to deliver. I won?€?t go??into??the intricacies of each solution because this is an example and comparing these technologies in the real-world will require testing, benchmarking, and in-depth analyses.,Now that the online operations are running smoothly, we want to analyze our data to find out where we should expand our territory. Which are the most suitable countries for marketing our products? ??In doing so, we need to merge the Sql Server customer Data Warehouse with the data from the online gaming database, ??and run analytical reports. That?€?s where Hadoop comes in. We configure a Hadoop system and merge the data from the two data sources. Next, we use Hadoop?€?s?? Mapreduce in conjunction with the open source R?? programming language to generate the analytics reports.,See 


It?€?s Friday September 14, 2046, and you are at the airport getting an alert from your car that your 10,000 mile service is due, but something peculiar comes with your alert. It?€?s a question: your car asks you if you wish to have the service taken care of within the next 10 days, or after.,You respond ?€?in the next 10 days.?€?,Your car?€?s smart system contacts the car dealer?€?s smart system and arranges everything, but first it checks your calendar and figures out if you are out of town on any particular day. It discovers you will be traveling for the day on Monday the 24th, so it books your car service to the airport from the service department, your airline and hotel reservations, and your dinner reservations with your client. It also updates your social profiles for you while sending you relevant information about where you are going, whom you know there, whom you have not been in touch with for a while that may be in the area, and a plethora of other intelligence useful for you.,Simultaneously, while you are waiting at the airport the activity on your social networks suggests a problem with the recent roll out of your new product, so your business intelligence system at the office sends you an alert, also with a question: ?€?Would you like me to resolve this issue??€? You reply, ?€?Yes.?€?,The system engages with consumers to gather information on the issue, and then collects the relevant data points and alerts R&D, marketing, sales, and the CFO. This isn't just any alert they receive ?€? this is a smart alert with resolution suggestions and action items built into it.,You get on your flight, which was picked for you specifically with safety and punctuality data in mind. After 4 hours, you land and receive another alert at 6:15pm, saying, ?€?Product problem resolved, oh and by the way the car service for your trip on the 24,??has been arranged, along with 3 other items, when you drop off your car for service that morning.?€?,Sounds like a dream right? Or is it a nightmare?,??,There is enough data out there on just about everything from weather patterns, markets, brand sentiment, consumer behavior, company performance across multi-platforms and environments, how much energy you use, how often you watch TV, read, sleep, and so on. The data isn't missing. So what?€?s missing?,What?€?s missing is the connection point?€?s intelligence to make sense of it all in a way that can be applied to everyday life, but the discovery of the missing link is not too far way. In fact, it was already discovered almost 5,000 years ago and documented.,What am I talking about?,I am talking about quantum mechanics. The laws of quantum physics and relativity suggest that everything is interconnected, in one form or another, and that one action can cause a ripple effect reaction elsewhere (aka The Butterfly Effect).,?€?Wait a minute, you say, quantum science was not discovered almost 5,000 years ago, this is a recent discovery.?€? A book written almost 5,000 years ago called the Sefer Yetzirah (Book of Formations), suggests the interconnectivity of not only human beings but all animate and inanimate objects across the universe.,Simply put, everything has a pulse, an energy pattern, or one could say a soul. If all things are connected, then it would only be logical that data objects from either a car?€?s black box or an energy grid are also interconnected. Right?,If machines can be taught to develop relationships, and respond to changes the way we as humans do, would AI introduce a whole new world that drives us forward??? Notice I said ?€?relationships?€?, not just connection points, but as in emotionally connected relationships capable of reason, logic, and perhaps even feelings.,??,It?€?s been well established today that our surroundings influence how we manifest our own reality, and that what we think dictates how we experience reality. Long ago, we as humans did not operate this way, the words emotional intelligence (EQ), consciousness, and spirituality were absolutely foreign to most living humans. No thought was given to how one action could cause a reaction elsewhere, let alone any related consequences ?€? until a few (EQ) giants of philosophy and spirituality began to speak of the connection between love, compassion, fear, rage, and everything that makes the human condition ?€? well ?€? non-animalistic.,While machines are smarter than ever today, they mostly take commands and instructions without much thought, reason, or rationality behind the execution; in reality there is nothing smart about software at all, other than what we?€?ve programmed it to do, on the flip side machines have no egos ?€? they don?€?t need to justify a decision, think about their reputation, or plan for the future of their families.,Let?€?s go back to a time when we as humans were not that smart either with the exception of a few that were able to dominate, control, and often treat humans as if they were machines that can be turned on or off.,Knowledge introduced via works such as the Book of Formation was so beyond its time that anyone who possessed it was murdered, and it was forbidden to record it, hence it was passed down through oral tradition from generation to generation.??,During the renaissance era, the printing press changed everything. You could say that the media has facilitated the explosion of knowledge we often take for granted, and we are much more civil today than ever before ?€? except for a few leftover dinosaurs that still exist, we have all learned to develop a consciousness and understanding of importance in relationships.,Social media is such a big deal because it facilitates the fundamental human need to connect with each other and to experience life as we were meant to experience it: collectively, as one. Not to make this too much of an esoteric discussion, I think we've covered enough of that, but why can?€?t we teach machines to develop relationships and respond to circumstances?,I mean can?€?t an AI app follow me for a few months, figure out what I like to talk about, and tweet for me, post a blog, and initiate new contacts that would be relevant to me??? As in the example earlier, if the software program in the car is designed to have a relationship with you, it can be designed to have a relationship with other software programs to be useful and to add value by interacting with each other on your behalf.,??,AI may not happen in our lifetime for a number of reasons, but there exists no reason why we can?€?t connect big data, social media information and perpetually connected consumers in such a way to simplify life by enabling the ability to, for example:,1)???????? Predict weather patterns in order to effectively manage crops planning,2)???????? Discover faster ways to introduce cures by not only getting feedback during R&D, but from people?€?s postings and behaviors; as in if someone starts behaving erratically after going on some new medication, by posting ?€?out of context?€? (for them as an individual) information on the social media networks that data could provide useful feedback to the drug manufacturer that needs to be addressed before a crises happens, this would be possible by tracking its customer base via perpetually connected technologies,3)???????? Streamline how we do simple everyday tasks of living, as in car maintenance, house cleaning, laundry, shopping for food, planning travel, and booking meetings.,4)???????? A company planning an IPO could predict how it will perform, and may be able to make better decisions.,5)???????? A hiring manager may be able to identify how a new hire will fit into the culture, and align a career path that is appropriate.,6)???????? The airline industry may be able to respond to conditions in advance, to avoid the need for holding patterns and unscheduled breakdowns.,But on the flip side of the equation, this may introduce the destruction of free-will if not properly implemented by giving the user complete control over the personalized configuration.,In this future reality where big data, social media, perpetually connected consumers (PCC) and artificial intelligence merges, markets will drastically change, as in some industries will simply cease to be relevant. Industries that will likely be doomed include:,Conversely, the following industries will likely flourish:,As I started to write this, my intention was to think of a world where we could facilitate efficient use of our time with the assistance of technology, yet I am in a pickle now: the realization of AI may very well put an end to free will as we know it.
Over the last several months, as I looked at addressing the business needs across various industries as someone leading a team of?? Data Scientists, the question of domain expertise invariably cropped up.,Attending one meeting with a Pharmaceutical company, I was posed with the question of, "Have you done work in the areas of Rare Signal detection?" In a similar vein, while preparing for a meeting with an Auto finance major, the question was in the area of using Auto telemetry data and deploying it to work on fraud detection in auto-insurance claims.,Multiply the business problems with the numerous industries and the enormity of the challenge becomes apparent. More so since it may not be possible to be a domain expert in every possible industry. Which begs the question, is there a line between domain knowledge and domain expertise? ,While domain knowledge would refer to the appreciation of the industry, its business processes and challenges, domain expertise is expected to be much deeper. It would be addressing meaningfully, through the effective use of technology, a problem or two which a particular industry is grappling with.,Is the expectation then from the prospective customer 'You need to understand my industry language (i.e. domain knowledge)' or is it 'You need the expertise to solve an industry specific problem (i.e. domain expertise)'?,How then does one manage the expectations here? A typical Data Scientist needs to be adept in understanding the business problem, have a good handle of the data on hand, and have a grasp of the algorithms which would aid him/her in the journey of?? discovery, design, deployment and ultimately delivering?? the results .,These would come in various shades across Data Scientists. As the area becomes mainstream at a furious pace, primarily driven by storage and accessibility costs, the need to balance out the three, otherwise known as the ?€?Triangle of Intelligence", namely business (knowledge or expertise), data (content) and algorithm (thinking)?? will possibly decide the difference between resounding success and abysmal failure, while addressing a business problem.,I came across an interesting note by Caitlin Garrett in the ,Caitlin rightly mentions that as a practitioner of Data Science, it is mandatory to have analytical thinking, mathematical/statistical ability, a knack for communicating results to non-data people, and creativity. Her blog however does not make a reference to the business (knowledge or expertise).,Can we surmise then, that the ability to articulate and appreciate the business problem and marrying the expected ways to get this addressed through analytical skills could be a good starting point to provide that confidence to the business user that?? the?? problem at?? hand can be addressed? ,The combination of business acumen and technical skill isn?€?t easy to come by.,David Logan in his interesting blog:, ,has mentioned about the "Purple People", folks who are blessed with the business acumen and the analytical abilities. Purple is the blend of Red (Business acumen) and Blue (Analytical abilities) and we would by now know that this would be a very hard to get profile.?? ,However as this area matures and moves beyond the hype cycle, we may have the luxury of seeing experts who cover both areas well.,The question still remains. The profile as described is rare but the expectations of the prospective customers most of the time is?? "Can you solve the point problem which I have been grappling with for so long?" for which domain expertise and NOT knowledge would be required.,The only way to address this is to borrow the domain expertise from the prospective customer and build that from the base of domain knowledge one is expected to have and move forward. ,But as one is trying to demonstrate credibility to the prospect , this may be easier said than done. I look forward to hearing what my colleagues think about this conundrum. ,Posted by, ,Chief Business Officer
 U.S. , U.S. 

Statistics.com, a provider of online education in statistics and analytics, announces a partnership with CrowdANALYTIX, a predictive modeling ?€?managed crowdsourcing?€? company, offering a new online course, ?€?,?€?, which will run from Oct. 11 to Nov 8, 2013.,The goal of this course is to teach users (who have basic knowledge of R programming, predictive analytics and statistics) to apply machine learning techniques in real world case studies. This course provides a hands on approach, presenting the opportunity to participate in a private educational competition hosted by CrowdANALYTIX. , , Business Case Study: We will study data from the "daily deals" industry (consisting of websites like Groupon, Living Social etc. which source local deals to offer each day). The daily deals industry is emerging and highly competitive. The goal will be to predict the revenue from each offering using the given data. , , Course Walk through: Each week, participants will be given a set of exercises and instructions to work on the raw data or processed data (details below). Users will apply their statistical/machine learning knowledge, along with their business understanding, to solve the problems and interpret the modeling results for the given business objective. The course will follow an iterative approach for problem solving, in which users are required to submit modeling responses multiple times. 
It seems there are many books available and there are many on the topics of Big Data, NoSQL and Cloud, however for non-technical managers/executives, these sound Buzz Words. The book on this topic is now available at the given link:??,. This is a book for the readers, who want to explore the knowledge about Big Data, NoSQL (Not only Structured Query Language) and Cloud, which is a paradigm shift that we are witnessing in our contemporary world.,The types of data has gone off the charts and we deal with data in so many formats in one of our ordinary day, that once analyzed can easily depict our pattern to live on daily basis. This data is stored somewhere, and is being analyzed by the organizations for various purposes.However, as a consumer of this data these organizations than target specific audience, which might be receptive to their products, services etc. The impact of Big Data is a reality and we cannot simply turn away from this humongous data.,This book is a collection of articles with references provided and are written for anyone to read and establish an understanding, where he/she is heading and what can be the best course that can be taken to make sure his/her data is secure, private and not being manipulated by someone unwillingly of the owner?€?s choice.
~Regards,Ashish Soni,COSTARCH,...company of statistical research...
In general, computer scientists treats code and data in two very different??ways. Virtual memory was originally developed to run big programs (code) in??small memory, while data are entities kept in external storage and must??be retrieved into memory before computing. As a result, today?€?s application??developers think by instinct the programming model based on storage and??explicit data retrieval. This model, referred to as storage-based computing,??plays an important role and has done a great job in transactional applications??such as banking and ERP systems, where data integrity is the primary concern??and the data size (per transaction) is assumed smaller than the code size.,Since the last decade, the weight of applications has been gradually??shifted from ?€?transactional?€? to ?€?analytic?€? ones, and the data size has been??increased from a few kilobytes to megabytes/gigabytes/terabytes or even bigger,??while the code footprint remains relatively unchanged. The assumption of??the data size smaller than the code size becomes no longer valid. With such??landscape changed between code and data, storage-based computing imposes??serious performance issues as follows.,The worst case is the mixed effect caused by juggling and swapping,??leading to a special type of double paging anomaly.,Storage-based computing model has been deeply rooted in developers?€???minds for more than forty years even when the landscape is changing gradually.??By observing the shift in data size and code footprint from transactional??application to big data analytic, we raise the first question for big data computing:,To know more about the details, please find the technical whitepaper ,.


There are plenty of??,??about what data science??,, what??,??a data scientist, and how to position yourself as a??,??applicant. Far fewer resources exist that help aspiring data scientists acquire the necessary skills. Here we will provide a collection of freely accessible materials and content to kick off your understanding of Data Science theory and tools.,., , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 

Recently I've been pondering on the idea of synergy, which I relate to the Honeybee Effect. Also, somewhat related is Hadoop's module Hive, for querying large databases. Apparently, Facebook has made their own big data module for SQL-like queries (Presto) open-source about 7 days ago, making waves in the big data scene. What are your views on these matters?,Thanks,1. ,2. ,3. 






Few days back i have attended a good webinar conducted by Metascale on topic , This post is targeting this webinar.,In summary, this webinar had nicely explained about how enterprise can use Hadoop as a data hub along with the existing Datawarehouse set up. ?€?Hadoop as a Data Hub?€? this line itself raised lot of questions in my mind:,Now, having these questions in place doesn?€?t mean Hadoop can?€?t be projected as a replacement/amendment of existing datawarehouse strategy. On contrary, I could see some different possibilities and ways for Hadoop to sneak-in into the existing enterprise data architecture. Here are my few cents:,I think, effort to bring Hadoop to enterprise require diligent changes in datawarehouse reference architecture. We are going to change a lot in our Reference Architecture when we bring Hadoop into the enterprise.


Plotly has a new??,, which showcases some of the best graphs made to date with the product. You can make graphs like these with a GUI, or with code, which can be found and copied on our??,??(with support for Python, R, MATLAB, Arduino, REST, Julia, and Perl).,Please pass along any you make so we can showcase them on the site and to other folks using the tool.
Plotly has a new ,, which showcases some of the best graphs made to date with the product.,You can make graphs like these with a GUI, or with code, which can be found and copied on our ,??(with support for Python, R, MATLAB, Arduino, REST, Julia, and Perl).

Well, when it comes about Big Data processing, there are a few platforms like Hadoop, mongoDB and Oracle in-memory DB etc. However, I wonder why the platforms must be complicated ???,Someone think in a same way, please look at following.??,Cheers,



Check out this ,??on using Plotly bubble charts to display three or even four dimensions of data:,Below is a chart with the interactive, text on the hover experience you can get if you view the page.
What is Information System Governance?,The objective of IT governance is to maximize the added value of IT to the company strategy in its definition as well as in its application. It means it leads the developments, the implementation and the use of IT. According to the governance experts it has to be pre-schedule, planned, transparent, fair and efficient. Thus it is important to:,Organize a governance structure (setting up of a department including stakeholder?€?s representatives and an executive in charge of the governance).,Define processes (governance planning and organization of about ten key processes such as budget, services billing, demand management, project portfolio management, service level agreements (SLA), human resources...).,Set up goals and way to measure them on the following points: creation of value, risks, compliance, processes efficiency and financial aspects.,Business Intelligence Governance Specificity,The governance of Business Intelligence projects has to take into account the specificity of the study, the implementation and the use of BI systems. For instance, in most Business Intelligence projects users have challenges to supply detailed specification of their needs and have to pass through learning stages which imply the system being built slowly and in an iterative way. Besides, the BI projects are firstly centered on the integration of data coming from diverse applications, and this stage is always at the start of data quality problem discovery. To face efficiently this situation, a good governance system is necessary in order to organize dialogues between the different departments of the company and the IT department.,The BI Governance specialists recommend focusing the efforts on the following points:,??,To go further look at the Teradata view on this subject:,??
This is a short video that contains the criteria that I use while choosing the appropriate clustering algorithm. If you have other criteria that you use, please do let me know by leaving a comment on my blog or by reaching out to me on Twitter @VRaoRao Thanks!???
Please join us in New York at the Sentiment Analysis Symposium Customer Insight Analytics Workshop on March 5 where Steven Ramirez, CEO of Beyond the Arc, will be presenting ,The afternoon workshop will offer a thorough, practical look at elements that business analysts, managers, and executives must master to compete in today?€?s hyper-competitive, high-velocity, Big Data world: Unstructured Data, Voice of the Customer, Social Customer Insights, and Predictive Analytics.?? ??,The phenomenon of Big Data has captured the imaginations of executives across industries and during the workshop we will examine a data-science case study and Steven will highlight best practices of some of the most mature and successful Voice of the Customer programs.?? As a workshop participant, you will be introduced to a straightforward methodology you can deploy immediately.,?€????? ??How to benchmark your customer experience, marketing, and social insights efforts,?€????? ??How to accelerate the maturity of your program, helping you to deliver greater value to your internal business partners,?€????? ??How to prioritize your data acquisition and data management efforts,?€????? ??What types of analyses you can perform to obtain the richest insights,If you've been challenged with how to derive real, demonstrable ROI, and would like to learn more then the Customer Insight Analytics workshop is for you. ,After the workshop, continue to learn from other leaders in the industry, including Amazon, American Express, Huffington Post, IBM and more at the symposium on March 6. To learn more, please visit?? ,. 
I have been reading about??,'s new ,, which starts soon (,). HR Analytics is a remarkably interesting new field -- or, one could say, an old field with exciting new dimensions! The application of data science techniques to human resource management is truly making a science out of one of the most important (if not, the most important) part of your business -- the people! Several studies and articles have discussed this topic in the past 2 years, including:,The field was given a detailed blueprint in a visionary book from 2010: ","??(by??Jac Fitz-enz), which truly focused on HR Analytics as a data science, with metrics and models as the cornerstones of the book.,The rapid maturity in this field is now seen in the emergence of HR Analytics training programs. One of the best opportunities for such training comes from well known Data Science and Analytics Training provider, ,, who offers a nice variety of other courses for Big Data and Data Science practitioners: ??,, ,, ,,??,, and??, (the latter offered jointly with Great Lakes Institute of Management).,You can preview the plans and content for Jigsaw's new HR Analytics Certification course by examining their??,??[PDF]. ??, by Jigsaw.,Here are 10 attributes of this program that make it worth considering:,So, what are you waiting for? Give yourself a New Year's gift and register to learn the power of analytics and data science in data-driven human resource management. Enroll in??,'s ,??now (... online classes start January 26)!

Last year we posted a popular piece offering our view on defining the ,. Perhaps we should have added to that the quality: "during the NFL playoffs, they have one hand in the chips-n-dip and the other typing away in an Emacs terminal.",OK well it may not describe all data scientists, but at least some of ours couldn't resist the opportunity to analyze the gridiron action with code. Specifically in this case they applied language analysis algorithms to study tweets being exchanged during yesterday's NFC and AFC championship games. We occasionally tests its code on Twitter data, and??,.,Someone always pipes up to ask "why do you bother with this analysis?" Simply, Twitter offers a good source of high velocity un-polished language. Training natural language algorithms on polished prose like books is a very poor simulation of the types of free text we often encounter in our analysis (e.g., e-mails and free response fields within systems).??,Enough, talk... let's take a look at some of the data captured yesterday: ?? ?? ?? ????,Yesterday was all about determining who will play in the Super Bowl two weeks from now. In studying conversations throughout the day the evolution of that matchup becomes quickly apparent. The chart below looks at the relative volume of chatter by team discussing that team and chances of playing in the Super Bowl (times in EST for Sunday January 20th). Following the AFC Championship game, we see a big spike in activity for the victorious Denver Broncos. That chatter then calms down as everyone settled in for the NFC game to determining the Broncos' opponents. When the Seattle Seahawks emerged victorious shortly before 10 PM eastern time the Super Bowl chatter erupted again for both the Broncos and the Seahawks. ????,??

During an??,??I wrote about how the use of ?€?small data?€? can still have a big impact on an organization?€?s ability to drive performance improvement. While so called ?€?big data?€? techniques can be incredibly powerful, not every data analytics challenge is a ?€?big data?€? analytics challenge.,That said, we?€?re still asked nearly every day ?€?should we invest in big data [or insert name of generic big data technology]?€???€?,The first question you should be asking is ?€?Have we effectively integrated data science into the management of our organization??€?,You should invest first and foremost in establishing a data science program and, if your data science needs warrant it and your organization has access to the right expertise, you should consider investing in big data. If you don?€?t have a formalized data science program in place, then it?€?s imperative you start there first.,Big data technologies are a lot of things, but they?€?re certainly not plug-and-play. Even some of the nice end-user graphical packages available still require a lot of preparation on the back end to make sure those apps have nice data to chew on. If a big data vendor tells you their technology is so polished and slick that you don?€?t have to worry about what?€?s going on inside their ?€?proprietary?€? black box, you should probably be getting worried not excited.,Before diving into big data an organization needs to be??
 , , , 

I found a very good link which explains about Big data , Hadoop fundamentals and Map Reduce in a very simple manner. Hope this will help everyone . ,Email not displaying correctly? View it in your browser


I will not discourage you to put up application using linkedin job posts. However, I like to share my experience with linkedin jobs and other jobs sites utilization, that I did in the past. We all look for jobs and jobs are nothing but data, and these days, we can count these data as Big Data, as it seems every other organization is hiring, they need someone to serve. It seems linkedin jobs are always there and most of the time are recently posted by the employers.??,I am trying to apply using linkedin for almost a year, NOT A SINGLE reply I got in positive, NOT A SINGLE reply, that had a thank you and that the organization is willing to talk to me. This made me think many times that, why I am wasting time, however, when I started contacting my friends over the phone or, when I was speaking at some events about Cloud Computing and Big Data, people approached me in person or called me and we had discussions about the possible opportunities that I could explore.,Same old fashioned way of knowing people in person, made me get some interviews and later couple of job offers also, however I am still not disappointed, it is a possibility that the employers or recruiters might got afraid of either my name or my education or my hefty experience that I have of over 17 years, with two MS degrees in Computer Science and my PhD is about to be completed in Cloud Computing Secured gateway for Big Data Sessions.,This information that I just shared with you is not to impress upon you, it is to shed light that whoever might have seen my resume/CV possibly got overwhelmed or was afraid that if I might get in, how damaging that could be for some other executives, it happens (we all know the red-tapes or political comfort zones established by executives, no offense to them, it is always good to be secured with a job these days). I know in person an executive, who has no idea, what he is speaking, however just because, he is a charmer and can talk and talk and talk, does not let anyone else talk and builds castles in the air, is still surviving in an organization.,In my personal opinion it is wrong, if we cannot perform our job well, we should ethically try to either go back to school or get professional education to enhance our skillset or try to find some career, which is aligned with our skillset. This is exactky, what I have done, I did two MS degrees in Computer Science 10 years a part to make sure, I am not legacy, I feel good that I can still code and I can design an entire System's Architecture by getting details in depth involving as many stakeholders as I can.,Now let us explore Big Data, everyone is into it, however there are very few who actually know the depth of the ISSUE, which we call Big Data. This is an information explosion and the use of social media actually has spread like fire, and making life easy for a consumer, and creating BIG ISSUES for the service providers.,NoSQL such as DOMBA (,) or Neo4J or Cassandra or Mongo are trying to assist, with few experts in the industry of Data Science to analyze this Big Data and find the golden needle. So, shall we find (job seekers) our golden needle from linkedin or shall we resort to actually calling people to see, if someone open a door for us, this is the main question for you all, who have read this article or for linkedin to see, how they can improve.
Now that you have that job vacancy, this might be a good time to reflect and analyze what that particular position?€?s role is in the organization. ??This is a timely opportunity for the organization where managers can self-reflect and ask some important questions about the position and about its fit to the overall mission. ??When was the last time this position was filled? ??How does this position add value to my organization? ??What other skill sets do we need in our organization that we should look for in our candidate? ??The job demands do change over time especially considering the rapidly changing business environment we now live in. ??There is always new technology, new softwares, new rules and regulations, new kinds of certifications, etc., and job descriptions do get outdated. ??This new job vacancy presents an opportunity for you and some subject matter experts (perhaps your other employees more familiar with the vacant position) to revisit the position description and revise as needed to match the current needs. ??The organizational goals and strategies also do change so take those statements and crosswalk them with the job description to help ensure that there is a close alignment between the position and the organization?€?s mission. ??This should help crystallize what knowledge, skills, and characteristics the hiring manager should be posting in the job ad to ensure that the right applicants are targeted., , , , , , , 
A Social Network is a theoretical construction useful in the social sciences to study social relationships.,Social network analysis refers to methods used to analyze social networks, social structures made up of individuals or organizations, which are connected by one or more specific types of interdependency, such as friendship, common interest, financial exchange, or relationships of beliefs, etc.?€?,??,Facing this new domain I have make a personal research, and realize a synthesis, which has help me to clarify some ideas. The attached presentation does not intend to be exhaustive on the subject, but could perhaps bring you some useful insights.,??,??

 , 

InformationWeek has an interview this week with resident Data Science Central blogger Michael Walker about the most common traps awaiting data scientists:




The Telecom market is saturating and to increase their turnovers, operators are very creative and launch many innovative offers combining devices (mobile phone, MP3 players...), software (music downloads,...) and network (IP calls, ...). On another level, operators would first like to set trustworthy relationships with their good clients and therefore identify them and propose them quality services. To do so, the advanced knowledge of the customers?€? past experience is fundamental. In fact, for the operators, the point is to define the most appreciated devices and services, to know what each customer does, which services he uses the most, the turnover?? and profitability generated as well as the impact of network quality issues on the company?€?s finances (loss, contract cancellation, ...).,??,To get an overview of the services performance and quality of a network, it is convenient to refer to measurable indicators that give indications on a network activity. However, obtaining these data requires setting up specific measures such as the ones proposed by Agilent, the global leader in this area. To get the business and technical views of the customers experiences (communication success/failures,...) the company has to implement the indicators in a data warehouse like Teradata does implement for its Telecom operators customers.,??,Agilent and Teradata have signed a strategic partnership in order to propose to the Telecom operators a unique and clear view of their own customers. A road map has been set to reinforce and go deeper with the technical implementation, as well as a joint team has been set up to promote the best solutions?€? uses of the two partners. In this partnership, Agilent brings its deep knowledge of networks and its solutions that deliver everyday an important amount of data; Teradata offers the possibility to get a unique view of the network and the clients, by allowing the storage of all the data needed, that are generally spread in several Data Marts developed by the different departments in charge of the customers and network management.,??,Finally, from a financial point of view the joint offer of Agilent and Teradata allows companies to get a highly scalable data warehouse with a very low TCO thanks to the rationalization brought by the Data Marts consolidation. From a business point of view, the implementation of the customers and network data increases the analysis possibilities, the knowledge a company has about its clients and for instance to develop programs aiming at improving customers?€? satisfaction and reduce the churn.,??????????
Anyone planning to join the course 'Tackling the Challenges of Big Data' from MITX? What do you think about the industry recognition??for this course?

Big Data continues to be the buzzword du jour.?? And as with most popular concepts espoused by everyone from marketers and consultants to purveyors of software and infrastructure, the phrase can mean different things to different people.?? For us at Sullexis, we think of Big Data as a set of technologies that enable our clients to consume and process high volumes and/or diverse types of information.,But our clients need ROI.?? Neither access to a large amount of diverse data nor possession of the most sophisticated Hadoop stack of Big Data technologies will generate ROI without the right application.?? Our clients need to derive real business value from their information.?? Luckily, there is already a proven concept within organizations to derive value from information: Business Intelligence (BI).?? And we think a significant value of Big Data technologies is in improving your current BI solutions.?? Here?€?s how.,Your data warehouse requires data to be precisely structured.?? You rely on complex ETL processes to prepare your data for load and ingestion.?? But Big Data technologies enable the ingestion of data from diverse data sources and new data types, both structured and unstructured.?? You can use Big Data technologies to analyze a huge variety of data types.?? For example, using natural language processing, you can pull keywords from Word documents in SharePoint or from emails on Exchange or from social media activity.?? You then have a choice ?€? you can feed this newly structured data into your data warehouse to leverage the analytics built into your current BI solution or you can build new analytics into your Big Data stack.?? Either way, your BI just got better.,Your current data warehouse requires data in a strict pre-defined format to drive descriptive analytics (e.g. reporting).?? It is not easy to modify your data warehouse structure.?? And to modify it, you need a detailed design.?? But sometimes you need to take a fresh look at a larger set of data to evaluate patterns and trends ?€? to explore your data.?? A Big Data environment can give you a flexible environment to explore your data (ALL of your data), which you can then use to drive new structures and better functionality out of your existing BI solution.,BI solutions are usually limited by technical resources, both hardware & software.?? This creates situations where data must be split and summarized to view an aggregate picture of the data. In many cases, this splitting and summarization of information causes a loss of granularity and weakens the possibility for detailed analysis. Big Data technologies enable horizontal scalability ?€? you can add resources as needed in order to store and process as much information at the most granular level needed. This horizontal scalability will enable detailed analysis of vast amounts of data, enabling your team to meet the requirements of your BI solution.?? You have the ability to conduct data modeling and analysis in your Big Data environment or your Big Data environment can feed data into your current data warehouse, leveraging analytics you have already built.,4.??,Many BI solutions are struggling with performance.?? We have seen many BI systems that require multiple days to generate cubes of information. Big Data technologies enable horizontal distribution of this massive amount of processing.?? This distributed processing power will process more information more quickly. ??Use a Big Data environment to pre-process your data, reducing the load on your current BI solution.,5.??,When a BI solution is resource constrained, data aggregation is the inevitable result.?? This causes a loss in granularity, reducing the quality of the data.?? This loss in granularity may reduce your ability to detect relationships within the data. ??The distributed power of Big Data technologies will enable you to perform data analysis at any level needed, without aggregation.?? For analysis that is straining your current BI solution, use the Big Data environment to augment your capabilities.,Eliminate client complaints regarding slow data reporting. Since Big Data technologies mean more power, you are able to obtain your information in less time, often in near real-time. This enables you to invest more resources in data visualizations, which drives improved user experience.?? A Big Data environment will give you options on where and how to ingest, store, analyze and visualize your data for optimum user experience.,7.??,As with any business solution, cost is a critical factor. Big Data technologies are designed for horizontal scalability. Traditional BI solutions require vertical scalability ?€? you need to acquire more powerful hardware to support the demands of the projects. Vertical scalability gets very expensive very quickly.?? Horizontal scalability means that you can use economical commodity hardware on a larger scale as one single powerful piece of hardware. The amount of data you must manage will keep increasing.?? Leveraging the horizontal scalability of Big Data will reduce your costs.,So, can Big Data technologies help your BI solution??? Yes.?? Need help?????,.,By??,??and??,.


 Data science might be one of the hottest buzzwords in 2013. But is it only a marketing gimmick? I don?€?t think so. In my opinion, data science can be the best protocol that reveals what?€?s happening every day in the real world.
Both R & Python should be measured based on their effectiveness in advanced analytics & data science. Initially, as a new comer in data science field we spend good amount of time to understand the pros and cons of these two. I too carried out this study solely for ?€?self?€? to decide which tool should i pick to get in depth of data science. Eventually, i have started realizing that both (R & Python) has its space of mastery along with their broad support to data science. Here some understanding on ?€?when-to-use-what?€?,Now, when you start getting into space of predictive modeling, machine learning and mathematical modeling, Python can give a easy hand. Mathematical functions, algorithmic problems find good support from Python libraries for k-means & hierarchical clustering, multivariate regression, SVM etc. Not limited to this, but it also has good support from data processing & data munging libraries like Pandas and NumPy. Here are some cents for python:,So in summary, we can bet on R when we start getting into statistical analysis and then eventually turn up towards Python to take your problem to a predictive end.,Original post:??

Want to make graphs with LaTeX? You can use Plotly and style from the??,, your??,??NB, and the GUI at plot.ly. Any and all feedback on the beta is welcome, and your expert advice is much appreciated.,Check out these Plotly graphs using LaTeX (click through for an interactive version).
Hi all,,Here's a??,??about using Plotly's ,??and the GUI??to make and share your graphs.,??
Also, remember this final data science commandment:,In other words, , due to all of the challenges associated with collecting, cleaning, and preparing distributed, heterogeneous, complex, dirty data. But, , too, because of the challenges in deriving timely and actionable insights from massive streams of data. Therefore, do not underestimate the amount of time that you will spend in the first mile and the last mile of a big data analytics project. Devote quality time and attention during those phases of the project (giving them proper respect) in order to produce lasting value and tangible ROI.





I often find myself following missing-persons cases. I am interested in the reasoning behind the use of resources. There can be major deployments of capital during investigations. I tend to wonder which scenarios trigger more spending than others. I also recognize the visceral side of missing-persons cases. There are publicly accessible databases of missing persons in Canada, the United States, and I am certain many other countries. I imagine that it can be difficult to regard such cases as interesting sources of data if one is personally affected. Yet the other reason I am drawn to the cases is indeed the data and relatively unstructured manner in which facts are presented. I would argue that missing-persons cases also inform us of the limitations of quantitative methods both in terms of locating missing individuals and prosecuting those responsible. I am touched by the choices people make for their eternal resting places, how their bodies are sometimes discovered alongside highways and train tracks. I tell myself, either these are great places not to be found, or these are places where people might be found by strangers. I think about the level of violence necessary to deprive somebody of their mobility and freedom. I am fascinated by the open exchange of data between police departments, communities, and families; it is something inherently natural like leaves rustling in a strong wind. Still, the data is in a language that is difficult to understand. It is often non-quantitative. It is bundled up and wrapped as if to disguise the contents.,I remember walking home one pleasant summer day and encountering a girl clinging to a lamppost on a bridge; this was overlooking a busy highway. Judging but the cuts to her wrist, I quickly assumed that she intended to jump. She seemed uncertain about the final step over the edge. I hope she is fine these days perhaps with many children and a stable outlook on life. I confess at the time, I wanted to know how this young person got into such a messed up situation. Since I felt that she was in an unpredictable frame of mind for conversation, I deliberately decided to say nothing. I understood well enough. She was broken. My main priority was to remain nearby until the police came. When I say nearby, it was near enough to hear her breathing. It is a close relationship indeed to be with a person near the edge. How does stuff like this get quantified? High fencing has been installed on a number of bridges in Toronto. I am not at all certain about the statistics on how many people jump. This post is about missing persons rather than depression and suicide; however, I am probably not the first person to notice, sometimes people that go missing are later confirmed dead as a result of apparent suicide.,I would now like to introduce the process of data embodiment that I am developing to engage my interest in missing-persons cases. I realize this is an unusual area of study, but like I said I find the data stimulating. My background is poorly suited in terms of helping people recover. I am not a psychiatrist or crisis counsellor. But I rather like the idea of doing things to help recover people. So apart from keeping myself occupied by sifting through the data, I would like to think that I can positively contribute to the lives of those who have lost others or who themselves are lost. I consider a blog most effective for the purpose of providing a general overview, less so to convey technical details. So I apologize in advance for covering only a few major points in a superficial manner.,Perhaps everyone has seen a linear portrayal of the Ludwig von Bertalanffy?€?s systems model. Because the depiction is so linear, I can easily summarize it here: input => process => output. There is often an arrow in the opposite direction for feedback. Below I offer a rather multi-dimensional conceptualization of the model. It emphasizes how a system is actually made up of many subsystems. Processes can be embedded within other processes increasing the level of complexity. Apart from the main arrows normally associated with production, there are also return arrows for feedback to support a process that I call ?€?articulation.?€? I can spend quite a bit of time on this ?€?recursive model?€? of systems theory. However, my underlying goal is to leap from the diagram to the table that immediately follows; this table presents the flow of data in the illustration as descriptive text and codes.,As a person collects data associated with different contexts, I believe that the need for tangible results can contribute to ?€?contextual projection.?€? The needs of the organization begin to define what gets included as data and how the data gets interpreted. This is not as sinister as it seems on the surface. Before I approached the girl clinging to a lamppost, I am sure that many dozens of people had gone down the same path. Some may have been ambivalent about taking action. There is also the intriguing possibility that a number of pedestrians and also drivers simply failed to notice. It is a related state, right? A missing person and a person who is not noticed occupy similar places. The contexts of our lives fail to intersect with those whose lives are at the edge. If they should die, then their bodies might remain missing for some time long after. I have found that elaborate systems-oriented models tend to force the evaluation of things that might otherwise be overlooked or misplaced; it then adds placement in relation to the system.,The table ?€?resembles?€? the diagram although it is not an exact analog. The table of codes (the ?€?protocol?€?) contains certain aspects of my graduate research on social disablement. In relation to missing-persons cases, I refer to social disablement in its proper sense: people might find themselves in highly controlled relationships or forcibly confined. Being abducted, sexually molested, beaten, and imprisoned are aspects of imposed disability. (These are methods of subjugation that literally lead to disability.) Consider the following line of reasoning that can be found in the field of critical disability studies: disability is something that is socially conceived. I simply explore the concept on a more basic, extreme, and rather visceral level. A person can cause somebody else to become missing. If the absence of motor ability, eyesight, or hearing can be regarded as disability, then the absence of the entire person must be likewise. A person might be missing because somebody made him or her missing; this external imposition is a form of social disablement. A case might also reflect our inability to locate a person. An individual might be missing because we cannot find him or her. The persistence of the missing condition then becomes related to developments in the investigation.,Codes from the table (such as H1, X2, and E3) are assigned to case details. This is easier said than done by the way. In my application of a protocol to human rights tribunal cases, I assigned codes directly to the comments made by adjudicators. I chose a slightly different approach with missing-persons cases. I now express facts as relations. I then assign codes from the table to the relations. This process makes it possible to maintain the following: 1) a body of facts; 2) information about the relationships between the facts and different contexts; and 3) additional information relating to the systemic interaction of contexts. I describe the mapping of facts and contexts into a systemic framework as ?€?data embodiment.?€? Embodiment helps to show the impacts of facts and also bring to light the potential absence of important facts. As the protocol above suggests, disablement for me involves a pathological construct. Assigning codes from the table represents an exercise in fault-recognition.,I feel that many would say, the methodology seems fairly coherent (in that there actually seems to be a methodology) although there is no evidence that it works. My response to this is that the data has to be collected regardless. I am simply adding a layer to tag the data based on a series of tests. I hesitantly compare a protocol to a ?€?checklist.?€? Checklists such as those that support quality control processes tend to be used to confirm conditions either before or shortly after a particular process has occurred; moreover, the level of confirmation is generally near the surface. Facts are apparent through inspection. ?€?Does the tank contain propane? Check. Does the primary pressure regulator work? Check. Is the secondary regulator delivering 10 inches W.C. of pressure? Check.?€? This sort of checking might occur to ensure safety and for basic diagnostic purposes. So on one hand we could have a pool of data relating to a person. We might also have a checklist called ?€?Confirmation of Human Subject?€?: two hands; five fingers on each hand; two legs; five toes on each leg; two eyes; one head; one face on the head; one nose on the face; and so forth.,A protocol can differ from a checklist in a few ways. The table asserts procedural direction or flow: from top to bottom at each column; then from left to right. So before a body is recovered or found in a missing-persons case, a person has to disappear or be absent. Before a person becomes missing, there has to be disablement; this should not be surprising given that the protocol confirms the relevance of social disablement. What if disablement as it relates to the protocol does not appear to be an issue? Then the protocol might not be relevant. Using the previous example, if ?€?Confirmation of Human Subject?€? fails, try ?€?Confirmation of Chimpanzee Subject.?€? Another important difference with a protocol is how it can be applied to facts and assertions that are conceptually distant from the immediate outcomes of a process. So rather than count a product as defective (near production), one might assert that the product caused a family misery (far from production) contributing to alcohol-abuse (further yet) and divorce (extremely far). The bakes failed causing the loss of several children in the family. Some might argue that this use of a protocol is not particularly scientific since the assertion of ?€?misery?€? is highly subjective.,In Canada, Supreme Court judges routinely differ on their decisions despite being presented with the same data. There is also a system in place for higher courts to overturn the decisions made by lower courts in certain circumstances. Given that differing expert opinions are possible, one might argue that the legal system is non-scientific. I just want to emphasize that subjectivity is not actually illegitimate. After all, if everybody agreed on the relevance of data and how data should be interpreted, there would be no need to have a judicial system. So for the sake of argument, consider the possibility that a protocol does seem to hold certain facts of a case. The parents of a missing teenager might claim that their daughter would never think about running away; in fact, according to the parents, she constantly reports her whereabouts to them. On one hand, it can be argued that this teenager is naturally obedient. Interpreted differently, it seems that the parents have normalized a certain level of disablement; for regardless of what kind of person the teenager is, the parents have imposed on her the label of an obedient child.,I do not suggest that disablement necessarily exists in any particular missing-persons case or that even my protocol represents the only way to regard disablement. Missing-persons cases where disablement would likely be inapplicable include situations of criminal avoidance: for example, a person might leave the country or fake his or her own death in order to avoid prosecution. (Of course, even these scenarios are subject to interpretation given the disabling nature of political prosecution in some countries.) The use of protocols provides for a frame of discourse. Together with a database of relations, protocols can help an investigator ascertain similarities between cases and determine which lines of reasoning seem to fit the unstructured discourse. I find it intriguing that a protocol developed in Toronto to study a case today might be applied to a missing-persons case many decades later. In this post, I have described a regime where unstructured data can be compared on an abstract level, using relations and a form of theoretical embodiment called a protocol. These protocols can become an asset to an organization, which might have thousands of protocols gained through many years of data-collection. However, it is necessary to structure the information collected in a particular way to ensure long-term accessibility through computer-assisted methods.



Predictions are in our DNA.?? Millions of us live with them daily, from checking the weather to reading daily horoscopes.???? When it comes to Big Data, the industry has shown no shortage of predictions for 2014.?? In fact, you might have read about insights on ,, ambitions for , or a vision for the ,.,It is quite difficult to accurately assess when these predictions will materialize.?? Some of them will see the light of the day in 2014 but many might take until 2020 to fully mature.??,Take the case of wearable devices.?? There is no question that mobile phones, tablets and smart watches will become pervasive over the next 5 years.?? According to Business Insider, the , and theses devices have a strong potential for changing our habits all together.??,The only issue is how quickly we will adopt them and in turn get clear value from them.?? , have made a great case for the opportunity but also have provided a down to earth perspective for the rest of us (his recent article on ?€?,???€? is a gem).,So, I predict that, while the tipping point for such technologies might be 2014, but the true disruption might not happen before 2020.?? Why??? Definitions and Center of Design.??,For starters, the definition of a ?€?wearable device?€? is still very loose.?? I?€?m a big fan of devices like the Jawbone UP, the Fitbit and the Basis watch.?? In fact, , that allows me to visualize my goals, measure and predict my progress already. My ?€?smart devices?€? collect information I couldn?€?t easily understand before and offer the opportunity to know more about myself.?? Big Data growth will primarily come from these types of smart devices.??,The wearables that are still confusing are the so-called ?€?smart-watches?€?.?? Theses watches, in my opinion, suffer from a ?€?Center of Design?€? Dilemna.,Let me explain: the technology industry is famous for wanting new technologies to sunset old ones.?? When Marc Benioff introduced Chatter, he said it would obliterate email.?? When PC shipments went down, the industry rushed to talk about the ?€?Post-PC?€? era.?? Have any of these two trends fully materialized yet?!??,The answer is unfortunately not simple.?? Smart watches, phones, tablets and PC all have a distinct use cases, just like email and social apps.?? Expecting that one technology would completely overlap the other one would be disregarding what I call a product?€?s ?€?center of design?€?.?? The expression refers to the idea that a particular technology can be stretched for many uses but that it is particularly relevant for a set of defined use cases.?? Let?€?s take the example of the phone, tablet and PC:,When I see an ad like this on the freeway, I really question the value of an additional device.?? What can a watch in this case add, if the wrist that wears it, is also connected to a hand that holds a much more appropriate device?,Big Data from Wearables is a Predictive Insight for 2020 in my opinion, because I think that, by then, the broad public will have embraced them into use cases that truly add value to their lives.,Bruno Aziza is a Big Data entrepreneur and author.?? He?€?s lead Marketing at multiple start-ups and has worked at Microsoft, Apple and BusinessObjects/SAP.?? One of his startups sold to Symantec in 2008 and two of them have raised tens of millions and experienced triple digit growth.???? Bruno is currently Chief Marketing Officer at ,, loves soccer and has lived in France, Germany and the U.K. ??

By ,??Team
Here is the USA map visual by state showing what percentage of people received flouride in their water.,Here is the data source??
Using Big Data technology effectively, such as Map/Reduce or NoSQL, has two components: technical skills and high level expertise.?? An article in Harvard Business Review, January 2013, entitled How IT Fumbles Analytics, stated "Improving how businesses extract value from data requires more than analytical tools. It involves creating an environment where people can use the company's data and their own knowledge to improve the firm's operational and strategic performance.",??,Data Science projects typically start out with a framed business problem, which leads to data collection, running some data mining algorithm or map/reduce program, and then analyzing the results. Data collection and running algorithms are dependent on technical skills. On the other hand, analyzing the results includes discussions, which depend on high level expertise. Analysis is where the work of the Data Scientist really shines in creating competitive advantage because this is where the technical skills are combined with the high level expertise.?? The technical skills of Big Data produced the results for analysis, and the expertise interprets and applies that information to creating new solutions and levels of success.,??,The problem then becomes a management issue of integrating the high level expertise with the technical skills in order to capitalize on the results of an algorithm or map/reduce program. There are many approaches to solving this problem, but for simplicity let's focus on one that inhibits open discussions. Two researchers, Mantere and Vaara, analyzed business oriented conversations. They found three approaches to conversations (social interaction) that suppress participation, which they termed Mystification, Discipling, and Technologization. Mystification occurs when a decision is handed out without explanation. The origin, reasoning, and cause of the decision is not known to the person carry out the decision. Discipling happens when the "how" is defined for the employee.?? The employee is not allowed to be creative or tailor the instructions when carrying out the directive. Technologization is when predefined measures guide employee actions more than the actual directive. If these three concepts are in place, employee involvement in discussions is quite limited.,??,Each of these three types of discussions has a means of avoidance, which need to be implemented long before the analytic discussion takes place. In the natural execution of daily activities employees happen upon unexpected obstacles continually. Each of these daily obstacles is a decision point for the employee. Without greater understanding of the original objective set by senior management, freedom to deal with obstacles as they come up, and confidence that creative solutions will be rewarded, employee actions can stray unintentionally.,??,One way to get rid of Mystification is to ask employees to interpret the directives. If different employees give different answers to the same directive, it is time for the senior managers to open up the dialogue more. One way to get rid of Concretization is to ask the employees if there are better ways to getting things done. Ask them if they feel restricted. Also, refrain from telling them how to do everything. Trust your employees to be creative. One way to get rid of Technologization is to ask employees if they feel recognized. Another means is to recalibrate the metrics with past rewards. Are the two in sync?,??,In summary, these three types of discussions are to be avoided. The three recommendations boil down to 1) asking employees to state the objective, 2) giving freedom to employees to be creative in the "how" part of any task, and 3) questioning whether the metrics are in line with directives. The above recommendations will help you increase employee participation in the discussions that are based on the results from analytic projects. Employee expertise combines with technical skills to produce innovative ideas. This combination will result in even greater insight, more informed decisions, and beneficial actions.
 

Hello??,I would like to know how can student find an internship within data scientist or statistic.,I am majoring in statistic and modeling in a top engineering school in france and I am looking for an internship in data mining within I will be able to performe my computional skills ( R,C++,JAVA) and also I will be able to bring my knowledge in statistic.,I am looking for all contries and regions so I am expecting advices or orientations from the members of big data practitioners.




This is data science from the trenches - both a case study, and a tutorial for data sciencist candidates. Here I illustrate how gut feelings, carefully selected data (rather than getting granular data), full understanding of business (,), high level vision, and outsourcing (to make data science almost free) combined together, makes a data science project successful. I also share with you the data set used in this project: top 5,000 webpages from ,??in the first few weeks of 2014, with rather detailed metrics; the time period is less than 3 months, but more than 1 month.,The goal here is to assess the effectiveness of our Google advertising campaigns, and how to better shift and optimize our traffic sources. This data science analysis is about our own Data Science Central websites and business model, but you will learn trends that apply to many other businesses, regarding Google, LinkedIn, Facebook, Twitter and Google+ traffic.,Example of complex optimization problem, with multiple maxima,Here are a few fun facts, as an appetizer:,The data scientist in charge of this study is the co-founder of the company. As a lean start-up with 0 employee (generating 10 times more revenue with 80% margin, than my previous money-losing VC-funded startup that had 20 employees), we just don't hire a data scientist, though we have the money to do so if we wanted. Instead, an executive familiar with all business aspects (me) spends 5% of his time on this type of investigations, and the money saved on a data scientist goes to profit sharing. Many executives nowadays, especially in technology company like ours, have a strong analytic acumen (and background) and can do just like us.,All the data collected comes from vendors: reporting is 100% outsourced and cost little to nothing. We get,There's no silo: one guy (me) has access to, understand, and blend all the data, including financial data (revenue, costs) broken down per product.,Here we will focus on Google Analytics data, as well as financial data, at a rather high level. The roles of data scientist and business analyst overlap here: one person wears both (and many more) hats, saving a lot of money in payroll to allow us to better compete with other, over-staffed companies.,I've worked only with the top 5,000 webpages but it would have been very easy to download the Google Analytics data for the 41,009 active pages. The reason to work with 5,000 is to get you familiar with sampling, and prove that you can get great results with just sample data. This being a tutorial part of our ,, it is important that you get familiarized with sample data.,Regarding our Google advertising campaigns, we selected keywords suggested by Google itself rather than doing our own time-consuming research. This is actually leveraging Google's massive multi-billion data base of priced keywords, at no cost. A next step would be to customize bids per keyword and ad group, but I believe it won't provide much added value. Finding the top 20 keywords that need customized bids and optimize them is good enough. More than that, and we might be doing ,. Of course it's a different story if you are eBay and manage 10 million keywords.,Another weakness is that we don't track conversions yet (new members) on Google analytics. But we have a pretty good idea about conversions and we even created a metric called ,, attached to each web page in the data set that we share with you in the last section. We will soon track conversions in Google Analytics, as this will help us drop poor-performing keywords: it is worth the effort (also it helps Google optimize our campaigns for conversions rather than for clicks, a much better solution).,I bought a book on Google Adwords (cost $50) and learned one great thing: how to set up display campaigns where your ads show up only on websites that you have selected (such as our competitors that accept Google ads, or other data science websites). This saved me a lot of money in attending ??classes or hiring a Google expert. Also, me might use the service of a SEM/SEO company in the future, but again it will be outsourced (vendor relationship). And for now, since our network is sitting on the Ning platform (saving sys admin and server costs), we automatically benefit from Ning SEO efforts. The reason I mention this is to show how analytic thinking / gut feeling help decide how deep you want to go with data science. As a small, lean start-up, we don't want to over-spend, we have a pretty good??idea when we spend too much (for instance, if all this activity eats more than 20% of our budget, unless it boosts total revenue).,One of the nice things is that all reporting activities are automated.,Our problem is complex. We don't have a dollar amount attached to a conversion, and in general, we don't charge clients by number of impressions or clicks: we typically offer fixed fees with guaranteed numbers in terms of leads, impression or clicks.,We want to keep ad spend below 10% of our budget (our margin is currently 80%). Currently Ad Spend is about 4% of our gross revenue. We can't easily increase this 4% figure, because to get more traffic, we would need to increase our bids, which would eventually generate negative ROI. It is important that you know your break-even point. For us, the maximum cost of acquisition (to maximize revenue) of a conversion has not been fully calculated yet, but it is below $10, in other words, $3 per Google paid click maximum.,The impact of ad spend on our traffic (page views) is small: less than 3%. But it is much bigger on conversions (our main source of revenue), accounting for 25% of all conversions, and diversifying our conversion sources to minimize risks. An easy way to measure the 25% is using a different landing page for each source (combined with proper taggings for the conversion URL) so that we can identify the origin of the conversion (Google AdWords, direct traffic, LinkedIn, etc.) Or you can turn on/off Google AdWords and see the impact.,Note that we purchase mostly US / UK / Canada / Australia traffic and avoid midnight to 4am traffic, in order to increase the quality of the paid traffic that we receive from Google: this is another way to leverage a vendor's (Google) big data capabilities without incurring big data costs. Indeed, now our Google paid traffic is better than our Google organic traffic, as it is well targeted and focused on driving traffic to the conversion page.,For every $100 of revenue that we make, $15 is coming from impressions (page views) and $70 is linked in some ways to the number of active subscribers and members.,Google ad spend eats $4 (from these $100) but produces only 2% of impressions. We haven't done survival analysis to assess how many page views a user generates over his lifetime, broken down by acquisition channel. Plus, attribution modeling would suggest that some of the new users coming from Google ads would still be acquired by a cost-free channel, if we did not use any Google advertising.,Nevertheless, it is clear that the Google Ad Spend has negative ROI with respect to page views, but the total dollar amount is small. Since we don't operate in silos, we also check the impact on conversions (subscribers, members). We estimate that 25% of our new members come from Google ads, that is, 25% of $70 in revenue can be attributed to Google ads (though some would join via a free channel if we did not advertise, and users acquired by Google ads have higher churn - just a wild guess). So I'll reduce the 25% to 15%. In short $10.50 = 15% of the $70 revenue, costs us $4 (Google Ad Spend), and thus, Google Ad Spend works for us, we can even increase our CPC and budget.,However, the situation is more complicated than it seems at first glance. Getting more traffic makes sense if we get more revenue. We could increase the fee for our services (email blasts) if we deliver to more subscribers, resulting in more clicks and more leads for the clients. But this is not obvious: increasing prices can deter clients - clients also have fixed budgets. We can easily get more clients, but we can not send more than one blast per day: at some point, our inventory is full booked. We could segment our member database, send more blasts to more targeted, smaller groups of people. That's the way to go to grow revenue along with traffic. Another way is to reach an equilibrium, have our company run on auto-pilot, and start another one (maybe a community for astronomers) and then another one. We are definitely contemplating this option.,Based on this analysis, we decided to:,Future steps will involve automated content syndication and content mix optimization. In particular, detecting how to optimize the following mix:,The year-over-year tab in the spreadsheet (see next section) shows a spectacular growth (> 80%) in incoming traffic, for Google organic traffic and in direct traffic (driven by email campaigns). Google organic and direct traffic represents 66% of the visits (33% for Google organic, a perfectly normal number, especially since we don't do any SEO) and 33% for direct traffic (quite good, with growth driven by membership growth after factoring in churn). LinkedIn, although the traffic is better with more page views by visit, is barely growing, which is good since our reliance on LinkedIn was too high in the past, representing a risk. Twitter is very promising and will eventually surpass LinkedIn, in terms of share of incoming traffic. Our Twitter advertising campaigns contribute to this shift. ??Facebook and Google+ bring modest contributions, and we don't expect spectacular growth from these traffic sources, though Facebook advertising has gotten better over time (less fake traffic, more reasonable CPC).,Finally, we've noticed that LinkedIn and Google organic traffic sources are negatively correlated. The more we get from LinkedIn (by posting on LinkedIn), the less we get from Google, as the LinkedIn links to our articles show up above our internal links,??on Google. This is actually an incentive for us to either do better SEO (to beat LinkedIn and the fact that Google wrongly attributes our articles to LinkedIn), or to post less on LinkedIn. We've chosen the latter. However, our posts on LinkedIn get re-tweeted or re-posted outside LinkedIn, resulting mostly in direct traffic to our website. We haven't quantified the amount of traffic indirectly generated via LinkedIn, but it might represent 10% of our direct traffic (based on some bit.ly statistics). The same applies to Facebook, Google+ and Twitter, but not to Google organic or paid traffic.,??to download Google Analytics report, with traffic metrics for 5,000 top pages and estimates for all 40,009 active pages on our websites, during time period in question.

??are among the most fun and profitable applications of data science in the big data world. Training data (corresponding to the historical search, browse, purchase, and customer feedback patterns of your customers) can be converted into golden opportunities for ROI (,??,??and Investment). The predictive analytics tools of data science yield a bonanza of mechanisms to engage your customers and enrich their customer experience. What better ,??can there be if not the one that ,??(and sometimes, even before they think of it for themselves). Yes, we know of some cases that have gone bad (such as the secretly pregnant teen and the??,), and we recognize that there is a fine line between ,, but usually people do like to receive offers for great products that they love.??,A new O?€?Reilly book (by Ted Dunning and Ellen Friedman)??on ,?? takes a look at the nuts & bolts, the mechanics and the implementation, and the theory and the practice of recommender engines. They describe the design of a simple recommender using ,, based upon the co-occurrence analysis of customers' product purchases.,??,In??,, I examine recommender systems -- their underlying principles, data science, history, and design patterns. ??Here are links to the articles, plus a short excerpt from each one:,??,??,We identify four different design patterns that are useful in recommender engines for predicting customer behavior in the customer experience environment (, online store, browser, smartphone app, or whatever): co-occurrence matrices, vector space models, Markov models, and ?€?everyone gets a pony (the most popular item).?€?,??,??,??offered a ,??for anyone who could improve upon their recommendation engine?€?s algorithm by at least 10%. There were over 44,000 entries in this contest, from over 41,000??teams, representing approximately 51,000 contestants. A ,??soon after??July 26, 2009 when the ?€?Bellkor?€?s Pragmatic Chaos?€? team submitted an algorithm that delivered 10.06% improvement. Another team matched their score on the test dataset, but the winning team scored best on the ?€?hidden dataset?€? that Netflix used to score contestants?€? entries. This latter detail provides a classic instructional example of how to avoid overfitting in a predictive analytics model,??which is built against a training dataset???€? you find the ?€?best solution?€? (which works best on a general set of data) through error measurement and verification of the algorithm against previously unseen data. ?€?Bellkor?€?s Pragmatic Chaos?€? algorithm was the winner of the Netflix Prize on the basis of having the best performance on the hidden dataset, but they may have won in another category (though this cannot be verified) ?€? their algorithm?€?s??,??must have been among the leaders in that characteristic also. Their winning algorithm was presented in detail in a ,. The algorithm is an emphatic example of the type of algorithm that seems to be a frequent winner in ,??crowdsourced data science competitions ?€? ensembles. Ensemble algorithms are in fact an amalgamation of multiple algorithms ?€? they combine the predictions from large numbers of different algorithms. The proof is in the prizes ?€???ensemble learning is one of the most accurate [and prize-winning] machine learning methodologies??for big data analytics problems. ... ...,??,I encourage you to take a look at the new book and at the full-length versions of the above articles.,(follow ,??on Twitter at ,)
 , , , The Scientist ascertains and catalogs the nature of sand. Each grain is unique. Each has a distinctive shape, weight, color and molecular structure. Like snowflakes, no two are alike. The scientist identifies different types of sand and how they might have gotten that way., , , Sand grains magnified 110-250 times reveal each grain is unique. Photo copyright Dr. Gary Greenberg.[/caption], , The , thinks about Big Sand. Which way is the dune traveling? What can we deduce about its macro movement by the size, shape and direction of the ripples? What new algorithms can we write to help anticipate the flow of sand in different wind or weather?, ,The sand dunes of Vietnam (source: ,), , The , takes a sample of the sand and creates a model that does not represent the sand, but the human side of the equation. An artist must understand the raw material well enough to know its limitations and its strengths, and then use that material to create something that communicates to others., , , , No model is an exact representation of the original. That's what prompted George Box to comment that all models are wrong, but some are useful., , A data model is a representation of the way your business and your customers interact. If the model is good enough, it can be used to see where improvements can be made and to predict outcomes., , These models can get quite complex, to the point of being impractical and unusable; much like an Excel spreadsheet that has too many formulas created by too many people. As George Box also said: overelaboration and over parameterization is often the mark of mediocrity., , , , Data models also have a time-value boundary. They begin to degrade as soon as they are completed. How companies and customers interact is influenced by a wide variety of forces:, Seasonality, Proximity to payday, Weather, Competition, Current events, , Therefore, models must be constantly updated to keep them current., , , , This is where the Data Artist runs into trouble with his tools. One can build a perfectly lovely sand castle with one's bare hands, but given a shovel, a pail, a trowel and some sticks, the model can get better and better., , We're at a new age in data analytics tools that allow for more options when it comes to data sculpting. The tools are getting more flexible and easier to flex., , Now that the Data Artist can stop spending so much time on making the model work and can spend more time making new models, there is a real opportunity to create and deliver new insights that can stir the imagination of the business decision maker - the consumer of the art., , ?€?Art is not what you see, but what you make others see.?€?,?€? Edgar Degas, , , ?€?Creativity takes courage. ?€?,?€? Henri Matisse, , ?€?The role of the artist is to ask questions, not answer them.?€?,?€? Anton Chekhov
Here's the introduction. ,.,This book is a type of ?€?handbook?€? on data science and data scientists, and contains information not found in traditional statistical, programming, or computer science textbooks. The author has compiled what he considers some of the most important information you will need for a career in data science, based on his 20+ years as a leader in the field. Much of the text was initially published on the Data Science Central website over the last three years, which is read by millions of website visitors. The book shows how data science is different from related fields and the value it brings to organizations using big data.,This book has three components: a multi-layer discussion of what data science is and how it relates to other disciplines; technical applications of and for data science including tutorials and case studies; and career resources for practicing and aspiring data scientists. Numerous career and training resources are included (such as data sets, web crawler source code, data videos, and how to build API?€?s) so you can start practicing data science today and quickly boost your career. For decision makers, you will find information to help you make decisions on how to build a better analytic team, whether and when you need specialized solutions, and which ones will work best for your need.,This book is intended for data scientists and related professionals (such as business analysts, computer scientists, software engineers, data engineers and statisticians) who are interested in shifting to big data science careers. It is also for the college student studying a quantitative curriculum with the goal of becoming a future data scientist. Finally, it is for managers of data scientists, and people interested in creating a startup business or consultancy around data science.,These readers will find valuable information throughout the book, and specifically in the following chapters:,The technical part of this book covers core data science topics including:,The focus is on recent technology. So you will not find material about old techniques such as linear regression, except anecdotal references. These are discussed at lengths in all standard books. Actually, there is some limited discussions on logistic-like regressions in this book, but it?€?s more about blending it with other classifiers, and proposing a numerically stable, approximate algorithm; we mention that approximate solutions are often as good as the exact model, as no data fits perfectly with a theoretical model.,Besides technology, the book provides useful career resources, including job interview questions (some are technical, some are not). Another important part is cases studies. Some have a statistical/machine learning flair, some have more of a business/decision science or operations research flair, and some have more of a data engineering flair.,Most of the time, I have favored topics that were posted recently and very popular on Data Science Central (the leading community for data scientists), rather than topics that I am particularly attached to,,The book consists of three main sets of topics:,The book provides valuable career resources for potential and existing data scientists and related professionals (and their managers and their bosses), and generally speaking, to all professionals dealing with increasingly bigger, more complex, and faster flowing data. The book also provides data science recipes, craftsmanship, concepts (many times, original and published for the first time), and cases studies illustrating implementation methods and techniques that have been proven successful in various domains for analyzing modern data ?€? either manually or automatically,The book contains few sample code, either in R or Perl. You can download Perl from , and R from ,. If you use a Windows machine, I would first install Cygwin, a Linux-like environment for windows. You can get Cygwin at ,. Python is also available as open source and has a useful library called Pandas.,For most of the book, 1 or 2 years of college with some basic quantitative courses is enough for you to understand the content. The book does not require calculus or advanced math ?€? indeed, it barely contains any mathematical formulas or symbols.,Yet some quite advanced material is described at a high level. A few technical notes spread throughout the book are for those who are more mathematically inclined and interested in digging deeper. Two years of calculus, statistics, and matrix theory at the college level is needed to understand these technical notes. Some source code (R, Perl) and datasets are provided, but the emphasis is not on coding.,This mixture of technical levels offers the opportunity for you to explore the depths of data science without advanced math knowledge. (A bit like the way Carl Sagan introduced astronomy to the mainstream public.),??
I just came across this blog and thought it was an interesting point. i disagree. but its worth a discussion :),cheers

While companies complain about lack of analytic talent, professionals complain about lack of jobs. Everyone wants to work for Facebook, LinkedIn, Google, Intel, Apple, Twitter or some hot start-up. It creates fierce competition getting a job interview, let alone a job. But companies that do not belong to this circle see very few candidates applying for their data scientist open positions; in addition, they are only hiring what I call technical developers (defined by a narrow set of technical skills, usually R, Python, NoSQL, Hadoop, Map-Reduce, software engineering). They are not interested in real data scientists, so many data scientists that would apply would (erroneously) not be perceived as bringing value, and not interviewed.,The problem with consulting is of a different nature. Companies are looking for the cheapest consultant having the minimum set of qualifications to perform the task (the candidate will be asked to provide details about previous projects). Because the work is performed from home, consultants compete with people all over the world to land a gig. Analytic professionals in India, found on websites such as Elance, charge $30/hour. On ,,??you can hire consultants in India for $59/hour.,When I wrote my article about ,, a few people mentioned that my consulting rates (from $45 to $100/hour) were absurdly low given my expertize. But compared with rates in India or Romania, it is actually not low. Those charging $150 to $250 per hour are having a difficult time finding new clients. And if all your great skills and expertise are not considered useful to a client, he won't pay for it, especially if this expertise is not used to generate greater revenue. Indeed, many PhD statisticians work as part-time adjunct professors with salaries even far lower, or write other PhD students theses for a fee - typically for $5,000 - as these are the only clients that they can get. So, in some sense, there is more talent than the job market can absorb, especially for PhD's.,Here's a list of ideas:,Finally, if you really have great expertize spanning across multiple domains, the easiest solution might be to just stop consulting and ,Instead of helping businesses protect themselves against hackers, you become a hacker, knowing that you can outsmart all the consultants and experts working for these companies. I'm talking about legal business and growth hacking. You can create your company, for instance a website selling books listed on Amazon; you don't actually sell the books, you get a commission each time a website visitor goes to Amazon to purchase a book listed on your website. Traffic hacking (one of the hacking systems among many others used to optimize your business) could consist in generating a huge volume of high quality web traffic, through creation of hundreds of (fake) interesting profiles that automatically post interesting stuff via a well diversified set of mailing lists and social networks, without being detected (each profile posting no more than 3 links or pieces of content per day; a different IP address is used for each profile). Your business acumen, network security, traffic scoring, and fraud detection expertize allow you to defeat the algorithms designed to block you. Instead, these algorithms generate false positives because they rely heavily on spam reported by users; you can take advantage of this to get your competitors blocked. Note that this business model does not require any sales or talking to people and is typically run from home. Other advantages include higher revenue, no meetings, no boss, and better job security. If you are good at financial engineering to reduce taxes and other money issues (I call it financial hacking), you will even make more money.
How do you know if an outlier is the result of a data glitch, or a real data point -- indeed maybe not an outlier. Difficult question to answer, but the chart below shows that in some cases, the outlier is not an error.,: ,In this example, you could argue that we are not using the right metrics: comparing health expenditures in US (twice above average among developed countries) when US salary (after tax) is twice above average among developed countries, lead to a bias. When corrected for this salary bias, US might not be an outlier anymore in the above chart.,Also, is life expectancy the right metric to use? What if a large group of people die very young because of gang membership, and another group (the majority) dies pretty old? What would be interesting to see is the impact over time, in US, of increased health expenditures on life expectancy, after eliminating people dying from gun shots or car accidents. Note that a more stressful life (typical in US) can cause early death despite higher health expenditures.,Note the massive impact of the USA dot (outlier) on R^2 (at the bottom right corner) - making it much smaller than it should be (R^2 = 0.51). But R^2 is a bad metric, sensitive to outliers, and should not be used. ,, to measure quality of fit. Indeed, the entire black curve going through the cloud, is bended too much towards the South-East, because of this outlier.
 , , , , , , 

"??is a connectivity tool to perform data transfer between Hadoop and traditional databases (,) which contains structured data. Using sqoop, one can import data to Hadoop Distributed File System from RDBMS like,??etc?€? and also export the data from Hadoop to any RDBMS in form of CSV file or direct export to databases.,There is a possibility of writing mapreduce programs that would use jdbc connectors to export/import data from/to Hadoop but sqoop automates most of such process. Advantage here is operations are parallelized and fault tolerance.,: user has to download the appropriate jdbc driver for the database. I will use MySQL so I would need MySQL jdbc driver. These drivers are developed already so not to worry. Do lookup cloudera, hortonworks for teradata, postgresql, oracle?€?s jdbc drivers.,JDBC driver,??,??is basically an API that lets you access data from relational databases and data warehouses. Each RDBMS provider has its own JDBC driver interface program.,One can import an entire table from RDBMS and specify a location where it needs to be stored in HDFS or can incorporate a query where only a subset of an entire table will be written in HDFS.,Sqoop is very easy to install. If you are a Red Hat/CentOS user, do a yum install on sqoop., ,Add the SQOOP_HOME in your runtime configuration file., ,Hadoop should be up and running.,Now, Download the Mysql connector jar from,??,extract it, copy the mysql-connector-java-5.1.22-bin.jar into SQOOP_HOME/lib,I do not have MySQL-server so I am going to install it.,it will show process running.,Now, sqoop tool has been setup. Say if I created a table called EmpRecordsData from a Database called FirstSQOOP with details of employees, salary, dept and wanted to import the table into HDFS, then we could enter,,This would perform a mapreduce of importing data from mysql database using the mysql jdbc connector. You can also import a subset of existing table,
Or even query the RDBMS and only import what you want,Now lets look at how we can export data from Hadoop/HDFS,You can update the existing database if you prefer as sqoop also provides such facility. Also, sqoop lets you import RDBMS data directly to hive. Here how thats done:,??,This is a repost of my original blog at wordpress. Do visit me??"
I have always had a great interest in how businesses organize in order to get things done.?? Here I raise some discussion points intended to stimulate debate.,??,Not that long ago, I was completing a graduate degree in ?€?critical?€? disability studies.?? The critical part deserves to be in quotations since it is probably subject to interpretation and all sorts of misinterpretation.?? I am going to suggest that in critical studies more generally, there is some emphasis on how certain things play a role in the formation of impacts and consequences.?? I know this sentence seems rather awkward.?? However, since the sentence is not my central point, I will just give an example to move matters along: ??importing cheap clothing can contribute to the cultural and economic oppression of foreign workers. ??So we have things that seem normal - cheap clothing - giving rise to deep social and structural problems. ??It is possible to regard many things in critical terms although this is certainly not necessarily the only worthwhile perspective. ??Critical studies occupy particular domains of discourse that tend to be excluded from day-to-day business discussions.?? Environmental discourse is rather critical in nature.?? When I did a degree in environmental studies, my thesis was on the effectiveness of public participation in local planning processes; so I was and continue to be concerned about the tangible and intangible impacts of institutional systems.,??,There are different domains.?? I sat back one day and placed the schemas that I had developed side-by-side along with different business models, and I found an interesting pattern.?? Assuming that the models exist in order to support production, there is a general tendency for the models to focus on particular domains of discourse.?? There is no array of pigeon holes per se across the different domains, but consider the following structure as an explanation:?? we have domains of perception to the left (inputs); domains of production near the middle (processes); and domains of impacts and consequences to the right (outputs). ??In production, there is a flow from left to right. ??I have just described a division that is already in the literature albeit in less developed terms.?? The interesting part here is how differently the data is handled across the domains.?? By and large, specific current events are most measurable near production.?? On the consequential or critical side, there is the historical build-up of data making it possible to make assertions and inferences; broader historical patterns can be extracted.?? On the perceptual or psychological side, this is where social learning occurs and where belief systems are sustained; there is a collection or more precisely a , of external market data and internal organization data. ??An organization that produces for its market also listens to it, or at least this is general idea. ??On the other hand, the expression "What gets measured gets managed" is applicable to the internal. ??One does not manage the market but one's participation in it.,??,??,These principles are easiest to explain using an example.?? I am sure that many are familiar with the concept of ?€?quality control?€? in the workplace.?? In a manufacturing environment, quality tends to be an issue of conformance; this is the idea that lack of sameness violates quality.?? Therefore, quite close to production where events occur, there is often some level of quality control to ensure that outputs conform to particular standards.?? Ideally, the checking generates a great deal of data.?? Of course, quality control is or at least can be a much larger concern:?? for instance, rather than simply assessing outputs, it is possible to measure the behaviours leading to production.?? Let us say for the sake of argument that there is some opposition to the entire metrics paradigm, which I admit is quite a mindset; and certain aspects of the operation simply decided to stop collecting data.?? What is not collected cannot be reported.?? Only what is measured can be managed.?? Is this necessarily a bad thing given the savings in terms of operational overhead??? ?€?Systemic insulation?€? is not necessarily a problem unless something goes wrong further downstream the consumer fulfillment system.?? Once something terrible happens, while it is true that there would be lack of incriminating data, there would also be lack of supporting data.?? Lack of supporting data coupled with the fact that something significant went wrong means that there is more to presume criminality in a manner of speaking.,??,However, consider the dynamics using a reasonable scenario.?? A car manufacturer decides to stop quality control protocols on its braking systems.?? ?€?Everything is just fine,?€? is the response followed by several nods and winks.?? Sub-standard braking equipment is being installed due to a shortage of supply.?? So nothing will necessarily stop such vehicles from making it out of the assembly line and onto the sales lots of dealerships.?? As cars are purchased, there are reports of accidents.?? Then there are law-suits, negative media coverage, followed by a sharp decline in future purchases.?? While the issue of brake quality had been removed from the production domain where it could have been handled near production, the issue had shifted to the consequential domain in order to correct organizational perception.?? ?€?The further right the issue goes, the further left the system throws.?€??? (I thought that up this morning.)?? One idea behind quality control then, phrased in these terms, is to minimize the extent to which problems reach right of production and lessen the resulting throw to the left of production.,??,The principles are not particularly complicated, and I feel there is quite a lot of supporting evidence.?? Before moving on, as a tribute to my father who was a mechanic most of his life, I just want to mention that quality through sameness and strictness of process is often presumed but not necessarily true.?? My father one day decided to modify a production machine in his area.?? The change was immediately noticed by his superiors.?? Management calculated that the modification if done to all of the machines in all the plants globally would save about $450,000 in raw materials annually.?? With an estimate like that, it can be assumed that the company maintained excellent production records and statistics.?? So I am not saying that conformity itself should be the objective of quality control; but rather more broadly, systemic insulation makes it difficult to determine whether or not change has been or would be useful.,??,??,This is the idea that certain aspects of selection can occur to give some organizations advantages over others.?? In a way, I suppose the concept is related to evolution.?? However, considering the concept in geographic terms, there was a time when humans really struggled to survive in natural environments.?? We have endured not because of physiological changes but developments in society and technology.?? Now, consider determinism from a rather different perspective.?? During good times, even the most poorly managed companies survive regardless of their systems, beliefs, and behaviours.?? During deterministic times, what may have worked well before might cease to do so, leading to organizational extermination.?? This means that when times are good, the data plays role in sustaining behaviours that might not necessarily be helpful when times become difficult; therefore the data as a bundle of capital might quickly deteriorate in value amid austere or radically changing conditions.?? It is for this reason that reconstruction rather than simple realignment is often necessary when businesses decline; the issue is more about changing direction rather than moving to a different lane.?? Within the context of environmental determinism, an organization during good times might be internally optimized to use data that is poorly connected to latent external risks.,??,As data scientists, I believe it can be tempting to regard the data as something constant describing external phenomena that are perpetual.?? However, in reality, historical data can be as much a liability as an asset given the shifting determinants of consumer fulfillment.?? The prefabricated solution is part of a mindset that should be avoided rather than placed on a pedestal.?? Nonetheless, my place here is not so much to question the decisions made by organizations but rather the relevance of their data capital and therefore big data amid failing business normatives.?? I am not saying that such normatives were necessarily incorrect or inappropriate at the time.?? However, if there is a systemically insulated organization, failure to change in the face of deterministic conditions can only lead to only one evolutionary conclusion.,??,??,This principle is rather obscure, and I?€?m not certain if the brevity of a blog post really helps matters.?? This is the idea that the nature of the data can itself cause an organization to become systemically insulated both in an internal sense (e.g. internal data from quality control) and externally (e.g. consumer sentiment).?? For instance, sales data actually provides us with little information.?? If people stop buying, the company can confirm this, although the exact reasons behind the change might be debatable.?? In other words, there was never any intention for the data to hold such information, and so this is precisely the case.?? In remember recently in the last university I attended, there was a shooting incident that seemed almost random in nature as per the media.?? ?€?Random?€? is a pretty bold word.?? What the reports really meant is that there was no apparent target or reason for the shooting.?? To say that an event is random implies that some situations can be predicted to lead to shootings.?? ?€?Well, yes, if this young man has a gun, and there is a fierce argument, the situation can be reasonably predicted to lead to a shooting.?€??? But really, there was no knowledge of a gun, an argument, or much of anything prior to the shooting; it seems more likely, minimal data was collected to suggest level of risk.?? Risk is a complicated thing that is difficult to characterize in data.?? Consequently, the data that is collected is ?€?unaware?€? of the situation.?? How can data be configured to become more aware??? I leave this question for readers to mull over.,??,So these are some general principles that I hope readers consider in their own examinations of data.?? I welcome any feedback.
This is an area of data science that the public is less familiar with. This example involves small data, simulations, and 18 years old crowdsourcing.,It's an attempt to explain the cause of the TWA Flight 800 that exploded near New York on July 17, 1996. I raised the possibility that a potential cause for the Malaysia Airlines flight that went missing last week, was being hit by a missile (accidental or not). Likewise, many people still believe that TWA 800 was destroyed by a missile.,However, the numerous investigations strongly debunk this myth. Here I summarize some of the data science concepts used to debunk the missile theory in the TWA flight 800 tragedy.,Traces of explosives were found, later attributed to military investigators who might have accenditally contaminated the wreckage AFTER the investigation started. But it was clearly ruled out (based on fractures and burns on human remains) that the fireballs were not caused by a bomb or a missile warhead.,Intervews with 258 witnesses scattered in the area, were recorded. Witnesses answered questions about the flight path, sound, time between sound and light streak in the sky (including light and sound intensity) and other questions. That's the crowdsourcing part, and it was crucial in ruling out the missile theory, as explained in the next paragraph. Note that this is data science, involving data recording, using the right metrics (that is, asking the right questions), and checking whether the answers are consistent across witnesses. Some criticized the FBI for asking questions that contained the keyword "missile", as this can create a bias in the answers.,Monte Carlo simulations, a data science technique, were used. After missile visibility tests were conducted in April 2000, the NTSB determined that if witnesses had observed a missile attack they would have seen: (1) a light from the burning missile motor ascending very rapidly and steeply for about 8 seconds; (2) the light disappearing for up to 7 seconds; (3) upon the missile striking the aircraft and igniting the CWT another light, moving considerably more slowly and more laterally than the first, for about 30 seconds; (4) this light descending while simultaneously developing into a fireball falling toward the ocean. None of the witness documents described such a scenario.,Note that there's a lot of variance in witnesses perception of what happened (based on location and personal factors). There's also a lot of variance in how a "missile hitting a plane" scenario could appear, based on where it is launched from, the size/payload of the missile and other factors. Reconciliating (or proving in this case that witness reports and missile simulations are totally incompatible no matter how you slice the various factors), is a challenging data science task.,Anyway, after reading ,, I am convinced that it was not caused by a missile.,Do you still believe in the missile theory? Maybe you believe these analyses are wrong, the truth is hidden? What about the recent Malaysia Airlines crash? Could it be a missile?
In-memory database technology is fashionable in recent years as the price of RAM drops substantially and gigabyte chips become affordable. By taking advantage of the cost-performance value of RAM, leading edge database developers are boosting the performance of next-generation databases with in-memory technology. However, many developers who intend to adopt in-memory technology only think of speed in terms of RAM, and do not exploit the true power of in-memory technology.,The argument here is that in-memory technology means not only taking advantage of the speed of RAM, but also suggests a new way ,.,Many developers apply the technology in limited ways as follows:,We understand that RAM offers lower latency than hard disk drives, including the newer SSDs. So when implementing an in-memory database, we may simply load the data tables into RAM during system initialization and take full advantage of memory speed at run time. This seems straightforward and intuitive to most people. However, speed is still quite limited since the actual size of virtual memory that can be allocated is subject to the size of disk swap space ?€? usually one or two times the size of RAM. Therefore, the size of the dataset will be constrained by that of swap space. Once the data size approaches the limits of the swap space, process time will jump exponentially and computation begins to bottleneck. In spite of the drop in price of memory, given the data size we work with today, the investment in hardware is still cost prohibitive.,64-bit architecture plays a critical role on in-memory technology, which allows more than 4GB of memory. Without 64-bit architecture, in-memory databases would be less effective and certainly would not be able to stand up as today?€?s breakthrough solution. Ever since the 64-bit CPU was introduced, we see efforts to recompile the code used to run on 32-bit architecture and port it to the new one. While 2^32 is a limited number, 2^64 is virtually infinite. When address space is nearly unlimited, it opens the possibility for us to redesign computational algorithms so that we can trade space for time., Peter Denning blew our mind in 1970 by introducing his work in ?€?Virtual Memory?€?, which manages memory resources in a more efficient way. However, if the concept is deployed properly, why do we still see ?€?out of memory?€? issues today? Shouldn?€?t we be enjoying 2^64 in virtual memory space? Despite what the name ?€?virtual?€? implies, memory space is preconfigured at OS installation with a limited swap space ?€? usually one or two times of RAM ?€? to back up the memory allocated from the heap. Once the data size hits its limitation, computations bog down and performance begins to suffer.,Now that the 64-bit CPU has arrived, and in light of the fact that a Linux system supports up to a 48-bit address space (256TB), this is ample volume for today?€?s big data needs. By using an adequate memory-mapping approach, one commodity machine can handle up to 256TB ?€? way more than swap space current in-memory technologies offer.,Imagine this. You are working on a 4x4 sliding puzzle. While you might easily solve it in five minutes, how about a 5x5, 6x6, or even a 10x10? As complexity increases, the computation problem comes to a head. Now, what if the puzzle is frameless? Let?€?s say you can put aside all the pieces, reorganize them on the side, and then reassemble them to solve the puzzle? You?€?re no longer constrained by space and you are free to arrange the puzzle at your leisure. This is what we say about trading space for time.,When the assumption that ?€?memory is a scarce resource?€? is no longer valid, we need to think outside the box when solving today?€?s big data issues. With virtual memory theory, 64-bit architecture, and affordable memory modules, you are now ,??to face the overwhelming challenges that big data poses.
The Big Data Yawn,Over the past couple of months we have met with a number of oil and gas executives to??demonstrate our Oil and Gas Solution built on Data-Tactics?€? Big Data Engine (BDE). During??these conversations it has become obvious that the very mention of "Big Data" produces an??involuntary physiological response among business leaders - eye rolls and yawns. It appears??that big data has reached the Gartner "trough of disillusionment". These executives have heard??from a bewildering array of software, hardware and services vendors trying to sell them a big data??solution. Essentially, every salesperson has presented: I have your solution, now tell me about??your problem. (Of course this is not a new sales strategy in the world of corporate IT.) There is??an inherent assumption that every business needs big data. However, the use cases touted for??the retail world just don?€?t apply to upstream oil and gas. These business leaders are desperate to??get past the technical jargon and talk about real solutions to real business problems.,Upstream oil and gas organizations are sophisticated, complex organizations with a rich??and diverse IT landscape. They already have search tools. They already have master datamanagement, data warehouses and BI tools. They already have visualization tools. They have??many, many systems that generate and manage operational and geospatial data. They don?€?t??need another shiny new tool. Just what exactly can big data do for them that their current??solutions can?€?t?,This is not an easy question to answer. To answer this question, you have to know a lot about??the current set of tools and their limitations. You have to understand the possibilities that??exist within the next generation of tools. You have to know how the old and the new can work??together. And you have to understand how their business operates and where the challenges or??opportunities exist. We at Sullexis have a deep background in BI, MDM and data warehousing.??We understand the current IT landscapes and the business challenges our clients in oil and gas??face. We are able to facilitate these discussions to define the use cases for big data solutions.,So the yawns stop. And the hard discussions begin. Our clients raise the challenges they have??either faced or read about: a big data solution requires a complex implementation with expensive??resources. The challenges of implementation surrounding the big data technologies include:,?€? the challenge of internal funding and sponsorship for a platform that could serve many??groups within the organization and could offer benefits that may not yet be quantified,,?€? the technical difficulty of standing up the big data stack,,?€? the cost and scarcity of experienced resources (both technical and data scientists),,?€? the support challenges,,?€? the lack of domain knowledge,,?€? and the vast and confusing eco-system of big data products and services.,Our partnership with Data-Tactics and the use of their BDE platform is demonstrating that many??of these challenges can be overcome very quickly and cost effectively. BDE is the product of??many years of partnership between Data-Tactics and various intelligence and military branches??of the U.S. government. Data-Tactics has wrapped the open source Hadoop framework, along??with many essential open-source tools, to provide an enterprise grade, production-resilient, multi-tenant platform that offers accelerators and tools to support high-volume data ingestion, data??indexing, management, search and discovery. The platform includes an out-of-the-box user??interface to discover and visualize data. The platform is extensible to support visualizations and??analytics using D3, R or other Hadoop compatible tools. With this platform, it is possible to start??loading data on day 1 and have end users begin exploring the data on day 2. This model lends??itself to an agile and iterative approach to involve end users in the visualization and use case??development process.,By leveraging the BDE cloud environment, we have been able to build relatively low cost and??quick proof of concept applications to help clients in proving out the benefits of their use cases??and recruiting internal support and sponsorship to move the big data platform forward. During the??proof of concept phase, we have been able to provide previously unseen insights to our client?€?s??data within a matter of days.,In one recent example, we completed a hugely successful proof-of-concept (POC) application??that was able to 'slide' the BDE in alongside a number of existing systems (SQL Server,??Exchange, SharePoint, File Server, structured database systems, etc.). We were able to ingest??some data and index in place other data from these systems and provide search capabilities??and advanced analytics for the end users. In this data fusion approach, we loaded >600 MBs of??both structured (databases), semi-structured (text files, email) and unstructured (PDF, MS Word,??images) data from more than 20 data sources in less then 6 days. By the end of day 21, answers??were being provided to questions that had been first asked 18 months ago but traditional BI??approaches were unable to answer.,By eliminating the need for up-front data modeling and embracing diverse datasets being??generated in most enterprises (e.g. sensor and operations data, email, SharePoint data, web/reference data), this platform opens the door to new kinds of analysis that can connect the dots??across all of this data. This capability has caught our clients?€? attention and the initial yawns??have been stifled. Most of our conversations are now about developing POCs to illustrate the??performance, lower cost of ownership, scale, and the speed of deployment to apply this new??technology to an existing, hard to solve problem. Bringing together disparate data platforms that??were previously silo-ed and providing the ability to report, explore or run analytics on the full data??set is indeed a solution for many problems.,Author:??,??and??
A traditional business problem customized here to data science.,1. Identify the problem,2. Identify available data sources,3. Identify if additional data sources are needed,4. Statistical Analyses,5. Implementation, development,6. Communicate results,7. Maintenance
Here's my list:
According to a report from Ministry of Higher Education and Scientific Research of UAE, polls published for the year 2013 by the American website ?€?Mashable?€? stated that three out of four people in the , own a smartphone making the country , , Saudi Arabia has been ranked third while Britain ranked ninth in the world. Surprisingly, with only 56.4% of penetration of the smart phones, the United States of America was ranked 13th.,The mobile market continues to be aggressively competitive in the UAE with prices dropping, new and advanced smart phones replacing the old ones every quarter and faster and more reliable wireless networks getting implemented. This can be one of the many reasons for consumers specially the youth to be connected and use trendy phones.,Increasing from 61% in 2012 to 74% in 2013, the number of smartphone users in UAE continues to grow and with the government?€?s initiatives to create smarter cities like Vision 2021, the penetration will just get higher.,The survey also noted that the Middle East?€?s largest Arab state, Saudi Arabia ranked first globally with 94% users accessing internet daily through smartphones followed by United States and Britain while UAE was positioned third with 91% smartphone web users.,These results further imply the growth of data in the region making the ,. Dubai?€?s smart city initiative, brainchild of His Highness Shaikh Mohammed bin Rashid Al Maktoum is yet another boost to the big data private players to integrate systems and work together with the public institutions.,Meanwhile on March 2nd, 2014, , to deliver a comprehensive range of enterprise mobility services and solutions to businesses of all sizes in the UAE. This will offer companies a lower entry cost for enterprise-grade mobility management with faster time to market, thereby minimizing the cost and risk of implementing and maintaining a mobility infrastructure. They also offer flexibility and scalability to any organization, geography and usage model.,4G still being nascent in the market, the country is already gearing up for the 5th generation networks that will generate and deliver greater data opening up new possibilities to create smart cities and smarter citizens.,Giving insights on such new developments, latest projects and government?€?s plan to use data to create smart cities, Smart Data Summit 2014 will be held on 19th and 20th May, 2014 at the Sofitel Dubai The Palm Resort and Spa.
This is an anonymized dump??of all ,. Each site is formatted as a separate archive consisting of XML files zipped via 7-zip using bzip2 compression. Each site archive includes Posts, Users, Votes, Comments, PostHistory and PostLinks. For complete schema information, see the included readme.txt.,??to see context.,Available when you participate in the ,.,The new Data Science Challenge: Detecting Anomalies in Medicare Claims will be available starting March 31, 2014. It costs $600 to partcipate. I guess they are worried that the data get re-sold or about some other potential data leaks. They also want real practionioners (an issue on Kaggle competitions), as students are unlikely to fork out $600. But if you participate, you get a copy of Hadoop to install on your laptop; this copy it emulates multi-node Hadoop.

All the regression theory developed by statisticians over the last 200 years (related to the general linear model) is useless. Regression can be performed as accurately without statistical models, including the computation of confidence intervals (for estimates, predicted values or regression parameters). The non-statistical approach is also more robust than theory described in all statistics textbooks and taught in all statistical courses. It does not require Map-Reduce when data is really big, nor any matrix inversion, maximum likelihood estimation, or mathematical optimization (Newton algorithm). It is indeed incredibly simple, robust, easy to interpret, and easy to code (no statistical libraries required).,Here we will address linear regression. Logistic regression can be turned into a linear regression problem using basic variable transformations, so the principles presented in this article also apply to logistic regression. In a nutshell, approximate solutions produce more robust predictions, and the loss in accuracy is so small, it is smaller than the noise present in the data, in most cases. We hope that the solution described here will be of interest to all data scientists, software engineers, computer scientists, business analysts and other professionals??who do not have a formal statistical training or lack proper tools (software). In addition, the solution offered here does not require fine-tuning mysterious parameters.,Beautiful data is when your data set is clean and meets all the conditions needed to make linear regression work well: in particular, the independent variables (sometimes referred to as features) are not correlated. This rarely happens in practice, but this is our starting point. Realistic situations are described in the next section.,Without loss of generality, we assume that the variables are centered, that is, their means are equal to zero. As a result, we are discussing a regression where intercept = 0, that is,Y = a_1 * X_1 + ... + a_n * X_n + noise,,where,For beautiful data, we also assume that the independent variables are not correlated, that is cov(X_i, X_j) = 0 if i < j.,Then the solution is straightforward:,a_i = cov(Y, X_i) / var(X_i), i = 1, ..., n.,Note that the regression coefficients have the same sign as the respective correlations between the associated independent variables and the response.,When independent variables are correlated - which is the case with ugly data, the typical type of data that we are all dealing with in practice - traditional linear regression does not work as well: it is subject to over-fitting, numerical instability and the fact that you can have many very different sets of regression coefficients that produce the same results.,My solution in this case is still the same:,a_i = M * cov(Y, X_i) / var(X_i), i=1, ..., n.,where M is a multiplier (a real number), chosen to minimize the variance of Z = Y - (a_1 * X_1 + ... + a_n * X_n). Note that Z = noise.,Remember that since we are working with centered variables, the mean value for the response, as well as for noise and independent variables, is zero. So multiplying by M has no effect on means. In the previous section (beautiful data), the multiplier is actually M = 1. There's a simple formula for M:,??M = { Y, (SUM b_i* X_i) } / [ (SUM b_i * X,_i) * (SUM b_i * X_i) ],,where,Confidence intervals can be computed by introducing noise in the data, and see how it impacts the estimated regression coefficients. Also note that even in the worst case scenario where all the independent variables are strongly correlated, my solution will work well and provide an optimum.,The quality of my approximate solution can be measured in different ways. Let's define,The following metrics measure how good my approximation is:,I generated a data set with 10,000 observations and four independent variables denoted as x, y, z, v in my program and excel spreadsheet. I have introduced cross-correlations among the independent variables to make it a realistic example. Note that in this case, because the data is simulated, we know the exact theoretical solution. The exact theoretical solution and the one provided by traditional linear regression are almost as good, their predictive power is almost identical, as measured using the three metrics described in the previous section.,My approximate solution is very good too, though not as good as the traditional regression. I went one step further: not only I used my approximate solution, but I computed the regression coefficients using only the first 100 observations, that is, only 1% of the simulated data set. The result is still very good. Here's the summary (the details are found in the??,):,Note that the three solutions (the three sets of regression coefficients) are very different, though the sum of the four regression coefficients is close to -0.85 in each case. But in each case, the estimated (predicted) values have similar correlation to the response: 0.69 for my approximation, and 0.72 for both theoretical and traditional solution. So in this case , = 0.69 / 0.72 = 0.96, pretty good. We also have , = 0.94. Finally, , = 0.89; both the traditional regression and my approximation manage to reduce the variance by approximately a factor 2.,My approximate solution has the advantage to be more stable (can't overfit) and easier to interpret, since the regression coefficients have the same sign as the correlation between response and associated variable., , , , , , , , , , , , , , ,More tests are needed to see when my solution really underperforms, especially in higher dimensions. Also, the traditional linear regression is performed in the Excel spreadsheet, using the Linest function. This computation should be moved to the program. Note that even to simulate the white Gaussian noise in the data, I did not use any statistical libraries. See program for details, in previous section.,Another research topic is to find out how sensitive the traditional regression coefficient are, compared to the coefficients obtained via ny method. One way to compare sensitivity is to introduce noise in the data and see how it impacts the regression coefficients. Another way is to reshuffle the white noise 1,000 times and see the impact on coefficients. Or you could also slice the 10,000 observations into 10 samples of 1,000 observations each, and compute the regression coefficients for both methods, in each sample, then use ,??to compute confidence intervals.,Finally, a better solution, when the number of variables is larger than 10, might be ,, where M is applied to a subset of variables, and M' to the remaining variables. This works if the independent variables are centered (if their means, after appropriate transformation, are zero). In our example with n=4 variables, one parameter M (a real number, not a matrix) works well. I suspect that with n=10 variables, two parameters work better. Such a solution might involve looking at all 2^n groupings of two variables (variables attached to M, and variables attached to M') for optimization, and for each grouping, compute the optimum M and M' by vanishing the first derivatives of var(Z) with respect to M and M'. This is just an easy generalization of what I did in the case where we use only one parameter M. More generally, I suspect that the optimum number of parameters might be of the order log(n), and also dependent on the data, when we have a large number of variables. More on this later.
Much has been written about??,??churn - predicting who, when, and why??,??will stop buying, and how (or whether) to intervene. Employee churn is similar - we want to predict who, when, and why employees will terminate. In many ways, it is smarter to to focus inward on employees. For one thing, it is far easier for an company to change the operations or even the behavior of an employee, than that of a customer. As will be seen, employee churn can be massively expensive, and incremental improvements will give big results.,The most important difference between employee vs. marketing churn is that a business??,??someone. Unfortunately, you usually don?€?t get to choose your customers. There is also more at stake - this person will literally be the face of your company, and collectively, the employees produce everything your company does.,Employee churn has unique dynamics compared to other problems. To jump-start the ?€?business understanding?€? phase of analytics efforts, we are writing a series of articles to translate employment processes into tractable data mining problems.,A new hire ideally ramps up to full productivity over months, going through on-boarding, training, certification. In one client engagement, a call center employee had to train for months to pass a Series 7 exam, before even being legally allowed on the phone. During all of that time, an employee delivered no value?€? they were just preparing to start working.,Figure 1,??shows a stylized cost/benefit plot for one employee across three years of tenure. At time zero, costs are very high - an expensive recruitment process, administration, training, supplies are all above the normal flow. In this model, after about a year, the main monthly expense is salary and overhead. In this hypothetical job, an employee takes a year to ramp up to full productivity. Different jobs will have different curves, but this sigmoid curve is common.,To decrease the overall costs due to employee churn,??,??has to budge on these curves:,Like quantitative scissors, there are no other options in this model.,Unfortunately, few companies have any idea of what these costs and benefit numbers are for any given role. Many have worked out the lifetime value of a customer to 5 decimal points, but few have ever considered the lifetime value of an employee. And, not all roles are ?€?producers?€? like sales reps or factory workers - for example, what is the monthly corporate contribution of a data scientist? Data Science may be??,, but no one really knows how much we ?€?make it rain.?€?,At Talent Analytics, we have found it simpler to evaluate employee cost relative to a potential performance level. Simple heuristics can begin to build the curves defined in??,. The shocker comes when we subtract (benefit - cost) and take the cumulative sum to find an break-even point..,In this stylized example, the employee starts providing monthly value after 10 months, and??,. By comparison, in our engagements we often see impressive attrition after just 3?€?6 months.,Customers provide profit right away, so customer churn analytics is just trying to keep the gravy train rolling. Employee churn analytics is more like trying to get the train to run long enough to provide any value at all.,With the employee value proposition laid out, we can begin to crack this nut and save the business some money. We are looking for signals that will let us score the likelihood of a person to stay in a role inside a given time window. By deploying the right predictive model, we can decrease the impact of one or more of the ?€?scissor points?€? above.,Hint: The most powerful place to solve this problem is before you cut the first paycheck.,There is much more to this subject. In future installments, we will consider:,As an experiment, we are putting the R code for this cost model and its plots on GitHub. It is a public project for all to try, modify, and share at??,??. Feel free to ?€?pull request?€? any improvements to make this even better. We will build up this toy model as an engine for this series. Please engage!
 , ,References:??,1) Pirttim??ki, V., L??nnqvist, A., Karjaluoto, A. (2006). Measurement of Business Intelligence in a Finnish Telecom-munications Company. ,, 4(1), pp. 83-90.,2)??, , , , , , 
Feel free to add your keywords. Here's a start:,:
Embodiment is comparable to the idea of an ?€?ecosystemic?€? or ?€?holistic?€? approach. In an ecosystem, each thing affects everything else. In light of the interrelationship, a person would not attempt to correct a problem by considering only a single piece of the puzzle. Instead, there is a need to bring together many aspects of the body. To understand embodiment, it is necessary to recognize how ?€?the body?€? separates an organism from its environment; in a manner of speaking, the body represents the system through which existence occurs. It is a line of defence against non-existence. It provides a buffer. Life buffers us from death. Without a body, there is no means of transition but only direct contact. I have said that data tends to be rather disembodied. Sales figures have few connections to the outside world; drastic declines might be difficult to explain until after the fact. People love a product until they hate it. People live forever until they die. The world loves Americans an act of terrorism. There is no lack of data. It is just that so much of the data is disembodied reflecting its intended use. The data that we collect is almost never meant to hold anything beyond what we would like to know. I believe that data embodiment is a step towards true understanding.,In order to achieve data-embodiment, giving shape to the system of the data, it is necessary to perceive the system within the context of its changing environment. The tendency will be to yank out a stream of data in a linear form as if this were the only way data can be handled. It might be the only way data can be comfortably handled by people. This is not the case when processing is done by computer. I would say that the interaction with environmental change is easiest to understand through the use of an index, ranking, ordered scale, procedural pattern, or fractal. The system can be shaped through event distribution, a separate and rather detailed topic. I am going to use some random data to just demonstrate the dynamics although a bit later will try to include some real data. Assume that an organization has 200 events available. The organization can distribute these events each day to delineate its existence in the local environment. A public school might distribute a few hundred events to note incidents of bullying. An event selection might include the following: stranger noticed in building; teacher absent; unauthorized vehicle at perimeter; assembly of children at bad location; poor lighting; gang members spotted. In terms of the scale or gradient, assume 20 divisions: 0 at one extreme means no bullying; at 19, there is violence and bullying with multiple perpetrators. Each day, the events are distributed under the applicable gradient measurements.,Since I have chosen to discuss a random sample at this time, I would expect the ?€?Crosswave Differential?€? to be somewhere near or at the middle of 0 and 19. A Crosswave pattern (XW) is established by how the events interact with the gradient: as one goes up the gradient, fewer events will exceed the set point, and more events will fall below it. Intersection occurs when there is a balance of events on either side of the set point. On the other hand, a Crosswave Differential (XD) is created when there are two sets of measurements as in the case of ?€?treated?€? and ?€?non-treated?€? events. This does not mean the event itself is treated; it means that the event has been invoked in order to signify treatment. Strictly speaking, the absence of a treated event does not necessarily mean the absence of treatment. One cannot safely infer details in the absence of data using the event-distribution method. Only the invocation of a non-treatment treated event can confirm that treatment never occurred. It might to difficult to appreciate the logic in this until a person has to deal with an actual problem scenario. I believe that the illustrations below provide a reasonably good explanation of the differential.,The next illustration is from the actual simulated data. ??I selected a handful of events to signify ?€?treated.?€? ??Both the treated and non-treated events result in the same general XW location, resulting in an XD of 0. ??In order to improve visibility, I left out some details such as the bars and gradient values. ??The XW locations are about the same when I randomly select events from a body of random events, which is my main point. ??(TO means treated optimal; TS treated sub-optimal; UO non-treated optimal; and US non-treated sub-optimal.),Ad Naseum, as Mr. Howard my high school math teacher used to say, we reach the midway point for all these illustrations. ??I now introduce another type of illustration that I call the ?€?Push-Pull Analysis?€? (PPA). ??Without much preamble, I feel that an objective person would find that the treated events have little bias either to their right or left. ??(Although this is not necessarily the case for other monitors, in this simulation the left represents the least amount of bullying while the right is the most.),I can?€?t say that I have a strong attachment to numbers. That is why I write computer programs to do all of the work for me. Alright, so now I will make a ?€?non-random?€? selection of events. As part of a diligent risk-management program to minimize bullying in the school, assume that I have asked to systematically identify those events that might contribute to bullying. I don?€?t know what these events will be ahead of time. I only know them by their XD score. I will pick a number of events with extreme differential values and then use the selection to define my next treated group. The previous events in the treated group were as follows: e10, e20, e30, e40, e50, e60, e70, e80, e90, and e100.,The ?€?non-random?€? event choices are e20, e148, e47, e171, e158, e43, e167, e4, e192, and e132. Usually the resulting illustrations are not as clear-cut as those shown below. I feel that in real life, changing events might not necessarily lead to any particular outcome. However, the rationale is to target the most pertinent events and then use actual field results to confirm effectiveness. An event might indeed be entirely random ?€? as is the case for each event here ?€? meaning that the outcome should not necessarily change even if I change the events. An intervention is a type of event where the outcome can be incorporated into the metrics. I feel that an act of intervention combined with its control-effect does more to suggest effectiveness than ineffectiveness, but I would agree that there is much room for debate. An event can be as simple as ?€?patrolled hallway between 2:30 ?€? 3:00 PM.?€? Below I show that the XD is no longer 0; the point of equilibrium for the treated events is now further to the left (closer to 0). The objective is to try to push the pattern towards the desired direction.,The next illustration is from the same non-random selection. ??The bars are rather flawlessly biased, which I must emphasize is extremely peculiar. ??I expect some bias just nothing quite this clean. ??I wonder if I should show a second random selection where in fact I doctored up the algorithm to be a bit biased for specific events; this would probably better reflect real-life conditions where events simply aren?€?t distributed randomly. ??For instance, there might be existing security protocols to prevent bullying and violence. ??The existence of events (that is to say, their distribution) can therefore be entirely deliberate, as is the case for an intervention event. ??I will ask readers to take my word for it: ??even the non-random distribution of events leads to similar dynamics. ??When events are deliberate rather than random, just by ?€?chance?€? sometimes the random event might seem superior! ??Make use of statistical tools to examine the situation further. ??In a worst-case scenario, learn by trail-and-error. ??I should also mention that in real life, I suspect that the reality is probably dynamic. ??People will adapt to each other. ??What works for a little might cease to do so. ??There is a perpetual need to gather information, study, strategize, and take action.,All of that effort has led to the successful identification of events to seem to reduce bullying as indicated by in distribution above; but this is not to say that taking action in real life will actually lead to a reduction. In fact, as I mentioned earlier, the data being entirely fabricated, it is possible to target events that are purely random in nature, thus rendering the selection superfluous. So what is a person to do? Well, if bullying were a game of chance, then it would indeed make no sense to try to control events. If however bullying were merely quite difficult to understand and control due to its complexity, then I genuinely feel that an embodied approach gives us the best chance of dealing with the problem. The knowledge would persist in an open framework. Every past strategy that has succeeded or failed is retained along with the context of both intervention and outcome. I therefore suggest, even in the event of intervention failure, irrespective the decision-making regime, data embodiment represents an excellent approach to record-keeping. We gain a more robust understanding of implementation problems; this can help organizations better learn from past experiences.,All right, before I end this blog I will go ahead and introduce some data from my own records. I keep track of how well I sleep. My assessment is entirely phenomenological although anchored. ?€?Phenomenological?€? as it applies to the data in this case means that I make a qualitative assessment based on the reality as I perceive it rather than through a method that is objectively verifiable by somebody else. Not all my monitoring programs are based on such gradients. For instance, I also keep track of my weight, which is not a matter of perception but confirmation. When I say ?€?anchored,?€? this means that I try to maintain consistency by having firm reference points. So I often ask myself whether or not a particular reference is applicable. In the left image, there are 54 samples in the untreated, becoming 164 in the right image; there are 79 treated samples becoming 140 in the right. Based on all of this, it seems that the first image is for first 133 days of data; the second for the first 304 days of data. I think that the treatment product is pretty familiar to most people. However, not being a doctor I will exclude the name of the product from this blog. Let?€?s just say it is a legal product that has been around hundreds of years with a good reputation apparently still used by some people for their headaches.,My sleeping is normally between ?€?better?€? and ?€?super.?€? The software that I created for my record-keeping, which I call Tendril, has the ability to incorporate lag into the analysis. This essentially means that it can connect what I did today with what happens tomorrow. I just want to point out a few things from the images. The first thing I want to point out is how I have images pertaining to my personal health, which is really a splendid thing. I also use Tendril to keep track of my exercise routine and driving routes. It makes life much easier being able to put all of the experiences into a safe and accessible place. An interesting idea about ?€?lag?€? is how benefit can persist over a period of time after treatment. The images show from left to right a slight shift towards super for the untreated samples. I can?€?t say that the shift is entirely about the product in question; in fact, the shift is probably about a lot of things. I also find that the differential can narrow as a result of rationalization: that is to say, as I apply treatment over a longer period of time, the apparent benefits might decline as I encounter different circumstances.,By no means am I saying that this product helps people to sleep. I am saying that for me it has been associated with improved sleep perceptions. I neither encourage nor discourage people from using such a product or any product. Nor should people consider taking this or any product without talking to a doctor. I just want people to understand that I am not nor do I claim to be a medical professional. ??Please do not misinterpret this post as any type of medical advice. This is a blog about data science focused on the handling of data and certainly not on the treatment of disease. Just to emphasize the point, I close the blog with one final image from my ?€?curling?€? below. In Canada, curling is competitive sport. I understand that we have among the best curlers in the world. However, when I say curling for me I mean lifting 10-pound weights, which has likewise been associated with improved sleep perceptions for me. I would like to mention that in fact that many of the more promising ?€?events?€? seem to be exercise related, which makes perfect sense given that I work under conditions of limited mobility. During a long the journey, given the right tools, I imagine that a person can build up a substantial pool of useful data through embodiment. ??Data-embodiment is about understanding the context of problems concealed in the murky fens of facts and details. ??How people might choose to deal with such insights is a separate issue.
Managing performance of enterprise applications and achieving high levels of Performance with minimum resources is topic of discussion in today?€?s large enterprises. Resolving performance issues is essential for database administrators (DBAs) when it happens however it is best to react to the problems proactively. Proactive management requires very high level of attention and to help make sense of the overwhelming data provided by the database engine.,In database management being proactive is essential because of its criticality, complexity and productivity. Formalized service levels reduce the tolerance for database downtime. Due increased number of features and metrics in new releases DBAs also face increased complexity form data volume growth and due to the fact the organization are deploying database at an ever increasing rate.,To address these challenges most proactive DBAs set clears objectives, about performance and the outcome. The also use high quality data to make better decisions. More urgent problems need faster resolution. Proactive DBAs may seek higher quality alerts earlier in the problem lifecycle, immediate visibility into root causes and options for automating recovery involving risky and time consuming tasks. Use of historical data to recognize degrading performance conditions and prevent them from causing problems is very effective.,Increasing the efficiency of existing hardware provides direct savings; For example, deferring a hardware upgrade by just a few months can be significant. The optimization process also generally improves transaction throughput, making for happier users.,Changes are required when new application code is deployed into production or platform components are upgraded (hardware, operating system or database). DBAs may have confidence that post-change performance will equal or exceed expectations. DBAs also need to anticipate workload changes and determine hardware break-points for applications that are subject to substantial volatility.
Here we go. Enjoy the reading!,Illustration of YARN (from first article below),The starred article is very popular.

The new content can be found ,.
: Update on the Closure of the Physics and Astronomy Reading Room,Hello Huskies,,As of June 13, 2014, ,. You can see the??University's statement about the initiative??,.,This change has been met with some ,??by??students that used the space for studying and its various texts and stacks, so we??decided to do some of our own research into it., ?€?, Our ,??at the President's??Desk describes the change, what it means??for students and our efforts on it.,From our research, it appears that the library will no longer be as secluded or??quiet, but the more active Data Science Studio will provide greater access for??students through increased operational hours and more group studying areas.,Although there are no new closures of departmental libraries planned, our role in??this process is to ensure that student input is better considered in any further??discussions about re-purposed space. We're currently working on developing the best??mechanisms for input in those decisions and welcome your thoughts.,Sincerely,,Michael Kutz | President, Associated Students of the University of Washington, asuwpres@uw.edu
This morning, I received the following in my mailbox, from Kaggle:,GE has revealed the private leaderboard on,-- and,The competition has been running since August as the second part of GE's Industrial Internet Flight Quest with a challenge to develop algorithms that increase flight efficiencies in real time, reducing delays and maximizing a flight's profitability. Using national airspace data provided by FlightStats, the winning algorithms determine the most efficient flight routes, speeds, and altitudes by taking into account variables such as weather, wind, and airspace restraints. The 1st place model by Jos?? Fonollosa proved to be up to 12 percent more efficient when compared to data from actual flights.,Flight Quest 2 was another massive step in GE's innovative approach to imagining the future of aviation. With some 3800 submissions to the leaderboard, Kagglers simulated well over 50,000,000 flights by modelling their waypoints into the scoring Simulator. Our hats are off to the hard work of,on the leaderboard?€? this is one of the most challenging problems we've ever hosted.,Our third Masters comp launched recently by a North American Credit Card Issuer who is modeling their,. Including this challenge, that's $70K + $125K + $100K =,in prizes over a six??month period! If you have not earned a second finish in the Top 10% elsewhere on Kaggle, these next few months will be a great time to seek your,.


As spring begins and bees collect pollen for honey, baseball statisticians collect millions of interesting facts about this year?€?s baseball season. Ever since the 1870s when sportswriter Henry Chadwick began pulling together players?€? data, people have been keeping score on everything from hits and runs to strikeouts and averages.,This year is no exception. There are more than??,??playing baseball in the United States alone, distributed between 30 Major League Baseball teams, college teams, little league teams, and guys like me playing on the weekends. That?€?s a lot of data to collect, even for the seasoned statistician.,Baseball stat fanatics love their averages. They even collect obscure facts such as Weighted On-Base Average (also known as wOBA). According to Riley Brown?€?s data blog entry ?€?,?€?:,?€?[The] wOBA combines the relative values of each offensive event and weighs them against the actual run value to their team. For example, a single during the 2012 season had a weight of 0.884 because that is the league-average run value that the single will produce. The final number is calculated similar to Slugging Percentage, but encompasses more ways in which a batter can reach base.?€?,Here?€?s how one MLB Division?€?s the 2010 wOBA looks, according to Beyond The Boxscore?€?s Bill Petti:,While the presentation looks simple enough, the data is based on multiple layers: batting average, walks, being hit by a pitch, etc.,This is where a personalized analytics tool like BIRT can help. Remember that in assessing Big Data such as baseball stats, you?€?ll need a powerful tool that can scan multiple sources of disparate data and present it in a compelling way. So swing for the bleachers with the BIRT Viewer Toolkit.,The Viewer Toolkit, a free companion package to the Eclipse open source BIRT Designer,IS MADE for developers seeking to embed or integrate BIRT reports within their web applications. The toolkit is a supported, commercial-quality alternative to the sample viewer included in the open source distribution of BIRT.,The Viewer Toolkit allows for:
As my first post on DSC, I want to provide a perspective on what is limiting the impact of Analytics on modern business. At present, I'm unconvinced of the arguments about enough talent to go around. While I do believe there is a talent shortage, I also consistently see a misuse of the talent currently in the market. What could cause this misallocation of resources? Ignorance, plain and simple.,I'm constantly frustrated by the lack of awareness by business professionals as to how to leverage their existing analytics talent. However, I'm also frustrated by the lack of interest--a??certain stubbornness--on the part of the analytics professionals in the domain or business side of the equation. Both??parties need to raise the bar on themselves in order for the broader industry to make real breakthroughs in efficiency and effectiveness.,If you are graduating with an MBA these days, you have absolutely no excuse not to have a basic understanding of statistical analysis and forecasting
There was a television interview of a man who had to deal with severe breathing problems through much of his adult life.?? He underwent various procedures although he found little relief.?? In fact, the adverse impact to his body was long-lasting.?? So he was surprised to discover how he was extremely allergic to his breakfast cereal, something that he ate almost without fail.?? When he stopped eating the cereal, his breathing problems disappeared.?? I am sure that many people would regard the plight of this individual as a unique case.?? However, I have found a number of cases in the literature showing that people are often capable of tolerating inhospitable conditions.?? I would even suggest that in the world we occupy, we have learned to normalize toxicity and stress.?? Our senses have learned to bury the reality such that they become imperceptible.?? A way out of this is through the use of technology, adding a sense or perceptive ability that few if any of us had at birth.?? I call this sense ?€?ghost tracking,?€? the ability to use technology to follow obscure and intangible pieces of information in order to solve different types of problems.,When I was younger, there was a movie called ?€?Ghost Busters.?€??? These were in-your-face ghosts that seemed unusually well embodied albeit rather oozy.?? I recall a Japanese anime character called a ?€?ghost detective?€? that seemed frequently surrounded by things spiritual as if those things were also tangible.?? But from my technical interpretation of ghosts, these are not things that necessarily conform to our perceptions of reality.?? For instance, one might not perceive the shadow of one?€?s own death in the numbers: it might be unclear how a particular course of action, if it occurs under specific circumstances, might lead to death.?? A slight breathing problem from a food allergy might not be particularly life-threatening on its own.?? If we add the flu to the equation, perhaps even a minor increase in incapacity can make a challenging medical condition quite dire.?? Not all of us get to enter the ?€?front gates?€? of Valhalla through mighty blows in combat:?? lance, rapier, axe, broadsword, or grenade launcher.?? Nay, we are haunted through life with relatively common disabilities; we might not be fully aware of the patterns and signs around us heralding the onset of our final battle.,A ghost is a ghost because it falls outside our primary senses.?? As such, what we might interpret to be a ghost might be something else completely.?? Many of us find ourselves stuck in traffic every day only to hear of further perhaps more serious construction to come in the future.?? It might not seem that somebody has just said, ?€?You will die early in life due to hypertension.?€??? Our human senses are just not designed to communicate with us in such a way.?? What is a human body to do??? It might have to invoke a symbolic message using an avatar in dream state.?? ?€?Dude, this is killing you.?€??? Well, putting aside speculative communication with the spirit world, there is actually a real need to come to terms with intangible data, the things that don?€?t ooze or come at the face but rather hide outside our immediate perceptual realm.?? I would say that if all of us could spend a lot of time trying to sort out our individual problems, we would likely have some success doing so.?? However, we frequently suffer from time deficits.?? Part of the problem might be related to our problem-solving methodologies and perhaps the math, something I expect cover in other blogs.,I often refer to a computer program to help me determine what things seem associated with better sleep.?? Consider my earlier story about the man dealing with breathing problems due to an allergy to his breakfast cereal; this is not a situation that would likely affect me given that I also monitor breathing problems and apparent allergic reactions.?? The implication here is that I do not rely on a central information authority.?? I should also point out that I have superior information compared to any central authority at least in the areas of interest to me as the data pertains to my own body.?? So if somebody releases a report pertaining to Omega 3 and Omega 3-6-9, without necessarily questioning the report I can determine whether or not there might be particular benefits to me specifically.?? This is not to say that I run clinical trials on myself.?? I can say whether visiting particular gasoline stations might affect my breathing.?? Those interested in making a blanket statement involving people more generally might need to conduct elaborate trials; however, after the money is spent, the end-result of such a process is a generalization relying greatly on human normalcy in order to be relevant.?? As sure as the sun rises in the morning, some percentage of the population will indeed be allergic to hydrocarbons or the detergents in the gasoline.?? Some people will have horrific allergies to breakfast cereal.?? No amount of expenditure on centralized information will alter the dynamics.,The question here is whether people are open to the idea of believing in something they cannot see or touch; rather distant from mainstream society; and having less social acceptance than even disease.?? To believe in personal ?€?signs?€? that others cannot confirm is indeed like believing in ghosts.?? ?€?Why don?€?t you want a slice of cake??€??? ?€?I just know this is going to kill me.?? It?€?s going to make me fat; then I?€?m going to die.?€??? If find myself in that situation, I boldly accept the cake just to impress those around me.?? ?€?No, sir, I don?€?t believe in such foolishness.?? Watch me eat that cake.?€??? Then I ?€?double-up?€? on the goodness.?? Okay, there is absolutely no evidence that cake is good for me.?? All right, eggs in virgin olive oil, cheddar cheese on top, decaffeinated black coffee, and some dark chocolate; the data suggests that these things ?€?might?€? help me.?? This is not to say that such a combination would necessarily be helpful to people in general in all sorts of circumstances but rather me specifically in those situations that I tend to find myself.?? Decentralized information works in this manner:?? it is person-, site-, and situation-specific.,To reach a point where a person deliberately makes decisions based on personal data is something of a leap of faith.?? A person needs to have trust in his or her own handiwork.?? But on a more fundamental level, it is necessary to believe in signs; to believe that in how the future can be foretold; and that the evidence is openly available for the taking.?? One only has to reach out and grasp its quivering mass.?? Within this framework, there are some simplistic perceptions that can interfere with how people make use of personal or decentralized data.?? I introduced the idea of ghosts early in the blog because I consider it useful on different levels.?? As explanation, I will boldly post some of my own data and discuss some of the basic features.?? The following images are from the ?€?how well I am sleeping?€? data, which I mention periodically in blogs I guess because it is my oldest and therefore most complete database.?? Now, the graphical interface called ?€?Storm?€? is actually a bit peculiar.?? I would summarize this peculiarity as follows:?? it makes use of kinetic decay gradients.?? If the underlying activity fades, the plume patterns start to become incoherent.?? Conversely, if the activity becomes quite intense, the patterns adjust in order to normalize the extreme conditions.?? For instance if the price of a stock increases dramatically, Storm automatically forms a new baseline to reflect the sustained level.,So here we take a look at my sleep data.?? The first thing that I want to point out is that the data is ?€?normal?€?; that is to say, it sort of resembles any other data that might be plugged into the graphical environment.?? My sleep perceptions exhibit cyclical highs and lows.?? However, I believe that most people would agree that there are more areas of green (highs) than areas of blue (lows) on the image; this is because I am deliberately trying to push the green.?? In the long-run, this is a futile effort, since Storm will compensate by raising the normal thereby preserving the kinetic sensitivity of the imaging environment.?? But consider the conceptual implications of being part of a cyclical pattern.?? It is actually a ?€?concept?€? to think that I can have any influence at all by altering the things that I eat, consume, and do.?? Some portion of my evaluation most certainly contains environmental determinants.?? Using an analogy, a person might take a pill to deal with a headache, but this doesn?€?t alter the existence of stressors in the environment.,All right, so where is the ghost??? Well, examining the bottom portion of the images carefully, one would see that I seem to be headed towards increased blue. ??I might experience some difficulty sleeping well. ??This is probably not a big deal given that the baseline for the systemic low has improved over time.?? But I consider it necessary to be more cautious and to squeeze every possible advantage out of the numbers.?? So over the next few months, it might not be a good idea to risk that deadly slice of cake.?? I might not be able to buffer the consequences.?? This is the ghost.?? It?€?s the spook.?? Actually, it?€?s not all that spooky if we accept the premise that each of us is connected to everything else.?? We are systems within ourselves, but we are also part of systems outside us; and more often than not, the parts of us that reside inside are becoming subordinate to the social constructs and determinants outside.?? Our identities are not entirely ours to define. ??So what seems to be a "sign" is really just plain normal. ??The ability to sense it is new.,I want to point out that the illustrations focus on a particular type of data:?? it is data that I would like to control or influence.?? It is not data that I directly control.?? I often directly control what I eat and do, and in certain respects it wouldn?€?t be constructive to examine data that I directly control through the graphical environment.?? I am distinguishing data resulting from the decisions made from the data representing a particular outcome of the decisions.?? If a delivery service makes many thousands of driving decisions every day, I would use this graphical environment to evaluate not the ability of the service to make decisions (unless it has a problem making decisions) but the multifarious impacts of those decisions.?? Why not use the graphical environment to consider each individual decision??? Since there might be millions of decisions each day for any given organization, the correct approach is to handle the decisions in an embodied, objectified state as big data.?? This provides an accessible and persistent framework for future analyses.,I wanted to be the first person to juxtapose ghosts and algorithms. ??I believe that I have succeeded to some extent.?? Usually when people talk about ghosts, they mean something spiritual.?? I mean something intangible.?? Others are haunted by ghosts.?? I am saying that there can be signs or a foretelling of events.?? Some feel that ghosts wander over the world after existence.?? I suggest that we are all connected, and our realities unfold not entirely within us but as a result of our interaction with society and the world.?? So our existence is not substantively ours to write but at times rather incidental.?? However, as humans, we sometimes lack the natural ability or perhaps inclination to perceive our existence outside ourselves.?? We can augment our senses.?? We can use algorithms.?? In our perception of ?€?sales,?€? there is some presumption of internal locus of control.?? But a sale is something we would like to control.?? The factors that we directly control - we leave these to disembodied spirit world - so great is our inability to harness the power of information.?? Sales wander the world decapitated and alienated, foraging behind desks and trash bins.?? It is time to reunite the metrics to their bodies.
Every time a new technology disrupts the job market a "skills shortage" is debated between economists and politicians. The story isn't new - 10 years ago recruiters were asking for programmers with 10+ years experience in Java. The gap is widened by non-technical recruiters employing rigid traditional hiring practices. , - we're generalists, not specialists. The onus is on the candidates to overcome this myth and sell their abilities as a member of a Big Data team.,How do I enter the race?,Some things never change: cold hiring is still rare. You have to play the popularity game. Your job will probably come from a friend neighbor, classmate, or family connection. Facebook can increase your network, but social media 'friends' will only help you find a job if they actually know what kind of worker you are. Social coding sites like StackOverflow and Github are biased towards stars - the payoff only comes to a few shining profiles. , The wild web is full of gems waiting to be mined for fun and profit. Stick to a subject you know and stay away from things that are overdone. We don't need yet another tag cloud - unless you have a way to make them more useful.,A blog gives you the freedom of using your own voice, which is best used sparingly. Save up your best thoughts over time and release them in a coherent series on the appropriate forum. I keep my LinkedIn and Twitter feeds uncluttered, so the links to these posts are easily found by recruiters. Thank you to everyone who has liked or commented one of my articles.,How do I get past HR?,The most effective way to cut through HR is to have exactly what the hiring manager is looking for. Describe your skills as specific and uniquely searchable as possible. ,Libraries, not languages.,Old style resumes don't cut it anymore. , Write the libraries you worked with instead. For example, writing "lxml" provides much better information than "parsing xml in python.",API calls, not libraries. ,Libraries that are very popular are equally problematic. It's best to list the API calls you used the most. Instead of writing "Hadoop", try "hadoop.mapred.MapRunnable". Now the hiring manager instantly knows what your core experience is. When they search "MapRunnable", your name will come up right away. , When the engine finds your blog posts that have code snippets with "MapRunnable", your name will bubble up to the top.,Geeks understand social media's privacy ramifications and sign up for it all the same. Facebook and LinkedIn pay the bills by serving up our personal information on a platter. , All they need is my full name and my StackExchange/Github/Twitter handle. ,Here is my current CV: ,{firstName: "Peter", lastName: "Higdon", username: "McPedr0", qualifications: "i m smrt"},Ok, so I'm not on the hunt, but still - I'm only half joking. , Such meme-worthy irony is why slashdotters troll "Data Science" as buzzword soup and empty hype.,The trolls aren't wrong: there's a lot of people that think being a data scientist means dumping everything you can find into Hadoop, running a few R scripts over it, and making a couple pictures with Spotfire - ,. Miko Matsumura's "Data Science Is Dead" makes a compelling argument against fostering a "rotting whale-carcass of data". In order to be taken seriously as a profession, we need to standardize credible work-flows, foster successful transitions of existing talent, and discourage confusing new marketing inspired words like "thick data". ,Who are the "hidden" candidates?,One of the beautiful things about working with data is that it always provides concrete context. ,. That's how baseball fanatics turned into actuaries 60 years ago and it's why hedge funds hire poker players today. Data context allows knowledge workers to transition from anywhere that is data intensive... which is nearly everywhere these days.,The field is less than a decade old and requires cultivation. ,. Financial incentives are so strong that new learners and trainers enter the field every day, ensuring that managers will get their chosen flavour of pre-packaged workers eventually.,In the meantime, ,There is no "talent shortage", and spreading the myth has consequences:,- Undermines professional credibility with ironic self-contradiction,- Excuses management from investing in employee training to bridge their gap,- Perpetuates unrealistic expectations of the "ideal candidate",- Bars entry for good candidates with relevant skills that don't fit into HR's pigeon hole
If I want to build a house, wouldn't it be wise to learn carpentry? Does the analogy hold for data-analytic multivariate models? Or is it simply enough to let a machine do it, with no knowledge by the machine operator of how to interpret the results from those modeling efforts? Or is it true, as one person has recently asserted, that he could replicate ALL statistical procedures and techniques using MapReduce, without knowing anything about statistics and probability, or the vast collection of discipline-specific applications of statistical science in economics, the social sciences, the physical sciences, (including physics and chemistry itself), business or organizational management, archaeology, anthropology, and other historical sciences (evolutionary biology and genetics), and biostatistics, to name a few? Will machine learning supplant all of these careful developed approaches to problems that are peculiar or particular to a very large array of efforts aimed at scientific advance? ??

" In this article, we will discuss the so called 'Curse of Dimensionality', and explain why it is important when designing a classifier. In the following sections I will provide an intuitive explanation of this concept, illustrated by a clear example of overfitting due to the curse of dimensionality.,Consider an example in which we have a set of images, each of which depicts either a cat or a dog. We would like to create a classifier that is able to distinguish dogs from cats automatically. To do so, we first need to think about a descriptor for each object class that can be expressed by numbers, such that a mathematical algorithm, i.e. a classifier, can use these numbers to recognize the object. We could for instance argue that cats and dogs generally differ in color. A possible descriptor that discriminates these two classes could then consist of three number; the average red color, the average green color and the average blue color of the image under consideration. A simple linear classifier for instance, could combine these features linearly to decide on the class label:, However, these three color-describing numbers, called features, will obviously not suffice to obtain a perfect classification. Therefore, we could decide to add some features that describe the texture of the image, for instance by calculating the average edge or gradient intensity in both the X and Y direction. We now have 5 features that, in combination, could possibly be used by a classification algorithm to distinguish cats from dogs.,To obtain an even more accurate classification, we could add more features, based on color or texture histograms, statistical moments, etc. Maybe we can obtain a perfect classification by carefully defining a few hundred of these features? The answer to this question might sound a bit counter-intuitive: ,. In fact, after a certain point, increasing the dimensionality of the problem by adding new features would actually degrade the performance of our classifier. This is illustrated by the following figure, and is often referred to as 'The Curse of Dimensionality'., , As the dimensionality increases, the classifier's performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance.,In the next sections we will review why the above is true, and how the curse of dimensionality can be avoided., In the earlier introduced example of cats and dogs, let's assume there are an infinite number of cats and dogs living on our planet. However, due to our limited time and processing power, we were only able to obtain 10 pictures of cats and dogs. The end-goal in classification is then to train a classifier based on these 10 training instances, that is able to correctly classify the infinite number of dog and cat instances which we do not have any information about.,Now let's use a simple linear classifier and try to obtain a perfect classification. We can start by a single feature, e.g. the average 'red' color in the image:,The above figure shows that we do not obtain a perfect classification result if only a single feature is used. Therefore, we might decide to add another feature, e.g. the average 'green' color in the image:,Adding a second feature still does not result in a linearly separable classification problem: No single line can separate all cats from all dogs in this example. Finally we decide to add a third feature, e.g. the average 'blue' color in the image, yielding a three-dimensional feature space:,In the three-dimensional feature space, we can now find a plane that perfectly separates dogs from cats. This means that a linear combination of the three features can be used to obtain perfect classification results on our training data of 10 images:,The above illustrations might seem to suggest that increasing the number of features until perfect classification results are obtained is the best way to train a classifier, whereas in the introduction, we argued that this is not the case. However, note how the density of the training samples decreased exponentially when we increased the dimensionality of the problem.,In the 1D case, 10 training instances covered the complete 1D feature space, the width of which was 5 unit intervals. Therefore, in the 1D case, the sample density was 10/5=2 samples/interval. In the 2D case however, we still had 10 training instances at our disposal, which now cover a 2D feature space with an area of 5x5=25 unit squares. Therefore, in the 2D case, the sample density was 10/25 = 0.4 samples/interval. Finally, in the 3D case, the 10 samples had to cover a feature space volume of 5x5x5=125 unit cubes. Therefore, in the 3D case, the sample density was 10/125 = 0.08 samples/interval.,If we would keep adding features, the dimensionality of the feature space grows, and becomes sparser and sparser. Due to this sparsity, it becomes much more easy to find a separable hyperplane because the likelihood that a training sample lies on the wrong side of the best hyperplane becomes infinitely small when the number of features becomes infinitely large. However, if we project the highly dimensional classification result back to a lower dimensional space, a serious problem associated with this approach becomes evident:,The above figure shows the 3D classification results, projected onto a 2D feature space. Whereas the data was linearly separable in the 3D space, this is not the case in a lower dimensional feature space. In fact, adding the third dimension to obtain perfect classification results, simply corresponds to using a complicated non-linear classifier in the lower dimensional feature space. As a result, the classifier learns the appearance of specific instances and exceptions of our training dataset. Because of this, the resulting classifier would fail on real-world data, consisting of an infinite amount of unseen cats and dogs that often do not adhere to these exceptions.,This concept is called overfitting and is a direct result of the curse of dimensionality. The following figure shows the result of a linear classifier that has been trained using only 2 features instead of 3:,Although this simple linear classifier seems to perform worse than the non-linear classifier illustrated above, the linear classifier generalizes much better to unseen data because it did not learn specific exceptions that were only in our training data by coincidence. In other words, by using less features, the curse of dimensionality was avoided such that the classifier did not overfit the training data.,The following illustrates the above in a different manner. Let's say we want to train a classifier using only a single feature whose value ranges from 0 to 1. Let's assume that this feature is unique for each cat and dog. If we want our training data to cover 20% of this range, then the amount of training data needed is 20% of the complete population of cats and dogs. Now, if we add another feature, resulting in a 2D feature space, things change; To cover 20% of the 2D feature range, we now need to obtain 45% of the complete population of cats and dogs in each dimension (0.45^2 = 0.2). In the 3D case this gets even worse: to cover 20% of the 3D feature range, we need to obtain 58% of the population in each dimension (0.58^3 = 0.2).,In other words, if the amount of available training data is fixed, then overfitting occurs if we keep adding dimensions. On the other hand, if we keep adding dimensions, the amount of training data needs to grow exponentially fast to maintain the same coverage and to avoid overfitting.,In the above example, we showed that the curse of dimensionality introduces sparseness of the training data. The more features we use, the more sparse the data becomes such that accurate estimation of the classifier's parameters (i.e. its decision boundaries) becomes more difficult. Another effect of the curse of dimensionality, is that this sparseness is not uniformly distributed over the search space. In fact, data around the origin (at the center of the hypercube) is much more sparse than data in the corners of the search space. This can be understood as follows:,Imagine a unit square that represents the 2D feature space. The average of the feature space is the center of this unit square, and all points within unit distance from this center, are inside a unit circle that inscribes the unit square. The training samples that do not fall within this unit circle are closer to the corners of the search space than to its center. These samples are difficult to classify because their feature values greatly differs (e.g. samples in opposite corners of the unit square). Therefore, classification is easier if most samples fall inside the inscribed unit circle, illustrated next:,An interesting question is now how the volume of the circle (hypersphere) changes relative to the volume of the square (hypercube) when we increase the dimensionality of the feature space. The volume of a unit hypercube of dimension d is always 1^d = 1.,
,The following plot shows how the , of dimension d and with radius 0.5 changes when the dimensionality increases:,This shows that the volume of the hypersphere tends to zero as the dimensionality tends to infinity, whereas the volume of the surrounding hypercube remains constant. This surprising and rather counter-intuitive observation partially explains the problems associated with the curse of dimensionality in classification: In high dimensional spaces, most of the training data resides in the corners of the hypercube defining the feature space. As mentioned before, instances in the corners of the feature space are much more difficult to classify than instances around the centroid of the hypersphere. This is illustrated by by the following figure, which shows a 2D unit square, a 3D unit cube, and a creative visualization of an 8D hypercube which has 2^8 = 256 corners:,For an 8-dimensional hypercube, about 98% of the data is concentrated in its 256 corners. As a result, when the dimensionality of the feature space goes to infinity, the ratio of the difference in minimum and maximum Euclidean distance from sample point to the centroid, and the minimum distance itself, tends to zero.,Therefore, distance measures start losing their effectiveness to measure dissimilarity in highly dimensional spaces. Since classifiers depend on these distance measures (e.g. Euclidean distance, Mahalanobis distance, Manhattan distance), classification is often easier in lower-dimensional spaces where less features are used to describe the object of interest. Similarly, Gaussian likelihoods become flat and heavy tailed distributions in high dimensional spaces, such that the ratio of the difference between the minimum and maximum likelihood and the minimum likelihood itself tends to zero., In the introduction we showed that the performance of a classifier decreases when the dimensionality of the problem becomes too large. The question then is what 'too large' means, and how overfitting can be avoided. Regrettably there is no fixed rule that defines how many feature should be used in a classification problem. In fact, this depends on the amount of training data available, the complexity of the decision boundaries, and the type of classifier used.,If the theoretical infinite number of training samples would be available, the curse of dimensionality does not apply and we could simply use an infinite number of features to obtain perfect classification. The smaller the size of the training data, the less features should be used. If N training samples suffice to cover a 1D feature space of unit interval size, then N^2 samples are needed to cover a 2D feature space with the same density, and N^3 samples are needed in a 3D feature space. In other words, the number of training instances needed grows exponentially with the number of dimensions used.,Furthermore, classifiers that tend to model non-linear decision boundaries very accurately (e.g. neural networks, KNN classifiers, decision trees) do not generalize well and are prone to overfitting. Therefore, the dimensionality should be kept relatively low when these classifiers are used. If a classifier is used that generalizes easily (e.g. naive Bayesian, linear classifier), then the number of used features can be higher since the classifier itself is less expressive. Earlier, we showed that using a simple classifier model in a high dimensional space corresponds to using a complex classifier model in a lower dimensional space.,Therefore, overfitting occurs both when estimating relatively few parameters in a highly dimensional space, and when estimating a lot of parameters in a lower dimensional space. As an example, consider a ,, parameterized by its mean and covariance matrix. Let's say we operate in a 3D space, such that the covariance matrix is a 3x3 symmetric matrix consisting of 6 unique elements (3 variances on the diagonal and 3 covariances off-diagonal). Together with the 3D mean of the distribution this means that we need to estimate 9 parameters based on our training data, to obtain the Gaussian density that represent the likelihood of our data. In the 1D case, only 2 parameters need to be estimated (mean and variance), whereas in the 2D case 5 parameters are needed (2D mean, two variances and a covariance). Again we can see that the number of parameters to be estimated grows quadratic with the number of dimensions.,It is well known that the , increases if the number of parameters to be estimated increases (and if the bias of the estimate and the amount of training data are kept constant). This means that the quality of our parameter estimates decreases if the dimensionality goes up, due to the increase of variance. An increase of classifier variance corresponds to overfitting.,Another interesting question is which features should be used. Given a set of N features; how do we select an optimal subset of M features such that M<N? One approach would be to search for the optimum in the curve shown by the graph in the introduction. Since it is often intractable to train and test classifiers for all possible combinations of all features, several methods exist that try to find this optimum in different manners. These methods are called , and often employ heuristics (greedy methods, best-first methods, etc.) to locate the optimal number and combination of features.,Another approach would be to replace the set of N features by a set of M features, each of which is a combination of the original feature values. Algorithms that try to find the optimal linear or non-linear combination of original features to reduce the dimensionality of the final problem are called ,. A well known dimensionality reduction technique that yields uncorrelated, linear combinations of the original N features is ,. PCA tries to find a linear subspace of lower dimensionality, such that the largest variance of the original data is kept. However, note that the largest variance of the data not necessarily represents the most discriminative information.,Finally, an invaluable technique used to detect and avoid overfitting during classifier training is ,. Cross validation approaches split the original training data into one or more training subsets. During classifier training, one subset is used for parameter estimation, i.e. estimating the decision boundaries of the classifier, while the other subsets are used to test the accuracy and precision of the resulting classifier. If the classification results on the subset used for training greatly differ from the results on the subset used for testing, overfitting is in play. Several types of cross-validation such as k-fold cross-validation and leave-one-out cross-validation can be used if only a limited amount of training data is available., In this article we discussed the importance of feature selection, feature extraction, and cross-validation, in order to avoid overfitting due to the curse of dimensionality. Using a simple example, we reviewed an important effect of the curse of dimensionality in classifier training, namely overfitting."
There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data. It is popular in machine learning and artificial intelligence text books to first consider the learning styles that an algorithm can adopt.,There are only a few main learning styles or learning models that an algorithm can have and we?€?ll go through them here with a few examples of algorithms and problem types that they suit. This taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.,When crunching data to model business decisions, you are most typically using supervised and unsupervised learning methods. A hot topic at the moment is semi-supervised learning methods in areas such as image classification where there are large datasets with very few labelled examples. Reinforcement learning is more likely to turn up in robotic control and other control systems development.,Algorithms are universally presented in groups by similarity in terms of function or form. For example, tree based methods, and neural network inspired methods. This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describes the problem and the class of algorithm such as Regression and Clustering. As such, you will see variations on the way algorithms are grouped depending on the source you check. Like machine learning algorithms themselves, there is no perfect model, just a good enough model.,In this section I list many of the popular machine leaning algorithms grouped the way I think is the most intuitive. It is not exhaustive in either the groups or the algorithms, but I think it is representative and will be useful to you to get an idea of the lay of the land. If you know of an algorithm or a group of algorithms not listed, put it in the comments and share it with us. Let?€?s dive in.,Regression is concerned with modelling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model. Regression methods are a work horse of statistics and have been cooped into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process. Some example algorithms are:,Instance based learning model a decision problem with instances or examples of training data that are deemed important or required to the model. Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take all methods and memory-based learning. Focus is put on representation of the stored instances and similarity measures used between instances.,An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing. I have listed Regularization methods here because they are popular, powerful and generally simple modifications made to other methods.,Decision tree methods construct a model of decisions made based on actual values of attributes in the data. Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems.,Bayesian methods are those that are explicitly apply Bayes?€? Theorem for problems such as classification and regression.,Kernel Methods are best known for the popular method Support Vector Machines which is really a constellation of methods in and of itself. Kernel Methods are concerned with mapping input data into a higher dimensional vector space where some classification or regression problems are easier to model.,Clustering, like regression describes the class of problem and the class of methods. Clustering methods are typically organized by the modelling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.,Association rule learning are methods that extract rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organisation.,Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks. They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types. Some of the classically popular methods include (I have separated Deep Learning from this category):,Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation. The are concerned with building much larger and more complex neural networks, and as commented above, many methods are concerned with semi-supervised learning problems where large datasets contain very little labelled data.,Like clustering methods,??Dimensionality Reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarise or describe data using less information. This can be useful to visualize dimensional data or to simplify data which can then be used in a supervized learning method.,Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction. Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.
My favourite explanation of the "butterfly effect" so far is as follows: Under particular conditions, even the tiniest movements of a butterfly can trigger storms and hurricanes. This principle is not limited to butterflies, of course. I think that many of us face pivotal moments in life that leave lasting effects. Perhaps no different than other students, I remember running out of cash during my undergraduate years. I consider this my personal butterfly moment. I had no money for food. I couldn't cover the next payment for rent. My old Skylark was running on vapours. I lived away from my parents who were also out of the country at the time. I really thought that was going to be "the end." However, there sitting on my bed when I got to my room was a letter from the federal government containing a cheque for about $980. This educational disbursement, my final grant from the government before completing my studies, was a critical turning point. A wisp of wind could have sent me in a different direction that evening. In this blog, I will be exploring the butterfly effect although probably not in the manner many would expect. I will be focused less on the butterfly and more on the setting that enables the effect. I call this setting the "event threshold" or edge: it is forms part of a broader discussion on the potential power contained in massive amounts of data. However, before proceeding to discuss edges, I should probably revisit what I mean by events. I use this term in my blogs generally without much preamble. From my explanation of events, I will progress towards a technique that I use to map out events called "tracing." Then hopefully it will be blue sky for butterflies.,A sentiment that I find frequently expressed in regards to big data is as follows: it is difficult to pin down the exact meaning. I completely agree. However, rather than mull over the exact meaning, I find it important to emphasize its absence (i.e. of meaning) not just in terms of the data system but the data itself. In the science from past centuries, the meaning of any data collected fell within the experimental boundaries: the data either supported or refuted hypotheses. We controlled the outcomes of experiments by imposing on the data. In big data, we face a situation where the data might lack clarity. The data could be dirty: it hasn't been collected experimentally but rather experientially as part of day-to-day routines. The data might contain all sorts questionable details - causing its exact meaning to be evasive. Any attempt to externally define the data and impose simplicity over it diminishes its usefulness; we lose sight of the underlying phenomena giving rise to the data. To skim through something complex with simple metrics is to violate its essence, purge its meaning, and subjugate our understanding of reality. People exist in the data system as data. So to herd the data, butcher it, and systematically dismember bits - to disembody it - is to assault a relationship of trust. People participate in decision-making through their data. "Institutional perspectives" will fail us in relation to big data: these are perceptions that assume authoritative understanding. Institutions for me include the science of past centuries and its methodologies: the focus has been on rendering a response or verdict rather than embracing the meaning of the underlying phenomena. I suggest that we need different ways to detect, rationalize, and describe phenomena through big data.,When most people take measurements, they have a standard of measurement against which to evaluate the things being measured. For instance, a yard stick or a meter stick measures distance. Expressed a bit differently, a person uses such a device to obtain measurements of distance. Distance isn't necessarily part of the object; but rather distance is what a person gains through the act of measuring. I hope that isn't too confusing. Try measuring poverty using yard stick. Phenomena don't necessarily have distance. Through the use of a measuring stick, it is possible to acquire readings of distance and nothing but distance. A basis of measurement can represent an external imposition over the intrinsic reality of an object. A pervasive metric that exists in western society is "cost." It is possible to measure a great many things on the basis of cost. A married couple can evaluate their children by acquiring information about costs associated with raising offspring: e.g. food, clothing, medical care, supervision, and education. A measurement is normally performed when the person doing the measuring has an expectation or preconception in relation to the thing being measured; further, the measurement normally serves to satisfy specific instrumental needs. Arguably, the act of taking measurements is itself rather instrumental in nature. On a spreadsheet containing sales figures, one does not enter tidal temperatures. We are more interested in extracting what we want and expect to gain from the data than allowing the phenomena to freely offer what it has to give or say. As such, our understanding is limited by the social construction of our desires, expectations, and plans for the data.,An "event" is conceptually different from a measurement. When I start throwing events, it is because I don't already know what to expect from the phenomena. If I don't know what poverty is about or why it occurs, it can be detrimental to knowledge for me to make use of measurements that limit the expression of the phenomena. If I don't know why people are sick so often in an office, then applying a cost or benefit analysis limits or steers my understanding of the situation to these metrics. The "metrics of criteria" can obscure the nature of phenomena since these metrics were never intended to explore the nature of anything. They are only meant to adhere to our prescriptive needs: to affirm or dispute what is being sought but not to undermine the broader assertions. (I have some examples of this perhaps to share in other blogs.) To understand the "nature" of things, it is necessary to use the "metrics of phenomena." When animals are caged in a zoo, they do not necessarily behave naturally. When information is forced into prescriptive boundaries, it likewise might not behave as it would under normal circumstances. "Tracing" involves distributing events that create delineations within data. I will be using some personal examples in just a moment. However, at this time I just want to emphasize, delineation is not about imposing what I think I know; but rather it helps me understand what I don't know or what might ultimately be beyond my full grasp. In the Java programming language, events are "thrown" often for diagnostic purposes to help the user come to grips with something that doesn't quite fit or make sense. My use of events is more elaborate. I throw events continuously to determine their conceptual placement in relation to dynamic feedback (as in systems theory).,Around this time last year, I started experimenting with an alpha (i.e. a development prototype intended mostly for research) that contains two relatively unique features: 1) it makes use of data that generally lacks any prescribed format; and 2) it is driven by events rather than measurements. The program holds data in a manner that is fundamentally different from a spreadsheet; in the case of the latter, the format is normally preset, and the main task of the user is to fill fields with the required data. A spreadsheet is well suited to fulfill prescriptive needs and criteria. However, I hoped to use my alpha on many different types of data where I might not know the structure in advance. For instance, in the real-life examples to follow shortly, I use the program to organize personal information. On any particular day, I don't necessarily know everything that I am going to do or everyplace I am going to go. So it seems structurally unsound to use a spreadsheet containing fixed rows and columns to hold rigidly defined data: this almost implies that I know what to expect, and I would only have to count each occurrence. Instead, the prototype retains those events that are relevant. I keep a record of these events not because I already know the outcome. I know far less than the outcome. , I would like to determine how the events relate to me. In contrast, for example in the financial industry, a company generally knows how its data relates to clients. If I have an account with a company, the transactions are the financial events that are important to me in terms of my relationship. This does not mean that every event associated with a person has relevance that can be clearly defined. It represents something of a departure to be unaware of the relevance of data and yet collect it anyways; this is particularly so given our lack of methodologies to extract meaning.,I normally associate events with different contexts in a structural sense. A context might be a scale indicating the quality of a product; perhaps a series of steps towards completion; maybe a theme or fractal. One context that I am sure everybody knows in relation to personal data is weight. (In practice, I associate personal data with about forty contextual interests; but of course I check my weight periodically.) This measurement is among the easiest to objectively confirm. I know which contexts are important to me, but I don't know how the events relate to the contexts. It is possible to examine the apparent contribution of each event towards particular contexts. However, I find this approach awkward given that there can be a large number of throwable events. A person can instead handle events as an aggregate - that is to say, as a class of events. In order to group events, I make use of keywords or "descriptors" for sorting purposes. Below is an image for the descriptor "exercise" in relation to a context on the alpha called "xbreathe." I call the visualization a sliding balance or usually just a slider. The xbreathe context is associated with events that lag by a day: this means if I do push-ups or sit-ups on one day, the association would occur on xbreathe the following-day. Handling events as an aggregate, it would appear that for me, exercise is associated with improved breathing perceptions for 52.77 percent of the events; decline in perceptions for 8.33 percent of events; and no clear improvement or decline for 38.88 percent of events. However, from the 38.88 percent, 30.55 percent tilts towards decline and 8.33 percent towards improvement. I offer these numbers merely as orientation to help readers understand the nature of the slider. This blog is certainly not about my exercise routine or xbreathe.,I can also offer numbers and some general ideas about weight, which as I mentioned is another context that I maintain. I want to remind readers that I am not a medical professional. The sliders are not meant to guide the general public. Indeed, the data is entirely about me in a literal sense. It contains no information about other people. The next three illustrations seem to point to the following generalities about how events relate to my weight (I'm sure of great interest to many people): 1) taken as a whole, weight is inversely related to events of food consumption; 2) physical activities contribute to weight loss; and 3) taking ordinary food supplements (i.e. excluding weight-loss formulations) hardly affects weight but if anything adds to it. I associated the context xweight with the descriptors "food," "exercise," and "vitamin." (Events can be sorted in different ways: e.g. fruit, protein, fast-food, natural, and so forth. With proper design, events and descriptors can be used to record specific retailers, brands, lots, and product variations.) For me, it seems that exercise hasn't been particularly relevant in weight reduction; but, it is still more relevant than food and vitamins. Any exercise I do, I admit that I tend do little. Nonetheless, as a class of events, exercise seems to be offer a promising lead towards activities that might become more relevant. I present these sliders not so much to inform readers of my weight circumstances bur rather demonstrate how sliders might be used to examine different aspects of phenomena.,The prototype can generate lists showing the events for each area on the slider. For the events associated with blood pressure, I found that most fast-foods and most salty foods are associated with higher blood pressure: 16 of the items listed on the high side taste rather salty compared to only 5 items on the low side. Interestingly, the event signifying 2 sticks of cheddar cheese (20 percent of the recommended daily allowance of sodium) appears on the high side; but 1 stick of the same cheese appears on the low side. I was surprised by how these events were distributed despite the 1-day lag; this means that elevated pressures were detectable the following day. On the lower blood pressure side, I found many plain (non-processed) foods and beverages along with a suspiciously large number of desserts. I'm uncertain if the foods and beverages themselves contribute to lower pressures or their relationship to broader eating habits. Consider a slider showing certain exercises associated with elevated blood pressures; equally intriguing is the possibility of reducing blood pressure by doing specific exercises. I have no background in kinesiology. However, if I had to guess on a common theme, I would say that for me, exercises associated with elevated blood pressure seem rather torso oriented. Once more for me and not necessarily anybody else, exercises away from the torso seem associated with reduced blood pressure. I will have to gather more data although I can't say that blood pressure has ever been much an issue in my life. Again, this exercise is meant to show how tracing can be used to map out the relevance of events given different contexts. I hope others find the sliding mechanism - or at least the idea of using one - worthwhile.,This is probably a good time for me to add a side note. The process of event distribution giving rise to sliders is highly systematic in nature in a way that to me seems relatively unique: it associates events with different contexts in a "big way." It is not ambivalent but obsessive. It is taxing on resources. If there are 10,000 different types of events, these are associated with the contexts all at once. (Of course, this is a conceptual explanation. Sadly in a literal sense, operations are carried out line-by-line exhaustively for all the data.) No attempt is made to pre-screen the applicability of events. As such, the alpha might find that "missing batteries" relates to "inventory shrinkage" despite the absence of clean or clear experimental evidence or rational explanation. Unless a person already knows the relationship to inventory, missing batteries might merely point to lack of portable lighting during rare power outages. From the standpoint of problem-solving through the use of massive amounts of data, confronting different concerns from a position of ignorance wouldn't work if our solutions require understanding ,. We would simply be promulgating vacuous preconceptions throughout the data system at an immense level. Through event distribution, our objective isn't really to determine whether or not batteries are missing but rather what it might mean to have missing batteries. I suspect that security personnel would encounter difficulty carrying out routine inspections given missing battery packs for their flashlights and communications equipment. This is not to say that a person necessarily has to know the meaning of missing batteries in a rational sense: it is unnecessary to know the "meaning" of anything in advance (a product of reasoning) to determine its meaning in terms of contextual relevance (a product of math and algorithms).,I refer to the vertical line that partitions the area in ambient green on a slider as the "edge," which represents a conceptual threshold. I find it difficult to dismiss the separation that occurs at the edge as incidental. In particular, if there is a change to the underlying circumstances of the data, the class leaders would likely emerge first along the edge. When seeking out intervention opportunities, I routinely forage for interesting events along the edge. To me, the edge contains aspects of the underlying phenomena that can sometimes be characterized as pivotal. For example, I consider consumers to be transient edge participants. They are not necessarily comfortable occupying extremes; therefore they might be found more frequently perusing the boundaries. I wonder if edge studies can be applied to the organization of isles at a retailer. Although I routinely throw a great many events, this is not to say that every possible event has been thrown. The reality as it is presented to me likely represents only a partial depiction that poorly reflects the whole. The balance remains accessible only through events that I have yet to throw. So it is great fun venturing along the edge and setting up events almost like antennas to detect things that pass nearby. In ecology, an edge delineates where two different habitats meet each other. For instance, there might be grassland followed by a transition to a forest. Or there could be an aquatic environment near some wetlands. The edge is where different species have the opportunity to interact. In the human ecology, we have edges for the conveyance of ideas, goods and services, and even risk-reward trade-offs. The edge is where people place their bets.,In an organization, there are also edges. Many people would probably separate management, administration, and production workers. Within these delineations there are many intersections and overlaps. Those familiar with my explanation of different informational sources in an organization would find that an edge that I call "direction" separates the organizational constructs "projection" and "articulation." (This is too in-depth to expand upon here.) In Kurt Lewin's Force Field Analysis, there is something of an edge. He said that for change to take place in an organization, unfreezing has to occur. To me, he was discussing equilibrium dynamics in a system containing many edges: interests collide, overlap, and mesh along these edges. Lewin seemed to invoke edge dynamics in relation to change and intervention. My conceptualization is more mathematical and algorithmic. While I do not suggest that an edge represents an event horizon - i.e. a critical turning point spawning different outcomes - I believe that event horizons are delineated by edges. Understanding a horizon ultimately requires knowledge of these intricate partitions in the data. A wayward butterfly can land at a sensitive delineation and trigger a shift. It might be possible to someday conduct warfare across the event horizon through silent campaigns against entire populations. Some nations might struggle while others prosper without the use of soldiers or guns. Maybe hackers and supercomputers come closest to interfering with the natural distribution of events. So perhaps I'm a little bit like Herodotus preparing to witness the formation of epic battles to come. My tools are just a bit different. Like him, I'm perfectly fine taking omens and signs into account since I need not know the meaning of things to understand the relevance. I chase ghosts and butterflies not so much because they threaten civilization; but rather I think that data-dependency makes us vulnerable to intervention by the least detectable and most intangible agents.,Perhaps the biggest obstacle in my use of the term "event horizon" is public preconception. In the movies that I have seen - being quite a fan of science fiction as many may have noticed in my use of specialized terms - an event horizon is where different time-lines are spawned usually by those with time-travel capabilities. Sadly, this is not an ability that I possess: Although I have pondered on reverse temporal intervention, I couldn't think of a way to detect success even if one could influence a time-line. Without detection, intervention would likely lead to chaos and mayhem whether over the time-line or anywhere else. For me, the idea of an event horizon simply extends from a discussion of probabilities. However, a person need not know probabilities in order to intervene. When children learn how to catch a ball, they usually don't have formal knowledge of physics or probabilities; they instead learn how to place themselves in the flow of sensory and motor data to alter the course of events. I call this principle of influence through involvement in the flow of events "kinetic intervention." I consider this concept important when the underlying phenomena exhibit great complexity. One cannot hope to fully internalize or grasp something massive and complex; doing so can lead to the most trivial and yet destructive outcomes. Nonetheless, perhaps as a general rule, every situation that a person faces was at some point too complex for him or her to understand entirely. While frequent exposure and learning can mitigate some consequences, this does not alter the inherent need to intervene or be involved in the "flow." The question is whether or not intervention can be executed deliberately and thoughtfully. A child catching a ball is unlikely to lead to anything resembling a catastrophic butterfly effect. But I have concluded, we are altering the settings that humans occupy such that more of our activities can cause thresholds to tilt or slide undesirably.,Although the prospects might seem intriguing for kinetic interveners, there are complications for any aspiring KI master. For instance, data viewed in relation to events lacks absolutes. A person is unlikely to definitively know the beginning or ending of any underlying phenomena or its placement in relation to other phenomena. This is because there might be no beginning, end, or placement in human scale: these are metrics of criteria meant to characterize phenomena in terms convenient to our needs. Imagine an evil KI master bent on destroying a particular government maybe in exchange for cash. Actually, just to be safe, consider a good KI master that hopes to do away with a terrorist organization. He or she would have to know the relevance of this organization or institution to other people. The building where the organization is situated is simply a piece of capital. The migration of influence over important segments of society and further towards the vernacular is not easy to pin down. So it is unclear what events to throw and what contexts might be important. I drew a diagram to show the complexity of phenomena-realm determination. (The little stick people are standing atop the base-structure for data-embodiment.),I don't know whether to describe that symbol on the right as infinity or a butterfly. I was trying to show that symbolic aggregation can occur indefinitely and even fold into other parts of an event horizon; but perhaps this is precisely what can make a butterfly so lethal. Another important constraint that I feel prevents humans from exploiting the horizon is the issue of detection as I previously mentioned. We can understand much more than what we can detect. Moreover, we might incorrectly detect things. Or we might misrepresent what we detect. For instance, my prototype can give me a list of events that seem associated with improved perceptions of mental concentration. But a "perception" of concentration is a fairly precarious thing. I also admit that attempting to make use of lists of events that seem to make me "feel smarter or dumber" is a radically fringe concept. (He writes as he pops another smart pill, imbibed by the unbridled power of vitamins.) I describe the process of gradually mapping the relevance of events to different contexts as "tracing." Through tracing, phenomena can tell me what metrics seem to apply or are relevant to it; these are the metric of phenomena. So in this blog, I have described a process where the phenomena itself gives give rise to its own metrics; this is a pretty complicated concept, really. But consider the benefits of interacting with something massive and organized, using it as capital rather than dissolving its value through instrumental preconceptions and behaviours.
Read , if you find these examples interesting.,:
Having looked at the fundamentals in the first blog, the natural next step is to understand the various types of strategies to "attack" the data and make it reveal useful information. However, there is one step we must take just before that: Understand the "enemy" i.e. the problem at hand and the data available.,The tree below is an attempt at categorizing the most commonly occurring problems and list commonly used techniques under each. While not all problems would fall under each of these branches, this is intended to be a good starting point for understanding the nature of the "enemy" and then developing a suitable strategy.??,TS - Time Series,AI/ANN - Artificial Intelligence/ Artificial Neural Networks,The 'Nature of Data' might also sometimes be a factor with choosing the technique to attach and is therefore added with a dotted line in the tree.,There are two important points related to the tree diagram above:,1. Since the area covered by the tree above is vast, some of the techniques common to multiple branches might be listed only under one. However, the purpose of the tree is to guide at a high-level on the techniques rather than classifying all the techniques with great accuracy.,2. A lot of new techniques might not yet be added to the tree and I plan to upgrade it as we go along and shoots emerge for new leaves or even branches to be added.
When designing a model for a data warehouse we should follow standard pattern, such as gathering requirements, building credentials and collecting a considerable quantity of information about the data or metadata. This helps to figure out the formation and scope of the data warehouse. This model of data warehouse is known as conceptual model. General elements for the model are fact and dimension tables. These tables will be related to each other which will help to identity relationships between them. This design is called a schema and is of two types: star schema and snowflake schema. The designing of these schema falls under physical model design (Jones and Johnson, 2010).,Before designing the physical model, logical model should be designed this is based on the conceptual model. Logical model mainly focuses on granularities arrangement, data refinement and the definition of logical relation pattern (Fu-shan, 2009).,Granularity refers to ?€?the level of detail or summarisation of the units of data in the data warehouse?€?. The low level of granularity contains high level of detail and the high level of granularity contains low level of detail. This is one the major issue of data warehouse design as it affects greatly to the data and its query (Inmon, 2005). A diverse category of analytical processing uses various levels of granularity. The level of granularity affects database performances. Data warehouse consists of several combinations and details of data commonly referred as granularity. If the DW has many levels of explorable data layers, it is supposed to be more granular. Generally conventional?? database operations are categorized as low granular, Whereas modern data warehousing operations are required to be more granular because of the needs of exploring data in several intensity, Thus in a DW environment, granularity?? directly represents the richness of data quality and consequently establish the intensity of database queries (Fu-shan, 2009).,It consists of single fact table at the centre linked with a number of dimension tables. OLAP focuses in the fact table and data related to facts are stored in dimension table. The dimension tables will not be in normalised form (Wang ,., 2005). The fact table contains the primary key of all dimension tables. The advantage of implementing star schema is that to get information we need simple join queries. But the disadvantage is that for complex systems, it becomes somewhat complicated to read and query a massive amount of data (Jones and Johnson, 2010).,It?€?s a modification of star schema. The dimensional hierarchy is presented clearly by normalisation of the dimension tables which will be used for drill-down and roll-up operations. Its advantage is that maintaining the dimension tables will be easy (Wang ,., 2005).,The implementation of these schemas depends on specific system requirement. Difference is in performance and usability.,References:,1)??, , ,3)??, , 
One of the marvels that the age of data and technology presents is the ability to analyze and determine the minutest of details in the world today. Several of these innovative breakthroughs pass unnoticed under the gaze of daily life. Yet it is this dissemination of data and integration of innovation that is intrinsic the modern world. One field which has risen from the fore of the data deluge is ?€?,?€?. Through innovation it combines analytics, with an inter-disciplinary approach to extract the best insights from data, and subsequently facilitate data-driven decision making. Decision Sciences, which holds the key to survival for the dynamic modern day business landscape, employs an interdisciplinary approach of business, applied math, technology, design thinking and behavioral sciences, to solve constantly shifting and ill-defined issues.?? However, it is not complete without its roots in cross pollination and constant innovation; drawing insights and learnings from several verticals and industries, businesses realize the tremendous potential of problem solving armed with cross industry knowledge.,Cross- Pollination of learnings has been flagged as a pillar toward category defining insights, in the fact that it draws knowledge from beyond the designated field of view. In 2009 for instance the outbreak of the unknown H1N1 was contained through the use of predictive and location analytics. It was done by using a clustering algorithm that monitored statistically unusual burst of incidences of flu in ER data, or over-the-counter sales in a geographically bounded region, to form geographic clusters. Subsequent localized factors were determined that were based on scanning countries, so as to conclude whether a particular disease occurrence was unusual in a population of that size, taking into account the minimum number of false alarms.?? Experts known as epidemiologists were effectively able to extract the movement and impact of the virus by employing predictive analytics.,This process of geographical clustering was employed in the energy sector, in order to designate and target geographic areas of customer attrition.?? Traditional analytics failed to determine which locations suffered the highest attrition, and the problem was solved through finding clusters of zip codes that could be tested for localized factors, as in the case of the H1N1 outbreak, that affected customer attrition. Finding clusters of localities at a greater risk of attrition is quite similar to finding clusters of disease incidences.,Epidemiology which is the science of determining trends in diseases is a field which has much to contribute to the world of business. It is indeed the need of the modern business landscape to draw from insights in other fields and industries, and subsequently translating this cross pollination into decision making insights. The use of predictive and location analytics stirred by epidemiology led to the discerning of clusters of zip codes.,The results of innovative learnings and cross-pollination answer the frequent Marketing Analytics question asked by marketers: ?€?How do I determine which customers are at risk of attrition and how are they effectively addressed??€? By drawing out similar problem solutions from parallel industries, CMOs are increasingly finding game changing solutions to problems that plague their businesses.,It is interesting to see the relevance of cross-industry learning in the business context. ??Whatever an organization markets, it is highly probable that it is sold to people.?? Therefore employing analytical techniques based on their usage in Epidemiology such as marketing, predictive ,and location analytics are highly important. Marketing analytics adds segmentation of data based on location to the market mix enabling companies to optimize their promotional campaigns and increase their customer base. Predictive analytics employs location based data in sync with algorithms ,such as knowledge discovery algorithms such as WSARE (What?€?s Strange About Recent Events), allowing businesses to detect unusual trends in data. Location analytics allows businesses to identify ZIP codes of a particular customer set (based on their current customer information) and hence plan acquisition targets and localized promotion strategies.,If certain metrics can be looked at on similar grounds, it may just show that like the correlation between customer attrition and percentage of flu diagnosed in a medical center, different industries all have problems which address the same core.?? Building solutions using clear problem definition and in understanding the overlapping principles of cross-industry knowledge, all of which are emphasized in the use of Decision Science, businesses today stand to change the industries in which they function.,So why not let ideas mix and integrate the learning from across the spectrum? The cross pollination of knowledge was never such a valuable asset, as it is today.
Here are several different ways to leverage Data Science Central for your benefit, at no cost.,All posts are subject to approval. Not all posts are featured. Featured content also appears in ,, ,, and on our Facebook, Google+, Twitter (,, ,) and LinkedIn accounts, and is shared by numerous readers. Check out posts by other members before submitting an article, news, question, contribution or discussion, to have an idea about the format. Commercial posts are not accepted (, for advertisements).
Here we blended together the , resources posted recently on DSC. It would be great to organize them by category, but for now they are organized by date. This is very useful too, since you are likely to have seen old entries already, and can focus on more recent stuff. We plan to update this reference of references on a regular basis.

During the past few years, I have been teaching many courses and doing a lot of consulting around the globe on the topic of Big Data & Analytics.?? Having talked to many business professionals and done lots of projects, I wanted to write a book which is relevant to decisions that all businesses will need to make in the coming years. As the number of practical applications for data skyrockets, learning how to extract business value from big data becomes a competitive requirement.?? Big data sets are assets that can be leveraged quickly and inexpensively, if tackled wisely! My book Analytics in a Big Data World addresses this seemingly Herculean task of coming to grips with multiple channels of data and sculpting them into quantifiable value. This book is for business professionals who want a focused, practical approach to big and data analytics. I hereby focus on case studies, real-world application, and steps for implementation, using theory and mathematical formulas only when strictly necessary!,??,In fact, I would recommend a couple of things. The first one is to set up a multidisciplinary analytical team.?? Analytics touches upon every aspect of a business setting and it is of key importance that these are all appropriately represented in the team.?? In other words, the team should be made up of database administrators, business experts, legal experts, data scientists and tool vendors.?? Next, involvement of senior management is important.?? The strategic impact of analytics is now bigger than ever before and it is crucial that senior management is aware of all analytical efforts throughout the enterprise.?? This will allow them to install the right logistics and procedures to fully leverage the power of analytics.?? It will also facilitate the coordination of all analytical efforts enterprise-wide instead of working with isolated islands of analytical expertise.?? The creation of a company specific center of analytical excellence, potentially headed by a CAO (Chief Analytics Officer), could be an interesting competitive asset.?? Finally, continuous education is highly recommended.?? The world is changing at a rapid pace and this also applies to analytics.???? New techniques are being developed on an on-going basis and it is important to keep up with these evolutions in order to see how they can be used to can create competitive advantage.,??,A first important bottleneck relates to the key ingredient of any analytical model: data!?? In order to have successful analytical models, data should be of good quality.?? This is commonly referred to as the GIGO principle: Garbage In, Garbage out, or bad data yields bad models.?? Hence, every company should continuously invest in diagnosing data quality issues and come up with ways to improve it using e.g. master data management programs.?? Another bottleneck is the focus on the business.?? Far too often, analytical models are being developed which look nice at first sight but actually do not solve the business problem.?? E.g., a fraud detection model can be highly performing in statistical sense, but should also allow to detect fraud as quickly as possible.?? The latter is often referred to as operational efficiency.?? Hence, besides statistical performance, also operational efficiency, economical cost and interpretability should be taken into account when gauging the performance of an analytical model.??,??,Well, let me discuss some trends which I consider important based upon both my industry and research experience.?? First of all, I believe analytics is about being actionable and simple.?? It?€?s not about complex numbers, black box models or statistics.?? In our analytics projects, we have found that simple analytical models (e.g. regression models, decision trees) typically perform well in many settings such as credit scoring, response and retention modeling, customer lifetime value modeling and segmentation. ??Hence, the best investment firms can make to boost the performance of their analytical models is not by buying expensive software and trying out complex techniques, but rather by investing in data and improving data quality! That?€?s why in my book I also devoted a whole section to this topic.?? From a technical perspective, next to the analytical models themselves, firms should also thoroughly consider how to appropriately monitor, backtest and integrate these models with their other applications such as advertisement, new product development, next best offer campaigns, ?€?.?? Closing this loop poses quite a bit of challenges which are also addressed in the book!?? Finally, data and analytics is everywhere and all around.?? It speaks for itself that this creates huge challenges from a privacy perspective.?? Firstly, data about individuals can be collected without these individuals being aware about it.?? Secondly, people may be aware that data is collected about them, but have no say in how the data is being analyzed and used.?? Hence, regulatory authorities have to think about new regulations, whereas researchers should focus more on the development of privacy friendly analytical techniques.??,??,Let me again answer this question based upon my consulting and research experience.?? A good data scientist should possess various skills.?? A first important one is programming.?? Although many powerful analytical software tools exist, real life data typically requires tailored processing and this can only be done by developing customized software programs.?? Next, a data scientist should have a good quantitative background in statistics, optimization, machine learning, ?€??? This will allow him/her to immediately recognize whether a given data set could be analysed using e.g. predictive, descriptive or even social network analytics.?? Finally, a data scientists should be creative and communicative.?? Data scientists never work individually, but are part of a team.?? Communicating the results of analytical models in a user-friendly and transparent way to the various stakeholders involved is hence also very important.??,First, I would say fraud detection.?? Fraud is an important phenomenon encountered in various settings.?? Popular examples are insurance fraud, credit card fraud, social security fraud, identity theft, ?€? In our research, we have recently developed some new exciting algorithms based upon social networks to detect fraud.?? We have benchmarked our techniques for both social security and credit card fraud detection and found some amazing results.?? Fraud is really a social phenomenon!?? Another important topic we currently work on is survival analysis.?? This is a set of (statistical) techniques aimed at predicting the timing of events.?? In fact, many popular classification problems (e.g. credit scoring, churn detection, response modeling, ?€?) have a time dimension associated with them (e.g. when a customer defaults, churns, responds, ?€?).?? Using survival analysis techniques, it becomes possible to predict when customers default, churn, respond, ?€? which is typically very useful information for e.g. profit scoring or customer lifetime value calculation.?? Finally, we do a lot of research on process analytics.?? Today?€?s organizations use a plethora of information systems to support their business processes.?? Such support systems often record and log an abundance of data, containing a variety of events that can be linked back to the occurrence of a task in an originating business process.?? Process analytics starts from these event logs as the cornerstone of analysis and aims to derive knowledge to model, improve and extend operational processes ?€?as they happen?€? in the organization.?? I believe this will be a very popular application of analytics in the next 5 years!??
Organizations are leveraging the use of data and analytics to gain a competitive advantage over their opposition. Therefore, organizations are quickly becoming more and more data driven. With the advent of Big Data, existing Data Warehousing and Business Intelligence solutions are becoming obsolete, and a requisite for new agile platforms consisting of all the aspects of Big Data has become inevitable. From loading/integrating data to presenting analytical visualizations and reports, the new Big Data platforms like Greenplum do it all. It is now the mindset of the user that requires a tuning to put the solutions to work.,"Getting Started with Greenplum for Big Data Analytics" is a practical, hands-on guide to learning and implementing Big Data Analytics using the Greenplum Integrated Analytics Platform. From processing structured and unstructured data to presenting the results/insights to key business stakeholders, this book explains it all.,"Getting Started with Greenplum for Big Data Analytics" discusses the key characteristics of Big Data and its impact on current Data Warehousing platforms. It will take you through the standard Data Science project lifecycle and will lay down the key requirements for an integrated analytics platform. It then explores the various software and appliance components of Greenplum and discusses the relevance of each component at every level in the Data Science lifecycle.,You will also learn Big Data architectural patterns and recap some key advanced analytics techniques in detail. The book will also take a look at programming with R and integration with Greenplum for implementing analytics. Additionally, you will explore MADlib and advanced SQL techniques in Greenplum for analytics. This book also elaborates on the physical architecture aspects of Greenplum with guidance on handling high-availability, back-up, and recovery.
Everything is connected, through the cloud all machine-generated data are collected and widely shared over the Internet. That?€?s how we imagine IoT ?€? the Internet of Things.,??,Correction: That?€?s how , imagine IoT. What , envision here is not just about the Internet of Things but also the ,. The idea is: When a device is equipped with connectivity and sensors, why not take another bold move to make the device intelligent? With an agile and affordable computing unit, every device has the power to analyze collected data and take fact-backed actions, thus making intelligence ?€?in-place?€? a part of the Internet of Things, anywhere and at anytime. Intelligence, according to Jeff Hawkins*, is defined by predictions.,??,Computers, home appliances, vehicles ?€? even the apparel and kitchenware ?€? can be turned into a thinking unit.?? They can help you act or react to the environment or your neighbours based on your behavioral routines and preferences. Your running shoes could control the friction of their soles according to your weight, the weather, and the kind of trail you choose. Your home theater system fine-tunes sound effects according to the movie genre and what time of day you are watching. There are plenty of exciting applications that come with the advent of intelligent things.,??,The question is, how does it work?,??,The data collected from sensors uploads to the cloud and is stored in (machine) learning systems, while streaming data input triggers an analytic engine to predict the best outcome and to react accordingly. Big data accumulates the background knowledge while small data evokes intelligence in-place.,??,, fully utilizing the unbounded memory space of our existing 64-bit architecture, opens up the window for this sci-fi-like scenario. In-place computing utilizes virtual memory space, and thus avoids hardware lock-in and offers cross-platform computing power. As Qualcomm announced the introduction of 64-bit CPUs for handheld devices, now all mobile devices are entitled to serve complicated computing jobs at your fingertips. In-place Computing, can thus be the catalyst for a new era of ?€?Intelligence of Things.?€?,??,??,*Check out this , where Jeff Hawkins explains how brain science will change computing
Data warehousing project is a complicated task that demands goals and resources from both business and technical departments. It is expensive but normally a basic project. If it is done by non experts and non skilled support, it can be an expensive and may cause project failure. Several business analysts believed most of the data warehousing projects are unsuccessful to meet their proposed objectives (Furlow, 2001).,Hwang and Xu (2007) have chosen eleven success factors:,Clearly defined business needs,Top Management Support,User involvement,High quality data source,Proper development technology,Enough IS staff and consultants,Project management,Practical Implementation Schedule,Proper planning/Scoping of project,Enough financial support,Measurable business benefits,Articlesbase (2010) has shown several factors that affect the implementation process of a business intelligence system. They are:,Business process and requirements.,ROI (Return on Investment).,Project management and resource assurance.,Company executives support,Take time to plan up front,Enough training to staff and change management.,References:,1)??Furlow, G. (2001).The Case for Building a Data Warehouse. Data Warehousing.?? , pp. 31-34.,2)??, , , 

Our , on this venue began outlining the business value for solving ?€?the other churn?€? - employee attrition. We introduced the ?€?quantitative scissors?€? with a simple model of employee costs, benefit, and breakeven points. The goal was to create a robust mental model for the cost of employee attrition.,In this entry, we will extend that model to tease out the factors that underlie attrition cost. With this work we hope to streamline the first step of , By understanding the underlying structure, analysts can systematically attack the problem rather than engage in an open-ended fishing expedition.,The histogram is a useful tool to see how attrition plays out in an organization. It is easy to produce from simple HR records, and the graphic tells a deeper story than simple averages or turnover rates. Most managers are able to understand histograms with some coaching.,Figure 1, shows a basic histogram of tenure. The horizontal ?€?X?€? axis maps out the number of years tenure in a specific role. The vertical ?€?Y?€? axis shows the count of how many employees had that amount of tenure. We can see a stack of early departures in the first 9 months, then another rise later.,Figure 2,Figure 3,But this is deceptive. The top ?€?double hump?€? pattern is actually the sum of two simpler employee clusters. ?€?Good Fit?€? individuals in , left the role by being promoted, or being hired away. ?€?Bad Fit?€? individuals in , left the role because of under-performance, problems with hours, or for disliking the work. It is not difficult to classify termination codes into business-oriented clusters, or to use machine learning for data-driven clusters. Of course the real world has more nuance and ambiguity, but the patterns are there to be found.,Figure 4,The histogram also mirrors how analytics is used to predict tenure - by modeling the probability of an individual to terminate at a certain duration. It is the same kind of ?€?survival?€? problem as customer churn or medical outcome research. The outcome of an analytics model will be a density curve, ,, much like the histogram, showing the probability of termination for ?€?Good Fit?€? (blue) and ?€?Bad Fit?€? (brown) employees, at each point of tenure. This simplified model uses the Weibull distribution, which is popular in this class of survival analytics.,Figure 5,Next we return to the cost/benefit information in ,, which we calculated in the previous blog entry. Different inputs will shift the shape of the cost and benefit curves, but it is inevitable that employees will have , net cost in the beginning, then face a breakeven point, then provide net positive value to the employer. This example is tuned to a short-tenured, fast-training job role, but you can design curves to meet your specific situation.,Figure 6,We sum these costs in ,, to make a cumulative net benefit. The plot shows the net cost or benefit accrued by an employee if they get to a specific tenure. The red region shows the net cost until breakeven, after which more tenure is pure benefit.,Now we have a probability at each tenure point ,, and a cumulative net benefit at each tenure point ,. Borrowing some concepts from finance, we will calculate an Expected Value at each tenure: we simply multiply the probability of reaching that tenure, by the net value of that tenure. Finally, we examine our mix of employee clusters - in this model we posit 60% good-fit employees, 40% bad-fit employees. We multiply each cluster?€?s Expected Value curve by this good-bad ratio, to get ,: an ,.,Figure 7,This is a mouthful, but it is very useful to describe the business costs of attrition. As in ,, the blue curve represents how we expect to derive value from ?€?Good Fit?€? employees, and the brown curve shows how we expect to lose value from ?€?Bad Fit?€? employees. The ?€?Bad Fit?€? people all leave before they break even. Some of the ?€?Good Fit?€? also leave before breakeven. But most stick around well past breakeven. A few ?€?Good Fit?€? folks even make it past the three-year cutoff of these graphs.,The sum of the area under both Expected Cumulative Net Benefit curves give us the overall expectation from hiring in this entire system, from all of the prior assumptions and models. We will call this the ,.,The higher the EVH, the better. Below zero means you are losing money with every hire. With these inputs, our model predicts that a ?€?Good Fit?€? employee will deliver an EVH of 48% of their potential benefit. Our ?€?Bad Fit?€? employees are predicted to deliver ?€?17% of their potential benefit - a loss. At our mix of 60% ?€?Good Fit?€? and 40% ?€?Bad Fit?€?, the ,.,Value is measured as a percentage of an employee?€?s fully-ramped-up productivity. This 100% is ultimately divided into ,: salary, EVH, and loss. In the models above, the employee was paid 50% of their productivity, so that , is up for grabs by the business to maximize. Our ?€?Good Fit?€? employees yield 48% EVH, is very close to the potential of 50%.,In dollar terms, we tend to net , dollars value from an average employee in this role. It is tempting to divide again for , for some kind of efficiency metric, but this is too much abstraction for today.,In the real world, when you reduce salary, you will reduce market demand for the role and increase turnover, while saving money in the short run.,Employment, and business in general, is not a laboratory environment. We don?€?t get do-overs for failed scenarios, and our ability to ?€?try things out?€? is limited. Customer analytics is slightly more amenable to A/B testing, just because the relationship is thinner, and there are often many customers.,With this model, we are able to play try out different approaches, so that predictive analytics can pursue the right target. We can move sliders to examine modeled outcomes, rather than hiring and firing thousand of workers. Of course ?€?all models are wrong,?€? but we have found this one to be useful. The next blog entry will examine the sensitivity of the output (EVH) variable to our 9 input variables, and lay out data science inquiries into several different hiring situations.,Data Science is popularly thought of as an inductive process, and it may seem odd to lay out concepts before collecting data. In practice, the best data science is not an open-ended, free-ranging search for vague patterns. The most powerful data science is directed at a specific business problem, with a clear understanding of the underlying elements of the problem. If we listen, the data will tell us where the our pre-conceptions of those elements are wrong, and we can continue to evolve.,That understanding is our goal, so that we can slash the cost of employee attrition, create a happier workforce, and deliver superior business ROI.,We have made this model available in R on GitHub. You can run it in the free and powerful ,, with interactive sliders to change inputs, recalculating EVH and new graphs on the fly. Console-based R (my workhorse) does not support the needed , library. We are working on a web implementation as well.,In the spirit of collaboration and learning, we have put this code, over 500 lines of R, up on GitHub so that other researchers can download, experiment, and engage. If you don?€?t like our Weibull distributions, you can swap in a Log-Logit or whatever you want. If you want to create a U-shaped cost curve, go ahead. You can share your progress back to us with a ?€?pull request?€?, or ?€?fork?€? your own variant. If you find a bug, create an ?€?issue.?€? Keep us posted. GitHub can be an important resource for collaboration in quantitative research - we encourage practitioners to dig into it.,You can find it instructions and code at , . We will continue to build up this model as an engine for this series. Please engage!

Interesting cartoon, epitomizing innumeracy?? (or simulated innumeracy). Necessary in today academia to survive and get grants.,Source: 
The??,??is always published Monday.??Starred articles are new additions, posted between Thursday and Sunday
From Slate, StatSoft, BBC, Venturebeat and niche publishers:,The following picture is from the first article. Check out our ,??and resources.
Everyone is talking about data and Big data Whether it?€?s big or small, simple or complex, freely accessible or locked up in spreadsheets, everyone is worrying about how to get their hands on it . Every company has one or multiple servers, virtual in the cloud, on premise, or both based on the size of the organization. Those servers run applications, websites and other software, which all generate data. only a small amount of people have access to it.??Now let me try to explain in simple word data which exist everywhere and big data is data which exists everywhere making it difficult to use or store using currently available database management tools. , But all these systems are designed and deployed in relative isolation from one another and operate in a linear or sequential fashion based on the industry and organization. It?€?s difficult to gain access to the data as it?€?s spread across?? entire infrastructure for a single individual. Now the question is How can we provide insights instead of data. Data access is only part of the problem building service measurement?? and analytical decision. The real fun comes later ?€? in understanding it and taking action on it. Data itself is not the end game, but rather the raw material in the whole analytical process. To support a real time intelligent decision making process, If we can use Big Data technology concept for real-time, intelligent decision-making we may have to build some predictive and analytical model that can connect each application area and crunch onlythe?? required data and provide the real time insight about the business, Infratstructure, risk,customer needs,growths, potential markets Gather all your?? data, from all your servers, services and websites in one place and Visualize it. Make the right decisions based on the data visualization. This will certainly reduce operational costs and improve data driven decision making without the need of an expert on all of these disciplines. Using New technologies and approaches for real-time analytics using big data plattform and turning?? data into actionable insight in real time, so businesses can react accordingly!!
 , , , , , , , That?€?s a long story, but let me give you a short answer. During the first 18 years of my career, I worked on various NASA projects as an astrophysicist. I was seeing more and more growth in the volume of data we were taking in, handling, and serving to the NASA scientific user community. About 15 years ago, I had an epiphany, so to speak. A colleague of mine had been working on a large astronomy project at a Department of Energy lab. While there was no national security issue associated with the data, the lab couldn?€?t serve the data publicly from that facility, and so my colleague asked me if we could serve the data through the astrophysics data facility at NASA. We were talking about just two terabytes of data, which isn?€?t much today. I have two terabytes on my laptop. But in 1998, that was quite a lot of data. NASA managers told me that the agency had done about 15,000 space science experiments to that date, and data from , of those experiments combined amounted to less than two terabytes. They couldn?€?t imagine taking in just one experiment?€?s data that would double the resource demands on the data facility. That was very revealing. I realized at that moment that the data volumes and what we could do with them were drastically changing. At the same time, as part of my discussion with my Department of Energy colleague, I heard about the concept of data mining and realized it could be important to the future of my own and others?€? scientific research: doing discovery from NASA data collections, finding the patterns, trends, correlations, surprising things in the data stack. At that point, I launched my career into data science. My later years at NASA were a combination of astrophysics and data science, before I moved to George Mason University in 2003. , , , , , , , One of my goals when I came to the university was to help create an undergraduate degree program in data science. George Mason University has had a Ph.D. graduate program in what we call Computational Science and Informatics since 1991. It?€?s clear that the people who created that program were 20 years ahead of their time. It?€?s been very successful, having now graduated more than 200 Ph.D.s. I worked with my colleagues at George Mason to develop a plan and curriculum for undergraduate data science, a Bachelor?€?s degree program. And in 2007 we accepted our first students, about five years before awareness of big data went mainstream. In March 2012, for example, the White House National Big Data initiative was announced. And it was in October of that year that the , called data science ?€?the sexiest job of the 21st Century.?€? Now many more students have discovered data science. In our Ph.D. program we?€?ve seen a significant shift in the interest of our graduate student applications toward data science. , , , , , , This concept came to me when I was doing that initial data mining research at NASA in 1998 and 1999. I realized much of what we could discover in data was already known to the scientific research team offering it. I called this category, the , ??? we confirm what we already know inside the data. The second category is called the ,. You expect certain behaviors, for example, but no one had found them because the signal was too weak or it had not been explored in enough depth. But the biggest potential for discovery by far are the things that you never expected to find in the data. These are the , I wrote a couple of articles on the unknown ?€?unknowns?€? and put together a website at NASA on scientific data mining resources. That gave me a bit of a reputation for being a data mining guy at NASA, so much so that I got a call from the Executive Office of the President after 9/11 to talk about data mining. I didn?€?t end up briefing the president, but did spend time with his staff. And these three concepts percolated over to the Secretary of Defense where the notion of unknown ?€?unknowns?€? was applied to unforeseen terrorist threats. In science, of course, the big discoveries are those you could never imagine until the data reveal them. , , 
I was reading through my Twitter feed the other day and saw a comment about the R language being too ad hoc for users. ??It got me thinking, "Is that bad? Aren't most languages initially seen as ad hoc?". ??,The beauty of R as a data science tool is its "ad hocedness" in that its use can satisfy multiple interests. ??Initially I can see this as troublesome in that learning the specificity of a tool's use can be daunting. ??But in the long-run I think this benefits a language to be specific to multiple interests and applications. ??I only started using R about a year ago and am still finding different packages that provide a different angle on an analysis I already did. ??I think the ability for the R language to pool resources from all it's users in an iterative way (thereby making it potentially more ad hoc) increases its appeal and application. ??,Perhaps in the future as other technologies emerge sifting through the ad hoc nature of the language may become easier as a beginner. ??For those who are familiar with its use, I think any initial ad hoc disdain is quickly overcome by the breadth of relevant applications to specific data analysis needs. ????
In digital analytics, scoring Internet traffic is used to detect click fraud, and to find types of search keywords that convert well (to a sale). Quite often (for large ad networks) conversion data is poor or challenging: some clicks have a 0.2% conversion rate, some have a 30% - depending on the type of website, price, product, conversion type and other factors (even hour of the day has an impact).,One way to create a generic scoring system, to predict if a click is genuine or not, could rely on IP flags rather than conversion metrics. By IP flag, I mean??, such as Spamhaus, Barracuda or??, user IP and referral (web domain) blacklists, with various reason codes indicating why the IP's in question are blacklisted.,Since these 3rd party blacklists are the result of scoring system used by the vendors in question (Spamhaus, etc.) our generic score would be a score based on 3rd party scores, that is, a meta-score blending multiple scores - even blending scores that predict conversions, if possible.,In practice, I've found that,Has such a strategy been used in other industries - finance or marketing? Of course the challenge is to identify data buckets either with very high or very low concentration of blacklisted IP addresses, using ,,??, and ,. And??, for the scores.
According to Weisensee ,, Data warehouse architecture follows following principles:,ETL process is the foundation of BI. Success and failure of BI projects depends upon ETL process. It plays a vital role to integrate and enhance the worth of data. After the extraction, cleansing and arrangement of data, it will be loaded into data warehouse. In short, ETL is the transferring process of data from data source to the target data warehouse. Design of ETL process will determine its efficiency, flexibility and maintainability.,ETL process consists of five modules as shown in figure above (Jun ,., 2009). This process will occur between:,Data sources and the data warehouse and,Data warehouse and data marts (Weisensee ,.).,In an organisation, there will be huge amount of data sources located in different database management systems (DBMS) which may be from Marketing Campaign System, Sales Tracking System, Customer Support System and other systems (Ramsdale, 2001).,These data sources must be physically and logically designed to identify wide view of organisational data review. Physical design will help to review DBMS tables and data sets that will be used by business processes. The logical design will help to make relationship among different tables of physical design through linkages to maintain information hierarchies for presentation and validation purpose. These processes will help in business intelligence (Weisensee ,.).,A typical architecture of data warehouse was designed by Wang ,, 2005. Figure 6 shows data warehouse architecture with its components. It includes:,Jones and Johnson, 2010 has differentiated data mart and data warehouse. Data marts stores data associated to a subset of an organisation such as a branch or particular product. On the other hand, a data warehouse stores data associated to entire organisation. So, it can be said that data warehouse combines the data from data marts.,There is an analytical processing technology between data warehouse and tools called Online Analytical Processing (OLAP). It is based on multidimensional model of business data in the data warehouse to generate business information. So, OLAP is referred as Analytical Processing or Dimensional Analysis (Kirkgiize ,, 1997). Multidimensional analysis is the manipulation of information through various related dimensions to ease analysis and understanding of the original data (Wang ,., 2005).,Reinschmidt and Francoise, 2000 defines OLAP as?? ?€?a category of software technology that enables analysts, managers and executives to gain insight into data through fast, consistent, interactive access to a wide variety of possible views of information that has been transformed from raw data to reflect the real dimensionality of the enterprise as understood by the user.?€? It is the multi-dimensional analysis of data that supports logical and navigational querying of data. It helps the user to produce organisational information through relative personalised presentation and analysis of historical and proposed data of different models.,Metadata is the information of the data which includes data types, mapping of fields, acceptable values, etc. of the tables. It also sketches the picture of how transformation of data will take place along with how the operation should be managed (Reinschmidt and Francoise, 2000).,A data mart is a subset of data warehouse. It is valuable for particular organisational unit, branch or user. It may contain historical, summarised and probably complete data stored from operational systems (Reinschmidt and Francoise, 2000). ??,These services are used by the business intelligence clients to interact with the available data mart or data warehouse data. It may be in Microsoft Office. (Weisensee ,.).,References:,1)??, ,2)??, , , , , ,7)??, , , , 
Authored by:,Pasha Roberts, Chief Scientist,Greta Roberts, CEO,July 2013,Digging into a cross-industry study of analytics professionals, we identify four distinct patterns of how these workers spend their week: (1) Data Preparation, (2) Manager, (3) Programmer, and (4) Generalist.?? These functional clusters are defined by unique time-usage patterns, and further, exhibit important differences across dimensions of education, demographics, and mindset.?? This brief quantifies these characteristics, and their deep implications for sourcing, hiring, managing, and retaining analytics professionals in each category.,The groundbreaking , by , and the , utilized many measures to understand the characteristics of modern analytics professionals and data scientists,. The study examined 302 active analytics professionals in a diverse sample of companies, industries, sizes and circumstances. For the purpose of this paper we will use the terms data scientist and analytics professional interchangeably.,Our premise? Data scientists have been discussed as if they performed a single role. We suspected this role was wider, containing a broader workflow of tasks. Clarifying the analytics workflow and tasks performed by data scientists could also provide insight into those looking to hire and retain professionals in this important role.,To this end, we asked participants in our study how many hours per week they spent in various analytics-related activities. Every attempt was made to capture and reflect tasks in a modern analytics workflow.,The study also gathered 11 factors pertaining to an individual's ?€?raw talent?€? factors, also known as aptitude or mindset. Aptitude can be distinguished as different from achievement.?? This study will show how aptitude and other factors differ across job types.,In aggregate, the study sample spent , the same percentage of time performing tasks in the following 11 categories:,However, upon deeper analysis it became clear that there were several ?€?types?€? of workweeks at hand in this sample.
The following??publication was presented at the ,??and received the Best Paper Award on 5/18/2014. ??The original IEEE LaTeX formatted PDF publication can also be downloaded from here:??,., , , , , , , , , , , , , , , , ,Cybercriminals have adopted two well-known strategies for??defrauding consumers online: large-scale and targeted attacks. ??Many successful scams are designed for massive scale. Phishing??scams impersonate banks and online service providers??by the thousand, blasting out millions of spam emails to??lure a very small fraction of users to fake websites under??criminal control [8], [20]. Miscreants peddle counterfeit goods??and pharmaceuticals, succeeding despite very low conversion??rates [12]. The criminals profit because they can easily replicate??content across domains, despite efforts to quickly take??down content hosted on compromised websites [20]. Defenders??have responded by using machine learning techniques to??automatically classify malicious websites [23] and to cluster??website copies together [4], [16], [18], [27].,Given the available countermeasures to untargeted largescale??attacks, some cybercriminals have instead focused on??creating individualized attacks suited to their target. Such??attacks are much more difficult to detect using automated??methods, since the criminal typically crafts bespoke communications. ??One key advantage of such methods for criminals is??that they are much harder to detect until after the attack has??already succeeded.,Yet these two approaches represent extremes among available??strategies to cybercriminals. In fact, many miscreants??operate somewhere in between, carefully replicating the logic??of scams without completely copying all material from prior??iterations of the attack. For example, criminals engaged in??advanced-fee frauds may create bank websites for non-existent??banks, complete with online banking services where the victim??can log in to inspect their ?€?deposits?€?. When one fake??bank is shut down, the criminals create a new one that has??been tweaked from the former website. Similarly, criminals??establish fake escrow services as part of a larger advanced-fee??fraud [21]. On the surface, the escrow websites look different,??but they often share similarities in page text or HTML structure. ??Yet another example is online Ponzi schemes called high-yield??investment programs (HYIPs) [22]. The programs offer??outlandish interest rates to draw investors, which means they??inevitably collapse when new deposits dry up. The perpetrators??behind the scenes then create new programs that often share??similarities with earlier versions.,The designers of these scams have a strong incentive to keep??their new copies distinct from the old ones. Prospective victims??may be scared away if they realize that an older version of this??website has been reported as fraudulent. Hence, the criminals??make a more concerted effort to distinguish their new copies??from the old ones.,While in principle the criminals could start over from??scratch with each new scam, in practice it is expensive to??recreate entirely new content repeatedly. Hence, things that??can be changed easily are (e.g., service name, domain name,??registration information). Website structure (if coming from a??kit) or the text on a page (if the criminal?€?s English or writing??composition skills are weak) are more costly to change, so??only minor changes are frequently made.,The purpose of this paper is to design, implement, and??evaluate a method for clustering these ?€?logical copies?€? of scam??websites. In Section II we describe two sources of data on??scam websites that we will use for evaluation: fake-escrow??websites and HYIPs. In Section III we outline a combined??clustering method that weighs HTML tags, website text, and??file structure in order to link disparate websites together. We??then evaluate the method compared to other approaches in??the consensus clustering literature and cybercrime literature to??demonstrate its improved accuracy in Section IV. In Section V??we apply the method to the entire fake-escrow and HYIP??datasets and analyze the findings. We review related work in??Section VI and conclude in Section VII.,In order to demonstrate the generality of our clustering??approach, we collect datasets on two very different forms of??cybercrime: online Ponzi schemes known as High-Yield Investment??Programs (HYIPs) and fake-escrow websites. In both??cases, we fetch the HTML using wget. We followed links to a??depth of 1, while duplicating the website?€?s directory structure.??All communications were run through the anonymizing service??Tor [6]., We use the HYIP??websites identified by Moore et al. in [22]. HYIPs peddle??dubious financial products that promise unrealistically high??returns on customer deposits in the range of 1?€?2% interest,??compounded daily. HYIPs can afford to pay such generous??returns by paying out existing depositors with funds obtained??from new customers. Thus, they meet the classic definition of a??Ponzi scheme. ??Because HYIPs routinely fail, a number of ethically??questionable entrepreneurs have spotted an opportunity to??track HYIPs and alert investors to when they should withdraw??money from schemes prior to collapse. Moore et al. repeatedly??crawled the websites listed by these HYIP aggregators, such??as hyip.com, who monitor for new HYIP websites as well??as track those that have failed. In all, we have identified 4,191??HYIP websites operational between November 7, 2010 and??September 27, 2012., A long-running form??of advanced-fee fraud is for criminals to set up fraudulent??escrow services [21] and dupe consumers with attractively??priced high-value items such as cars and boats that cannot be??paid for using credit cards. After the sale the fraudster directs??the buyer to use an escrow service chosen by the criminal,??which is in fact a sham website. A number of volunteer groups??track these websites and attempt to shut the websites down??by notifying hosting providers and domain name registrars. ??We identified reports from two leading sources of fake-escrow??websites, aa419.org and escrow-fraud.com. We used??automated scripts to check for new reports daily. When new??websites are reported, we collect the relevant HTML. In all, we??have identified 1 216 fake-escrow websites reported between??January 07, 2013 and June 06, 2013.,For both data sources, we expect that the criminals behind??the schemes are frequently repeat offenders. As earlier??schemes collapse or are shut down, new websites emerge. ??However, while there is usually an attempt to hide evidence??of any link between the scam websites, it may be possible to??identify hidden similarities by inspecting the structure of the??HTML code and website content. We next describe a process??for identifying such similarities.,We now describe a general-purpose method for identifying??replicated websites. Figure 1 provides a high-level overview.??We now briefly describe each step before detailing each in??greater detail below.,??We identified three??input attributes of websites as potential indicators of similarity:??website text sentences, HTML tags and website file names. ??To identify the text that renders on a given webpage, we??used a custom ?€?headless?€? browser adapted from the ,. We extracted text from all pages associated??with a given website, then split the text into sentences using??the , sentence breaker for C#. ??We extracted all HTML tags in the website?€?s HTML files,??while noting how many times each tag occurs. We then constructed??a compound tag with the tag name and its frequency. ??For example, if the ?€?<br>?€? tag occurs 12 times within the targeted HTML files, the extracted key would be ?€?<br>12?€?.,Finally, we examined the directory structure and file names??for each website since these could betray structural similarity,??even when the other content has changed. However, some??subtleties must be accounted for during the extraction of this??attribute. First, the directory structure is incorporated into??the filename (e.g., admin/home.html). Second, since most??websites include a home or main page given the same name,??such as index.htm, index.html, or Default.aspx, websites comprised of only one file may in fact be quite??different. Consequently, we exclude this input attribute from??consideration for such websites., For each input attribute, we??calculated the Jaccard distance between all pairs of websites.??The Jaccard distance between two sets S and T is defined as??1- J(S; T), where:,Consider comparing website similarity by sentences. If??website A has 50 sentences in the text of its web pages and??website B has 40 sentences, and they have 35 in common,??then the Jaccard distance is 1 - J(A;B) = 1 - 35??/ 65 = 0.46., We compute clusterings for each input attributes??using a hierarchical clustering algorithm [11]. Instead??of selecting a static height cutoff for the resulting dendogram,??we employ a dynamic tree cut using the method described??in [15]. These individual clusterings are computed because??once we evaluate the clusters against ground truth, we may find??that one of the individual clusterings work better. If we intend??to incorporate all input attributes, this step can be skipped., Combining orthogonal distance??measures into a single measure must necessarily be??an information-lossy operation. A number of other consensus??clustering methods have been proposed [2], [5], [7], [9] , yet??as we will demonstrate in the next section, these algorithms??do not perform well when linking together replicated scam??websites, often yielding less accurate results than clusterings??based on individual input attributes.,Consequently, we have developed a simple and more accurate??approach to combining the different distance matrices. ??We define the pairwise distance between two websites a??and b as the minimum distance across all input attributes. ??The rationale for doing so is that a website may be very??different across one measure but similar according to another. ??Suppose a criminal manages to change the textual content of??many sentences on a website, but uses the same underlying??HTML code and file structure. Using the minimum distance??ensures that these two websites are viewed as similar. Figure 2??demonstrates examples of both replicated website content and??file structures. The highlighted text and file structures for each??website displayed are nearly identical.,One could also imagine circumstances in which the average??or maximum distance among input attributes was more appropriate. ??We calculate those measures too, mainly to demonstrate??the superiority of the minimum approach., We then cluster the websites for a??second time, based upon the combined matrix. Once again,??hierarchical clustering with dynamic cut tree height is used.,When labeled clusters are available for a sample of websites,??the final step is to compare the combined clustering following??stage 2 to the individual clusterings based on single input attributes. ??The more accurate method is selected for subsequent??use.,One of the fundamental challenges to clustering logical??copies of criminal websites is the lack of ground-truth data??for evaluating the accuracy of automated methods. Some??researchers have relied on expert judgment to assess similarity,??but most forego any systematic evaluation due to a lack of??ground truth (e.g., [17]). We now describe a method for??constructing ground truth datasets for samples of fake-escrow??services and high-yield investment programs.,We developed a software tool to expedite the evaluation??process. This tool enabled pairwise comparison of website??screenshots and input attributes (i.e., website text sentences,??HTML tag sequences and file structure) by an evaluator.,After the individual clusterings were calculated for each??input attribute, websites could be sorted to identify manual??clustering candidates which were placed in the exact same??clusters for each individual input attribute?€?s automated clustering. ??Populations of websites placed into the same clusters??for all three input attributes were used as a starting point??in the identification of the manual ground truth clusterings. ??These websites were then analyzed using the comparison tool??in order to make a final assessment of whether the website??belonged to a cluster. Multiple passes through the website??populations were performed in order to place them into the??correct manual ground truth clusters. When websites were??identified which did not belong in their original assigned??cluster, these sites were placed into the unassigned website??population for further review and other potential clustering??opportunities.,Deciding when to group together similar websites into the??same cluster is inherently subjective. We adopted a broad??definition of similarity, in which sites were grouped together??if they shared most, but not all of their input attributes in??common. Furthermore, the similarity threshold only had to??be met for one input attribute. For instance, HYIP websites??are typically quite verbose. Many such websites contain 3??or 4 identical ??paragraphs of text, along with perhaps one or??two additional paragraphs of completely unique text. For the??ground-truth evaluation, we deemed such websites to be in??the same cluster. Likewise, fake-escrow service websites might??appear visually identical in basic structure for most of the site. ??However, a few of the websites assigned to the same cluster??might contain extra web pages not present in the others.,We note that while our approach does rely on individual??input attribute clusterings as a starting point for evaluation, we??do not consider the final combined clustering in the evaluation. ??This is to maintain a degree of detachment from the combined clustering method ultimately used on the datasets. We believe??the manual clusterings identify a majority of clusters with??greater than two members. Although the manual clusterings??contain some clusters including only two members, manual??clustering efforts were ended when no more clusters of greater??than two members were being identified.,In total, we manually clustered 687 of the 4 191 HYIP??websites and 684 of the 1 221 fake-escrow websites. We??computed an adjusted Rand index [24] to evaluate the combined clustering method described in Section III against??the constructed ground-truth datasets described in Section V. ??We also evaluated other consensus clustering methods for??comparison. Rand index ranges from 0 to 1, where a score??of 1 indicates a perfect match between distinct clusterings.,Table Ia shows the adjusted Rand index for both datasets for??all combinations of input attributes and consensus clustering??methods. The first three columns show the Rand index for each??individual clustering. For instance, ??or fake-escrow services,??clustering based on tags alone yielded a Rand index of 0.672.??Thus, clustering based on sentences alone is much more accurate??than by file structure or website sentences alone (Rand??indices of 0.072 and 0.10 respectively). ??When combining??these input attributes, however, we see further improvement. ??Clustering based on taking the minimum distance between??websites according to HTML tags and sentences yield a??Rand index of 0.9, while taking the minimum of all three??input attributes yields an adjusted Rand index of 0.952. This??combined score far exceeds the Rand indices for any of the??other comparisons.,Because cybercriminals act differently when creating logical??copies of website for different types of scams, the input??attributes that are most similar can change. For example,??for HYIPs, we can see that clustering by website sentences??yields the most accurate Rand index, instead of HTML tags??as is the case for fake-escrow services. We can also see that??for some scams, combining input attributes does not yield a??more accurate clustering. Clustering based on the minimum??distance of all three attributes yields a Rand index of 0.301,??far worse than clustering based on website sentences alone. ??This underscores the importance of evaluating the individual??distance scores against the combined scores, since in some??circumstances an individual input attribute or a combination??of a subset of the attributes may fare better.,We used several general-purpose consensus clustering methods??from R?€?s Clue package [10] as benchmarks against the our???€?best minimum?€? approach:,Table Ib summarizes the best-performing measures for??different consensus clustering approaches. We can see that??our ?€?best minimum?€? approach performs best. It yields more??accurate results than other general-purpose consensus clustering??methods, as well as the custom clustering method used to??group spam-advertised websites by the authors of [18].,We now apply the best-performing clustering methods identified??in the prior section to the entire fake-escrow and HYIP??datasets. The 4 191 HYIP websites formed 864 clusters of at??least size two, plus an additional 530 singletons. The 1 216??fake-escrow websites observed between January and June??2013 formed 161 clusters of at least size two, plus seven??singletons.,We first study the distribution of cluster size in the two??datasets. Figure 3a plots a CDF of the cluster size (note the??logarithmic scale on the x-axis). We can see from the blue??dashed line that the HYIPs tend to have smaller clusters. In??addition to the 530 singletons (40% of the total clusters), 662??clusters (47% of the total) include between 2 and 5 websites.??175 clusters (13%) are sized between 6 and 10 websites,??with 27 clusters including more than 10 websites. The biggest??cluster included 20 HYIP websites. These results indicate that??duplication in HYIPs, while frequent, does not occur on the??same scale as many other forms of cybercrime.,There is more overt copying in the escrow-fraud dataset. ??Only 7 of the 1 216 escrow websites could not be clustered??with another website. 80 clusters (28% of the total) include??between 2 and 5 websites, but another 79 clusters are sized??between 6 and 20. Furthermore, two large clusters (including??113 and 109 websites respectively) can be found. We conclude??that duplication is used more often as a criminal tactic in the??fake-escrow websites than for the HYIPs.,Another way to look at the distribution of cluster sizes is??to examine the rank-order plot in Figure 3b. Again, we can??observe differences in the structure of the two datasets. Rankorder??plots sort the clusters by size and show the percentage of??websites that are covered by the smallest number of clusters. ??For instance, we can see from the red solid line the effect??of the two large clusters in the escrow-fraud dataset. These??two clusters account for nearly 20% of the total escrow-fraud??websites. After that, the next-biggest clusters make a much??smaller contribution in identifying more websites. Nonetheless,??the incremental contributions of the HYIP clusters (shown??in the dashed blue line) are also quite small. This relative??dispersion of clusters differs from the concentration found in??other cybercrime datasets where there is large-scale replication??of content.,We now study how frequently the replicated criminal websites??are re-used over time. One strategy available to criminals??is to create multiple copies of the website in parallel, thereby??reaching more victims more quickly. The alternative is to reuse??copies in a serial fashion, introducing new copies only??after time has passed or the prior instances have collapsed. We??investigate both datasets to empirically answer the question of??which strategy is preferred.,Figure 4 groups the 10 largest clusters from the fake-escrow??dataset and plots the date at which each website in the cluster??first appears. We can see that for the two largest clusters??there are spikes where multiple website copies are spawned??on the same day. For the smaller clusters, however, we see??that websites are introduced sequentially. Moreover, for all of??the biggest clusters, new copies are introduced throughout the??observation period. From this we can conclude that criminals??are likely to use the same template repeatedly until stopped.,Next, we examine the observed persistence of the clusters. ??We define the ?€?lifetime?€? of a cluster as the difference in??days between the first and last appearance of a website in the??cluster. For instance, the first-reported website in one cluster??of 18 fake-escrow websites appeared on February 2, 2013,??while the last occurred on May 7, 2013. Hence, the lifetime??of the cluster is 92 days. Longer-lived clusters indicate that??cybercriminals can create website copies for long periods of??time with impunity.,We use a survival probability plot to examine the distribution??of cluster lifetimes. A survival function S(t) measures??the probability that a cluster?€?s lifetime is greater than time t. ??Survival analysis takes into account ?€?censored?€? data points,??i.e., when the final website in the cluster is reported near the??end of the study. We deem any cluster with a website reported??within 14 days of the end of data collection to be censored. ??We use the Kaplan-Meier estimator [13] to calculate a survival??function.,Figure 5 gives the survival plots for both datasets (solid lines??indicate the survival probability, while dashed lines indicate??95% confidence intervals). In the left graph, we can see that??around 75% of fake-escrow clusters persist for at least 60 days,??and that the median lifetime is 90 days. Note that around??25% of clusters remained active at the end of the 150-day??measurement period, so we cannot reason about how long??these most-persistent clusters will remain.,Because we tracked HYIPs for a much longer period??(Figure 5 (right)), nearly all clusters eventually ceased to??be replicated. Consequently, the survival probability for even??long-lived clusters can be evaluated. 20% of HYIP clusters??persist for more than 500 days, while 25% do not last longer??than 100 days. The median lifetime of HYIP clusters is around??250 days. The relatively long persistence of many HYIP??clusters should give law enforcement some encouragement:??because the criminals reuse content over long periods, tracking??them down becomes a more realistic proposition.,A number of researchers have applied machine learning??methods to cluster websites created by cybercriminals. Wardman??et al. examined the file structure and content of suspected??phishing webpages to automatically classify reported URLs??as phishing [27]. Layton et al. cluster phishing webpages??together using a combination of k-means and agglomerative??clustering [16]. ??Several researchers have classified and clustered web spam pages. Urvoy et al. use HTML structure to classify web pages,??and they develop a clustering method using locality-sensitive??hashing to cluster similar spam pages together [25]. Lin uses??HTML tag multisets to classify cloaked webpages [19]. Lin?€?s??technique is used byWang et al. [26] to detect when the cached??HTML is very different from what is presented to user. Finally,??Anderson et al. use image shingling to cluster screenshots of websites advertised in email spam [4]. Similarly, Levchenko??et al. use a custom clustering heuristic method to group??similar spam-advertised web pages [18]. We implemented and??evaluated this clustering method on the cybercrime datasets in??Section IV. Finally, Leontiadis et al. group similar unlicensed??online pharmacy inventories [17]. They did not attempt to evaluate??against ground truth; instead they used Jaccard distance ??and agglomerative clustering to find suitable clusters.,Separate to the work on cybercriminal datasets, other researchers??have proposed consensus clustering methods for different??applications. DISTATIS is an adaptation of the STATIS??methodlogy specifically used for the purposes of integrating??distance matrices for different input attributes [3]. DISTATIS??can be considered a three-way extension of metric multidimensional??scaling [14], which transforms a collection of??distance matrices into cross-product matrices used in the crossproduct??approach to STATIS. Consensus can be performed??between two or more distance matrices by using DISTATIS??and then converting the cross-product matrix output into into??a (squared) Euclidean distance matrix which is the inverse??transformation of metric multidimensional scaling [1].,Our work follows in the line of both of the above research??thrusts. It differs in that it considers multiple attributes that an??attacker may change (site content, HTML structure and file??structure), even when she may not modify all attributes. It??is also tolerant of greater changes by the cybercriminal than??previous approaches. At the same time, though, it is more??specific than general consensus clustering methods, which??enables the method to achieve higher accuracy in cluster??labelings.,When designing scams, cybercriminals face trade-offs between??scale and victim susceptibility, and between scale and??evasiveness from law enforcement. Large-scale scams cast a??wider net, but this comes at the expense of lower victim yield??and faster defender response. Highly targeted attacks are much??more likely to work, but they are more expensive to craft. ??Some frauds lie in the middle, where the criminals replicate??scams but not without taking care to give the appearance that??each attack is distinct.,In this paper, we propose and evaluate a combined clustering??method to automatically link together such semi-automated??scams. We have shown it to be more accurate than??general-purpose consensus clustering approaches, as well as??approaches designed for large-scale scams such as phishing??that use more extensive copying of content. In particular, we??applied the method to two classes of scams: HYIPs and fake-escrow??websites.,The method could prove valuable to law enforcement, as??it helps tackle cybercrimes that individually are too minor to??investigate but collectively may cross a threshold of significance. ??For instance, our method identifies two distinct clusters??of more than 100 fake escrow websites each. Furthermore, our??method could substantially reduce the workload for investigators??as they prioritize which criminals to investigate.,There are several promising avenues of future work we??would like to pursue. First, the accuracy of the HYIP clustering??could be improved. Second, it would be interesting to compare??the accuracy of the combined clustering method to other??areas where clustering has already been tried, such as in??the identification of phishing websites and spam-advertised??storefronts. Finally, additional input attributes such as WHOIS??registration details and screenshots could be considered.,We would like to thank the operators of??, and , for allowing??us to use their lists of fake-escrow websites.,This work was partially funded by the Department of Homeland??Security (DHS) Science and Technology Directorate,??Cyber Security Division (DHS S&T/CSD) Broad Agency Announcement??11.02, the Government of Australia and SPAWAR??Systems Center Pacific via contract number N66001-13-C-0131. This paper represents the position of the authors and??not that of the aforementioned agencies.
Posted on Sandhill.,2),?€? Infrastructure player with a compelling ROI value proposition of minimizing copies of data ?€? a key hygiene factor in managing Big Data in the enterprise. They have a lot of momentum with a potential IPO in 2014.,3),???€???Real-time Big Data analytics with a hybrid approach. They promise the speed of an In-Memory database with the persistence of rotational drives. They are classified as the only ?€?visionary?€? in the Gartner Magic Quadrant for operational database management systems.,4),?€? Predictive analytics platform using Hadoop. Targeted at customers that have taken the first step with Hadoop and want to deploy advanced analytics solutions. They have several banking customers including Barclays. Other customers include Sony, Nike and Kaiser Permanente.,5),?€? SAS alternative for statistical analysis applications such as marketing analytics with an advanced visualization story based on R statistical-programming language. Their success will depend on how well they execute on the consumer-friendly promise with traditional users. Customers include Paychex, Kroger, Michaels and Equifax.,6),?€? Addresses an immediate practical requirement to manage the coexistence of Hadoop in the traditional IT environment. They promise to reduce waste by analyzing business activity and data usage across traditional data warehouses and identify data that can be offloaded to Hadoop. Customers include Pfizer and Union Bank of California.,7),?€? Advanced content analytics across data silos with a few twists such as intelligent correlation. This is a variation of Endeca (acquired by Oracle) with a technical value proposition with an engineering centric DNA from Mathworks and Ab Initio.,8)??,?€? Machine learning with high-end visualization of complex data sets based on topological data analysis. Partnering with Texas Medical Center and Lawrence Livermore National Laboratory. Customers include UCSF, Merck and GE.,9),?€? Predictive operational analytics for manufacturing, energy and utilities based in Scotland with a measurable ROI value proposition. Customers include Chevron, National Grid (UK) and SA Water (Australia).,10),?€? High-speed data analytics and visualization using In-Memory database technology and,clustering system. Google pedigree from the designers of Google Analytics and Google Adwords. Customers are the Dannon Company, Kantar Media and DataSift (see below).,11),?€? Market leader that was a pioneer in 2009 with the Hadoop platform and founders from Google, Yahoo, Facebook and Oracle. They have parlayed their pioneer status to become an influential member of the Big Data ecosystem.,12),?€? Outstanding story of non-profit of data scientists for social change. They bring high-end skills to disenfranchised communities and social organizations and tackle complex problems such as natural disasters and crimes using data analytics.,13),?€? Brings Big Data technologies to business users familiar with using spreadsheets for analyzing and presenting data for traditional BI solutions. Extensive list of customers includes Sears, Workday and Visa.,14),?€? Leading data aggregator and reseller for Twitter and other social media sources. Based in the UK. Major player in emerging data ecosystem around Twitter. Prominent customers include Dell, Yum Brands and CBS interactive.,15),?€? Ecosystem player and commercial vendor for enterprise-ready Casandra, Apache Hadoop and Apache Solr. Rapid adoption in the last two years leading to 300 customers including Adobe, eBay, Thomson Reuters and Netflix as well as 20 of the Fortune 100. 
Specifically designed in the context of big data in our research lab, the new and simple??,??synthetic metric proposed in this article should be used, whenever you want to check if there is a real association between two variables, especially in large-scale automated data science or machine learning projects. Use this new metric now, to avoid being accused of ,??and even ,.,In this paper, the traditional correlation is referred to as the ,, as it captures only a small part of the association between two variables: , results in capturing spurious correlations and predictive modeling deficiencies, even with as few as 100 variables. In short, our , (with a value between 0 and 1) is high (say above 0.80) if not only the , is also high (in absolute value), but when the internal structures (auto-dependencies) of both variables X and Y that you want to compare, exhibit a similar pattern or correlogram. Yet this new metric is simple and involves just one parameter ,??(with , = 0 corresponding to ,, and , =1 being the recommended value for ,). This setting is designed to avoid over-fitting.??,Our , blends together the concept of ordinary or , - indeed, an improved, robust, ,??(or see ,??pages 130-140) - together with the concept of X and Y sharing similar ,??(or see ,??pages 125-128).,In short, even nowadays, what makes two variables X and Y ,??related in most scientific articles and pretty much all articles written by journalists, is based on ordinary (weak) regression. But there are plenty of other metrics that you can use to compare two variables. Including bumpiness in the mix (together with weak regression in just one single blended metric called , to boost accuracy) guarantees that high , correlation means that the two variables are really associated, not just based on flawy, old-fashioned , correlations, but also associated based on sharing similar internal auto-dependencies and structure. To put it differently, two variables can be highly , correlated yet have very different bumpiness coefficients, as shown in ,??- meaning that there might be??,??(or see ,??pages 165-168) or hidden factors explaining the link. An artificial example is provided below in figure 3.,Using ,, rather than , correlation, eliminates the majority of these spurious correlations, as we shall see in the examples below. This , metric is designed to be integrated in automated data science algorithms.,Let's define,Note that c1(X), and c1(Y) are the ,??(or see??,??pages 125-128)??for X and Y. Also, d(X, Y) and thus r(X, Y) are between 0 and 1, with 1 meaning strong similarity between X and Y, and 0 meaning , dissimilar lag-1 auto-correlations for X and Y, , lack of old-fashioned correlation.,The , between X and Y is, by definition, r(X, Y). This is an approximation to having both spectra identical, a solution mentioned in my article ,??(see also ,??pages 41-45).,This definition of strong correlation was initially suggested in one of our ,.,When , = 0, weak and strong correlations are identical. Note that the , r(X, Y) still shares the same properties as the , c(X, Y): it is symmetric and invariant under linear transformations (such as re-scaling) of variables X or Y, regardless of ,. ,In figures 1 and 2, we simulated a large number of uniformly and independently distributed random variables Y (> 10,000) each with n observations, and computed the correlation with an arbitrary variable X with pre-specified values. So in theory, you would expect all the correlations to be close to zero. The following scatterplots (figures 1 and 2) disprove this fact.,In practice, tons of , are still well above 0.60 if you look at figure 1, though few , are above 0.20 in the same figure (, is the minimum between , and ,). Figure 2 is more difficult to interpret visually because n is too small (n = 4), though the conclusion is similar and obvious if you check the results in the spreadsheet (see next section). In this example, , = 4.,The spreadsheet shows simulation of a variable X with n observations, stored in first row, with thousands of simulated Y's??in the subsequent rows. There are two tabs: one for n = 4, and one for n = 9. For instance, in the n = 9 tab, column J represents the weak correlation c(X, Y), column M represents c1(Y), and column N represents the strong correlation r(X, Y). The parameter , is stored in cell P1, and summary stats are found in cells Q1:T12. The spreasheet is a bit unusual in the sense that rows represent variables, and columns represent observations.,??(about 20 MB in compressed format),The Excel spreadsheet has some intrisic value besides explaining my methodology: you will learn about Excel techniques and formulas such as percentiles, random numbers and indirect countif formulas to build cumulative distributions. For better random generators, ,??or ,??pages 161-163.,Confidence intervals for these correlations are easy to obtain, by running 10 times these simulations and see what min and max you get. For details, ,??or ,??pages 158-161.,The strong correlation is useful in contexts where you are comparing (or correlating or clustering) tons of data bins, for instance for transaction scoring, using a limited number , of features (, corresponding to the one used in this article). This is the case with granular ,??(see also ,??pages 153-158 and 224-228).,It is also useful when comparing millions of small, local time series, for instance in the context of HFT (High Frequency Trading), when you try to find cross-correlations with time lags among thousands of stocks.,Note that , = 4 (as used in my spreadsheet) is too high in most situations, and I recommend , = 1, which has the following advantages:,Note that in the spreadsheet, when n = 4 and , = 4, about 40% of all , are above 0.60, while only 5% of , are above 0.60. Technically, all the simulated Y's are uniform, random, independent variables, so it is amazing to see so many high (,) correlations - there are indeed all spurious correlations. Even with n = 9, the contrasts between weak and strong correlations are still significant. The ,??clearly eliminates a very large chunk of the spurious correlations, especially when , > 2. But it can eliminate true correlations as well, thus my recommendation to use??, = 1, as a compromise. A high value for , has effects??similar to over-fitting and should be avoided.,It is possible to integrate auto-correlations of lag 1, 2, and up to n-2, but we then risk to over-fit, except if we put decaying weights on the various lags. This approach has certainly been investigated by other scientists (can you provide references?) as it amounts to do a spectral analysis of time series as mentioned in my article ,??(or see ,??pages 41-45), compare spectra, likely using Fourier transforms.,It would be great to do this analysis on actual data, not just simulated random noise. Or even on non-random simulated data, using for instance the artificially correlated data (with correlations injected into the data) described in my article ,.??Would we miss too many correlations, on truly correlated data, using strong correlation with a moderate , = 1 and n between 5 and 15? ??,Finally there are many other metrics available to measure other forms of correlations (for instance on unusual domains), see for instance my article on ,??or ,??page 141.,For those participating in our ,??(DSA), we have added , as one of the projects??that you can work on: specifically, answer questions from section 5 and 6 from this article (see project #1 under "data science research", in the ,).,I haven't done research in that direction yet. I have a few questions:??,The , is a synthetic metric, and belongs to the family of synthetic metrics that we created over the last few years. Synthetic metrics are designed to efficiently solve a problem, rather than being crafted for their beauty, elegancy and mathematical properties: they are directly derived from data experiments (bottom-up approach) rather than the other way around (top-down: from theory to application) as in traditional science. Other synthetitic metrics include:,??for details, or check ,??pages 187-194.????,This article builds on previous data science (robust correlations, bumpiness coefficient) to design a one-parameter, synthetic metric, that can be used to dramatically reduce - by well over 50% - the number of spurrious correlations. It is part of a family of robust, almost parameter-free metrics and methods developed in our research lab, to be integrated in black-box data science software or automated data science, and used by engineers or managers who are not experts in data science.
The??,??is always published Monday.??Starred articles are new additions, posted between Thursday and Sunday,These profiles are randomly selected among our active members. To be selected, you need to have an interesting profile with picture; it helps to post a blog, or share / comment / like contributions from other members.
AQL - Annotation Query Language, AOSD - Aspect-Oriented Software Development, ACID - Atomicity, Consistency, Isolation and Durability, BDA - Big Data Analytics, CQL - Cypher Query Language, CQL - Cassandra Query Language, CQL - Contextual/Common Query Language, COTS - Commodity off-the-shelf, CART - Classification and Regression Trees, CCA - Canonical Correlational Analysis, CEP - Complex Event Processing, DAD - Discover, Access, Distill, 3DM - Data Driven Decision Making, DHSL - Distributed Hadoop Storage Layer, DAG - Directed Acyclic Graph, EDA - Exploratory Data Analysis, Event Driven Architeture, ECL - Enterprise Control Language, EPN - Event Processing Nodes, FUSE - Filesystem in Userspace, GEOFF - Graph Serialization Format, HPCC - High Performance Computing Cluster, HPIL - Hadoop Physical Infrastruture Layer, HAR - Hadoop Archive, IDA - Initial Data Analysis, JSON - JavaScriptObjectNotation Query Language, JAQL - JSON Query Language , JSON - JavaScriptObjectNotation Query Language, KFS - Kosmos File System, LZO - Lempel?€?Ziv?€?Oberhumer, MDM - Master Data Management, NLP - Natural Language Processing, OLAP - Online Analytical Processing, OLTP - Online Transactional Processing, PMML - Predictive Model Markup Language, Q - {pending, please share if you have some thing}, RDD - Resilient Distributed Database, SOA - Service Oriented Architeture, S4 - Simple Scalable Streaming System, TDA - Topological Data Analysis, UIMA - Unstructured Information Management Architecture, UDAF - USer Defined Aggregate Function, UDTF - User Defined Tablegenerating Function, VC - Vapnik Chervonekis Dimension, W3C - World Wide Web Consortium, XML - Extensible Markup Language, YARN - Yet Another Resource Manager, ZFS - Zettabyte File System by Sun Mircosystem
I have been slowly moving down the path of becoming a data scientist for the previous six months.?? I have always been interested with data and statistics, and am excited to have found this great website.??,I am currently finishing the first course in the data science specialization through coursera hosted by John Hopkins, and filling in my programming gaps in R through taking courses on Datacamp.?? I appreciate all the time and energy that has been put forward to create this website and to share the successes and challenges that working with massive data sets provides. ??I look forward to continuing to learn and grown in the field of data science!??

Specifically designed in the context of big data in our research lab, the new and simple??,??synthetic metric proposed in this article should be used, whenever you want to check if there is a real association between two variables, especially in large-scale automated data science or machine learning projects. Use this new metric now, to avoid being accused of ,??and even ,.,In this paper, the traditional correlation is referred to as the ,, as it captures only a small part of the association between two variables: , results in capturing spurious correlations and predictive modeling deficiencies, even with as few as 100 variables. In short, our , (with a value between 0 and 1) is high (say above 0.80) if not only the , is also high (in absolute value), but when the internal structures (auto-dependencies) of both variables X and Y that you want to compare, exhibit a similar pattern or correlogram. Yet this new metric is simple and involves just one parameter ,??(with , = 0 corresponding to ,, and , =1 being the recommended value for ,). This setting is designed to avoid over-fitting.??,Our , blends together the concept of ordinary or , - indeed, an improved, robust, ,??(or see ,??pages 130-140) - together with the concept of X and Y sharing similar ,??(or see ,??pages 125-128).,In short, even nowadays, what makes two variables X and Y ,??related in most scientific articles and pretty much all articles written by journalists, is based on ordinary (weak) regression. But there are plenty of other metrics that you can use to compare two variables. Including bumpiness in the mix (together with weak regression in just one single blended metric called , to boost accuracy) guarantees that high , correlation means that the two variables are really associated, not just based on flawy, old-fashioned , correlations, but also associated based on sharing similar internal auto-dependencies and structure. To put it differently, two variables can be highly , correlated yet have very different bumpiness coefficients, as shown in ,??- meaning that there might be??,??(or see ,??pages 165-168) or hidden factors explaining the link. An artificial example is provided below in figure 3.,Using ,, rather than , correlation, eliminates the majority of these spurious correlations, as we shall see in the examples below. This , metric is designed to be integrated in automated data science algorithms.,Let's define,Note that c1(X), and c1(Y) are the ,??(or see??,??pages 125-128)??for X and Y. Also, d(X, Y) and thus r(X, Y) are between 0 and 1, with 1 meaning strong similarity between X and Y, and 0 meaning , dissimilar lag-1 auto-correlations for X and Y, , lack of old-fashioned correlation.,The , between X and Y is, by definition, r(X, Y). This is an approximation to having both spectra identical, a solution mentioned in my article ,??(see also ,??pages 41-45).,This definition of strong correlation was initially suggested in one of our ,.,When , = 0, weak and strong correlations are identical. Note that the , r(X, Y) still shares the same properties as the , c(X, Y): it is symmetric and invariant under linear transformations (such as re-scaling) of variables X or Y, regardless of ,. ,In figures 1 and 2, we simulated a large number of uniformly and independently distributed random variables Y (> 10,000) each with n observations, and computed the correlation with an arbitrary variable X with pre-specified values. So in theory, you would expect all the correlations to be close to zero. The following scatterplots (figures 1 and 2) disprove this fact.,In practice, tons of , are still well above 0.60 if you look at figure 1, though few , are above 0.20 in the same figure (, is the minimum between , and ,). Figure 2 is more difficult to interpret visually because n is too small (n = 4), though the conclusion is similar and obvious if you check the results in the spreadsheet (see next section). In this example, , = 4.,The spreadsheet shows simulation of a variable X with n observations, stored in first row, with thousands of simulated Y's??in the subsequent rows. There are two tabs: one for n = 4, and one for n = 9. For instance, in the n = 9 tab, column J represents the weak correlation c(X, Y), column M represents c1(Y), and column N represents the strong correlation r(X, Y). The parameter , is stored in cell P1, and summary stats are found in cells Q1:T12. The spreasheet is a bit unusual in the sense that rows represent variables, and columns represent observations.,??(about 20 MB in compressed format),The Excel spreadsheet has some intrisic value besides explaining my methodology: you will learn about Excel techniques and formulas such as percentiles, random numbers and indirect countif formulas to build cumulative distributions. For better random generators, ,??or ,??pages 161-163.,Confidence intervals for these correlations are easy to obtain, by running 10 times these simulations and see what min and max you get. For details, ,??or ,??pages 158-161.,The strong correlation is useful in contexts where you are comparing (or correlating or clustering) tons of data bins, for instance for transaction scoring, using a limited number , of features (, corresponding to the one used in this article). This is the case with granular ,??(see also ,??pages 153-158 and 224-228).,It is also useful when comparing millions of small, local time series, for instance in the context of HFT (High Frequency Trading), when you try to find cross-correlations with time lags among thousands of stocks.,Note that , = 4 (as used in my spreadsheet) is too high in most situations, and I recommend , = 1, which has the following advantages:,Note that in the spreadsheet, when n = 4 and , = 4, about 40% of all , are above 0.60, while only 5% of , are above 0.60. Technically, all the simulated Y's are uniform, random, independent variables, so it is amazing to see so many high (,) correlations - there are indeed all spurious correlations. Even with n = 9, the contrasts between weak and strong correlations are still significant. The ,??clearly eliminates a very large chunk of the spurious correlations, especially when , > 2. But it can eliminate true correlations as well, thus my recommendation to use??, = 1, as a compromise. A high value for , has effects??similar to over-fitting and should be avoided.,It is possible to integrate auto-correlations of lag 1, 2, and up to n-2, but we then risk to over-fit, except if we put decaying weights on the various lags. This approach has certainly been investigated by other scientists (can you provide references?) as it amounts to do a spectral analysis of time series as mentioned in my article ,??(or see ,??pages 41-45), compare spectra, likely using Fourier transforms.,It would be great to do this analysis on actual data, not just simulated random noise. Or even on non-random simulated data, using for instance the artificially correlated data (with correlations injected into the data) described in my article ,.??Would we miss too many correlations, on truly correlated data, using strong correlation with a moderate , = 1 and n between 5 and 15? ??,Finally there are many other metrics available to measure other forms of correlations (for instance on unusual domains), see for instance my article on ,??or ,??page 141.,For those participating in our ,??(DSA), we have added , as one of the projects??that you can work on: specifically, answer questions from section 5 and 6 from this article (see project #1 under "data science research", in the ,).,I haven't done research in that direction yet. I have a few questions:??,The , is a synthetic metric, and belongs to the family of synthetic metrics that we created over the last few years. Synthetic metrics are designed to efficiently solve a problem, rather than being crafted for their beauty, elegancy and mathematical properties: they are directly derived from data experiments (bottom-up approach) rather than the other way around (top-down: from theory to application) as in traditional science. Other synthetitic metrics include:,??for details, or check ,??pages 187-194.????,This article builds on previous data science (robust correlations, bumpiness coefficient) to design a one-parameter, synthetic metric, that can be used to dramatically reduce - by well over 50% - the number of spurrious correlations. It is part of a family of robust, almost parameter-free metrics and methods developed in our research lab, to be integrated in black-box data science software or automated data science, and used by engineers or managers who are not experts in data science.

Interesting blog post from a Berkely graduate (statistician) who was on the team that created Advil...,The guy graduated in the fifties, is very smart (wrote at least one great book that I bought, ,), but for whatever reasons, he is very bitter about his entire career. Here are a few extracts from his ranting - very funny - contrasting with the modern idea that anyone, barely or not qualified, can make tons of money quickly in data science:??
What are the differences between data science, data mining, machine learning, statistics, operations research, and so on?,Here I compare several analytic disciplines that overlap, to explain the differences and common denominators. Sometimes differences exist for nothing else other than historical reasons. Sometimes the differences are real and subtle. I also provide typical job titles, types of analyses, and industries traditionally attached to each discipline. Underlined domains are main sub-domains. It would be great if someone can add an historical perspective to my article.,First, let's start by describing data science, the new discipline.??,Job titles include data scientist, chief scientist, senior analyst, director of analytics ,. It covers all industries and fields, but especially digital analytics, search technology, marketing, fraud detection, astronomy, energy, healhcare, social networks, finance, forensics, security (NSA), mobile, telecommunications, weather forecasts, and fraud detection.,Projects include taxonomy creation (text mining, big data), ,, recommendation engines, simulations, rule systems for statistical scoring engines, root cause analysis,??automated bidding, forensics, exo-planets detection, and early detection of terrorist activity or pandemics, An important component of data science is automation, machine-to-machine communications, as well as algorithms running non-stop in production mode (sometimes in real time), for instance to detect fraud, predict weather or predict home prices for each home (Zillow).,An example of data science project is the creation of the ,, for computational marketing. It leverages big data, and is part of a viral marketing / growth hacking strategy that also includes automated high quality, relevant, syndicated content generation (in short, digital publishing version 3.0).,Unlike most other analytic professions, data scientists are assumed to have great business acumen and domain expertize -- one of the reasons why they tend to succeed as entrepreneurs.There are ,, as data science is ,. Many senior data scientists master their art/craftsmanship and possess the whole spectrum of skills and knowledge; they really are the unicorns that recruiters can't find. Hiring managers and uninformed executives favor ,??over combined deep, broad and specialized business domain expertize - a byproduct of the current education system that favors discipline silos, while true data science is a silo destructor. Unicorn data scientists (a misnomer, because they are not rare - some are famous VC's) ??usually work as consultants, or as executives. Junior data scientists tend to be more specialized in one aspect of data science, possess more hot technical skills (Hadoop, Pig, Cassandra)??and will have no problems finding a job if they received ,??and/or have work experience with companies such as Facebook, Google, eBay, Apple, Intel, Twitter, Amazon, Zillow etc. Data science projects for potential candidates ,.,Data science overlaps with,Finally, there are more specialized analytic disciplines that recently emerged: health analytics, computational chemistry and bioinformatics ??(genome research), for instance.
I have been reading and collecting data science resources for years (back in the days when BI / BA was all the rage). While there are lots of resources on the net, not all are great and some are even misleading.,Now, I have updated my collection and placed them into a neat Trello list, open to all.,Of course it features some of the great and interesting articles from data science central:,As well as great stuff across the net:,And much, much more. Most are free and some are from the Ivy League (note: they can be challenging for the uninitiated),. And if you have any resources to recommend, please comment below. You can also drop me a note on the ,??or you can mail to thiakx@gmail.com and I will add to the list.,Have fun learning!??
Granted, the fear of public speaking is often considered the most common of all phobias. In some (non-scientific) studies, evidence suggests that people fear public speaking more than death itself. It is not unlikely that analysts fear public speaking even more ?€? as they are often??,. In our projects, the most frustrating experiences for analysts were created when the results of the analysis were not adopted. Analysts often give their heart and soul in analyzing complex matters. They often spend much less mental energy in trying to ?€?sell?€? the results of their work. In general, however, analysts really care about their work being used and being useful ?€? hence they should care about maximizing the odds of adoption of their work within the organization. This often implies stepping away from the formulae and the data and translating the results of complex procedures into intuitively acceptable results. In order to present a research project successfully to non-experts, public speaking skills play a major role.,. Today, there is an interesting lack of clarity into the ?€?natural?€? career path for analysts. In our network, we?€?ve seen it all: analysts who become managers of analytical teams (who are suddenly challenged on the core competencies needed to fulfill their new role), analysts who become managers of other teams, and analysts who will eternally remain analysts. In a reality where people are ?€?encouraged?€? to work longer, one needs to ask the crucial question: will I remain forever happy as an analyst? A lack of public speaking skills has been considered a major factor??,??in general - and this is probably not much different for analysts.,??There is one way??,??There?€?s an excellent upcoming opportunity: Predictive Analytics World London is still looking for speakers (analysts and their managers) for their next event, October 29 & 30 in London. Additional details on??,Original article on??,??by??
"
"
Fifty copies of my ,??are available for the first 50 bloggers posting an original, relevant, non-promotional article, in our ,.,Your article will be featured in our ,. Other benefits of blogging with us:,Your article can be technical and even include source code, or it can be high-level, more mainstream but still relevant to ,.,If interested, email me at vincentg@datasciencecentral.com, once your blog post is featured. Don't forget to mention your mailing address. All postings are subject to approval. Duplicate content (posted elsewhere) may not be accepted. Not all blog posts get featured.,This offer is currently limited to US and Canada (due to shipping costs).,Best regards,, Vincent
Twitter has more than 250 million monthly active users who tweet more than 500 millions tweets per day. In the case someone is following many people, it isn?€?t feasible to read each tweet from them.,Do you have any idea about what people are twitting about Data Science, Big Data, Business Analytics, Hadoop, Machine Learning or R programming?,Word clouds are one of the simplest and most intuitive ways of visualizing text data. The clouds give greater prominence to words that appear more frequently in the tweets. During the last week, we collected all public tweets related to such topics, allowing us to track what people are talking about around the world and here we present the world cloud for each of them.,Using these word clouds we can more likely get answers to questions such as: What is Big Data? What can we do with it? Who uses Big Data? Who tweets about Big Data? What platforms and software products are dealing with Big Data? But even more, if we collect tweets for long periods of time, we will be able to generalize and extract definitions from buzzwords and new trends.,Some interesting facts based on the Big Data word cloud and the collected data:,As it can be noticed, this word cloud could help us to understand the phenomenon that is Big Data, but also other new trends.,Could you find other interesting facts for the other word clouds? Let us know in the comments.,All new tweets related to specific topics (Big Data, Data Science, Hadoop, etc.) were collected using a Python code, running in real-time in our dedicated Amazon EC2 Linux server, that saves them in a file.,The Natural Processing Language Toolkit (NLTK), was used to process the tweet data. Let?€?s explain simple things you can do with NLTK, for example: tokenizing and tagging of a recent @datasciencectrl tweet (terminal commands in python):,>>> ,??,>>> ,="10 types of regressions for #DataScientists, #Statisticians and other #Analytic practitioners, which one to use? ,",??,>>> ,??,>>> ,['10', 'types', 'of', 'regressions', 'for', '#', 'DataScientists', ',', '#', 'Statisticians', 'and', 'other', '#', 'Analytic', 'practitioners', ',', 'which', 'one', 'to', 'use', '?', 'http', ':', '//ow.ly/zsvs4'],??, processes the tweet text and attaches a tag to each word, e.g. CD: cardinal numbers, NNS: plural nouns, NNP: singular nouns, etc.,??,>>>,??,>>> ,[('10', 'CD'), ('types', 'NNS'), ('of', 'IN'), ('regressions', 'NNS'), ('for', 'IN'), ('#', '#'), ('DataScientists', 'NNS'), (',', ','), ('#', '#'), ('Statisticians', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('#', '#'), ('Analytic', 'NNP'), ('practitioners', 'NNS'), (',', ','), ('which', 'WDT'), ('one', 'CD'), ('to', 'TO'), ('use', 'VB'), ('?', '.'), ('http', 'NN'), (':', ':'), ('//ow.ly/zsvs4', '-NONE-')],??,Nouns can be printed using the tag information and functions from the regular expression package (,).,??,>>>,>>> for i in range(0,len(tagged)):,...???????? if re.match('N',tagged[i][1]):,...???????????????????????? print (tagged[i][0]),types,regressions,DataScientists,Statisticians,Analytic,practitioners,http,??,The processed data was visualized using a R package, called worldcloud (,).,The word clouds are represented for the most 200 frequent words in the tweets (least frequent terms were dropped). The font size of each word depends on the frequency of appearance on the tweets. The color of words comes from the ColorBrewer palettes, specifically the ?€?Dark2?€?, the frequency range from the most 200 frequent words were split in 8 groups. Then, words were colored from least to most frequent (see palette below).,1-Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O?€?Reilly Media Inc.
Moving forward, we plan to offer additional features that will only be accessible from our newsletter:,This include, but is not limited, to the following:,To make sure that you benefit from these exclusive advantages, ,:,The sender (the name we use in the "From" field) is ,, and all messages have our physical address (Issaquah, WA) at the bottom, as well as an unsubscribe button. If you use Gmail, check your Social or Promotion tab, and make sure to move our messages to Inbox. This can be done with a simple drag and drop, pointing to the message in question to move it to Inbox. You can choose to have us in your Inbox all the time: just do a Google search on "moving a sender to inbox". On Yahoo mail (and Gmail too), check your spam box: you might find many other valuable messages burried there, and maybe some of our messages.,We are in the process of removing subscribers who never check out our newsletter (for 6 months in a row), so you might be missing on some great stuff. To stay in our mailing list, make sure that you get our newsletter (and that you like the content of course), and move us to the right folder for you. Likewise, if you are no longer interested in our messages, the easiest way is to unsubscribe: it is much better than wait 6 months to be automatically removed due to lack of activity. Once removed, you can re-subscribe, though currently you need to use a different email address. We are thinking about making this process easier.,Example of recent messages from us include:,Not all our email messages are published on the web, and we reserve the right to stop publishing them on the web.,Finally, here's ,. Please check out this page if you do not remember how you subscribed.,Best regards,, Vincent
 , , , 
There is much discussion these days about , and its benefits for developing high performance analytic architectures.?? It offers a combination of a high performance, low latency ETL with a real-time layer, and a slower, more accurate, and flexible solution that runs in batch. ??,As I work with it, I have learned to appreciate Cassandra?€?s relative ?€?immortality?€? and fit for such analytic systems.?? In a complex distributed system it?€?s nice to know you have one component that you can rely on without much tending.?????? Need to ingress 500k messages per second? No problem. Need to be highly available and regionally distributed? Again, no problem.,Cassandra makes an excellent database for storage in the real-time layer for several reasons:,Cassandra as an analytics store requires a different mindset than you would use in a relational, Massively Parallel Processing (MPP) or Hadoop based system.?? In those platforms, arbitrary queries are easy and relatively performant. As traditional data modelers we store the data in a very low level of granularity (ideally at the atomic detail), typically in a ,.?????? Our dimensional data (attributes) are normalized from the fact in separate tables. ??This provides better data management capabilities with the ability to perform joins.?? We rely on aggregate functions to help us easily group and rollup data.?? In many cases, one fact table can satisfy all analytic questions on a particular set of metrics.,Cassandra, however, does not have this same query flexibility. Cassandra does not support joins or aggregation. These features are costly to support in a distributed environment and therefore have not been pursued, as they would compromise the performances and SLA of the Cassandra cluster. ??,As a result, we have to rely on two primary techniques to make our data useable for analytic queries:?? , and ,.??, is required as there is no join support.???? Data must be ?€?flattened?€? into fact table if it is to be used in analytic queries.?? Since aggregation is not available, the same data will typically be kept in several different tables depending on usage pattern and aggregation level.???? These separate tables are maintained by ETL via parallel stream processes or batch processing (typically, map reduce).,It is important to recognize these techniques are not all that alien.?? The traditional star schema approach relies heavily on these techniques as well -- although they manifest themselves more subtlety. Facts and dimensions are themselves denormalized structures.?? We group metrics at the same grain and natural key into the same fact table and our dimensions flatten all related attributes, and even hierarchies, into single tables as well.?? In the MPP world we often compromise and denormalize attributes to the fact tables themselves. ????,With regard to ,n, although we try to maintain a single fact table per business process, it is not uncommon to have multiple models containing the same data.?? In some cases this is to allow simpler and more performant queries based on a different query access pattern.?? We might also store aggregates of this data to avoid scanning millions or billions of rows to satisfy a common summary query.??,This commonality between Cassandra and the traditional world is driven by a few core concepts:,I can now walk you through an example demonstrating a real-time model for trading data:,The incoming data represents individual buy/sell information from trading activity.?? The following data elements are common to all records, although there are 100+ optional tags that may also be included:.,This incoming detail should be captured in its atomic form, as this practice is beneficial for two reasons:??,There are several ways to store this data in Cassandra.?? Thankfully, Cassandra?€?s data model makes it easy to deal with the flexible schema components (100+ variable fields).?? Unlike the relational world where we would need to predefine all possible fields, or normalize to the point of being useable, Cassandra offers several options. ??,My preferred option these days is to take advantage of Cassandra?€?s new collection data-type map.?? ????We normalize out a few key elements as fields and put the rest of the payload in a map field.???? This map field under the hood is stored in a standard CQL data structure, but is abstracted as a field resembling a map/dictionary data structure.,??,??,Normalizing a key field such as trade date allows us to define an index and to query against date.?? This is helpful for drill down, replay/recast or extraction to batch analytic layer.,We serve analytic queries against Cassandra by creating materialized views of the incoming data.???? I commonly refer to these materializations as ,.?? These cubes are transformed and generally ?€?lightly aggregated?€? by several key dimensions, providing descent query flexibility by putting a small amount of overhead on the client process in order to perform final aggregation.?? These views can be calculated in real-time as source data is ingested, or in frequent batch leveraging map-reduce.,Before we review some design examples, let?€?s first discuss the functionality of the Cassandra primary key. ?? ??,A primary key can be defined on one column much like we did for the raw trades table, or include multiple columns (known as composite key).?????? A composite key is created when the first column is treated as the row key (which is how data is distributed around the cluster), and subsequent columns as column keys (how the data is organized in columnar storage).?? Row keys allow for ultra-fast seeks via equal and in clauses.?? Column keys allow for very efficient range scans of data including equal, greater than, and less than.,In our analytic models we typically choose a lower cardinality dimensional attribute as a row key. This row key will be a common filter in all our queries and should be chosen to give nice, even distribution across the cluster. In general, date or time alone does not make a good row key as it can create hotpots on single servers when all traffic goes to one set of servers during a given period.???? However creating a composite row key of client_id|trade_date would be fine if it suits your query pattern.,We pick attributes for the column key in an order of a ?€?drill path?€? that typically includes date and/or time columns to take advantage of the range scan capabilities.,In the example below we demonstrate a lightly aggregated time-series cube that stores client aggregate trading activity by hour:,??,??,And the following query flexibility can be achieved:,??,??,You can even bypass the row key. However, this is not recommended unless you are running against a relatively small table.,??,??,Although this cube provides good query flexibility, it would be one of many that represent the same data.???? Here are a few other cubes (and corresponding keys) we might instantiate to improve analytic query coverage:,Cassandra is a great platform for serving a Lambda or any other form of real time analytic architecture.?????? With bullet proof, scalable architecture and SQL-like query language, Cassandra can be the simplest part of a complex architecture.??
 
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. If you haven't received this message in your mailbox, ,.,These profiles are randomly selected among our active and new members. To be selected, you need to have an profile with short bio. It helps to post a blog, or share / comment contributions from other members.

The answer is data, according to Greta Roberts, chief executive officer and co-founder of ,.,??,Roberts says data can illustrate and even predict employee attrition, or ?€?employee churn?€?, and can save your business enormously in the long run.,?€?Employee churn is actually extraordinary expensive,?€? Roberts tells??,??via phone from Boston. ?€?When you actually graph it out and look at it analytically, you see the massive amount spent on on-boarding, finding staff, training them and a lot of a manager?€?s time.?€?,Roberts?€? team at Talent Analytics has collected data from a wide range of companies across the globe and they?€?ve shown ?€? much like customers ?€? it is much cheaper to retain an employee than it is to find a new one.,A 2008??,??showed Australian employers were losing $20 billion per year on staff turnover.,Roberts tells??,??how basic data analysis can bring employee churn to a grinding halt in five easy steps.,?€?Do a lot of work up front and think, ?€?what does this person need to do when they get here??€??€? says Roberts. ?€?Not whether I like them, not what university they?€?re from or what degree they have. But what do you need them to??,??€?,Roberts says humans are typically biased and employers tend to think that if they like someone, then they?€?re going to do well, regardless of their skills. ?€?Liking someone doesn?€?t necessarily relate to how they?€?re going to do in the job.?€?,She says employers, especially SMEs, need to make sure everyone in the company is in agreement about what the employee should be doing in the role and avoid giving mixed messages about the role?€?s function and requirements.??,Roberts says employers should start collecting data about employees straight away. She says start and finish dates are important to record, but so are performance quality data and the motivation staff may have for leaving.,?€?As much as possible, try to understand why they?€?re leaving,?€? she says.,Roberts says it is important to give employees the opportunity to say why they?€?re leaving in a comfortable space, so that you can track any emerging trends. She says methods such as surveys will often yield better information than exit interviews because employees don?€?t feel as immediately uncomfortable about leaving.,It is important to quantify an employee?€?s performance as unbiasedly as possible, says Roberts. Whether or not you liked them is irrelevant to how successful they were in the role.,?€?Take an analytics approach. You?€?ve got to compare apples with apples,?€? she says.,Roberts suggests putting all the data into a spread sheet or a database and then graphing out what you find.,?€?Eventually you?€?re going to start to see some trends,?€? she says. ?€?Start collecting what you do have, so even if you don?€?t have it today, a year from now you will.?€?,This empirical evidence will help a company understand its hiring trends separate from its internal culture. Roberts says many companies feel guilty and assume staff turnover is a sign that they are at fault.,?€?A lot of organisations think, if I just give more training, if I just give more mentoring, more coaching, then [the employee] will be better. So the organisation takes on the responsibility for performance.?€?,?€?I think if you get the right person in the job that?€?s enthused about the role, then they will do a lot of their own training, take on a lot more responsibility themselves.,?€?So instead of throwing more training at someone who doesn?€?t have the right mindset for the role, find the person that has the right mindset and your training dollars will go down.?€?,Roberts suggest looking at data role by role. The minimum size to really begin seeing trends is about 40-50 people in the same job.,She says the model needs to focus on the outcome the business most cares about. So, if you want to predict a staff member who will last longer than 12 months, then you must input the amount of time employees stay at the company. If you want to predict people who sell above 100% of their target, then you need to input current net sales performance in the model.,?€?If you find the people that are doing well, try to find out what it is they have, and then try to find more people like that,?€? says Roberts.,She also says it is just as important to look at bottom performers, as well as top performers, to see what sort of person works and what sort of person doesn?€?t work within your company.,Once you have found a model that predicts your best employees, you can then use it to employ the right people. But Roberts says companies often don?€?t know how to apply the model to the recruitment process.,One strategy that Talent Analytics uses is to use questionnaires. If you find top performing staff members have a certain type of mindset, ask job candidates to fill in a questionnaire measuring how they stack up against the desired mindset.,Roberts says that while data can point to many employee trends, it will always rely on an employer?€?s expert opinion to anticipate all the different variables involved in hiring staff.,?€?Some people may get a little afraid and think the robots and the models are taking over, but it?€?s not the case. You need an expert opinion to say, ?€?well that?€?s what the model says but we?€?re going to go against it?€?.?€?,?€?What?€?s cool is that once you have [the model] set up, then you can really watch to see how the model performs versus how the expert opinion performs.?€?
Starred articles are new additions, posted in the last three days.,: The full version is always published the following Monday (,).??
Very short time periods (6 months) with several crashes, as well as long time periods (3 years) with no crashes are expected. An even distribution of plane crashes is indeed NOT expected - it would look very suspicious, and definitely not random. Here we assume that all events (plane crashes) are independent. We also assume that the average is two major plane crashes per year - which is realistic if you include all passenger airlines flying anywhere in the world - sometimes in dangerous weather, or above dangerous locales.,We did our own simulation ,. The password for our spreadsheet will be published in our ,. If you don't receive our digest in your mailbox this Monday, check out your promotion, social network or spam box in your email client (look for an email with subject line ,). We simulated 10 time series with on average 2 crashes per year: each time series represents 10 years of simulated observations. That is, 20 data points = 10 years x 2 crashes per year on average, for each of the 10 time series, with each data point representing a crash event (with time stamp simulated using the RAND function in Excel - for random number simulation).,We performed the Monte-Carlo simulations for you, but now we invite you to solve the problem using mathematical models. There is indeed an exact solution, easy to compute, for this problem. Let us know if your theoretical solution yields similar results. Here's how to proceed:,Even if there are many crashes in a particular small time period, it does not mean that the likehood for a new crash in the next month is reduced, or increased. We are dealing here with memory-less processes.


Inbound and content marketing are not going anywhere anytime soon. The content marketing association reports , of both enterprise B2B and B2C companies are using the tactic. There are a million different ways to leverage content strategy, and here at TechnologyAdvice, we?€?ve experimented with plenty of them. It?€?s been a fun, albeit, educational experience to say the least. While some of our methods fell flat, others helped us take our business to the next level. In retrospect, I think the most important part of our journey so far has been figuring out how to effectively measure and judge each individual strategy?€?s impact. One of the coolest parts about digital marketing, in my opinion, is just how granular one can get with the analytics surrounding their efforts. With that said, I?€?d like to talk about two methods that we have found particularly helpful in our data collection.,??,??,This is a tactic that took us a while to implement, and I wish we?€?d started using it from day one. For every piece of internal content our digital marketers share, they must now create a unique URL. These URLs look something like ?€?,.?€? Here?€?s what that means:,??,??,If somebody enters our site through this URL, all of the text included after the ?€??tid=?€? parameter will accompany any lead capture forms they submit on our website. It?€?s a pretty simple system that you can expand to include almost any relevant metric, including article titles, affiliate information, or product names. Our system allows us to see the effectiveness of each marketing channel, as well as the effectiveness of each individual team member. We can also dive deeper into the analytics, and see which types of content (using the category tags) performed best on which social networks (using the network tags). Same thing goes for paid vs. organic posts.,??,As you can see, this type of information can help you prioritize the networks with the highest ROI, identify which team members regularly draw in the most visitors or leads, and give you a benchmark for how effective a given network?€?s paid advertisements are. While most social platforms provide analytics covering the number of ?€?clicks?€? or ?€?impressions?€?, I?€?m guessing the number you actually care about is conversions. Tracking which social network visitors arrived from in the URL allows you to see which ones ultimated converted, and which network you need to invest more resources in.,??,??,A tag management system is another great tool I wish we?€?d implemented earlier. Through a tag management system, every visitor to TechnologyAdvice.com is assigned a unique User ID (UID) on their first visit, which continues with them if they return later (there are a few browser constraints here, as the system uses cookies). Their UID can also be captured by a web analytics tool such as Google Analytics, which gives us the ability to drill down into more specific data associated with that UID, such as which pages they viewed, how long they viewed them, where they entered our site, and where they exited. If that user ends their visit by submitting their information on our site (signing in to leave a comment, downloading a PDF, or requesting more information on a product), we?€?ve now got great insight into what prefaced their form submission. What?€?s more, it?€?s possible to transfer all of this data automatically into most ,, which can provide valuable context for your sales team once they reach out to this person.,??,Using UIDs and tracking IDs, we can see which social platforms or forums a user came from, see if he was retargeted, and track how long it took him to visit the site again, and eventually request information. Knowing all of the touch points involved in generating an inbound lead helps us prioritize our content efforts, and set realistic goals. For instance, if the typical path involves a user visiting multiple times and consuming over two pieces of content before request any services, then we know that it?€?s unrealistic to expect immediate conversion for articles on new software categories., Since we?€?ve started using these two tactics, we?€?ve seen a noticeable impact on our key performance metrics, and much greater return on our marketing efforts. With a dedicated, data-driven approach to content marketing, there?€?s no reason your company can?€?t realize such returns as well. Start tracking the performance of your content, measure the effectiveness of your team, and and prioritize your resources for the highest conversion rate.
Being the son of a mechanic, I have spent many years handling power tools. I'm especially fond of a couple of hammer-drills in my possession. They can effortlessly drill holes through concrete. At least, this is what my father once claimed. He handed down his most treasured tools to me. I'm big on pliers and screwdrivers. This might be due to my vocational training as a technician. Even today - long after I completed my diploma and continued to further my education - I still carry a licence allowing me to handle gas appliances in Ontario under 400,000 BTUH. While I received formal training on the use of tools, my ensuing social perspective on tools is probably a bit unique. It opens up a bigger discussion on the subject of "instrumentalism in data" or "data instrumentalism," as I am more likely to say. Instrumentalism is something that can both enable and disable. It can, for instance, help improve how a person shovels coal by focusing on key aspects of production at a particular level - that is, the most apparent. But these days, some might regard the use of a shovel to physically extract coal inherently unproductive. Heavy machinery can probably do the job better and faster. Data instrumentalism can cause an organization to generate metrics fitting its preconceptions of progress; this is irrespective of the underlying reality. In this blog, I will consider the possibility that instrumentalism has become pervasive because it is "structural" rather than merely ideological. The objectively verifiable constructs and technologies supporting a data system can bring about instrumentalism perhaps due to lack of awareness of adverse consequences; but it can also be the result of underdevelopment and poor design.,I would argue that the basic idea behind a tool is as follows: a tool causes the diverse attributes of a person to be converted into specific actions intended to achieve particular outcomes from the tool. It is irrelevant, for example, if the person handling a screwdriver has no education or several university degrees. A wrench works if the user can apply the correct action: it hardly matters if the person is a great piano player, a firefighter, or mechanic. Of course, some people might be able to perform the required actions better than others. Similarly in relation to a workplace - given our ability to rapidly replace workers - it seems that the attributes of a person have become less important than the extent to which he or she can generate conforming behaviours. The organizational structure represents a sophisticated type of tool: we use it and become used by it. The more complex and integrated an organization must be to fulfill its design, the greater the extent to which employees must work within tight parameters. Everybody performs their duties in specific ways and often at a particular pace to achieve the intended outcomes. In this environment, we can say that people make use of tools; but also, the tools make use of people. The types of tools that people are given or are compelled to use to some extent dictate their behaviours.,Consider the basic attributes of tool conversion as it relates to data in the following: an agent is asked to perform specific tasks and only these tasks to obtain particular outcomes and no other outcomes; data is gathered but only the data pertaining to those specific tasks and outcomes. I found that a bit tedious to write, but I hope readers catch the gist. At no point am I saying that data-collection is unimportant. It is extremely important. However, the nature of data is shaped by the desired outcomes and objectives. A tool can best perform what it was designed to do. The highly constrained nature of data extends from its instrumental use. The user reduces the proxy such that it represents the most instrumental aspects of the underlying phenomena; his or her actions lead to contextually constrained data. The user as an agent of the environment exhibits conforming behaviours. Data becomes an instrument of conformance to accomplish the predefined tasks of an organization.,Somebody or something must "become" a tool in order to use a tool to achieve "specific" outcomes (extreme emphasis in quotes). There would still be outcomes if resources were not treated like tools, but these might not be those "specific" outcomes originally sought. In light of this principle, the size and complexity of an organization can lead to high levels of instrumentalism. As an organization grows in complexity - spatially, functionally, and structurally - there is increased risk of disagreement among and between the individual components. Thus, instrumentalism is about defining how things operate and imposing rigid controls; this can result in adherent functionality at the cost of autonomy. Many years ago, clocks and watches were amazing specimens of design containing many moving parts working together. Each part had a role to play in the overall process. We might conflate a part with the role that it plays. The role is so rigid that it could be played by nothing else but the intended part. Of course, any part could be changed. But not the role that it plays. So in relation to tools, it is necessary to bring about conformity in ourselves to make use of a tool as intended. The act of conforming is the result of ourselves becoming tools, that we might be used as intended. The data that we gather becomes part of this process of "projection" as I will explain further.,I will provide a conceptual explanation followed by a simulation. The general concept is as follows: as people perform a greater number of tasks together as parts of an organization, it is necessary to apply additional controls to ensure that these behaviours lead to the intended productive outcomes. Expressed a bit differently, as the demand increases for specific productive outcomes, people have less opportunity to exhibit autonomous behaviours; moreover, the more control they must give to the production environment to participate as a collective. Personal autonomy tends to decline as production demands increase.,I don't know how many readers have had the opportunity to work at an assembly or production line. I have never been opposed to doing this sort of work myself. It requires considerable self-control and intellectual peace - a willingness to accept the guidance of repetitive external forces. I used to load skids by taking boxes off a conveyor belt. A line travels at a particular pace. The inability of any person to maintain the pace quickly becomes apparent to other workers. The speed of the line regulates behaviours - or at least it necessitates certain types of behaviours. But the underlying control - for it determines the speed of the line - is the production target. Here then is how data-collection can control people. Does it seem like instrumentalism? A worker might be reduced to a few specific behaviours to achieve particular goals. I recall a coworker at the production line who was also a professional artist. He explained how he had successfully sold a number of his paintings at art exhibits. So when one of his hands fell deep between a couple of conveyor belt rollers - stripping off the skin - the accident meant more to him than simply being unable to pick up boxes. However, he did not appear on the data except in the metrics of production - its behavioral criteria and targets.,I was wondering how I might substantiate productive instrumentalism given lack of organizational data. I decided to provide a rather mathematical explanation. Below are some plumes produced by "nomads": a nomad is a sprite designed to walk across a graphics array in both images using random y-axis shifts. For the plume on the left, the nomads walked from left to right, creating a relatively random-looking distribution. In the case of the right plume, the nomads started from the right at a particular point or area, and I programmed them to start walking left. Another way to look at the right image is as follows: imagine many of workers starting at the left demonstrating all sorts of behaviours in order to achieve specific production outcomes on the right.,I realize the simulation is rather simplistic. Perhaps some readers might accept my simple assertion at face value: people naturally exhibit all sorts of behaviours, but systemic controls must be in place for them to achieve "specific" outcomes. A specific outcome is an outcome that cannot reliably happen by chance. The more complex the required outcome - for example to produce an automobile - the more risks associated with leaving behaviours to chance. Random behaviours are unlikely to lead to deliberate outcomes. It can still happen perhaps by accident, of course. However, productive complexity necessitates sophisticated and rigid operational controls. I therefore argue that production is inherently instrumental. As companies grow in size, make use of computers, and engage their competitors, they have to incorporate into their structural capital many processes for the sake of efficiency. This gives rise to an environment where workers have little personal autonomy. Similarly, in a society handling all sorts of complex and competing interests, there would tend to be more laws and enforcement of laws: for neither laws nor enforcement would exist if similar outcomes could be achieved through happenstance and autonomy.,I ran the nomad simulation a number of times allowing for increasingly more spread at the right indicating greater personal autonomy. Then I compared the level of similarity between the convergent patterns and their non-converging counterparts. "Compliance" indicated below is the percentage similarity. The chart basically shows that as more autonomy is allowed, the level of similarity between the convergent and non-converging plumes increases. Worded differently, if we had to achieve a highly complex convergent pattern, compliance would worsen through reduced autonomy; this gives rise to the need for greater control. If we want people to follow a detailed regiment of behaviours, and assuming production is dependent on adherence, then it is necessary to reduce autonomy.,I know that it seems at least on the illustration that greater compliance is achieved through increased autonomy. This is a bit like saying that everybody is innocent if we have no laws; so in order to reduce crime we should have fewer laws. (I added this just to confuse readers, really.) However, both I and the illustration are saying that as people lose autonomy, they are inclined to break more rules unless there are effective controls. Autonomy is related to the amount of spread at the right-hand side of the plume: a tighter spread is associated with less autonomy. If we want people to achieve specific goals, the spread must tighten. Since this causes compliance to decline, assuming people behave like these nomadic sprites, it would be necessary to apply more control. I'm glad that I can reflect on the code in these situations. By the way, for those interested in the code, a portion of it is attached to the blog (,). Since a random generator is used, the plumes that others might produce will differ. Again, I hope that people take my general contention at face value since the next part of the blog builds upon it.,The main part of this blog pertains to instrumentalism dealing with data perhaps on a more abstract or symbolic level. The idea of "structural capital" relates to how people within an organization can change (through recruitment and attrition); and yet the operations continue as before. There are forces in the background that bring about conformance behaviours. Many of us are familiar with the experience of being trained to perform particular duties. The trainer was trained by others. Yet the head office of an organization located in another state or province does not witness any of these activities. A manager might not be aware of all the day-to-day events taking place in the office. The process of maintaining the operating structure of the organization is based on flows of data. A manager certainly can't recall every detail of every event: this person would refer to records and particularly statistics.,I don't have any managerial responsibilities myself. But I collect lots of data. I am always surprised by how my personal opinions can sometimes conflict with the data that I collect. This has led me to conclude that nobody should rely on gut instincts if real data can be collected. At the same time, I realize that the data gathered and what the data says are determined by design; this is the case in relation to conventional prescriptive metrics. I believe that at some point, it should be unlawful for adverse decisions regarding employees to be made on the absence of reliable data. Given the persistence of data and its relevance to the lives of people, stakeholders should be able to review and question the facts leading up to decisions in the determination of compensation and on matters of employment history. I will cover this on some other blog.,When data is collected, not just any data will do. "Josh parked a foot from the fence," is certainly data although it doesn't seem all that business related. Criteria will be set to determine whether or not particular events represent conforming data. Therefore, the "number of mistakes made" will tend to receive more attention than "brand of jam" in Josh's lunch. I am not writing about any Josh that I might be working with, by the way. The jam and mistakes are both present in the workplace. The choice of criteria determines the ontological relevance of events, giving rise to data mostly about mistakes rather than jam. (Josh of course might be slightly allergic to the jam causing him to make more mistakes.) This is the general concept of the "metrics of criteria": data is emerges from the application of criteria - but mostly certain types of data. The data indicates compliance: it can be used to ensure conformity and identify apparent deviations. I have described this process as "projection," a type of disablement. I think that many people would also recognize how this is also a process of "alienation" leading to distance between data and its underlying phenomena. An organization brings into its substantive discourse only certain aspects of the environment.,I find it tempting to approach the point of instrumental data from a purely polemic standpoint. However, in this blog my focus is really on the structural contributors and determinants. I said that in an organization, workers change all the time, but the structural capital ensures the continuation of productive behaviours. I also said that the process of conformance and control is driven by the metrics of criteria. The suggestion that projection can occur "structurally" implies that it should be possible to find persistent articles or artifacts in the production setting giving rise to compliance.,Classification of Data: The meaning of data in a particular context tends to be constrained. I believe that many people would regard the term "pigeon-holing" negatively. It just doesn't seem like a pleasant thing to do to anyone or with anything. Nonetheless, the idea of having fixed slots to hold predefined data types seems pervasive. Some organizations store their data on relational databases where pigeon-holing is common. For instance, a database might contain slots to hold sales data. If we discover over the course of routine data-collection that some clients were "troubled" or "anxious," depending on the structure of the database at the time these facts might have to be discarded. Therefore in certain database environments, the failure to account for everything during design can contribute to exclusion later. This seems ironic given that the whole point of gathering data is to learn about matters not already known. I recall once handling calls from customers about their appliances. I was required to select the "nature of the call" from a lengthy drop-down menu. I consider the approach rather ineffective: it can contribute to losses and liability. (Do I have any data-oriented solutions in mind? Yes, although I won't introduce it on this blog on data instrumentalism. I'll save it for a future blog.),Amount of Data: The amount of data tends to be limited. By "amount," I mean the aspects (i.e. columns) rather than number of events (i.e. rows). This is a bit like the idea of slots (limited room for expression) except that the structural constraint is more fundamental (inadequate room for expansion). At some point, it might be necessary to add information about something not anticipated during the design of the database; or to add to a certain aspect of something anticipated but never fully rationalized; or where the circumstances have since changed. For instance, a database could be designed to record information about bedding replacement: e.g. "Changed bedding? Please check yes or no." However, a mitigation program might require that bedding be periodically tested and specially treated; this creates a need to hold data beyond what the system was originally designed to handle. I'm not saying that it's impossible to get around the problem perhaps using bulk migration or by linking references to other files. I merely underline the overhead involved in accommodating new data. After expanding the database, there might be compatibility issues.,Contextual Relevance of Data: Data generally lacks contextual information. The data held by an accounting department is easy to distinguish from the data held by shipping. The "separation" of organizational functions tends to maintain a contextual separation of the data. However, if an organization tried to preserve and make active use of contexts, this would open up the deep issue of what it means for data to be contextualized. This brings me to my colourful saying that data is often "headless"; it can be found in this horrific state, floating or wandering aimlessly in the data system. I believe that a sophisticated data system should be designed to hold hundreds or even thousands of different contexts associated with particular events. In doing so, an event that seems entirely shipping related can have multifarious organizational impacts in the form of contextual outcomes. If there is a single overriding context, this indicates instrumentalism. The data would tend to convey only those details as per the details expected to be conveyed. I believe that this isn't a purely philosophical or ideological obstacle, but it is also structural.,Instrumental Criteria: On the surface, it might not seem that "sales" is the product of criteria. The act of purchasing a product could generate an enormous number of data events: e.g. desire to do well in school; need to blend in with other students; anticipated physical changes to the human body. (I am of course referring to back-to-school products.) Yet retailers study something of questionable predictive relevance: the revenues collected during sale. It must seem like the collection of revenue is the sale. I am not saying that this measurement is "completely" pointless. It is useful for financial guidance. However, customers might decide to stop buying products the following year; it then becomes apparent how superfluous the data is. It just so happens, the retailer has established certain criteria, and sales figures are generated through the application of these criteria. "I want to know how much money I collected after I sold this to the customer." "I want to know how much money was made during this time period." I consider it healthy for data scientists to examine data in more critical terms. If the data is primarily criteria-driven, it becomes necessary to question the criteria itself. For me, the problem isn't so much that there were faults with the original criteria. Over time, all criteria can become faulty since the environment is unlikely to remain static. The real problem is the inability to adapt criteria to the changing realities.,Poor Conceptualization: Before criteria is set, or perhaps even as an alternative to formal criteria, it is necessary to determine what events "might be" important. I covered this point in a previous blog on the geography of data dealing with transpositionalism. Take for example the idea of following a particular course of treatment. An interesting question might be as follows: is this medication good to take all spring, summer, fall, and winter? I'm not actually suggesting that seasonality is necessarily important; this is merely the sort of question one might ask. The change in seasons is a major event for living organisms occupying different climates; for humans, this influences indoor environmental conditions and the availability of indigenous varieties of food. Is it "good" for a person to take the exact same treatment if he or she is a librarian or a firefighter? Of course, if we refer to the product label, it probably doesn't distinguish between different seasons or occupations; in fact, it probably doesn't alter the recommended dosage even based on weight. Peanut butter seems to be fine for many people to eat; but it can kill a certain segment of the population. Having prescribed normatives of a person creates normatives of solutions and preconceptions pertaining to that individual. But not everybody fits the norm. Similarly, an organization should gather data and make use of data from the reality that it confronts rather than discussing realities faced by other organizations.,Scientific Method: Data can become be instrumental if its collection and recognition (as something materially substantive) is designed to satisfy a methodological normative. My point is therefore not necessarily limited to the Scientific Method. Within the context of this method, there is experimentation, deliberation, debate, reexamination, revision, and often restatement. It is a highly rational process driven by many big wheels. It can take some time to get anything done. If we inject massive amounts of data into this process, the defining nature of the methodology becomes more apparent. Researchers often want to substantiate specific points - so they need clean and therefore highly controlled data. Indeed, researchers want the data to be convincing. In the process of trying to extract a sliver of light from a big reality, data as a proxy can become both pristine and alienated. Noise is screened out. This is not to say that the noise is unimportant, but rather its relevance has been dismissed. So people with an incomplete understanding of the phenomena screen out or never collect data premised apparently on authoritative understanding. Researchers make their point. The wheel turns a little bit. The process repeats. In an environment with many competitors and shrinking markets, one might question the feasibility of such an approach. An organization can go bankrupt long before its environment can be adequately examined for actionable insights.,Insulation of Structural Capital: The previous points dealt with the structures giving rise to instrumental data. For any number of reasons, the policies, practices, and philosophies of an organization might steer the data in particular directions perhaps as a matter of strategic choice in order to accomplish particular outcomes. This might be described as the structural manifestations of ideology. There isn't really a shortage of examples, but I can share one that immediately comes to mind. There is a popular human rights case involving the Toronto Transit Commission and a visually-impaired customer. This customer wanted the TTC to announce all stops. I have been on the TTC during packed hours when the visibility was terrible even for people with no visual impairment. It is certainly a curious business perspective to consider signals and stop-announcements a problem rather than a selling point for a public transit service. The TTC lost in this human rights case. Perhaps the TTC received all sorts of customer feedback to encourage improvement in those days. (Read here "client data.") If the TTC had so much time and money fighting a visually impaired patron in court, one might be reasonably led to question how they addressed "customer feedback" more generally. I haven't used the TTC in many years, and the court cases occurred some time ago; so I can't say if conditions are the same. However, to me this is an example of structural insulation. One would expect a bridge or path, but instead there is a wall.,I recall a science teacher once explaining why cells have limited size: he offered a rather mathematical explanation. There are some structural impediments given that nutrients are taken from the outside: the amount of nutritional intake depends on the surface area, which starts to decline in relation to mass as a cell increases in size. Consequently at some point, geometry prevents the cell from supplying itself with adequate nutrition. While I am not talking about cell dynamics in relation to organizations, I believe that math likewise plays an important role. I suggested earlier, as an organization gets larger, if it hopes to carry out specific tasks involving many parts, it must exert more control over those parts. I said that this is probably relevant in relation to any data collected under a prescriptive regime. I also discussed how this control might structurally manifest itself within an organizational context specifically as it relates to data. I believe that my list is fairly short; there are perhaps many more structural manifestations of data instrumentalism. To me, the crux of the problem is how an organization might be unable to correct itself once it loses direction. Given the insulation of structural capital from the environment, organizational decline seems almost inevitable. I believe this is one of the reasons why organizations sometimes ironically "parachute" outside consultants - presumably to advise internal experts on how to run their own operations better.,An outside consultant might bring to an organization a fresh set of eyes and possibly a perspective more open to different possibilities. However, if the data itself is instrumental, the likelihood of an outside consultant reaching radically different conclusions seems remote. The data is designed to provide guidance. The structure made the data what it is; so the data will tend to say what the structure intended for it to say. As an escape, the data must instead articulate the environment - both the one that an organization occupies and the other that exists within the organization itself. In many discussions surrounding big data, there seems to be a persistent question of what should to include. One reference point is the market - the external environment. But this can't really be the focus. A car manufacture might study the demand for home repair products and yet be unable to expand into this area. The real question is what the organization can reasonably deliver in light of its current circumstances. This requires input from internal systems - the internal environment - as it relates to different potential markets. This problem is solved not just by any data - for instrumental data cannot help - but rather by the metrics of phenomena gathered through environmental articulation. This ensures that any data gathered can help an organization adapt. I call this alternate type of data that comes from the environment "participatory data." This is a different kind of data: more massive, complex, and organizationally engaging than its instrumental counterpart.,In a couple of weeks, I will be assigning some real-life personal health events to a proxy - the closing prices for the Toronto Stock Exchange - using a technique that I call warping. Why in the world would anyone want to warp something? Data becomes surreal and freakish in the dark art. I still recall some sage advice offered during a movie: a Jedi must learn to build his or her own light-saber. My next blog will be about building weapons from data. ??Well, not really. ??But it's as close as I get to the topic.
This blog post is a follow up post to??,Neo4j Cypher is a declarative graph query language that allows for expressive and efficient querying and updating of the graph store. Cypher is a relatively simple but still very powerful language. Very complicated database queries can easily be expressed through Cypher. This allows us to focus on the domain instead of getting lost in database access.,Cypher is designed to be a humane query language, suitable for both developers and operations professionals. Being a declarative language, Cypher focuses on the clarity of expressing what to retrieve from a graph, not on how to retrieve it. The complete blog post is at??,??and covers the following scenarios:
, another apache licensed top-level project that could perform large scale data processing way faster than Hadoop (I am referring to MR1.0 here). It is possible due to Resilient Distributed Datasets concept that is behind this fast data processing. RDD is basically a collection of objects, spraed across a cluster stored in ram or disk, automatically rebuilt on failure. It is purpose is to support higher-level, parallel operations on data as straightforward as possible.,Apache Spark is often referred to as data processing engine. Simply put, Spark is cluster computing engine that made it easy to handle a wide range of workloads: ETL, SQL-like queries, machine learning and streaming. The amount of code you write is also minimized to a great extent compared to traditional mapreduce development. Also, it has been proven to be 10x faster than Apache Mahout.,The Spark engine has four major components.,To install Spark, we need the following in the OS (Mac/Debian):,Visit??, and??, for more information.,Pavan Kumar,Lead blogger,,Data Science Hacks
The proliferation of smartphones, tablets, and other mobile devices ?€? here come the ?€?wearables?€? ?€? has opened up new opportunities for businesses to leverage employee-owned technology for competitive advantage. That being said, the use of such devices in the workplace ,, especially when comprehensive BYOD policies are not implemented and enforced., , 
As humans, we navigate our lives largely by the recognition of patterns. These patterns include the sound of a mother?€?s voice, the appearance of a dangerous animal or poisonous food, the familiarity of kin, and the attraction to potential mates. Accurate pattern recognition is key to an animal?€?s survival and progress, and has allowed humans to become the socially complex and advanced species we are today.??,It should come as no surprise that scientists and engineers have long been fascinated with the mind?€?s ability to rapidly and accurately recognize patterns, and much research has been geared towards attempting to recreate this ability in a machine as a demonstration of Artificial Intelligence (AI). ??One of these abilities is facial recognition; something humans do with relative ease, but which has been exceedingly difficult to mimic using programming logic and advanced algorithms.,In Ray Kurzweil?€?s book??,??it is argued that the human brain contains approximately 300 million ?€?general pattern recognizers?€? arranged in a hierarchical pattern. ??Input signals feed through various layers, and concepts are learned in an increasingly abstract sense until a new pattern is learned or an existing pattern recognized.,The closest parallel to this in machine learning are artificial neural networks (ANNs). ??ANNs use mind-inspired mechanics including simulated neurons and layering to learn concepts from experience (exposure to data). ??ANNs have been applied to a number of problems in pattern discovery including facial recognition but until recently have had limited success in real-world facial recognition systems.,This is now changing with the use of Deep Learning; a set of algorithms in machine learning that attempt to model high-level abstractions using an approach more closely aligned with how our minds recognize patterns. Using a large number of layers, so called Deep Neural Networks (DNNs) can mimic human learning by adjusting the strength of connections between simulated neurons within many layers, just as the human mind is believed to strengthen our understanding of a concept. ??Each layer can model an increasingly abstract concept built from the more basic concepts learned at earlier layers.,Deep Learning is breathing life back into the use of ANNs and some researchers consider the use of deep learning neural nets to be a small revolution in AI. ??What makes deep learning so attractive is its property of getting better by simply throwing more data through its networks. ??More data to a DNN is like more experience to a child. Although these networks have been around for decades, only now do we have the volume and variety of data to expose them to enough information for a DNN?€?s ?€?understanding?€? to rival that of humans.,In science and industry, deep-learning computers are being used to search for potential drug candidates and to predict the functions of proteins. Companies like Google and Facebook are using their massive data stores of images and text to??,. A small start-up in Las Vegas called NameTag has developed a beta version Facial Recognition App for Google Glass. The App uses the??,database to search through millions of photos, learn what features define a human face, and match them to facial features detected from the App. ??The exact use of this in??,??is still unknown and Google has yet to approve the use of facial recognition Apps for Google Glass.,acebook is moving forward with a facial recognition project called DeepFace. ??DeepFace can detect whether two faces in different photos are of the same person and is reporting accuracies that rival a human?€?s ability to do the same. ??DeepFace uses DNNs to model highly complex data and multiple features. ??What makes DeepFace so exciting is its ability to detect faces regardless of lighting or camera angle; two factors that have stumped most facial recognition algorithms to date. This could lead to new photo tagging applications and authentication technologies. DeepFace uses a 3-D modeling technique to rotate a single flat image in 3-dimensions, thereby allowing the algorithms to ?€?see?€? different angles of the face. ??Using a DNN with over 100 million connections, and its database of millions of photos, DeepFace teases out the features that can be used to recognize human faces, and uses this knowledge to discover very high-level similarities between 2 photos of the same person. Facebook has reached an accuracy of 97.35% on the so-called Labelled Faces in the Wild (LFW) dataset, and has reduced the error of the current state-of-the-art by more than 27%, approaching human-level performance.,The advantage of DNNs over other learning approaches such as Support Vector Machines and Linear Discriminant Analysis (LDA) is its ability to scale to extremely large datasets; something required for problems in speech and facial recognition. This means large, inexpensive clusters of computers are required to process and calculate all the data and make learning using DNNs feasible. ??In the now famous??,, Google scientists used 16,000 processors and the internet as a data source to recognize the appearance of a cat. ??What makes the experiment so game-changing is that the algorithms were ever told what a cat is or looks like. The DNN came up with the concept of a cat all by itself.,Many researchers and start-ups will be jumping onto the DNN bandwagon to see where we can push machine learning. Any application requiring the most important features to be detected among myriad variables is a potential candidate for DNNs. Neural networks have two big advantages; they deal well with many ill-defined features, and they scale efficiently, meaning extremely large datasets can be used. This is the latest approach to machine learning and the true point to Big Data; that throwing more data at the problem bypasses the problems associated with traditional statistical analyses on small datasets. Although the somewhat blackbox nature of DNNs take us further from understanding true causality, they do offer the exciting opportunity to mimic human intelligence and automate more sophisticated tasks. More abstract areas like psychology may soon see benefits in the application of DNNs in both targeted marketing and in helping us understand how the human mind works.,With the application of DNNs to facial recognition, a new era of AI is being ushered in and will lead to new and exciting technologies. It is already raising, however, some ethical concerns surrounding privacy. ??The use and acceptance of such technology will bear out over the next few years.
Data Science is often brought to companies as a potential game changer. An investment that may pay off if the company's data can be leveraged to provide insight and gain a competitive edge. But bringing analytical offerings to organizations as a "maybe solution" to their pain points misses the mark. Data science is today's answer to our most pressing enterprise and socially innovative challenges given the data-driven nature of our markets and society as a whole. If an investment in data science does not pay off for an organization it is rarely a symptom of the offering, but rather a symptom of an organization's lack of data culture required to make 'competing by the numbers' a reality.,Culture gets to the heart of available data, legacy infrastructure, red-tape prerequisites for approvals, company vision, and the willingness to adopt proposed solutions. All the potential in the world will not deliver value if the organizational culture is not aligned with feeding data-driven solutions directly into the company's value proposition; the promise made that what gets delivered will be granted and experienced by the customer.,Whether they know it or not, most organizations are rapidly moving into the big data market where their technically savvy consumers are using commoditized solutions to find value. This means that the consumer in B2C is becoming less reliant on businesses, and that the receiving business in B2B is using cheap, self-service solutions that do not require outside organizations.,Remaining competitive means finding a way to still offer value to individuals and businesses in a world becoming more self-driven and technology savvy. Amazon is not competitive because of the great books and other items it offers. It is competitive because it uses data in a way that brings value to the online consumer experience. It takes advantage of today's current consumer habits and massive transaction data to deliver value to its customers.,To still offer value to savvy clients and be competitive in today's market companies need to leverage data intelligently using approaches that compliment their domain expertise.,The ultimate bottleneck to any data science solution will be the culture shift required to make competing analytically a real way forward. This shift involves reevaluating what information is currently collected, how it is disseminated and used, how it compliments employee training and organizational standards, and most importantly how it lives inside products and services directly impacting the organization's value proposition. Each of the above should be inline with using data science effectively and each piece is ultimately dependent on the company culture being willing and ready to move forward.,To shift the culture in this direction requires a healthy dose of self-awareness on the part of the organization, and the ability of today's best data scientists to educate CXOs on how to make this shift. It involves identifying the individuals, processes, and antiquated belief systems that are toxic to the alignment discussed above. It involves eliminating outdated business 'intelligence' approaches, targeting processes that have more gloss than substance, replacing yesterday's technology with relevant stacks, building out proof-of-concepts for products that use machine learning as their workhorse, and defining a culture that iterates on these steps in an ongoing value-producing fashion.,The question is not about whether or not to use data science; we are living in the world of big data and those unable to adopt these approaches will be rapidly left behind as numerous studies show. The question is which companies can be taught to shift their people, processes and vision to be aligned with driving data science into their value proposition so that it is relevant and competitive. This is the first and most important task to seeing data science successfully deployed and used in an organization. Shift the culture, and the unparalleled advantages to utilizing data science will naturally follow.
The official title of this free book available in PDF format is ,. But it's more about elements of machine learning, with a strong emphasis on classic statistical modeling, and rather theoretical - maybe something like a rather comprehensive, theoretical foundations (or handbook) of statistical science. Anyway, very interesting, and it's free. See table of content screenshot below.??,??(119 pages).,The chapters 17 to 28 (the most interesting ones in my opinion) seem like a work in progress - I'm sure the authors intend to make them a bit bigger. For a more modern and applied book, get ,.,And here's the detailed table of content:, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, . . . . . . . . 17, . . . . . . . . . . . . . . . . . . . . . . . . . 25, . . . . . . . . . . . . . . . . . . . . . . . . 31, . . . . . . . . . . . . . . . . . . . . . . 39, . . . . . . . . . . . . . . . . . . . . . . . . 41, . . . . . . . . . . . . . . . . . . . . . . . 45, . . . . . . . . . . . . . . . . . . . . . . . 51, . . . . . 55, . . . . . 59, . . . . . . . . . . . . . . . . . . . . . . 69, . . . . . . . . . . . . . . . . . . . . . 77, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79, . . . . . . . . . . . . . . . . . . . . . . . 87, . . . . . . . . . . . . 89, . . . . . . . . . . . . . . . . . . . . 91, . . . . . . . . . . . . . . . . . . . . . . . 93, . . . . . . . . . . . . . . . . . . . . . . . . . . . 95, . . . . . . . 97, . . . . . . . . . . . . . . . . . . . . . 99, . . . . . . . . . . . . . . . . 101, . . . . . . . . . . . . . . . . . . . . 103, . . . . . . . . . . . . . . . . . . . . . . . 105, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107, . . . . . . . . 109, . . . . 111, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113, . . . . . . . . . . . . . . . . . . . . 115,Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
There has been a number of interesting articles recently, discussing the skills a data scientist should or might have. The one entitled??The ,??is a popular one (see 22 skills listed below, or click on the link to read the full article). Earlier this morning, I read another one on LinkedIn:??,. The picture below comes from this LinkedIn article. Some of these articles have been posted on our network, by external bloggers, for instance, ,??or??,. Popular ones include??,??and??,I tend to have some level of disagreement with many of these authors. My disagreement can be summarized as follows:,??,Would you add or remove some to this great list created by ,? First, I'd categorize these skills. Then, I certainly would add business acumen, domain expertise, hacking skills, presentation and listening skills, good judgment, not trusting models, ability to work in a team or with clients, all sorts of databases and file management systems, some data engineering, some data architecture and dashboard design, data detection, real time analytics, data vendor expert (vendor selection, benchmarking), be the metric expert in your company (even decide which metrics to track, how to collect the data).??
We've given Hadoop almost 10 years to mature, invested billions, and very few companies are?? seeing the return on investment.?? Several companies have tried to make Hadoop a real-time analytical platform, incorporating SQL-like facades on top, but the latency is still not where it needs to be for interactive applications.?? Even Google, a true big data user, has moved on and is using more dataflow / flow-based programming approaches.?????? Why??? It just makes sense...,Instead of saving all data into an HDFS, and then trying to run full parallel table scans to reduce the data, alternative systems can answer the same questions while the data is in motion.?? We live in an extremely chaotic data environment.?? Data is everywhere, exists in many different formats, and comes at us from many different directions.?? It's unforseeable that we'll be able to duplicate all of this data into a single HDFS. ??, , By defining your processing scheme as functional blocks with inputs and outputs, and then describing dependencies between the functional blocks as dataflow connections, a platform can systematically parallelize the execution of the defined processing scheme.?? As data is flowing, it can be filtered, routed, aggregated and disseminated to other systems.?? This results in a system that is making decisions and getting answers out of its data in real-time. ??, , It's platforms like ,??which are going to be the big players in this next big data wave.?? Flow-based programming approaches provide agile and fluid mechanics for processing data.?? ??, , Data got you overwhelmed??? Go with the flow!
k
This was the subject of a ,??posted on Oracle's blog, two days ago. It certainly shows how far from the reality some big companies are. They confuse people who call themselves data scientists (or get assigned that job title), with those who are true data scientists, and might use a different job title. Many times, the issue is internal politics that create the confusion, and not recognizing a real data scientist with success stories to share, or not leveraging them., from ,??and ,. You can add yours in the comment section below.
Healthcare providers have made major breakthroughs over the last two decades by creating and implementing increasingly sophisticated disease management programs (DMPs).?? At their core there are always two motives, improve the human condition by preventing lesser symptomologies from becoming more severe diseases, and secondly to reduce total systems cost by using less expensive DMPs to prevent or delay the onset of a disease more costly to treat.,In the last few years, the most advanced and analytic of healthcare providers have discovered that DMPs themselves can be optimized using the rapidly spreading techniques of predictive modeling.?? This is a hypothetical example of how a DMP can be optimized in this way.,Optimizing a DMP means recognizing that not all the members of the targeted pre-disease group will respond equally to the DMP.?? Does it make sense to provide the DMP and incur the investment of the program for individuals with provable very low probability of responding positively??? Probably not.?? Were we able to know in advance that specific individuals were very unlikely to respond, it is likely we would elect to not provide the DMP but perhaps provide an alternate course of action.??,Optimization can be calculated against many standards.?? For example, creating the greatest number of cases in which the severe disease does not manifest means providing a very large array of interventions including the DMP and incurring very high cost.?? Most providers recognize that this is simply not possible since funds are not unlimited.?? In this example we will illustrate a simple financial optimization but the cutoff or breakeven point can be adjusted by the healthcare provider management to meet any combination of financial and ethical criteria desired.,In our example, based loosely on a pre-diabetic DMP designed to prevent or defer the onset of full diabetes we will start with the following assumptions:,Using predictive modeling techniques applied to member data already on hand including diagnostic, treatment, and pharmacy codes plus demographics, we create a model (an algorithm) that calculates a score for all members of the population based on their likelihood to respond positively to the DMP.?? This technique is widely used in many industries and is based on a known sample of members who both did and did not respond to the DMP.?? The predictive modeling tools, sometimes called machine-learning tools, are capable of detecting even very weak correlations among large numbers of variables and produce the scoring algorithm that is then applied to previously unseen member data to predict who in the future is most likely to respond.,The accuracy of these models can be determined during development and it is typical for accuracy to be in the 70% to 90% range, sometimes higher.?? However, even models that are less accurate in aggregate can be quite accurate at the margins and therefore useful.?? Your professional predictive modeler will guide you in interpreting the implications.,In our example, we use an actual scoring model that scored slightly over 79% accuracy.?? That is, in about 8 cases out of 10 the model would correctly categorize members who would respond to the DMP versus those who would not.?? The technique then calls for us to score all members and rank the scores from high to low.?? After having done this, we will look at the cost versus savings for each decile of the sample (a decile is simply 1/10th of the sample).?? A typical analysis using the 79% model would look like this. ,Since we have sorted the sample by score before dividing it into the 10 deciles, the first decile will contain the highest scoring members who are predicted to be most likely to respond.?? Conversely in the 10, decile are all members scoring lowest and therefore predicted to be least likely to respond.?? This allows us to concentrate responders together so that in the first decile, without scoring we would have expected 10% of responders (4,000 would be expected and would be the same in each decile without scoring) but now the predictive model has allowed us to concentrate almost twice that many (18.4%) into just the first decile. Conversely, the 10, decile now contains less than 1% of those who respond positively.,Comparing the savings for each decile (savings from responders less the cost of the DMP) allows us to calculate savings by decile.?? Using only those deciles where the savings is positive turns this program which was previously breakeven into one which now saves a net $4.3 Million and may be more responsible in allocating scarce resources to those mostly likely to respond.,Note that there is a medical cost savings of $228 for each of the 7,351 responder in the first decile.?? There is an additional medical cost incurred of $4,920 for each of the 369 responders in the 10, decile when both the cost of the DMP and treatment cost are considered.,Should the 6, or 7, decile members continue to receive the DMP even though it is a money losing proposition??? This is open to interpretation by the healthcare provider's management, but now you have quantifiable data on which to proceed.
There are lies, damn lies, and Amazon reviews. Why are so many Amazon or Yelp reviews bogus? Do they have bad data scientists who can't detect fraudulent reviews? No, they have unethical CEOs ready to do anything to make money short-term. And complaining about being unable to find real data scientists to solve their problems. This is a challenge for ethical data scientists who want to create value, but get punished by top management for not condoning their misdeeds.,In the case of Amazon, most items found on its website have excellent (but too many times, bogus) reviews, because they make money selling them - a big conflict of interest. In the case of Yelp, if your business is listed but you don't purchase advertising with them, your business will only get (bogus) bad reviews. Both cases are worth a class action lawsuit. Amazon will also generate (bogus) bad reviews (easy to detect - see below) against authors who do not comply with their ridiculous policies. As a data scientist, is it worth working for such companies?,In the case of Amazon, bad reviews can also arise because of publisher wars - Wiley against Elsevier or O'Reilly - or because of disruptive content, like proving that SQRT(2) is an irrational number. The author of the proof, ??,was murdered 2,000 years ago for stating and proving this fact; interestingly, an algorithm to generate all digits of this number ,. Even today, things haven't changed.,Anyway, here are two of these bogus reviews that any working brain could easily detect - no need for advanced data science algorithms! ,??has promised to add , as a project in our??,.,Note how short these reviews are, providing no explanations, no facts.??,I think it is time to build a new Yelp or ignore both Amazon and Yelp reviews. These companies have many other problems, see e.g.
Most of you will read this article to discover the most popular blogs, but the real purpose here is to show what goes wrong with many data science projects as simple as this one, and how it can easily be fixed. In the process, we created a new popularity score, much more robust than any ranking used in similar articles (top bloggers, popular books, best websites etc.) This scoring, based on a decay function, could be incorporated in recommendation engines.,The data covers almost three years worth of DSC (Data Science Central) traffic: more than 50,000 posts, and more than 6 million pageviews (almost half of it in 2014 alone), across our channels: ,, ,, ,, and ,??(but not ,).??,I actually included 46 articles, but only the top 30 have a highly accurate rank. Some articles have been filtered out as they belong to a cluster of similar articles (education, books, etc.) Finally some very popular pages (in the top 5) are not included because the creation date is not available (the ,, ,, and Jobs tabs at the top of any page), or because they just should not be listed (my own profile page, the sign-up page, the front page, etc.),The new scoring model is described below, in the??,??section below. Also, you will find useful comments in the??,??section below.??,The number in parentheses represents the rank if instead of using our popularity score, we had used standard methodology. The date represents when the blog was created. By just looking at these fields, you might be able to guess what our new scoring engine is about. If not, explanations are provided in a section below.,These top pages represent 21% of our traffic. The front page amounts to another 9%. And the top pages that are filtered out (for various reasons, read introduction) to a few more percents. Here are some of the highlights:,Let's say that you measure the pageview count for a specific article, and your data frame goes from t = t_0 to t = t_1. Models like this typically involve exponential decay of rate ,, meaning that at time t, the pageview count velocity is f(,, t) = exp(-,t). With this model, the theoretical number of pageviews between t_0 and t_1 is simply,P = g(,, t_1) - g(,, t_0),,where g(,, t) = {1 - exp(-,t)} / ,.,If t_0 is set to zero, then??,To adjust for time bias, define the , as S = PV / P, where PV is the observed page view count during the time period in question. When r = 0 (no noticeable decay, which is the case here) and t_0 = 0,??then P = t. Note that the only two metrics required to compute the popularity score S, for a specific article, are: time elapsed since creation date, and pageview count during the time frame in question, according to Google Analytics, after aggregating identical pages with different URL query strings.,Using the basic model with , = 0 (in section 4) makes a big difference with traditional rankings, as you can see when comparing our rankings to traditional rankings, in our list of top articles in section 2 (sorted according to our popularity score with , = 0). It allows you to detect ??trends about what is getting popular over time.,This is what makes the difference between good and bad data science. Note that refining the model, estimating a different , for each article, testing the exponential decay assumption, and adjusting for growth, is also bad data science: it makes the time spent on this project prohibitive, make your model subject to over-fitting, and may jeopardize the value (ROI) of the project.,Data science is not about perfectionism, but about delivering on time and within budget. In this case, if I spend one month on this project (or outsource to people who work with me), it's time wasted on something that could yield far more value than the little incremental gain obtained by seeking perfection. Yet ignoring the decay is equally bad, it makes this whole project worthless. The data scientist must instinctively find what level of perfection is needed, in his models. Data is always imperfect anyway.,One interesting project is to group pages by categories and aggregate popularity scores. Maybe create popularity scores for categories. Indeed,??Nikita Nikitinsky has been working on this problem, indirectly. It was his project during his data science apprenticeship (DSA): we will soon publish the results and announce his successful completion of the ,??(see applied project #3).??He is the first candidate to complete the DSA, besides our intern Livan (who worked on a number of projects including our ,), and the winner of the ,. ??,Other potential improvements include:,Another area of research is to understand why webpage pageview counts??,.,The articles below come with detailed explanations about the (sound) methodology used to obtain the results.
This rudimentary statistics textbook, entitled , (3rd Edition), sells on Amazon for $157.79. Not sure if everyone sees the same price as me (maybe prices are user-customized), if price changes over time, but it seems stable. Below is a screenshot.,Surprisingly, this book is meant for first-year college students, so there's nothing original in the book. Just basic standard stuff that anyone can find for free on the Internet. Books covering far deeper and broader material, at the research level, filled with new intellectual property that you can directly apply to new-world data, ,.,What justifies such a high price? And why would someone - necessarily a student just starting college - spend so much money on this classic, three century old material? I mean, even if you are forced by your stats professor to buy this book, you will never purchase it at full price, you'll get a used copy, borrow it from the library, or get a copy from a friend. The fact that there are so few reviews (over a two-year time period) might indicate that they are very few buyers, but who knows?,To make things worse, it's 800-pages thick, but contains so little material that it could be compressed (summarized) in just 10 pages. For instance, the author spends 350 pages introducing material such as sampling, random variables, moments, binomial distribution, probability distributions (though there is no mention of generating functions or any proof of the central limit theorem) before introducing the concept of confidence intervals. It takes another 50 pages to review the concept of confidence intervals, yet another 50 pages to discuss the basics of hypothesis testing, and then yet another 50 pages to discuss group comparisons.,By contrast, if you look at ,, it has the following features:,So what make people,I am absolutely stunned by this level of absurdity. Maybe you have some explanations. I can't find any... Is this book an exception, or is this the general trend with traditional publishers? Or is a publisher bubble on its way, regarding college textbooks?
In order to write a tutorial about classification, it was necessary to find an example that was broad enough that it would need to be sub-divided. Since I actually care about whether you remember this stuff, it needed to be something that a lot of people like and would relate to. And since I have a lot of international subscribers, it needed to be cross-cultural as well. So what is universal, cross-cultural, and dearly loved?,Beer.,There?€?s American beer, Mexican beer, German beer, Belgian beer?€?.hell, even the Japanese make beer. There?€?s IPA, Lager, Pilsner. Dark, light, stout. There are so many ways to classify beer that we could spend weeks??doing it (so naturally, I did).,Now, before you can classify anything you have to determine the characteristics that you?€?re going to use. For beer you could use country of origin, color, alcohol content, type of hops, type of yeast, and calorie count among other things. That way you could sort based on any of those characteristics to judge similarities between the various brews.,And just like that, you?€?ve done??classification. Simple, right?,To take the example further, let?€?s assume that my favorite beer is Sweetwater ?€?Take Two?€? (a pilsner made here in Atlanta) but I?€?m in Santiago, Chile this week for a conference. ??The Chileans are a lovely people, but the management at my hotel doesn?€?t know about the wonderful goodness made by Sweetwater and they don?€?t have it at the lobby bar. I explain my predicament (read: ?€?impending crisis?€?) to the bartender. What would a good bartender do?,If he?€?s been in the business??for any length of time, he?€?s already gone through the classification step for beers but probably didn?€?t realize it. He has them sorted by characteristics in his head. He starts asking me questions about Take Two: ?€?How dark is it??€?, ?€?How ?€?hoppy?€? does it taste??€?, and ?€?How many can you drink before passing out??€?. Based on my answers he knows that what I?€?m describing is basically a golden-blonde pilsner with spicy hops and an earthy tone.,He might??also figure out that I?€?ll need help back to my room at the end of the night because of all this???€?field research?€?.,So now that he has the characteristics of my favorite brew figured out, he compares that against the beers he knows. The ones with the most matching criteria form a ?€?cluster?€? that he can make recommendations (and hopefully free samples) from. My night is saved, and his tip is big. Everyone is happy.,And just like that, you understand clustering.,How does this apply to the business world???There are many potential applications of classification and clustering, but a common one is identifying the characteristics of a company?€?s best customers and then searching a pool of potential customers for ones that meet those characteristics. If your best customers have between 1000-2500 employees, are in the manufacturing and retail verticals, and are located in the New England area of the US, that?€?s good information to know.,What applications can you think of?
Over the years, our firm has had many discussions with employers on the eve of a new talent analytics project. Often, it is the firm?€?s first deep-dive look at employee data. Sometimes we act as a strategic sounding board, and sometimes we can help them move directly forward into predictive analytics. It is always interesting.,This article will discuss two analytics approaches that we have seen. We will describe the value of each approach, and why you might want to begin with one over another. Though we are speaking about employee analytics, the same logic applies to any kind of business analytics.,Basic business awareness leads most firms to the conclusion that (a) employees are their largest expense, and (b) employee behavior is a significant driver of corporate success or failure. Other human behavior domains (customer analytics, patient analytics, voter analytics) continue to demonstrate clear ROI from using predictive analytics, and it is natural for employers to ask how the practice can be applied to employees and job candidates for similar ROI.,This potential often results in a decree to ?€?look for something interesting in the HR data.?€? We often speak with the professionals who have been given this vague task: to explore HR?€?s system of record: hires, terminations, payroll, home information, demographics, performance reviews. Sometimes, the agenda can also include unstructured data, such as internal email, chats, and social media in or outside of the company.,Often the effort is scoped to include a wide swath of employees in diverse roles: all managers, all professional roles, high and low potentials. The objective is almost always a vague ?€?something interesting?€? rather than any specific objective or business outcome.,Inevitably, with enough data, enough time, and an expensive visualization package, , will be found. Let it be noted that you can almost always find something of interest in any dataset. Whether it is actionable or not is another story. For example, analysis of HR data may find that:,The problem is, of course, that these types of projects are fishing in a vast sea of input variables, with no known business outcomes. The problem is that the HR data is essentially meta-data of the employee?€?s life at a company, absent the results of their work.,The interesting, high value data - the work, the performance - is generated on the job, in their department after they?€?ve been hired and on-boarded. HR doesn?€?t typically track this data it is tracked by the Line of Business.,The real business outcomes, the KPIs, the reasons the business hired the employee, are lived out and documented at each line-of-business for each role: in the sales department, or in the call center, or in the bank branches, or in software development.,The problem with the ?€?fishing expedition?€? type of analytics effort is that it wasn?€?t framed correctly to begin with. An effective talent analytics effort simply cannot span , lines of business, across , job description. It needs to be focused and framed.,Without measurement of actual business performance from the line-of-business itself, the analysis is fated to discover trivial relationships between input variables. Even with performance data, such an effort can?€?t hope to understand and normalize the subtleties of Key Performance Indicators (KPIs) in each role, line-of-business and location.,Our experience with these fishing expeditions is that the results are unlikely to yield patterns that are actionable - if you want greater sales, do you limit hiring to a small number of schools??? No ?€? this limits your sourcing options.?? If you want higher influence scores, do you simply start hiring from more expensive zip codes? No. And is there any proof yet that influence is tied to any kind of performance, in this role, in this company? No.,The result of a fishing expedition is descriptive analytics - glorified reporting - not predictive analytics. The only thing that is predicable is that employees may be less than comfortable finding that their employer has been reading their email and social media profiles. It seems obvious, but only by considering , in the analysis?€? can one predict ,.,However beautiful interactive dashboards and visualizations of these patterns may be, fishing expedition projects are unlikely to receive follow-up executive sponsorship. And sadly, the reputation of talent analytics projects at the firm becomes trivialized or suspect.,A more narrowly focused ?€?business win?€? that can be implemented to save hundreds of thousands of dollars a year will win over a vague, ambitious effort just about every time. These more focused wins are indicative of millions that can be saved when implemented at full scale.,Targeted wins can be led by either HR or line-of-business management (or together).?? They identify a business ?€?pain?€? in a particular role, such as:,??,The successful talent analyst goes beyond HR?€?s carefully manicured ?€?system of record?€? and merges actual performance data from the line-of-business. Business relevance is here. Actual outcomes are being tracked in the sales office, where the sales are being made.,??,The data are often quite structured. Even so, performance and KPIs can be confusing and contradictory. It can take effort to untangle signals to devise simple data experiments that deliver results, measured in line-of-business terms (not HR terms). It takes a data scientist with keen control over the data to tease patterns apart to find the truth and present it in a way that can be understood by the business.,??,As in all analytics efforts, a targeted effort with a targeted group is needed for the effort to yield strategic results - not just interesting results.,??,In marketing analytics, the costs of acquiring and maintaining customers are a vital component of a predictive decision. Likewise for employees, the costs of acquiring, on-boarding, and maintaining staff in a role matter.,We don?€?t understand why every line of business or HR department doesn?€?t already know and publish these cost curves, at least for high-volume roles. Most managers know that attrition is expensive, but still only count employee costs in terms of compensation and perhaps the cost of the job advertisement.,To understand attrition cost, one must factor in training, productivity, and break-even as outlined in , , and soon churn 203. The cost information provides a vital link to place a value on employee turnover, prioritize efforts, streamline improvement, and to tune predictive models. Managers who care about performance or attrition need this information.,??,Analysts, HR and line-of-business executives need to obsess over finding incremental fact based solutions to business problems, rather than grandiose patterns seen on a dashboard. HR and analytics teams need to earn their way into bigger budgets and bigger toys.,??,Employee analytics projects should focus on a targeted win.?? Cost modeling is an example of a targeted win and is the ideal way to get started.
Businesses have always faced security threats, whether is someone breaking in to steal some cash or equipment, a disgruntled employee selling company secrets, or something else altogether, there has always been cause to be careful as a business owner., , 

 
With high levels of ID theft occurring each day (if you believe the media, you'd think 50 million people have experienced critical ID theft in US alone over the last two years) and how hard it is to recover from this crime, one wonders: should we all have multiple ID's?,If your answer is yes, then the next big question is: how do you implement this? You'd think that if your primary ID has been compromised, chances are that your secondary ID has been compromised as well, unless these ID's are stored on non-connected servers. And the second big question is: how do you convince everyone (banks in particular) that your ID #2 and your ID #1 point to the same person - you?,Just food for thoughts, I would love to read your opinion. Any sound computer infrastructure has mirror (redundant) databases and backups, I think that we - human beings - should the same level of security / backup / redundancy as machines. It is a big data / data science question.
The success of any big data or data science initiative is determined by the kind of data that you collect, and how you analyze it. In this article, we describe a simple criterion to select great metrics out of dozens, hundreds or even millions of potential predictors - sometimes called features or rules by machine learning professionals, or independent variables, by statisticians.,This criterion is especially useful for practitioners lacking domain expertize or vision, and can be applied automatically. It belongs to a class of synthetic metrics that we have developed in our ,.,Traditional metrics for variable selection ,??and derived exclusively for their mathematical elegancy. To the contrary, synthetic metrics are derived for their usefulness, and take advantage of modern computing power.,In a nutshell, synthetic metrics are to traditional metrics what synthetic pharmaceutical drugs are to traditional drugs: more powerful, more flexible, offering far more opportunities, and less expensive. Our research lab has created ,??so far, including ,??most recently.,Let's discuss our synthetic predictive power - a metric that measures how good a variable is at predicting the future. The traditional equivalent is the R-Squared, ,.??A more modern metric is entropy - still very much a natural metric, derived from physics principles rather than built from scratch.,The predictive power W is an hybrid (semi-synthetic, close to entropy), easy-to-interpret, robust statistic to measure the strength of a predictor. It is defined ,??(see subsection called predictive power). In the context of scoring and fraud detection models, it has the following properties:,This metric W has been used to ,??out of trillions of trillions of combinations, to optimize predictive scoring systems, in particular in data science techniques such as ,.??The source code and examples will be published shortly in our upcoming book on automated data science, and some can be found ,.,??is a visionary data scientist, author, publisher, entrepreneur, growth hacker, and co-founder of ,.
There is a , in the engineering industries. The decreased cost of sensors, the increased amount of instrumentation on assets and need for new revenue streams are forcing engineering firms to ,. ??The , promises to , which generate additional revenue streams predicated on intelligence generated from the data. As machines increasingly become nodes in a vast array of industrial network, , towards the , which controls machines. , , of machines has begun,Keeping in mind this fundamental shift in value from atoms to intelligence, Flutura has defined , to assess the , of an engineering organisation. The highest level of maturity is "Facebook of machines" with ubiquitous sensor connectivity and the lowest is an asset which is "unplugged" where the device is offline. As organisations embark on a journey to intensify the intelligence layer in their IOT offering it makes sense to map where they are in their current state of maturity.,The 5 levels of machine intelligence with specific illustrative examples are outlined below,This is the lowest level in the maturity in the maturity map. At this level of maturity, the device or sensor is 'unplugged' from the network. There are , to see the state of the machines at any point in time. The machine is offline to the engineering organisation. A vast majority of engineering firms manufacture assets which fall into this category. For example a vast variety of industrial pumps still are completely mechanical devices with no sensors to instrument them,This is the next level of machine intelligence which exists in the maturity curve. At this level of intelligence the device is connected to the network. There is also rudimentary intelligence exists on the device to take ,. Examples of assets having edge intelligence include cars which can alert the drivers to basic conditions which need intervention. Other examples include a boiler which has edge intelligence to switch on/switch off valves based on steam pressure
 

*Note this was originally posted at??,When we first began working on Leada, we sought to better understand the data science industry by interviewing professionals in the field. As students simply wanting to learn more about data science, we ultimately created a free resource to inform both undergraduates and professionals about the data science industry. We accomplished this by having Q & A interviews with experts such as Mike Olsen, Hal Varian, Tom Davenport, and data scientists at LinkedIn, Facebook, Yelp, and more.,The Data Analytics Handbook was not only instrumental in giving us the understanding we needed to feel confident in what we were creating; but was downloaded over 25,000 times, gave us dozens of contacts, and an immediate group of early adopters. Some experts took longer to contact than others (I emailed Hal Varian over 8 times) but you would be surprised who you can get 25 minutes of time to help inform others.??,As a conclusion to The Data Analytics Handbook, we thought it would be fun to see what questions our readers had about data science, and then had them answered by the #3 ranked most powerful data scientist in the world today by Forbes and Director of Research at Google, Peter Norvig.,A month ago, we allowed readers to submit questions about the data science industry and we took the top 8 most popular questions. Below are Peter?€?s responses, thanks to everyone who participated,??,??for designing the handbook, and a big thanks to Peter!,I think more important than either ?€?formal education?€? or online education would be real-world experience. Yes, you do need to get some theory, but it is more important to get lots of practice. So take a University class if that is convenient, or an online class if that makes more sense for you, but then get to work on some real data and figure out how to apply what you learned to a data set, then keep at that.,It is sometimes overlooked that different machine learning problems are in totally different regimes. Obviously there are differences in the size of problems: the number of rows (examples) and columns (features) can vary from dozens to billions, but there are other differences that are less obvious: stationarity (are the examples changing over time), transfer (can we train on one data set and apply what we learned to a different set), sparsity (how many of the possible examples are represented in the data), structure (can the problem be represented as a vector of real numbers, or is some other representation necessary), and so on.,Google has a lot of interesting projects. In terms of problem areas, obviously there are many projects to better match search results, and separately, ads results, to your query. But there is also work in speech recognition, machine translation, image and video understanding, handwriting and gesture recognition, recommendations of music, apps, friends on G-Plus, etc. All these involve machine learning. And behind the scenes we apply machine learning to optimize our own operations: how we allocate jobs to different computers, flow data through our networks, etc.,In terms of tools, Google is building a variety of tools to handle different types of machine learning problems. One big issue is scale: how can we handle ever-bigger data sets. Another is moving from batch models (here is a fixed data set) to stream models (the data set is continually updated, second by second).,Structure and Interpretation of Computer Programs really solidified and gave names to the concepts I had been learning about programming. Kevin Murphy?€?s new ?€?Machine Learning?€? book is, for me, the best of the current ML books. I also learned a lot from the lecture notes by Andrew Ng and Andrew Moore. Judea Pearl?€?s book Probabilistic Reasoning in Intelligent Systems, and before the book his papers and hearing him talk in person, demonstrated to me why I was having so much trouble trying to build systems based on Boolean logic, and showed that probability is the right foundation for dealing with any situation that involves uncertainty. Also: The Inmates are Running the Asylum, Programming Pearls, and The Practice of Programming.,I?€?m not really sure what ?€?data science?€? means. It seems to be a term first promoted by journalists, and now we?€?re all left trying to figure it out what. Wikipedia says it is statistics plus programming skill plus expertise in a particular subject matter. If that?€?s so, then it is hard to tell the difference between ?€?applied machine learning?€? and ?€?data science?€?.,I?€?ve never been able to pick lasting papers in the past, so don?€?t trust me now, but here are a few:,I never thought that ?€?neural networks?€? was a useful category. We want to train some function to set parameters to minimize an expected loss function, and whether the function you are training is called a ?€?neural network?€? or not just seems like an unimportant detail. The fact that they are ?€?semi-parametric?€? ?€? they have a very large number of parameters, but do not rely on keeping all data points around ?€? is certainly important, and I think the semi-parametric space is a very important one. As for deep learning, it is certainly also extremely important to be able to create representations at multiple levels, even when the intermediate levels are not accessible in the data. The current work called ?€?deep learning?€? has an approach for dealing with this issue, but it is not the only possible approach.,I don?€?t think any group has a monopoly on good work. No matter where you are, you have an opportunity to advance the field.,I think this gets back to the deep learning question. In non-naive physics, we have a great theory of the world that can be used with precise measurements of quantities. In naive physics, we apply lessons learned from that theory to the case where we don?€?t have precise measurements. So naive physics tells us that water flows downhill, but doesn?€?t tell us how fast. So think of this as a level of representation that is above the particle-by-particle laws of physics. I think if we make progress at having different types of representations, we won?€?t need specialized theories of naive physics.
 Episode 8 looks at the best sites to learn about the world of Big Data.,Here are my top 6 sites. They range from the easily digestible to the totally immersable and possibly overwhelming.,Finally, if you are using a particular tool or solution - then find a good source of information for that and get into the habit of checking those sites regularly.,If you use all or some of the sites I have mentioned then you will quickly absorb a lot of new information about Big Data - but don?€?t forget to look at these sites critically as you?€?ll get a lot more of of them that way!

In this article, I further emphasize the difference between data scientists and other analytic practitioners. I wrote last week that ,: there's some overlap, but less than most people think. Here, I elaborate on this theme.,Other disciplines such as data mining and machine learning (and of course statistics) produce frozen statistics: analyses produced by an employee or consultant, resulting in insights delivered to managers, or integrated in tools developed by software engineers. Synonymous include static or solid analytics.,To the contrary, data science is also about liquid analytics. Synonymous include adaptive or interactive analytics. Extreme liquid analytics is called real-time analytics.,Many tools and software packages are designed to handle frozen analytics. Few can handle liquid analytics, though if you know your tool well (and integrate it with other tools) or write your own code, you'll be able to work the liquid analytics part. I did it with a combination of HTML, SAS, Perl/CGI, sendmail, cronjobs, UNIX commands, shell scripts, Sybase/SQL (and an API offered to users), back ,: read the section ,??in??,.,Also, not all data scientists work on liquid analytics. Many actually assigned the , job title to themselves, while still carrying on traditional frozen analytics: ,. Finally, without liquid analytics, it's very hard to produce robust, automated, scalable data science, your best option being then ,.
Here is a non-exhausting list of curious problems that could greatly benefit from data analysis. If you think you can't get a job as a data scientist (because you only apply to jobs at Facebook, LinkedIn, Twitter or Apple), here's a way to find or create new jobs, broaden your horizons, and make Earth a better world not just for human beings, but for all living creatures. Even beyond Earth indeed. Help us grow this list of 33 problems, to 100+.,The actual number is higher than 33, as I'm adding new entries.
??for explanations.

Three thoughts this time, for our first edition of ,.,An estimate that is slightly biased but robust, model-independent, easy to compute, and easy to interpret, is better than one that is a non-biased, difficult to compute, mysterious, or not robust. That's one of the differences between data science and statistics.,Learning how to code, especially SQL, should be the last step in becoming a data scientist, because it creates highly rigid thinking, while data science is about ideas, problem solving, vision, initiative and flexibility (the opposite of coding).,Data science is not built on stats taught in traditional stats programs. It is based on modern stats that are not found in stats textbooks, though quite a bit can be found in modern machine learning books.,Follow us on Twitter:??,??|??
If you haven't checked out our newsletter recently, I invite you to do so. The next weekly digest will announce our upcoming , book, and a complimentary copy (eBook) will be offered to our subscribers later on.,To make sure that you benefit from these exclusive advantages, ,:,The sender (the name we use in the "From" field) is usually ,, and all messages have our physical address (Issaquah, WA) at the bottom, as well as an unsubscribe button. If you use Gmail, check your Social or Promotion tab, and make sure to move our messages to Inbox. This can be done with a simple drag and drop, pointing to the message in question to move it on Inbox. You can choose to have us in your Inbox all the time: just do a Google search on "moving a sender to inbox". On Yahoo mail (and Gmail too), check your spam box: you might find many other valuable messages burried there, and maybe some of our messages.,We are removing subscribers who never check out our newsletter every six months or so, so you might be missing on some great stuff. To stay in our mailing list, make sure that you get our newsletter (and like the content of course), and move us to the right folder for you. Likewise, if you are no longer interested in our messages, the easiest way is to unsubscribe: it is much better than wait 6 months to be automatically removed due to lack of activity. Once removed, you can re-subscribe, though currently you need to use a different email address. We are thinking about making this process easier.,Examples of recent messages from us include:,Not all our email messages are published on the web, and we reserve the right to stop publishing them on the web.,Finally, here's ,. Please check out this page if you do not remember how you subscribed.,Best regards,, Vincent
??Whether you check your online bank account, monitor your workouts, discover the energy consumption of your house, check your pipeline in??your CRM system or view remaining vacation days on your HR application, visualizations are part of the large majority of web applications.,When data??visualizations are embedded well, they appear to be??seamlessly integrated into the application. But they may, in fact, be managed and generated by a dedicated visualization server.,In a previous??,??I presented four different categories of??users that interact with applications: Content Viewers, Data Discoverers, Content Creators and Query Experts. In this post I?€?ll present three major trends affecting these user categories.,Today, the large majority of applications present static content in a ?€?take it or leave it?€? approach. But with??the??trend of Interactive Content Viewing, visualization content will increasingly respond to the viewer?€?s commands. Content Viewers will be able to personalize the way information is displayed, without querying the data, using easy, web-based, secure tools.?? This ??trend affects millions of users.,Today?€?s data discovery tools are good-looking and powerful, but they tend to be standalone applications. The next trend, Embedded Data Discovery, provides Data Discoverers with prepackaged data and the appropriate toolset, seamlessly embedded and secured within an??application. However, even with embedding, data discovery will remain confined to a relatively modest subset of business users in my opinion.,This is by far the most interesting and far-reaching trend. Getting the pieces of the big data puzzle together is challenging: You need data scientists, data architects, ETL expertise, storage and technology in order to access unstructured data. To??expand the population of users who can access??big data, the challenge is to bring together Query Experts and Content Viewers to create easy-to-use applications that are accessible to both business and casual users. We call these Data Driven Applications. Under this trend, branding and user experience are at least as important as having??the right machine learning algorithms.,The key to success is to think about your??end users. What will big data mean when they??look at the screen of a PC, tablet or smartphone? They??may see some fancy networking graphs showing interconnected social media activity, but that?€?s of marginal usefulness. The large majority of visual impacts will come from??alerts, recommendations and relatively straightforward graphs. It?€?s all about the data and a new generation of applications that will deliver value from that data.,Which??brings us to the??topic for my next blog post:,- See more at:??,This post comes from??Geert Meulenbelt, an OEM Sales Rep for Actuate's EMEA Group.
For explanations about the methodology, including source code and possible improvements,??,. It also provides links to our other three listings.
It seems like more and more companies are very interested in either, improving or setting up their analytical capabilities. All these companies are quite attracted to Hadoop, Spark or other similar solutions, not necessarily because they solve real problems they?€?re facing, but because they are shiny, trendy pieces of technology.,Hadoop, Spark and others are fantastic pieces of technology, and there are plenty of use cases to justify their use. But they aren?€?t right or even slightly helpful to everyone.,Here are??,:,If you, or others you know, have been a victim of shiny technology syndrome and fallen for Hadoop when you didn't need it, please share your story.??
"This ""docu-drama"" compares the experience of two enterprise developers creating a mission critical application.,All we can say is... Don't be a Larry!,
"

When we perform machine learning of type classification, the target variable is a categorical (nominal) variable that has a set of unique values or classes . It could be a simple two class target variable like "approve application? " with classes (values) ??of "yes" or "no". Sometimes they might indicate ranges like "Excellent", "Good" etc. for a target variable like satisfaction score. We might also convert continuous variables like test scores (1 - 100) ??into classes like grades (A, B, C etc)., , , , , 
Any author would like to know if his/her article will be successful or not. Here is an attempt to deal with this task.,We crawled 5000 URLs and for each URL we downloaded the title, body of the article and parameters: number of likes (not including Facebook likes), number of comments, number of views, article creation date and date of the last comment.,First, we got rid of empty (or deleted), very short (less than 100 characters long) and ?€?not found?€? articles, thus getting 2000 articles with associated parameters. Then we removed articles with missing parameters and ended up with only 1207 articles.,Second, for every article we conducted tokenization of words. We deleted all the punctuation marks, stop words and all words (non-abbreviations) shorter than 3 characters. Considering the fact, that Data Science Central is a community dedicated to special topics (most of terminology is not common), we used 2000 most common English words as stop words list. Thus, we saved all the terminology for topic identification and significantly reduced the size of vector space for topic modeling. For all tokenized words, we carried out lemmatization.,??,We used plots to detect outliers. Here you can see some of them:,Basing on the plots we detected and removed the outliers (there were 7 of them),To get the most probable number of topics for all articles we conducted cluster analysis on different number of topics. To do this, we used hybrid topic modeling/clustering algorithm consisted of well-known LDA topic modeling algorithm (with Collapsed Gibbs Sampling for inference), Gaussian Mixture Model clustering algorithm and hard clustering approach. With this algorithm, we iteratively clustered articles into different (from 2 to 40) number of clusters, estimating Silhouette coefficient (internal clustering validation measure) on every iteration (to learn more about this approach please read ,After that, we obtained keywords for 6 topics to analyze if this number of topics was indeed the best choice. As soon as 6 topics were really the optimal choice, we estimated TF-IDF scores for every word in every topic to later use for modeling.,The first question to be answered during the modeling phase was ?€?how to measure success of an article??€?,So, we decided to evaluate success as the combination of parameters (page views, likes, number of comments and lifetime) for an article belonging to certain topic compared to mean/median value of parameters of other articles belonging to this topic.,We estimated minimum, maximum, mean and median values of page views, likes, number of comments and lifetime for all the articles in every cluster. To estimate lifetime of an article we assumed that the lifetime should be a period when someone reacts to the article with comments, so we considered lifetime the period between the day when the author posted the article and the day when the last comment was published.,We consider the article normal if the actual value of at least 3 of 4 of parameters falls between the median and mean value of parameters for the topic. If 2 or 3 parameters are above the mean value ?€? we consider the article partially successful. If the actual parameters of the article are all higher than mean values ?€? we consider the article successful.,We conducted topic modeling of words with LDA (+Collapsed Gibbs Sampling) into 6 topics and extracted 500 top key words for every topic with their TF-IDF values. Then we created 1200x3000 training matrix (where 1200 was the number of articles and 3000 was number of keywords, matrix was not Boolean ?€? we filled it with appropriate TF-IDF weights) and trained a supervised classifier.,We carried out multiple experiments in order to choose the most suitable classifier and classifying strategy. SVM and Random Forest classifier showed the best results, so we ended up with SVM with linear kernel (C=0.5) and one-vs-all strategy for training.,The result on 10-fold cross-validation was as following:,??,In this section, we will answer the DSA questions.,How would you handle 500,000 pages rather than 5,000:, The crawling, using distributed architecture, maybe 2-3 computers, or at least multiple threads of this process running on the same machine. You might find that running 20 web crawling??processes??in parallel on the same machine will boost performance by a factor 5. Explain why the improvement is less than a factor 20, but much bigger than a factor 1. How to design a web crawling algorithm that can be resumed with one click (from where you left just before the crash) if you lose power or your Internet connection suddenly goes off?, The idea is that if we want to run a multi-threaded crawler with 20 threads on a, say, PC with two or three cores we need to know that we must synchronize the threads of the crawler. We must do this in order to improve the performance of the crawler by having common storage to write crawled data in or/and common storage of URLs for all threads (just to get rid of duplicating crawled data by parsing the same URL multiple times). Of course, if the storage is common, only one thread may write (or read) data into it at the same time, so other threads must wait. In addition, there are even more limitations for multi-threading like CPU performance and internet connection bandwidth. That is why performance boosting will not be factor 20, unfortunately.,But, to be frank, we may boost our crawler?€?s performance almost by a factor 20 by having 20 cores (one core per one thread) on our PC and totally no synchronization between threads of a crawler.,The simplest and most obvious solution to create an easily ?€?resumable?€? web-crawler is the chekpointing strategy. Checkpointing is ?€?writing a representation of the crawler?€?s state to stable storage that, in the event of a failure, is sufficient to allow the crawler to recover its state by reading the checkpoint and to resume crawling from the exact state it was in at the time of the checkpoint.?€? (from ,),So, in the event of a failure, any work performed after the most recent checkpoint is lost, but none of the work up to the most recent checkpoint., The clustering process. How to scale from 5,000 pages to 500,000?, To handle 500??000 documents using the current algorithm we would try to lower the dimensionality for feature matrix for topic modeling by:,1. Using stop-word list of 5000 most frequent English words,2. Selecting words (only nouns, verbs and adjectives) with high TF-IDF score ?€? say, 0.7 and higher,3. Getting rid of very small articles, say, less than 500 characters long.,These actions will significantly decrease the size of an input feature matrix for topic modeling.?? For example, authors of ,??succeeded to decrease a number of unique tokens from 272,926 to 65,776 for 2,153,769 documents. Assuming that our measurements will decrease the dimensionality of matrix to 450??000 x 65??000 that must be enough to conduct topic modeling on one PC with descent performance.,Topic modeling with LDA will decrease the dimensionality of feature matrix to m x n where m is number of articles and n ?€? number of topics. Values of the matrix are probabilities of document to belong to particular topic. Assuming that we may have about 500 topics for 500??000 articles, the input matrix for clustering will be 450??000 x 500. This is also ok for being processed on PC (even keeping in mind high computational complexity of clustering).,There are, however, other strategies to handle large set of articles (we would use it to handle 1-2 million articles or more):,1. Distributed topic modeling and clustering algorithms, like Approximate Distributed LDA (perhaps, with variational EM algorithm for inference rather than Gibbs Sampling, even Collapsed one) (see, for example, ,),2. RRI (Reflective Random Indexing) to decrease the dimensionality for clustering.,3. Other algorithms for large-scale clustering of text data (not connected with topic modeling)., Keyword recognition. How to make sure that 'San Francisco' or 'IT Jobs' are recognized as one keyword, rather than being broken down in two tokens, or 'IT' being mistaken for the stop keyword "it" and just ignored?, In general, this class of problems is called Entity recognition.,There are some simple yet descent algorithms mostly connected with word co-occurrence detection:,1. Simple n-gram approach ?€? just consider most frequent n-grams collocations (simple but not very efficient),2. Entity recognition using (Frequency based) Symmetric Conditional Probability ?€? has descent quality of entity recognition (we?€?re likely to use it for general entity recognition task in our work),3. Other statistical methods like computing mutual information for a pair of words and then choose pairs with highest MI score.,4. You name it,There also exist some methods to deal with Named entities:,So, there exist many methods to recognize entities successfully enough. There are also many implementations (mostly for English) of statistics/corpus-based NERs, like Stanford NER.,In addition, of course, we need to make use of some heuristics to improve the quality of our entity detector, like using case-sensitive approach in recognizing collocations to not to confuse IT in ?€?IT Jobs?€? with ?€?it?€?., Machine learning. How would you update the clusters every 6-month, to reflect potential new trends in the data, without re-running the whole algorithm? Is 6 month a good target? What would be the ideal updating schedule? Daily? Monthly? Yearly?, Having read several articles about current and future trends in Data Science, we assume that every year we may have about 5-6 new trends. Let us assume once again that 2-3 of them will contain new terms and, possibly, topics. Thus, we had better update our model 2-3 times per year (every 4 or 6 months) to keep up to date with new technologies and words.,For the current experimental prototype, it is impossible to update clusters without rerunning the whole algorithm, because we need to reconstruct vector space using new TF-IDF weights for existing words and new words as well in order to detect the presence of new topics. Therefore, we need to conduct both topic modeling and clustering.,However, we may try to use RRI instead of LDA, which may allow us just to add new words to the existing model.,??,We have created a prototype to predict the number of topics, the most probable number of page views, likes, comments, and estimate possible lifetime based on the topic of the article with the precision of 73-75%. The model also can decide whether the article is successful or not based on current parameters?€? values.,We found out that text length and author?€?s name significantly affect general article successfulness; the title may affect the number of article views. We also think that not only number of comments is significant for the article successfulness, but the distribution of comments in time as well. These findings will be used in our further research.,Unfortunately, current experimental model cannot predict the success of the article based only on the text and title of an article. Furthermore, the model is currently unavailable to estimate and predict topic trends.,??,We plan to continue working on the problem and conducting experiments in order to create good and scalable solution for estimating successfulness of articles and predict topic trends. We will publish the results of the further research in the blog.,We also plan to publish here some articles dedicated to NLP algorithms for data analysis and other interesting things.,??,Here you can find the topics by predicted mean/median successfulness by descending order (1 - most successful, 6 - less successful),1. Data Science and web-analytics (median 2550 views, 3 likes, 1 comment),2. Statistics/mathematics (median 1300 views, 2 likes, 0 comments),3. Programing/Machine learning (median 1100 views, 1.5 likes, 0 comments),4. Big Data analytics/Databases (median 900 views, 0.5 likes, 0 comments),5. Other/Research (median 650 views, 0 likes, 0 comments),6. Business/Predictive Analytics and Education (median 490 views, 0 likes, 0 comments),??,What types of articles tend to be most successful?,??,1. Tutorials and How-to-do-smth-yourself articles tend to be successful,2. Any posts written by famous Data Scientists (especially by Vincent Granville) tend to be successful,3. Lists of interesting web-resources/books/etc tend to be partially successful (views and likes above average),4. Long articles tend to be relatively more successful than short ones.,??,Top-10 key words for the topics:,??,:,keyword,??,:,??,??,??,??
Last weekend, I was waiting in New York?€?s Penn Station, when the public announcer gave the familiar ?€?See Something Say Something?€? message. It took a minute to sink in, but I had to laugh. Midtown Manhattan , suspicious and unusual activity.,In practice, data is dirty and big data is filthy.?? Analysts munge, wrangle and clean their sources, and a good analysis will recognize the rejected observations.??In August, the , joined the recent crowd calling this "janitorial" work and claimed that data scientists spend "50 percent to 80 percent of their time mired in this more mundane labor".?? It is not glamorous, and it is getting more difficult.?? But, it is necessary, even priceless.,Suppressing data can be argued well with,All those dropped observations have value, though.,First, when we find a problem, we should tell someone.?? We don't have to, but we should. Like that "See Something, Say Something" announcement, communicating exceptions is an analyst's responsibility.?? Software gets fixed, other analysts save time, lessons get learned, customers get a better experience.??,Second, this data may deserve some digging.?? If there's a process, people will find a workaround.?? Machine generated data shows that computers do the same thing with controls. Data exceptions have stories that lead to new business rules and pattern discoveries.?? As with data errors, we don't have to pursue these stories, but we should.?? Researching outliers has a poor "a priori" business case.?? You don't know what you'll find. Tracking the value of what you have already learned is almost as good.?? That's an anecdotal business case.,The next time a package promises to automatically clean data, report that suspicious and unusual activity to anyone who will listen.
The popularity of Big Data lies within its broad definition of employing high volume, velocity, and variety??data sets that are difficult to manage and extract value from. Unsurprisingly, most businesses can identify themselves as facing now or in future Big Data challenges and opportunities. This therefore is not a new issue yet it has a new quality as it has been exacerbated in recent years. Cheaper storage and ubiquitous data collection and availability of third party data outpaced the capabilities of traditional data warehouses and processing solutions. Businesses investigating Big Data regularly recognize that they lack the capacity to process and store it adequately. This manifests either in an incapability to utilise existing big data sets to the fullest or expand their current data strategy with additional data.,Today, as a consequence of the Big Data trend, Businesses can turn to Big Data as a Service (BDaaS) solutions to bridge the storage and processing gap. Interestingly, a definition and classification of BDaaS is missing today and various types of services compete in the space with very different business models and foci. Businesses investigating Big Data and BDaaS, however, would be well served to review the types of services and how they align with their business goals before drilling down and evaluating instances of these services. What are the different types of BDaaS available?,This article discusses the following types of BDaaS:,.
Didn't read Part 1? Read it here ->??
There has been a few people questioning the value of big data recently, and predicting that big data is going to get smaller in the future. While most of these would-be oracles are traditional statisticians working on small data and worried about their career, or practitioners in small countries (Canada and France in particular) who do not have access to big data, I was surprised to see Mike Jordan - a famous machine learning professor at Berkeley - ,. Obviously big data will be dead in 5 years: it will be replaced by titanic data.,The purpose of this article is to discuss whether or not data grew too big, to the point that it is better (for some companies) to not harvest big data and reverse back to a small-data world, or maybe even a data-free world, where executive decisions are made based on gut feelings.,Here I do not criticize the gut feeling approach, I actually use it a lot myself with great success, ,, though it needs tremendous vision to get it to work. I just want to share some thoughts:,Part of the problem with big data is cultural. Americans want bigger stuff:,But sometimes, bigger is better. A bigger army, if used properly, will yield significant benefits (unless it is too big and costs too much taxpayer money). And armies require big data to work properly - think about the NSA. Bigger universities, if well managed, are good: it allows each student to choose courses from a very large pool of professors - including highly specialized training that small universities can't afford to deliver.,Finally, big data can be a great investment for any company. Start collecting data now even if you don't use it: when you sell your company, your data might be one of your core assets. Besides, ,. However you need to carefully chose the metrics and data that you want to invest in, and harvest. Just like any investment after all.,In our case, as a small company (zero employee, 7-digit yearly profit), we leverage big data from our vendors. The value is tremendous. It is used in our growth hacking strategy, and as an unfair competitive advantage. As an example, we use smart computational advertising,Related Article:??
In a??,??on the Web Summit Blog, founder??,??explains that this growth has been largely propelled by??,, or in his view, network science.,While traditional conference companies hire event managers, he hired physicists with PHDs in areas like complex systems and network analysis. They were asked to create and optimise social networks.??,As Paddy explains it,??,.,He calls this approach ?€?engineering serendipity?€?, and he believes it?€?s the secret to their success.,Read his story??,??and if??,This story was also posted on??
??is the first to complete our DSA, using NLP, web crawling, statistical techniques and Python to cluster our content in top categories:??,.,To be fair, our intern??,??(nuclear physicist, post-doctorate at Columbia University) was one of the first candidates, but instead joined us (internship) to work on various projects, including??,??on Twitter, and track their growth over time.,And then, there is??,, who worked on the??,??and won the award ($1,000) attached to it.,We are currently reviewing many new applications. Accepted candidates will be notified soon. Due to the large volume of applications, priority is given to US residents with a quantitative background (college level plus one year of experience minimum) and familiar with R, Python, and the material in the ,. Other applicants are in our waiting list, and we may partner with an organization to move faster. Keep in mind that our program is mostly do-it-yourself, on-demand, for self-learners, with limited support. But we do accept people coming out of nowhere, sending us or posting a great solution to ,??(involving real or simulated data sets): this might be the fastest route to complete the apprenticeship.,We will soon add a new project: detecting fake reviews on Amazon, and estimating the proportion of fake reviews on Amazon.,Finally, make sure that you receive our announcements. This week, we posted a notice about??,.,Best regards,,Vincent
 

Chances are that you might have purchased a book, or visited a restaurant, as a result of reading fake reviews. The problem impacts companies such as Amazon and Yelp, while on Facebook, massive disinformation campaigns are funded by political money, hitting thousands of profiles and managed by public relation companies: they create fake profiles and try to become friends with influencers. Here the focus is specifically on Amazon book reviews, the Facebook issue will be discussed later, while the Yelp issue is well known and has resulted in a class action lawsuit: Yelp's account managers create bad reviews for restaurants, and if you pay a monthly advertising fee, suddently your rating dramatically improves.,Amazon is selling books, so it has ,??when it comes to book (or product) reviews. The purpose of this article is three-fold:,This is the new project for candidates interested in our data science apprenticeship. The full list of projects ,. The project description is as follows:,Y,Note that we do not study here the impact of reviews and stars on purchasing behavior or pricing, this will be the subject of another article.,Which metrics would you use to detect fake reviews?,These are features that should probably be included in any fake review detection system. HDT (,) is a great data science technology to design such scoring engines, to score reviews. What other metrics would you suggest?,Here the data science apprentice is asked to try various strategies to post fake reviews for targeted books on Amazon, and check what works (that is, undetected by Amazon). The purpose is to reverse-engineer Amazon's review scoring algorithm (used to detect bogus reviews), to identify weaknesses and report them to Amazon.,Strategies will involve,You might have to fine-tune the suggested parameters, to optimize performance of your fake review posting process. Success here is measured by the proportion of 4- or 5-stars books where you managed to reduce the number of stars, to 3 or below. Deliverable is a paper summarizing the results of your test, how scalable your strategy is (can it be automated?) and recommended fixes to make Amazon reviews more trustworthy (that is, designing a better review scoring system). A review scoring system score the reviews, and automatically "review the reviews" to decide which ones should be accepted.,Amazon authors are vulnerable to the following fraud, that would eventually result in significant business loss for Amazon.,A start-up company selling good reviews for $500 per book with a $100 monthly fee. It would work as follows.,How scalable is this? A college student could easily make $500 a day, targeting only a few books each day. That's $100k per year, and collect the money via Paypal. Because the money is relatively easy to make, a large number of (educated and under-employed) people could be interested in setting up such a scheme, eventually targeting thousands of authors each day when combined together. Or someone might find a way to automate this activity, maybe using a Botnet, and make millions of dollars each year. Many authors would eventually refuse to have their books listed on Amazon, and choose to self-publish with platforms such as??,. Publishers would also opt out of Amazon. Revenue on Amazon (from book sales) would drop. Or Amazon could simply eliminate all reviews and not accept new ones.,Interestingly, it appears that Yelp might be making money with a similar scheme: ,??and blackmailing small businesses listed on its website. And I've seen companies selling ,??or Facebook profiles, though they quickly disappear. Even LinkedIn was recently victim of a massive scheme involving ,. , , ,Website relying on reviews (books, products, restaurants reviews, etc.) are vulnerable to massive attacks that could destroy their reputation, and eventually their income.,How could Amazon protect itself from such a risk? Using a better review scoring engine. Relying more on their recommendation engine (user who purchased A also purchased B). Design a better fraud-resistant user reputation engine, and integrate user reputation as a metric in the review scoring engine. Display reviews with high score at the top, or more frequently. Or dropping user-generated reviews altogether.,Also, Amazon could categorize users, so that a data science book review by a user categorized as "interested in web design" does not carry the same weight as a data science book review by a user categorized as "interested in data science". Or a new company could emerge and start competing with Amazon, by offering much better user experience. Such a company could make additional revenue by offering authors the possibility to have their book featured at the top, when a user is searching for books - just like Google does with webmasters who want to promote their website.

are currently witnessing a land rush of investment in Big Data architectures promising companies that they can turn their data into gold using the latest in distributed computing and advanced analytical methods. Although there is indeed much potential in applying machine learning and statistical analysis to??,datasets, many companies are hardly sitting on the kind of data that will allow them to compete using hundreds of machines chugging through terabytes of data.,But that's okay. There is a massive benefit to just getting an organization to understand what data they do have and how they can deploy intelligent models on this data to disrupt their current approaches to doing business. This does not require the latest in parallel computing or bursting into new map-reduce paradigms just to derive insight. It does not require huge data warehouses or terabytes of unstructured data. What it does require is??,. This is where organizations need to start; becoming ''data aware' and building an organizational culture that understands data as a real asset.,The majority of organizations have barely moved beyond static BI reports and are unaware of the actual potential their data holds. Going from being 'data unaware' to investing in a big data architecture in one leap sets a company up for a bad ROI in analytics. A solid investment must begin with understanding what data is actually available and identifying the low hanging 'data fruits' that can lead to real value for the company. This can be used to build lightweight solutions that are imperfect but hugely beneficial. It can provide real-world tools for decision support via recommended actions or highlighted opportunities in real-time. It can offload much of the routine repetitive decision-making to algorithms so that professionals can operate with a more strategic view of the organization and bring their creative talents to their company's challenges.,With time, this foundation can lead to some huge benefits as the organization starts understanding how to compete using data. As their data-awareness matures, the fruits of Big Data become a real possibility and the architectures for dealing with terabytes of data and highly advanced models now stand a chance to offer a real return on the company's investment in analytics.,When it comes to analytics we are in the inevitable bubble that comes from any new technology that appears highly disruptive. It makes it difficult for organizations to separate the real potential from the over-hyped claims of vendors looking to get every company running their latest Big Data stack. But the potential for Big Data is real and can make a true difference to your organization; it just takes time to build a real analytical foundation.,Start with doing good science on good data. Hire a great scientist to identify what data you do have and how this can be used immediately to start offering real-world solutions to your challenges. Get them to build simple but effective models that can automate routine decision-making and elevate your CXOs with a more strategic view of the organization. Get them to develop 'data-forward' strategies by identifying where you are data-poor so that you can start collecting rich data for future analytics efforts. With time, your company will move into a position where they can reap the real benefits of Big Data using the latest machine learning algorithms and distributed computing architectures. It's an investment worth the effort.

In a recent engagement with a Fortune 50 client which I had blogged about ,??earlier, I find there to be considerable debate within the IT department on the nature of the ??role that Domain Subject Matter Experts will need to play in the design of Advanced Analytics Solutions. Some argue that the role of the Domain Expert needs to be extensive in all stages of the design and implementation process. Others,myself included, argue that the engagement of the Domain Expert while extensive, will need to be more iterative , increasing in scope as domain experts learn to think more like data scientists.??,A lot of the information in organizations today is stored in free form text and documents both within transactional systems and outside formal system boundaries in the form of Word documents, Excel Sheets, SharePoint Sites, etc usually managed by the Line of Business. Needless to say, the domain expertise that experienced knowledgable employees bring to the table are critical in interpreting this information, which in turn, enables day to day operational decision making. That being said, there are several drawbacks on relying too much on the business domain expert.??Most business users almost exclusively depend on Excel for their Data Analysis. Jeremy Howard of Kaggle.com in his talk on "," explains how????"Excel is very easy to make errors in" which can lead to some very wrong conclusions. Also in my experience, domain experts tend to come in rightly or wrongly with preconceived conclusions and are more likely to fit the data with what they believe ,rather than the other way around, allowing the data to ??guide them to the correct conclusions.,I agree with the assertion that Domain Subject Matter Experts need to play a big role in the design of Advanced Analytics solutions. Practical common sense, good judgement and an understanding of the real business issues are relatively uncommon and hard to acquire skills and we need domain experts for that. However, their inputs in the design and implementation needs to be of an iterative nature increasing in scope as the analytical model matures and/or evolves.??Some of the tasks in advanced analytics design that a domain expert is best suited to perform are things like creating organization specific dictionaries, defining classification categories,??creating an effective Training Set for the learning algorithms,helping understand the desired business outcomes at a more granular level amongst others.,As advanced analytics techniques and technology become more mainstream, training business users and data scientists to think more like each other, may be the real challenge. Those business experts and data scientists who learn these cross functional skills will be at a distinct advantage to their peers who do not and will be able to make a much bigger impact on business drivers and decisions within their organizations.
Any parent can relate to the anxiety of being encharged with a newborn completely dependent on you yet unable to communicate its needs. This worry is amplified when your baby is born prematurely and spends its first few weeks of life in the Intensive Care Nursery hooked up to cardiorespiratory monitors.,Here's the thing about cardiorespiratory monitors. These devices track a newborn?€?s heart rate, breathing rate and oxygenation so that nurses can attend to health problems like??,??and??,??and parents can rest assured that their kid is okay.,But watching your kid?€?s heart rate drop and waiting for a nurse to determine if he is breathing is not a reassuring experience.,What if, instead, these monitors could tell parents exactly what each piece of data means for their child, alert them when something is about to happen and empower them with instructions on what to do before the nurse arrives?,Well, I know from years working in the big data industry that it is technologically feasible for these systems to connect to and learn from every single data point about every single child and device. They could even provide notifications like ?€?Hi, Rob. In 30 seconds, your baby?€?s heart rate is going to drop. You should pick him up and pat his back.?€? And I know from my experience as the father of a prematurely born child that this would be extremely useful. The technology exists, and it could tremendously impact people?€?s lives ?€? but we?€?re not using it.,To give you an idea of the technology I?€?m talking about, I?€?ve been working at WibiData on big data applications that collect and apply data in real time, not only to describe what is happening, but also to predict what will happen in the future. Businesses have successfully used our technology to power ecommerce websites and TV recommendations and even analyze how people can reduce their energy footprint. And believe it or not, the recommendation engine that tells our retail clients?€? websites ?€?Online shopper X will like this shirt?€? has the same underlying technology as the hypothetical cardiorespiratory monitor that says ?€?Your baby?€?s heart rate will drop.?€?,As you may have gathered from the??,, there?€?s plenty wrong with Silicon Valley. But if there?€?s one thing our healthcare system can learn from the gentrifiers and hipsters and tech bros, it?€?s about rapidly experimenting, learning and getting stuff done. Move fast, fail fast, try out new ideas and applications of technologies. Challenge the status quo, discard the past and start anew, recognize barriers that don?€?t need to exist and then break them down.,Big data applications have already proven their abilities in sectors looking to profit from them. Businesses have long been using their customers?€? data to target them with ads and product recommendations. But sectors like healthcare that are less financially motivated to take advantage of big data also could use it to make the greatest social impact. There?€?s nothing wrong with using data for profit ?€? but we should also be using it for good.
I am finishing up my masters in Predictive Analytics at Northwestern which the curriculum has been my only experience in modeling data. All of my data related professional experience has been within the database layer as well as data visualization which I have a great passion for.,I feel that I am at a vital stage in my career as I want to start down the data science path but the experience required in the field is quite extensive.,I would like to ask for any recommendations that anyone may have. How would you suggest someone start in data science with a master degree but minimal professional experience?,.,Thank you so much for your time!

??from ,??has published a new report which outlines business models for Linked Data including a??case study of the publishing house and information service provider Wolters Kluwer:,"With the increasing availability of , and its reutilisation for commercial purposes, questions arise about the economic value of interlinked data and business models that can be built on top of it. The Linked Data Business Cube provides a systematic approach to conceptualize business models for Linked Data assets.,Similar to an ,, the Linked Data Business Cube provides an integrated view on stakeholders (x-axis), revenue models (y-axis) and Linked Data assets (z-axis), thus allowing to systematically investigate the specificities of various Linked Data business models.",Linked Data creates an ecosystem in which data and metadata become a network good. By sharing RDF as the mutual data model, Linked Data allows various data traffic patterns in the provision and reutilization of data. Thus it generates significant benefits in the integration and processing of internal and external data, fulfilling specific economic functions at each step of the content value chain.,The report also provides a practical case study of the publishing house and information service provider ,??describing the utilization of Linked Data as an enabler of service diversification.
A couple of weeks ago, my team was asked to come up with a solution for an Enterprise Complaints Platform with Advanced Analytics capability for a Fortune 50 Bank. The initial scope statements were high level requirements like for example, Identification of high risk complaints that were likely to be escalated to regulatory agencies, Complaint Root Cause Analysis, etc.,It quickly became apparent that while the solution did include Advanced Analytics components what was really needed was a repeatable process for Data Discovery and Descriptive Modeling that would provide the Associates with a semi automated way to complete the Root Cause Analysis or provide a way to identify the initial set of categories that could be used as inputs for Advanced Predictive and Prescriptive Analytics.,Furthermore, the data intake channels for the organization had grown from the traditional banking centers, call centers and ATMs to include ?€?big data?€? channels including social media and mobile channels where a lot of the data was textual and was not necessarily relevant to the transactional data they got from the more traditional channels.,With these challenges in mind, it became apparent that any proposal we made for an Advanced Analytics solution would require considerable pre-processing capability on the input data including Big Data and a workflow capability to provide the ?€?value add?€? repeatable automated processes that the front line associates were looking for.??While this may seem obvious in hindsight, it was a concept that was not immediately apparent when we started the evaluation with the business looking for a One stop solution to meet their Advanced Analytics needs. ??
How has the interest in ,??changed over the years?,One easy way to gauge the interest is to measure how much news is generated for the related term and??,??allows you do that very easily.,After plugging all of the above terms in Google trends and further analysis leads to the following visualizations.,Aggregating the results by year,Before Big Data and Hadoop came into picture the term ?€?Analytics?€? exhibited a stable ground closer to dashboards but now the trend for Analytics seems to be following Big Data and Hadoop.,Let us take a deeper look into each week since 2004

Hi all,,As a Big Data practitioner, I want to share these reflexions with all the DSC community.??,I spend some time researching and writing this document, currently, apart from my family, these two disciplines are my passions, they are filling almost of my time.,Sincerely, I hope that you enjoy reading and I'll appreciate all your feedback and comments.,1. Introduction, 2. Definitions, 3. Similarities Big Data vs. Haute Cuisine,4. Conclusions,Carlos Franco,cfrancoh001@hotmail.com

The outsourcing model which led to the ?€?,?€? ?€?,?€? model, has taken off with increasing adoption of cloud-computing and mobility. What started out with the SaaS ?€? software as a service model, has now diversified into several other services.,Indeed, cloud computing has come to rest on three of these as its core pillars:
According to Deloitte?€?s 2014 Human Capital Trends survey, 86% of companies do not have any analytics capabilities in HR, and 67% are ?€?weak?€? at using HR data to predict workforce performance and improvement.,At the , (March 9-10, San Diego) experienced HR leaders will share how they have reshaped their HR analytics from reporting factories to strategic solution providers that answer some of the most important business questions., To find out more about the summit, ,??see the brochure.,To find out who else is speaking and see the full agenda, ,.


??,??,Conversations with business users invariably start with the question ?€?what is Big Data?€?.?? Implicit in the question is that if it can be defined then they can understand where it currently exists; where the opportunities to be exploited may lie, and when and how will the business user need to deal with this.,Sounds simple enough, but as we observed in a prior posting there are many different characteristics of Big Data on which data scientists agree, but none which by themselves can be used to say that this example is Big Data and that one is not.?? Happily, almost everyone who has weighed in on this conversation has chosen descriptors that begin with ?€?V?€?, hence the name of this article.?? Most common you will hear Volume, Variety, and Velocity.?? These may be the most common but by no means the only descriptors that have been used.,You would think this would be settled by now but a scan of the literature says otherwise. In fact we were able to find eight, count them eight different characteristics claimed for Big Data.??,Volume always seems to head each list.?? There is general agreement that if volume is in the gigabytes it is probably not Big Data, but at the terabyte and petabyte level and beyond it may very well be.?? Volume is a key contributor to the problem of why traditional relational database management systems (RDBMS, data warehouses as we know them today) fail to handle Big Data.?? Underlying that failure are more complex issues of cost, reliability, long query times, and their inability to handle new sources of unstructured or semi-structured data like text.,Big companies are no strangers to Big Data.?? As early as the 1980?€?s UPS began to capture and track data on package movements that now number 16.3 million packages per day while responding to 39.5 million tracking requests per day, now storing over 16 petabytes of data.,?? Wal-Mart records more than 1 million customer transactions per hour, generating more than 2.5 petabytes of data.,?? And in one survey 17% of companies report currently managing more than a petabyte of data with an additional 22% reporting hundreds of terabytes.,So if close to 40% companies report already managing terabytes of data or more what?€?s changed??? What?€?s changed is the desire to unleash the knowledge contained in transactional stores and external data sources through analysis, and when that happens the new NoSQL storage and retrieval architectures and tools become important.,?? Variety describes different formats of data that do not lend themselves to storage in structured relational database systems.?? These include a long list of data such as documents, emails, social media text messages, video, still images, audio, graphs, and the output from all types of machine-generated data from sensors, devices, RFID tags, machine logs, cell phone GPS signals, DNA analysis devices, and more.?? This type of data is characterized as unstructured or semi-structured and has existed all along.?? In fact it?€?s estimated by some studies to account for 90% or more of the data in organizations.??, Variety is also used to mean data from many different sources, both inside and outside of the company.?? What?€?s changed is the realization that through analysis it can yield new and valuable insights not previously available.,There are two primary challenges here.?? First, storing and retrieving these data types quickly and cost efficiently.?? Second, during analysis, blending or aligning data types from different sources so that all types of data describing a single event can be extracted and analyzed together.,Then there is the interaction of variety with volume.?? Unstructured data is growing much more rapidly than structured data.?? Gartner estimates that unstructured data doubles every three months and offers the example that there are seven million web pages added each day.,In terms of opportunity, Variety is seen by business users as the major focus of new Big Data initiatives.?? Companies have been handling large volumes of data for many years and view that process as incremental and business and usual.?? But the new and unique opportunity to add unstructured data to the analytic mix is seen by many as a game changer.,?? Data scientists like to talk about data-at-rest and data-in-motion.?? One meaning of Velocity is to describe data-in-motion, for example, the stream of readings taken from a sensor or the web log history of page visits and clicks by each visitor to a web site.?? This can be thought of as a fire hose of incoming data that needs to be captured, stored, and analyzed.?? Consistency and completeness of fast moving streams of data are one concern.?? Matching them to specific outcome events, a challenge raised under Variety is another. Velocity also incorporates the characteristics of timeliness or latency ?€? is the data being captured at a rate or with a lag time that makes it useful., A second dimension of Velocity is how long the data will be valuable.?? Is it permanently valuable or does it rapidly age and lose its meaning and importance.?? Understanding this dimension of Velocity in the data you choose to store will be important in discarding data that is no longer meaningful and in fact may mislead., The third dimension of Velocity is the speed with which it must be stored and retrieved.?? This is one of the major determinants of NoSQL storage, retrieval, analysis, and deployment architecture that companies must work through today.?? When you visit a sophisticated content web site such as Yahoo or the Huffington Post, those ads that pop up have been selected specifically for you based on the capture, storage, and analysis of your current web visit, your prior web site visits, and a mash up of external data stored in a NoSQL DB like Hadoop and added to the analytics.?? When you sign on to Amazon or Netflix and see recommended purchases or views just for you the same process has taken place.?? The architecture of capture, analysis, and deployment must support real-time turnaround (in this case fractions of a second) and must do this consistently over thousands of new visitors each minute.?? Real Time Big Data Analytics (RTBDA) is one of the main frontiers of development in Big Data today.,What?€?s changed??? The data was always there but the ability to capture, analyze, and act on it in (near) real time is indeed a brand new feature of Big Data technology.,Although Value is frequently shown as the fourth leg of the Big Data stool, Value does not differentiate Big Data from not so big data.?? It is equally true of both big and little data that if we are making the effort to store and analyze it then it must be perceived to have value.,Big Data however is perceived as having incremental value to the organization and many users quote having found actionable relationships in Big Data stores that they could not find in small stores.?? Certainly it is true that if in the past we were storing data about groups of customers and are now storing data about each customer individually then the granularity of our findings is much finer and we approach that desired end-goal of offering each customer a personalization-of-one in their experience with us.,Another take on Value is that Big Data tends to have low value density, meaning that you have to store a lot of it to extract findings.,?? This is likely true but since new Big Data storage and retrieval technologies are so much less expensive than previous, low value density should not be a hurdle that prevents us from searching for those valuable kernels.,Finally, there is at least one reviewer who goes to philosophical extremes quoting Sartre ?€?existence precedes essence?€?.?? By which he means that we may choose to store Big Data before even understanding exactly what use we have for it.,?? We?€?re not entirely sure about this.?? We still encourage business users to work backwards from the desired outcome before deciding exactly what Big Data to capture.,There are at least four additional characteristics that pop up in the literature from time to time.?? All of these share the same definitional problems of Value.?? That is they may be a descriptor of data but not uniquely of Big Data.,?? What is the provenance of the data??? Does it come from a reliable source??? It is accurate and by extension, complete., There are several potential meanings for Variability.?? Is the data consistent in terms of availability or interval of reporting??? Does it accurately portray the event reported??? When data contains many extreme values it presents a statistical problem to determine what to do with these ?€?outlier?€? values and whether they contain a new and important signal or are just noisy data.,?? This term is sometimes used to describe the latency or lag time in the data relative to the event being described.?? We found that this is just as easily understood as an element of Velocity.,?? Defined by some users as the rate at which the data spreads; how often it is picked up and repeated by other users or events.,I?€?ve been working with the US Department of Commerce National Institute for Standards and Technology (NIST) working group developing a standardized "Big Data Roadmap" since the summer of 2013.?? Reaching a common definition of Big Data was one of the first tasks we tackled.?? Those grand qualifiers from our college philosophy classes, ?€?is the characteristic BOTH necessary and sufficient?€? turns out to be extremely useful.,In fact, we elected to stick with Volume, Variety, and Velocity and kicked the last five out of the Big Data definition as broadly applicable to all types of data.?? Unfortunately, as you may know if you?€?ve grappled with explaining this yourself, Volume, Variety, and Velocity do pass the necessary and sufficient test but not all Big Data opportunities demonstrate all three characteristics.?? One suggestion was to call it Big Data if it met two out of three but even that didn?€?t completely pass muster.,Variety comes close when speaking narrowly of unstructured data since storage and retrieval techniques for these data types has really been revolutionized by new NoSQL tools and techniques including blending these with traditional structured data.?? Likewise, Velocity comes close when talking about Real Time Big Data Analytics for the same reason.??,We argued in a previous post that Big Data is not so much about the data itself as it is about a whole new NoSQL / NewSQL technology .?? Big Data is about this new set of tools and techniques in search of appropriate problems to solve.?? Each business application may be different and it is growing apparent that real solutions in real companies are frequently hybrids of NoSQL and traditional RDBMS and analytic tools.?? These definitions may help sort down opportunities at a high level, but before proceeding, each opportunity needs to be carefully analyzed for realistic business value and realistic technology applications.,Bill Vorhies, President & Chief Data Scientist ?€? Data-Magnum - ?? 2014, all rights reserved.,??,About the author:?? Bill Vorhies is President & Chief Data Scientist of Data-Magnum and has practiced as a data scientist and commercial predictive modeler since 2001. ??He can be reached at:,The original blog can be viewed at:,??,??
Hi All, I am Oracle DBA by profession (9 years experience), but Mechanical engineer (in fact post graduate) by education.??Now,??I want to work in the field of BIG DATA & Analytics, using AI techniques (most are open source now-a-days like machine learning Mahout , Spark etc.). However, I am not getting any right opportunity/path to do so. None is such in my current profile. So, seek you all guys expert advise and guidance, what should I do ? How to reach someone who could harness my abilities to optimum. Thanks for your time and advice guys, please help.
This post looks at practical aspects of implementing data science projects. It also assumes a certain level of maturity in big data (more on??,??in the next post) and data science management within the organization. Therefore the life cycle presented here differs, sometimes significantly from purist definitions of 'science' which emphasize the hypothesis-testing approach. In practice, the typical data science project life-cycle resembles more of an engineering view imposed due to constraints of resources (budget, data and skills availability) and time-to-market considerations.,The CRISP-DM model (CRoss Industry Standard Process for Data Mining) has traditionally defined six steps in the data mining life-cycle. Data science is similar to data mining in several aspects, hence there's some similarity with these steps.,The CRISP model steps are:, 1. Business Understanding, 2. Data Understanding, 3. Data Preparation, 4. Modeling, 5. Evaluation and, 6. Deployment,Given a certain level of maturity in big data and data science expertise within the organization, it is reasonable to assume availability of a library of assets related to data science implementations. Key among these are:, 1. Library of business use-cases for big data/ data science applications, 2. Data requirements - business use case mapping matrix, 3. Minimum data quality requirements (test cases to ensure minimum level of data quality to ensure feasibility),In most organizations, data science is a fledgling discipline, hence data scientists (except those from actuarial background) are likely to have limited business domain expertise - therefore they need to be paired with business people and those with expertise in understanding the data. This helps data scientists gain or work together on steps 1 and 2 of the CRISM-DM model - i.e. business understanding and data understanding.,The typical data science project then becomes an engineering exercise in terms of a defined framework of steps or phases and exit criteria, which allow making informed decisions on whether to continue projects based on pre-defined criteria, to optimize resource utilization and maximize benefits from the data science project. This also prevents the project from degrading into money-pits due to pursuing nonviable hypotheses and ideas.,The data science life-cycle thus looks somewhat like:, 1. Data acquisition, 2. Data preparation, 3. Hypothesis and modeling, 4. Evaluation and Interpretation, 5. Deployment, 6. Operations, 7. Optimization,Data Acquisition - may involve acquiring data from both internal and external sources, including social media or web scraping. In a steady state, data extraction and transfer routines would be in place, and new sources, once identified would be acquired following the established processes.,Data preparation - Usually referred to as "data wrangling", this step involves cleaning the data and reshaping it into a readily usable form for performing data science. This is similar to the traditional ETL steps in data warehousing in certain aspects, but involves more exploratory analysis and is primarily aimed at extracting features in usable formats.,Hypothesis and modeling are the traditional data mining steps - however in a data science project, these are not limited to statistical samples. Indeed the idea is to apply machine learning techniques to all data. A key sub-step is performed here for model selection. This involves the separation of a training set for training the candidate machine-learning models, and validation sets and test sets for comparing model performances and selecting the best performing model, gauging model accuracy and preventing over-fitting.,Steps 2 through 4 are repeated a number of times as needed; as the understanding of data and business becomes clearer and results from initial models and hypotheses are evaluated, further tweaks are performed. These may sometimes include Step 5 (deployment) and be??performed in a pre-production or "limited" / "pilot" environment before the actual full-scale "production" deployment, or could include fast-tweaks , deployment, based on the??,??model.,Once the model has been deployed in production, it is time for regular maintenance and operations.??This operations phase could also follow a target ,??model which gels well with the continuous deployment model, given the rapid time-to-market requirements in big data projects. Ideally, the??deployment??includes performance tests to measure model performance, and can trigger alerts when the model performance degrades beyond a certain acceptable threshold.,The optimization phase is the final step in the data science project life-cycle. This could be triggered by failing performance, or due to the need to add new data sources and retraining the model, or even to deploy improved versions of the model based on better algorithms.??,Agile development processes, especially continuous delivery lends itself well to the data science project life-cycle. As mentioned before, with increasing maturity and well-defined project goals, pre-defined performance criteria can help evaluate feasibility of the data science project early enough in the life-cycle. This early comparison helps the data science team to change approaches, refine hypothesis and even discard the project if the business case is nonviable or the benefits from the predictive models are not worth the effort to build it.
Every data scientist worth her salt will immediately notice that the biggest Earthquakes (magnitude above 9) took place in the last 60 years or so.,Most journalists, and even some scientists, will claim that indeed, giant Earthquakes are getting more severe, and more frequent. Even less compelling stories, in particular regarding global warming, based on even weaker or questionable numbers, have been published including in scientific journals.,But is it really the case that Eartquakes are getting worse? Or just a coincidence? Or could a giant Earthquake triggers a few giant Earthquakes 10,000 miles away and 15 years later? Or do giant Earthquakes appear in clusters? It feels it does if you look at the table below, and if yes, we might be entering a quiet period that could last 30 years, after the numerous big ones of the last decade. In short, could the time distribution of Earthquakes be explained?,That's why looking at correlations or time series parameters is not good enough. Potential causes (even if the true cause is yet unknown) should be established before stating that Earthquakes are getting worse.,It turns out that if you look at data since 1900, there is only a 17.5% chance to have been hit by so many giant Earthquakes recently (none of them occurring before 1950). If you look at the entire data set, the probability drops to 8.5%, but the data is less reliable, and maybe fewer people were living close to the worst fault lines (Alaska) back then, to report these events.,The first things that should come to any data scientist's mind is that the data collection process is possibly faulty, that the magnitude scale changed over time (so comparing two time periods is comparing apples and oranges), and that we are better equipped today at measuring the magnitude of giant Earthquakes (maybe in the past, local sensors would have been destroyed by the Earthquakes).,But even if the data is correct, at least after 1900 (prior to that, most of the data is missing), the chance of observing no giant Earthquakes in the first 50 years, is an event with a 17.5% probability of occurring just by pure chance. So, it could really be a coincidence. The use of additional good quality data (volcanoes, small Earthquakes and other related events) could help answer this question. Our data source is ,. Below is the list of big Earthquakes since 1900.,You can use Monte-Carlo simulations to compute the probabilities discussed above. But in this case, a simple computation will do.,If older Earthquakes have their magnitude overestimated, then these two probabilities will be even lower.,The increase in giant Earthquakes is likely a coincidence, given the not-so-low probabilities of random occurrence, and the apparent absence of a rational explanation to justify a worsening. On a different note, are these Earthquakes causing more damages? Maybe not, with new Earthquake-resistant constructions, and better Earthquake predictions (gas pipelines and other utilities being shut down 30 seconds before the quake, and alerts sent to the population via text messages). Note that a 9.5 Earthquake is 10 times more powerful than a 8.5, though I don't know if it translates in 10 times more costs.,The insurance companies so far do not offer affordable Earthquake insurance protection.,Maybe a journalist reading this post will write an article entitled 'The next giant Earthquake will be a 10'. Unfortunately, this is the kind of baseless news that sells. We've proved here that the increase in giant Earthquakes might just be a coincidence, just like the ,??is very much expected.??


We then took the normalized results and mapped them back into a simple spreadsheet structure and put the data up into Silk.co.??
When learning data science a lot of people will use sanitized datasets they downloaded from somewhere on the internet, or the data provided as part of a class or book. This is all well and good, but working with ?€?perfect?€? datasets that are ideally suited to the task prevents them from getting into the habit of checking data for completeness and accuracy.,Out in the real world, while working with data for an employer or client, you will undoubtedly run into issues with data that you will need to check for and fix before being able to do any useful analysis. Here are some of the more common problems I?€?ve seen:
This applies to data science research as well as any other analytic discipline. For centuries, scientific research was performed in Academia, by university professors managing their own labs. Much of the research was carried out by young scientists who just completed their PhD. The selection process has always favored the same type of personality. The basic rule is "publish or perish" which produces the following drawbacks:,With the tenure process, research directors must be careful not to engage in revolutionary experimentation, in order to please their grantors and faculty boards. They also spend a considerable amount of time chasing money, rather than doing research.,This hurts innovation. The private industry and some agencies have their own research labs. But they hire the same type of individuals: the kid that always had perfect grades at school, assuming that this is a predictor of research quality (and since they define what quality is, we are stuck in a loop here). Yet the private sector provides an alternative to Academia, though many times, research results are kept secrets and incorporated into patents.,Here I propose an new approach to scientific research, and discuss how it could be implemented on a larger scale, via proper monetization. It consists of independent professionals performing their research and publishing in popular blogs rather than in scientific journals, and obtaining themselves the data that they need for their tests and experimentation (many ,??are free, many projects are posted on Kaggle, and research-oriented projects are ,, some using simulated data). You can call it crowd-research.,:,In my case, I realized that publishing in blogs takes 1 hour per article, rather than 50 hours for scientific journals. At $1,000/hour (my hourly rate), and since scientific journals don't pay authors, it's a $49,000 saving per article, that is, hundreds of thousands of dollars saved per year. Also, my articles are shorter, published much faster, reach a thousand times more users, are easier to read (with source code that you can copy and paste, data sets that you can download), and written so as to be understood by many professionals from various applied disciplines, not just a dozen highly specialized theoretical experts. You can compare ,??with one published by a traditional statistician, ,, independently and at the same time. I believe that mine is more useful, provide code to make much faster, longer videos, and is in essence, of superior value.,??,The money can come from various sources. As a data scientist interested in doing research, you have the following options; you can combine several of them:,If you spend 25% of your time in these money-making activities (listed above), 25% of your time in building your network and reaching out to clients, 25% on doing scientific research (including working on projects that support your research), and 25% managing your business (organizing, planning, operations, finance), you will soon make more money than working in a cubicle, and at the same time doing things that you enjoy, with a real control on your life.,I'll write more articles on how to get started with this career path, and offer mentoring, in the near future. For now, feel free to check out our ,.
"A picture is worth a thousand words" or in the case of Data Science, we could say "A picture is worth a thousand statistics". Interactive Data Visualization or Visual Analytics has become one of the top trends in transforming business intelligence (BI) as technologies based on Visual Analytics have moved into widespread use.,Conventional Charts and Dashboards show conclusions but not the thinking behind it. The charts do not empower the user to ask questions and get further.??Visual Analytics needs to enable Visual exploration where querying, exploring and visualizing data are a single process along with the ability to shift and link multiple visual perspectives. As a result, the brain's ability to process pictures faster than text is leveraged. For an interesting take on the seven essential elements of a Visual Analytics Application refer to ", from Tableau.,One of the most important steps in the Data Analytics process is Feature Selection where an analyst helps select inputs or features to help the end user understand and explore the data better.??The Analyst or Data Scientist performing the feature selection is usually someone with a statistical background having expertise in one or more of the many Analytics platforms like SAS, SPSS, R, etc. The End User on the other hand is someone more familiar with the business and with a limited understanding of various Feature Selection and Modeling Algorithms.??,There is a need for interpretability and transparency in the feature selection and model creation process which will require greater interaction between the Analyst and End User.??Visual Analytics will need to play an important role to enable this interaction between different groups.,In a sentence, by creating more Interactive model visualizations where the End User can interact with the model and visualize future scenarios by changing input parameters. For an interesting example, of the power of Interactive Visualizations take a look at this ,??and compare it with the original CDCC spreadsheets which can be found in the same article.??,In a similar fashion, it is possible to design a more interactive Feature Selection process between the Analyst/Data Scientist and the End User where the user is empowered to iterate through different feature subsets using Interactive Visualizations like the one above, rather than having to work with a feature subset chosen for him.
Linear regression is arguably one of the most widely used techniques in the data science world. But, a comprehensive understanding of this technique is not universal and it is at a level that is less than desired.,First,??a??little history, the term regression was first used by??,,??a 19th century polymath. Galton was a pioneer in application of statistical methods in many branches of science, he studied the relative sizes of parents and their offsprings in various species of plants and animals. During this study he observed that a larger than average parent tends to produce a larger than average child, but the child is likely to be less large than the parent in terms of its relative position in its own generation. Galton termed this phenomenon ?€?a regression towards mediocrity?€?.,Linear regression is an approach to model a relationship between a dependent variable (y) and one or more independent variables (x). In this paper, I discuss,.
An , by Vincent Granville posted to Hadoop360 introduces a formal method to generalize the notion of variance based on L^p norms. Whereas the formal generalization suggested in the article did meet several desired criteria, it left other desirable criteria unmet. In particular, there was no formal connection between the generalized variance and an associated generalized mean, and there was no guidance as to determining the existence of the generalized mean or variance for a given distribution. Leaving aside the motivation for stable, efficient computation, I wanted to find out if such a formulation was possible that would meet all desired criteria and would provide robust estimates of both locality and scale of deviation.,As it happens, such a generalization has recently been published in the open access journal Entropy by George Livadiotis of the Southwest Research Institute. In his , he introduces a generalized expectation value and an associated variance based on L^p norms. His construct differs from that of the more familiar Kolmogorov-Nagumo means, to which examples like the geometric mean and harmonic mean belong, and in which one computes a strictly monotone function at each data point, computes the arithmetic mean of the result, and then computes the corresponding inverse function of the mean. Livatiodis adopts the term ?€?quasiarithmetic?€? to describe this type of mean and to differentiate it from his own construct.,To summarize his construct concisely, we define the L^p mean m_p as the solution to the following balance condition:,Sum_k |y_k ?€? m_p|^(p ?€? 1) sgn(y_k ?€? m_p) = 0,For p=1, this condition is a ?€?mass balance?€? equation, where each data point can be considered as a unit mass placed on the left (?€?) or right (+) plate of a balance, thus rendering m_p as the median value of y.,For p=2, the condition is a ?€?torque balance?€? equation, where each data point can be considered as a unit mass placed on a balancing lever at a distance to the left (?€?) or right (+) of the fulcrum that is proportional to the data point value. The value m_p is the position of the fulcrum along the lever using the same distance metric and is the arithmetic mean of y.,For p between 1 and 2, we have a construct, the only one of which I?€?m aware, that allows us to make a???? smooth transition between the median and the arithmetic mean using L^p norms while preserving all of the desirable properties of a location parameter. In his paper Livatiodis proves that, in conditions where it exists, the L^p mean is unbiased. The L^p mean exists provided the (p?€?1)-norm of y over its distribution is finite.,Livatiodis defines the associated L^p variance V_p as a quantity with proportionality to the sum of p-deviations, Sum_k |y_k ?€? m_p|^p, so that its minimization, as desired, results in the balance equation given above. His actual construction is, in parallel to the standard variance, is:,V_p = E[(Y ?€? E(Y))L_p(Y ?€? E(Y))],,where E() denotes the expectation value, Y denotes the random variable generating y and L_p is an L^p norm operator defined so that m_p = E[L_p(y)]. This construct leads to:,V_p = [Sum_k |y_k ?€? m_p|^p] / [(p ?€? 1) Sum_k |y_k ?€? m_p|^(p ?€? 2)],For p=1, V_p is the mean absolute deviation (MAD) of y. For p=2, V_p is the standard Fisher variance of y. Livatiodis further shows that, using this form for the variance, the variance of the N-point L^p mean estimator approaches the actual L^p mean with error decreasing as 1/N, regardless of p, thus preserving a more generalized form of the central limit theorem. The existence of the L^p variance, however, requires that the p-norm of y over its distribution is finite.,Thus, we have a generalized L^p variance that, provided the variance exists for the distribution that generates the data, is stable to compute, has a clear connection to a mean value, and satisfies all seven of Granville?€?s desired properties, and then some. It does incur some computational cost, however, in that the solution to the balance equation is in general a nonlinear root-finding problem.

Interesting infographics??,. In the hot category, I would add data plumbing, sensor data to better predict Earthquakes, weather or solar flares, predictive analytics for flu and other health or environmental issues, automating data science and man-made statistical analyses, pricing optimization for medical procedures, customized drugs, car traffic optimization via sensor data, properly trained data scientists involved in decision and replacing business analysts, the death of the data silo.
Notes:,Happy new year. My first blog post here!,Note: In this post ?€? I am not interested in the Datalogix ?€? store card model. More to the implications of what it could mean for IoT .,Late last year, Oracle acquired a company called Datalogix ..,A??,???€? but with profound and disruptive implications,Datalogix does something very unique .. and had been on my radar especially for it?€?s relationship to facebook,This is very interesting and powerful,But lets think beyond store cards .. Think IoT / Beacons,So, my point is .. the model(independent?? of Datalogix the company) could be used to close the loop between the Physical and the social. IoT / Data Science / Data analytics will pay a key role here,Comments welcome - either here or on twitter??

The??,??is a Kaggle??competition ?€? with $175,000 in prize money and an opportunity to help improve the health of our oceans ?€? to classify images of plankton.,Domino is a platform that lets you build and deploy your models faster, using R, Python, and other languages. To help Data Science Bowl competitors, we have packaged some sample code into a project??that you can easily fork and use for your own work.,, it describes how our sample project can help you compete in the Bowl, or do other open-ended machine learning projects.

To be more precise, this kind of attack would rely on business hacking, rather than computer hacking. Other attacks, some potentially as massive as to turn Google into the worst search engine, are described below.,I believe that such an attack could be accomplished by an insider (disgruntled employee, or employee paid by an external organization), and benefiting from poor IT security at Sony. Especially the email part, consisting of cross-emailing every employee of interest (including executives) with scary messages, or messages aimed at generating massive lawsuits and firings.,Indeed, in many companies, it is easy for some employees to download the email database (or parts of it), the CRM database (clients with email addresses and associated internal contacts), or the hierarchical database of employees (who report to whom, email addresses, and job titles). Once this database is acquired by a rogue employee or consultant, it can be sold or used for many nefarious purposes: announcing (fake) layoffs, sending porn to executives with cc to external parties such as clients, exchange of racial insults between executives and clients (using spoofed email addresses) followed by fake apologies, and the list goes on. Before the ??culprit gets caught, the damage is done, billions of dollars have evaporated, and reputation is destroyed.,Never hire a guy that looks too smart. Never hire a guy like me who posts articles like this one. Identify a dangerous employee as early as possible (using predictive analytics), and use , to get rid of him. Make it impossible for anyone (employee or contractor or client) to download vast amounts of HR information. Beware that some rogue employees will accumulate the dangerous data very slowly, over a period of months. Detect and block spoofed email (originating from an IP address not in your white-list authorized to send internal email), as well as employees having infected computers (that can be used to send fake email that appear legitimate - not spoofed).,Plenty of silent, smaller attacks, happen daily as part of normal business activities. These attacks are aimed at killing business competitors. Sometimes these attacks are a reaction to what is perceived as ??unfair treatments by big companies or government. Combined together, these attacks result in far more damages than the Sony or Target hacks. These attacks might not even be illegal, unlike the Sony attack.,A digital company got banned or penalized on Google, and it feels as unfair punishing by the victim. Typically, the wrong punishing is the result of an algorithm defect. Algorithms have no feelings, and in the case of a big company like Google, there's nothing you can do about it: no customer support, no phone number to call, and anyway they deliver free traffic to your website. So they can pull the plug at any time for no reason, and make you loose all your revenue (too bad if you rely entirely on Google organic traffic for your revenue, not a smart business strategy). Occasionally, Google will??have to face??a tech-savvy victim that can and will kill all other competitors on Google, as a last resort to recoup losses and regain market share. There are techniques to succeed in such attacks, such as,You can go one level further and over time, corrupt massive amounts of Google search results. Especially if you are a kid (maybe one rejected after a Google job interview), and want to prove to the world that you are one of the greatest data scientists. You would need to target the top 100,000 keywords with the right amount of fake but well statistically distributed traffic. How to find these keywords? On Google itself, or just ask me. Such an attack could be called??a??,. It does not involve hacking into anyone's server, but algorithm intelligence and external data collection and processing.,Other companies such as Amazon or Yelp, extensively relying on user reviews, have made a lot of businesses very unhappy, due to undetected fake reviews. Rather than fixing their faulty algorithms, their policy is to further attack those few tech-savvy companies that they inadvertently (but unsuccessfully) tried to kill, once these little guys retaliate. These counter-attacks (e.g. by Amazon) are laughable, and make you think that they underestimate their enemies. I will provide an example in a future article. In the case of Amazon, attacks targeted at authors and publishers are starting to backfire, as articles against Amazon are getting a lot of traction recently.,Note that all these companies have stellar data scientists and other great employees. The problem is with some top decision makers, still acting as if we small people (their customers) are idiots that can easily be crushed on demand. That might be true for 99.999% of their users, but once you hit the 0.001% resilient ones ??- which will eventually happen when you make thousands of users furious every day - that's when you get an attack like the Sony one.,So even if you - the big company - are swamped by millions of customer messages every week, and can only provide automated answers, at least use an NLP-based algorithm to detect potential big trouble-makers. Not just automatically browsing what people write on your blog or to your mailboxes, but also on social networks, about your company. Follow thought leaders, they might be the first ones to detect a new trend, or new risks. Always be nice. Even when your algorithms randomly turn bad and mean against someone. Listen to what people say about your company, but learn how to filter out the vast amount of noise from these conversations, using both algorithms and analyses performed by human beings. The most dangerous hackers might be those who never say anything, so having well-behaved, human-friendly (as opposed to computer-friendly) algorithms to deal with your users, is always the best strategy.??,: This type of attack could also be used to manipulate the stock price of the target company, especially to short the stock before the attack and buy back after the collapse, to finance the operations of hacking or terrorist groups.,??
Today, an increasing number of institutional clients are looking for solutions, strategies and roadmaps to implement Big Data and Predictive Analytics initiatives within their own organizations. While the exact nature of the solutions and recommendations may differ from client to client, based on a number of factors, like the industry they operate in, the size of their operations and business model, there are common threads that can be applied to their needs.,While looking for these common threads, I came across an interesting white paper titled??,?? by??James Taylor (CEO, Decision Management Solutions) in which he shares his thoughts on the subject.??This blog post summarizes some of the key points that the author makes, along with some of my own thoughts from my engagements with both mid and large sized clients, within and outside the US, which I hope you find useful.,In the past, a predictive analytic model was generated using a single proprietary tool like ,for example??SAS,????against a sample of structured data. The model would then be applied in batch to generate scores for future use in a database or data warehouse.,Today, there is a focus on ,, that is, building models and applying these models in their day to day operations, turning the organization's data into useful, actionable insight (e.g. real time scoring) which can be used NOW to improve customer engagement, manage risk, reduce fraud, etc.,The , that organizations make while trying to build an Analytics and/or Big Data Strategy is ??focusing on the technology before understanding the business problems/opportunities??and decisions that need to be made. In other words, , You need to take the effort to better understand the insight and the underlying business problem that the insight will hopefully help solve. Once identified and well understood,??the desired insight will naturally drive the analytics and big data requirements.,The growth of Predictive Analytics has increasingly merged with the growth of Big Data. Increased digitization and the internet has exponentially increased the amount of big data available as well as the range of data types and the speed at which data arrives. This is commonly described as the "3 Vs": Volume, Variety and Velocity of Big Data.,Organizations are finding that the data they need for predictive analytics is no longer all structured data and no longer data stored in ??their databases and data warehouses. There is increasing evidence to support the notion that predictive , , when compared to analytic models built using structured data alone.,R is free and open source making it appealing as a tool to learn advanced analytics with. Because R is open and designed to be extensible, the number of algorithms available for it is huge with over 5300 packages today. Ironically. this proliferation of packages has led ??some to question ,??(but I digress...).,While scalability and performance has traditionally been an issue with R, commercial Vendors like Revolution Analytics are providing their own R??implementations for Big Datasets that overcome these limitations.,Hadoop consists of two core elements- the Hadoop Distributed file System or HDFS and the MapReduce programming framework.,While some newer organizations, like web 2.0 companies are putting all their data in Hadoop, a mixed database/data warehouse/Hadoop approach is more common. In the mixed environment, Hadoop is used as a landing zone for data where it can be pre-processed before being moved to a data warehouse. This allows for rapid addition of new data sources to an existing environment. Hadoop is also used as an active archive, where older data that might have been archived inaccessibly in the past to be available for analysis, can now be used to build predictive models.,??If predictive analytic models cannot be effectively operationalized and injected into operational systems, there is the risk that they will sit on the shelf and lose value.??Most analytic environments are batch oriented and are often loosely attached to the production environment. There is a need ??to move models built in a variety of analytic tools, into their production environments including workflow engines, business rules management systems, etc. PMML (Predictive Model Markup Language) has emerged as an important way to achieve this.
 

In this article we perform analytics on a huge dataset available from??,Finally let us compare the number of deals happening between today and 2000,??,During the peak of 2000, as high as 8000 deals were registered and comparing that to 2014, it stands at half value of roughly 4000 deals.??,??,Here is a??,??by each year, just select the year,??
Music fans with long memories probably recall the lamentable fiasco that was Woodstock 1999. In contrast to its '60s antecedent, Woodstock '99 was a cravenly commercial venture that replaced Jimi Hendrix and Janis Joplin with Kid Rock and Godsmack. Temperatures hovered in triple digits. Food and water were in short supply. Candles distributed for an anti-gun vigil were used to start plastic bottle bonfires. Limp Bizkit was prominently featured.??,It was the worst of times.??,Thankfully, the music world only needed to wait three months for something far better to come along. That thing? The Coachella Valley Music and Arts Festival, affectionately known as Coachella. It was held for the first time in October, 1999 and though the festival lost three-quarters of a million dollars and took the year 2000 off, it has been an annual fixture ever since. The aggregate lineup of the last decade and and half is a veritable Who's Who of popular music and the mere publication of each year's lineup is cause for both rampant speculation and breathless hyperbole. This year's announcement was no different. But with one of the more diverse and, frankly, , lineups in recent memory, what do music fans actually think of Coachella 2015? Let's find out.??,First, we scraped Twitter for hashtag , immediately after the lineup was announced. We asked our contributors to flag all the tweets that were from actual people as opposed to spammers and radio stations and ended up with more than 3,800 units. So far, so good.??,Next, we ran a sentiment analysis??job on those tweets, asking whether Twitter users were positive, negative, or indifferent about the lineup. Once we had that, we ran a data categorization??job where we had our contributors determine which bands and artists people were tweeting about.??,Then we looked at the data.??,Overall, Twitter users were really enthusiastic about the festival as a whole. We saw four times as many positive tweets than negative ones, which is refreshing considering Twitter's reputation for widespread disdain. But things got far more interesting when we looked at what artists people are actually excited about.??,Like, say, Drake. Man. People have a , of opinions about Drake. A little under a thousand of original tweets we looked at actually mentioned an artist, but 23% of those were about Toronto's favorite Degrassi High alum. To put it another way, Drake got 211 times the Twitter traffic as Built to Spill.,So what else did we learn about Drake-chella 2015? Well, with apologies to LL Cool J, ladies love cool Drake. Women were twice as likely to tweet something positive about , than men were and, overall, he received the most attention from women across the board. More men tweeted about AC/DC because more men think wearing shorts with a blazer is a good idea.??,Here's the drakedown:,If you expected the festival's headliners to get the most chatter, you'd be two thirds right. Drake and AC/DC received far and away the most attention on Twitter, regardless of sentiment or gender. In third place? Steely Dan.??,That's right, folks. Steely Dan, a band famous for its historical disdain of touring got about 50% more tweets than Jack White. And he's that last headliner.??,The bad news? Steely Dan received the higher percentage of negative tweets out of any artist. The good news? It was still only 25% negative. Far more people were either excited they'd finally be able to hear "Hey Nineteen" live or were simply perplexed that Steely Dan was playing Coachella. There were also a lot of jokes about dad rock. Can't forget about those.??,Out of the 162 acts playing Coachella this year, 113 got , kind of Twitter chatter those first couple days. And really, that's what makes Coachella special. You're not going to love it all. You might not even like most of it. But any time you can get Steely Dan and Lil B together for a common cause, you're probably doing , right.??

Data Scientists are a unique group. We come from a variety of backgrounds with a unique set of life experiences. At Booz Allen our data science team is now 500+ strong. We have the people you would expect ?€? those with degrees in math, stats, and physics. We also have a forester (me), a jazz musician, and a computer graphics major. There?€?s no set approach or ?€?one size fits all?€? model. We all took a different path to get to where we are today, but we all share one common trait ?€? we never get tired of the thrill of the chase.,You?€?ve felt it before; that moment you?€?ve been working toward for days, months or even years ?€? when an inspiration becomes an idea, an idea with the power to change everything. We live for these moments; moments that define who we are and leave an impact on the world. Moments that drive us forward and push us to better ourselves and those around us. They are the reason many of us found our way into a career in data science. ??,We?€?re lucky enough to experience that ?€?thrill?€? every time we solve a complex problem or reach that light bulb moment when the seemingly impossible becomes possible. We never tire of it. It keeps us coming back for more and encourages us to solve ever-harder challenges.,Last year, the idea of creating a national ?€?chase?€? for the data science community took shape. And so, in partnership with our friends at Kaggle, and with the encouragement of a broad community of supporters,??we launched the , last month. The goal is to give you the opportunity to have one more of these precious moments and to leave your mark on the world forever. ??,??,For the next two months, participants will be racing to the finish line -- reviewing about 100,000 images collected by scientists at OSU?€?s , and developing algorithms that will enable large-scale analysis and classification of the species within. These algorithms ?€? once complete ?€? will allow researchers to assess ocean health at a speed and scale that were previously impossible. Participants will have the opportunity to solve an extremely difficult problem while advancing the state-of-the-art in the marine science community. But if that?€?s not enough, the top competitors will split a cash prize pool of $175,000.,The challenge is not easy, but don?€?t feel intimidated. Whether you are an experienced expert or a novice, the competition provides an opportunity for you to contribute. The data science community has come together to create a variety of tutorials to get you started, from basic feature extraction to complex deep learning. And if you?€?re interested in creating or joining a team, just visit the Kaggle forums. There are multiple opportunities for you to partner with fellow data scientists to complete the challenge.,Many of you have already started the chase for your own moment. In fact, as of January 16,, we have had more than 3,200 entries from 400 teams. And in the first hour after we launched, the Kaggle platform received the ,. We thought that was pretty exciting. ??,With plenty of time left in the competition, I encourage all of you to throw your hat in the ring, test your skills, collaborate with the community via the Kaggle forums and make your mark. ??Whether a skilled veteran or a newcomer, everyone has an opportunity to learn something new, have fun and solve a little piece of the puzzle.
Hans Zimmer asks the question: "What if I could define a character in one note?",He spends his days running "experiments" and processing "Big Data" (the imaginary world painted in the film's script, his cultural and artistic background, etc.) using the distributed neuronal network architecture of his innate biological computer and extracts "models" comprised of emotionally-stirring themes full of rich patterns, context, and meaning "visualized" through the auditory medium (the Score) that either makes or breaks the ability of a producer/director to tell a story. ??Now, in order for his patterns to be effective in communicating the thematic message, they must strike a fundamentally common cultural chord within the audience that responds to the thematic information being conveyed, perhaps through some kind of shared library of archetypal cultural-emotional reference points (a shared prior), that facilitates some kind of visceral mutual appreciation.,One might think of the universe of all possible scores for a film being embedded into some kind of high dimensional space that also contains the score of interest (like the sculpture already present in the marble), and Hans essentially exploring this musical space through 'experiments' to find that subspace of specific themes that have a high degree of correlation with the various elements of the film. ??In a sense, he uses both a bayesian approach relying on his vast prior knowledge of themes, motifs, etc., a dimension-reduction approach by whittling away inessential elements, and adding raw innovations to identify those particular threads that have the best correspondence to the action in a particular scene. ??His objective function is essentially that which maximizes the energy of some kind of product of the cultural-emotional response against the physics of the scene for which the score is being developed. ??Being the non-convex space that it is, a lesser composer may perhaps settle for a nearby local maxima after an exhaustive search; whereas, the best composers do not rest until they land on the optimal solution that maximizes the emotive response to a particular scene or character.,His (/The?) solution: "...A taut string that gets tighter and tighter but never breaks.",...A completely different kind of "Data Scientist" from the world of Art. #MindBlown

It depends entirely on how broadly you categorize them. In reality, of course ?€? there are as many ?€?types?€? of data scientist as there are people working in data science. I?€?ve worked with a lot, and have yet to meet two who are identical.,But what I have done here is separate data scientists into groups, containing individuals who share similar skills, methods, outlooks and responsibilities. Then I grouped those groups together, again and again, until I was left with just two quite distinctly different groups.,I?€?ve decided to call these two types , and ,.,Just to be clear, individuals that fall into either of these groups will doubtless have a lot in common. But in order to best examine who these two types of data scientists are, and how they bring value to an organization, it?€?s obviously useful to focus on the differences.,Broadly speaking, a strategic data scientist will have a firm understanding of business performance and growth, strategic thinking and communication skills, but be less well versed in the technical, nitty-gritty of setting up database systems and defining or selecting algorithms.,On the other hand, the operational data scientist is more likely to come from a background of programming, statistics or mathematics, and will use these skills to implement systems to probe and interpret the data and draw out the most relevant results.,In other words ?€? and here we get to the crux of the difference, and see why both are essential ?€? the strategic data scientist sets the questions, and the operational data scientist provides the answers.,In order to drive positive organizational change (or business growth) through data analysis, asking the right questions and arriving at the correct answers are both essential parts of the process. Both are equally worthless without the other.,Once organizations have both the questions and answers needed, both ?€?types?€? of data scientist will then work with other members of the organization to put them to practical use.,(Some of you, perhaps if you have limited experience in the corporate world, might ask ?€? is it not the responsibility of senior management to ask the questions? Well ?€? unless you happen to have data scientists within senior management, which is often not the case, particularly at larger organizations ?€? no. They will generally just identify problems).,Pair a great strategic data scientist with a great operational data scientist and you have an unstoppable team, capable of crunching their way to the most useful and innovative insights. You might occasionally stumble into someone who has the qualities to fill both roles exceptionally well ?€? but in my experience this is rare!,Of course, it isn?€?t always essential to break down data scientists into these two types. Especially in smaller companies that might only employ one data scientist the distinctions will become much harder to make. Here is it particularly important to ensure any data scientist has the strategic business understanding as well as the data crunching skills.,As a final thought, I would like to say don?€?t be fooled into thinking that the split is along the lines of ?€?suits?€? and ?€?geeks?€? - this would be a lazy interpretation. Both are very much scientists, as long as they are formulating experiments in order to test hypothesis and record conclusions. Those looking to work their way up the career ladder in an organization with a large data science operation should certainly look to develop skills relevant to both top-level ?€?branches?€? of the data scientist tree ?€? while remaining mindful of where their own particular strengths lie.,I hope you found this post useful. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., : , is a globally recognized expert in strategic metrics and data. He helps companies and executive teams manage, measure, analyze and improve performance.,His new book is: 
?? ?? ??I am a newbie to Bigdata and would like to highlight some significant advantages if incorporated in a company's supply-chain management strategies, expecting the reader's views and suggestions.,?? ?? ??Because, in recent past I have developed a online supply-chain management systems in which sellers and customers are matched using an algorithm. It acted as a decision support system and I needed to dig deeper on the available data to get more insights over the data pattern (even for evaluating its correctness).,?? ?? ??We know that data is generated throughout the supply chain ?€? So, for sure the manufacturers of consumer goods or services analyze key performance indicators to ensure the resources are delivering at its capacity. So, there is a need for efficient data storage/management and information retrieval techniques.,?? ?? ??Current technology can track and capture data effectively and efficiently, even in vast quantities. Meaning that it can then be used to plan collectively to provide precise forecasting that ensures informed decisions can be made quickly. This type of analysis uses variety of data in huge quantities than ever before.,For example, a company can use big data to analyze petabytes of data on demand and supply, sales, identifying business insights etc.,,While thinking of SCM (Supply Chain Management), Big data analytics can also be applied even on:,?? ?? ??, can be done to predict market trends, demand & supply patterns over a period of time.,?? ?? ??, :A web-based automated procurement system with a procurement model using efficient matching algorithms, searching, negotiation and evaluation may improve supplier selection, price negotiation and supplier evaluation and the approach for supplier selection/evaluation.,?? ?? ??,: Volume of data can be used to ensure the correctness and effectiveness of supply-chain management by analyzing time to time. Key Performance Metrics can also be evaluated by looking for data patterns. These revolutionary analytics results in a predictable decision support systems.,?? ?? ??Because , forecasting may result in loss. If we make too much, we're wasting cash and losing profit. If we make too little, we're missing revenue. If you make it at the wrong time, we're probably getting hit by all three. It depends on the supply-chain factors. By implementing fluid demand and supply plans that are updated in real-time, based on true demand signals, material/resource availability and capacity, your revenue and profit potential is maximized.,?? ?? ??Industries get a huge benefit from analytics-driven insight that enable them to more intelligently track and manage. In recent trends, using automated data collection software to feed information into its big data analytics program, many merchandise retailer has dramatically boosted profits and more proactively met consumer demands.,?? ?? ??Since huge companies and industries are mainly focusing on big data analytics to improve their supply chain, primarily targeting to supply-chain management systems, since it is now better able to predict customer buying habits and track the effectiveness of special sales offers, last but not least "the demand".
In my last post, I explained the difference between what I consider the two core types of data scientist ?€? ,.,Broadly speaking, they require many of the same skillsets ?€? but the distribution of your expertise and experience within these skillsets will vary, depending on whether your role is more strategic, or operational.,So here?€?s an overview of what I feel are the five essential skillsets that are required from any data scientist who wants to be competitive in today?€?s market. Some of them are more valuable to organizations with a need for strategic planning of their data-driven enterprises, and some are more valuable for organizations needing people who are willing to get their hands dirty with the nuts-and-bolts mechanics of data.,However with a broad understanding of all of them, as well as an idea as to where your own particular strengths and interests may lie, you?€?re in a strong position to sell yourself to the growing number of companies looking to hire top-notch data talent.,Particularly for those whose calling lies towards the ?€?strategic?€? end of the spectrum, a thorough understanding of what keeps businesses ticking ?€? and more importantly, what causes them to grow ?€? is an essential field of expertise.,You should feel comfortable with the KPIs and metrics that business strategists use to evaluate every aspect of an organization, from its stock performance to its human resources. You also need to be able to evaluate what it is that makes your business thrive and stand out from the competitors ?€? and if it doesn?€?t, you need to have ideas about how to make it so.,I also include communication skills under this heading, although of course they are important across all disciplines. But particularly in business, the ability to clearly put across the ideas so that every member of the team knows what you are doing, why you are doing it, and how you are going to achieve it, is essential. ??,The ability to spot patterns, discern the link between cause and effect, and build simulated models which can be warped and woven until they produce the desired results is the domain of the both the operational and strategic data scientist.,Once your distributed storage is threatening to spill over with the reams of structured and unstructured data your machines have pulled in for you, it?€?s still going to take a human brain to make any sort of sense out of it. As such, you?€?ll need a thorough understanding of interpreting the reports and visualizations wringed from your reams of data.,You will need a grounding in industry-standard analytics packages such as SAS Analytics, IBM Predictive Analytics and Oracle Data Mining and a firm idea of how to use them to spot the answers to the questions you?€?re asking.,If an eager, fresh-faced graduate from college or high school has had any exposure to the world of data science before throwing themselves into the workforce, it will probably have been in the computer science lab.,Data is of course essential to everything that computers do, so it?€?s natural that those with an interest in programming, networking and system architecture often gravitate towards analytics and predictive modeling.,And it?€?s a good job, too ?€? as techie types are needed for everything from plugging together the cables to creating the sophisticated machine learning and natural language processing algorithms ?€? or whatever happens to be pushing the boundaries of what we can do with the help of our silicon-based assistants today.,In particular, candidates with a firm grasp of key open source technologies ?€? Hadoop, Java, Python etc. - are keenly sought, as these are the foundations of many organizations?€? plans to use data to dominate the world.,A statistician?€?s skills come into play in just about every aspect of an organization?€?s data operations. They will help to define relevant populations and appropriate sample sizes at the start of a simulation and to report the results at the end. Statistics (and its big brother, maths) is another academic wellspring from which gushes a torrent of talent into the data science workforce.,Whether your role is strategic or operational, a basic grasp of statistics is essential, but if you veer towards the operational, a more thorough education in the subject will be highly desirable.,Mathematics, too, will come in very useful ?€? despite the huge increase in the amount of unstructured and semi-structured data we are analyzing, most of it still comes out as good old-fashioned numbers.,In this sense, creativity is the ability to apply the technical skillsets mentioned above, and use it to produce something of worth (such as an insight), in a way other than following a pre-determined formula.,Anyone can be formulaic ?€? today, businesses want innovation that will set them apart from the pack, both in terms of their corporate results and the image they present to their consumers.,The possibilities made available by the application of data science are constantly evolving. With the explosion in the number of organizations realizing the advantages of leveraging data for insights that will prompt growth, people able to come up with creative methods of applying these skillsets will have a bright future ahead of them.,I hope you found this post useful. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., : , is a globally recognized expert in strategic metrics and data. He helps companies and executive teams manage, measure, analyze and improve performance.,His new book is: 
I thought I would share it on here in case anyone is interested.,The project can be found here:??,And the course I am doing is described here: ??,It would be fun to go into more detail for this, but I have more than filled the project brief and have run out of free time to add to it.
For any data science project, if you start with the wrong question, you are bound to end up with the wrong answer, and fail. Who should identify the right question? I believe data scientists should be involved in the process, otherwise, they will be held responsible for the failure.,We illustrate this problem in the context of vaccination: the CDC and other organisations want more people to be vaccinated in US, and they have interesting data to make their point about how vaccination is good and should be more generalized. Yet they are unable to convince a large minority of anti-vaxxers, despite all their efforts. Worse, it backfires, making these anti-vaxxers more convinced about not vaccinating, and spreading the word.,This discussion is not about whether vaccination is good or not, but about why public awareness campaigns failed to produce positive results, because of the following reasons:,We will review each of these issues.,The definition of anti-vaxxer is too broad: it seems to even apply to people who vaccinate (at least partially) their kids, who were vaccinated long ago against the worst 3 or 4 diseases, and who are not interested in flu shots, not for themselves. This is a large segment of the population, These people are not against vaccines in general, so the appellation anti-vaxxer is not correct, and part of the problem.??,If the CDC vaccination awareness campaigns (deployed by public relation agencies) result in making these so-called "anti-vaxxers" appear as idiots. But many of them are well educated, in good health, not living in crowded cities, and not relying very much on official healthcare: you are upsetting these people. ??It does not matter what ??the intents are, if it the target audience perceive it as bullying: it will not resonate and even backfire. Every marketer knows that, and every marketer tests the results of all their campaigns (they also do A/B testing), and adjust their messages and channel distribution accordingly.,Part of the problem is the fact that the CDC is of monopoly, when it comes to collecting data about diseases, and handling vaccination issues. As most monopolies, they might not care about the impact of their messages. Just like electricity companies sending letters to heavy customers to tell them that they are bad neighbors, and that they should reduce consumption, without knowing the cause (e.g. your house is much bigger than neighbors homes, with many kids).,Those who want to attack the root cause of the anti-vaccination movement believe to have identified the reason: anti-vaxxers believe in the fact that vaccines cause autism.,Now, not only can't they identify real anti-vaxxers (see section 1), but they arbitrarily decide on a cause. There is no scientific evidence proving that anti-vaxxers believe in autism, conspiracy, hazards linked to mercury, or that their rejection is based on religion. It might be true for a small minority, but not for the majority. It would be easy for a data scientist to conduct a survey, to identify the real causes of vaccination refusal. The causes could be:,As a result, disproving the myth that vaccines cause autism, is a waste of time and money, since anti-vaxxers don't believe in this theory in the first place. Data scientists and domain experts should try to understand the real causes.??,And of course, there will always be 5% to 10% of the population that distrust science and propaganda enough, that convincing them to get vaccinated is a waste of time, and an attack against their lifestyle. See section 6 in this article. And true autism believers - the minority of anti-vaxxers - can't be converted. Again data science should easily prove this.,It is doubtful that any A/B or multivariate testing was done to identify which messages and channel combinations work best, to convert anti-vaxxers. Likewise, no yield analysis was performed to measure the success of these campaigns. It looks like conversion rates were very small, and it backfired by possibly creating more adversarial people - in short, losing more pro-vaxxers than converting anti-vaxxers. The target market (US residents) was not segmented: the exact same message was broadcast over and over, unchanged, to all segments of the population, with no attempt at per-segment customization, or at avoiding the dangerous segments. In short, the yield over the baseline - that is, not running any campaigns at all - is negative.,This is compounded by Facebook paid-to-post pro-vaxxer trolls, who will copiously insult anyone who disagree with their masters, ,. Public relations agencies are known to use them (some may be bots).,It is surprising that life and health insurance companies do not offer a discount to people who are vaccinated. These companies (life insurance at least) are very good at market segmentation to optimize premiums based on health risks.??,At the end, it leaves the anti-vaxxer with a sour feeling that she is being manipulated by some sort of a vaccination cult. The fact that no one at the CDC really understands why educated, healthy people in NYC and Malibu are getting more and more reluctant to vaccinate, makes you think that the CDC's workforce is not diversified. Many of its hires might be risk-adverse, and most of their scientists need a security clearance. Nothing bad with that, but it makes the workforce not diversified - and they are in a monopoly position. It reminds me of Philip Morris (cigarette manufacturer): they would only hire smokers who would say good thinks about cigarettes (the first face-to-face job interview question used to be: do you want a cigarette?) ?? ??,How are statistics about vaccinated people computed? Is there a centralized database? What about parents who lie on their application form, when enrolling a kid in a public school? And what about those with vaccinated kids, but who prefer to claim religious exemption, because they don't remember where / when / for what their kids were vaccinated, maybe because they moved out of state. I'm not saying that the data is significantly inaccurate, but errors might impact statistical significance of some results - it is important to know how accurate your data is, especially if data is provided by schools, or comes from surveys.,When my wife enrolled last year in a university program, she was asked about her vaccinations. She never provided evidence of the required vaccinations, and the school eventually dropped the issue (after all, they'd rather get $24,000 in tuition, than argue about some paperwork). My wife might have been properly vaccinated, who knows, but at 50 years old, few people remember anything about past vaccinations. It's a fact of life. ????,Finally, is the increase in (say) measles cases entirely attributable to unvaccinated people, or are there other causes explaining this trend? Increased density population could be a factor, easy to rule out (or not) using statistical models. Virus mutation another one. If other causes are found, they must be addressed.,When an organization publishes statistical statistics, building and keeping trust is a paramount, or soon, nobody believe in your reports anymore. Unemployment statistics from the BLS is a good example. Many people take it with a grain of salt: lower unemployment rates are masked by low-paid jobs, college graduates with high student debts and overqualified for the few jobs they can find, and people not fully employed. Again, it's a question of definition: how do you define "unemployed", and is the metric, as currently defined, of any use to measure the strength of the job market? Who should come with a better metric?,In the case of the CDC, it would be great to see how good they have been, regarding past predictions. Looking at their website, I see similar issues. One of their featured articles this week was about binge drinking and alcoholism. Not only the meaning of "binge drinking" varies by individual: many drinkers ??can drink 5 times more than the "heavy drinker" threshold defined by the CDC (14 glasses of wine a week) and be perfectly fine. A good chunk of these "spectacular drinkers" are never detected because they are never arrested and never cause any problems. They don't show up in the statistics, causing a bias. The article also has a chart (see below) that tries to convince you that the problem is worst for White people. However, the high 68% is because there are far more Whites than (say) American Indians. The issue is indeed far more severe for American Indians. Looking at the chart, you feel that it is misleading, just misinformation. And you are wondering how and why they collect data about race.,Your next question is then: what about the claims regarding vaccination? Is the reality also distorted?,There is a segment of anti-vaxxers that is not worth reaching out to: you won't convert them. Just like there is a proportion of the population that will never be able to read, no matter how much money is spent to train them. That's the baseline - the people that can't be converted no matter what, and especially when making the false assumption that they believe in the link between autism and vaccination.,Likewise, you will never be able to stop some people from rock climbing, or driving a car, or doing a barbecue, or playing with fireworks, despite the risks (higher risks than not being vaccinated). A better strategy might be to identify the population segments properly. Some people are open to receiving a limited number of vaccines, but the current anti-vaxxer propaganda, by bullying them, turns them off, or worse, makes them become more bold. Maybe there is a way to better reach out to them, and offer reduced vaccination schedules (only the most useful vaccines, via trusted channels) to at least better protect the entire population.,Data science should help find better approaches to deal with this problem, with "domain expert" data scientists participating in executive meetings, and in the design of databases and metrics to be tracked, including the definitions of terms such as anti-vaxxer, and data collection processes.
Human resources analytics can provide businesses the keys to improving processes, reducing workforce costs and making the right policy changes to improve efficiency. Leaders in the space and speakers from the upcoming Workforce and HR Analytics Summit West 2015 conference from companies like Visa, Wells Fargo, PepsiCo??say the best place to start is to turn analytics into insightful action through business understanding.,Developing business insights and aligning HR analytics to core business operations largely rests on the shoulders of HR leadership. The key to a successful start is their relationship with other business units to understand challenges and target analysis toward solving those concerns.,Our thought leaders stressed the importance of developing a full picture of HR and noted that the success of analytics requires a specific set of actions:,Proper support begins before any analysis by having leaders work closely with other business units to understand their challenges and priorities. Understanding gives validity to analytics because it provides context for metrics?€? efficacy,???? thresholds and measures of success.,.
High resolution version can be found via ,??or ,.
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
This is an announcement regarding my upcoming book: ,. The subtitle is ,.,Just like ,, it will first be available as a free PDF document to members of our community. It will feature,You can already have a peek at the content, ,??and looking at the double-starred articles: they will be incorporated in our new book. I will complete the listing (of contributions to be added to our new book, the double-starred articles) in the next few weeks. A link to the eBook (once the first draft is completed) will be provided here. So bookmark this page!
 , , , , , ,Now I am writing Bigdata and datascience articles on what to do, on upcoming days I am going to highlight the tools and analytical techniques for capturing and analyzing volumes of data with examples on respective domains., 
Often, Data Science for IoT differs from conventional data science due to the presence of hardware.,Hardware could be involved in integration with the Cloud or Processing at the Edge (which Cisco and others have called??,).,Alternately, we see entirely new classes of hardware specifically involved in Data Science for IoT(such as??,),Hardware will increasingly play an important role in Data Science for IoT.,A good example is from a company called??,??which natively implements classifiers(unfortunately, the company does not seem to be active any more as per their twitter feed),In IoT, speed and real time response play a key role. Often it makes sense to process the data closer to the sensor.,This allows for a limited / summarized data set to be sent to the server if needed and also allows for localized decision making.?? This architecture leads to a flow of information out from the Cloud and the storage of information at nodes which may not reside in the physical premises of the Cloud.,In this post, I try to explore the various hardware touchpoints for Data analytics and IoT to work together.,Comments welcome,Image source:??


The?? purpose of Financial Modeling is to build a Financial Model which can enable a person to take better financial?? decision.The decision could be affected by future cash flow projections , debt structure for the company etc. All these factors may affect the viability for a project or investment in a company.The Applications of Financial?? Modeling mainly includes the followings :,Basic Financial Analysis tools include,In??Financial Modeling??it is desired that the working should be error less and should be easier to read and understand for audit purposes. By following these key principles, model will be easier to navigate and check, and reliable.,The following points should be kept in mind:,a) Financial Modeling??,??For most companies revenues are a fundamental driver of economic performance. A well designed and logical revenue model reflecting accurately the type and amounts of revenue flows is extremely important. There are as many ways to design a revenue schedule as there are businesses. Some common types include:,b)??,??Drivers include:,c),d)??,??(or Net interest expense):,e)??,:
I always make the point that data is everywhere ?€? and that a lot of it is free. Companies don?€?t necessarily have to build their own massive data repositories before starting with big data analytics. The moves by companies and governments to put large amounts of information into the public domain have made large volumes of data accessible to everyone.,Any company, from big blue chip corporations to the tiniest start-up can now leverage more data than ever before. Many of my clients ask me for the top data sources they could use in their big data endeavour and here?€?s my rundown of some of the best free big data sources available today.,The US Government pledged last year to make all government data available freely online. This site is the first stage and acts as a portal to all sorts of amazing information on everything from climate to crime. To check it out,,A wealth of information on the lives of US citizens covering population data, geographic data and education. To check it out, click here. To check it out,??,As the above, but based on data from European Union institutions. To check it out,,.,Data from the UK Government, including the British National Bibliography ?€? metadata on all UK books and publications since 1950. To check it out,??,.,Information on history, population, economy, government, infrastructure and military of 267 countries. To check it out,??,.,125 years of US healthcare data including claim-level Medicare data, epidemiology and population statistics. To check it out,??,.,Health data sets from the UK National Health Service. To check it out,??,.,Huge resource of public data, including the 1000 Genome Project, an attempt to build the most comprehensive database of human genetic information and NASA?€?s database of satellite imagery of Earth. To check it out,,.,Although much of the information on users?€? Facebook profile is private, a lot isn?€?t ?€? Facebook provide the Graph API as a way of querying the huge amount of information that its users are happy to share with the world (or can?€?t hide because they haven?€?t worked out how the privacy settings work). To check it out,??,.,Compilation of data from sources including the World Health Organization and World Bank covering economic, medical and social statistics from around the world. To check it out,??,.,Statistics on search volume (as a proportion of total search) for any given term, since 2004. To check it out,??,.,40 years?€? worth of stock market data, updated in real time. To check it out,,Search and analyze the full text of any of the millions of books digitised as part of the Google Books project. To check it out,,Huge collection of environmental, meteorological and climate data sets from the US National Climatic Data Center. The world?€?s largest archive of weather data. To check it out,??,.,Wikipedia is comprised of millions of pieces of data, structured and unstructured on every subject under the sun. DBPedia is an ambitious project to catalogue and create a public, freely distributable database allowing anyone to analyze this data. To check it out,??,.,Free, comprehensive social media data is hard to come by ?€? after all their data is what generates profits for the big players (Facebook, Twitter etc) so they don?€?t want to give it away. However Topsy provides a searchable database of public tweets going back to 2006 as well as several tools to analyze the conversations. To check it out,,.,Mines Facebook?€?s public data - globally and from your own network - to give an overview of what people ?€?Like?€? at the moment. To check it out,??,.,Searchable, indexed archive of news articles going back to 1851. To check it out,,.,A community-compiled database of structured data about people, places and things, with over 45 million entries. To check it out,??,.,Metadata on over a million songs and pieces of music. Part of Amazon Web Services. To check it out,??,.,I hope this list is useful and you now agree with me that a lack of data is not a valid excuse for delaying any big data initiatives? As always, feel free to comment and add any other of your free big data sources to this list using the comment field below.
Here is my selection of time-insensitive ,,??that were viewed more than a thousand times each, in 2014.??,Single-starred articles??are written by external/guest bloggers. Our upcoming book on??,????will be based on some of these (edited and revised) articles:??these articles are double-starred, with red starring??,. Double-starred articles, with blue starring??,, were published??,.,Here's a snapshot, corresponding to the week of December 19:,??to access the full list, containing hundreds of articles.??
Want to apply Data Science to Business Process Management? Have a look at process mining, explained in this poster!,.
Imagine the following business problem:,A call center has a rule that if more than 8 customers calls in 24 hours about Issue X, then there should be an alarm & that that Issue X should be forwarded to Tier 2 team for further investigation. However, the Tier 2 team believes that 24 hours is too long to wait since the customer experience could suffer. They want to predict BEFORE the 24 hour interval. Therefore, they want the probability at any given time based on historical hourly calling pattern if the 8 customers calling in 24 hours will be breached?,Enter the Poisson distribution. ,.,The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare. How many such events will occur during a fixed time interval? Under the right circumstances, this is a random number with a Poisson distribution.,A discrete random variable??, ??is said to have a Poisson distribution with parameter , > 0, if, for , = 0, 1, 2, ?€?, the probability mass function of , ??is given by:,where,The positive real number , is equal to the expected value of , and also to its variance,Scipy is a python library that is used for Analytics,Scientific Computing and Technical Computing. Using , we can easily compute poisson distribution of a specific problem,To calculate poisson distribution we need two variables,??,We can check the Poisson distribution values with an online ,????
Feel free to share and leave questions/comments
The sniper scopes in on a young grinning child holding up a gold Rolex. Its a long shot. To his right is the ?€?institutionalist?€? and to his left is the ?€?realist?€? both brandishing armor and holding a plethora of weapons. Their weapons are visible yet ineffective at this distance. The sniper is too far away way, way too accurate; deadly accurate. Yet at least one will survive as the effect of a long distance bullet is obvious, sudden and frightening.??,The dark figure in crime data is the coveted elusive holy grail of crime data. This dark figure or formula determines the number of people that get away with a crime. If found this figure alone has the ability to change society as we know it today. It has the ability to cause large scale rebellion that makes the Civil War look primitive. Little has changed in the 1000 years since the first All Thing and clearly change is needed. Data may be the brush to paint this figure. Yet this is not easy as it may seem.??,Our collective excuses are numerous. Claiming the existence of no perfect measure and assuming that everyone in a high crime area is guilty could be looked at as excuses. Faulty Racial profiling and cleared cases are convincing testimony to a failing system. If you want more the flawed victim dependent testimony leaves some people feeling lucky that they are not under the skirt of the dark figure. The large ominous budgets supported solely by known reported crimes mostly discovered and reported by insiders or witnesses is just the fringe of the jungle.,Real glimpses are revealed through the shadows of trauma, stress and horror by victim reports. Another glimpse is potentially through controversial reports like Dohahue and Levitt relating abortion numbers to crime rates. More over as the population clock ticks forward the dark figure grows in power causing a widening separation of crime and justice.??,2012 Federal crime data was just released; its 2015. This data assembled at an enormous cost with questionable accuracy creates a confusing fog over the whole data jungle. Leaving heroic family members to shoulder the burden of protection on the young.?? The mass of threats are growing with no foreseen improvement. What are we paying for when the odds of survival are better in a real jungle than in a high crime area.,Just getting to the data with the right level of detail is difficult. The Law steps on itself prohibiting access requiring scrutiny in order to show any kind of detail. For example with data sets that contain living people there are ethical considerations. Personal data cannot be geolocated in masse without permission of the living people. This also means it is illegal to track a person showing a geocode. Regardless blatant disregard for personal rights is common and online sell this data to anyone for a dollar. In contrast data on criminal is owned by the criminal. Data on the crime in is owned by the collecting agency. Leaving an open debate for improvement in these laws. The public pays for the criminal to be incarcerated so there is big potential for a lot of different options here too. There are so many variables and typically laws are broken first then adjusted.?? A gross example of this is some of the recent crime map applications and cyber defense. And not to mention recent applications that track police locations.??,Ideally justice is sold to the public as ethical and accurate. In reality there is limited useable public data at any level of detail to be used with other data sets. For what ever reason this is it shows a great potential for improvement. The use of hard accurate timely data is used in other areas of government to improve. For example Traffic accident data is used to improve intersections, traffic flow, motorist and pedestrian safety.??,Then maybe there has to be a crisis to change the way crime and criminal data is mixed and evaluated. One thing for sure there is real human value waiting to be revealed that can be used to improve systems and public perspectives. The question becomes if improvement can happen naturally in other parts of government there is hope for the future. Clearly the current justice environment keeps the wheels of commerce turning. And how effective is it really only the dark figure knows. It is hard to tell when we are all in the shadow of the dark figure.??,So in conclusion, calling all super heroes this dark figure is growing bigger by the day. For sure our public systems of justice have some tough questions ahead and cannot keep up a level l of quality with the current demand. The dark figure is elusive yet real and barely defined. Tough questions that have to be answered quickly and effectively to be successful. The juggling act eventually comes to a an end leaving all of us to carry burden of failed systems.
Data Scientist communities have their own complex jargon; multivariate regression models, Big data engineering, Hadoop, Map Reduce, Deep Learning etc. But, unfortunately businesses do not seem to care about how complex the term is or how impressive the math is! They want the results explained in non-tech terms.,While working on Big Data & planning to implement it for the benefit of business, it is very important to explain the insights & valuable knowledge in a way that non-technical business user can actually understand.,Here is my recent experience while working on a project for one of the largest food retailers. The goal of this project was how incentivisation would help improve their overall profits.,After an extensive and impressive study, , came up with a collection of (what they thought) elegantly done slides. They discussed deeply about variance inflation factors, Akaike information criterion that would scare even seasoned practitioners of the art.,Now the client did not have a clue of what was going on during the presentation and rushed and escalated to me. I had to work several days De-technifying the slides! And make them business friendly,More often than not, I notice that I spend 50% of the time processing and cleaning the data and 20-25% of the time De-technifying the results and tell stories. Interestingly, while I find enough doers, the story tellers who understand the subject and business at appropriate depth are rare.,Unfortunately, this skill is missing in traditional MBAs & Managers as this is not a peripheral exercise of language. A fairly deep understanding of Data Science must be coupled with a even deeper research of the client organization.,Need to rush now. But, I shall talk about my thoughts on how to solve this problem on a later post.,Contd,Article idea & guidance by - ,Script, Design & Edited by - Suman Malekani
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
Over the course of a startup?€?s lifetime, a company can face a range of data challenges. In the beginning, startup founders do most analysis themselves using tools like Google Analytics. As a company grows, business and product leaders often have analytical questions that these tools?€?and busy executives?€?can?€?t answer.,For many companies, reaching this point is a signal to start building an analytics team. But who do you hire? An analyst might be able to deliver value right away, but will she be blocked because nobody?€?s focused on collecting and organizing your data? A data engineer could solve these problems, but will it take too long for her to be able to answer your critical business questions? Should you look for someone who can be a jack-of-all-trades?,Even after the first hire, these questions don?€?t get any easier. As analytics teams begin to help more areas of the business, more analysts?€?and often ones with different skill sets?€?are needed. But so, too, are data engineers who can integrate data from different data sources, maintain data pipelines, and handle a growing data infrastructure.,I?€?ve seen a number of startups go through this process. Though every company has its own unique needs when considering its first?€?or next?€?analytics hire, there are some universal questions that can help leaders make the right decision about who to hire next.,??,If you can get sufficient answers to most of your questions using existing data from a single source, an analyst can start finding insights on day one. It?€?s incredible what you can do with just an event stream; in many cases, one or two tables might be all you need to??gain a deep understanding of your users. Similarly, it?€?s possible to go a long way using only data from your production database.,If you?€?ve only been tracking events in a product that doesn?€?t make data accessible for ad-hoc analysis (Google Analytics, for example), an analyst might be at a disadvantage. Similarly, if you?€?re starting to ask questions for which analysts need to combine data from multiple sources, they?€?ll be able to analyze the sources individually but will hit roadblocks when bringing it all together. At both stages, you might be better served by a data engineer who can build a data warehouse and/or data pipeline.,Of course, you might not know what state your data is in; it can be hard to determine without an analyst. Talk to some friendly analysts at other companies to get a sense of where you stand.,??,When beginning to answer business-critical questions with data, you?€?ll want to understand your precision comfort level. For many startups we know, the level is somewhere around ?€?precise enough to make a sound decision.?€? Imagine you?€?re faced with a dilemma: you need to better understand a particular part of your product but it?€?s not instrumented particularly well. You have two choices: you could get a quick approximation using the data you do have or you could re-instrument the product?€?and wait for data to roll in?€?to get an exact answer. If approximations will work most of the answers you need right now, an analyst will be able to develop the guide-posts you need.,Even if precision isn?€?t paramount today, keep in mind that as your company grows, you?€?ll likely encounter situations everywhere on the precision spectrum. Whomever you hire should be able to determine the level of precision that different problems demand, and be able to solve (or delegate) problems accordingly. In other words, for some problems, she should be comfortable providing quick, imprecise answers; for others, she needs to be capable of doing rigorous mathematical work. She?€?ll also need to recognize that precision requirements for certain types of problems may need to change over time.,??,Sure. If you?€?re thinking about technical skills alone, that is.,Ask an engineer what an analyst does and you?€?ll hear about math, stats, and defining metrics. But often engineers don?€?t understand the job completely: developing strategic questions?€?and figuring out ways to answer them quantitatively?€?is where an analyst shines. Many of the best analysts we know come from social sciences and are able to frame problems and think of answers in terms of trade-offs.,That being said, starting your analytics team with an engineer can be advantageous, (more on that in an upcoming post). In this situation, the person corralling data is also the end user. Your data structures are likely to evolve in a pragmatic way that analysts will find flexible down the road.,When I asked friends about the analyst/engineer hybrid, it elicited very polarized responses. One person argued that there exists an ideal balance?€?an analyst who understands the technical underpinnings of the product she works on (but is an analyst at heart). Another argued that finding such a person would take forever, especially if you throw in that this person has to be a great communicator as well. I?€?m not firmly in one camp or the other (clearly there?€?s no right answer), but I do have some advice if you choose to go the hybrid route:,If you find someone who can build a data warehouse??, analyze its data, think about what you would want that person to work on if the team grows to five people. If you think she?€?d become a data engineer, compare her to the best engineering candidate you?€?ve met; if she?€?d become an analyst, compare her to the best analyst candidate?€?just to make sure you?€?re not settling for someone who is mediocre at both.,??,In any growing startup, there?€?s a natural pull for engineers to work on customer-facing products. This will affect your fledgling analytics team:,If you?€?re planning for a data engineer hire, protecting her can be as simple as reporting structure. Consider having your new hire report to someone with priorities other than engineering. At first, this could be the VP of product or, eventually, a Director or VP of analytics. Keep in mind that, especially at early stages, the people you hire and the structure you put in place will heavily influence that team?€?s priorities and culture.,??,While you?€?re working your way through the hiring process, you may be able to leverage members of your current team. Look to someone in finance or business development to find insights using Excel and consider pulling in an engineer to tidy up data and event tracking.,One thing to keep in mind: Analysts can learn technical skills. If you have a great communicator who can frame problems with the right context, they might be champing at the bit to add skills to their tool-belt. SQL is easy to learn. Other common languages (R, Python) are slightly harder, but approachable by the right person. It might not be right for your first hire but it could?€?especially if you have an experienced team member who can act as mentor.,However you proceed, be sure to keep the above questions in mind and make sure you?€?re setting your new hire up for success.,This post originally appeared on the Mode Blog:??
Spatial Visualization Using R:??One of the less understood aspects of R is in spatial data visualization. The below article will outline two case studies on using R to spatially visualize data.,??,Our first step is figuring out how to use the Census API within R. Given below are the key data Source Details from the Census ACS Data, ???????????????????? Access the Census data using the API. You can , ???????????????????? For , ???????????????????? ,??,We use the acs.lookup function & use the keywords to find the required data across all ACS tables. ????For example, the following are the search results for the keywords owner, occupied, and median.,>acs.lookup(endyear=2012, span=5,dataset="acs", keyword= c("owner", "occupied", "median"), case.sensitive=F),An object of class "acs.lookup",endyear= 2012 ??; span= 5,variable.codetable.number ?????????????????????????????????????????????????????? ??????????????????????????????table.name,1?????????????????? B25021_002 ???? B25021 ???????????????????????????????????????????? MEDIAN NUMBER OF ROOMS BY TENURE,2?????????????????? B25037_002 ???? B25037 ???????????????????? MEDIAN YEAR STRUCTURE BUILT BY TENURE,3?????????????????? B25039_002 ???? B25039 MEDIAN YEAR HOUSEHOLDER MOVED INTO UNIT BY TENURE,4?????????????????? B25119_002 ???? B25119 ???????????????????????????????????????????? Median Household Income by Tenure,??,Using the Choroplethr package make it really easy to create thematic maps in R. There is native support for Choloropeth?€?s from US Census data & it can be accessed by the choroplethr_acs function. The R Choroplethr package does not store data locally. Instead, it uses the ??Census API from the R ACS Package to get the ACS data. Here is an example to use the Census API key to get the data using choroplethr.,>library(acs),>api.key.install(key=" your secret key here"),>choroplethr_acs("B01002", "state", endyear=2012, span=5),Table B01002 has 3 columns. ??We have shown the median age of each state with different gradients for each each age bracket in the below Cholropeth map.,The second case study is using??Home Insurance Rates data by vHomeInsurance,??and using GGMap, to show average home insurance prices for some of the most populated cities in the US. The map will show more expensive home insurance cities in bigger circles and less expensive cities in smaller circles. For example, Tampa homeowner insurance??is $1100 and in a bigger circle than Denver??which is cheaper at $860.,??,#Reading the Home Insurance Data from File,>mapdata<-read.csv("average_home_insurance.csv",header=T),>install.packages("ggmap"),>install.packages("mapproj"),>library(ggmap),>library(mapproj),>map<- get_map(location = 'US', zoom = 4),>ggmap(map),> TC <-ggmap(map)+geom_point(data=mapdata,alpha = .7, aes(x=longitude, y=latitude,size =Home.Insurance),color='red')+ggtitle("Average Home Insurance By City($)"),> TC
, we looked at some of the top locations in the US which had data scientists and also the top companies who existed in our LinkedIn Networks in the ,. ??In this article, we look at the top hirers of data scientists to see where the job market is hot. We also look at the top 2 locations ?€? San Franciso-Bay Area & New York to see the companies looking at people.,Across the United States here are the Top 15 hirers for Data Scientists:,??,Below are the top companies recruiting for data scientists. As expected, the top ranks are made up of technology and internet/e-commerce companies.,??,New York is dominated by Financial service firms but there is a fair representation from consulting, technology & media firms as well.,??,Are we missing any top hirers? Let us know?
Since it was founded in 1975 by Bill Gates and Paul Allen, Microsoft has been a key player in just about every major advance in the use of computers, at home and in business.,Just as it anticipated the rise of the personal computer, the graphical operating system and the internet, it wasn?€?t taken by surprise by the dawn of the big data era. It might not always be the principle source of innovation, but it has always excelled at bringing innovation to the masses, and packaging it into a user-friendly product (even though many would argue against this).??,It has caused controversy along the way, though, and at one time was called an ?€?abusive monopoly?€? by the US Department of Justice, over its packaging of Internet Explorer with Windows operating systems. And in 2004 it was fined over $600m by the European Union following anti-trust action.,The company?€?s fortunes have wavered in recent years ?€? notably, they were slow to come up with a solid plan for capturing a significant share of the booming mobile market, causing them to lose ground (and brand recognition) to competitors Apple and Google.,However it remains a market leader in business and home computer operating systems, office productivity software, web browsers, games consoles and search ?€? Bing having overtaken Yahoo as the second most-used search engine.,It is now angling to become a key player in big data, too ?€? offering a suite of services and tools including data hosting and analytics services based on , to businesses.,But Microsoft had a substantial head-start over the competition ?€? in fact their first forays into the world of big data started way before even the first version of MS-DOS. Gates and Allen?€?s first business venture, two years before Microsoft, a service providing real-time reports for traffic engineers using data from roadside traffic counters. It?€?s clear that the founders of what would grow into the world?€?s biggest software company knew how important information (specifically, getting the right information to the right people, at the right time) would become in the digital age.,Microsoft competed in the search engine wars from the beginning, rebranding its engine along the way from MSN Search, to Windows Live Search and Live Search before finally arriving at Bing in 2009. Although most of the changes it brought in appeared designed to ape the undisputed champion of search Google (such as incorporating various indexes, public records and relevant paid advertising into its results) there are differences. Bing places more importance on how well-shared information is on social networks when ranking it, as well as geographical locations associated with the data. ??,Microsoft?€?s Kinect device for the Xbox aims to capture more data than ever from our own living rooms. It uses an array of sensors to capture minute movements and is already able to monitor and record the heart rate of users, as well as activity levels. Patent applications suggest there are plans for much wider use, including monitoring the behaviour of television viewers, to provide a more interactive watching experience. The move fits in with Microsoft?€?s strategy of rebranding the Xbox ?€? generally thought of as a games console ?€? into an intelligent living room activity hub which monitors, records and adapts to users?€? behaviour. No, you are not the only person who finds that idea a little bit scary!,In the business-to-business market, where Microsoft made its first fortunes with its OS and office software, it is now throwing all of its considerable weight into big data-related services for enterprise.,Like Google with its Adwords, Bing Ads provides pay-per-click advertising services which are targeted at a precise audience segment, identified through data collected about our browsing habits.,And like competitors Google and Amazon it offers its own ?€?big data in a box?€? solutions, combining open-source with proprietary software to offer large-scale data analytics operations to businesses of all sizes.,Its Analytics Platform System marries Hadoop with its industry-standard SQL Server database management technology, while its ubiquitous Office 365 will soon make data analytics available to an even wider audience, with the inclusion of PowerBI ?€? adding basic analytics functions to the world?€?s most widely used office productivity software.,It is also looking to stake its claim on the , with Azure Intelligent Systems Service. This is a cloud-based framework built to handle streaming information from the growing number of online-enabled industrial and domestic devices, from manufacturing machinery to bathroom scales.,It may have missed a trick with mobile ?€? prompting many premature declarations that Microsoft was falling behind the competition ?€? but its keen embrace of data and analytics services show that it is still a key player.,When CEO Satya Nadella took up his post at the start of this year he emailed all employees letting them know he expected huge change in the industry, and the wider world, very soon, prompted by ?€?an ever-growing network of connected devices, incredible computing capacity from the cloud, insights from big data and intelligence from machine learning.?€?,So it?€?s clear that Microsoft aims to put big data at the heart of its business activities for the foreseeable future, and provide (relatively) simple software solutions to help the rest of us do the same.,I hope you found this post useful. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., : , is a globally recognized expert in strategic metrics and data. He helps companies manage, measure, analyze and improve performance.,His new book is: 
Digital banking offers unprecedented opportunities both for banks as well as customers. It has provided banks breadth and ease in delivering their products and services to customers. As for customers, it has pushed service options and expectations to a new high. Given the opportunity spectrum, it is no surprise that banks have invested heavily in upgrading their digital preparedness. 2015 promises more investments in digital banking.,A definition of digital banking will be of help here. Obviously many definitions exist. Banking transactions and experience through the internet and electronic devices - desktops, laptops, tablets and mobile phones ?€? structure our digital banking experience. This is a simple and easy definition.,Digital banking is also the prime force that has helped in proliferation of banking services to remote areas and brought huge unbanked segments into the ambit of mainstream banking. Arguably, it has the deepest transformational impact on any industry. Most important, it has provided customers with the information they need at a time, place, format and device of their choice. This has been a true empowerment.,Consulting companies and IT majors, as would be expected, are deeply invested in the digital transformation initiatives at banks. They have proffered channel integration/optimization as the central pillar in enabling this transformation. Banks?€? response, however, have been mixed.,Channel integration/optimization, in short, seeks to fine tune different digital channel touch points so that the customer has a unified experience at the bank. Consequently, this synchronization or optimization can help banks elevate customer experience by calibrating the delivery of best in class products and services.,But the problem with this approach is that every organization , to level up and offer top customer experience. That is the , and ?€?, expectation today. Anything less will jeopardize business and could potentially result in loss of customers. The result is that all major banks have geared up and offer par or exceed expectations in customer experience.,Further, channel integration/optimization is, by no stretch of imagination, a disruptive innovation by itself. It is an evolving standard or touchstone that every organization must meet to remain competitive, albeit an attractive revenue generating opportunity via the integration business. By definition it is glued to the physical architecture and seeks to achieve operating efficiency or synchronization as its end result. To that extent it may be handicapped and only obliquely impact profits.,There is, for instance, no room to incorporate analytical insights and the consequent learning as the motive force in reinvigorating banks business drivers. Hence it is no surprise that this channel optimization has not greatly enthused banks.,A recent study by McKinsey, in this context, is an eye opener. The study identified areas of bank digitization that made financial sense and those that did not. It concluded that while back office digitization / automation ?€? like document digitization (e.g. mortgage financing) , automation of credit decision and sales side analytics - impacted bottom line, investments in multi??channel integration do not appear to have been as effective. Interestingly, huge investments in select areas of digitization and analytics distinguished highly profitable banks.

It?€?s hard to think of a more worthwhile use for big data than saving lives ?€? and around the world the healthcare industry is finding more ways to do that every day.??,From predicting epidemics to curing cancer and making staying in hospital a more pleasant experience, big data is proving invaluable to improving outcomes.,This is very good news indeed ?€? as the cost of caring has skyrocketed in recent years and is expected to continue to do so as the population ages ?€? to the point where we could be headed for serious trouble.,I?€?ve spoken before about the hospital unit which found it could detect infections in newborns 24 hours before symptoms showed, by monitoring a live stream of heartbeats and breathing patterns.,And I?€?ve also mentioned Google?€?s (disputed but interesting) claims that it could detect outbreaks of flu more accurately than standard prediction methods by monitoring search activity.,But these are just the tip of the iceberg in an industry which generates mountains of data across every area of its operations.,In fact last year a survey by IDC Health Insights found that 50% of the hospitals and healthcare insurers put increasing their analytics capabilities as their top priority for investment over the next year.,And the body of medical literature from which further research evolves continues to grow every day ?€? with an estimated one million records per year added to Medline, the online repository of scientific studies related to medicine.,Efficiency is the great driver here ?€? with the cost of healthcare in the US currently standing at around 18% of GDP and forecast to rise, payment models are changing. While traditionally providers have been paid according to number of patients they treat, a move towards payment based on results and quality of treatment is taking place. These more complex metrics require more data and a different analytical skill set, rather than simply counting the number of patients coming through the door.,McKinsey & Company compiled a report for the Center for US Health System Reform which identified four main sources of big data in the healthcare industry.,They are:,These are the basic figures showing the amount of care which has been supplied by providers in the system, and the cost of paying for that care. Analysis of this tells us about the spread of diseases, and the priority that should be given to dealing with specific health threats. The most cost-effective treatments for specific ailments can be identified and the number of duplicate or unnecessary treatments can be significantly reduced. In the United States, Methodist Health System has used a tool which analyses Medicare claims data to highlight groups and individuals who may need expensive care in the future, allowing for less costly preventative action at an early stage.,These include patient medical records and images gathered during examinations or procedures, as well as doctors?€? notes. For example, the Carilion Clinic, in Virginia, says it used natural language processing algorithms to analyse 350,000 patient records, identifying 8,500 people at risk of heart problems. Similarly, the American Medical Association reported that analysis of patient records found only 26% of children who had recorded three high blood pressure readings at separate visits to their doctors had been diagnosed as suffering hypertension ?€? highlighting a significant number of failures to spot the condition.,Over the last few years a large number of partnerships have sprung up between pharmaceutical companies ?€? as if they have suddenly become aware of the huge benefits of pooling their knowledge. In the US major firms such as Pfizer and Novartis pool their data from trials into the clinicaltrials.gov website. And in the UK GlaxoSmithKline recently unveiled its partnership with the SAS Institute which aims to increase collaboration based on data from clinical trials. Suitable candidates can be found for trials more effectively by looking into lifestyle information. And comparison of data from multiple trials can throw up surprising results which can lead to new breakthroughs. For example the antidepressant desipramine is being trialled for its potential to destroy cancer cells in patients with small cell lung cancer.,This is data from over-the-counter drug sales combined with the latest ?€?wearables?€? which monitor your activity and heart rates, patient experience and customer satisfaction surveys as well as the vast amount of unstructured information about our lifestyles broadcast every day over social media. At the moment wearable devices are mainly used for personal fitness, but this is set to change ?€? spending on bringing this information from smart watches, wrist bands, running shoes and other wearables is expected to reach $52 million by 2019, according to a study by ABI Research. Services such as ginger.io already allow care providers to monitor their patients through sensor-based applications on their smartphones. And Proteus manufacture an ?€?ingestible?€? scanner the size of a grain of sand, which can be used to track when and how patients are taking their medication. This gives providers information about ?€?compliance rates?€? ?€? how often patients follow their doctor?€?s orders ?€? and can even alert a family member to remind them.,??Of course with medical matters patient privacy is always high priority, and big data brings big challenges in this respect. How insurance companies will act on the vast increase in information about our lives that they are able to glean is a concern ?€? will we see individuals turned down for cover because their running shoes have snitched that they are lazy?,It is plain to see that there are huge benefits to be had from analyzing the data about our health that is out there. The mantra of ?€?prevention is better than cure?€? has led to a focus on predicting problems in the early stages when they are easier to treat, and outbreaks can be more easily contained.,For example, Global Viral monitors data sources including a network of ?€?listening posts?€? across Africa and Asia, as well as social media chatter, to detect the spread of disease from wildlife to humans ?€? considered to be the source of 75% of diseases which are harmful to human health.,In the future we are likely to recover more quickly from illness and injury, and we will live longer. New drugs will come into existence and our hospitals and surgeries will operate more efficiently ?€? all thanks to big data.,I hope you found this post useful. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., : , is a globally recognized expert in strategic metrics and data. He helps companies and executive teams manage, measure, analyze and improve performance.,His new book is: 
I saw a chart on Twitter, about ,. I was surprised to see that their graph is missing many links and entities such as computer science. Data science ,. This view is biased towards making statistics the birth place of all modern data processing.,I then decided to create my own graph, summarizing my thoughts on data science. My graph is illustrated in figure 1, the other one in figure 2. Which one do you agree most with? Note that in my graph, an arrow from data science to statistics means that data science contributes to statistics.??
??This blog is a review of two books. Both are available for free from the MapR site, written by Ted Dunning and Ellen Friedman (published by O Reilly) :??,??and,??The ??MapR platform is a key part of the??,??and I shall be covering these issues in my course,??In this post, I discuss the significance of Time series databases from an IoT perspective based on my review of these books. Specifically, we discuss Classification and Anomaly detection which often go together for typical IoT applications. The books are easy to read with analogies like HAL (Space Odyssey ) and I recommend them.,??,The idea of time series data is not new. Historically, time series data can be stored even in simple structures like flat files. The difference now is the huge volume of data and the future applications possible by collecting this data ?€? especially for IoT. These large scale time series databases and applications are the focus of the book. Large scale time series applications typically need a NoSQL database like Apache Cassandra, Apache HBase, ??MapR-DB etc. ??The book?€?s focus is Apache HBase and MapR-DB for the collection, storage and access of large-scale time series data.,?? Essentially, time series data involves measurements or observations of events as a function of the time at which they occurred. The airline ?€?black box?€? is a good example of a time series data. The black box records data many times per second for dozens of parameters throughout the flight including altitude, flight path, engine temperature and power, indicated air speed, fuel consumption, and control settings. Each measurement includes the time it was made. The analogy applies to sensor data. Increasingly, with the proliferation of IoT, Time series data is becoming more common and universal. The data so acquired through sensors is typically stored in Time Series Databases. ??The TSDB (Time series database) is optimized for best performance for queries based on a range of time,??,Time series databases apply to many IoT use cases for example:,From these readings captured in a Time Series database, we can derive analytics such as:,??What are the short- and long-term trends for some measurement or ensemble of measurements?,How do several measurements correlate over a period of time?,?? How do I build a machine-learning model based on the temporal behaviour of many measurements correlated to externally known facts?,????Have similar patterns of measurements preceded similar events?,????What measurements might indicate the cause of some event, such as a failure?,??,The books gives examples of usage of Anomaly detection and Classification for IoT data.,For Time series IoT based readings, anomaly detection and Classification go together. Anomaly detection determines what normal looks like, and how to detect deviations from normal.,When searching for anomalies, we don?€?t know what their characteristics will be in advance. Once we know characteristics, we can use a different form of machine learning i.e. classification,Anomaly in this context just means different than expected?€?it does not refer to desirable or un?€? desirable. Anomaly detection is a discovery process to help you figure out what is going on and what you need to look for. The anomaly-detection program must discover interesting patterns or connections in the data itself.,Anomaly detection and classification go together when it comes to finding a solution to real-world problems. Anomaly detection is used first in the discovery phase?€?to help you figure out what is going on and what you need to look for. You could use the anomaly-detection model to spot outliers, then set up an efficient classification model to assign new examples to the categories you?€?ve already identified. You then update the anomaly detector to consider these new examples as normal and repeat the process,The book goes on to give examples of usage of these techniques in EKG,For example, for the challenge of finding an approachable, practical way to model normal for a very complicated curve such as the EKG, we could use a type of machine learning known as deep learning.,??Deep learning involves letting a system learn in several layers, in order to deal with large and complicated problems in approachable steps. Curves such as the EKG have repeated components separated in time rather than superposed. We take advantage of the repetitive and separated nature of an EKG curve in order to accurately model its complicated shape to detect normal patterns using Deep learning,The book also refers to a Data structure called??,-Digest for Accurate Calculation of Extreme Quantiles ??,-digest was developed by one of the authors, Ted Dunning, as a way to accurately estimate extreme quantiles for very large data sets with limited memory use. This capability makes??,-digest particularly useful for selecting a good threshold for anomaly detection. The??,-digest algorithm is available in Apache Mahout as part of the Mahout math library. It?€?s also available as open source at,Anomaly detection is a complex field and needs a lot of data.,For example: what happens if you only save a month of sensor data at a time, but the critical events leading up to a catastrophic part failure happened six weeks or more before the event?,To conclude, much of the complexity for IoT analytics comes from the management of Large scale data.,Collectively, Interconnected Objects and the data they share make up the Internet of Things (IoT).,Relationships between objects and people, between objects and other objects, conditions in the present, and histories of their condition over time can be monitored and stored for future analysis, but doing so is quite a challenge.,However, the rewards are also potentially enormous. That?€?s where machine learning and anomaly detection can provide a huge benefit.,the book covers themes such as,Storing and Processing Time Series Data,The Direct Blob Insertion Design,Why Relational Databases Aren?€?t Quite Right,Architecture of Open TSDB,Value Added: Direct Blob Loading for High Performance,Using SQL-on-Hadoop Tools,Using Apache Spark SQL,??Advanced Topics for Time Series Databases(Stationary Data, Wandering Sources, Space-Filling Curves ),Windows and Clusters,??Anomalies in Sporadic Events,Website Traffic Prediction,Extreme Seasonality Effects,Etc,??,Links again:,??and??,????by Ted Dunning and Ellen Friedman (published by O Reilly).,Also the link for??,??where I hope to cover these issues in more detail in context of?? MapR
It takes just a little talk with me to know that I'm a fan of the financial market and many subjects related to economics. Today I want to show an application involving news on the web, Python, MongoDB and the Gephi, a software for visualization and manipulation of social networks.,Our goal is to verify that the amount of joint occurrences of G20 countries (specifically Brazil) published in the news related to the financial market may reflect the data of the Brazilian Ministry of Development, Industry and Foreign Trade. For those who are unfamiliar with the term, the G20 (20 major economies) is a group consisting of ministers of economy and central bank presidents of 19 major economies plus the European Union. The group was established in 1999 in the wake of various economic crises of the 1990s, and is a kind of forum for cooperation and consultation on international financial matters. Member countries are (in alphabetical order):,Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Republic of Korea, Mexico, Russia, Saudi Arabia, South Africa, Turkey, the United Kingdom, the United States and the European Union.,In the table below we have the top 30 destinations for Brazilian exports based on data from the Brazilian Ministry of Development, Industry and Foreign Trade, referring to February 2014.,The names of the countries are in Portuguese because the news was too. As our goal was to have Brazil in the spotlight and the news sources were all brazilian, we have the country as the main node of our network. We can check, by the thickness of the edges, that Brazil is strongly related to China, USA, Japan and Germany. If we look at the table of export destination again, we see that these countries are, respectively, the 1st, 2nd, 5th and 6th destinations of our exports.,A more careful analysis shows that Brazil, in the news from our database, is related to almost all countries, except for Saudi Arabia and England (but which somehow has a relation represented by UK). I.e, of the top 30 destinations of our exports, only Saudi Arabia was not related. We show that conclusion in the next graph, highlighting the relations of Brazil (England and Saudi Arabia are more distant and with a smoothed color).,Despite having the national news as a primary data source, we can see the influence of the United States, the largest economy in the world, in your relationship with the other countries (except Saudi Arabia, but maybe by the amount of our data).,Given all these data, we conclude that the present relations in economic news actually reflect the data from our commercial relations. Maybe it was not different, but it is a way to show how everything is connected and in fact, given that markets are efficient (there is much discussion here and I tend to disagree with the theory), we have that trade relations will be reflected in some way in the behavior of market players and consequently,will be reflected upon pricing of financial assets.
Hello all, my name is Randall (go by Randy), and this is my first post to this forum. Please visit my??,to learn more about my credentials, and background.?? Also, I invite you to follow me on Twitter:,Just FYI, I am happily employed with my current employer, and am starting this blog simply to share ideas and engage in hopefully intellectual conversations and debates on the merits of Big Data technologies. The views and opinions I express are solely my own.??,My opinion of Big Data is that it is another tool in the Data Engineer's toolbox. However, I do feel that like with SOA, Big Data is in danger of being oversold and ultimately underutilized as a result of the well-intentioned, but over zealous disciples of the technology.?? There must be a business problem that requires a Big Data solution.,Notice I used the job description title of Data Engineer above, and I define that as the application of accepted best practices for managing data, as well as selecting the correct technologies for a given problem. I promise not to frequently use old metaphors or cliches, but some technologies, such as SOA, ETL, MDM, Big Data, and Hadoop, have all sounded a lot like the equivalent of the data engineers' golden hammer (depending on the group providing input), and every problem looks like a nail. It is a data engineer's job to make sure this does not happen.,While the focus of my blog will be on Big Data topics, I will reenforce the fact that the reason for the solution being applied is that it is the best fit for the business problem discussed. I believe Big Data is big enough to stand on its own merits without hyping the technology. We are not heading into "The Minority Report" type of predictive analysis this year. However, between Big Data, Artificial Intelligence, Machine Learning, the Internet of Things, and let's not forget IPv6, we are truly headed for some innovative technological advances. It is exciting to be a part to this!,Two topics that I see repeatedly addressed are:,The first topic is partially true, but a caution is warranted. A Big Data solution involves a wide variety of skills. A Data Scientist is just one of them, albeit an important team member/lead. Data experts are going to have to brush up on a few skills that were not previously used. I myself have taken several courses from Johns Hopkins and Rice Universities on several Big Data related courses. I have also dusted off my college Calculus and Statistics course books. Data Scientists will be hard to find, but you won't need two of them for every data project. They will, or should, have a support team of data specialists, which are a little more plentiful.,However, even if all skills reside in one individual, there are not enough hours in the day for one person to work all projects. In addition to a qualified data scientist, the company will need someone to define use cases for the data science team to work on. ,??There must be a bridge between the Big Data team and the business where the ultimate goal is to solve business problems, or develop a leading initiative for a given line of business. The questions should be asked, "What business problems do we need solved?" not "We have a Big Data Platform, what can we do with it?" ??Before the hunt begins for a data scientist, the business requirements should be defined at least down to the use case level so that the data scientist can properly scope the work, and team size.,A Big Data shop is not likely to consist of only one data scientist, nor will one shop likely consist of multiple data scientists. In addition to the data scientist, there will be system administrators for the Hadoop clusters (or other Big Data platform), there will be statisticians/analysts, R/SaS/Python programmers, possibly some junior employees that can use Excel, Access, or other tools needed to "tidy" the data. Even if you are successful in finding a data scientist that has all of the requisite skills (and he/she will likely be capable of doing all of the work), there is just too much work for one individual to perform. Of course, this depends on the size of the company, and scope of work, but generally Big Data solutions require a team of skilled data technicians.,There are plenty of qualified data scientists, or engineers that can transition their skills, and will be more than qualified to manage the activities of a room full of data specialists. These can mostly be young Computer Science graduates, or self taught developers. It will be the senior data scientist's job to manage this group of talent effectively to accomplish the desired result.??,On the second topic above, Hadoop and Big Data are not synonymous. A couple of caveats here, IF: 1) Your data is very large (volume ?€? terabytes in size), 2) Your data is received at a very fast rate (velocity - GB's/Hr - not GB's/wk), and 3) Your data is extremely diverse (variety ?€? change in entities and attributes), then with all these factors considered, you can say that you probably have a Big Data problem, and possibly Hadoop will be required to solve the problem. Dr. Kirk Borne wrote a nice article on the V's titled ??,. We can discuss this in more detail later. Hopefully I explained my point of view clearly enough that you understand where I am coming from. The details will follow in other posts.,In future posts, the focus will be on use cases from a variety of industries, and will discuss potential solutions. Because we (and I mean that inclusively) are all so enamored with Big Data, Analytics, and Hadoop (yes, please don't take the above to mean I don't love Hadoop), I will focus on those technologies as much as possible. However, I do believe it will be interesting to present a few use cases where the implementation could go in several directions, and take other factors into consideration, like Return on Investment (ROI), Net Present Value (NPV), and Internal Rate of Return (IRR).,I have worked for financial institutions (Fortune 100, 500), for the DoD, and numerous retail and wholesale companies across various industries, and I have seen the same problematic patterns. Silos of information closely guarded by those who, for whatever reason, refused to share knowledge and expose data. Not only do these people not share knowledge, but they go out of their way to hinder progress; even though doing so puts at risk the very thing they are trying to protect, their jobs. This is another area where I would like to discuss potential solutions to this problem. This type of issue is less technical, but this problem is so pervasive that I would want to throw some ideas out there just to generate some comments and ideas.,You get the gist of where I am coming from, and hopefully I will not ramble and will stay on point. I have a destination in mind, but we will start small and see how we move forward as a group. I truly am doing this to share my ideas and concepts, but just as importantly, learn from you. Constructive feedback, and thoughtful debates are welcome.,As I stated earlier, I am a data geek. I enjoy learning new things. If you reviewed my LinkedIn page you might have noticed that I have a networking background as well. The reason for this will be explained later, but capturing network traffic and providing real time feedback on messaging is a classic Big Data problem that I have been working on for some time ?€? definitely requires a Hadoop implementation. Hopefully, we will get a chance to discuss this topic in some detail in the near future.??,Please feel free to provide your input and feedback. If you have something you would like to have researched, please send me your ideas. I will be happy to address the topic to the best of my ability. I look forward to hearing from you!
About??:??,??is a globally recognized expert in strategic metrics and data. He helps companies manage, measure, analyze and improve performance.??His new book is:??
Interesting article by Karolina Alexiou.,Regarding mistake #1, I disagree. I do it all the time, and it's faster than finding, understanding, and fine-tuning a piece of code that will work for you, unless you are looking for something basic such as computing correlations for weighted observations. If you are good, your reinvented wheel will be better, implemented faster, more robust, and more customized than existing ones.,Actually, these mistakes below apply to any language, not just Python.,:,:
??,Many of the reasons for sharing personal data are driven by the desire of self-expression and making our lives more transparent. At a psychological level, I believe it falls under Maslow's hierarchy of needs as self-actualization. We have a basic fundamental need to feel accepted, to feel a sense of worth in life, and to know that our voice is heard in a world where 7 billion people are all talking at the same time. As a consequence, we look past the risk of putting our privacy on display, because if we are able to fulfill this inherent need, then the juice is worth the squeeze.,Another reason for data sharing is the ability to keep in contact with friends and family members, or ideally whomever you desire. ??Once we graduate and begin our adult lives, we are bombarded with our careers, social activities, relationships, financial obligations, travels, family, children, that is becomes nearly impossible to keep up with those who were important to you during your childhood and whom you saw on a daily basis.,Sharing personal data is also socially acceptable method of boasting a bit about your accomplishments. I used to joke that Instagram's sole purpose is to show others how great your life is compared to others, with the intention of making others jealous of your life. Now when you see accounts such as??,?€? ?€?.that seems to be exactly the case.?? However, from another perspective this is just another form of sharing what you are proud of and what makes you unique compared to the rest of the world. ??,Finally, data sharing benefits us as individuals and is able to show us who we are. Every time we tweet a status about how we feel, check-in to a certain bar/restaurant, or upload a photo showing which corner of the world we are in, we are creating a written story of our lives in the form of data. It creates a reflection of ourselves at any given point in time that later in life we will be able to look back and reflect on.,?? ??,This post represents the highlights of why I think data sharing is beneficial, however there are certainly more pros and cons out there. If you have some ideas, please share. Let me know why it is that you would or would not share your data openly.
In-memory technology for business intelligence and analytics first appeared in the early 2000s. Since then, they have been gaining prominence. Today, in-memory tools have finally come to a stage of maturity as widely accepted enterprise-scale solutions for a diverse range of organizations. Enterprises have become enamoured with the software?€?s ability to provide much faster results than could be supplied by previous tools.,However, as data continues to grow at astonishing rates, in-memory tools will no longer suffice and will fail to produce the performance rates they previously could. Accordingly, BI software developers will have to start thinking about new ways to provide the fast querying capabilities consumers have grown to expect.,??,In-memory technology relies on a simple and accurate premise: processing data in RAM is much faster than doing so in disk storage.,This fact, combined with the drop in RAM hardware prices that occurred early in the previous century, made a new type of analytics method viable and lucrative. Simply put, the software would copy data into RAM (with or without compressing it) and perform calculations in-memory, producing much faster results than could previously be achieved via databases relying on technologies such as OLAP cubes and distributed databases (e.g. Hadoop).,??,??,This type of platform does indeed make good on its promise and allows for rapid data discovery and exploration. Queries that previously would have required days of processing could suddenly be answered in minutes. End-users and consumers could suddenly do much more with their business intelligence dashboards??- asking new, on-the-fly questions and getting near immediate answers.,However, the advantage of in-memory technology is also its Achilles?€? heel: if performance depends entirely on RAM, what happens when you run out of RAM?,??,Previously, the possibility of a RAM glass ceiling would in itself seem unlikely. After all, RAM is much more of a commodity than it was a few decades ago, and it seemed perfectly viable and sustainable for companies to keep upgrading their hardware to accommodate for the growth in their data. But what this approach failed to take into account is the , in which data is growing.,Though somewhat cliched, it is true nonetheless that data is growing at an exponential rate due to new ways of automated data collection as well as large datasets that are generated daily from such sources as worldwide web usage information, sensor data, machine logs and the Internet of Things. By all accounts, this trend is expected to continue, and according to one estimate, we can expect a , by 2020.,In this reality of truly big data, the hardware requirements of in-memory technology are beginning to shed doubt on its sustainability and soundness.,Once the system runs out of RAM, an inevitable occurrence when the size of the data grows larger than the amount of available memory, there is a considerable drop in performance. And while RAM has grown cheaper, it is still relatively expensive. Data-intensive organizations are beginning to discover that to actually process all of their data using in-memory BI software they are being forced to make investments so large that they often exceed the value they intend to derive from their data analytics project.,Companies with less resources, unwilling to make such a risky investment, will instead opt to reduce the , of the data they work with. That is, they will leave out certain sources or fields, and settle for higher-level analysis. But this approach seems to contradict the basic assumption of data analysis: true insights are reached when one can take all data into account, finding new insights and unexpected connections.,Hence, working with partial data is also a highly unsatisfactory solution.,??,It is for these reasons that in-memory business intelligence has reached a crucial point in its development. If it keeps on trying to do the same thing (i.e., to cram all data into memory and demand consumers to keep upgrading their hardware to retain the fast performance they have grown accustomed to), it will simply not manage to provide a reasonable solution for companies that have large amounts of data and limited resources (needless to say, many companies fall into this category).,The solution to this is not to abandon in-memory processing altogether. Indeed, the unique advantages of performing queries in RAM should not be yielded so quickly. On the other hand, storing the bafflingly large amounts of business data in RAM on a permanent basis will also not do.,To balance the unique performance of in-memory processing with the limitations of hardware costs, we turn to smarter algorithms, caching and making full use of available computational resources (disk storage, RAM and CPU). Identifying the parts of data that can remain ?€?in rest,?€? and using past queries to shorten the calculation times of new ones, can give BI software developers a new way to crunch Big Data, without relying on endless RAM upgrades.,While the industry has already started moving in this direction with the introduction of In-Chip analytics, there is still much ground to be tread. But if Business Intelligence expects to keep up with the needs of the business community it will , to start looking for alternatives to in-memory.?? It is only a matter of time before the major players in the industry realize this., ,??SiSense.
Have you ever wondered what the deal was behind all the hype of ?€?big data?€?? Well, so did we. In 2014, data science hit peak popularity, and as graduates with degrees in statistics, business, and computer science from UC Berkeley we found ourselves with a unique skill set that was in high demand. We recognized that as recent graduates, our foundational knowledge was purely theoretical; we lacked industry experience; we also realized that we were not alone in this predicament. And so, we sought out those who could supplement our knowledge, interviewing leaders, experts, and professionals - the giants in our industry.??,What began as a quest for the reality behind the buzzwords of ?€?big data?€? and ?€?data science,?€? The Data Analytics Handbook, quickly turned into our first educational product of our startup Leada (see??Teamleada.com). Thirty plus interviews and four editions later, the handbook has been downloaded over 30,000 times by readers from all over the world In them, you?€?ll discover whether ?€?big data?€? is overblown, what skills your portfolio companies should look for when hiring a data scientist, how leading ?€?big data?€? and analytics companies interview, and which industries will be most impacted by the disruptive power of data science. We hope you enjoy reading these interviews as much as we enjoyed creating them!,.
In the mid 90s, while working in government, I was given an invaluable bit of advice from an Air Force colonel who would become a trusted advisor and a dear friend. He said:,At the time, I understood the specific context that compelled him to share this advice; yet I didn't appreciate the broader significance of this statement.,Years later, as a researcher in the national security community, I routinely questioned myself about how I was approaching a particular challenge. Was my view of the problem too narrow in scope? Was I missing larger opportunities for impact by focusing on a smaller, more tractable problem?,My mission as an applied researcher was to seek transformational advancements in the capabilities of our sponsors. It demanded that I scan for opportunities at the edges of what was currently understood and at the intersections of domains where creative combinations of ideas offered better odds for significant advances.,Operating in the gray areas was tough. While our organizational mission was clear, the day-to-day demands for signs of progress pushed many towards incrementalism. Problems were defined with the veneer of risk-taking and innovation but the reality often fell short. While failure was valued in theory, in practice it was an uncomfortable outcome tolerated by few.,It was during these years that I began to appreciate the cognitive burden associated with pursuing the uncertain path. It necessitates that we struggle with a number of internal forces. At the core of this struggle, I believe, is a confrontation with loss aversion - our bias toward mitigating loss at the expense of potential gains.,In the context of organizational decision-making, loss aversion leads to often frustrating and predictable outcomes. Higher risk initiatives with the potential for significant benefits are evaluated in the context of organizational risks that are invariably clearer, such as reputational and monetary losses incurred with failure. When faced with known downside risks, uncertain upside potential, and limited perceived options for recovery, many opt to avoid challenging the established order. This has profound implications in how solutions are framed for complex problems.,In the face of rampant complexity, problem definition and solution development are naturally intertwined and iterative. One learns about the problem by taking actions to test hypotheses. I watch with curiosity to see how others frame their approaches to complex problems. How much of the complexity do they choose to embrace? What frame of reference do they take? How is that shaped by the context they are embedded in?,Routinely I see others adopt and retain a narrow frame in the interest of expediency. It is perfectly reasonable and warranted to decompose a problem into subproblems that are more tractable. The difficulty comes when one stops trying to situate local actions in the broader context; in those cases, the opportunity is missed to question whether local actions are supporting positive outcomes on a larger scale.,As we wrestle with the toughest challenges of our time, this behavior will continually hinder our progress. At the extremes, even with thoughtful reflection, it is impossible for a single individual to conceptualize and hold a representation of the problem in mind. The complexity is simply too vast. Problem definition and solution development naturally become group activities out of necessity. This requires enlightened organizational leadership that sees the larger opportunities and builds the collaborative coalitions that enable scalable impact. At the level of the individual, it requires humility about the extent of our knowledge and willingness to continually question our present framing of the challenges before us.,The resistance to embracing this mindset can be strong. When a long-held frame of reference is questioned, feelings of unease and insecurity may drive a strong negative reaction. The longer a fixed view has remained the intellectual foundation for decision-making, the more difficult it becomes to revise that view. Compassionate leadership will be necessary to help others expand their perspectives and guide those who are willing toward larger opportunities.,The years ahead are rife with possibilities. We are learning more and more each day about the human condition. It is with this perspective that we have the opportunity to create new organizational models to meet the challenges of our time. I'm looking forward to participating in this process of experimentation and redefinition.
Today's advancing technology is improving the ways in which all industries do business. But regardless of what industry you work in, one thing that has always remained a constant is the importance of security. Data breaches can significantly impact a company in a highly negative way, ruining consumer confidence, as well as causing your company to have to pay large fines as a result of the breach. Sometimes a security breach cannot be prevented, as there are very skilled hackers that work hard to get past even the toughest defenses. But sometimes it is simply the fault of a careless employee who doesn?€?t follow proper security protocols.,Other forms of security, such as video surveillance, have also come leaps and bounds, enabling you to monitor things in the office from a anywhere in the world on your mobile device. And access points ensure that only authorized employees can gain access to particular areas of the business in which sensitive information may be stored. Depending upon your industry and business, you might not need all of the security options described below, but they are definitely security options to consider as your business grows.And as the technology continues to advance seemingly each day, you find even more options for better security at your disposal in all aspects of your business.,Obviously cybersecurity is at the forefront of the new security technology trends, with a cloud based antivirus system being one of the most popular and recommended choices for improved security. With cloud-based applications being utilized for everything from sharing and editing files, team and project management, online presentations and general storage of files and documents, a , is needed to ensure that all of the data stored on the Cloud is kept safe and secure from those who are not authorized to access it.,Remember, hackers are always striving to come up with new ways to attack or access business data, sometimes with a clear purpose, and sometimes just for fun. Thankfully, as hard as the hackers might be working, those who develop security technology are working just as hard, and cloud based antivirus systems are more than ready to provide protection for many companies, offering the utmost security for their data and networks.,Video surveillance in the past meant having a bunch of cameras placed around the office, with a central terminal located somewhere in the office that had to be monitored and maintained. New and advanced video surveillance systems let you easily provide surveillance for your business without a massive amount of equipment, and , from anywhere. Video can be stored on hard drives or cloud space, saving you time and expense on new videotapes or DVDs and recording equipment, and the quality and imaging has grown by leaps and bounds, providing, sharp, crystal clear HD video.,If you want to prevent unauthorized access to your office or specific areas within your office, you can either post a security guard, or use identity management tools and resources. Keycards and other forms of identity management allow only authorized employees and personnel to enter the office or areas of the office, and the technology can also be expanded to your database, ensuring that only those who have authorized access can see certain databases or other areas of your network. You can also prevent unauthorized users from using PCs or mobile devices that do not belong to them, ensuring privacy and security.,Some businesses have to rely upon several different security systems in order to provide full security on all aspects of their business. But integrated systems offer complete control with one intuitive, advanced application system, so that video, access control, and other areas of your security needs are all easy to manage through one complete system. This can save you a great deal of money wasted on separate systems as well as maintenance for each system.,When an alarm is tripped, you or the authorities often only receive an alert. With video verification, video of the area in which the alarm is sounding can also be sent so that you or the authorities know exactly who or what to look for. This not only helps to reduce false alarms, but also serves to improve security and give the authorities a more clear picture of who to look for in case of a break in, as well as providing you with an image of any offenses that are made by employees entering areas they shouldn't be in or engaging in acts that may harm the business.
It was 2005. The war in Iraq was raging. Many of us in the national security R&D community were developing responses to the deadliest threat facing U.S. soldiers: the improvised explosive device (IED). From the perspective of the U.S. military, the unthinkable was happening each and every day. The world?€?s most technologically advanced military was being dealt significant blows by insurgents making crude weapons from limited resources. How was this even possible?,The war exposed the limits of our unwavering faith in technology. We depended heavily on technology to provide us the advantage in an environment we did not understand. When that failed, we were slow to learn. Meanwhile the losses continued. We were being disrupted by a patient, persistent organization that rapidly experimented and adapted to conditions on the ground.,To regain the advantage, we needed to start by asking different questions. We needed to shift our focus from the devices that were destroying U.S. armored vehicles to the people responsible for building and deploying the weapons. This motivated new approaches to collect data that could expose elements of the insurgent network.,New organizations and modes of operation were also required to act swiftly when discoveries were made. By integrating intelligence and special operations capabilities into a single organization with crisp objectives and responsive leadership, the U.S. dramatically accelerated its ability to disrupt insurgent operations. Rapid orientation and action were key in this dynamic environment where opportunities persisted for an often unknown and very limited period of time.,This story holds important and underappreciated lessons that apply to the challenges numerous organizations face today. The ability to collect, store, and process large volumes of data doesn?€?t confer advantage by default. It?€?s still common to fixate on the wrong questions and fail to recover quickly when mistakes are made. To accelerate organizational learning with data, we need to think carefully about our objectives and have realistic expectations about what insights we can derive from measurement and analysis.,In recent years, decision-makers have embraced a number of simplistic misconceptions. One of particular concern is the idea that our ability to predict reliably improves with the volume of available data. Unfortunately reality is more complex.,One of the key drivers of prediction performance is the stability of the environment. When environmental conditions change, our ability to predict often degrades. No amount of historical data will inform us about the duration of a particular pattern or the nature of the change to follow.,Our globalized world relies on complex, interconnected systems that produce enormous volumes of data; yet network effects and cascading failures routinely surprise us. In many ways, we know more than ever about the present; meanwhile the future remains stubbornly uncertain.,Data??-driven prediction is viewed by some as a potential antidote to the risk associated with delay in action. This is a dangerous belief in complex environments. Overconfidence coupled with delay significantly magnifies the cost of prediction errors when they occur.,To combat this, a shift in mindset is required. We need to shift from predicting the future to understanding the now. By focusing our attention on the present, we uncover and pursue existing opportunities as opposed to projected ones that may never come to pass. By accelerating our pace of response, we increase our potential to benefit from surprises that will surely come. At the same time, we mitigate the cost of our mistakes.,Difficulties in the organizational learning process can take many forms. The following diagram highlights problems that commonly arise at different stages of the process.,In many respects, the challenges we face are struggles with preconceptions at both the individual and group level. Adaptation requires an openness to alternatives and a rejection of the temptation to simply confirm existing beliefs. Leadership is absolutely key to foster a culture where curiosity and experimentation are core values.,In an adaptive organization, measurement and analysis can be valuable tools for understanding the present environment and evaluating the effectiveness of our actions. Advances in Internet and mobile technologies have dramatically expanded the scope and rate at which certain types of information can be collected. With clearly defined objectives, these capabilities can be leveraged to uncover opportunities much more rapidly.,Once a course of action has been selected and implemented, adaptive organizations also reflect on both the derived benefit and the efficiency of execution. Measurement and analysis can illuminate the resulting changes in the environment and the level of time and effort required to achieve that outcome. This can serve as the basis for more thoughtful discussion of ways to accelerate the organization?€?s response to changing conditions.,Too often when lauding the potential of data-??driven decision-making, the technology sector focuses solely on the data analytic tools they believe are central to supporting their envisioned future. All the while, more fundamental organizational issues determine the ultimate impact of data in the learning process. As the IED threat in Iraq made clear, an unwillingness to adapt coupled with sluggish action can have dramatic consequences in a dynamic environment. Only a dogged focus on present opportunities coupled with efficiency of action will mitigate the risks of a persistently uncertain future. Data can be a powerful resource for accelerating the learning process. Yet organizational culture and leadership remain central determinants of the organization?€?s ability to effectively leverage its potential.
These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
Wisconsin Gov. Scott Walker may be surging as an early favorite for the Republican presidential nomination?€??€??€?but the numbers??,??the person leading among potential Iowa caucus-goers often doesn?€?t win. In the Democratic field, Hillary Clinton is the most dominant position??,??for a non-incumbent, with odds of clinching the nomination as high as 91 percent.,Over at the 538 site launched by statistician Nate Silver, the??,??is already on to handicap the 2016 presidential contest, a reminder that the season of campaigning, polls, predictions and surveys is descending upon us.,Which makes it a perfect time to think about statistics.,Even this soon in the game, we?€?ll start to see polling from all sides of the political spectrum, making us ponder which results are valid and which might be biased. We?€?ll find ourselves targeted by messages that seem tailored specifically to us?€??€??€?how exactly did that happen? To make an educated choice in any campaign, we need more than ever to understand how statistics serve as the basis of political calculations, framing what we learn and observe about campaigning and the political process.,As you?€?ll begin to notice, modern campaigns are all about??,??or using data mining techniques to identify and segment groups of voters with similar characteristics and behavior, and, increasingly, to target individual voters. Campaigns then use that data to craft specific ads and messages aimed at particular groups, and to direct get-out-the-vote efforts.,We?€?ll explain more in this post about microtargeting, and how it works. But before we get there, it?€?s useful to first understand the history of political campaign polling, and the origins of accurate sampling and surveys. That?€?s where the legacy of The Literary Digest comes in.,A popular national weekly opinion magazine of the 1920s that featured Norman Rockwell illustrations on its cover, The Literary Digest mailed out sample ballots to its readers during election seasons, and used the results to predict the outcome of the vote. Its poll was a much-anticipated event; up through 1932, it had been accurate. In the summer of 1936, the magazine mailed out 10 million ballots and got back 2.4 million. At the time, this qualified as ?€?Big Data.?€? On the strength of those results, it predicted a landslide victory?€??€??€?for Republican Alf Landon.,Franklin D. Roosevelt, of course, won in a landslide, with 62 percent of the popular vote.,What went wrong? Along with polling readers, The Literary Digest also mailed ballots to lists of automobile owners and telephone subscribers. At a time when a majority of the nation was jobless and destitute, those who could afford such things were hardly representative. They were wealthier and more Republican than the average voter, and therefore produced a biased prediction.,By contrast, a young advertising executive named George Gallup became convinced in 1935 that what mattered in measuring public opinion was not the quantity of people surveyed, but rather their representativeness?€??€??€?the degree to which they reflected the views of the general population. He believed that 2000 people chosen scientifically would be a better predictor of electoral outcomes than would the millions chosen in the way The Literary Digest did. He conducted bi-weekly polls of that same presidential campaign, showing Roosevelt leading by increasing amounts from August through October?€??€??€?the beginning of a continuum of similar tracking polls for presidential elections still used in various forms today.,Gallup not only predicted correctly that Roosevelt would win, he also accurately predicted the outcome of The Literary Digest Poll. He understood the concept of??,, meaning that a small representative sample is more accurate than a large sample that is not representative.,Gallup?€?s lessons resonate today, even with the use of far more sophisticated polling devices and information. Take Big Data, for example. Big Data are not necessarily good data. In fact, well-designed small sample surveys can produce more accurate results than huge datasets that are just lying around.,All this brings us, as promised, to the advent of microtargeting, a campaign tool that grew to prominence beginning with the 2004 presidential campaign, and a widely-used strategy today. At the most basic level, microtargeting involves taking what we know about some voters, (usually the results of survey calls) and combining it with what we know about everyone (census data, commercial marketing data, etc.) From there, we build statistical and machine learning models that make predictions about the people who weren?€?t surveyed.,, who teaches a microtargeting course for??,, and who served as Barack Obama?€?s targeting director in 2008, said it?€?s perfectly reasonable to wonder why campaigns use microtargeting. Why do campaigns want to target specific voters? If the campaign has a good message, why not just get that message to all voters?,First, campaigns are dealing with limited resources. They need to spend their resources where they can get that biggest bang for the buck. That means targeting people who are actually persuadable, and supplying them with the messages most likely to motivate them. Many voters are already solidly committed to one candidate or the other long before Election Day. Communications to these voters are a waste of the campaign?€?s limited resources.,Later, when the campaign shifts to GOTV (Get Out The Vote), they want to turn out their supporters. For this purpose, it?€?s important that the campaign has a good sense of who is supporting them, and who might not vote unless contacted. You don?€?t want to turn out your opponent?€?s supporters, and you don?€?t want to waste your resources on your supporters who are going to vote anyway. The ideal GOTV target is someone who will support your candidate if they vote, but who may or may not turn out.,Ideally, a campaign would survey everyone, then only communicate with those who were persuadable, and only turn out those who were supportive but might not vote. It?€?s impossible to survey everyone, but the statistical and machine learning methods behind microtargeting give us the next best thing: accurate predictions of how any voter would have answered the survey if he or she had been called.,Now that you have a basic understanding of microtargeting, you?€?ll understand why, if you?€?re a middle of the road voter, that phone keeps ringing at election time. And here?€?s a question to think about: Which party is better at it?,Strasma explains that it?€?s often hard for voters to tell, since press coverage tends to overstate the microtargeting expertise of the winners, and to exaggerate the failings of the losing campaign. For now, he would handicap the microtargeting contest this way:,?€?My current assessment is that the best Democratic operatives have about an 18 month lead on our Republican counterparts, but that the Republicans are working hard to close that gap,?€? Strasma says.,Both of those campaigns will be working hard to reach you. Understand the basics of statistics, surveying and microtargeting, and you increase your odds of making an informed choice.,(Peter Bruce is founder of The Institute for Statistics Education at Statistics.com, the leading online provider of analytics and statistics courses since 2002. He also is the author of the newly-released Introductory Statistics and Analytics: A Resampling Perspective. (Wiley),Follow Peter:,Twitter: @petercbruce, @statisticscom,Websites:??,,??
Normally I would send such people to see a specialist ?€? no, not a guru, but a sort of health specialist, but because this has happened to me so many times now, I eventually decided to put pen to paper, push the envelope, open up the kimono, and to record my advice for posterity and the great grandchildren.,So, here are my top seven tips for cashing in quick on the new big thing on the block.,Like every new religion, trend or fad, Big Data has its own founding myths, theology and liturgy, and there is money to be made in it;??,??lovely??,??money. By predicating and evangelising Big Data you will be welcomed with open arms into the Big Data faith, and will receive all the attendant benefits that will miraculously and mysteriously fall upon you and your devout friends. Go on, I dare you. Be a Big Data guru, a shepherd to a flock of sheep, and enjoy the wealth, health and happiness that most surely will come your way. You too can look cool in red Prada slippers, a flattering and flowing gown and matching accessories.,Simply stated, this is about acquiring other people's data, by sacred means or profane, marking it up and then selling it on. The value you add is that you act as a trusted conduit, a conduit for good. You may care to enrich the data, swop the order of data, replicate and embellish data, make stuff up, etc. which all serves to 'add value' to the data. You may even consider adding nuggets of value to the data, just for kicks and giggles. My best friend's favourite is injecting the good old 'diaper and beer' and 'friends and family' clich??s into every Big Data collection, as it never fails to thrill, please and delight.,The good thing about making money from Big Data is that it doesn't need to be anything to do with Big Data. Make a 20GB Enterprise Data Warehouse? Call it a Big Data success. Sell 20 boxes of dodgy doughnuts down the alternative market? Proclaim a Big Data triumph. Sell your digital porn stash to your best mate? Point to the incredible invisible hand of the Big Data market at work. See what I'm doing there. Anything can be anything, and you too can cash in on that opportunity, big time.,Tense, nervous headaches? Do you like making up stories about Big Data, or for that matter anything else? Are you a natural born fibber but are strapped for cash? Then worry no longer. If you get a Big Data patron you will be sorted for 'life'; get two and you'll be sorted for the afterlife as well. With a Big Data patron you can get the most tenuous, crappiest and superficial of pieces published, promoted and vaunted ?€? globally. Can't make it up yourself, then outsource and offshore it, after all, just get the keywords right for SEO ranking and the gullible will flock to you in droves. The down side of this profession is that you will be targeted for writing half-truths, quarter-truths and downright lies, and you will be pilloried as a purveyor of rank hyperbole. But don't worry, take heart and never lose the faith, you will be in good company. As one Big Data guru was want to say " If you repeat a lie often enough, people will believe it, and you will even come to believe it yourself." Amen! brother.,By 2016 there will be global demand for 30 billion Big Data professionals. Are you prepared to cash in on that inevitability? No? Then consider this.,One of my best friends makes his living as a completely phony Big Data Scientist. For two hundred bucks he can make you a Data Scientist or a Big Data guru. Some guys give you an education but this guy gives you immediate access to high paying jobs, sex and a life in the city. Moreover, for an extra 250 bucks you can also become a certified Big Data Trainer, which will allow you to do unto others what has been done unto you.,Big Data has heralded in the biggest innovations known in the history of computing, and arguably in the entire history of humankind. One of those new inventions has been the now widely acclaimed and revolutionary 'flat file data base' (FFDB), and this has been accompanied with developments in low level operating system primitives that allow for the processing of these collections and hierarchies of FFDBs. So, if one has a mind to do so, one can get some real business leverage off of these new tendencies by borrowing 21st century technology found in old operating system hacks from the sixties and seventies and eighties and nineties and?€? Well, the point is that in order to get serious funding it is no longer good enough to have a half page business plan, it is also necessary to eke out 'stuff' that works within the new paradigms of Big Data and Big Data Analytics. For my next venture I will be looking for serious funding for my 'Arbitrary Dawdle Down Data Street' (AD3S) Big Data Analytics platform, a platform designed to support virtual 1k bit processing and the massively parallel provision of global regular expression search and match (S&M), concatenation and listing, and cooperative data-driven and streamed data extraction and reporting. I'm hoping to attract the attention of governments, the EU, the Manic Street Preachers, the UN, China, Vladimir Putin, the DOD, HP, Oracle, Gartner, Lana Del Rey, Deloitte and IBM. So, this is going to be absolutely massive. Word!,According to leading management consultants and industry watchers Gartner, McKinsey and Deloitte, data needs to be managed and accounted like any other asset, such as money. To get into a similar view-point requires a massive leap of faith, but it is a conversion that might drive dividends. One avenue to be explored in eking out value from the apparently massively valuable Big Data lakes, silos and pools is through the operation of a Big Data Brokerage. A Big Data Brokerage is a business whose main responsibility is to be an intermediary that puts Big Data buyers and Big Data sellers together in order to facilitate a transaction. Big Data Brokerage companies are compensated via commission after the Big Data transaction has been successfully completed. They may also charge introductory fees. Just imagine the wealth of business opportunities in that. You could become the Goldman Sachs of data.,I hope you enjoyed this piece and would be pleased to hear your views on this and other subjects.,Whilst I understand the attraction and even the need of creating a new and significant growth industry, I would also advise a degree of restraint, and whilst I see that "Big Data" (the consideration of the potential value of All Data) has its allure, I also think that some good sense and informed caution should also prevail.
I made a recent discovery that I would like to share with the community. In my previous blog, I introduced a special algorithmic shell that distributes stocks based on their price movements (along the x-axis) and volume movements (y-axis). Using this shell, it is possible to visualize the trading behaviours of dozens of stocks simultaneously. I noticed one day that the stocks seemed to be lining up in formation. I decided to test the accuracy of my visual interpretation. Below I present the shell containing two reference lines: the "value line" and the "liquidity line." I adjusted my coding to measure the correlation between the shell distribution and the line. The correlation is positive when the distribution is aligned with value, negative in the case of liquidity. I found that the distribution can migrate between value and liquidity. Given the coding behind the algorithms, I surmised that there might be something deep and unstudied influencing the shell distributions. In this blog, I will be elaborating on this rather orderly behaviour of stocks, which I have named the Value-Liquidity Cycle.,I'm uncertain if fellow bloggers and blog followers have ever noticed, but I find the task of creating proper names difficult. I need terms to make blogging possible even when I have an incomplete understanding of phenomena, and the terms chosen aren't ideal. The mere act of applying a term to observable behaviour exposes me to criticism on my choice of terms; this can detract and distract from the points I am trying to make. There is also the issue of audience acceptance: a term that is reasonable for academics might be less so among those in the industry. In any event, I want to suggest that if price motion increases along with volume motion (in a manner of speaking, if price increases with volume), it is reasonable to suggest that investors value liquidity: this is the rationale behind the liquidity line. The more volume, the more value attached. If volume motion declines as price motion increases, it seems that investors no longer regard liquidity as an aspect of value: this is the principle behind the value line. The less volume, the more value attached. Whether or not my conceptualization is accurate probably requires considerable research. I admit that I have not conducted it; but for the purpose of furthering discussion in this blog, I will be making use of the terms as outlined.,Another weakness in my terminology, there being three words in the title, is cycle. A cycle is definitely an assertion since it suggests regular and coherent movement - rather than sporadic and random - to and from particular set points. I just haven't reviewed enough data to confirm whether the cycle consistently occurs in all sorts situations. Faced with the choice of posting this blog many years later or sharing some preliminary findings now, I clearly chose the latter but with explanation. In fact, I am uncertain about my willingness to carry out more elaborate and systematic research: the amount of discipline necessary would likely reduce my spontaneity.,I first want to substantiate my earlier assertion that in fact stocks on the algorithmic shell are in motion. In order to establish this, I prepared a movie clip showing the movement of stocks on the shell during a relatively minor correction. I left out the dates on the image below deliberately in order to discourage speculation. I emphasize that the Value-Liquidity Cycle, although formed using price movement, appears to have no stable correlation to price based on data studied so far. I have observed some association between the cycle and certain extreme market conditions. It is not my intention at this time to discuss price correlation specifically because it is a complicated issue. I call the clip the "Dance of Dread" since long investors are losing money. The shell correlation approaches zero during the correction. I am not suggesting that market downturns and low correlations are necessarily connected. In fact, it is possible for the market to pass zero without any apparent impact to price. I call the zero mark the neutral plane. When full migration occurs over the plane, I describe this as a crossover.,Although I would like to offer readers a clip containing many more frames, I had to limit the number in order to keep the size of the file reasonable. The change in correlation is apparent even in this short sample. On the first frame on the clip, the shell correlation to the value line is 0.3886; on the last, it is -0.0099. In this cycle, I have found that the movement to negative is uncommon. It is peculiar for the liquidity pattern to exist for long. On the other hand, it seems normal for the value pattern to persist. Since these findings might be due to limited research, I caution against making too much of them.,In order to prove to readers that the distribution is the result of migration rather than chance, below I present just the shell correlation over a much longer time period. The image shows that the shell can change gradually. I included the trading pattern for one of Canada's largest companies. Despite a prolonged period of price increases for the stock, the correlation continued to fluctuate in a manner indicative of a cycle; but it offered little guidance in terms of future price for the stock. In a year containing much more negative correlation - that is to say, exhibiting more liquidity - the price of the stock merely continued to increase. However, I need to emphasize that the shell presents the dynamics between rather than for individual stocks. It seems inappropriate to treat the correlation as insight to the future prospects for any individual stock. Due to the apparent lack of association, perhaps data scientist more so than professional traders would find the presence of coherent cycles in the background interesting. (I left out an important detail. In this sample, the correlation and the price are correlated at almost 0.69. As I mentioned earlier, correlation to price is a complicated issue. My main objective in this blog is to point out these curious cycles.),Having shared details of the market behaviour made detectable through my algorithmic shell, I also want to give readers a bit of perspective. I want to explain my relationship to the markets. I am certainly not a professional in finance, portfolio manager, or even an avid investor. I have a degree in environmental studies. I never concentrated on natural habitats during my studies - being more focused on social environments. An important topic covered in relation to nature is the practice of clear-cutting. This is when the trees occupying large areas of land are cut all at the same time resulting in horrific scenes of desolation. Apart from the impact on trees, there are lasting consequences to wildlife. Humanity is also affected since forests function as our lungs. Forests also retain large amounts of moisture. As an alternative to clear-cutting, some environmental groups have suggested selective harvesting; this is a more expensive and complicated approach even if it is feasible. Still, I consider the idea of being selective in order to mitigate the impacts of consumption an important concept in light of mass consumerism.,As investors, similar to our behaviours as consumers, I think many of us remain at a gorging phase much like caterpillars preparing for metamorphosis: we would like to extract as much money as possible from the markets without ever having to consider how this is a market of human lives. What we take out, we take away from people. The methods of engagement at our disposal reify and alienate others, contributing to an atmosphere of exploitation. When capital is scarce and lives are in the balance, it is necessary to elevate the discussion. I don't look at technical analysis as a means of furthering this philosophy of resource extraction; instead, it can help us responsibly sustain our capitalist system for the benefit of those in society that wish to be a part of it. I ask myself, who am I to question everyone else. I still live in this semi-dream-like state, thinking that I can ride my bike in a rainy day and worry about getting wet rather than not being able to retire. It is the sort of dream that I think all of us deserve to have. We should be able to afford to worry about small things because other people are looking after the really big concerns.,This brings me to my belief that big data - although we might think about it in relation to big business presumably because they have so much data - is actually part of the everyday lives of people. Everyday, people are exploited. We live in a world of victims and oppressors. However, this is a world that cannot last. There really has to be more research into areas of ethical and responsible engagement. Usually, this argument might be interpreted in a purely polemic or political sense. I suggest however that there are algorithmic dimensions towards the path to ethics and sustainability.,Highway congestion would be terrible if everybody decided to take the same roads and highways at the same time. The resulting bubbles and crashes from investors jumping in and out of the markets serve no useful purpose but to turn people away and deprive them of their dreams. People need to be motivated and confident to remain engaged with society. Since the primary purpose of the markets is to raise capital to achieve worthwhile ends - to provide services and products that people need - having alienated investors is also bad for corporate finance. We have to deliberately enable the possible. I think that the future requires levels of responsible engagement that only certain types of algorithms can provide. The stock market is important to study for the purpose of promoting social engagement and stability.
This week, movie fans around the country engage in the annual ritual of handicapping the Oscar odds, debating public predictions and seeking signs of how their favorites will fare. Can ?€?Birdman?€? knock out ?€?Boyhood??€? Will the ?€?Selma?€? snub continue?,Hollywood watchers sometimes try to base their calls on statistics, citing the higher Oscar chances for films already honored with British Academy Film or Screen Actors Guild awards. In 2013, Huffington Post even created an Oscars Predictions Dashboard, based on its own statistical models.,If only mastering statistics were simply another form of entertainment, a parlor game for the Oscars telecast or the NCAA Tournament bracket. But the unfortunate and real life consequences of misunderstanding statistics stretch far beyond a losing office pool. We make poor choices and suffer for our lack of understanding of even basic statistics. Yet our unacceptable level of statistical acumen persists unnoticed. An entire public relations industry pushes the notion of financial literacy and taking personal responsibility for our finances, even as we ignore our inability to grasp even the simplest statistical concepts that determine how we live our lives.,The measles outbreak that counts among its causes a wildly off-the-mark assessment of risk is only the most public of these consequences. Consider two favorite summer pastimes?€??€??€?beach trips vs. going to a baseball game. Fear of shark attacks is always lurking at the beach, and yet the odds of getting killed by a shark are one tenth the odds of getting hit on the head by a fly ball in the baseball stands. Then there?€?s the recent cholesterol ?€?retraction,?€? an apparent reversal of three decades of medical advice regarding the kinds of foods we should eat. It?€?s only the latest health research to draw conflicting headlines over the years and leave us more confused than ever.,The root cause of that confusion isn?€?t always fraud or scientific error?€??€??€?it?€?s often a misunderstanding of probability and statistics. A study a few years ago looked at more than four dozen health claims that researchers arrived at by examining existing data for possible associations?€??€??€?not by conducting controlled experiments. These four dozen claims all had one thing in common?€??€??€?they were tested later by controlled experiments. Astonishingly, not one of the claims held up in the controlled experiment.,A key issue here is what statisticians call the ?€?multiple comparisons problem.?€? Even in completely randomly generated data, interesting patterns appear. If the data are big enough and the search exhaustive enough, the patterns can be very compelling.,That doesn?€?t mean they are true, or significant. But since we don?€?t understand the underlying concepts of statistics, like random sampling, we can?€?t judge for ourselves whether the patterns mean anything. And we can?€?t base everyday decisions, like what to eat or how much to exercise, with any degree of certainty.,It?€?s not only our health decisions that take a hit from our statistical illiteracy.,Regardless of our political leanings, most of us would agree that we?€?d like to see a more stable world and a reduction in global poverty. But an economic historian from British Columbia, Morten Jerven, created a stir last year when he contended economic statistics in many African countries are often suspect. Governments around the world use that data to determine foreign aid, development, credit and other policies, without accurate knowledge of how those countries are really faring.,There?€?s also an entirely personal aspect to statistics, a growing interest in what scientific research and data can tell us about relationships. It could be useful?€??€??€?but only if we understand whether it?€?s meaningful. Some recent media attention has highlighted this notion, like linking smaller wedding sizes to shorter marriages. But think of other factors that could be at play, like family disagreements that might mean fewer wedding guests and bigger relationship problems later. Or that lack of money could be a root cause yielding both smaller weddings and relationship troubles. Before expanding your invitation list, it would be more worthwhile to understand how correlation doesn?€?t necessarily imply anything about causation.,There are some signs that more attention might begin to be paid to our statistical illiteracy problem. STATS.org is a new non-profit, non-partisan organization that describes its aim as analyzing and explaining numbers and statistics in the news, and promoting statistical literacy. As political campaigns increasingly use Big Data and tools such as microtargeting, the public has more direct exposure to statistical concepts, perhaps prompting an interest in them. If you?€?re an undecided voter, you?€?ll experience statistical modeling first hand with all those phone calls, texts, and messages.,Recently, a New York Times Modern Love essay on using a scientifically- based survey of 36 questions as a way to establish intimacy and maybe even fall in love went viral, with readers following up with their own stories of how the tool turned out for them. Although things look dire at the moment for our levels of statistical literacy, that doesn?€?t mean we might not end up embracing the data-driven life at some point. First we?€?ll have to know how to figure out the things that are real in life, as opposed to the ones simply due to chance.,(Peter Bruce is founder of??,, the leading online provider of analytics and statistics courses since 2002. He also is the author of the newly-released??,. (Wiley),Follow??,:,Twitter:??,,??,m,Websites:??,,??
Before Los Angeles Times reporter Laurie Becklund died of metastatic breast cancer earlier this month, she wrote a??,??final column. She asked friends and family not to say that she died fighting a courageous battle, a meaningless, trite phrase. She excoriated the pink ribbon and early detection breast cancer campaigns, calling the focus on awareness shamefully out of date. And, in a particularly telling excerpt that has caught the Internet?€?s attention, she decried a ?€?criminal?€? lack of available data about the people at the center of all this, the ones facing the fatal diagnosis.,?€?We now know that breast cancer is not one disease,?€? she wrote. ?€?What works for one person might not for another: There is no one ?€?cure.?€? We are each, in effect, one-person clinical trials. Yet the knowledge generated from those trials will die with us because there is no comprehensive database of metastatic breast cancer patients, their characteristics and what treatments did and didn?€?t help them.,?€?In the Big Data-era,?€? Becklund concluded, ?€?this void is criminal.?€?,Becklund?€?s poignant plea is only the latest and most dramatic appeal to unleash the potential of meaningful Big Data, and its power to change people?€?s lives for the better. The ability to sift through huge amounts of raw data can offer us everything from personalized health medicine to shortcuts in the struggle to wrestle with the vast information available on the Internet. The enthusiasm Big Data creates is entirely legitimate?€??€??€?and so is our need to use it to our greatest advantage.,Before I talk about those advantages, a note of caution. Big Data?€?s popularity has prompted some commentary that I?€?d describe as frothy and meaningless?€??€??€?a faddish approach to a serious subject. Browse an airport bookshop and you night find the popular book Big Data placed between airplane reads on using people tools in business, and how children succeed. Big Data is caught in same hype??,??that accompanied the early days of the Internet, with outsized claims followed by disappointment, settling back into a more realistic view of its transformational potential.,The Big Data airport book authors jump into this by incorrectly declaring the end of the need for statistical sampling, one reason why statisticians and data scientists seem at war regarding the Big Data phenomenon.,Let?€?s look more carefully at this question. It?€?s true that, in some cases, interesting patterns and information emerge only when huge amounts of raw data are collected. It?€?s also true that once the pattern is identified, the data that carry the information of interest may still be of modest size.,Here?€?s where Big Data really hits a home run: It can do things that can?€?t be done with small data, no matter how well designed the sampling procedure. Take Google search?€??€??€?it works by storing all the search strings that have ever been run (or run recently) and finding out who else searched for the same thing that you just searched for. Then it has to collect enough such searches so that it can evaluate what people subsequently clicked on, and thus make good recommendations about what links you should see and in what order. If you search for ?€?nude pictures of Angelina Jolie?€? you might be able to collect a big enough sample of similar searches fairly quickly. But if you search for ?€?nocturnal habits of aardvarks,?€? you really need Big Data. Note that the relevant data here can still be small in size?€??€??€?the number of qualifying searches needed to make a good first-pass judgment might be in the low thousands. But you have to sift through hundreds of millions, or billions, to get those relevant records.,When it comes to medical and other research, we can take Big Data and apply statistical design and sampling principles to learn more, and learn more definitively, about what these large amounts of information are really telling us. One principle is to separate exploration (roaming around the data to see what looks interesting) from confirmation (stating a hypothesis in advance then looking systematically at fresh data to see if it is confirmed).,After obtaining medical records of 2 million people and following them for 30 years, researchers in Denmark concluded that the chances of premature death are higher for people who have ADHD, or attention deficit hyperactivity disorder, according to a study published in the Lancet.,As the news site Vox??,??deconstructed, the study had limitations. It was was done in Denmark, and Vox speculated that the medical profession there may diagnose and treated ADHD differently from other countries, making the findings unique to the Danes.,But the main caution is to determine which realm the study falls in?€??€??€?exploration or confirmation. Did the investigators have some other basis for believing there was a link between ADHD and early death, then look to these data for confirmation? Or were they exploring the data, looking for some interesting pattern? Rapid growth in the digitization and availability of patient data and health data in general holds great potential for medical research and personalized medicine. Yet the appropriate statistical methodology is still needed to unlock this potential, and guard against error.,Laurie Becklund?€?s plea tells us in a particularly powerful way how crucial it is for us to capitalize on the ways Big Data can transform our lives. At the same time, we need to use our full array of statistical tools and techniques to fully achieve that transformational potential. In assessing the impact of Big Data, that?€?s the reality, and not the hype.,(Peter Bruce is founder of??,, the leading online provider of analytics and statistics courses since 2002. He also is the author of the newly-released??,. (Wiley),Follow??,:,Twitter:??,,??,m
The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.??,These profiles are randomly selected among our active and new members. To be selected, you need to have a profile with short bio. It helps to post a blog, or share / comment contributions from other members.
In continuation of our series exploring the data science community, we are now exploring salary patterns of data scientists. The below chart shows data from Indeed.com & the top 10 locations with the highest concentration of data scientists.,The median of these top 10 cities is between $142,000 to $148,000/year. The higher end of the scale is in New York & San Francisco which reflects the demand from employers as well as the relative costs of living there. At the lower end of the scale is Austin, LA & Seattle.,We explored salary trends for three other common analytics job titles: Director, Marketing Analytics, Manager Analytics & Analytics Engineer. We found the same location salary differentials with New York & San Francisco being the highest & Austin, Seattle & LA being at the lower end.,While we found that Director level positions having a roughly $10,000 difference from Data scientists across locations but they were paid about $5-$15,000 more than Managers of Analytics. Among the 4 job titles we studied, we found that analytics engineers were the lower paid and ranges from $102,000 in Austin to $147,000 in New York.,We wanted to do a quick survey among the Data Science Central community members to benchmark salaries among various job titles, industries & locations with the larger data set. The survey shouldn?€?t take more than a minute to fill out and when we have a large sample set, we can share the data back with the community.
Try introducing yourself as a Competitive Intelligence or CI analyst. Chances are that the reactions will range from (a) That's what you do all day, Google search? to (b) Identifying 007 ways to look at competition, Corporate Bond?,Of course, one can always resort to the classical definition of CI. It is defined as the action of??,,??,and,??,??about products, customers, competitors, and any aspect of the environment needed to support executives and managers making,??for an organization.,Given today's dynamic business environment and the rise of disruptive innovators that are challenging traditional business models across industries, this should be the golden age of CI, right? But a look at Google Trends shows this:,What's gone wrong? Remember that old joke - a man was too busy driving to stop for gas? Is that what's ailing CI? Failing to recognize the disruptive innovations that is changing its own game? As the CI ninjas faithfully use the well-known and unchanged frameworks and send out, with unfailing frequency, the key deliverables such as,,do they hear questions like "I already knew this", "Where's the insight?", "I need something real-time.", "How can I measure success from CI investment?", "What strategic action can I take now?" from the key CI consumers in the business? (Consumers including Strategy, Marketing, Product, Research, Sales teams),Maybe there's help. From one of its own techniques. Let's zone in on the wargaming technique. Defined as a role-played simulation of a business situation, it involves a set of teams representing a market or customer, a set of competitors, and a series of other uncontrollable factors or entities. If we apply it on CI today, what does the world look like for CI?,Assume, you are part of the Strategy Planning team of an investment management group analyzing the competitor, BlackRock??,. $4.3T in assets. When Fortune published its 2013 list of the world?€?s 50 Most Admired Companies, five among them were 25 years old or less. Four were the usual suspects from tech land: Amazon, eBay, Facebook, Google. And there was BlackRock, formed in 1988, a Financial Services company. $4.3T in assets. But the CEO has still set a target for 5% organic growth y-o-y. Is that achievable? That's the question. Can a clear understanding of who and why we are doing CI for, help in the first step of??,better per the classical definition? Our customer here is the Strategy team, not Marketing, not Sales, not Product. That's critical for the CI analyst to know.,Assume your usual deliverable is a detailed competitor profile on BlackRock. Is that enough? What could change the competition for your BlackRock profile and provide the real insight the customer asked for?,a)??,: Annual report, investor presentations, earnings transcripts, SEC filings, fund performance data from SimFund, eVestment, Morningstar, analyst views, industry news from Financial Times, Investment News, Pension and Investments. Phew. An ocean of data the CI analyst needs to wade through to even start attacking the problem.,Luckily there is help. Tools such as Feedly can help automate the newsfeeds and personalize the news the CI analyst can read to remove all the additional noise that can result in data drip. (data rich, insight poor),And then there are tools such as ClearCI which use all the CI sources and provide automated monitoring dashboards for CI.,Could such tools help in step 2 of the CI definition - information??,b)??,: Digital data has unleashed a new surge of tools that claim to track all digital competitive data from social mentions, text analysis of online product feedback to understanding keywords that drive traffic. Check a few on this link below:??,Imagine adding these to step 3 of CI, the??,??in the traditional CI definition. You could check what customers are saying about BlackRock, which funds are they advertising more and which ones are bringing in more traffic. Can this enriched analysis make CI more predictive?,c)??,: Once a data-provider, Bloomberg is also a competitor for CI today, providing powerful, interactive visualization of the key industry and competitor statistics that earlier used to consume hours of excel and powerpoint time. Check the link below. On a single scroll, you can look at the marketshare, the historical financial performance of BlackRock vs. competitors. ,Spooked? But why wait for Bloomberg, anyone with a visualization like Tableau can use this technique for the final step, easier??,as per the classical definition.,Of course there are other uncontrollable factors as always, in a wargame. But by holistically looking at the customer and the competition for itself, can CI make itself more relevant in today's world?,Let's go back the question we were trying to answer on BlackRock's growth target. Equity fund performance, management changes, retail build out of BlackRock are few of the themes that the CI analyst would hopefully get through the wargaming exercise and the plethora of tools. These themes could then be analyzed in more detail to assess the key drivers and the impact contributors and then monitor only those, real-time, interactively, to understand the real game.,Can all this make CI more relevant? The tools will help for sure. But the real reason why interest in CI seems to be declining is not just the emergence of new tools and techniques. It is the lack of real predictive insight from CI that companies can use to drive strategy and and stop freezing Kodak moments forever. Vincent Barabba's classical book on Kodak's failure named "The Decision Loom" brought home this point powerfully. Wargame or no wargame, can CI analysts bring back the strategy in CI? And comment on whether BlackRock's 5% goal seems too much to swallow?,Would love to hear your comments and thoughts.
A basic mantra in statistics and data science is??,, meaning that just because two things appear to be related to each other doesn?€?t mean that one causes the other. This is a lesson worth learning.,If you work with data, throughout your career you?€?ll probably have to re-learn it several times. But you often see the principle demonstrated with a graph like this:,??,One line is something like a stock market index, and the other is an (almost certainly) unrelated time series like ?€?Number of times Jennifer Lawrence is mentioned in the media.?€? The lines look amusingly similar. There is usually a statement like: "Correlation = 0.86". ??Recall that a correlation coefficient is between +1 (a perfect linear relationship) and -1 (perfectly inversely related), with zero meaning no linear relationship at all. ??0.86 is a high value, demonstrating that the statistical relationship of the two time series is strong.,The correlation passes a statistical test. This is a great example of mistaking correlation for causality, right? Well, no, not really: it's actually a time series problem analyzed poorly, and a mistake that could have been avoided. You never should have seen this correlation in the first place.??,The more basic problem is that the author is comparing two trended time series. The rest of this post will explain what that means, why it?€?s bad, and how you can avoid it fairly simply. If any of your data involves samples taken over time, and you?€?re exploring relationships between the series, you?€?ll want to read on.,There are several ways of explaining what's going wrong. Instead of going into the math right away, let?€?s look at a more intuitive visual explanation.,To begin with, we?€?ll create two completely random time series. Each is simply a list of 100 random numbers between -1 and +1, treated as a time series. The first time is 0, then 1, etc., on up to 99. We'll call one series Y1 (the Dow-Jones average over time) and the other Y2 (the number of Jennifer Lawrence mentions). Here they are graphed:,There is no point staring at these carefully. They are random. The graphs and your intuition should tell you they are unrelated and uncorrelated. But as a test, the correlation (,) between Y1 and Y2 is -0.02, which is very close to zero. There is no significant relationship between them. As a second test, we do a linear regression of Y1 on Y2 to see how well Y2 can predict Y1. We get a??,??(R,??value) of .08 ?€? also extremely low. Given these tests, anyone should conclude there is no relationship between them.,Now let's tweak the time series by adding a slight rise to each. Specifically, to each series we simply add points from a slightly sloping line from (0,-3) to (99,+3). This is a rise of 6 across a span of 100. The sloping line looks like this:,Now we?€?ll add each point of the sloping line to the corresponding point of Y1 to get a slightly sloping series like this:,??,We?€?ll add the same sloping line to Y2:,Now let?€?s repeat the same tests on these new series. We get surprising results: the correlation coefficient is 0.96 ?€? a??,??unmistakable correlation. If we regress Y on X we get a very strong R,??value of 0.92. The probability that this is due to chance is extremely low, about 1.3??10,. These results would be enough to convince anyone that Y1 and Y2 are very strongly correlated!,What's going on? The two time series are no more related than before; we simply added a sloping line (what statisticians call??,). One trended time series regressed against another will often reveal a strong, but spurious, relationship.,Put another way, we've introduced a mutual dependency. By introducing a trend, we've made Y1 dependent on X, and Y2 dependent on X as well. In a time series, X is time. Correlating Y1 and Y2 will uncover their mutual dependence ?€? but the correlation is really just the fact that they're both dependent on X. In many cases, as with Jennifer Lawrence?€?s popularity and the stock market index, what you?€?re really seeing is that they??,??in the period you?€?re looking at. This is sometimes called??,.,The amount of trend determines the effect on correlation. In the example above, we needed to add only a little trend (a slope of 6/100) to change the correlation result from insignificant to highly significant. But relative to the changes in the time series itself (-1 to +1), the trend was large.,A trended time series is not, of course, a bad thing. When dealing with a time series, you generally want to know whether it?€?s increasing or decreasing, exhibits significant periodicities or seasonalities, and so on. But in exploring relationships between two time series, you really want to know whether variations in one series are correlated with variations in another. Trend muddies these waters and should be removed.,There are many tests for detecting trend. What can you do about trend once you find it?,One approach is to model the trend in each time series and use that model to remove it. So if we expected Y1 had a linear trend, we could do linear regression on it and subtract the line (in other words, replace Y1 with its residuals). Then we?€?d do that for Y2, then regress them against each other.,There are alternative, non-parametric methods that do not require modeling. One such method for removing trend is called??,. With first differences, you subtract from each point the point that came before it:,?? ?? ????,Another approach is called??,. Link relatives are similar, but they??,??each point by the point that came before it:,?? ?? ????,Once you?€?re aware of this effect, you?€?ll be surprised how often two trended time series are compared, either informally or statistically. Tyler Vigen created??,??devoted to spurious correlations, with over a dozen different graphs. Each graph shows two time series that have similar shapes but are unrelated (even comically irrelevant). The correlation coefficient is given at the bottom, and it?€?s usually high.,How many of these relationships survive de-trending? Fortunately, Vigen provides the raw data so we can perform the tests. Some of the correlations drop considerably after de-trending. For example, here is a graph of??,??vs??,:,??,The correlation of these series is 0.88. Now here are the time series after first-differences de-trending:,??,These time series look much less related, and indeed the correlation drops to 0.24.,A??,??from Alex Jones, more tongue-in-cheek, attempts to link his company?€?s stock price with the number of days he worked at the company. Of course, the number of days worked is simply the time series: 1, 2, 3, 4, etc. It is a steadily rising line ?€? pure trend! Since his company?€?s stock price also increased over time, of course he found correlation. In fact, every manipulation of the two variables he performed was simply another way of quantifying the trend in company price.,I was first introduced to this problem long ago in a job where I was investigating equipment failures as a function of weather. The data I had were taken over six months, winter into summer. The equipment failures rose over this period (that?€?s why I was investigating). Of course, the temperature rose as well. With two trended time series, I found strong correlation. I thought I was onto something until I started reading more about time series analysis.,Trends occur in many time series. Before exploring relationships between two series, you should attempt to measure and control for trend. But de-trending is not a panacea because not all spurious correlation are caused by trends. Even after de-trending, two time series can be spuriously correlated. There can remain patterns such as seasonality, periodicity, and autocorrelation. Also, you may not want to de-trend naively with a method such as first differences if you expect lagged effects.,Any good book on time series analysis should discuss these issues. My go-to text for statistical time series analysis is??,??by Farnum and Stanton (PWS-KENT, 1989). Chapter 4 of their book discusses regression over time series, including this issue.
The term ?€?Big Data?€? is a massive buzzword at the moment and many say big data is all talk and no action. This couldn?€?t be further from the truth. With this post, I want to show how big data is used today to add real value.,Eventually, every aspect of our lives will be affected by big data. However, there are some areas where big data is already making a real difference today. I have categorized the application of big data into 10 areas where I see the most widespread use as well as the highest benefits [For those of you who would like to take a step back here and understand, in simple terms, what big data is, check out the posts in my??,].,This is one of the biggest and most publicized areas of big data use today. Here, big data is used to better understand customers and their behaviors and preferences. Companies are keen to expand their traditional data sets with social media data, browser logs as well as text analytics and sensor data to get a more complete picture of their customers. The big objective, in many cases, is to create predictive models. You might remember the example of U.S. retailer Target, who is now able to very accurately predict when one of their customers will expect a baby. Using big data, Telecom companies can now better predict customer churn; Wal-Mart can predict what products will sell, and car insurance companies understand how well their customers actually drive. Even government election campaigns can be optimized using big data analytics. Some believe, Obama?€?s win after the 2012 presidential election campaign was due to his team?€?s superior ability to use big data analytics.,Big data is also increasingly used to optimize business processes. Retailers are able to optimize their stock based on predictions generated from social media data, web search trends and weather forecasts. One particular business process that is seeing a lot of big data analytics is supply chain or delivery route optimization. Here, geographic positioning and radio frequency identification sensors are used to track goods or delivery vehicles and optimize routes by integrating live traffic data, etc. HR business processes are also being improved using big data analytics. This includes the optimization of talent acquisition ?€? Moneyball style, as well as the measurement of company culture and staff engagement using big data tools.,Big data is not just for companies and governments but also for all of us individually. We can now benefit from the data generated from wearable devices such as smart watches or smart bracelets. Take the Up band from Jawbone as an example: the armband collects data on our calorie consumption, activity levels, and our sleep patterns. While it gives individuals rich insights, the real value is in analyzing the collective data. In Jawbone?€?s case, the company now collects 60 years worth of sleep data every night. Analyzing such volumes of data will bring entirely new insights that it can feed back to individual users. The other area where we benefit from big data analytics is finding love - online this is. Most online dating sites apply big data tools and algorithms to find us the most appropriate matches.,The computing power of big data analytics enables us to decode entire DNA strings in minutes and will allow us to find new cures and better understand and predict disease patterns. Just think of what happens when all the individual data from smart watches and wearable devices can be used to apply it to millions of people and their various diseases. The clinical trials of the future won?€?t be limited by small sample sizes but could potentially include everyone! Big data techniques are already being used to monitor babies in a specialist premature and sick baby unit. By recording and analyzing every heart beat and breathing pattern of every baby, the unit was able to develop algorithms that can now predict infections 24 hours before any physical symptoms appear. That way, the team can intervene early and save fragile babies in an environment where every hour counts. What?€?s more, big data analytics allow us to monitor and predict the developments of epidemics and disease outbreaks. Integrating data from medical records with social media analytics enables us to monitor flu outbreaks in real-time, simply by listening to what people are saying, i.e. ?€?Feeling rubbish today - in bed with a cold?€?.,Most elite sports have now embraced big data analytics. We have the IBM SlamTracker tool for tennis tournaments; we use video analytics that track the performance of every player in a football or baseball game, and sensor technology in sports equipment such as basket balls or golf clubs allows us to get feedback (via smart phones and cloud servers) on our game and how to improve it. Many elite sports teams also track athletes outside of the sporting environment ?€? using smart technology to track nutrition and sleep, as well as social media conversations to monitor emotional wellbeing.,Science and research is currently being transformed by the new possibilities big data brings. Take, for example, CERN, the Swiss nuclear physics lab with its Large Hadron Collider, the world?€?s largest and most powerful particle accelerator. Experiments to unlock the secrets of our universe ?€? how it started and works - generate huge amounts of data. The CERN data center has 65,000 processors to analyze its 30 petabytes of data. However, it uses the computing powers of thousands of computers distributed across 150 data centers worldwide to analyze the data. Such computing powers can be leveraged to transform so many other areas of science and research.,Big data analytics help machines and devices become smarter and more autonomous. For example, big data tools are used to operate Google?€?s self-driving car. The Toyota Prius is fitted with cameras, GPS as well as powerful computers and sensors to safely drive on the road without the intervention of human beings. Big data tools are also used to optimize energy grids using data from smart meters. We can even use big data tools to optimize the performance of computers and data warehouses.,Big data is applied heavily in improving security and enabling law enforcement. I am sure you are aware of the revelations that the National Security Agency (NSA) in the U.S. uses big data analytics to foil terrorist plots (and maybe spy on us). Others use big data techniques to detect and prevent cyber attacks. Police forces use big data tools to catch criminals and even predict criminal activity and credit card companies use big data use it to detect fraudulent transactions.,Big data is used to improve many aspects of our cities and countries. For example, it allows cities to optimize traffic flows based on real time traffic information as well as social media and weather data. A number of cities are currently piloting big data analytics with the aim of turning themselves into Smart Cities, where the transport infrastructure and utility processes are all joined up. Where a bus would wait for a delayed train and where traffic signals predict traffic volumes and operate to minimize jams.,My final category of big data application comes from financial trading. High-Frequency Trading (HFT) is an area where big data finds a lot of use today. Here, big data algorithms are used to make trading decisions. Today, the majority of equity trading now takes place via data algorithms that increasingly take into account signals from social media networks and news websites to make, buy and sell decisions in split seconds.,For me, the 10 categories I have outlined here represent the areas in which big data is applied the most. Of course there are so many other applications of big data and there will be many new categories as the tools become more widespread.,What do you think? Do you agree or disagree with this data revolution? Are you excited or apprehensive? Can you think of other areas where big data is used? Please share your views and comments.,Follow us on Twitter:??,??|??

Here is a summary what is wrong with various frameworks for learning. To avoid being entirely negative, I added a column about what's right as well.
My least favorite description is, ?€?It works just like the brain.?€? I don?€?t like people saying this because, while Deep Learning gets an inspiration from biology, it?€?s very, very far from what the brain actually does. And describing it like the brain gives a bit of the aura of magic to it, which is dangerous. It leads to hype; people claim things that are not true. AI has gone through a number of AI winters because people claimed things they couldn?€?t deliver.,I need to think about this. [Long pause.] I think it would be ?€?machines that learn to represent the world.?€? That?€?s eight words. Perhaps another way to put it would be ?€?end-to-end machine learning.?€? Wait, it?€?s only five words and I need to kind of unpack this. [Pause.] It?€?s the idea that every component, every stage in a learning machine can be trained.,Yeah, the public wouldn?€?t understand what I meant. Oh, okay. Here?€?s another way. You could think of Deep Learning as the building of learning machines, say pattern recognition systems or whatever, by assembling lots of modules or elements that all train the same way. So there is a single principle to train everything. But again, that?€?s a lot more than eight words.,That may be a better question. Previous systems, which I guess we could call ?€?shallow learning systems,?€? were limited in the complexity of the functions they could compute. So if you want a shallow learning algorithm like a ?€?linear classifier?€? to recognize images, you will need to feed it with a suitable ?€?vector of features?€? extracted from the image. But designing a feature extractor ?€?by hand?€? is very difficult and time consuming.,?? ?? ?? ?? ,??,The interview also includes the following sections:,??in IEEE Spectrum.
You've made up your mind to become a data scientist.??You've taken every data science MooC, you've eaten a lifetime of pizza at machine learning meetups, you even attended a data science "academy." Why hasn't it worked?,Data Science is not knowledge to be acquired but rather a skill that can be learned and improved through practice.??The number one qualification employers look for when hiring a data science candidate is previous experience. Startup.ML is launching a fellowship to give aspiring data scientists the chance to hone their skills by building real machine learning applications for startups and established data science teams. ??,Upon completing the program, fellows will have direct access to a network of hiring partners, a letter of recommendation, and a portfolio of real world projects.,We strive to have each fellow work on two kinds of projects to give them exposure to the full range of problems they will confront in industry.,We ensure that our fellows work on real world problems sourced through partnerships with established data science teams.??This aspect of the work is well defined, large datasets have already been gathered, and there is an opportunity to learn from leading practitioners.,The challenge fellows will typically confront is how to get results when working in a large team with lots of differing opinions, approaches, skills and priorities. ??,Another key skill that fellows learn by working with an established data science team, is the ability to communicate ideas and how to build consensus. ??We ask fellows to present their work to outside team members on a weekly basis which hones their ability to deliver crisp results.,Startup.ML works with early stage companies in the IoT and Health 2.0 space to help them incorporate machine learning into their products. ??We afford fellows the opportunity to work on these problems along with us. ??The learning opportunity for this part of the fellowship is different from the previous experience of working with an established data science team. ??,There may be a lack of clarity about what exactly needs to be done, the data may not exist (or if it exists it often doesn't have the necessary signal), the founders are uncertain about the direction they want to take their product, etc. ??,Fellows need to learn how to iterate quickly and laser-focus on a minimum viable product.,Data Science is an interdisciplinary field which combines aspects of computer science, mathematics and statistics. Rarely do we see someone that has a deep background in all of these areas. ??We encounter software engineers that are not familiar with probabilistic approaches to problem solving and quants ??(mathematicians, statisticians, physicists, computational biologists, etc.) that can't write a recursive function.,Instead of holding out for the perfect unicorn, we pair fellows from each of these backgrounds to work on a problem together. ??The pair-programming methodology has already proven to be extremely effective in agile software development and we believe it?€?s also the right approach for data science.,While it is true that the problem should dictate the choice of the tool, being active practitioners in the field we have developed some??,.,Julia is a new but very promising language for scientific compute. ??We have successfully used Julia for digital signal processing, state-space tracking and anomaly detection problems.,Fellows are free to pick their choice of tools but using a tool that mentors are familiar with, makes it easier to get help.,At the completion of the program, we help fellows find the right career opportunity in a data science field. Fellows gain intimate knowledge of the startups and the work being done by established data science teams, so there are many options to pursue:,We are accepting applications for the ,.,Data science is an incredibly powerful art, practiced by an incredibly small community of artists. ??We are trying to change this! ??Our objective is to grow the data science community by 500+ active practitioners over the next 3 years. ??We want fellows to become the in-house data scientist of one of our startups or join our partner network of data science teams.,Someone with 2-5??years of professional experience with either a software engineering or quantitative analysis background that has decided to pursue data science as a career. ?? PhD/MS is not required however most quants ??that??apply??successfully??have some??level of graduate training.,Currently the program is only offered in San Francisco. ????,Due to the??immersive nature of learning experience, part-time /??telecommuting??is not an option.,Generally the answer is "No." ??The program is designed to provide last-mile experience for those ready to start a data science career. ??In rare cases we'll consider the application of someone that is still in school if their research work could lead to a??promising??startup.,Our fellowship program??is completely free. ??,Absolutely! ??If you have an interesting startup idea in IoT, Health 2.0 or other areas and want to build your first prototype during the fellowship, we'd love to help.,There are many ways??to??,??in growing the data science community. ??You can help us mentor fellows, become a hiring partner or even provide??a small project for the fellows to work on.
I took only a single biology course during my years in university. My environmental toxicology professor explained that when testing for the LD50 (the dose that kills 50 percent of a population) a certain percentage will probably die right away; on the other hand, some might be able to tolerate unusually high exposure levels. There is a distribution of responses. A related principle applies to stress. People respond differently to stress: some might flee from their stressors (avoid or evade) while others fight (engage). Many stress researchers have concentrated their attention on the person experiencing stress; this is made possible by characterizing stress as a purely internal response. I think that many students experience "exam stress" long before doing their exams; this bolsters the argument that stress is psychological. Perhaps a fair number of environmental toxicologists would point to the importance of external conditions on stress: the differences in internal response might reflect statistic distribution - maybe more involving chance rather than psychology. However, consider the basic argument that there are differences among members of a population. In this blog, I will be discussing the fractal representation of differences among social members. The purpose is to algorithmically assess the progression of environmental change through the distribution of their responses. Not only do I resurrect this notion that stress is brought on by an externality, I suggest that stress involving a population can be used to understand the externality. I have chosen to use technical stock market data for this discussion. Some investors have suffered shocking losses. A few have even shared their stories with me. The stock market is absolutely a stressor. It's not possible to run away from one's apprehensions - again supporting the idea that stress is internal. Yet upon closure of the trading account, the stress seems likely to go away as if external and material (the result of something real rather than merely perceived). I think that both sides are reasonable: stress can extend from external conditions and be internally realized by individuals.,Apart from introducing the general idea noted above (the use of response gradients to sensitize algorithms to social phenomena), I also intend to provide structural details. Indeed, perhaps my main purpose is to share details. I simply need a way to frame the discussion. Stock market activity can be captured in many different ways. For instance, there is the use of moving averages. I will be using kinetic plumes. I sometimes call these differential plumes since they are constructed using the differences between sequenced pairs. (As such, it is possible to build these plumes using patterns resembling genetic code. This is not to say that they are actually genetic.) The sequencing serves as containment for the underlying data. Having disclosed the sequencing structure for these plumes in at least a couple of blogs, I'm uncertain if anybody has tried replicating the objects. Below, I provide my structural interpretation for a plume having "reactionary" sequencing. My reasoning behind the lows and highs on the illustration is as follows: 1) when the past coincides with the present, the current impact is amplified; and 2) when it counters the present, the current impact is mitigated. However, the measurable extent to which the past influences the present is difficult to calculate. I made use of real stock-market data for the pattern below: the Bank of Montreal (BMO in Toronto).,I removed the plume colours to bring attention to the formations, which I consider highly distinctive. In the "classic formation" scheme as shown above, the structures bringing on lows taper to the right; those preceding highs open to the right. In practice, sometimes just the opposite is true (during a prolonged market slump); or there might be lack of coherent tapering and opening. In my previous blog, I discussed how stock-market data is a product of social construction. We are not measuring anything about stocks or the underlying companies but rather the market and its participants. The use of an array or lattice of many plumes can help us to understand the social "environment." I have observed these dynamics using much simpler forms of data: if a data scientist has data from a single person, that data is best associated with that person; if the data is from many people, taken as an aggregate it starts to reflect environmental conditions. For example, if a single salesperson generates sales data, this data reflects just that person. If thousands of salespeople are taken into account, their efforts start to reflect market patterns for the products they are selling. Similarly, if I make use of a large number of kinetic plumes, I enable the detection of broader phenomena affecting the population as a whole. The body is never quite the same as the sum of its parts since certain phenomena are detectable only using the entire organization, system, or ecology.,The plume array shown below all terminate at same trading date: 2015-02-20, which I refer to here as the "set date." The stock symbols all appear on the Toronto Stock Exchange: TD Bank (TD); Bank of Montreal (BMO); Scotiabank (BNS); CIBC (CM); National Bank (NA); and the Royal Bank of Canada (RY). I placed a reference date of 2014-11-17 on one the patterns to give readers a sense of time: there is about 3 months between the reference and set date. Had I purchased $1,000 of each stock at the reference date, accepting fractional shares, the decline after this 3-month period was about 7 percent. (The decline started happening earlier for some of the stocks.) The underlying companies are related to each other by virtue of their shared business markets. However, perhaps more importantly, the plumes are related as a result of those participating as investors. The correction dragging down prices can be characterized as a "social movement": as individual internal responses to external phenomena - bringing forth a collective stress response.,I can offer a numbers of reasons why the correction occurred (the social phenomena). I think each point is worthy of detailed discussion not necessarily suited for a forum on data science. Perhaps the interesting data-centric story is in the following: 1) not all stocks traded on the exchange have been affected by the correction; 2) for those apparently affected, the timing and intensity seem different; and 3) using their plume patterns, the top three stocks appear to respond more similarly than the lower three. It can be said that are similarities and differences in the perceptions of investors that brought about these themes and variations in their plumes. However, I suggest that it is perfectly reasonable to find a collective response in relation to external threats affecting society as a whole. Despite differences in how the threats are recognized, these are not purely "individual responses" linked to perception but also material concerns relating to the market.,It is not possible to say by looking at these plumes when exactly the correction will end. (I have seen corrections last a long time.) Fortunately, this blog is about detection rather than prediction, allowing me to completely side-step the issue. The idea of a "correction" is interesting by the question of what exactly is being corrected. I think there is a belief among some investors that a portion of a stock's price relates to a company's current profitability; another part is more concerned about future expectations. These considerations are both subject to change; but perhaps the most painful dose of reality occurs when one fails to substantiate the other. If we extend this idea to companies more ecologically, their economic conditions (climates) and business settings (habitats) can affect populations more broadly. For example, an agricultural community might decline not because stop buying the products they produce. The weather might become hostile during a particular growing season. Or the primary means of shipment might become impaired. Expectations may have been perfectly reasonable; however, the locus of control had never been the farmer's entirely. Consequently, the thing being corrected might not be price perceptions per se but our understanding of the environment - our recognition of what remains possible in light of materially deterministic conditions. Having a good attitude and positive outlook will not help failing crops.,I think some readers would complain that the plumes seem suspiciously smooth in light of the choppiness of their stock prices. These particular plumes make use of an averaging technique that I call Phi-averaging, which affects not just the smoothness of the plumes but also the sharpness of their slants. Phi-averaging tabulates from the set date to the dates defined by pair-sequencing. Sometimes I use Pho-averaging, which occurs from the sequencing dates back a certain number of days specified by the user. Then there is a mode that I call "Feral" that makes no use of averaging; as such, it leads to aesthetically unpleasant plumes. (Sequenced pairs that are Feral bring about roughness to the plume patterns.) For presentations intended for people, I normally use Phi-averaging, the gentlest. I keep the data Feral if the patterns are meant for machines. I also don't use the entire plume as will be explained shortly.,Although algorithms are used to generate the plumes, the resulting images are probably much easier for humans to evaluate than computers. For computer interpretation, I offer four measurements to summarize kinetic plumes: core, score, above, and below. I strategically placed a few counters inside the main algorithmic cycle while studying the Bank of Nova Scotia, resulting in the summary lines indicated below. These summaries will be presented as bars in the next section dealing with the use of fractals to detect social movements.,This next image contains a snapshot of the plume summaries for a number of stocks. I introduced the placement methodology for the stocks in a blog on the social construction of technical trading data: x-axis represents a gradient of price activity; the y-axis shows the same but for volume. While the plume pattern for each stock is inspired by paired-sequencing, this is also the case in relation to placement. The pairing is determined fractally. As such, the placement is thematic in nature. Stepping back from the illustration below, I believe there is evidence of a "super plume" judging by the amount of red inside. Perhaps the reader should try to find it now. I think that much more research is necessary to properly explain the manifestation of social movements within populations; however, success in this area might make it possible to infer the progression of social radicalization through the behaviours exhibited by individuals in a population. I feel that the main difference between individual constituents and the ecological body rests in the systemic perpetuation in the latter made possible through social entrenchment; this is something that is imposed by the environment but realized by individuals.,In this blog, I described how it is possible to use the individual responses of a population to evaluate phenomena affecting the collective. I have been discussing stocks. The applicability of this technique on other forms of data remains to be determined. Also at question is the effectiveness of the approach on geographic phenomena - that is to say, events that can be distributed spatially. (I have been describing technology called the "Earthshield" that I actually developed many years ago for earthquake and hurricane data intended for geographic distribution.) Nonetheless, in relation to non-spatial data and social phenomena, I believe that fractals can be used to study movement, progression, and migration.
From the descriptions of thousands of Android apps, contributors selected functionalities from a list of 25 options. Those options ranged everywhere from "uses the phone's flashlight" to "entertainment." This data set includes the original description of each application, the assigned functionalities, and, like all our data sets do, the confidence score of each judgement.,A large data set of various labeled biomedical images. They range from x-rays to MRIs to graphs and diagrams (like you'd see in a biology text). The data set includes live image URLs, the specific modality for each image. ??,One of the many fascinating language data sets in our library, this one looks at the familiarness of real words and made-up ones. Contributors were given a fantastical word (like granjug) to stand in for a real one (like canine). The nonce word was used in a sentence and contributors ranked how like the words felt, based on contextual clues, word sound, and so on.??,Currently our largest data set at 225,000 rows, contributors were given an image and asked whether a given word described that picture. This data set includes live URLs for every image, the assigned descriptor, and a simple yes/no judgement (with confidence score).??,The data set upon which we based our??,??on. In it, we looked at major U.S. air carrier customer service handles and analyzed what sort of complaints each airline received. This data set includes tens of thousands of tweets, their corresponding carriers, the positive, negative, and neutral sentiment, as well as the specific reasons why users were negative about airlines.??
This week went exceptionally well for me. Not only did I finally move to my new apartment, I also got to hang out with my friends and sing some karaoke. But the best part was the call from my best friend who currently lives on the other side of the world (Jerusalem, Israel) and works for a non-profit organization. As soon as I picked up the phone she announced that she just got admitted to U Penn Law program and she starts in September 2015. Now she is waiting for NYU to respond and if NYU extends its acceptance she will choose NYC over Philly (where U Penn campus is). I was super excited anyway. For one thing, I was psyched to learn that she is going to Ivy League school and for the other thing, this meant that I would get to see her every weekend at a minimum (it is only an hour to get from Philly to NYC)! This is how I came up with an idea for today's Start Data Science Simple blog post. I know that many people dream to study at Yale and Harvard Law Schools, and the biggest question for them is - "What should I do to get accepted?" I will take a quantitative approach to answer this question.,I was just wondering how many applicants are now waiting for application decisions from their dream Law programs. I'm guessing thousands... But I found 7,512 applicants who were brave enough to register with??,??and share their application details with the world. Here is what they reported:,Unfortunately, this website doesn't provide download capabilities, so in order to use this data I had to scrape the website. In my??,??I also had to use a scraper to obtain movie box office data, and later on I got some requests to share the data with other data science enthusiasts. So this time around I will share with you all how I typically use scraper tools so you can do it yourself :),There is an awesome tool designed for non-programmists like myself who need to scrape structured data from websites that don't provide download capabilities. Check out??,??where they explain how their tool works and how it literally takes minutes to get the data you are looking at into csv or json format,.,After I specified what data objects I want to extract the hard part began. There are 100 users listed on the page and there are 76 pages in total, so ~7,600 rows needed to be extracted. KimonoLabs recommends to use their??,??to continuously extract data from all pages, but this only works if there is a button "Next" that allows a user to go to the next page. In my case this button was absent, so I had to extract the data and then enable,??with generated URL list where I specified that I need continuous data extraction from page 1 to page 76.,I also noticed that if I click on each individual username (I had to register on the website first), I will be able to see the user's profile that includes information like user's gender, school s/he is attending, application year and college major. I decided to scrape this information as well. Finally, I synced Kimono API with Google spreadsheets and extracted my data into a google spreadsheet.,Now when I got my data I realized that before I can do any analysis I have to transform it in a way that allows slicing and dicing the data, creating visualization and performing statistical analysis.,Chart 1 shows how I got my raw data from Kimono into Google spreadsheets. The main problem with original data file was that information in Applied, Accepted and Rejected columns was very unstructured. These fields list all schools an applicant applied to, got accepted or rejected by, but since my data is represented at the applicant level, I had to have a better way to link students to each school.,In my opinion, decisions related to data transformations have the biggest impact on the data analysis because data transformation:,So here goes...,.,Again, I took my own lesson and went back to my main question.,Why do I need to present applicants uniquely in order to answer this question? In the end of the day, Tatiana with LSAT 170 and GPA of 3.7 may be accepted by Columbia and rejected by Yale, but from the application standpoint there will be two Tatianas who applied to two programs, but one case would be accepted and the other one would be rejected. So it makes more sense for me to have a dataset where??,??are uniquely presented, not people. In other words, I will have a dataset where each row is a unique combination of username, LSAT score, GPA, School applied to and status of application (Accepted/Rejected).,Since my best friend got into U Penn I decided to only focus on the top 10 schools in the US Law School ranking (which includes U Penn) because it is a pain in the neck to manually transform free text into columns and then into rows (I did it in excel). Here are 3 steps I followed:,Chart 2 shows how the dataset looked like after I completed my transformations.,Now every row no longer represents a unique applicant, instead it represents a unique "case". In other words, a combination of username, LSAT, GPA, School and Status forms a unique row.,Chart 3 shows the dataset I worked with after I appended students' application year (when a student applied) and their college major (I got this data from scraping each individual's profile page).,First, one of the things I was interested in was the "score profile" of students who applied to top 10 Law Schools. What were their LSAT and GPA scores? Were these applicants accepted or rejected? Do all top schools accept students with the same GPA and LSAT scores or is there a difference?,Chart 4 shows average GPA and LSAT scores broken up by school.,It looks like top schools are accepting top students (what a surprise?) with average GPA ranging between 3.7 - 3.9 and highest LSAT score between 170.6 - 174.0 (out of 180). So second, I wanted to know if there are any outliers in Law program choice. In other words, are all students who get accepted by Yale and Stanford in the first 1% based on GPA and LSAT? Or maybe there are applicants who did not have as high GPA and/or LSAT but still made it to the program? Does college major play any role in these schools' decision-making process?,Chart 5 shows visualization I put together to answer these questions. This visualization is interactive. On the top you can see a scatterplot that shows each applicant's score (GPA & LSAT) along with his/her acceptance status (Accepted/Rejected) for all top 10 schools. On the right you can select a school you are interested in (my BFF will most likely click on U Penn and NYU) and see only students who applied to this school and their acceptance status. You may also select application year and see historical dynamics of applications. On the bottom you can see breakout of application status by college major. This histogram also transforms as you filter by individual Law school and/or application year.,As mentioned earlier, top schools demand top talent. If you want to be accepted by Yale, Harvard, Stanford and Columbia Law School on average you will need GPA of 3.8 and LSAT of 173-174. Oh well...what if you don't? Any chance to still get accepted?,It looks like you still have a chance ;) There were ~40 students (out of 600) who had LSAT < 173 and GPA < 3.8 and still made it to the top 10 Law Schools.,Another big factor in Law school admission decision-making process, it seems, is applicant's major in college. In general, students with Engineering, Economics and Philosophy major are more preferred by Law schools than students with Business or Journalism degrees. Students with Law major (my BFF has Law Major) have 57% of being accepted by the top 10 Law school, 100% by U Penn (,) and 60% of being accepted by NYU (,).,There are a lot of conclusions that can be derived from this exploratory analysis. What did you learn?,Or maybe there are other ways this data could be presented? How would you look at it?
Jake Drew, Marie Vasek, and Tyler Moore,Computer Science and Engineering Department Southern Methodist University Dallas, TX, USA,{jdrew, mvasek, tylerm}@smu.edu,Large-scale attacks are commonplace in the e-commerce market for counterfeit goods. Moore et al. recently estimated that as much as 32% of online search results point to websites selling counterfeit goods with 79% of those results including at least one fraudulent online retailer within the first page search results [10]. They estimate that 33% of the time, the first hit users are presented with while searching for top selling, brand name merchandise is a link to counterfeit goods.,Wang et al. investigated legitimate websites that were compromised to promote luxury goods [11]. They identified distinct "campaigns" tied to the affiliate programs whereby sellers of counterfeit goods pay for referral traffic, using clustering techniques described in [7].,This case study complements such "macro"-level investigations by delving deep into the nuts and bolts of a particular breach of the website jakemdrew.com, operating on GoDaddy?€?s shared web hosting platform. This website is but one of many websites running Microsoft IIS that has been compromised to promote websites selling counterfeit goods.,Figure 1: The iSKORPiTX hack page replaced the homepage of 38,500 websites in 2006,The paper reviews the steps taken to deobfuscate code running on the compromised server in order to reverse-engineer its operation and help trace the attack to its source. We also estimate the prevalence of such compromises on GoDaddy?€?s network.,In December 2014, Internet sources reported a mass compromise of websites located on shared web hosting servers running Microsoft IIS. Specifically, infected servers and their associated websites were being used to promote selling Black Friday and Cyber Monday counterfeit goods within search engine results [9]. Because GoDaddy appears to be the largest host of webservers running IIS, their customers have been affected most. Previously, a similar IIS vulnerability in 2006 impacted ?€?tens of thousands?€? of GoDaddy customers causing over 38,500 websites to be defaced in a single day [12]. Figure 1 shows the results of the famous iSKORPiTX hack page which replaced the homepage of its targeted websites around 2006 [12].,Figure 2: An invisible <div> tag injected into a compromised cyber monday hack website (click to enlarge),The entire scope of the most recent hack impacting IIS webservers is currently unknown. However, the internet secutity??company Sucuri reported in December 2014 [9] that they have independently confirmed 1,782 domains and 305 IP Addresses ?€? 61% of which are hosted on GoDaddy representing 1,095 websites and 95 hosts. While these numbers alone are very concerning, representatives at Sucuri concede that their list only represents "the tip of a very large iceberg" [9].,Both of the aforementioned compromises belong to a much larger and more general cybersecurity problem. Once such a compromise occurs, it can be nearly impossible cleanup all of the security holes, or backdoors, which are left behind. Once a criminal has write access to a web server?€?s directory structure, a backdoor could be left in any number of places. Unfortunately, many companies experiencing a breach merely patch the vulnerability believed to cause the breach and delete any inserted content. However, any number of backdoors could remain indefinitely allowing criminals ongoing access to the host.,Before the iSKORPiTX hack in 2006, reports as far back as April 2005 reference the SSFM directory and scripts which are believed to be responsible for the hack [12]. In this case, some backdoors may have been in place for almost a year before the intended payload was delivered. This underscores a common strategy for criminals ?€? start out with very small exploits and escalate over time as more profitable opportunities arise.,In the case of the Cyber Monday and Black Friday exploit, we will demonstrate how the most recent IIS vulnerability was used first to install a backdoor on an IIS webserver and when used for blackhat SEO purposes, injecting fake links to websites selling counterfeit goods. Furthermore, we uncover striking similarities which suggest that the hackers?€? method for gaining initial access to shared IIS web servers may be silently operating under the radar since as far back as the 2005 attack, leaving researchers to wonder if the original vulnerability was ever successfully resolved.,During December 2014, an unusual <div> tag showed up on the website jakemdrew.com, a GoDaddy-hosted IIS webserver maintained by one of the paper?€?s authors. The first modification occurred on the website?€?s home page and included a new <div> tag at the bottom of the page containing a number of website links with text such as:,Figure 3: The original obfuscated function dedicated to the purpose of decrypting strings.,Furthermore, the entire <div> tag was invisible as shown??in the style attributes of Figure 2.,Less than two weeks later, the same <div> tag was updated and almost all of the original websites were removed. This confirmed that not only a breach had occurred, but that the criminals were still able to update the content.,The second update prompted a thorough search of all directories on the web server where an unusual file named picture.asp was located in the Scripts directory. While it was obvious this was the hacker?€?s backdoor, the contents of the file were completely obfuscated and nearly impossible to decipher in their current form. Figure 3 illustrates only one of the obfuscated functions used by the script.,We now describe the steps taken to deobfuscate the backdoor script.,Many production ready web programming packages such as jQuery [4] are ?€?minified?€? to remove all characters unnecessary for successful compilation. This typically removes extra whitespace and sometimes uses additional techniques such as??shortening variable names to shrink the overall package file size as much as possible for efficient transport over the Internet. This is also a form of obfuscation as the code becomes nearly impossible for humans to read.,When reviewing the script the first and most obvious clue is that the script was written using VBScript. This can be identified in Figure 3 where the LANGUAGE and CODEPAGE attributes are set. We were then able to quickly ?€?prettify?€? the script using the website , to properly indent the VBScript code. Figure 4 shows the obfuscated code after it has been properly indented making it much easier to proceed further with the deobfuscation process.,Figure 4: A ?€?prettified?€? version of the Figure 3 function highlighting all instances of a single variable.,The next obfuscation technique identified was the extensive use of matching length variable names using only the two characters ?€?X?€? and ?€?7?€?. The variable XX777X can be seen occurring 10 different times within the function displayed in??Figure 4. However, since all variables within the code have been named using matching length combinations of the letters ?€?X?€? and ?€?7?€? it is very challenging to tell them apart.,Figure 5: The final deobfuscated version of the Figure 3 function including meaningful variable names.,Figure 5 shows the final version of the deobfuscated Figure 3 function with more meaningful variable names included. This function was the first to become of interest for three primary reasons:,The class initialization routine shown in Figure 6 highlights yet a another obfuscation technique. All 201 string values within the script are further obfuscated and made unreadable to the human eye. In fact, these strings are also meaningless to the VBScript interpreter. The deObfuscate() function shown in Figure 5 is used within the script to convert all 201 strings into meaningful values which are hidden from humans yet resolved during the script?€?s execution.,Figure 6: The class initialize routine shows extensive use of the deobfuscate function shown in Figure 5.,Numeric values are also obfuscated using a more simplistic approach. Every place a numeric constant is used, that constant is replaced with a more convoluted equation. For example, the statement Type = 2 can be obfuscated to Type = (11 * 24 - 262) and the statement mode = 3 can be obfuscated to mode = (43 * 105 - 4512). While this approach may appear rudimentary, when combined with multiple other methods of obfuscation, this further hides the true intent of the script.,The example function in Figure 7 shows all three of these techniques used within the malicious picture.asp backdoor file.,Figure 7: Three different obfuscation techniques used within the same malicious function.,Since VBScript is very similar to VBA (Visual Basic for Applications), we used Microsoft Excel to quickly port the final version of the deObfuscate() function shown in Figure 5 with no??additional coding changes. Next, a second VBA function was written to parse the picture.asp file replacing all instances of the deObfuscate() function with its intended output. For example, the class initialize routine previously shown in Figure 6 can now be seen in Figure 8 revealing all of the intended text inputs.,Figure 8: The class initialize routine with all deobfuscate() function calls replaced with deobfuscated text.,After reviewing the picture.asp backdoor script, it is clear that the script is intended to ensure that the criminals have a method to access and download files to the infected client machine. Once the backdoor script is placed on the web server, it can be activated by the criminal simply visiting or loading the file using a web browser or another program. For example, the criminals could access my infected web server by navigating to ,.,Once the script has been activated, the script variable csvalue points to a query string within the http request which is expected to contain the file name that is targeted for download from the attacker?€?s command server located at the obfuscated IP address hidden within the script. In this particular case, the expected query string value containing the target file is named video. The infected client then performs a GET request to the attacker?€?s command server downloading the appropriate file location provided within the video query string variable. This variable can be modified ?€?on the fly?€? using any query string parameter value with the URL such as picture.asp?video=targetFile.htm. In this manner, the actual file on the attacker?€?s command server need not be included within the script and is further obfuscated from detection. The targeted file is downloaded using a binary adodb stream. If the download is successful, the script performs a series of regular expression searches targeting all href URLs within the downloaded file contents pointing at HTML, asp, htm, css, gif, jpg, and png files. Each of the URLs identified are updated to match the client?€?s directory structure for the targeted site.,For example, the regular expression href="\"/(.*?)\.(html|asp|htm)\"" is used to target all URL?€?s pointing at html, asp, and htm file types. Each URL located is then replaced with the second regular expression href="&filename&"?"&csvalue&"=$1.$2". On our particular server, this expression translates to href="/Styles/picture.asp?video=filename" where filename contains the original file name and file extension requested in the link. This behavior allows the criminals to display any web page which is located on the attacker?€?s command server. The malicious script will actually download and install any missing files required to support the successful rendering of the criminal?€?s web page content. In addition, the script will create any folders missing in a given URL?€?s mapped file path on the targeted server to ensure the referenced content will successfully render.,At first, it may seem counterintuitive that all links to html, asp, and htm file types are updated to point recursively back to the picture.asp file. However, when each link is activated, the script can be executed once again to download and install any files and folders necessary to render and display the requested link?€?s content.,Using the picture.asp backdoor script in combination with any redirect script placed on any page within the targeted server allows the criminals to display dynamic content from their attack command server. In this particular attack, the criminals were observed creating both blackhat SEO link farms in an effort to boost page ranks for counterfeit good websites and using the picture.asp backdoor script to display dynamic counterfeit goods web content at will.,After the text deobfuscation process is performed on the entire script, the new text values reveal many important features of the criminal?€?s backdoor program which could reveal the hacker?€?s identity. In addition, we created the ?€?Link Spider?€? to recursively follow all of the links originating from the infected webpages at jakemdrew.com and identify malicious link farms and website redirects which may be pointing to websites selling counterfeit goods.,We can now tell that the script code X7X7X77.dizhi = XX777X("bf]e?€?]aba]?€?fb") actually points to the criminal?€?s IP address for the attacker?€?s command server. Decoded, the new text reads backDoorObj.dizhi = "37.61.232.173". A quick WHOIS on that IP reveals that server is hosted on the UK internet service provider ?€?Host Lincoln Limited?€?.,The script sets a very unusual request header prior to making its HTTP GET request to the criminal?€?s server. The suspect request header value is X-Realsdflkjwer3l234lkj234lkj234l-IP. This particular request header is always set to the originating IP address of the client connecting to the criminal?€?s command server. The X-Forwarded-For or XXF request header is the ?€?de facto?€? standard for identifying this information [8]. Setting this value within such an unusual request header appears to indicate that the hackers are encoding a message within the GET request to the criminal?€?s server that this particular incoming request has originated from an infected client.,A quick search of the suspect request header value ?€?X-Realsdflkjwer3l234lkj234lkj234l-IP?€? on Google turns up only two hits. The first hit appears to be a yet another infected website with a very similar copy of the backdoor script which is actually in a deobfuscated form [5]. This site also turns up a second ip address pointing to a criminal server 69.163.33.18 hosted by DirectSpace Networks, LLC. in Portland, OR. The second deobfuscated script also confirms many of our assumptions regarding the picture.asp file.,The second Google hit provides even more valuable information by locating the same request header within a PHP reverse proxy script which had been decoded at , [3], a website associated with Sucuri SiteCheck. The PHP reverse proxy script also included a copyright URL pointing to bseolized.com which turns out to be a website selling its ?€?shadowMaker?€? software for industrial-strength cloaking and IP delivery. Based on its description, this software is a blackhat SEO tool generating phantom pages and shadow domains for its users [1]. The tool currently sells for 3497 USD. The occurrence of the X-Realsdflkjwer3l234lkj234lkj234l-IP request header within both scripts appears to tie the US based owners of bseolized.com directly to the GoDaddy shared web hosting mass compromise.,The bseolized.com website also sells a product called ?€?Template Spinner?€?: an obfuscation software package for generating truly unique content for each shadow domain created [2]. This is concerning since the software uses many of the same obfuscation techniques used within our picture.asp script, but would make it challenging to locate the sites generated by the Shadow Maker software. This tool currently sells for 495 USD.,A program named the ?€?Link Spider?€? was written using the C# programming language. The ?€?Link Spider?€? accepts a list of urls as input and proceeds to check each url for the hidden <div> tags left by the Cyber Monday hack. The program also recursively follows all link urls collected within the targeted <div> tag applying the same logic until there are no more links left to follow.,We started out by searching for opening <div> tags ending with opacity:0.001;z-index:10;?€?>. All searches were also case insensitive. During identification of each infected <div> tag we collected all link urls, and the link text included within each link <a> tag. All link tags within the infected div were identified using the following regular expression: (<a.*?>.*?</a>).,After reviewing the preliminary results, we identified three additional hidden html tag elements which also included bad links:,These elements were integrated into the Link Spider?€?s search criteria.,In addition to collecting the infected links, we also searched for both inline and linked <script> tags containing redirects which were only specific to the major search engines. For example, we searched for any scripts containing inspection of the HTTP REFERER server variable for the major search engines Google, Bing, AOL, and Yahoo using script code similar to: if(document.referrer.indexOf("goo gle")>0) Then self.location=... Furthermore, this code must include a redirect which directly follows the HTTP REFERER condition.,Using this approach the ?€?Link Spider?€? was able to identify??a total of 29616 links which were directly referenced by the Cyber Monday hackers and specifically related to jakemdrew.com (either directly or indirectly). Table I shows all infected items identified by the ?€?Link Spider?€?.,Backdoor files similar to the picture.asp file were also located at the root of grass.ag, which is another website hosted at jakemdrew.com. These files give a clear picture of how the overall hack operates. A quick search for grass.ag on Google reveals the results displayed in Figure 9.,Figure 9: Google search results showing grass.ag selling counterfeit Nike shoes.,When the Google link is followed, the backdoor script redirects anyone searching for grass.ag to poshjordan.com, a website selling counterfeit Nike footwear. However, the backdoor script only redirects traffic which is referred by the major search engines Google, Bing, Yahoo or AOL. This is accomplished within the backdoor script by requesting the server variable HTTP REFERER and only redirecting the website when it contains the appropriate value. For example, Figure 10 shows the presentation of grass.ag when accessed directly from its URL vs. a referral from a major search engine. Using this technique, the hackers avoid detection by webmasters and individuals visiting an infected website directly using its URL.,Figure 10: The grass.ag website redirects to poshjordan.com only when the request comes from a major search engine.,Using the approach described above, hackers are able to create any number of redirection websites which eventually lead to the solicitation and sale of counterfeit goods to unsuspecting search engine users. Search term poisoning must be used to ensure that each of the redirection websites have the greatest opportunity to show up within a search engine?€?s search results. This is accomplished by injecting large numbers of malicious links into infected web pages which then influence the placement of redirection websites within the search results for a particular search term.,As shown in Table I, a total of 27616 links using 2451 unique search terms were identified within links associated directly with the jakemdrew.com hack. Table II shows the top 20 search terms used which represent over 60% of the total links identified.,We manually reviewed a total of 63 unique websites selling counterfeit goods which were identified by our Link Spider. There were a total of 46 websites which were still active and selling counterfeit products at the time when we visited the URL, and 8 of these websites had been shut down under a DMCA takedown notice.,Table III shows the primary brands represented across each of the 63 websites. When a shop sold more than one brand, we selected the brand which appeared to receive most of the web page?€?s content. Nike was the top counterfeited brand observed followed closely by Louis Vuitton.?? However, Louis Vuitton also had the highest level of brand enforcement observed. This was represented by the largest total number of active DMCA takedowns.,A large majority of the websites which we reviewed (63%) were registered in China. Table IV shows each of the website?€?s registration countries.,While the websites were predominately registered in China, a large majority (68%) were hosted in the US and Sweden. When inactive websites from unknown countries are removed from these calculations, 74% of the counterfeit goods websites are registered in China, and 88% are hosted within the US and Sweden. These findings are consistent with Moore et. al. whom observed that websites selling fakes are 17 times more likely to be registered to a Chinese person or business while counterfeit producing countries such as China are more likely to host these websites in countries with stronger IT infrastructures. [10],Larger clusters within the counterfeit storefronts also seem to appear at both the website host and registration organization levels. For example, the two companies ?€?SHANGHAI MEICHENG TECHNOLOGY INFORMATION DEVELOPMENT CO., LTD.?€? and ?€?GUANGDONG NAISINIKE INFORMATION TECHNOLOGY CO LTD?€? are listed on 37 out of the 40 websites registered in China. In addition, only 3 hosts represent 67% of these sites. Table V shows all of the hosts associated with the counterfeit websites.,The Chinese company ?€?GUANGDONG NAISINIKE INFORMATION TECHNOLOGY CO LTD?€? is also cited by legitscript.com as being number two on their ?€?Top 10?€? list of safe haven registrars where rogue Internet pharmacies cluster. This list was published during October 2014 and identifies the company as being ?€?considered non-compliant by LegitScript for a longer period of time?€?.,We looked further at Jazz Networks Inc., which is a hosting company located in Tampa, FL. We selected the company based on its relatively small number of hosted websites at 528. While manually reviewing the first 100 websites hosted by Jazz listed on ,, we identified 45 websites selling counterfeit goods. If this percentage is representative, then the hosting company facilitates an estimated 238 counterfeit store fronts [6].,According to , the registration organizations for the 63 counterfeit storefronts that we reviewed are associated with an additional 150,212 other domain names. We also obtained web traffic statistics for 24 of the 63 websites. These 24 websites averaged 2,030 visitors per day each with a total of 48,720 visitors per day. At similar traffic volumes, Jazz Networks Inc., would support over 482,328 visitors per day or close to 14.5 million customers per month browsing and purchasing illegal counterfeit goods.,We took two general approaches to approximate the continued prevalence of this attack. Our first approach scanned 74 528 domains on IIS shared servers hosted by GoDaddy on January 29, 2015. Of these, 41 361 were parked at the time of the scan. In the remaining domains, We looked for the filter:alpha(opacity=0);opacity:0.001; part of the inserted div tag which is uniquely identifying and a part of every update we have noticed. From these, we found that 128 of them (0.3%) showed signs of this particular infection.,Our second approach mirrored Sucuri?€?s approach in their analysis [9]. We did a targeted Bing search on ip:IP cyber monday for 50 randomly chosen IP addresses from GoDaddy?€?s IP range running Microsoft IIS server software (out of 3 871 candidates). We observed that 24% of the IPs showed results for the Cyber Monday hack. Additionally, we noticed this hack remained prevalent in the IP addresses that Sucuri found hacked in December 2014 (50% of their sample of GoDaddy shared hosting IIS domains).,Hence, we conclude that this particular attack vector remains prevalent in the wild.,We have presented an in-depth examination of an attack targeting shared webservers running Microsoft IIS. The attacks are part of a blackhat search engine optimization (SEO) scheme to promote websites selling counterfeit goods. We have deobfuscated a backdoor running on the GoDaddy-hosted website jakemdrew.com, revealing how the vulnerable website can be repeatedly updated to promote websites on demand.,Using other infected websites connected to jakemdrew.com, we have shown that this website is part of a counterfeit goods supply chain representing over 27616 links used to poison search results within the major search engines.?? We have estimated that one safe haven host, Jazz Networks Inc., could be supporting up to 14.5 million customers per month visiting websites selling counterfeit goods.?? We have also shown that a large majority of the active counterfeit goods storefronts (74%) are registered in China.,Why does this matter? We showed that on GoDaddy alone, at least 0.3% of its 36 million shared hosting websites and 24% of around 4 000 shared hosting servers running IIS have already been hacked in the same manner. Furthermore, these servers have remained hacked for at least one month after yet another attack. Despite the relative simplicity of the backdoor, it appears to have operated with impunity for many months, if not years. It is our hope that by explaining how the hack works and estimating its prevalence, we might motivate the security community to eradicate the mass compromise at scale.,This work was partially funded by the Department of Homeland Security (DHS) Science and Technology Directorate, Cyber Security Division (DHS S&T/CSD) Broad Agency Announcement 11.02, the Government of Australia and SPAWAR Systems Center Pacific via contract number N66001-13-C-0131. This paper represents the position of the authors and not that of the aforementioned agencies.,Combining over 15 years of Fortune 100 experience in banking analytics and revenue optimization with cutting edge Computer Science techniques, Jake Drew is producing innovative results in the fields of Bioinformatics and Cybercrime Economics.?? Jake Drew is currently a Ph.D. student and research assistant at Southern Methodist University under the supervision of Professors Tyler Moore and Michael Hahsler.?? He has also recently acted as an expert witness, expert assistant, and technical consultant in over 20 intellectual property related cases.?? He holds a Master of Computer Science from Southern Methodist University, and a B.S. degree in Computer Information Systems from The University of Texas at Tyler.?? Previously he worked as a Vice President in Revenue Optimization at Bank of America, spending almost 15 years in the banking industry.,Tyler Moore is an Assistant Professor of Computer Science and Engineering at Southern Methodist University. His research focuses on the economics of information security, the study of electronic crime, and the development of policy for strengthening security. Tyler is Editor in Chief of the Journal of Cybersecurity published by Oxford University Press, and serves as Director of the Economics and Social Sciences program at the Darwin Deason Institute for Cyber Security. Previously he was a postdoctoral fellow at the Center for Research on Computation and Society (CRCS) at Harvard University and the Norma Wilentz Hess Visiting Professor of Computer Science at Wellesley College. He holds B.S. degrees in Computer Science and Applied Mathematics from the University of Tulsa. A British Marshall Scholar, Tyler received his Ph.D. from the University of Cambridge under the supervision of Professor Ross Anderson.,Marie Vasek is a PhD student in the Computer Science and Engineering at Southern Methodist University and the research scientist at StopBadware. Her research interests include security economics and cybercrime, particularly web-based malware.


The subject of this blog might??seem rather rudimentary for those who fully understand the importance of properly managing data. For those people, hopefully, you will find the post worth reading and provide constructive feedback and augment the discussion. The purpose of this post is to highlight the necessity to keep data clean and orderly so that the results of the analysis are reliable and trustworthy - if data integrity is intact, information derived from this data will be trustworthy resulting in actionable information.,The title for this post is meant to be a bit tongue-and-cheek. ??The word "data," over the years has seldom been used singularly to mean anything other than the standard definition:,However, the word data takes on different meanings when coupled with other words. In some cases, the coupling of two words conjures up a fairly accurate meaning, and in other cases, I believe it can be very confusing. For instance, Data Analysis makes sense; values are looked at and analyzed to extrapolate some sort of meaningful information. Data Modeling is a bit ambiguous if the concepts of Normal Forms, Relational Integrity, Dimension and Fact Tables, Stars, Snowflakes, and Cubes ??are not readily understood. Still others, like Big Data, add just as much, if not more confusion. What is Big Data? ??Is it a really big number, or very voluminous data, with a very high velocity data set, and a lot of ??variety (I had to stick with the V's, no matter how contrived),Still, Big Data gives you an idea that you are dealing with a lot of data, and with a little imagination an IT professional can surmise with just a little reading, that Big Data poses great challenges on how to obtain near real-time values with existing operating systems and analytical tools. Big Data might possibly require a different way of looking at data all together. In fact, Big Data does require us to look at data differently. There is no question about it, Big Data that satisfies the definition of at least the three V's listed above, requires a clustered operating system, with distributed storage, with distributed processing across commodity hardware (bring in Hadoop).??With a little imagination you can see how taming these large data sets can provide a wealth of meaningful information, and be truly a disruptive technology.,That said, the quality of the results derived from Big Data, Data Analytics, and Data Mining technologies are only as good as the data. Properly managing, and gathering the data in the first place will save money and time, reduce errors, and simplify the process of gleaning knowledge from a data set. ??Properly managing data involves a variety of approaches that maintain Data Integrity throughout the lifecycle.,Before I go further, I want to explain that this is not an attempt to revive the teachings and virtues of EF Codd, CJ Date, and WH Inman (the list could be much longer). Each of these made significant contributions to the world of data management and engineering, and their contributions are still applicable today. To bolster that claim, I would refer you to more contemporary subject matter experts like??,??and his paper on ",", and numerous other postings from professors, data scientists, and companies that are producing tools to work with messy data. Performing any proper data analysis requires a thorough understanding of the data being analyzed. Furthermore, there are many data sets that are simply not suitable for analysis until extensive processing of the data has occurred. Why not do what is necessary to ensure data integrity and avoid the hassle and costs associated with cleaning the data afterwards?,Data is everywhere today. You can go to numerous online sites where data is made available, and you will find a thoroughly defined Application Program Interface (API), and a data dictionary that defines each data element in the set. Take a look at some of the following sites and you will find either an API for extracting data, or a csv file, along with a definition of the attributes. Some of the data is in better shape than others, but at least some basics are provided to get you started:,(For examples on how to access some of these data sets using R, see??,),Once the data is acquired, the quality of that data must be evaluated:,Data Integrity should, in my opinion, be enforced at the source to the greatest extent possible, to avoid unnecessary work at the end. I hate to bring it up in this day of Hadoop, but Relational Database Management Systems (RDBMS) still have their place in the world of data management. As stated earlier, there is no question about it, Big Data that satisfies the definition of at least the three V's listed above, requires a new approaches, e.g., Hadoop. However, if your data is not that big, consider more traditional means of managing your data to still obtain cutting edge advantages, like predictive analysis. Analysis of good data is worth a ton of analysis on bad data. Even in a Big Data environment, there are principles that, when enforced, will ensure the quality of the data, and enhance its ??value.,All of this discussion should be taken in the context of the business, or scientific use cases. Not all data needs to be perfect at a precise moment in time. However, there are proven technologies that enforce the integrity of data, and unless there is a good reason for not implementing them, it is my belief that it is better to err on the side of integrity.,Let me provide some examples of what I am referring to regarding maintenance of data integrity. Data in and of itself is simply data. ??Values that stand alone have very little meaning without a given context for meaning. For instance, what does the data in the following table tell you?,There are numerous problems with this data, and if there were thousands, or millions of rows of this data, there would be a considerable amount of work involved. ??Just to simplify, here is what each column, or attribute is supposed to be:,There are too many problems to address all of them here, but for starters:,Long way to go just to state that Data Integrity is still, and will always be a critical part of data management. No matter what technologies are in play, if the data is bad, then the information coming out cannot be trusted. ??Hopefully, we start hearing the words "Data" and "Integrity" used together in the context of Big Data discussions.??,If you made it this far, you either agree with me and understand the point(s), or you are thinking this guy is stuck in the 90's. ??If you agree, good! If you are of the latter group, please wait for the next post where I will discuss how some of the above concepts apply in the context of Big Data solutions. ??,??,??
I published two such , about a year ago, ,. This one is a new one, focusing on machine learning libraries (R and Julia). And it is interactive, with access to the various libraries listed in the table, when clicking on an element (only ,).,Here's a non-interactive screen shot (does not cover the whole table):
I've received an unsolicited email today from Pedro Marcus, from DataOnFocus. While usually I don't even open them due to the volume that I get each day, this one was actually very interesting, thus I'm sharing it with you.,You can check the full list ,.
It probably comes as no surprise, but we talk to a??,??of data scientists at ,. We like learning the tools they use, the programs that make their lives easier, and how everything works together. Today, we'll really pleased to unveil the first of a three-part series about the data science ecosystem. Here it is in infographic form because, let's face it, everybody likes infographics:??
 is turning out to be one of the hottest growing fields for data analysts & scientists. Although, there is ,, the general connotation implies a function, activity or person which is primarily focused on growing a set of metrics such as users, revenue, visits & profits. You can see an ,.,We examined a set of job postings which had required growth hackers in their job descriptions & the requirements that they made on data analysis/science skills or the lack of it.,While all the below job description examples used the growth hacker term liberally and meant different things, almost all of them asked for a rare mix of creative and analytical skills to jumpstart growth.,StackCommerce is the leading platform to discover, share, and buy trending tech. StackCommerce is looking for an experienced Growth Hacker that will be responsible for developing, implementing, and measuring tactics for scalable customer acquisition and remarketing across our entire network of 500 publishers.??,Data Science/Analyst Qualities:,The Muse helps people answer the question ?€?What do I want to do with my life??€? It offers a one-stop destination for engaging job search, smart career advice, and long-term professional development.,Data Science/Analyst Qualities: Gathering & analyzing data like a boss. You understand numbers can tell you what is happening, but not why. You know when it?€?s appropriate to get qualitative answers to your questions. You are comfortable using data to make changes and drive growth.,Shopkick helps consumers discover great deals and earn in-store rewards at over 270,000 major retail locations.,Data Science/Analyst Qualities:,McMURRY/TMG, the largest independent content marketing agency in the U.S.,Data Science/Analyst Qualities: You?€?re part growth hacker, part data scientist and their favorite question is ?€?but, why??€? Successful candidates know that analytics is about more than KPIs and metrics, it?€?s about understanding the story behind the numbers.,Common Census builds incredible software that makes benefit enrollment and administration possible for normal, everyday people.??,General Skills Required:,BuzzFeed is the social news and entertainment company.,General Skills Required:,This role is within a startup division of Dannon, the MNC food company.??,General Skills Required:,??to comment on this post.
Good data is the driving force behind successful marketing. Data can be analyzed to determine what your customers are looking for, what will drive them to purchase, and to establish a best prospect profile. According to a report by GlobalSpec, the primary marketing goals for manufacturers are customer acquisition (43%) and lead generation (29%), with 54% planning to increase marketing spend., However, many firms in the manufacturing industry are overwhelmed with the sheer volume of data they face on a daily basis. And in a challenging economy, getting this data under control is imperative to sustain a competitive advantage.,As data tends to accumulate from many sources within the organization, it develops specifically for each individual department?€?s needs. Formats vary, discrepancies develop, and a multiple of application systems are created to support the growing data.,The larger the company, the stronger the risk is for communication breakdown arising from distorted information. Silos of data within an environment such as this are usually not cross-referenced. The result is what is commonly referred to as ,.,Establishing consistent data management methodologies across an organization is not a new concept, but one that can be extremely challenging. So how do you get to that single version of the truth, where everyone can depend on the same view of clean data?,Here is a look at an integrated 5-step approach to better data management. By incorporating a data quality and integration solution that follows this methodology, you can ensure your data is fit to be shared across the enterprise for any number of new operational and marketing opportunities.,Addressing data chaos to achieve a single version of the truth ultimately leads to a better understanding of end-users and channel relationships. Armed with clear insights and a comprehensive 360-degree customer view, manufacturers are able to proactively identify, prioritize and address issues, improve organizational alignment, enhance resource utilization, and open a whole new world of marketing opportunities.,.
The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
During my 30 years of analytics career, prospective employers and clients have often asked me: ",". I argue for greater emphasis on machine learning skills in the data scientist and their partnership with domain experts as an effective pathway to bring data science to a business.,Clearly, the description of data scientist as the mythical unicorn who has computer science skills, statistical knowledge and domain expertise (,) has had an impact. The proliferation of different analytics disciplines such as social network analysis, digital analytics, bio-informatics and supply chain analytics, lends weight to the argument that??,.,??,There are also anecdotes on the web of data science projects that went pear shaped because the analysts were not subject matter experts. A deeper look into these anecdotes reveals that the issues are not due to a lack of domain expertise, but due to poor data science such as over-fitting of data, bad sampling methods and unnecessary data cleansing. Still the myth that domain expertise trumps all else continues!!,Data mining competitions such as??,??and??,??have demonstrated the opposite and shown how data science can be successfully outsourced to people without domain expertise. Many companies have run competitions on such diverse topics as optimizing flight routes, predicting ocean health and diabetic retinopathy detection. Data scientists with little or no expertise in the domain have responded brilliantly with useful solutions.??,??and I won the??,??with no background in biology. Some data scientists, such as??,??and??,, have even won across multiple domains, indicating that data science skills are transferable across domains.,The counter argument to Kaggle?€?s success is that in these competitions, the domain experts have already generated the??,??by posing the right??,??and??,(,), and the competitors need only??,. But, in the brave new world of massive data along with the mathematical tools and computing power to crunch these numbers, old world paradigm of hypothesizing before modeling is likely to be challenged. Google has shown a whole new way of understanding the world without any??,models or theories with their approach to language learning.,So, if domain expertise is not necessary for the steps of posing the??,??and??,what about??,??and??,?,In my experience, domain knowledge about data capture and transformation processes at the sensors can be acquired through exploration of the raw data. Often, good data scientists become subject experts just by playing with the data and asking questions to domain experts about the data anomalies.??For instance, using just such a process, my analytics team in a manufacturing company identified a long standing, but previously undiscovered anomaly in the summarised sales and inventory feed from a large retailer. This anomaly materially affected the retail inventory reporting and had to be fixed programmatically. Subsequently, my data science team members were the??,!!,Domain expertise is most relevant, perhaps, in the??,, particularly those insights gained using unsupervised learning about the workings of complex physical processes. An example of just such a situation was the use of??,??to perform??,??in a multiple aircraft fleet from aircraft sensor and maintenance data. While the analysis started with no??,??model,??,interpretation of the results from the path analysis and the subsequent follow-up to improve aircraft safety certainly required domain expertise.,Returning back to the original question: ?€?,???€?, my response is as follows.,So, when hiring a data scientist, focus on the machine learning aspect, particularly, the desire to play with the data using a number of different techniques and languages. Consider also the analytical skills to question and solve problems iteratively. Partner the data scientists with domain experts so cross-pollination can occur. This, to me, is a better pathway for bringing data science to a business than searching for the elusive unicorn depicted in Figure 1.
 ,Fresh data is usually pristine. It?€?s data in it?€?s clearest, most accurate form ?€? straight from the customer or client. If you?€?ve put measures in place to cut back on data input errors, such as form validation, you can be reasonably sure that the newest records in your CRM are the ?€?latest and greatest?€?.,If your CRM has been active for some time, you?€?ll have a number of older records that have accrued. These records are the ones your sales and marketing teams will rely on when it?€?s time to approach existing customers and sell to them again. Chances are, the quality of these records will be fairly good, but it will have fallen since they were first collected. As data quality slips, data goes from ?€?great?€?, to ?€?good?€?, to decidedly ?€?bad?€?.,Data management is a huge cost to businesses, but it?€?s the bad data that is the real drain. According to Gartner, the average business wastes as much as $13.5 million sorting out data quality problems every year.,Poor management is rife. Because data is stored electronically, many businesses leave its management to the IT department. Yet there is no role within IT that is adept at managing data, and no role that traditionally takes ownership of it. People expect data to retain its accuracy when stored, yet the exact opposite is true.??,.,Enter the Chief Data Officer.,One job role cannot be a panacea for poor data management. And in itself, the role isn?€?t a data quality cure-all. But the Chief Data Officer can take ownership of a potential pain point, and they can make sure that the business?€? most valuable asset is not allowed to depreciate over time.,The role of Chief Data Officer is relatively new, yet it?€?s a role that is well overdue in tens of thousands of businesses. We?€?re all harvesting more data about our customers, and that calls for investment in people who can nurture and manage that data. The Chief Data Officer is tasked with making order from chaos, and with protecting the investment with the digital contents of some of the most important storage silos the company has.,Take the customer relationship management system, for example. As CRMs begin to age, the data within them is the tell-tale sign of trouble ahead. Once ?€?great?€? data is now simply ?€?good?€? data. If left unchecked, this data will turn ?€?bad?€? pretty quickly.,Once in post, the Chief Data Officer can migrate data management responsibilities to the business as a whole, taking it away from IT (or marketing, or whichever function is currently responsible for it). This puts data quality at the core of the business?€? operations, making it a central point of discussion when new business processes are developed. It should also result in data quality receiving adequate resources.,The Chief Data Officer can also keep track of data assets: where they?€?re stored, who?€?s got access, and how often they are cleansed and checked. They can put data quality processes in place to better manage the purity of critical business data, and they can make sure the business is not paying to store duplicated, old, unverified or corrupted data.,The end result is a cleaner, clearer dataset for everyone in the business, and a more secure, timely and effective management of data for the customer or client.,It sounds as though the Chief Data Officer will wave their magic wand and solve the business?€? data quality problems. But let?€?s be clear: one person cannot be solely responsible for managing data in any business.,There must be a broader strategic aim to treat data as an asset, care for it while it?€?s at rest, and use it responsibly when it?€?s needed. This is something the Chief Data Officer can oversee, but with support and buy-in from the boardroom.,This isn?€?t a solo mission, and this is why the Chief Data Officer is expected to take a senior role in the company, according to Gartner?€?s paper.,From medical records to loyalty cards, there are myriad rules around how personal data can be used and stored. It?€?s the Chief Data Officer?€?s responsibility to oversee this. Yet other staff using the data will still be expected to understand compliance, and must handle data responsibly according to those regulations.,Businesses are capturing more and more data from an increasing range of sources. Customer data, such as names and addresses, represent the core of most CRM systems. As we become better at capturing data, we?€?re certainly going to acquire more of it, more quickly than ever before.,From the Internet of Things to increasingly sophisticated web analytics, businesses are going to need to be more selective about data, and store only the data that really matters to them. This increasing resource will need ongoing management to ensure it does not go from ?€?great?€?, to ?€?good?€?, to ?€?bad?€? ?€? or even ?€?useless?€?.,And as our data silos grow, they become more appealing to hackers who will try their best to gain access by stealth. Often, by a time a breach has been discovered, the data has been sold on multiple times, and the business is powerless to do anything about it.,This is the unfortunate end result of data being badly managed, or not managed at all.,By 2017, Gartner says that 25 per cent of organisations will have a Chief Data Officer. In industries where regulatory compliance is key, the figure is expected to be much higher; perhaps even 50 per cent. If your business collects data, stores it and uses it to determine strategy, a Chief Data Officer could be the key person who will??,??across the business.
Have you experienced or thought how corporates manage their analytical assets which are mission critical to the business? A Bank or a Telecom Service Provider may often have more than 100 predictive model assets developed over a time period, but faces an important issue of how to effectively manage,store,share or archive these assets.,Many corporates have now realized the need for a centralized repository of storing predictive models along with detailed metadata for efficient work-group collaboration and version control of various models.This blog is about understanding the underlining idea about model management tools.,Model management involves a collaborative team of modelers, architects, scoring officers, model auditors and validation testers. Many organizations are struggling with the process of signing off on the development, validation,deployment, and retirement life cycle management milestones. They need to readily know exactly where each model is in the life cycle, how old the model is, who developed the model, and who is using the model for what application.The ability to version-control the model over time is another critical business need which includes event logging and tracking changes to understand how the model form and usage is evolving over time.,Model decay is another serious challenge faced by organizations. Metrics are needed to determine when a model needs to be refreshed or replaced. Retired models also need to be archived. More reliable management of the score repositories is also a key requirement to ensure that quality representative data is available to evaluate model ,SAS Model Manager is designed for selection,maintenance and continuous enhancement of analytical models for operational decision making.This tool enable process to effectively manage and deploy analytical models by delivering all the necessary functionality for each stage of model life cycle. ,SPSS Collaboration & Deployement tool also provide similar functionality as of SAS model Manager tool.,Below URL shares the features of both these tool.
In this post, we?€?ll use a supervised machine learning technique called logistic regression to predict delayed flights. But before we proceed, I like to give condolences to the family of the the victims of the Germanwings tragedy.,This analysis is conducted using a public data set that can be obtained here:,Note: This is a common data set in the machine learning community to test out algorithms and models given it?€?s publicly available and have sizable data.,In this blog, we will look at small sample snapsot(2201 flights in January 2004). In another post, we can explore using Big Data technologies such as Hadoop MapReduce or Spark machine learning libraries to do large scale predictive analytics and data mining.,Let?€?s load in our small sample set here and see the first 5 rows of data:, , , , , , , , , , , ,The goal here is to identify flights that are likely to be delayed. In the machine learning literature this is called a binary classification using supervised learning. We are bucketing flights into delayed or ontime(hence binary classification). (Note: Prediction and classification are two main big goals of data mining and data science. On a deeper philosophical level, they are two sides of the same coin. To classify things is predicting as well if you think about it.),Logistic regression provides us with a probability of belonging to one or the two cases(delayed or ontime). Since probability ranges from 0 to 1, we will use the 0.5 cutoff to determine which bucket to put our probability estimates in. If the probability estimate from the logistic regression is equal to or greater tha 0.5 then we assign it to be ontime else it?€?s delayed. We?€?ll explain the theory behind logistic regression in another post.,But before we start our modeling exercise, it?€?s good to take a visual look at what we are trying to predict to see what it looks like. Since we are trying to predict delayed flights with historical data, let?€?s do a simple histogram plot to see the distribution of flights delayed vs.??ontime:,We see that most flights are ontime(81%, as expected). But we need to have delayed flights in our dataset in order to train the machine to learn from this delayed subset to predict if future flights will be delayed.
You sometimes hear from some old-fashioned statisticians that ,, and that they - the statisticians - know everything. Here we prove that actually it is the exact opposite: data science has its own core of statistical science research, in addition to ,, ,,??and ,. Here we highlight 11 major data science contributions to statistical science. I am not aware of any statistical science contribution to data science, but if you know one, you are welcome to share.,:,All this research is available for free.
Finding insight within one data stream is a challenge. Finding insight from multiple streams can be significantly more so.?? The simple example? Two different databases created independently of each other that claim to capture the same kind of data.?? The larger the dataset, the more challenges we face aligning columns, de-duping content, making sure we don?€?t overwrite newer data with old data, and otherwise cleaning and preparing data for analysis. Ask anyone who has worked trying to align data across databases. It can be a pain.?? (And while we?€?re at it, make sure you pay them well.?? No. Really.?? They deserve it.),But what about when the data isn?€?t all neatly structured into columns and rows? What if you need to have some of your data coming from structured streams but most of the other data you need to tell your story is semi-structured, or even unstructured? How are you going to make sure that the Jane Doe listed in two separate databases as the same person is the same Jane Doe interviewed in an article mentioning your organization? How do you determine whether that mention has any relevance to what you are doing?,Well that?€?s a whole different level of challenging, one that my friend and colleague Dan Hirpara understands intimately in his work as a Senior Data Architect managing and fusing multiple, massive data streams for use in analysis.?? Dan recently wrote a blog about data fusion on our website. ??It?€?s one of a series we will be publishing from time to time highlight the challenges of implementing enterprise level data driven solutions in different environments. So if you?€?re interested in learning more, ,.
The??,??is always published Monday.??Starred articles or sections are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
Data Scientist communities have their own complex jargon; multivariate regression models, Big data engineering, Hadoop, Map Reduce, Deep Learning etc. But, unfortunately businesses do not seem to care about how complex the term is or how impressive the math is! They want the results explained in non-tech terms.,While working on Big Data & planning to implement it for the benefit of business, it is very important to explain the insights & valuable knowledge in a way that non-technical business user can actually understand.,Here is my recent experience while working on a project for one of the largest food retailers. The goal of this project was how incentivisation would help improve their overall profits.,After an extensive and impressive study, , came up with a collection of (what they thought) elegantly done slides. They discussed deeply about variance inflation factors, Akaike information criterion that would scare even seasoned practitioners of the art.,Now the client did not have a clue of what was going on during the presentation and rushed and escalated to me. I had to work several days De-technifying the slides! And make them business friendly,More often than not, I notice that I spend 50% of the time processing and cleaning the data and 20-25% of the time De-technifying the results and tell stories. Interestingly, while I find enough doers, the story tellers who understand the subject and business at appropriate depth are rare.,Unfortunately, this skill is missing in traditional MBAs & Managers as this is not a peripheral exercise of language. A fairly deep understanding of Data Science must be coupled with a even deeper research of the client organization.,Need to rush now. But, I shall talk about my thoughts on how to solve this problem on a later post.,Contd,Article idea & guidance by - ,Script, Design & Edited by - Suman Malekani
Apache Drill includes a distributed execution environment, purpose built for large-scale data processing. At the core of Apache Drill is the ?€?Drillbit?€? service which is responsible for accepting requests from the client, processing the queries, and returning results to the client. When a Drillbit runs on each data node in a cluster, Drill can maximize data locality during query execution without moving data over the network or between nodes.,??Providing interfaces such as a command line interface (CLI), a REST interface, JDBC/ODBC, etc., for human or applicationdriven interaction.,??Allowing for pluggable query languages as well as the query planner, execution, and storage engines.,??Pluggable data sources either local or in a cluster setup, providing in-situ data processing.,??that works with a number of underlying data sources. It is primarily designed to do full table scans of relevant data as opposed to, say, maintaining indices. Not unlike the MapReduce part of Hadoop provides a framework for parallel processing, Apache Drill provides for a flexible query execution framework, enabling a number of use cases from quick aggregation of statistics to explorative data analysis.,Apache Drill can query data residing in different file formats (CSV, TSV, JSON, PARQUET, AVRO) and in different data sources (Hive, HBASE, HDFS, S3, MongoDB, Cassandra, and others). Drill provides a unified query layer that can interact with different file formats in different data sources thus avoiding any ETL necessary to bring data in different places to one location.,Check out the ,??that demonstrates some of the Drill's capabilities
Interesting??,. I think many of these visualizations are just pure art, but delivering no insight. So we selected a few of the most , ones for you.,??to check the 50 visualizations. ,??on great visualizations. Or do a ,??to find all our articles about visualization.??
Interesting list published on ,. Here are the main categories:,Each section features software, research and lectures. Here's their selection for ,:,Software,Research,Lectures,A few categories are missing, including feature selection, scoring models, neural networks, and association rules. But it's a great resource nevertheless.

Analyzes of texts put lights in two main types of information ?€?facts and opinions?€?. Most current treatment methods of textual information aim to extract and use factual information, this is the case for example of research we do on the web. Analysis of opinions is concerned about feelings and emotions expressed in the texts, it has grown much today because of the space taken from the web in our society, and the very large volume of daily comments expressed by consumers with the advent of the Web 2.0 world.,What is opinion analysis? It identifies the orientation of an opinion expressed in a piece of text (blog, forum, comments, website, document sharing site, etc.). In other words, it determines whether a sentence or a document expresses a feeling positive, negative or neutral regarding a defined object. For example: "The movie was fabulous" is an expression of opinion while saying "the main actor of the film is Jean Dujardin" is the formulation of a factual matter. Opinion analysis may occur at different levels. At the word level: the film is entertaining and motivating; on the sentence level: the police (subject) hunt (verb) smuggling (object), or finally at the document level, that is to say a set of sentences: his early films were very good, but this one is worthless.,??,In fact an opinion can be characterized by a formula of five components, a quintuple: Oj, Fjk, Hi Tj, SQijkl, where Oj is a target object; Fjk a characteristic of the target object; Hi a bearer of opinion; til the time the opinion is expressed, and SOjkl the opinion orientation, of the bearer Hi, about the Fjk characteristic of the OJ object at Tl time. Using this formula we can structure an entire unstructured web document, highlighting all quintuples included in the text. Quintuples represent structured data that can be analyzed qualitatively or quantitatively, and visually represented with traditional tools of decision systems. All kinds of analyzes are possible. Analysis of opinions is not only to characterize the opinion of one person by words and phrases, but also for example to compare the opinions of different people or groups.,The first step is to delete sentences that contain only facts, keeping only those who express and define the polarity (positive, negative or neutral). Specifically you have adjectives (red, metallic) that indicate facts or positive feelings (honest, important, mature, big, patient), negative (harmful, hypocritical, ineffective) or subjective being neither positive nor negative (curious, strange, odd, perhaps, likely). It is the same for verbs, positive (praise, love), negative (blaming, criticizing), subjective (predict), or for the names: ??positive (pleasure, enjoyment), negative (pain, criticism) and subjective (prediction, impression).,Be careful with sequence words meaning, a sentence can be complicated and punctuation that is of great importance, can play tricks. ?€?Not good at all?€? is different with ?€?Not all good?€?. The sentence might express sentiment not in any word: ?€?convinced my watch had stopped?€? or ?€?got up and walk out?€?. We must consider that words or phrases can mean different things in different contexts and domains, or the subtlety of the expression of feelings when someone makes ironic statement.,??,Ultimately, however, the analysis of opinions and feelings is able to provide much information about the populations studied and warned marketers already know how take advantage. This is true of many Teradata Aster customers, like Barnes & Noble for example. If you want to go further on this subject you can usefully consult the following site:,??
Developing machine learning solutions that give a lift from your existing prediction algorithms is not an easy task. They require a multitude of activities to get it right including cleaning up the data, setting up the infrastructure, testing &re-testing the model & finally deploying the algorithm.,Here are five machine learning services that can help reduce the pain of deploying your machine learning solution.,1.?? ,Based on Microsoft?€?s Azure Cloud Platform, Azure Machine Learning offers a streamlined experience for all data scientist skill levels, from setting up with only a web browser, to using drag and drop gestures and simple data flow graphs to set up experiments. Machine Learning Studio features a library of time-saving sample experiments, R and Python packages and best-in-class algorithms from Microsoft businesses like Xbox and Bing. Azure ML also supports R and Python custom code, which can be dropped directly into your workspace. Experiments are easily shared, so others can pick up where you left off.,2. ,Google?€? Cloud Prediction API provides pattern-matching and machine learning capabilities. Given a set of data examples to train against, you can create applications that can perform the following tasks:,3. ,The Algorithms.io cloud platform makes it easy to use machine learning algorithms to classify streaming data from connected devices. Algorithms.io?€?s catalog of machine learning algorithms classifies streaming data into discrete actions in real-time with up to 99% accuracy. They provide the infrastructure necessary to collect, store, and classify streaming data, all as a service.,4.?? ,BigML takes the complexities out of creating a high-availability, low-latency Machine Learning system created especially for your data. You will not only gain valuable insights from your data, you will most likely enjoy it. You can upload your data or connect to your cloud data (such as Google Drive or Google Cloud) via APIs, create data structured data sets from those sources, construct models and finally make predictions.,5.?? ,Ersatz is a web-based general purpose platform for machine learning with support for GPU-based deep learning. It's geared towards aspiring and working data scientists with stuff to do. Ersatz has a number of components designed to make modern machine learning workflows much more efficient. Primarily, these include tools for data wrangling, model training, and machine learning infrastructure.,6. ,Nutonian's Data Science as a Service offering, Eureqa, enables industry leading organizations to solve their most challenging business problems.?? With more than 80,000 installations globally, the Robotic Data Scientist provides vertically-focused application modules for financial services, life sciences, retail, telecommunications and utilities to compute millions of potential solutions every second of every day.
??,??,involves analyzing social media data without intervention or interaction from the researcher.?? In this mode of research, you search for, look at, collect, synthesize, and analyze data that exist in the social media sphere (blogs, newsgroups, forums, message boards, and microblogs).?? The goal is to measure and gauge public awareness and sentiment of issues, products, or brands.?? Not long ago I headed up a??,??that focused on this type of social media research, and what it might mean for our industry.,It is true that social media is the newest channel of communication for researchers to deal with, and it presents us with unique research challenges and possibilities.

Continued from -??,During the execution of a Data science initiative, one person has to constantly think big and about the business application of the project. May be this is needed in any IT project. But the catch is that in a typical IT project, the teams are large. Hence, 1 big thinker means 1 in a hundred member team and hence may be they are not feeling the pain of finding such members. 20 big thinkers in an organization of 2000 may not be that difficult to find. However, in a data science project with 2-4 members, one big thinker per team means 25-50% of the entire team should be big thinkers. Finding that many is difficult. Also, a typical IT support project offered by the IT head demands a lot less business understanding than a consulting data science project offered by the CEO or COO offices.,Whenever I employed the senior data scientist of the team for this role, results were anything but great!,Let me take an example project that did not end too well. It had all the elements that a data scientist would swoon over. In the first three months of execution, our engineering team set up a complex big data environment, extracted millions of sparse data records, ran fairly sophisticated machine learning models and designed interactive dashboards! I know for sure that this could be a poster project in any big data event. It was a brilliant effort by any yard stick.,But, I, despite being the primary client contact, got too involved in the engineering. My slides were purely technical (how we extracted a complex object with an equally complex query in record time kind of stuff!). During the quarterly review, it became painfully obvious to me that I failed to set the right expectations, contextualize the results or communicate the business benefits of the work constantly to the users.,Client said they understand that the team was working real hard and solving some very complex things. But, they simply did not know what that would mean to them!! After three months of truly cutting edge exceptional engineering, ,I also tried a three member team approach for a few projects. One of the members would be a Math Statistics expert, one would be a Programmer/hacker/Big Data Expert. The last one would be a domain expert.I expected that the domain expert would be the story teller in this case and also bridge the gap between the data scientists and the Business users. However, these experiments also failed commonly.,The cause was complete break-down of internal communications. A typical domain expert I would find would be a ?€?retired expert?€? with over 30 years of experience in his/her area. Typically the other two members would be much junior (<10 years of experience). This senior-junior combo with a vast gap in age, exposure and experience never was very effective with just one way communication! It never yielded any results.,I exhort every organization that?€?s keen on establishing a center of excellence in data science to appreciate this difficulty. I have been talking to experts and am formulating a few strategies., It is as big if not a bigger challenge as training people in math and programming.,We cannot and should not expect a domain expert or a senior data scientist to automatically take up the job of a big thinker. A dedicated specialized design thinking training is required for shaping professionals to fill this role. They should become good at,,A close friend of mine who is an expert in design thinking believes that we should be able to train any inquisitive and creative person (with sufficient quantitative bent of mind) for this role. They should be able to quickly grasp any domain and become the bridge. Through ,, I began working with a group of MBA schools to design the curriculum and incorporate these skills in fresh MBA grads. Early signs are encouraging!,I shall explain the other aspects in a follow up article.,Contd,Article idea & guidance by - ,Script, Design & Edited by - Suman Malekani

"Talk on PMML and Predictive Analytics to the ACM Data Mining Bay Area/SF group at the LinkedIn auditorium in Sunnyvale, CA., , Data mining scientists work hard to analyze historical data and to build the best predictive solutions out of it. IT engineers, on the other hand, are usually responsible for bringing these solutions to life, by recoding them into a format suitable for operational deployment. Given that data mining scientists and engineers tend to inhabit different information worlds, the process of moving a predictive solution from the scientist's desktop to the operational environment can get lost in translation and take months. The advent of data mining specific open standards such as the Predictive Model Markup Language (,) has turned this view upside down: the deployment of models can now be achieved by the same team who builds them, in a matter of minutes., ??In this talk, Dr. Alex Guazzelli not only provides the business rationale behind PMML, but also describes its main components. Besides being able to describe the most common modeling techniques, as of version 4.0, released in 2009, PMML is also capable of handling complex pre-processing tasks. As of version 4.1, released in December 2011, PMML has also incorporated complex post-processing to its structure as well as the ability to represent model ensemble, segmentation, chaining, and composition within a single language element. This combined representation power, in which an entire predictive solution (from pre-processing to model(s) to post-processing) can be represented in a single PMML file, attests to the language's refinement and maturity., Presentation slides are available for download ,., ??,
"
Netflix is known for its personalized recommendations. By analyzing data on what you and other people have watched, they know what movie to recommend to you next. Customers are happy because they get suggestions for movies they like. Netflix is happy because you?€?re more likely to watch another movie, according to Mohammad Sabah, who spoke at the Hadoop Summit. Now that same approach is changing how colleges educate. By recommending courses and majors to students based on data, colleges and universities are increasing graduation rates.

I rarely get to use a walkie-talkie during a course in school. As the snapshot of my desktop shows on the image below, I had both a multi-line telephone and portable radio. Just before the exam, I participated in a simulation. Our tabletop exercise contained an emergency scenario: a train derailment involving the evacuation of residents. I served as the Social Services Director. Although I didn't choose this role for myself, I thought it made sense given my graduate degree in the area of disabilities. I found it intriguing watching the organizational process unfold - and to some extent collapse before my eyes. Order and good intentions transformed into disorder and some level of chaos. The idea of having a "plan" is extremely important particularly in the delineation of roles and responsibilities. When such things are not determined in advance, there can be lack of organizational coherence during the emergency. Today I will be blogging about some informal research that I initiated shortly after the exam. It relates to the systematic use of electronic documentation. I gathered some "emergency response plans" from the internet. In Ontario, municipalities have to develop and adopt these emergency plans. After collecting several dozen of these documents, I quickly wrote some code to mine for incidents of important keywords.,During an emergency, it is sometimes necessary to evacuate people. The evacuation of people with disabilities is challenging due to their unique needs and demands on resources - challenging that is, if there is no plan. Adding to the complexity of the situation, not all people with disabilities necessarily self-identify. For example, a person with cancer might not "appear" to have any special needs. At any given time, the data that we have about people might barely reflect the realities that they face. I think within a population there is always a spectrum of different abilities. It is quite inappropriate to lump everybody together as if differences never exist. They always exist. It is reasonable to assume that during crisis scenarios, people become more challenged; and even those that never identify as having a disability start to exhibit levels of impairment. They are pushed to their limits. Their ability to cope with a crisis might be limited. It is therefore necessary to plan - to accommodate diversity. I thought it would be interesting as a data-mining exercise to search the electronic documents for keywords indicating different aspects of sensitivity.,As I mentioned earlier, this research is rather informal. I wasn't too careful with my choice of keywords. Plus, I only made use of 35 emergency plans. (There are definitely more than 35 filers.) There is also no guarantee that I have the most up-to-date plan, or that the plan I am using is in fact the plan submitted to the province in order to comply with the legislation. Theoretically, in support of 6.2(1) of the Emergency Management and Civil Protections Act, presumably there is a large database containing every plan ever submitted. Perhaps I will be able to locate it at some point if the database is accessible to the public. Below I present the percentages for keyword series' that contain at least one match. The table will show that nearly all of the plans (97.14 percent)??mention "death fatality fatalities casualty casualties" (at least one of these words in the series). These are emergency plans, after all. The high percentage would tend to confirm the effectiveness of the mining approach: standardized keywords seem show contextual alignment.,I was surprised that less than a third of the plans (31.42 percent) mention "disabled disability disabilities." When I originally ran the mining program, I didn't use the term "special needs"; including this term in the series, the distribution would increase from 31.42 to 40 percent. I'm going to suggest that "special needs" is almost too vague to be useful from a planning standpoint. I can imagine myself running the program using thousands of keywords and performing complex algorithmic comparisons: e.g. given a body of emergency plans known to be "good or excellent," I might be able to determine non-compliant plans (neither good nor excellent) through differences in sensitivity map patterns. Of course, such a scoring regime would only provide helpful "indicators." In order to make a specific comment about a particular filer, it is necessary to review the documentation manually. The bulk comparisons are probably best for identifying extremely good and bad plans - the latter indicating a need for further investigation - and to do aggregate assessments. "Our algorithm suggests that your plan is extremely poor. Upon closer inspection, it seems you submitted payroll documentation rather than an emergency plan.",If a town has a committee to review emergency procedures involving people with disabilities, an emergency response plan would certainly be a reasonably good place to mention who to contact - well, in case of an emergency. I can imagine the impacts of a disaster such as a massive flood. There might be a need to evacuate some the population. But what if none of the vehicles on the scene can handle wheelchairs? ??Or perhaps those with visual or hearing impairments are not made aware of the rescue vehicles. Some residents might face might face literacy or comprehension challenges. Who is going to get saved during an emergency when resources are stretched and people are running about like headless chickens? When decisions have to be made during an emergency, they don't necessarily consider all of the social consequences. Having a plan helps to promote inclusion of people with diverse needs. It also fosters greater levels of accountability among those responsible for planning. In my next chart, I decided to give each filer points based on the frequency of use of keyword; then I set the scores from highest to lowest. There is a clear winner in the bunch.,Below are the "Top 10" scorers from highest up top to lowest on the bottom. There were 35 documents scanned. The filer at the right end had no points! Presumably its plan doesn't even mention how to handle fatalities in an emergency. I say "presumably" since I haven't read the plan. Perhaps I downloaded something only resembling an emergency plan. I was thinking that it might make sense to enhance the checking system - to make the software more intelligent. The benefit of this systematic approach - which I readily accept is far from ideal - is its low-cost nature particularly for repetitive checking. There might be hundreds of plans to review annually. I believe that policy-makers sometimes simply want an idea of how the collective seems to be handling a particular social issue or emergency.,I was thinking of posting detailed sensitivity maps online as a public service. I might consider this in the future. The ultimate objective is always to save lives and serve the public good. Even a plan that needs work is better than no plan at all. I certainly wouldn't want to contribute to any kind of confrontational atmosphere. However, in relation to the mining exercise, I think that many people would agree with the following: it can be useful to systematically mine documents of public interest to make comparisons, report on the state of communities, and determine the impact of policies. If I add geographic locations and dates - since the plans are meant to be reviewed and updated routinely - it would possible to gain a sense change to local disposition. Word-mining is made possible through the existence of keywords (standardized vocabulary), which is perhaps more prevalent in governmental documents than other forms of written text. Emergency response plans represent a uniquely accessible resource for computers. In closing, if anyone has all of the emergency plans either posted online or indexed for rapid access, I would certainly be interested in a more thorough tabulation.
In the previous post we presented a few methods of data analysis, which are used to identify customer needs and preferences and allow us to predict their behavior. Such knowledge results in building better marketing and sales offers which meet specific customer expectations.,In today?€?s article we present further examples of Data Mining methods that can be applied in daily business operations.,Interested in similar content???
Hi all... I have my new publication on large scale machine learning with Packt publishers coming up in the next few months. Request help from experts in machine learning for help. Please inbox me interest to sunila.gollapudi@broadridge.com and??,A quick note on the scope of content covered in the book:,??Practical Machine learning - to be published by Packt, 
 businesses are feeling the pressures of data overload. Consequently, many businesses have databases that look like the picture below.,Businesses are stockpiling more data than ever before. From machine-generated data, social sentiment, transactional data, and emails, many companies feel as if they are drowning in data. According to a report by the Economist, 7 in 10 companies are collecting syndicated third party data such as weather information (72%) or government data (70%), while many gather data such as staff data (66%) and location-based information (41%).,For many companies, this data is often siloed or is in a format that can?€?t be used ?€? making strategic decision-making from so much data almost impossible. The Economist reported that 40% of executives are struggling to understand which data actually matters, while another 34% believe that their decision-making is being affected by this data overload.,Data can be one of the most powerful tools to improve customer experiences and increase customer acquisition and retention. In the report by The Economist, the top 3 areas where data has made a positive impact for CMOs are:,1)?????????? Increasing customer understanding an segmentation (50%),2)?????????? Increasing sales (40%),3)?????????? Helping assess potential demand for new products and services (37%),Determining which data to use and what to toss depends on your objectives.,Here?€?s how to get started with transforming data into useful information.,A business needs analysis should be performed to understand what is required of data moving forward. For example, what information is required to meet strategic objectives and is the data accessible to users? Are data gaps occurring, limiting the availability of required information to support decision-making? What data issues may be impacting revenue, increasing costs, or causing inefficiencies in operations? , , Documenting business objectives helps determine what data should be captured, how the data is related, and how data should be structured to create value.,Useless and duplicate data will only provide needless information that will bog down marketing, sales, and operations. Collecting and maintaining the data can also be very expensive.,If you aren?€?t sure where to begin, get a data assessment. Many vendors offer complimentary assessments to help identify areas where data quality can be improved, duplicate data, and other areas where performance can be improved., , If your data isn?€?t already integrated, data integration and quality software automates integration processes and improves data quality by performing the following tasks:,Business processes should also be established to ensure data manually entered into systems is of the highest quality possible. Many organizations experience data errors when information is manually entered, at a rate of 2% and 8%.?? Even one wrong number entered incorrectly can cause a payment to fail, a wrong part number to be shipped, or apparently a man to become pregnant.,Data validation controls can be integrated into on-line forms, using rules to check the validity of data sets. For example, an on-line website form may require a visitor to enter data in specified formats. Or an IRS form may utilize controls to check that positive numbers are being entered into fields.?? Training employees to be more aware of the importance of data quality is also a crucial step to achieve a company-wide awareness of maintaining high quality information.,Once a system has been put in place to flush out the bad data, you don?€?t want to fall bad into bad data hoarding habits so be sure to appoint a ?€?keeper of the data?€?.?? You don?€?t need to rush out and hire a data scientist, but someone within the organization should be responsible for the data, such as a data steward who understands what to do with all the data that is being collected. Responsibilities of this role should include:, When it comes to information, too much can hurt. Take action to manage the clutter and concentrate on the insights that matter.
Recently I read book??,??by??,The sixteen data scientists across sixteen different industries were interviewed to understand both how they think about it theoretically and also very practically what problems they?€?re solving, how data?€?s helping, and what it takes to be successful.,Here are some Thoughts that Inspired & Make Me Happy.
Today?€?s digital world demands information to reside in databases. The ability of a business to manage information can be the difference between death and life. Like it or not, the world today is digital. Organizations need information to survive.,Every year, the amount of data that is stored in databases continues to grow tremendously. When you compound this fact with the current global economy, you will notice that a database plays such a crucial role in any organization whether small, medium or large sized. Any database needs to be functional and available seven days every week and twenty-four hours every day. This can put so much stress on a company?€?s internal resources, i.e. database administrators.,One issue that accelerates the problem mentioned above is the fact that the specialized resources that are needed to maintain databases are usually hard to hire and retain. Additionally keeping up with the ever-changing technology landscape presents yet another big problem. Recognizing these facts, most organizations ease the problems by hiring remote DBA support.,Historically, most companies seeking outside help can go out and employ a consultant to provide professional help on a time and material basis. This way, companies will pay eight hours of work regardless of whether you had eight hours worth of tasks to do. The professionals hired using this criterion present themselves to your work site and offer the services needed.,Anyone who has passed through this form of hiring can attest to its expensive nature. It is even much more expensive when you need an overly priced expert to work off the hours. Most times, working off-hour shifts is one of the job requirements of a DBA. Obviously, the last thing you want is to have your point of sale app crumbling down when clients are placing their orders in the store.,Database administration has helped countless companies effectively manage their databases. More importantly and specifically, remote DBA is a much more efficient solution compared to full time DBA. You should hire a remote DBA if your organization is struggling with any of the following issues:,You should also seek the services of a remote DBA professional when your organization is:, , , 

 , , despite plentiful guidance on why ,. Pundits tell us, ?€?,?€? Yet it is easy to feel the cloud is just beyond our grasp. So let's??take??a look at some real-life use cases from sectors that are leading the way in enterprise adoption of the cloud.,Ask a few CIOs about the cloud and you are likely to hear a wide range of responses, from concern that the cloud endangers security and privacy to elation that the cloud can be the ultimate platform for change. While much of this reflects well-reasoned advice and counsel, some is pure hype. When even The Onion takes on ?€?,?€? we should realize that we are at hype and jargon saturation. With all the noise around cloud computing, cloud storage and cloud apps and debate about the pros and cons of public, private and hybrid clouds, we need to consider what is real and what is merely illusion, and moreover why we should ultimately care. These beautiful lyrics from the 60s seem to foretell our current state of confusion over the cloud:,The cloud is a growing reality. CIOs and IT teams need to clearly understand how it can best be applied to advance their strategic interests. IDC research forecasts , and spending on private cloud will top $24 billion by 2016. CompTIA predicts that the next decade will see cloud computing becoming even more accepted as a foundational building block. We are seeing??, in the public sector, and Gartner predicts ,, advising CIOs and other IT leaders to continually adapt to leverage increasing cloud capabilities. The Open Group Cloud project analyzed ,driving adoption. In general, the rationale can be classified in five areas: agility, productivity, QoS, cost and the ability to take advantage of new business opportunities ?€? all of which have been guiding principles for applying technology in the past. So how well are our past years of enterprise hardware and software know-how translating to the cloud for large-scale applications? Here are three sectors that are forging the way with successful cloud implementations in order to drive efficiency, improve time to market, and effect business transformation.,World Economic Forum research reveals that , at higher than expected rates. The growing adoption of cloud technology is happening at all levels of government around the globe. We are already seeing cloud play a role in changing how government agencies fundamentally spend money and allocate their IT resources.,While adoption is being driven in part by cloud-first mandates, the cloud is clearly aligned with government mission objectives. The public sector has embraced a data-driven approach ?€? including open data and big data initiatives ?€? to be responsive to citizens. Cloud implementations are seen as a means of moving beyond data transparency to achieve a cost-effective state of operational excellence. , highlights the cloud as a means to be responsive to citizens?€? wants, needs and ideas. For municipalities, the cloud provides equal, on-demand cost-effective access to a shared pool of computing resources. The , hosts 1.5 million guests for the La Merce festival using the cloud to help manage the surging foot, bike, auto and public transportation traffic. The state of??, has implemented a cloud-based CRM application for constituent tracking in two months, adopting a cloud-first policy that piggybacks on federal policy. The state set up a private cloud and virtualized 85 percent of the state?€?s physical servers, saving $4 million per year. Delaware now has 70 applications in the cloud ?€?from event notification to cybersecurity training. For central government organizations, including the US Department of the Interior, shared services are eclipsing ?€?cloud-first?€? mandates as the driver behind cloud adoption. DOI?€?s groundbreaking , consolidates all the records information programs under one IT governance system, and this shared service is expected to save an estimated $59 million in taxpayer dollars by 2020.,Gartner Research identified financial services banking and insurance segments as two of the top cloud adopters. These segments are driven by the need for more innovation and the value they get from that innovation. Financial services firms are rewarded for systems that can process transactions faster and more securely and are providing new services, such as mobile banking and claims, that are ready-built for cloud-based systems. There is also growing competition with startups that are shifting the playing field. Way back in 2013 (a decade in cloud years), my article , talked about how bankers would increasingly take innovation cues from consumer tech and smart retailers as they practice the art of banking. Over the past year, the cloud has proven to be both a major disrupter and an enabler for innovation. Like the other big research firms, IDC sees digital transformation as key for businesses and a bridge that CIOs must learn to cross, and that bridge includes the ,. A recent article from??Banking Technology, ",,"??declares that traditional banks are now in a race to remain relevant as they face a slew of non-bank competitors with offer models that consumers increasingly value. Accenture found that one in five consumers would be happy to bank with PayPal ?€? a cloud firm born in Silicon Valley. Though often a cost-saving measure, CIOs are seeing the potential in the cloud to create a flexible platform for future innovation. A poll of financial services sector decision makers revealed the top two benefits of adopting cloud platforms as cost savings (voiced by 62 percent of respondents) and a simplified IT environment (52 percent). It is this simplification of the IT environment that will enable banks to level the playing field with the upstarts:,While banking has definitely upped its cloud game in the last few years, insurance is perhaps the granddad of cloud adoption. In ,, Accenture highlighted Insurance as being in the forefront of cloud growth and predicted that the cloud would transform the industry. On their list of reasons to adopt cloud, the ?€?ability to respond to market change and reshape operating model[s] to address new and emerging opportunities and challenges.?€? , of cloud adoption trends in insurance found that 35 percent of participants said the cloud "provides companies with the flexibility needed to respond quickly to changing needs." In retrospect, while cost savings has been a driver for insurers to adopt the cloud, there are already a number of , that illustrate , as a means of innovation and competitive advantage in a changing market with a changing customer demographic. Andre Nieuwendam, director of IT for United Property & Casualty describes their cloud success in customer-centric terms:,In a recent Forbes article, ",," Ray Wang (@rwang0) highlights cloud as the single most disruptive of all the new technologies. ?€?Cloud not only provides a source of unlimited and dynamic capacity, but also helps users consume innovation faster.?€? The idea of leveraging the cloud as a platform for speed in a changing market is appealing and especially resonates in the communications, media and entertainment sector, one that Gartner has identified as second only to banking in cloud adoption. In ,, I wrote about the digital media supply chain and how entertainment and broadcast companies are experiencing no less than an industry revolution:,According to Accenture?€?s ,, the cloud can be the platform on which the digital media supply chain operates to better serve changing markets and consumption models. Cloud technology is, tells the story of how Lionsgate is using cloud to run their studio and compete with the ?€?big guys?€? in the industry. Cloud has been helping them deal with their dispersed global environments during film production: media complexity, an unprecedented influx of massive amounts of data, and unique data and workflow requirements.,Perhaps the cloud is not so mysterious after all. In ,, David Linthicum (@DavidLinthicum) shared his perspective that businesses that adopt cloud gain a strategic advantage:,As industries increasingly digitize, the cloud is proving to be a useful partner to the CIO in an increasingly digital-first world. It is not surprising that KPMG?€?s recent survey, ,, found the top uses for cloud are to drive cost efficiencies and enact large-scale change including enabling a flexible and mobile workforce, improving alignment and interaction with customers, suppliers and business partners, and better leveraging data to provide insightful business decisions. The key to success, as with any new bright shiny technology, is to apply the cloud to achieve critical business and mission objectives. As Jim Buczkowski of Ford Motor says,,So here?€?s to accomplishing great things with the cloud! Just keep these tips from KPMG in mind as you resolve to make your cloud initiative a success:,Plus one bonus tip from me: Avoid the trap of ?€?cloud for cloud?€?s sake,?€? lest we discover the biggest truth in , is ?€?So many things I would have done but clouds got in my way.?€? A version of this article first appeared in ,.,Director Industry Marketing
 ,This tutorial will walk you through integrating Python with AzureML.??,You are planning to move out of the place you are currently staying and are looking for a place place which is similar to the current place. How will you decide where to go?,The below example data set is given here & we used Cosine Similarity to determine the closest places. However, the cosine similarity code in Python was used on AzureML. ??The testing data set for??,??,The scripts can be executed on azure machine learning studio using ?€?Execute Python Script?€? module which is listed under ?€?Python language modules?€?.,The module gives 2 types of outputs: 1)one is a dataset in internal dataset format and console and png graphics output can also be seen via ?€?Python device?€? output,To get going, login into azure machine learning studio. ??,or you can also search for the dataset. Just drag and drop into the canvas, , , ,To start running the experiment click ?€?Run?€? button at the bottom. Once the experiment is finished you can see the console output by clicking on the bottom right bubble of the ?€?Execute Python Script?€? module and selecting ?€?Visualize?€? from the dropdown.,The data frame can be downloaded as a csv by adding a ?€?convert to csv?€? module and connecting it to the bubble and running the experiment once again. Once done the csv can be downloaded by clicking on the bottom bubble and selecting download
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
US agricultural manufacturer John Deere has always been a pioneering company. Its eponymous founder personally designed, built and sold some of the first commercial steel ploughs. These made the lives of settlers moving into the Midwest during the middle of the 19, century much easier and established the company as an American legend.,Often at the forefront of innovation, it is no surprise that it has embraced Big Data enthusiastically ?€? assisting pioneers with the taming of the virtual wild frontier just as it did with the real one.,In recent years, it has focused efforts on providing Big Data and , solutions to let farmers (and in the case of their industrial division with the black and yellow logo, builders) to make informed decisions based on real-time analysis of captured data.,So in this post I want to take a look at some of John Deere?€?s innovations in the virtual realm, and how they are leading to change which is said to be ?€?revolutionizing?€? the world of farming.,The world?€?s population is growing rapidly, which means there is always going to be an increasing demand for more food. With the idea of genetically modified food still not appealing to public appetites, increasing the efficiency of production of standard crops is key to this. To this end, John Deere has launched several Big Data-enabled services which let farmers benefit from crowdsourced, real-time monitoring of data collected from its thousands of users.,They are designed by the company?€?s Intelligent Solutions Group, and the vision is that one day even large farms will be manageable by a small team of humans working alongside a fleet of robotic tools, all connected and communicating with each other.,To this end, they are working on a suite of services to allow everything from land preparation to seeding, fertilizing and harvesting to be controlled from a central hub.,The total land available can be split into sections and ?€?Prescriptions?€? issued with precise instructions for seed density, depth and fertilization. These decisions are informed by Big Data ?€? aggregated data from thousands of users feeding their own data back to the service for analysis., is an online portal which allows farmers to access data gathered from sensors attached to their own machinery as they work the fields, as well as aggregated data from other users around the world. It is also connected to external datasets including weather and financial data.,These services allow farmers to make better informed decisions about how to use their equipment, where they will get the best results from, and what return on their investment they are providing.,For example, fuel usage of different combines can be monitored and correlated with their productivity levels. By analyzing the data from thousands of farms, working with many different crops in many different conditions, it is possible to fine-tune operations for optimum levels of production.,The system also helps to minimize downtime by predicting, based on crowdsourced data, when and where equipment is likely to fail. This data can be shared with engineers who will stand ready to supply new parts and service machinery as and when it is needed ?€? cutting down on waste caused by expensive machinery sitting idle.,Another service is Farmsight, launched in 2011. It allows farmers to make proactive decisions about what crops to plant where, based on information gathered in their own fields and those of other users. This is where the ?€?prescriptions?€? can be assigned to individual fields, or sections of fields, and machinery remotely reprogrammed to alter their behavior according to the ?€?best practice?€? suggested by the analytics.,As well as increasing farmers?€? profits and hopefully creating cheaper, more abundant food for the world, there are potential environmental gains, too.,Pesticides and fertilizer can often cause pollution of air and waterways, so having more information on the precise levels needed for optimum production means that no more than is necessary will be used.,Of course, with all of this data being generated and shared ?€? there is one question which needs answering ?€? who owns it?,Deere offers what it calls its Deere Open Data Platform, which lets farmers share data with each other (or choose not to, if they wish) and also with third party application developers, who use can the APIs to connect equipment by other manufacturers, or to offer their own data analysis services.,But this has not stopped many farmers asking why they should effectively pay for their own data, and asking why John Deere and other companies providing similar services shouldn?€?t pay them ?€? according to ,.,Talks are currently ongoing between the AFBF and companies including John Deere, Monsanto and DuPont over how these concerns should be addressed. As well as privacy worries, there are concerns that having too much information could allow traders in ,.,Farming is one of the fundamental activities which makes us human and distinguishes us from animals. Once we developed farms, we no longer needed to constantly be on the move in the pursuit of food and fertile foraging spots, leading to the development of towns, cities and civilization.,With the development of automation and Big Data, we are starting to delegate those responsibilities to robots ?€? not because farmers are lazy (they really aren?€?t, as anyone who lives in an area where agricultural activity goes on will tell you!) but because they can often do it better.,Sure, John Deere?€?s vision of vast areas of farmland managed by a man sitting at a computer terminal with a small team of helpers will lead to less employment opportunities for humans working the land, but that has been the trend for at least the last century, regardless.,And the potential for huge positive change?€? in a world facing overpopulation and insufficient production of food ?€? particularly in the developing nations, is something that has the potential to benefit everyone on the planet.,I hope you found this post interesting. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have.,??|??
We are giving away , copies of our eBook "," featuring??real world use cases of how big data is being used by companies to achieve tremendous success.,Titus Blair is a family man, full-time world traveler, futurist, entrepreneur and Director of User Engagement at??,. As an industry leader in big data, he has the inside track on how companies are using big data in the real world and he has compiled a collection of those use cases in "Big Data In Action". Titus hopes that you will learn a ton from these real examples and do something truly amazing in the world of big data!
Uber is a smartphone-app based taxi booking service which connects users who need to get somewhere with drivers willing to give them a ride. The service has been hugely controversial, due to regular taxi drivers claiming that it is destroying their livelihoods, and concerns over the lack of regulation of the company?€?s drivers.,This hasn?€?t stopped it from also being hugely successful ?€? since being launched to purely serve San Francisco in 2009, the service has been expanded to many major cities on every continent except for Antarctica.,The business is rooted firmly in Big Data and leveraging this data in a more effective way than traditional taxi firms have managed has played a huge part in its success.,Uber?€?s entire business model is based on the very Big Data principle of crowd sourcing. Anyone with a car who is willing to help someone get to where they want to go can offer to help get them there. ??,Uber holds a vast database of drivers in all of the cities it covers, so when a passenger asks for a ride, they can instantly match you with the most suitable drivers.,Fares are calculated automatically, using GPS, street data and the company?€?s own algorithms which make adjustments based on the time that the journey is likely to take. This is a crucial difference from regular taxi services because customers are charged for the time the journey takes, not the distance covered.,These algorithms monitor traffic conditions and journey times in real-time, meaning prices can be adjusted as demand for rides changes, and traffic conditions mean journeys are likely to take longer. This encourages more drivers to get behind the wheel when they are needed ?€? and stay at home when demand is low. The company has applied for a patent on this method of Big Data-informed pricing, which is calls ?€?surge pricing?€?.,This algorithm-based approach with little human oversight has occasionally caused problems ?€? it was reported that fares were pushed up sevenfold by traffic conditions in New York on New Year?€?s Eve 2011, with a journey of one mile , over the course of the night.,This is an implementation of ?€?dynamic pricing?€? ?€? similar to that used by hotel chains and airlines to adjust price to meet demand ?€? although rather than simply increasing prices at weekends or during public holidays, it uses predictive modelling to estimate demand in real time. ??,UberPool,Changing the way we book taxis is just a part of the grand plan though. Uber CEO Travis Kalanick has claimed that the service will also cut the number of private, owner-operated automobiles on the roads of the world?€?s most congested cities. In an , that he thinks the car-pooling UberPool service will cut the traffic on London?€?s streets by a third.,UberPool allows users to find others near to them which, according to Uber?€?s data, often make similar journeys at similar times, and offer to share a ride with them. According to their blog, introducing this service became a no-brainer when their data told them the ?€?vast majority of [Uber trips in New York] have a look-a-like trip ?€? a trip that starts near, ends near, and is happening around the same time as another trip?€?.??,Other initiatives either trialled or due to launch in the future include UberChopper, offering helicopter rides to the wealthy, UberFresh for grocery deliveries and Uber Rush, a package courier service.,The service also relies on a detailed rating system ?€? users can rate drivers, and vice versa ?€? to build up trust and allow both parties to make informed decisions about who they want to share a car with.,Drivers in particular have to be very conscious of keeping their standards high ?€? a , showed that those whose score falls below a certain threshold face being ?€?fired?€? and not offered any more work.,They have another metric to worry about, too ?€? their ?€?acceptance rate?€?. This is the number of jobs they accept versus those they decline. Drivers were told they should aim to keep this above 80%, in order to provide a consistently available service to passengers.,Uber?€?s response to protests over its service by traditional taxi drivers has been to attempt to co-opt them, by adding a new category to its fleet. UberTaxi - meaning you will be picked up by a licensed taxi driver in a registered private hire vehicle - joined UberX (ordinary cars for ordinary journeys), UberSUV (large cars for up to 6 passengers) and UberLux (high end vehicles) as standard options.,It will still have to overcome legal hurdles ?€? the service is currently banned in a handful of jurisdictions including Brussels and parts of India, and is receiving intense scrutiny in many other parts of the world. Several court cases are underway in the US regarding the company?€?s compliance with regulatory procedures. ??,Another criticism is that because credit cards are the only payment option, the service is not accessible to a large proportion of the population in less developed nations where the company has focused its growth.,But given its popularity wherever it has launched around the world, there is a huge financial incentive for the company to press ahead with its plans for revolutionising private travel.,??If regulatory pressures do not kill it, then it could revolutionise the way we travel around our crowded cities ?€? there are certainly environmental as well as economic reasons why this would be a good thing.,Uber is not alone ?€? it has competitors offering similar services on a (so far) smaller scale such as , , , and ,. If a deregulated private hire market emerges through Uber?€?s innovation, it will be hugely valuable, and competition among these upstarts will be fierce. We can expect the winners to be those who make the best use of the data available to them, to improve the service they offer to their customers.,The most successful is likely to be the one which manages to best use the data available to it to improve the service it provides to customers.,I hope you found this post interesting. I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., , ,Follow us on Twitter:??,??|??
A funny thing happened in the last few years. We began to lose the Closed World Assumption.,Now I can understand that this is not exactly huge, earth-shattering news; most people do not in fact realize that they've been using the Closed World Assumption to begin with. However, I'd contend that this event is having a transformative effect upon the way that we interact with data, one that may very well change the perspective about information in ways perhaps as profound as Ted Codd's introduction of the relational model in the 1970s.,In basic terms, the closed world assumption can be stated as "When we model something, our model is complete." ??Most people who have had to define a data model recognize that this statement is at best a convenient fiction - any effort to completely define almost any object ultimately comes down to identifying which attributes of that object are relevant to the particular business domain - yet even with this observation, the necessity of restricting attributes is so fundamental to the way that models are designed and built that it is seldom challenged.,Part of the reason for this is due to the programming and analysis tools that derive from the model. It is the rare programmer that designs an application with the assumption that the attributes of the data that he or she is working with may very well grow and mutate over time. With the closed world assumption comes surety; it is not necessary to perform discovery upon a model to find out all of the attributes that are available, instead, semantics are clearly articulated for every attribute, , are well established, and there is neither ambiguity nor uncertainty about the interpretation of the data.,Take those assumptions away, and programming becomes much more ... exciting. During the first wave of web services, in the early years of the 2000s, one of the central issues was both discovery about the structure of data and the discovery of data provenance. The solution to the former problem was to create a schematic representation of the data via WSDL, an XML representation of schematic structure that acted as a contract to any requester that what would be returned from a service, given a set of parametric end-points, would validate according to that structure.,Typically, most programmers would then immediately run this structure against a tool such as JAXB that would map the service to a particular method and map the data to a corresponding data structure in Java or similar languages. In other words, rather than deal with the structure in ambiguous terms (using XML navigational tools such as XPath or XSLT to extract relationships that assumed an open world assumption), these programmers would, with great alacrity, convert them back into closed world objects.,Now, XSLT (currently up to XSLT 3.0) is a curious beast. It is a templating language, and works upon the notion that the incoming XML source will have certain patterns, that if caught, can be transformed. In other words, XSLT does not in fact require that an incoming XML satisfies any constraints. This is an especially useful capability to have when dealing with documents, because such documents will, almost by definition, have a great deal of unpredictability about which elements are in it in which order. That it uses recursive processing to do this, typically (though not always) walking down the various branches of a hierarchical structure to do this, points out just how radically different this form of programming is from that of the Java-esque world (and may help to explain why few Java programmers feel comfortable working with XSLT in the first place).,The evolution of Javascript from being a simple client "scripting" language into being increasingly a first class enterprise language (via node.js and the growing number of NoSQL databases such as Couchbase or MongoDB) has followed a similar trajectory. A Javascript object (or JSON object, though the two are not quite the same thing) is a composite of linear arrays and hash maps. There is no explicit requirement for schematic type definitions (though the latest ECMAScript proposals incorporate the ability to specify type characteristics as advisory, rather than required, metadata).,This means in practice that a JSON object has no requirement to be a closed world entity; discovery comes from querying, and actions take place when specific patterns with the query are matched in the source objects. Again, there is an instinctive tendency upon programmers to assert semantics into these structures and work upon the assumption that certain properties are de-facto required, but the reality is that applications built this way are usually very fragile and highly sensitive to changes in data model.,Indeed, mutability has become a key characteristic of modern JSON objects. Objects add and shed properties throughout their processing life-cycle. They take on interim semantics that provide more information about the object within the context of the operation, adding properties and methods seemingly without abandon. There are potential problems with such approaches, namespace collisions being the most significant, but the reality today of most Javascript programming is that it routinely jumps the divide to the Open World Assumption, especially once higher order functions are incorporated into the mix.,RDF is, of course, completely characterised by its abandonment of any Closed World Assumption requirement whatsoever. There are schema languages (RDFS/RDFS+/multiple flavors of OWL, SPIN and other such tools) that can provide constraints on existing RDF data, but these constraints working upon the assumption that all things are possible unless they are explicitly limited. This is actually a major limitation of the XSD families of XML schemas, which assume the opposite - that nothing nothing is possible unless explicitly declared, though there are "outs", such as the <xsd:any> tag, that can violate that assumption. Still, it's noteworthy that for many data modelers who come from an XML background, <xsd:any> is considered a failure of effective modeling, rather than an acknowledgement that no model can be completely described.,The open world assumption has a profound impact upon data science (and provides a very good reason why data analysts should understand the deep vagaries of modeling). One of the key goals of the analyst or statistician comes in understanding what constitutes both the independent and dependent variables that most accurately describe a given model. Put another way, most stochastic methods work upon the open world assumption, even if it's not stated as such.,At the same time, with the rise of "big data", there is a growing amount of data where the models have been explicitly defined. At the same time, however, these models reflect business processes that in many cases fail to account for critical variables (attributes) that reflect why specific phenomena occur. The role of the data scientist in that case is to re-establish context into this data, as this context is critical for any type of predictive analysis. The statistician can look at this information to detect trends, patterns and anti-patterns, but the analyst then needs to be able to ascribe causal agents - weather variability, real-world events, regional or cultural variations, even psychological motivations - to these patterns to move them out of "random" events and into anticipated ones.,What this means in practice is that the models that such analysts build themselves become open-ended, and account for what amounts to the mutability of the data models over time. This doesn't necessarily mean that the models suddenly change their reflection of what is "real". Rather, what emerges is that the data lifecycle itself will likely grow in terms of what is known when, and as this model becomes more complete (and complex), this will change the nature of the analytics performed upon it.,In a similar vein, these more complex models in general are not easily represented via SQL, and many analytics tools that exist today are still very much tied into the SQL paradigm. That is changing, but as the change happens it also provides new ways of looking at information that simply was not easily done in the past. Best practices for analysing data are changing as open models replace closed, and this in turn is changing the very shape of data analytics.,For instance, hierarchical data forms provide a way of more efficiently encoding textual information. The structural metadata is augmented by semantic metadata - data enrichment, semantic tagging and mapping, natural language processing and conceptual inferencing, each of which makes it possible to get more context from what has traditionally been a fairly limited data source previously for analysts and statisticians (beyond fairly simple lexical analysis). This is undeniably open world data, where pattern templates and advisory models replace clearly defined attributes.??,Overall, this open model (or open world) assumption makes the data scientist's job more challenging, but also provides a much richer and more dynamic view of what is, in essence, ways of looking at the real world. Semantics, natural language processing and textual analytics, long a sidewater of analytics, is becoming more critical to the overall modeling, statistical processing and analysis of all data, and in the process is reshaping the field of data science dramatically.,??is a data scientist, information architect and writer, working for ,.

??,Enterprises that adapt, evolve and exploit this new digital reality will thrive, while those that do not, will be lost to the sands of time like Dinosaur !!
Several members of the Talent Analytics team had the opportunity to attend Teradata's ,??in Boston on May 22. With a great speaker lineup, the educational and networking event offered both a technical and digital marketing track that covered the latest advancements across the big analytics ecosystem and highlighted how data-driven products and services such as Hadoop and Aster could support business goals.,While the topics were certainly technical in nature, I found it interesting that the need to better understand how analytics professionals tick was interwoven into several presentations (,). Specifically, three themes stood out when Bill Franks, Chief Analytics Officer from Teradata and Simon Zhang, Director of Business Analytics from LinkedIn talked about some of the criteria they use when hiring, motivating and retaining analytics professionals., Bill said "commitment" is one of the five most important traits he looks for among analytics candidates. He identifies commitment by determining whether the candidate is willing to go the extra mile vs the "almost done" mentality given how difficult analytics work is.,Simon Zhang supported the importance of commitment by indicating passion and belief made up 80% of what he looked for among analytics candidates at LinkedIn. He said a strong belief in the data is critical in order for analytics teams to establish and maintain trust with the internal customers they support., Simon pointed out skills make up just 5% of his total quotient in what he looks for among good analytics candidates. Bill echoed this sentiment by suggesting skills can be learned over time and instead to look for other, more intrinsic qualities like commitment, creativity, business savvy, and intuition., Both Bill and Simon highlighted the importance of being able to present data in a way that the business can understand and appreciate. Bill emphasized the importance of strong data presentations by suggesting results are only 50% of a successful analytics initiative. He offered pragmatic examples of "business-friendly" data storytelling. Simon supported this sentiment, stating "analysts are artists.", The continued emphasis on innate traits in the technical track at the Big Analytics conference underscores the urgent need for a broader understanding of analytics professionals - in a quantitative way.,In order to facilitate greater insight in this area, some of the top minds in the analytics industry are stepping up to the challenge ,.??With the goal of??adding significantly to the knowledge base around analytics talent for businesses,??Talent Analytics and the International Institute for Analytics (,) have announced a ,.,The study runs through the end of June 2012 and is open to anyone who works with deep dive analytics or manages a team of analytics professionals and wants to be part of something with potentially??,implications for the growth of the entire business analytics industry.,If that's you, click the banner below to participate!

Step by Step Tutorial to Deploy??,??Cluster (fully distributed mode):,to read complete post Please visit



Extract CDR information and summarize it for each unique combination of caller and called number,For each caller and called number, count the frequency of calls made, the number of smses sent, the number of prime time calls etc,Use this model to develop call behavorial profiles to target . Example : Friends and families program,Interrogate the social network database for specific behavior . For example : Who are my existing customers who make more than 20 % of calls to competitive networks during peak time and either the call duration for day exceeds 90 minutes or number of calls per day exceeds 12 ?,Can we target them with a ?€?friends and families?€? scheme to bring their most frequent called numbers into our network fold and incentivise them in the process,This is just the tip of the iceberg in terms of leveraging Big Data Network link analysis or graph analysis to impact ARPU ( Average revenue per user ) in the telecom industry,more at??


Apple created the most valuable company in history with this philosophy. Each and every Apple product communicates back to the Apple data warehouse about its usage seamlessly, with the customer?€?s tacit agreement. The oceans of data gathered through the Operating System, Genius, iTunes, and AppStore are then used to craft elegant, useful products like MacBookAir, iPhone, and iPad, which then provide even more extensive insights into how people use them. It?€?s a virtuous circle between Big Data and Big Design., ,We believe that each industry or endeavor will have an ?€?Apple-like?€? organization that avoids direct competition, and is focused on re-imagining products and services based on Big Data insights. Dyson, Starbucks, Volkswagen and Method. One after another, industries are being transformed by the concepts of Better Design, Deeper Insights. Governments and non-profit organizations have felt the impact too. Singapore is the first (ART+DATA) government, with virtuous circle between data collected and the urban planning design of the city itself. The Gates Foundation is a shining example of applying a data-driven approach to the design in humanitarian programs.,The key attributes of an (ART+DATA) organization:,1. The concept is the output of analyzing 50 cases of successful and unsuccessful companies spanning more than 10 industries over 20 years (1990 ?€? 2010).,?? 2. (ART+DATA) is not about Information Systems or Technology. It is about defining the insights desired, then determining the best way to collect and extract the data.,?? 3. (ART+DATA) organizations don?€?t design products and experiences for aesthetics and appearances. They design for usefulness, effectiveness, and more efficient data management.,?? 4. (ART+DATA) method is fact-driven, and avoids intuition, even in design.,?? 5. (ART+DATA) organizations have the leaders of design and data at the highest levels.,?? 6. The best organizations live for the question, ?€?What data do we need to make a better product, a better service, a better experience??€? All Research, Product Development, Technology Infrastructure, Distribution Methods, even Accounting decisions, flow from that question.,??,@artplusdata


Most analytics and data projects have started thinking of investing in big data initiatives.?? With so much buzz about big data, organizations have started investing or are thinking of investing in Hadoop While it is great to stay on top of trends, it often ends up being another investment where the full benefit and potential is simply not realized.??The learning curve is too steep and the time to implement too high. Current analytics resources lack the strong programming skills required to conduct even simple analysis tasks and activities using Hadoop. In this post, I would like to focus on providing a better understanding of what types of analysis are better suited for Hadoop vs. non-Hadoop technologies in order to simplify big data analytics and will provide an example of a big data ecosystem for delivering a successful data strategy.,Big data analytics is dominated by analysis of clickstream data and number of early projects started with analyzing massive data sets generated by digital media. ??Consequently, for the purposes of this post, I will focus on clickstream or online data.,When we think of analysis of data for clickstream, there are three distinct areas of analysis:,Analytics initiatives for big data typically focus on product analytics ?€? understanding and optimizing the site and user behavior.?? The three key challenges here are: a) datasets are massive b) change constantly c) access to granular data (data at a user or session level), important for actionable insights, is difficult to get and manage. Consumers of these insights are mostly Product Managers used to optimize product and user experience. Product Analytics has two key distinct types of analysis:,1) Managing and optimizing the site -??analyzing traffic, users, user engagement, site testing, VOC etc.,2) Advanced Analytics ?€? typically iterative analysis like ranking, cohort analysis, algorithm development, collaborative filtering, product or usage affinity etc.,The tools available up to this point typically managed one part of the analytics needs ?€? either being good at website analytics (usually at some level of summarization) or advanced analytics, that required data to be pumped in, using an API or other data movement tools, to databases or big data systems.?? What this meant to the analytics team was the need to deal with multiple tools/platforms and an enormous effort to move data across various systems.?? Quite often, this resulted in delays in providing conclusive analysis and actions from the insights in a timely fashion. Web interactions happen quickly and any delay can mean lost opportunity on user engagement or conversion.,At Splunk, we have thought about this problem extensively.?? We put the customer needs as well as the necessity for a data platform at the forefront of solving this hard problem (BTW: we love to solve the hard problems in a simple way).?? We recognized that not all data is equal and the analytics from each datasets vary based on purpose. Most of the data is needed for real-time and historical analysis of multiple data types or sources. However certain data needs to be used for advanced analytics, mostly??iterative??analysis, using batch processing.,With these requirements in mind, we worked on a unique solution that allows you to have the best of both worlds. A data platform for real-time analysis with the ability to reliably export events to Hadoop. The ability to explore and browse HDFS directories and files (to decide what to import), and the ability to import data into Splunk from Hadoop.?? Being announced today to deliver this is Splunk Hadoop Connect.?? As an analytics practitioner for number of years and having dealt with big data in the past, I am very excited about this launch.?? How will this help speed up analytics and provide value for big data??? Three distinct ways:,For clickstream analysis, all relevant data sets (web logs, IT ops logs, offline system logs, POS transaction logs) will be reliably collected, indexed and made available for real-time analysis and visualization in Splunk. Alerts will be triggered from further analysis??or with appropriate integrations; Splunk will be able to integrate with systems (CMS) that facilitate changes to the website. High value data will be sent to Hadoop using Splunk Hadoop Connect.?? With this ecosystem, analysts will be able to quickly respond to the basic and intermediate analytics questions using Splunk and then use Hadoop to conduct advanced analytics without worrying about data movement to and from Hadoop ?€? a major shift from spending efforts on data movement to conducting analysis that moves the needle for the business.,A good example is capturing site usage, user behavior data from clickstream, marketing and Voice of Customer (VOC) data, etc.?? This data is collected and indexed in Splunk to perform various type of analysis for understanding user behavior and bottlenecks in the conversion funnel.?? Insights from this analysis or trends are used to perform actions in real-time like optimizing pages, SEM spend or offering a different version of pages (etc.) by passing parameters to the content management system (CMS). Part of the same data or all of it is sent to Hadoop from Splunk to conduct advanced analytics like ranking, cohort analysis or predictive analytics.?? In other words, analysts have created a data fabric in Splunk and integrated Hadoop for specific batch analysis.,Want to??,??about this exciting and innovative approach to solve big data problem??? Go to Splunk.com or download Splunk for free.?? Did I mention that the Hadoop Connect App is free too??? Here is the download??,.,PS:?? We are also introducing??,??today.?? This will help your friend in IT to manage the Hadoop infrastructure or help you make a new friend in IT that manages the Hadoop infrastructure ?€? break the silos between IT and the business.??,??or??,??HadoopOps.




In this article we will talk about some of the successful applications of analytics. We will start with 2 different examples which I was reading recently from a podcast of Accenture and then move to some of the other industrial examples., ,??,Gary Loveman, CEO of Harrah?€?s, was presenting at an event and sitting in the front row were all of his competitors. He stood up, he showed slide after slide that showed precisely how they use their loyalty card, how they did the analysis, what kinds of metrics they looked at. And their competition was writing everything down.,Finally, after about an hour of this, someone raised their hand and said, "Dr. Loveman, doesn't it bother you that your competitors are taking down every word you said?" He just looked quizzically at them for a minute and said, "No, not really, because by the time they figure out how to do what I just described to you, we will be so far ahead of them that they will never be able to catch up.", , ,??, ,??,That is really what makes analytics a sustainable differentiating strategy for these companies. It is because it is not about a single insight; it is about a set of processes they have, a way of using data and incorporating it into their decision making that really helps them transform their business. It makes them much more able to maneuver changing business conditions; it makes them much more likely to anticipate changes in customers and markets; and, most importantly, it allows them to come up with different scenarios and understand how they ought to react to these changing market conditions.
Educating savvy and business-minded Indians on the importance of numbers??and analytics in your business is like teaching the properties of sand to someone in the desert, but here is my effort anyway.,The simplest definition of analytics is "the science of analysis." However, a practical definition would be how an entity, e.g., a business, arrives at an optimal or realistic decision based on existing data. Business managers may choose to make decisions based on past experiences or rules of thumb, or there might be other qualitative aspects to decision making, but unless data is considered, it would not be an analytical decision-making process.,Analytics have been used in business since the time management exercises that were initiated by Frederick Winslow Taylor in the late 19th century. Henry Ford measured pacing of the assembly line, thus revolutionizing manufacturing. But analytics began to command more attention in the late 1960s when computers were used in decision support systems. Since then, analytics have evolved with the development of enterprise resource planning (ERP) systems, data warehouses, and a wide variety of other hardware and software tools and applications,Today, businesses of all sizes use analytics. For example, if you ask my fruit vendor why he stopped servicing our road he will tell you that we try to bargain a lot and hence he loses money, but on the street next to mine he has some great customers for whom he provides excellent service. This is the heart of analytics. Our fruit vendor TESTED servicing my street and realized that he is losing money ?€? within one month he stopped servicing us, and even if we ask him, he will not show up. How many businesses today know who their MOST PROFITABLE CUSTOMERS are? Do they know who their MOST COST GENERATING customers are? And given the knowledge of most profitable customers, how should you target your efforts to ACQUIRE the MOST PROFITABLE customers?,Large business uses analytics to drive the entire organizational strategy. Some examples include:,Common applications of analytics include the study of business data using statistical analysis to discover and understand historical patterns with an eye to predicting and improving future business performance. Also, some people use the term to denote the use of mathematics in business. Others hold that the field of analytics includes the use of operations research, statistics and probability; however it would be erroneous to limit the field of analytics to only statistics and mathematics.,While the concept is simple and the notion is intuitive, the common leverage of analytics to drive business is just in its infancy.
Data Analytics is a process of summarizing data with the intent to extract predictive information and develop conclusions from the data and using it for making strategic decisions and operational policies. A huge demand for data analysis services in India generates ample earning opportunities to companies which are offering data analysis services.,Data analytics in India is extensively used in Banking and Financial sectors. Data mining and data analysis being carried out by BPO firms in India is done in very sophisticated manner so that there is no value loss in due course. KPO?€?s in India coming up with financial Research is going to have a very bright future as many international companies start outsourcing.,Data Analytics can be broadly categorized in Data Mining and Predictive analysis, the primary task in data analysis is to do data mining which is also considered as a particular data analysis technique. Data mining focuses modeling and knowledge discovery for predictive purposes to help in some strategically important descriptions to be used for the advantage of firms or the corporate organizations.,Whereas predictive analysis focuses on application of statistical or structural models for predictive forecasting or classification, the text analysis is done using statistical, linguistic, and structural techniques.,Data analysis requires professionals who are excellent in data interpretation and analytical mindset. Data Analysis Services India Companies Data analysis services intensify the research method and research outcome exclusively and successfully. Teams of experienced Data Analysis work towards the quality research projects and help clients save their time. Optimum utilization of resources is made possible by data analysis as it helps in to concrete the entire data for better output.,Some other works included in data analysis such as quality optimization in electronics, paints, cosmetics, pharmaceuticals, food, beverages, consumer products, insurance, banking, etc. are being helped by data analysis services in India. Moreover, with a varied range of data analysis services in India help clients optimize the design of new products, do association rules discovery. And check customer loyalty or hurl analysis, etc.,Companies looking for customer segmentation and profiling as well as classification of respondent and market segments, etc. hire the data analysis services being offered by a range of IT and ITES firms.
Analytics is not just pure science; it is part art as well. Organizations that master the fine art of using analytical tools realize increased revenues and enjoy cost savings.,The scientific approach involves the following four key steps:,Analytics begins with observing the phenomenon and setting up the right business problem. It requires understanding the facts, to which you have ready access, and then drawing conclusions from it to identify the business problem which needs to be solved. For example, a manufacturing company is suffering from declining profits. By looking at their balance sheet we realize that revenues have declined while the costs have remained constant. Through these two facts, we can identify a simple business problem ?€? the manufacturing company must reduce costs or increase revenue if it wants to have the same profitability as before.,??,As you can see, you can achieve increased profitability by both increasing and decreasing marketing budgets. There are several implications of each action beyond the primary implication and all need to be evaluated. The key element of the hypothesis-building phase is that you should have a mutually exclusive and collectively exhaustive set of hypothesis. This means we should think about all the possible sets of relevant hypothesis for the situation at hand and ensure they do not overlap and that together they are complete.,A good experiment usually tests a hypothesis. However, an experiment may also test a question or test previous results. The fundamental reason for following this process is to ensure the results and observations are repeatable and can be closely replicated given similar circumstances. Let?€?s continue with the example above and set up a test for the manufacturing company to learn whether increasing the marketing budget would affect revenue. In this case, we would set up a TEST where we run the EXISTING marketing programs and call it GROUP A while in GROUP B we run the increased marketing program. At the end of the observation time frame (assume 2-3 months), we would measure revenue for GROUP A and GROUP B and understand the differences. As long as the groups have a statistically significant size we should be able to repeat these results.,Continuing our manufacturing company example, let?€?s assume that GROUP B performed far better than GROUP A. Let?€?s also assume that at the same time we increased marketing our competitors decreased it in the GROUP B target market. Now the question becomes, was the incremental benefit driven by our increased marketing or the fact that competitors reduced their marketing? Assimilating all possible and relevant information is extremely important in order to reach a good decision.,As you can tell, while scientists have been utilizing the above mentioned technique for a long time, businesses are just beginning to use it. This requires a strong commitment to the scientific process and a systematic approach to create a TEST & LEARN environment where you are constantly testing, learning and evolving to create increased bottom line benefits for a company.
Merriam-Webster defines analytics as ?€?the method of logical analysis.?€? In business, analytics is the process of analyzing quantitative data to glean information about performance. This process is often used in marketing departments and in risk management. Analytics is also a very important tool for the management and maintenance of websites.,??,The field of analytics is growing, as more companies realize the benefits of the method. This is especially true for internet-based companies. As the field grows, so does the number of jobs. If you are interested in statistics, research and marketing, a career in analytics may be ideal. Although specific undergraduate programs in analytics aren?€?t very common, the following three degrees will prepare you work in the field.,Statistics is a branch of mathematics dealing with the collection, analysis, interpretation and presentation of masses of numerical data. This is the degree that comes closest to matching the field of analytics and will prepare you the most for handling and interpreting data. Statistics is a degree that can be earned completely online, so if you are interested in switching careers, you can attend online school without having to quit your current full-time job. In addition to analytics, a degree in statistics is also valuable for a career in several other fields and is one of the most sought after degrees in the current job market.,If you enjoy learning about computers and are fascinated by the internet, a degree in information technology will allow you to apply these interests to the field of analytics. Information technology is field that involves developing, maintaining and using computer systems, software and networks for the processing and distribution of data. This degree is offered at technical colleges, both on-campus and online, at the associate?€?s level, allowing you to make a quick career change through a rewarding two-year program.,Marketing is a business process or technique that works to promote and sell products and services. Although a marketing degree technically prepares you for a career in sales and marketing, analytics is an integral part of most businesses?€? marketing strategies. The topic of analytics is often reviewed as a part of your studies, and additional training is usually provided once you gain employment in a marketing department. Marketing is subject that can be learned both through traditional and online schooling.,More information about a career in analytics and any of the above degrees can be found through a quick internet search, or you can contact the admissions department of your local college or university.,??

What drives the decision maker(s) to say ?€?yes?€? or ?€?no?€? for any particular decision? Let?€?s look at some of the key business challenges that many organizations are facing today where decision making is critical.,??,Most organizations realize that innovation and culture dictate the long-term success and even survival of the company, as we see it today. Is there an approach or a methodology that can really help organizations understand what analytics maturity looks like and more importantly, how they can embed analytics into their culture and increase organizational maturity in analytics?,??,As a result, innovation then comes from all corners of the company by igniting the brilliance and not just from the corner office that is dedicated to innovation. Though adding a new value into company?€?s culture is never easy, with the right approach, proper tools and right leadership, cultures can be reshaped ?€? with amazing results. Every organization that is adapting Analytics will be going through a maturity curve moving from operational or descriptive analytics to knowledgeable or predictive analytics to transformational or prescriptive analytics. Below is the organizational maturity curve moving from operational to transformational analytics.,??,??,The approach that has worked very effectively for us over the years in helping customers navigate across the Analytics Maturity Curve is ?€?Think BIG, Start Small, Deliver Value?€? (,) as it allows organizations to evolve at their own pace.????????,??,??,??,Referred as Business Analytics Competency Center (BACC).,??,1.???????? Analytics COE steering committee will be setting the vision and strategy for Analytics COE by combining business and IT stakeholders.,??,??,??,??,??,great analytics look like if users don?€?t trust the data they see in them, users will go back to their old ways be it spreadsheets or access databases.,??,Hence there is a greater need for organizations to comprehensively address issues related to data management by creating a practical permanent data governance function that becomes part of corporate core operational process to instill a culture and discipline of treating enterprise data as most valuable and key competitive asset.,??,Very similar to PMO (Project Management Office) in many organizations, having a DGO (Data Governance Office) is one of the most effective ways to instill the culture of analytics.,??,??,??,??,??improvement opportunities (BIOs) and iteratively collaborating with the business stakeholders to co-innovate and produce transformational solutions.,??,??,into company?€?s culture is never easy, with right method, cultures can be reshaped ?€? with amazing results.,??,The Results Pyramid,???€?the classic model that organizations throughout the world have used to create a culture of greater employee ownership and engagement for achieving Key Organizational Results.,The Results Pyramid,??explains how to make culture change happen effectively, accurately, and efficiently in an organization or team.,??,The Results Pyramid helps leaders (in our case, the steering committee of?? Analytics COE) add emphasis on how people need to think differently. Once leaders learn how to shape beliefs by providing new experiences, they find that their efforts make a much stronger and longer lasting impact on their company?€?s culture by driving appropriate actions and realizing the results continually.,??,New experiences create or reinforce beliefs that motivate necessary actions to produce desirable results.,??,??,??,Adoption is not just about top-down push on business users to use analytics rather finding right analytics that deliver value to business users. Below are some of the most common obstacles for widespread analytics adoption:,??,??,??,I will attempt to simplify it without making it simplistic:,??,??,??,One of our very large customer who started their analytics journey with us a couple of years ago leveraging the principles and the framework laid out here shared the following news beaming with pride "IT isn't order taker any more, we are now part of strategic group that partners with business leaders in the ideation stage to drive innovation. Just a couple of year?€?s back this was an impossible dream and today it's a reality.",Wishing you greater levels of user enagement and adoption in your organization leading to transformational outcomes.,??,??

Includes great charts from FlowingData.com.

Big Data is a term used to categorize an excessive amount of aggregated data. But, how do Data Miners manage all of this data? Hadoop is one of the popular tools that data analysts are using to store and mine immense volumes of data.,Here are , , to store and process ,. As open sourced software, Hadoop eliminates up front licensing costs for managing and processing large volumes of data. In other words, anyone can use Hadoop on any computer ?€? for free. Other cost advantages include the fact that Hadoop can exist in a stand- alone environment, eliminates inter-processing communication between mappers. Finally, one cannot overlook that the Hadoop ecosystem creates a stable, fault tolerant environment.,??,If one is an expert computer programmer, Hadoop will serve to be extremely helpful, , such as: text mining, risk assessment, sentiment analysis (link to text analytics), predictive models, pattern recognition, and index building.,??, , ,, which is great for social media or retail data. Speaking of which, Hadoop can handle many types unstructured and semi-structured data, including but not limited to: web logs, system logs, audio/video, email, transactional data.,??, , that make the Hadoop File System run smoothly and quickly. These parts of the??, have fun names like, Pig, Hive, Oozie, Flume, and if one is a computer expert, one can customize each component to fit their need.,??,??,??, , in order to use Hadoop properly. Prerequisites include being knowledgeable in Java and/or other programming languages to run data processing jobs. If an organization does not have in-house expertise, there will be a cost to customize. However, the cost may be worth it ?€? dismissing the??, in a company?€?s data may translate into untapped opportunity, opportunities that your data savvy competitors may discover.
Initiating the LPG (Liberalization, Privatization, and Globalization) policies, the consumer behaviour is also changed drastically with their preferences and choices of the products available in the market. Consumers are demanding Global Products at Local Markets. Consumer Point of View (40-60% of the total consumer?€?s of the economy or total population), with available time if he found more products available (all different products at one place), he will ready to pay higher prices. Retail industry usually follows marketing strategies to attract customers/consumers. As it is evident, 80% of consumer?€?s will visit the same stores with increase their level of satisfaction (psychology of customers).??,Real analytics for retail should be effective, robust and scalable applications for the effective decisions. Analytics should able to understand the rapid changes and needs of the modern retailer.????Analytics should always should able to capture the above information to provide a better solution to the retailers. Analytics in retail domain needs a complete solution which could be dynamic and flexible for its applications and widely useful in order to take decisions more efficient and innovate. ,??,A microeconomics principle suggests how the markets functions and its consumer preferences. Major important factors that analysts keep in their consumer analysis including a)????????Consumer Theory/ Behaviour, b) Consumer preferences, c) ????Nature and differentiation of the products, d)??????Demand and Supply differentiate with Price of the products with respect to the market price, e) ????Budget Constraints of the market participants, f)??????Revealed Preferences, g)??????Utility maximization of the products with respect to the various similar products, h) Markets and their competitors?€?, i) Asymmetric of Information, which consumers doesn?€?t have but produces have and finally uncertainty.,Retail analytics should deliver scalable, flexible, advanced and cost effect analytics solutions for optimizing merchandizing and marketing decisions. In point of retailers, the following points should keep in mind, which includes, a) Basic objectives is minimize the cost of production (fixed cost, variable cost) with optimizing profit; b) Profits including economic profit or normal profit; c) asymmetric??information of future producing units, product initiatives etc., d) knowing their stakeholders and competitors, e) uncertainty, f) utilizing of the opportunity cost, for the betterment of the industry, g) factors including price ?€? demand ?€?supply ?€? equilibrium ?€? profit, g) Managerial Cost (fixed, variable cost) includes salaries, rent etc., i) resources, technology, information, j) ??Threatens and k) ??Damaged, Loss in management and marketing.,Vendors are the major source for the retailers for the smoothing their business. In view of the vendors the following points should consider by the analysts including, a) supply chain, b) demand forecast, c) timely supply, d) price and profit, e) costs (fixed, variable cost, transportation cost), f) contracts and contact within the industry.,These components can be deployed on various computer platforms and easily enhance the functionality of a retailer?€?s different information systems such as for example: ,??has built a comprehensive series of sophisticated modelling and optimization tools to provide analytical support to essential business functions. Retail Analytics components can be deployed either as ?€?standard?€?, or as building blocks of a more customized implementation project, when unique requirements derived from specific business processes, rules or information sources need to be met.,??





 


Sign up for a, ,And monetize ALL your data, , 
??
Quoting Wikipedia: - Unstructured Data (or unstructured information) refers to information that either does not have a pre-defined , and/or does not fit well into , tables. Unstructured information is typically ,-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and , that make it difficult to understand using traditional computer programs as compared to data stored in fielded form in databases or , (,) in documents.,Yes, most big data sources, including Facebook, twitter etc., have unstructured data. And nearly no analytics can work directly on this unstructured data. Unstructured data is the starting point, but it has to be metamorphosed into some structured format before we can start with any actual analytics technique application. So what is the process?,Business requirement: - Tweets, Facebook postings and other social comments have to be analysed to determine the sentiment of the population.,Creating semi structured/ structured data (which can fit into relational tables) will involve dissecting the text into words and phrases which can then be categorised from ?€?good?€? to ?€?bad?€? and everything in between. Converted numerically, this would be in a range of +1 to -1. This set of fully numeric data is then ready for use. And all the analysis techniques can then be used to conclude and arrive at results. ,So does this process of converting unstructured to structured data have to be manual, through heuristics? Or machine driven through algorithms? Algorithms reduce accuracy but increase scale. So a judicious decision or a gradual shift from manual to algorithm can be used to standardise this process within the organisation.,In fact, in this whole landscape, the decision on what data to delete becomes of paramount importance. And there are a new set of data guardians who are experts on helping organisations retain only relevant data.,This understanding and coming together of all the bits and pieces has given a lot of confidence to decision making process on dynamic and big data.,About the Author: -?? Subhashini?? is currently active in the Analytics Training (,)?? , Blogging and Consulting ??arena, and?? has a decade of experience across roles in Analytics in Retail Finance and Banking . These roles have been across Risk Management, Collections strategy, Fraud Control and Marketing. Her area of interest is the integration of results / outputs of Analytics with Business Decisions ?€? Tactics and Strategy.,(Link to profile - , )




Past week I attended a Marketing colloquim where I was invited as a panel member to discuss the new realities of marketing performance and metrics. What struck me was the growing importance of Data across sectors and company size. But aside from the consensus that existed??on the importance of Data in defining,??adapting and evaluating go-to-market approaches, ??there were a multitude of opinions on , to??realise this ?€? in particular on the amount ??of technology investments and number crunching activities that are required.,The ensuing discussion reminded me of a quote I read in an earlier Harvard Business Review article (June 2010) in which was stated that ?€?,?€?.,Provided that humans take these decisions (disregarding the impact of machine intelligence for a minute), I reasoned?? that marketing executives (and all executives for that matter) should focus their new investments on the ability to take better and faster decisions. Technologies that??address the ?€?triple V?€? challenges of customer data (,ariety, ,elocity, ,olume) should therefore be on the radar of every senior Marketeer.,The ??challenge however is where to start first. Most companies are not the Amazons, Linked-Ins, or Capital One?€?s of this world, and are??tuning their??multichannel business models??to optimally match the new economic realities. For those companies the question is not whether to invest in Analytics or BI, but how to phase such??investments in????incremental, meaningful and value-adding steps, all while keeping in sync with the required organisational changes, and??- let's not forget - ??IT.,What I am starting to learn is that good storytelling??- vividly describing how a data-driven, tool-enabled decisision process could??impact time-to-market and ultimately profit??-??is critical to convince executives that analytics is not just another hype. ??,Being, like many, in an organisation with analytical aspirations, I find myself spending??a lot of??time in crafting the stories that resonate best with my management and help them taking the next best step. Suprisingly, my spreadsheet-based business case has been left untouched for a while now.. It's more about??finding??a good business phrase rather than a good business case for BI.
 
Curating use cases from big data seems to be more of an art than a science. Wanted to share 10 big data use cases specifically unearthed in the digital side. Here they come
Big Data can help in mapping and understanding customer behaviors, and in developing one-to-one marketing programs or innovative services. However, Big Data is too often presented as a technological capability subsequently requiring armies of data scientists to mine and analyse data.,Yes, managing and exploiting the growing amount of internal and external data is a necessary condition to steer business performance. But it is??far from a sufficient condition.,In a recent meeting around a business case for a new investment, one manager formulated as it follows ?€?,?€?. ??Data science should support decision making. For this to work properly, I think a good decision process could look as follows:,(1)?????? , ?€? what do we want to learn from data, i.e. how much uncertainty do we want reduced ?,(2)?????? , ?€? one that describes the bigger picture as well as the connections between the questions ?€? a value chain, a process diagram, a sales funnel,.. This can keep a group or audience??in sync with respect to the scope of what we want to measure.,(3)?????? , ?€? should be about the data as much as about the analysis. I mean linking results back to the business problem and communicating/presenting conclusions is crucial.,(4)?????? , ???€? questions provoque more questions. This is OK, as long if we don?€?t deviate from the goal, exceed timings or boile the ocean.,(5)?????? , ?€? and don't forget to communicate??and??followup??underlying assumptions or uncertainties.,A book that treats the role of measurements in taking business decisions very well is Douglas Hubbard?€?s ?€?,?€?.
, ACM's Special Interest Group on Knowledge Discovery and Data Mining (KDD), is the premier global professional organization for researchers and professionals dedicated to the advancement of the science and practice of knowledge discovery and data mining. It established the Innovation and Service Awards to recognize outstanding technical and service contributions to the KDD field. , Innovation Award recognizes one individual or one group of collaborators whose outstanding technical innovations in the field of Knowledge Discovery and Data Mining have had a lasting impact in advancing the theory and practice of the field. The contributions must have significantly influenced the direction of research and development of the field or transferred to practice in significant and innovative ways and/or enabled the development of commercial systems.,The previous SIGKDD Innovation Award winners were Rakesh Agrawal, Jerome Friedman, Heikki Mannila, Jiawei Han, Leo Breiman, Ramakrishnan Srikant, Usama M. Fayyad, Raghu Ramakrishnan, Padhraic Smyth, Christos Faloutsos, and J. Ross Quinlan., Service Award recognizes one individual or one group for their outstanding professional services contributions to the field of knowledge discovery and data mining. Services recognized include significant contributions to the activities of professional KDD societies and conferences, educating students, researchers and practitioners, funding R&D activities, professional volunteer services in disseminating technical information to the field, and contributions to society at large through applications of KDD concepts to improve global medical care, education, disaster/crisis management, environment, etc.,The previous SIGKDD Service Award winners were Gregory Piatetsky-Shapiro, Ramasamy Uthurusamy, Usama M. Fayyad, Xindong Wu, the Weka team led by Ian Witten and Eibe Frank, Won Kim, Robert Grossman, Sunita Sarawagi, Osmar R. Zaiane, and R. Bharat Rao., Nominations should include a 1-2 page summary statement justifying the nomination along with other supporting materials. Each nomination should be co- sponsored by at least 3 people. At most one award will be given each year in each category. All communications will be via email. Nominations will be valid for a period of 3 years.,Please email all nomination and support documents by July 1, 2012 to , , with the subject line ,.,The 2012 awards will be presented at the 18th ACM SIGKDD 2012 Conference in Beijing, China, August 12-16 2012 , ,For more information, see 



When an organisation embarks on a Big Data project, it?€?s a journey laden with lots of landmines. Even if one is unable to manage one risk, it has the potential to derail a Big Data project even if one was able successfully evade other risks. As they say ?€?A chain is as strong as its weakest link?€?. So what are the weak links in a Big data project? What are the 5 key questions to ask before embarking on a big data project so that one is able to mitigate the risks from these weak links and steer it towards a successful implementation. Based on real life experiences from the trenches we have outlined out top 5 Big Data questions which we feel are extremely vital to pose upfront before spending dollars on a big data project,We have seen the importance of managing weak links in a Big Data implementation by asking 5 important questions. ,1. ?€?Dent test?€? : What is $ denting use case using the big data stack?,2. ?€?Intersect?€? test : Which event data streams are we finding value in?,3. ?€?Tool Components?€? test : Which Big data Components are required and when?,4. ?€?Chunk?€? test : Are we delivering a high impact business output in 60-90 days?,5. ?€?Coexistence?€? : How do we co-exist with existing DW/BI solutions in place?,Each of these 5 questions are ellaborated in detail at 
In most cases with Big Data projects you have to ingest 6-10x more data than has any immediate value just to start and glean value. This creates a large volume of data with no apparent value, and sometimes it is a long time before there is any value if ever. This creates a large expense and a lot of issues for entry, which has derailed ??a number of??big data projects. So be prepared going in that unless you really know your data you will need to ingest and explore to find the value. When you do the numbers for your project keep this in mind.??


In this article we will discuss about using??,??as replacement of??,??(Hadoop Distributed File System) on??,??(Amazon Web Services), and also about what is the need of using S3.??Before coming to original use-case and performance of S3 with??,??let?€?s understand??,??and??,To read complete article please visit:
There is no question that the USA (in fact, most of the world) would be well-served with more quantitatively capable people to work in business and government. However, the current hysteria over the shortage of data scientists is overblown. To illustrate why, I am going to use an example from air travel.,On a recent trip from Santa Fe, NM to Phoenix, AZ, I tracked the various times:,??,??,As you can see, the actual flying time of 60 minutes represents only 19% of the travel time.?? Because everything but the actual flight time is more or less constant for any domestic trip (disregarding common delays, connections and cancellations which would skew this analysis even farther), this low percentage of time in the air is a reality. For example, if the flight took 2 hours and fifteen minutes, it would still work out to 135/386 = 35%. The most recent data I have, from 2005, shows the average non stop distance flown per departure was 607 miles, so we can add about 25 minutes to the first calculation and arrive at 85/336 =?? 25%.,Keep in mind, again, these calculations do not account for late departures/arrivals, cancelled and re-booked flights, connections, flight attendants and pilots having nervous breakdowns, etc. It?€?s safe to say that at most 25% of your travel time is spent in the air. Just for fun, let?€?s see how this would work out if we could take the (unfortunately retired) Concorde.?? We would reduce our travel time by flying at Mach 2.5 by 40 minutes, trimming out journey from five hours and eleven minutes to four hours and 31 minutes, about a 13% improvement.,What?€?s the point of all of this and what does it have to do with the so-called data scientist shortage?,Based on our research at Constellation Research, we find that analysts that work with Hadoop or other big data technologies spend a significant amount of time NOT requiring any knowledge of advanced quantitative methods ?€? configuring and maintaining clusters, writing programs to gather, move, cleanse and otherwise organize data for analysis and many other common tasks in data analysis. In fact, even those who employ advanced quantitative techniques spend from 50-80% of their time gathering, cleansing and preparing data. This percentage has not budged in decades. Keep in mind that advanced analytics is not a new phenomenon; what is new is the volume (to some extent) and variety of the source data with new techniques to deal with it, especially, but not limited to, Hadoop.,The interest in analytics has risen dramatically in the past two or three years, ??that is not in dispute. But the adoption of enterprise-scale analytics with big data is not guaranteed in most organizations beyond some isolated areas of expertise. Most of the activity is in predictable (commercial) industries ?€? net-based businesses, financial services, and telecommunications, for example, but these businesses have employed very large-scale analytics, at the bleeding edge of technology for decades.?? For most organizations, analytics will be provided by embedded algorithms in applications not developed in-house and third-party vendors of tools and services and consultants.,The good news is that 80% of the expertise you need for big data is readily available. The balance can be sourced and developed. ???€?The cr??me-de-la-cr??me of data scientists will fill roles in academia, technology vendors, Wall Street, research and government.,There are related and unrelated disciplines that are all combined under the term analytics. There is advanced analytics, descriptive analytics, predictive analytics and business analytics, all defined in a pretty murky way. It cries out for some precision. Here is how I characterize the many types of analytics by the quantitative techniques used and the level of skill of the practitioners who use these techniques.,???€?Data Scientist?€? is a relatively new title for quantitatively adept people with accompanying business skills. The ability to formulate and apply tools to classification, prediction and even optimization, coupled with fairly deep understanding of the business itself, is clearly in the realm of Type II efforts. However, it seems pretty likely that most so-called data scientists will lean more towards the quantitative and data-oriented subjects than business planning and strategy. The reason for this is that the term data scientist emerged from those businesses like Google or Facebook where the data is the business; so understanding the data is equivalent to understanding the business. This is clearly not the case for most organizations. We see very few Type II data scientists with the in-depth knowledge of the whole business as, say, actuaries in the insurance business, whose extensive training should be a model for the newly designated data scientists (see my blogs??,??and??
Medicaid is a USA government run healthcare program for the poor, elderly and disabled, which is jointly funded by the states and the federal government
?€?What is the sales value in this month? What is the increased percentage compared with the previous month? Which sales persons are among the top 10??€? ?€? The similar data analysis is mostly required in the sectors like commerce, finance, government, and scientific research. SQL usually is the only choice for such kind of analysis.,As a query language initially developed in early 70s, SQL has a relatively higher technical requirement on analysts. Lacking of advanced IT professionals is the root cause that hinders the data analysis from going any further.,Here is a data analysis script of the new generation. It not only provides all features of SQL, but also requires lower technical requirements. Even those developers without SQL expertise can use esProc to analyze the data by themselves.,A certain insurance company requires the following data analysis: How many newly-added polices this year? Of which how many policies are of the ?€?life insurance?€? ? How many policies valued above 500 thousand dollars?,The data are mainly stored in the insurance table of database, and the main fields are policyID, amount, time, type, and other fields. Some data are given below:,Of which ?€?policyID?€? is the serial number of policy, ?€?amount?€? is the policy amount, ?€?time?€? is the date of signing the policy, ?€?type?€? is the type of policy, and ?€?L?€? represents the ?€?life insurance?€?.,Let?€?s check the SQL solution below.,If using SQL to solve this type of problem, three SQL statements are required as given below:,For the script solution, the computation is carried out in the grid. Its user can just put a simple expression in each Grid. Grids can make reference to each other with the Grid name, and the analysis procedure is that the observing along with the thinking. The operation habit is almost the same to that of Excel??.,This problem is of the progressive style, and the later problem is the further explore on the previous one. The three problems are related mutually. As for esProc, the esProc user only need to process a bit based on the results of previous computation according to the business description.,SQL users are hard to perform the step by step computation and the previous computation cannot be referenced. Therefore, each problem is a new problem to SQL user. With the further explore on the problem, the statement will become more and more complex, and finally the SQL user has to resort to the technical specialist to solve this problem. For example, based on the problem 3, seek the monthly increased percentage of the policies valued high.,SQL style is like this:, , select amount, month,, ???????????? (case when, ?????????????????????????? prev_amount !=0 then (amount)/prev_amount, ?????????????????????????? else 0, ?????????????? end) compValue, from ( select month, amount,, ???????????????????????? lag(amount,1,0) over(order by month) prev_amount, ?????????????? from (, ??????????????????????????select month,sum(amount)amount from(, ???????????????????????? SELECT amount,to_char(time,?€?MM?€?) month, ???????????????????????? FROM insurance, ???????????????????????? WHERE to_char (time,'yyyy') = to_char (sysdate,'yyyy') and type=?€?L?€? and amount>500000, ???????????????????????? ) group by month order by month, ?????????????????? ), ),Not understand? Never mind. We can use esProc to implement the same computation:,A4: Group the results from the previous step by month., ??????????????A5: Sum the data of every month., ??????????????A6: Seek the increased percentage of this month, compared with the previous month. The ?€?~?€? represents every month in A5, and ?€?~[-1]?€? represents the previous month.,It is obvious that the technical requirements of esProc remain the same when exploring the problem. Even the business man or nontechnical person can always grasp the usage of such evident, easy-to-understand, and Excel??-like expression. On the contrary, in this case, the technical requirement of SQL statement will experience the exponential increase and soon reach the bottleneck of technical capability. Therefore, regarding the age-old and complex analysis tool of SQL, the analyst is in a complex that they both love and hate the SQL, and feels hard to grasp it.,Analysts can analyze data without SQL expertise and experience, and easily solve the tough problems that the SQL programmer encountered, thanks to the great features of script solution.,??
Like many industries the Infrastructure/Security/Compliance function within large telecom companies is becoming more data driven. Here are 3 powerful use cases which vividly bring out new possibilities in Telecom big data,How can??Surge in Contact centre keyword frequency be used as a lead indicator to infrastructure bottlenecks and be used as an input for throttling network traffic ?,How can we fire 'needle in a haystack' queries to isolate??collocation??events from the massive call detail records ( CDR) data ocean using Hadoop and columnar architectures ?, , ,How can we get a 360 degree view of an intrusion from the patterns of event data across devices ?,A central event log repository streaming thousands of events per second across firewalls, IDS, routers, switches etc is the need of the hour. Please find a detailed ellaboration of these 3 gaming changing Big data use cases at??
??

 ,After attending several analytics conferences over the last month, I?€?m beginning to understand an important nuance about the community we call ?€?analytics?€? worked by ?€?analytics professionals?€? or ?€?data scientists.?€??? It seems as if the defining boundary of our discipline is almost always that we data scientists apply ourselves to business, organizational, and market data.,The important nuance??? Businesses, organizations, and markets all involve interactions between people.?? Always.,Several other domains use very similar computational techniques to look at purely physical things ?€? the hard sciences and engineering.?? As an example, astrophysicists or metallurgists may use the same statistical programs as data scientists, but their world is very different.?? Their data does not involve humans.?? For example, the electrical lifespan of a battery doesn?€?t vary with human sentiments, though sometimes it may seem that way.,Since a data scientist?€?s work is typically in the service of learning about, bringing value to, and bringing change to an organization, we have to deal with people.?? It?€?s not about the size of our datasets ?€? compare your data to Computational Fluid Dynamics data someday ?€? but it?€?s that we are looking at these sometimes fickle, non-linear, yet often-predictable critters called employees or buyers or sellers.,Finance, in particular, is famous for ?€?physics envy,?€? leading to very mathematical, yet sometimes fatally flawed models of market and ultimately human behavior.?? In the Analytics business, no matter how many physics Ph.D.?€?s we hire, our analytics professionals often only get one pass at the data ?€? we can?€?t repeat experiments as if we are Edison looking for a light bulb filament.,Just because our ultimate subject matter (people) maybe influenced by Madonna one decade and Lady Gaga the next, does not make them impossible to model, analyze, and even predict.?? And since only people do the work and the buying, this analysis is very valuable with even small correlations.,Maybe this seems obvious, but I think it can sometimes be easy to fall into thinking about the ?€?market?€? or ?€?transactions?€? or ?€?attrition?€? or ?€?performance?€? in a more mechanistic way that forgets about the involvement of people making a Data Scientist?€?s work far more complicated than predicting the airflow over a wing.,The above nuance feels like an important one, to learn and to pass along as it highlights the unique, powerful and human side of our work.?? This concept may be lost in the seeming trivia of scanning social media text, but in fact the closer to humanity we are, the closer we are to being Data Scientists.


 

??,Two weeks ago I attended the Customer Insight & Analytics Exchange in London, in which I participated as a speaker. See Maz Iqbal's account of these two days here:,One of my personal highlights??from day 1 was Suresh Vittal's??presentation (Forrester). He distuingished three stages of maturity towards customer-centricity in today's organisations: (1) Functional intelligence (2) Marketing Intelligence and (3) Strategic intelligence.,According to his research the first group accounts for the largest part (54%). Silo's are common in these organisations and data-based insights are delivered on an??ad-hoc basis. About a third of organisations?? fall in the second category (34%). They??are characterized??by bringing structured and unstructured information together. Qualitative and quantitative data are used jointly in some analyses and decisions, but not in a recurrent or automated fashion. Finally, only??12% of the organisation are customer-focused,??learning organisations that actively??manage interactions across all touchpoints.,Interesting, but not surprising, about the last category is that these organisations are more technology-driven, and have found a way to ,.??,And this is why, according to me, BI is hot again in Marketing.,Enabling?? a broad user group to explore customer behavior, customer satisfaction??and??the outcome of marketing??interactions??by means of visually intuitive and interactive tools is a necessary condition to challenge the status quo and??stimulate innovation in larger companies. Add , and a , on top,??and??an important??milestone agile marketing is taken.,It's clear but not easy - as Maz describes on his blog:,??,Couldn't agree more.,??,??,??,??,??
A Big Data decision support system requires particular capabilities in terms of volume, variety of data and processing speed. ,Today companies to improve their knowledge models and forecasts, do not hesitate to take into account hundreds of factors, and do not hesitate to bring up new means of analysis that can handle large volumes of data. But the processing of large volumes of data is a challenge for traditional BI infrastructure. Storing large volumes is not a problem, but requires the use of massively parallel architectures, such as those offered by Teradata for example, or "MapReduce solutions" such as Hadoop or Aster Data. Here the choice of solution depends on the variety of data types to be treated and the expected velocity. MapReduce is indeed better than a relational database to handle unstructured data, and if Hadoop is then batch Aster Data is real time. Since there is no silver bullet solution, big companies bring a mix of resources to allow them to enjoy the benefits of different types of solutions.,From the moment you want to take into account all kinds of data, text, data from various sensors, geolocation data, data from social networks, pictures, etc. ..., these data do not present in a perfectly ordered and are not immediately ready for analytical use. Even the data from the web are not perfect from the start. Big Data systems common task is to support unstructured or multi-structured data, and process them to make them consumable by humans or analytical applications. A classic example in word processing is to determine what a word refers: Is Paris the capital of France? Is Paris city in Illinois? Is Paris the famous people? Etc. With Big Data, you have also to find the best way to store data, and relational databases are not always the most successful way possible, in particular for XML data or networks (graphs) data for example. Even where there is not an incompatible data type, a drawback of the relational database is the static nature of his patterns, and you should prefer Semistructured NoSQL databases, which provide enough structure to organize data, but do not require an exact pattern of the data before storing.,Speed requirements for data processing in recent years have increased similarly as volumes. This no longer concerns only a few specialized companies such as financial operators (traders), but most key economic sectors. In the era of mobile Internet the pace of business has accelerated, we consume differently, the forms of competition have changed and information flows as well. For example online retailers are able to track clicks for each client, from their first interaction to final sale. Those who are able to quickly use this information, recommending additional purchases for example, acquire a distinct competitive advantage.,The challenge is not only to take in charge the volume of incoming data, but especially speed analysis and relevant trigger actions. The freshness of the information delivered is paramount. For example: did you will walk across a street without looking, relying only on a view of traffic taken five minutes before? The speed feedback is a source of competitive advantage, especially for all web activities.,??,Faced with such needs, usual decision support technologies are enabled to support the rhythm, and only a mix of solutions enables businesses to meet expectations. Thus enterprises like eBay or LinkedIn for example, use a mix of traditional DBMS and new NoSQL solutions.

Latest KDnuggets Poll asked,On average, KDnuggets readers used 2.5 languages, with R, Python, and SQL being most popular ones, with highest growth in Lisp/Clojure(*), Python, and Unix tools. R is now used by over 50% of data miners. However, Hadoop-based languages were used by only about 7% of voters.,Comparing with,, the languages with the highest growth were,Most popular language used along with R was Python (and vice versa).,Here are the results:


Two new research reports on big data flash out its early impact on enterprise IT.,David Newman, Research VP at Gartner and a member of its Enterprise Architecture team, warns in , that ?€?Big data will disrupt your business. Your actions will determine whether these disruptions are positive or negative.?€? Newman sees three areas where big data has the biggest impact and three corresponding strategies:,Given Newman?€?s enterprise architecture expertise it is particularly interesting to see what he has to say about what he thinks is the ?€?significant impact?€? of big data on the discipline of enterprise architecture (EA): ?€?For the EA practitioner, the balance shifts from traditional methods that focus on optimization, standardization and efficiency, to lightweight approaches that focus on harmonization, externalization and growth. Big data also overturns traditional information architectures ?€? from a prior focus on data warehousing (data storage and data compression) toward data pooling (flows, links and shareability).?€?,What Newman doesn?€?t say but I think is a fair conclusion from his discussion of EA and big data, is that external data sources play a significant role in enterprises?€? big data strategies and will radically change the concept of ?€?enterprise architecture.?€? With big data, the ?€?architecture?€? covers much more than the enterprise, encompassing all the relevant data flows, links, industry-wide shared data, and sources of public data.?? A great example, mentioned in the Gartner report, is the ,, ?€?a precompetitive information commons?€? formed by companies in the pharmaceutical industry.?? Based on such efforts, ?€?Gartner contends that enterprises will cooperate to co-create big data pools, then compete to extract value from them.?€? Indeed, that?€?s what big data is all about.,The other new report is the 1st ,, focusing on ?€?understanding the business benefits and strategic implications of big data.?€? It is based on a survey of 300 UK-based ?€?senior decision-makers?€? and its key findings provide a glimpse into the current state of big data:,The last finding in particular is in stark contrast to the general consensus regarding the dearth of big data skills and Gartner?€?s advice to ?€?produce resource planning deliverables to identify gaps for interdisciplinary roles (such as data scientists) and senior-level positions (such as chief data officers).?€?,Even more interesting, ?€?a significant proportion (31%) appear confident they can execute a big data strategy with their existing staff.?€? The Big Data Insight report suggests that this is because people are doing ?€?tech-led pilots and that IT people are the ones who are experimenting with the technologies. As such many are unaware if they will require new personnel for a business focused strategy.?€?,But big data is about better business decisions and will require not only hard-to-find data scientists and creation of new roles such as Chief Data Officer, but a new business mindset led by business executives with a fundamental grounding in and experience with big data analytics. The Big Data Insight report makes a great point about where the emphasis in any enterprise?€?s strategy should lie: ?€?What shines out from these findings is that big data is not simply about new tools and technologies to deal with increasing amounts of data. It is about taking an intelligent approach to using that data to answer clear, predefined, business orientated objectives from which an organization can reap the rewards.?€?


??Founded in 2010, Locu's mission is to structure the world's information, and it is currently focused on unstructured local data. Locu combines document analysis, ??crowdsourcing, ??and machine learning to provide??structured and semantically annotated data sets made available through a set of APIs.??Locu?€?s first product ??helps restaurants better manage their online presence.??,Rene Reinsberg,??Founder & CEO;??Marek Olszewski,??Founder & CTO;??Stelios Sidiroglou-Douskos,??Founder & Chief Scientist;??Marc Piette,??Founder & COO,????Factual founder??Gil??Elbaz, Google?€?s??Bruno Bowden, Cloudera's??Jeff Hammerbacher, Hubspot?€?s??Dharmesh Shah, Twitter's??Michael Abbott, Facebook's??Andrew McCollum,??MIT?€?s??Ed Roberts,??Jean Hammond,??Matt Ocko;??Mitch Roberts; Quotidian Ventures;??General Catalyst Partners;??Chris Sacca?€?s??Lowercase Capital; Lightbank, SV Angel;??Naval Ravikant;??and??Babak Nivi.,??Rene Reinsberg?€?Local for us was an obvious [choice to focus on first] because of the rise of smartphones. People want to have a lot more actionable data at their fingertips and in the apps that they?€?re using.?€?,Founded in 2011,,Essess is a SaaS-based solutions provider addressing commercial and residential??energy efficiency. It??collects and analyzes building energy efficiency performance information on all individual buildings across large geographies, synthesizes the analysis into easy to understand metrics and scores, and automatically generates reports, providing??building owners with tangible and actionable ways to reduce their energy bill.,Storm Duncan,??CEO;??Long Phan,??President & VP/Engineering;??Dr. Jonathan Jesneck,??Chief Scientist,Vocap Ventures is the largest institutional investor in this round of funding, with additional investment from DFJ Athena, Minder Cheng, Richard Tsai and the company?€?s CEO, Storm Duncan,Essess CEO Storm Duncan sees 'big data' as the great enabler to encourage energy efficiency for businesses and homeowners. 'We can capture and zero-in on energy use for buildings down to individual windows and help building owners increase efficiency and reduce costs,' says Duncan.",??Founded in 2011, Coursera offers high quality courses from the top universities for free to everyone. In addition, it provides an education platform at scale, based on machine learning and crowdsourcing to enable mass peer grading and efficient class forums.,Daphne Koller, Co-Founder and Professor of Computer Science, ??Stanford;??Andrew Ng, Co-Founder and??Associate Professor of Computer Science, Stanford.,Kleiner Perkins Caufield & Byers; New Enterprise Associates.,Daphne Koller: "We see a future where world-renowned universities serve millions instead of thousands";??Andrew Ng:??"Our mission is to teach the world and make higher education available for everyone."
 , 


For the very smart data science geek who wants to avoid all kinds of social interactions (that is: having no boss, no employee, no colleague, no client, no customer, no contractors, no interaction with vendors etc.), there are a few options. All of them allows you to work from home. Some (when automated) allow you to not work at all. They can generate good complimentary income or (if scalable) great income.,In practice, these opportunities are not real jobs but rather entrepreneurial initiatives that leverage the most sohisticated data science techniques, and they bring a complement of revenue rather than a full income. Also, these "occupations" are usually not fully automated (when they are - and sometimes they indeed are, you are litterally making money when sleeping). Only top talent with strong business acumen is able to succeed due to massive competition.,: Are men more likely than women to be interested in these "no human interaction" types of activities?


Many data set resources have been published on DSC, both big and little data. Some associated with our data science apprenticeship. A list ,. Below is a repository published on Github, ,.??

This was a great question posted on Quora.com, and attracted ,. Here we summarize the must interesting contributions for you.
Most of us would probably like to work in a profession recognised for its legality, decency and honesty. At least I hope so. In my line of work, what we have right now is palpable evidence that the IT industry lacks a reliable moral compass.,Imagine this. A major sensationalist tabloid pulls together a team of diverse journalists who are set to work on a national campaign to promote very high usage of sunbeds as a cure for cancer. Why? The newspaper owner's son owns the sunbed franchise.,The health experts criticise the publisher for being irresponsible, unprofessional and lacking in scruples.,The public is mainly undecided, but many take the story on face value and adopt the fad. The intensive use of sunbeds sharply increases. Elsewhere, in unrelated news, the cases of skin cancer show a marked increase. Some blame it on EU legislation for bangers and bananas.,In spite of protests, the press campaign continues over many months.,Eventually, and based on the evidence of recognised health experts and bodies, the press regulatory association tries to get the offending publisher to temper their claims, but without any success. It is only when the government's lawyers step in and threaten the newspaper owners with legal proceedings, do they freeze their campaign. Much later, the editor resigns and the board of directors issue a short apology on the back pages of their much vaunted organ.,We have that in IT. Our current sunbed cure for cancer, if you believe those who are 'bigging it up', is undoubtedly Big Data.,I occasionally post content to Linkedin, some of it (maybe even this piece) gets promoted through the Pulse??,??channel. There are some reasonable pieces pinned to that channel, but unfortunately, for much of the time what we get is total and moronic Big Data??,. Tantamount to the equivalent of Big Data's very own??,??campaign.,The Linkedin??,??channel reflects life, and it is full of self-aggrandising and shameless marketing guff, shot-through with scandalously flimsy promotions of tendentious success stories, specious claims of value, half-truths about realisable benefits and embarrassing conjecture about the importance of social media and internet logs.,What I am referring to mainly are superficially neutral (yet virally toxic) pieces placed in the public domain in order to promote Big Data at any cost.,Now let's step back a bit.,For over 125 years, the??,??(FT) has built up a solid professional reputation for accurate reporting, reliable journalism and informative editorials. The FT is a newspaper trusted by its discerning readership and admired everywhere. In fact, I could not imagine their journalists writing about markets, securities and financial houses the same way that pundits elsewhere write about Big Data, Dark Data and the Internet of Things. Because the FT knows, that maintaining the trust of their readership is far more important than winning the short-term favours of a few market players.,So consider this; if we in IT cannot bring our standards of communicating with the public up to the levels of the financial industry, at minimum, you know what that means don't you?,Exactly. The IT industry will have a far worse public image problem than the bankers and the solicitors currently have, and we all understand the general public appreciation of those professions.,Now, call me old fashioned, but for me that possibility is worthy of serious consideration, and especially by those in IT who confuse no holds barred pimping of fads, trends and technology, in which truth, decency and honesty are optional, for ethical, candid and informative analysis and reporting of the industry.,How will the industry take these criticisms?,To go back to the sunbed analogy what we will most certainly get comments in this vein:,Whilst those who rail against 'the cancer curing advantages of sunbed use' may be right - or at least partially right - the sunbed revolution will continue, just as the IT revolution industry has done, and in spite of people saying that the age of computing would be a passing mania.,So, when someone tells you ?€?intensive sunbed use is just a dangerous fad?€?, what they actually mean to say is that we don?€?t need the term any more, as intensive sunbed use is here to stay, as are those who are shrewd, unprincipled and cynical enough to cash-in on the public's gullibility and wilful stupidity when it comes to fads.,Yes, it does get that bad.,We have people who seemingly spend all their waking lives working out not-so-original ways and means of riddling the IT industry with vacuous bullshit, and what Big Data promotion has shown us clearly is that what we have palpable and comprehensive evidence that the IT industry in general lacks a moral compass.,Is that a reflection of IT, of those who create and manipulate IT fads, or of society in general?
It?€?s happened to all of us sooner or later: The hypothesis seemed plausible, the data was clean, the conclusions sound. Our recommendation was damn near foolproof. Yet when put into practice, the result was anything but favorable.,How could that happen? Data science is about, well, DATA. And science, which implies a reliable method. We have the information, we have the models, we aren?€?t just shooting in the dark here. Where did it go wrong?,The truth is there are lots of things that can go wrong, and when attempting to determine root cause for why this happens, let us begin with ourselves.,You as a person have mental characteristics that influence how you work with and interpret data of any form, regardless of whether it comes from your senses, a database, or the written page. Some of these are common to our race, some are formed through our life experiences. Psychologists call these cognitive biases, and they are present in varying degrees to every person on the planet.,As data scientists it is tempting to think that we are objective and impartial. That would be nice if it were true, but unfortunately there are certain cognitive biases we must be continually mindful of at various stages in our process??or risk the corruption of our results.,Below are but a handful of the more prevalent (and relevant) forms??that are particularly relevant to us:,But the granddaddy of them all (in my opinion) is one called ?€?Bias Blind Spot,?€? in which you can readily identify the cognitive biases in other people but believe you have significantly fewer than those people. I?€?ll leave it to your imagination to picture the chaos that can cause.
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
??,irst we must map each document in the students table to a number one(to denote the count of a single student) and pass the sequence of 1s to a reduce to get the number of students in a class, , , 
Retailers know they need Big Data and are charging forward to get in the game.?? But many retailers continue to face challenges. What type of data should be collected? How should the data be used to generate insights? How do I measure ROI?,101data recently surveyed US retailers, across a range of sizes. When asked about which processes would be most impacted by Big Data technology, 50% stated that targeted offers and promotions would have the biggest impact. Responses included:,However, while retailers see the potential, many are holding back on implementing Big Data inititiatives. The biggest reasons cited were that retailers need to better understand how Big Data can solve their business problems (46%) and the cost of complexity of implementing Big Data solutions needs to come down (42%).,Nevertheless, retailers still believe Big Data is important for staying competitive. Most of the respondents categorized Big Data initiatives as ?€?important?€? (38%), followed by ?€?very important?€? (35%), or ?€?moderately important?€? (23%), with just 4% stating that Big Data initiatives are of little importance or unimportant.,Big Data is not an ?€?out of the box?€? solution. It involves some creativity and an understanding of the business challenges you want to solve.,Data-as-a-Service (DaaS) is a big data solution that is increasingly being used by retailers in marketing to harness Big Data insights ?€? and without the heavy investment in a Big Data implementation. DaaS is a service approach in which a vendor sources unique and Hard-to-Find data from the Big Data ecosystem. This data is then structured to deliver a constant stream of in-market shoppers who are actively searching for products a retailer (or their competitors) are selling. Retailers can target shoppers with immediate, relevant offers, such as dynamic pricing, personalized recommendations, shopper-specific discounts and more.,DaaS combines three types of data which are uniquely customized to each company:,Internal data combined with additional demographic and firmographic enhancement and specialty data. These specialty data sets have been aggregated from hundreds of Big Data sources and go well beyond third party lists.?? As an example, these may be highly specialized sources of furniture or fashion interests, or spend data on specific businesses by categories.,Offline data transformed into addressable online identities. Onboarding provides new opportunities to reach customers and prospects in the digital universe. For example, targeted display campaigns can be displayed to specific customer and prospect segments. A specialty retailer company may want to target key customer groups with display ads that cross-sell another product. Or an auto dealer may show ads to people whose leases are up for renewal.,Real-time behavioral data. Fast Data aggregates event and behavioral-driven data to determine purchase intent as it occurs. Examples may include social purchase signals, such as People posting to social networks such as ?€?Excited about the new baby?€? or ?€?Taking a family vacation.?€? Or these may be discretionary purchase power signals, such as customers and prospects who are securing new credit sources, selling and buying cars or planning to move residences.,When retailers have access to key data insights (like in the above image), they become especially equipped to send targeted offers to prospects or customers. Surprisingly enough, this data can easily be integrated into a retailer?€?s CRM system, marketing platform, or other data management platforms. DaaS makes big data accessible! (And easy to use.) Retailers can harvest data that is relevant and important to them, such as purchasing signals on social media networks, like Twitter, based on keyword searches, like ?€?Furniture?€?.,DaaS doesn?€?t stop there. There are endless opportunities for retailers to receive data from almost any source. This could include social media, but in many cases, it stretches far beyond social media unto web searches, public records and online or offline transactional activity (to name a few). Big data is the accumulation of opportunities brought to you by data, and DaaS is the key to these big data opportunities.
Creating interactive visualizations of your data for web is a cakewalk using silk.co, all you need to do is to import your spreadsheet and start generating your interactive visualizations, , , 
We are living in a world dominated by data. Data overload and fatigue is drowning most corporations and individuals. There was a time when we had an opinion and we would speak our mind to our friends and families. Now we instantaneously write our minds on Facebook, Instagram, Twitter and LinkedIn. Everyone has a strong opinion and we don't hesitate to take our opinions public. In the last five years, we have generated as much as data as we have never done before. As a company, I want to make sense out of that nonsense and look for key value pair attributes to understand the hidden message lying beneath all that noise. Big Data plays an important part in mining such unstructured data. The Internet of Things is going to further aggravate the situation by adding many more data points.?? It?€?s the wild wild west for data practitioners. Data governance therefore becomes a critical skill for individuals and organizations alike in the 21st century,??,Data governance is process of owning a piece of data and running it through the organization without losing its value. At every step of the way in the food chain, this piece of data can be enhanced or modified. Data governance is sum total of all process, policies and technology that organizations use to store data in whatever native format they generate??it in, process it, morph it into any form that an user needs, protect that data as a custodian and eventually, maintain shelf life.,Data governance has several key components:,A data audit within the company routinely throws up the idiosyncrasies of the data at a business or department level. If I am sales & marketing, I have several data points that I might routinely discover from my campaigns/regions/customers etc. Some of these are blind spots that were previously not known and there is a definitive need for it now. For eg, within a community school context, with the amount of aid tied to student performance, I now need to fill in this blind spot with data that was previously not captured. Student performance indicators is a growing field and many schools and colleges are looking at their data discovery processes to fill in this gap. Likewise after the collapse of companies like Enron, WorldCom and others, Sarbanes Oxley Act required companies to report on certain data points from 2002 onward. In the last 13 years, companies have become adept at reporting such data.,Data discovery is an ongoing process that doesn't stop. The industry that the company is in defines the extent and the velocity with which the discovery has to happen. For an industry that is perennially under pressure (such as telecom and retail), the need to be abreast of that data is very significant. For others the frequency of data collection isn't that often. Concepts like customer churn rate and sales/sq foot of retail space have become common place now.,Every company needs data evangelist(s) /custodian(s)/steward(s) in each of its departments. These folks are responsible for owning the data pieces that belong to them. They define all the touch points they have with their customers (internal and external) and create a data dictionary and metadata for their departments. ??It is upon them to define the longitudinal data sets for their business units. They also keep the currency of the data and define periods at which the audit needs to happen.,Once we have defined the metadata and dictionaries of various different types of data, we need to either produce that data or mold existing data stores into a format that can be universally accepted by the organization. ??Architecture is much easier now and so is modeling. All touch points- one department to another department and department to customer are now obvious. The data network can be visualized easily.?? By accepting the data definitions and the metadata, we now create data repositories that can accommodate our user needs.?? In this stage, we are setting the stage to do all the plumbing at the back-end and accept the conceptual model we created in the discovery phase and create framework for Master Data Management. The questions that are posed to us here are,MDM is a systematic approach to manage all the data put into production. The departments that we talked about could very well have their own data stores, data dictionaries and metadata. MDM is the integration of all these data stores if they exist. If these data stores don?€?t exist, the MDM sets up the framework to manage all of these different data types in a way that makes most sense to each of the businesses. Think of the MDM as your airport- flights come in and go out every sometime, yet there is no chaos. The air traffic control language is known to everyone, the landing and take-off strips are known to everyone, the baggage handling is also known. Similarly within the MDM framework, many businesses can send their data to the airport (MDM repository). The repository knows how to handle that data, where to send it and give appropriate commands.,Every company has process assets. Depending on the maturity level of the organization with respect to processes, the OPA can be anywhere from weak to strong. A progressive company with strategic goals will invest in OPA. OPA?€?s are like the neural networks of the organization. ??Much like how neural networks emit signals, processes emits data. If the process is absent, so is the data. Invest in the process. Define how you will interact with your customer for sales and for service- using multi-channel communication model. Define for each function ( sales and service) how many data points you want captured. For sales for eg you want to define processes for funnel management from prospect to suspect to customer. For service, you might want to define SLA?€?s (service level agreements) and measure the process around that.?? Many companies have the processes but do not have a way to look at the byproduct of that process- the data. From my experience of running IT shops/ Data departments, I have seen first-hand, companies discard their daily work. Everyone in the company works religiously everyday but lot of that work is discarded as garbage. Not enough is captured. For eg, you may have a process to collect data around funnel management from suspect to prospect to customer but you don?€?t have a reliable system to collect that data and measure it. After a while, the pain sets in and garbage recyclers like me are called in to fix the problem. We collect the garbage, recycle it and mine it. ??Before we get to it, we have to set the foundation to set policies, procedures and the infrastructure. No build out is ever initiated without a solid foundation. Your process is invaluable. It tells you why you are succeeding or why you are not succeeding. Set up Data stewards and give them enough authority to map the neural network within the department /business unit and allow for free flow of data.,Data governance is not for the faint of the heart. Specially, in these times, when the data is in constant flux because of the competitive pressures. Set up a war room to deal with the exponential growth of data, the velocity and the volume.??Hire a Chief Information??Officer/Chief Data Officer and give him /her the same latitude as others in your organization.??Set up pilots with couple of departments. Build those small neural networks and let the data flow through those networks. Speak the data language. Set up data posters all over the offices. It?€?s a culture that is contagious and eventually data will persist.??
Data mining tools are used to create models that predict customer behavior by using historical data. These methods can be applied to answer questions like:,Below we present examples of challenges which can be solved by applying Data Mining methods.,In the next part we will show, what types of analyses are used in estimating the customer value over time, in selecting best parameters for the given product or service and in effective sales forecasting.,You can follow us at 
 ,When Jeff our architect first ran the benchmark I could not believe it! I was sitting in front of the terminal screen trying to take in what I had just seen. ?€?Jeff is this correct?!?€? I asked. ?€?Yep!?€? he said grinning. I had patiently waited , (close to 10 minutes) for MySQL to execute a query and then watched as BigObject cranked out the same query in 0.1 seconds! That?€?s a , boost in performance! ,!,I know, it needs some amazing proof right? Check out the screenshot:,?€?,We wanted to demonstrate the sheer power of BigObject and Pre-Cache technologies but needed a compelling benchmark. We knew that we could easily increase database performance by a factor of 100x but could we go higher? Could we achieve 1,000x? We love a challenge so we got to work!,This experiment was conducted on 1 PC with the following specifications:,Intel Core i5?€?4200H CPU @ 2.80GHz,Dual Core up to 4 threads,8 GB RAM,The data set we worked with:,Sales: 1,000,004 records,Product: 4,434 records,Customer: 10,000 records,Execution time: 588.86 seconds,Execution time: 0.1 second,Performance boost: 5,888.6x,We did not only achieve 1,000x we did it 5 times over! I would never have anticipated we could have achieved a 5,000x performance boost.,I would like to note that we worked with the MySQL tables not being optimized and when they were optimized there was still a 150x improvement on performance. That being said, a lot of databases out there are not optimized so you can expect a turbo-boost of up to 5,000x!,I want to re-emphasize that the MySQL table was NOT optimized. At first I did not think much about this and then it dawned on me, optimization does have a cost. With BigObject and the Pre-Cache layer the response was still 0.1 seconds regardless of the state of the database. This is HUGE because it means that no matter how optimized your database is, with BigObject you can instantly turbo-boost what you are doing with data!,Without BigObject, MySQL takes a long time to perform queries making real-time data analysis and feedback virtually impossible. With BigObject and the Pre-Cache layer, there is a dramatic turbo-boost that allows you to build amazing real-time data analysis and feedback functionality into your application. Just think of the possibilities!, is a positive futurist who has successfully created e-commerce, mobile and consulting startups.,He has consulted fortune 500 companies providing key direction in the utilization of future technologies to achieve market dominance.,As the chief evangelist at ,, Titus is driven to revolutionize data capture and analysis as well as help companies derive maximum value from their data.,He travels the world full-time with his wife and 4 children. He is presently RV?€?ing around the USA.
Broken down in eight categories.,1. Algorithms and Data Structures,2. Artificial Intelligence, 3. Computer Architecture and Engineering,4. Concurrency,5. Computer Security,6. Computer Network,7. Software Development Methodologies,8. Common Beginner?€?s Questions,??to read all definitions and answers.
Very interesting list of algorithm, data science, machine learning, and computer science keywords. To check the definition for any keyword, go to??,. For whatever reasons, and like in many similar lists, the top three letters have more entries than subsequent letters, as if the editor suddenly became lazy when hitting letter D (maybe product developers create products that start with letter A, B, or C, to show up at the top). Yet the winner is letter S. Lot's of stuff is missing, for instance "Naive Bayes", but all in all, this is a great list.,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z
 , , , , , , , , , , , 
 is proud to announce the launch of a computational , to predict the future progression of disease in Amyotrophic Lateral Sclerosis (ALS), also known as Lou Gehrig?€?s disease. The??, (or ALS Prediction Prize) is being run by , in collaboration with , (and the sponsorship of Nature, Popular Science and The Economist) and carries a $25,000 prize for the winning prediction.,??,??,The DREAM Phil Bowen ALS Prediction Prize4Life challenge aims to answer a puzzling and problematic question in ALS. It is well known that most ALS patients are like Lou Gehrig: their disease progresses aggressively, quickly leading to death within 2-3 years. However, a small number of patients turn out to be more like Stephen Hawking, where their disease progression is slow and patients can live for 10 or more years. What separates the Lou Gehrigs from the Stephen Hawkings? Solving this mystery is important, both for patients and their families and for those planning clinical trials of new treatments. Now we finally have the data that may hold the key. The only thing missing is you.,??,??The ALS Prediction Prize challenge will be run on the InnoCentive prize platform (,).,Use your computational expertise to win a Prize4Life,,??,??,??
Today Analytics is the heart of a Business. Companies are challenged with a high volume and broad array of data which requires active and effective analysis. Analysis can help them make enhanced and improved business decisions, and hence help the business to maintain profitability. "Companies need to compete on the basis of key business processes, and how they optimize these processes with analytics," says Thomas Davenport, professor and director of research, Babson College, USA. Business analytics means simply to resolve business issues by applying the appropriate analytical techniques on the available business data. However, a practical definition would be how a business arrives at an optimal or realistic decision based on existing data.,Data Analytics is the technology to leverage the business data in order to make keener decisions which can be helpful in solving operational and managerial problems in a well-organized and productive manner. Now-a-days, sectors such as telecom, retail, healthcare and finance are investing heavily in analytics to stay ahead of competition. For example, the Telecom Industry uses Analytics for Proactive Network Monitoring, Capacity Planning, Revenue Assurance, Subscriber Profiling, Social Network Modeling and Analysis, Personalized Advertising and Churn Management.,The Retail Industry uses analytics and Business Intelligence for Assortment Optimization and Shelf Space Allocation, Customer-Driven Marketing, Fraud Detection and Prevention, Integrated Forecasting, Localization and Clustering, Marketing Mix Modeling, Pricing Optimization, Product Recommendation, Supply Chain Analytics etc.,Analytics benefits all below listed aspects of an organizations value chain:,To learn business analytics please visit ,.

Posted on behalf of??,.,Hadoop (MapReduce where code is turned into map and reduce jobs, and Hadoop runs the jobs) is great at crunching data yet inefficient for analyzing data because each time you add, change or manipulate data you must stream over the entire dataset.,In most organizations, data is always growing, changing, and manipulated and therefore time to analyze data significantly increases.,As a result, to process large and diverse data sets, ad-hoc analytics or graph data structures, there must be better alternatives to Hadoop / MapReduce.,Google (architect of Hadoop / MapReduce) thought so and architected a better, faster data crunching ecosystem that includes Percolator, Dremel and Pregel. Google is one of the key innovative leaders for large scale architecture.,Percolator is a system for incrementally processing updates to a large data sets. By replacing a batch-based indexing system with an indexing system based on incremental processing using Percolator, you significantly speed up the process and reduce the time to analyze data.,Percolator?€?s architecture provides horizontal scalability and resilience. Percolator allows reducing the latency (time between page crawling and availability in the index) by a factor of 100. It allows simplifying the algorithm. The big advantage of Percolator is that the indexing time is now proportional to the size of the page to index and no more to the whole existing index size.,See:??,Dremel is for ad hoc analytics. Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data and allows analysts to scan over petabytes of data in seconds to answer queries. Dremel is capable of running aggregation queries over trillions of rows in seconds and thus is about 100 times faster than MapReduce.,Dremel's architecture is similar to Pig and Hive. Yet while Hive and Pig rely on MapReduce for query execution, Dremel uses a query execution engine based on aggregator trees.,See:??,Pregel is a system for large-scale graph processing and graph data analysis. Pregel is designed to execute graph algorithms faster and use simple code. It computes over large graphs much faster than alternatives, and the application programming interface is easy to use.,Pregel is architected for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.,See:??


The Hadoop stack includes more than a dozen components, or subprojects, that are complex to deploy and manage. Installation, configuration and production deployment at scale is challenging.,The main components include:,The range of applications that use Hadoop show the versatility of the MapReduce approach, and reviewing them provides some of the typical characteristics of problems suited to this approach:,Some good examples that display some or all of these characteristics include:,?€? Applications that boil lots of data down into ordered or aggregated results ?€? sorting, word and phrase counts, building inverted indices mapping phrases to documents, phrase searching among large document corpuses.,?€? Batch analyses fast enough to satisfy the needs of operational and reporting applications, such as web traffic statistics or product recommendation analysis.,?€? Iterative analysis using data mining and machine learning algorithms, such as association rule analysis or k-means clustering, link analysis, classification, Na??ve Bayes analysis.,?€? Statistical analysis and reduction, such as web log analysis, or data profiling,?€? Behavioral analyses such as click stream analysis, discovering content-distribution networks, viewing behavior of video audiences.,?€? Transformations and enhancements, such as auto-tagging social media, ETL processing, data standardization.

I'm very pleased to announce the formation of Data Community DC, Inc., a new organization dedicated to the support of the Data Science, Statistics, Analytics and related communities in the Washington, DC area! Formed by the organizers of the rapidly-growing Data Science DC and R Users DC Meetups, and the nascent Data Business DC Meetup, DC2 will support those groups and help to create new Meetup groups and other events and services.,If you live or work in the DC Metro area, we'd love to have you participate in our events and benefit from our services and other opportunities. For now, please stop by our web site and subscribe to the RSS feed, or subscribe to our Twitter feed:,Thanks!

Companies, products, and technologies included in the Big Data Landscape:,- Splunk, Loggly, SumoLogic,- Predictive Policing, BloomReach,- Gnip, Datasift, Space Curve, Inrix,- Oracle Hyperion, SAP BusinessObjects, Microsoft Business Intelligence,IBM Cognos, SAS, MicroStrategy, GoodData,- Tableau Software, Palantir, MetaMarkets, Teradata Aster, Visual.ly, KarmaSphere, EMC Greenplum, Platfora, ClearStory,- HortonWorks, Cloudera, MapR, Vertica,- Couchbase, Teradata, 10gen, Hadapt,- Amazon Web Services Elastic MapReduce, Infochimps, Microsoft Windows Azure,- Oracle, Microsoft SQL Server, MySQL, PostgreSQL, memsql,- Hadoop, MapReduce, Hbase, Cassandra,See:??
??,Companies are looking increasingly to take advantage of Big Data, especially textual information, those generated via user tools by web or desktop applications. The analysts specialized in this subject believe that 70% of information of interest to business are nestled in word documents, excel, email, etc. These data are not predefined in a model and cannot be perfectly stored in relational tables. They occur most often in the very free form, but contain dates, numbers, key words, facts that can be exploited.,A new challenge for companies in data analysis is to significantly advance the operation of this type of unstructured data. In terms of customer knowledge for example, it is possible to better use of the archives of business proposals and contracts, or listen to web conversations or take advantage of dialogues via email. Master relationships, including discussions about the company with its community of customers and stakeholders in its ecosystem, are very important for marketing today which has to change, push in that way by customers who largely use new technologies (mobile, social media).,The amount of this type of digital information usable is constantly growing, and as "manual extraction" of information is extremely difficult or even impossible on a large scale, so the use of specific computer tools for data processing unstructured text is required. Thus recently appears text mining tools, which automate the processing of large volumes of textual information, to identify statistically different topics raised and extract key information.,Text analytics techniques apply to the documents linguistic processing, including morphological, syntactic, semantic, and various other techniques of data analysis, statistical classification, etc. A major objective is to synthesize texts (classify, organize, summarize) by analyzing relationships, structures and rules of association among textual units (words, groups, phrases, documents). In the end it automates the production and the management of documents (including abstracts) and information (extraction, research, dissemination).,Text analytics has many applications for example in the field of customer relations, it allows in particular: exploring the contents of documents (e.g. open-ended questions in a survey, comments and complaints from customers, analysis of warranty claims) ; assign documents to predefined topics (redirect, mail filtering, organizing documents into categories, ranking contacts for call centers) ; compose text summaries (abstraction and condensation) ; examines texts by concepts, keywords, subjects, phrases to get results sorted by relevance like Google ; and finally increase the performance of predictive models by combining text and structured data.,To conclude, text analytics are a collection of technologies that detect the elements, or building blocks, within language, turning them into a type of data that can be manipulated and computed. To go further you can click on the link below to discover why it is necessary to use advanced analytical tools such as specialized Teradata Aster, to fully exploit unstructured data. Retail or Internet companies like Barnes & Noble or LinkedIn are already using these text analytics solutions in order to get a competitive advantage.,??
Copyright ?? SAS Institute Inc., Cary, NC, USA. All Rights Reserved. Used with permission.,Example: Given business priorities, resource constraints and available technology, determine the best way to optimize your IT platform to satisfy the needs of every user.

 , 

Data scientists are the new astronauts. Everyone wants to become one. And it is not difficult to understand the reason for this.,In this age of ?€?Big data?€?, more and more businesses are relying on people who can make sense of the vast amounts of information generated around us ?€? people who can use sophisticated tools and complex-sounding statistical techniques to derive insights from larger and larger mounds of data.,Businesses have started to understand the power of data. They realize they can use it to make better and faster decisions, outwit their competitors and be more successful. More and more, they are reaching out for people who have the skills to do this.,It is no wonder that there is a huge demand for trained analytics professionals. A recent report suggested there will be a shortfall of over 150000 analytics resources in the US, another study suggested a similar shortfall in India as well.,The gap between demand and supply is increasingly rapidly and this is reflected in the increasing salaries data scientists can command. In the US, data scientists are already commanding higher salaries than MBAs. In India, starting salaries range from 4 lakhs to 8 lakhs.,So how does one go about becoming a data scientist? What are the skills required to succeed in this field?,Well, there is no simple answer to this. Data scientists are a curious breed. They need to possess not just one skill but a combination of multiple skills. Let us examine the skill set requirement in more detail.,?€? Data scientists need to have a good understanding of basic and advanced statistical concepts. These concepts form the basis for most predictive modelling techniques and therefore one needs to understand them well. Knowledge of concepts like measures of central tendency and dispersion, probability distributions, hypothesis testing and probability are essential for most sophisticated analyses.,?€? Predictive modelling techniques like regression, clustering and decision trees are applied on historical data to predict the future. It is these predictions that guide a business?€?s strategy. Thus a knowledge of common predictive modelling techniques, their application, best practices involving their use etc. is a must in this field.,??, ?€? Analytic tools are specialized tools that are used to analyse large amounts of data. These tools allow the data scientists to perform descriptive as well as predictive analytics. A data scientist needs to be proficient in one or more analytic tools in order to do her job effectively. Microsoft Excel is the most popular analytic tool in the world. It does an excellent job of performing descriptive analytics on limited amounts of data. SAS is also an extremely powerful and popular tool. It allows users to build many kinds of predictive models on huge amounts of data.,For a more detailed discussion on various analytic tools available in the market, ,.,To find out which is the best tool for you, ,., ?€? Data scientists need to have a strong quantitative aptitude. They should be comfortable dealing with numbers, large excel sheets or even larger databases., ?€? Call it thirst for knowledge, intellectual curiosity or inquisitive nature ?€? a data scientist needs to work like an investigator. She has to sift through mounds of data to find useful things. She needs to know where to look and what to look for. She needs to have the ability to ask the right questions and the persistence to find the answers., ?€? Analytics is always applied in the context of a business situation ?€? usually a problem or an opportunity. A data scientist cannot work in isolation. She has to work with multiple business stakeholders. A good data scientist will have the ability to explain the results of an analysis in non-technical terms in order to build consensus amongst the business stakeholders.,These are all the skills and qualities that are needed in order to succeed in the exciting and high growth field of analytics. If you feel you have the inherent traits to become a data scientist, you should seriously think about equipping yourself with the requisite technical knowledge as well.,At Jigsaw Academy, our Foundation course has been designed by industry veterans and covers all the 3 required technical skills. Our course has helped thousands of people move into analytics. Learn more about ,.,Reach us at , and ,.
The goal is to design and build a data warehouse / business intelligence (BI) architecture that provides a flexible, multi-faceted analytical ecosystem for each unique organization.,A traditional BI architecture has analytical processing first pass through a data warehouse.,In the new, modern BI architecture, data reaches users through a multiplicity of organization data structures, each tailored to the type of content it contains and the type of user who wants to consume it.,The data revolution (big and small data sets) provides significant improvements. New tools like Hadoop allow organizations to cost-effectively consume and analyze large volumes of semi-structured data. In addition, it complements traditional top-down data delivery methods with more flexible, bottom-up approaches that promote predictive or exploration analytics and rapid application development.,In the above diagram, the objects in blue represent traditional data architecture. Objects in pink represent the new modern BI architecture, which includes Hadoop, NoSQL databases, high-performance analytical engines (e.g. analytical appliances, MPP databases, in-memory databases), and interactive, in-memory visualization tools.,Most source data now flows through Hadoop, which primarily acts as a staging area and online archive. This is especially true for semi-structured data, such as log files and machine-generated data, but also for some structured data that cannot be cost-effectively stored and processed in SQL engines (e.g. call center records).,From Hadoop, data is fed into a data warehousing hub, which often distributes data to downstream systems, such as data marts, operational data stores, and analytical sandboxes of various types, where users can query the data using familiar SQL-based reporting and analysis tools.,Today, data scientists analyze raw data inside Hadoop by writing MapReduce programs in Java and other languages. In the future, users will be able to query and process Hadoop data using familiar SQL-based data integration and query tools.,The modern BI architecture can analyze large volumes and new sources of data and is a significantly better platform for data alignment, consistency and flexible predictive analytics.,Thus, the new BI architecture provides a modern analytical ecosystem featuring both top-down and bottom-up data flows that meet all requirements for reporting and analysis.,See:??

Let's keep it simple. The best application of Big Data is in systems and methods that will significantly reduce the Big Data footprint.,If you are interested in the approach to Big Data mentioned here and in particular want to know more about the definition, architecture and use of 'data governors' applied to Big Data, then please leave a comment below.
It is the tragedy of the 21st century! Terrorism has become big business to some blood thirsty tyrants who would stop at nothing to use human beings as collateral in their quest for more resources.,The Islamic State, Al-Qaeda, Al-Shabaab, Boko-Haram...the list is endless. They are found all over the globe. Their objective is to kill, to steal and to destroy. Therefore, their master can only be what you and I can rightfully guess at the slightest instance.,But what is the solution to the sophisticated terrorism that we witness on both a local and global scale today?,Well, Big Data coupled with specialized Predictive Analytics at the right place and at the right time will not only provide the right answers to the terrorism menace--You will be amazed how the possibilities of Open Data and a willing global citizenry collaborating and uniting against terrorism will definitely deal a fatal blow to the architecture of terror.,It is an open secret that terrorist groups are highly financed, trained and equipped. This means that in the wake of today's Cyber Warfare, the most likely winner of an encounter with Cyber terror is the person who has the ultimate trump card of the Information Age--the card of Big Data that has been transformed into smart data through Predictive Analytics!,By Christopher Alvin Mokaya,afromediablog@gmail.com
Analytics is becoming critical in all part of our lives. Biostatistics has been a big driver of this analytics demand in the field of pharmaceuticals, biotech, health & medicine.,But, What is Biostatistics? From the ,:,Biostatistics (or biometry) is the application of statistics to a wide range of topics in biology. The science of biostatistics encompasses the design of biological experiments, especially in medicine, pharmacy, agriculture and fishery; the collection, summarization, and analysis of data from those experiments; and the interpretation of, and inference from, the results. A major branch of this is medical biostatistics,[1] which is exclusively concerned with medicine and health.,From the abstract description of a Biostatistician?€?s role, here is an example analysis that a Biostatistician would need to do. ,.,:,A study was undertaken to evaluate the effect of percutaneous transluminal coronary angioplasty (PTCA) in patients with one-vessel coronary artery disease. A random sample of one hundred and seven patients with coronary artery disease were given PTCA.?? Patients were given exercise tests at baseline and after 6 months of follow-up. Exercise?? tests were performed up to maximal effort until symptoms (such as angina) were present.,The ?€?change?€? in the duration of exercise was calculated. ?€?Change?€? is defined as the 6 month test minus the baseline test. The mean change was 2.1 minutes and the standard deviation of the changes was 3.1.,(a) What statistical test can be performed to see if there has been a statistically significant change in duration of exercise for this group of patients given PTCA?,(b) Compute a 95% confidence interval for the mean change in exercise duration.,(c) Can we conclude from this study that PTCA is effective in increasing exercise duration? Are there any limitations or weaknesses in this study for answering that question?,??,The above case study is a very simple & beginners level type of question that one needs to answer. For actual job qualifications we analyzed 408 job postings and found that nearly all of them required a PHD or Masters level degree. Furthermore, we looked at the analytics tools skills required for the job. 75% of the job postings (308 of 408) required SAS.?? R, SPSS, Stata & Python come after SAS in terms of the skills required becoming a biostatistician.,Given the interest & educational qualification requirements for Biostatistics, the number of Masters Degrees being given also increased. According to ,, data shows growth in Biostatistics degrees from 2012 to 2013,??,Some of the top companies & the number of open job positions (in the brackets) are given below:,Intrigued about Biostatistics? Here are some resources to get you started:
As part of a deeper dive into domain specific analytic careers, we are exploring the Insurance industry. The below data & charts gives context & explains the opportunities for an analytics careers in Insurance.,Our first examination involved analyzing the top insurance companies hiring for analytics related positions.?? The below chart shows the open job postings where the title was ?€?Data Science?€? or ?€?Analytics?€? or ?€?Actuary?€? & plural or related word. We found a total of 185 job postings in the US which fit this criteria. ??The top employers & the chart shows the number of open job postings next to the employer. The top 3 employers are Senior Life Insurance with open job postings of 20 in Georgia, Allstate with 18 in Chicago and New York Life Insurance with 16.??,We then explored the open job postings by location and found that Chicago was ranked the highest with 36 open job posting with Allstate & CAN contributing to the highest job postings in that area. The biggest employer in Georgia was Senior Life Insurance & New York had an assorted set of employers. The top 5 locations with the most open job postings is given below.??,We went to the next step to understand what statistical tools was mentioned in these job posting. The leader in the insurance industry was SAS (45), followed by R (19) & SPSS (8). ??Most of the job postings that required these statistical tool trainings were more weighted towards data science & analytics related positions and less towards actuarial type of positions. Although, we must add that quite a few actuarial related job postings did have requirements of familiarity with statistical tools.??,We tried to give more specific context to just data science & analytics related positions to see what disciplines are hiring for these positions. We analyzed 50 open job postings & found that 12 of them were in marketing, 8 of them were in Claims & 7 of them were in underwriting. This is an interesting shift since the insurance industry would have focused most of its analytical horsepower in improving its underwriting models but it seems that they are now interested & focused more on driving marketing & sales growth.??,We hope that this overview of analytics careers & jobs in the insurance industry has piqued your interest & we look forward to your feedback.
There is a debate raging (ok, maybe not raging) about terms such as ?€?data science?€?, ?€?analytics?€?, ?€?data mining?€?, and ?€?big data?€?. What do they mean, how do they overlap, and perhaps most importantly, who are the people who work in these fields?,Along with two other DC-area Data Scientists, Marck Vaisman and Sean Murphy, I?€?ve put together a survey to explore some of these issues. Help us quantitatively understand the space of data-related skills and careers by participating!,It should take 10 minutes or less, data will be kept confidential, and we look forward to sharing our results and insights in a variety of venues, including DSC! Thanks!
Our latest contest is live! Feel free to signup. This contest is based on the Euro Soccer championship. You will develop an algorithm that can predict the ranking & outcome of each of the participating nations playing in the Euro 2012 championship. Our results will be announced after the 1st of July, 2012 (after the finals of Euro 2012). Solver celebration and prizes of $500 to be won!,The contest link -??,Cheers,??,The CrowdANALYTIX Team

Tim O?€?Reilly famously declared in 2005: ?€?,.?€? It well may be that big data?€?the organizational skill of using data as the key driver of performance?€?will make the IT function the new Intel Inside, the most strategic component of any organization.,Based on the success of Google, Amazon, and eBay at the time, O?€?Reilly correctly asserted that database management was a core competency of Web 2.0 companies and that ?€?control over the database has led to market control and outsized financial returns.?€? The upcoming?€?and outsized?€?Facebook IPO is a testament to O?€?Reilly?€?s foresight, saying in 2005 that ?€?data is the Intel Inside of [Web 2.0] applications, a sole source component in systems whose software infrastructure is largely open source or otherwise commodified.?€???,The linking of documents by web 1.0 and the linking of people by web 2.0 created what we now call big data. The explosion in the volume, variety, and velocity of data and the technologies invented to manage it occurred outside enterprises and, so far, had limited impact on enterprise IT.,But companies?€?and their IT departments?€?cannot ignore big data anymore. While??,??at the end of last year that ?€?through 2015, more than 85 percent of Fortune 500 organizations will fail to effectively exploit big data for competitive advantage,?€? leading-edge organizations are moving fast to update their business intelligence, data mining, and business analytics practices with the new tools and skills set of big data.,Prime example is P&G, whose CEO Bob McDonald ?€?,??the company's processes from end to end,?€? and whose CIO, Filippo Passerini, said recently in??,??published in??,: ?€?I believe in our industry the differentiator in three to five years will be the ability to predict what is going to happen in the business. It is very possible to detect those key indicators early on, or roll out a predictive model that will allow us to be more accurate in the way we go to market with a new product. It is so critical to run the business in real-time. The ability to analyze big data?€?massive quantities of data?€?is important.?€?,As I see it, CIOs have three options right now:,The potential for business transformation is huge.??,??that ?€?We are on the cusp of a tremendous wave of innovation, productivity, and growth, as well as new modes of competition and value capture?€?all driven by big data as consumers, companies, and economic sectors exploit its potential.?€? Research conducted by??,??that companies that use ?€?data driven decision making?€? are about 5% more productive and profitable than their competitors.,In a few years, we may see an even bigger impact of big data on business performance, acting as a key differentiator between winners and losers. The winners will make big data their own unique organizational skill, a ?€?sole source component?€? (O?€?Reilly?€?s phrase) that determines whether a business can stay on top of its rapidly-changing environment and way ahead of its competitors.,IT can play a leading role in this transformation and become the next Intel Inside, this special organizational skill. It may be a unique moment in IT?€?s history, an opportunity to finally close the age-old ?€?business-IT gap,?€? and lead the business in a once-in-a-century re-definition of the nature of business competition. The business of the future will be based on data and data is IT?€?s business.





The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??
CRMs are supposed to be used to achieve better efficiency. By investing time in the CRM, sales teams should be able to identify leads, retain existing customers and successfully recruit new clients to the fold.,Your research and development team should be able to use the metrics from the CRM to drive next year?€?s products, and the marketing team should be able to feed this back into their campaigns next year.,That?€?s great, in theory.,Sadly, many CRMs fail to perform well. No system could feasibly solve every problem in your business, but if the CRM is creaking under the weight of dirty data, it could actually be hindering progress.,We?€?ve heard estimates of CRM project failure rates between 30 and 60 per cent. And when you consider that the cost of a CRM starts at hundreds of pounds per month, the potential for waste is enormous. That?€?s not counting the cost of implementing the CRM, including change to systems, processes and workflows.,After all of that cost and investment, it can be extremely disheartening to find that staff simply don?€?t trust their data.,The greatest success in data quality comes from good planning.??Unfortunately, many businesses do not have the experience to plan effectively, or do not realize the havoc that dirty data can wreak. But if you?€?ve not yet embarked on your implementation, there are lots of practical steps you can take to ensure a good standard of data quality.,Our recommendations are threefold.,Ensure the data you are putting into the CRM is cleanvalid, deduplicated and fit for purpose. Putting dirty data into a brand new CRM is like forcing a square peg into a round hole. The two just don?€?t fit, and it isn?€?t worth trying to force them.,Good preparation is critical if you are to inform other parts of the business. You cannot create meaningful or reliable reports, or formulate accurate intelligence, if your data is not cleansed, filtered and structured correctly.,Look at systematic failures and find ways to improve processes prior to CRM roll-out. You may find that data capture methods are broken; you may discover a flaw in another application. This can help you??weed out problems that are spawning new dirty data, adding to the problem you?€?ve got.,By adopting data quality tools at an early stage, you can use self-service methods to prepare your own data, improving productivity as the implementation progresses.,If your CRM is broken now, and your data is decaying, things are only ever going to get worse.,Think of your dirty data like a bad debt. You?€?re going to need to invest in the quality of your data; repay the interest you owe. The longer the data is neglected, the more you will owe, and the more the interest will be compounded. In a few short years, your data debt could be so great that you?€?re left bankrupt of any meaningful information.,Often, well-meaning advancements in data management can cause more problems than they solve. For example, we might import data from another system to try to replenish our faltering CRM. But if that data isn?€?t properly prepared, you could end up with invalid entries that can?€?t be opened or saved, or cause the system to crash. If you continually leave bad data at rest in the database, you will reach a stage where no single record can be relied upon.,If your CRM is not fuelled with good quality data, you must:,Neither of these things need be tedious; neither need be a wholly manual process. Data quality software can help you to polish the rough diamonds in your database, resulting in data that is current and fit for purpose.,Proper data management means finding gaps in your data, eradicating invalid data, and removing data that is duplicated or out of date. This focus on data quality must be broad enough to encompass the whole organisation, yet fine-tuned enough to pick up phonetic matches of someone?€?s surname.??,??each minute to achieve this.,The ultimate goal for many companies is the single customer view, a state where every customer is represented by one comprehensive database entry. Without a mature and managed approach to data, the single customer view will always be pure fantasy.,If you don?€?t clean up your CRM now, what will happen?,Respected think tank Gartner says that the market for data quality tools is growing. It predicts that businesses will spend $2 billion in 2017. The pursuit of pure, usable, efficient data is a goal shared by businesses globally, and it?€?s a goal we must all realise if we are to compete effectively in the years to come.
??,??,??,??,??,??,??,This information is a part of community edited list at??
Zettaset today announced the release of Version 4 of its big data management solution, which offers several new service management features, including the industry's first NameNode Failover, as well as JobTracker Failover, Oozie Failover and a unique visual user interface (UI). Built on Hadoop and other high-volume, open-source technologies, Version 4 offers greater stability within Hadoop while providing a solution to manage big data that is more accessible to IT pros, yielding productivity and cost efficiency benefits.,Version 4 of Zettaset's big data management solution provides a fault-tolerant, automated, highly available version of Hadoop. The goal being to provide a Hadoop system that seamlessly manages itself, with no single points of failure.,First NameNode Failover and Increased Security: With the addition of failover for NameNode, JobTracker and Oozie service managers, Zettaset has created the first self-healing big data management environment within Hadoop that anticipates potential system failures and corrects them before problems occur. It accomplishes this by shifting jobs from malfunctioning to fully functioning nodes, ensuring there are no service interruptions while reducing the risk of data loss.,Version 4 also features a Network Time Protocol to prevent clock drift and eliminate vulnerabilities in the system. It also uses Kerberos authentication to add a secure layer tied to Zettaset user sessions and mitigate security related risks.,"NameNode, JobTracker and Oozie Failover are a first for our industry, providing an effective out-of-the-box solution to data loss issues within Hadoop," said Michael Dalton, CTO and co-founder of Zettaset. "The new intuitive point-and-click interface is easy enough for any IT professional to operate and eliminates the need to hire extra specialists, making Hadoop accessible to small and mid-sized businesses rather than just large enterprises.",Streamlined Browser-Based Service Management UI: Zettaset's new service management UI adds a sophisticated browser-based point-and-click interface that facilitates visual system health monitoring while doing away with cumbersome hand-coded command lines. With support for most major web browsers, this streamlined UI makes it possible to perform tasks in minutes that may have previously taken hours.,The new UI also simplifies interaction with MapReduce, allowing users to customize and submit map reduce jobs in seconds, as well as browse and monitor jobs visually. It also adds interactive batch processing to data management technology Hive and boosts productivity by checking for syntax errors from within the browser.,New Tree View Filesystem: The Zettaset solution now includes HDFS FileSystem Explorer to simplify file navigation by presenting a tree view of the raw Hadoop file system. Using this tree view file system, Zettaset users can:,-- List files, owners and permissions,-- Create, upload, download, delete, rename and display files,-- Create, delete and rename directories,"Our main goal in Version 4 was to give our clients what they were desperately asking for in their enterprise deployments, which was a safe, highly available Hadoop solution, while also making our platform more intuitive," said Brian Christian, CEO of Zettaset. "We've accomplished both those goals, and we're proud of our ability to push the management of big data to the next level by making it accessible to more organizations. Since all business processes generate data, both small and large businesses that want to scale a solution can benefit from our big data management solution. We couldn't be more excited to roll out this newest upgrade.",Pricing and Availability,Zettaset's data management solution is scalable to fit the pricing needs of the enterprise as well as small- to medium-sized businesses. For pricing information, visit , .,About Zettaset,Zettaset delivers a fault-tolerant and highly available solution for massive data aggregation. Built on Hadoop and other leading open-source technologies, the company's big data platform is a high-performance, easy-to-deploy solution with no single points of failure. Zettaset's software manages the health, security and administration of the enterprise Hadoop environment. The company's solution enables true redundancy and real-time business intelligence for organizations of all sizes, while its simple licensing model leads to a significantly lower total cost of ownership. For more information, visit ,??
Jaspersoft??announced a second-generation native connector to MongoDB, an open source database. 10gen, the company behind MongoDB, and Jaspersoft have teamed together to deliver an enhanced tool for companies to provide easier reporting, analytics, and visualization of Big Data. Jaspersoft is a sponsor of the upcoming MongoSV, to be held in Santa Clara CA today,??December 9, 2011 and will be showcasing the combined solution there.,Building on the popularity of the first generation connector, which has been downloaded by thousands of users, the new Reports Server??connector for MongoDB solves a key challenge in providing insight from Big Data??systems with terabytes or even petabytes of data. Jaspersoft's intelligent connector integrates 10gen's NoSQL platform with the full Jaspersoft BI??Suite, providing flexible and affordable reporting, ad hoc analysis, and dashboarding of MongoDB data.?? Jaspersoft's unique drag and drop analytic user interface, introduced with its JasperReports Server v4.5 release yesterday, delivers faster and easier insight to data stored in MongoDB. As a result, both business and technical users can visualize and analyze Big Data??more rapidly for competitive advantage.,"Jaspersoft is excited to continue working with 10gen to develop an industry-leading solution to address the need to derive insights from MongoDB Big Data??solutions," said Karl Van den Bergh, Vice President of Product and Alliances at Jaspersoft. "Our JasperReports Server connector for MongoDB delivers proven, flexible tools for reporting and interactive data exploration, setting the standard in Big Data BI","Making MongoDB easily accessible for reporting and analysis is an important goal," said Erik Frieberg,??VP of Marketing??and Alliances for 10gen. "Jaspersoft's modern BI??suite provides a compelling way for businesses to easily visualize and analyze large amounts of data stored in MongoDB."
As big data use cases proliferate in telecom, health care, government, Web 2.0, retail etc there is a need to create ,. . We have created a big data workload design pattern to help map out ,. There are , showcased which have common patterns across many business use cases.,??,?? ,It essentially consists of matching incoming event streams with predefined behavioural patterns & after observing signatures unfold in real time, respond to those patterns instantly.,Let?€?s take an example: ??In?? registered user digital analytics ??scenario one specifically examines the last 10 searches done by registered digital consumer, so ??as to serve a customized and highly personalized page?? consisting of categories he/she has been digitally engaged. Also depending on whether the customer has done price sensitive search or value conscious search (which can be inferred by examining the search order parameter in the click stream) one can render budget items first or luxury items first,Similarly let?€?s take another example of real time response to events in ??a health care situation.?? In hospitals patients are tracked across three event streams ?€? respiration, heart rate and blood pressure in real time. (ECG is supposed to record about 1000 observations per second). These event streams can be matched for patterns which indicate the beginnings of fatal infections and medical intervention put in place,??,??,These Big data design patterns are ,. The big data workloads stretching today?€?s storage and computing architecture could be human generated or machine generated. The big data design pattern may , like telecom, health care that can be used in many different situations. But irrespective of the domain they manifest in the solution construct can be used. Big data patterns also help prevent architectural drift. Once the set of big data workloads associated with a business use case is identified it is easy to map the right architectural constructs required to service the workload - ,It is our endeavour to make it , with subsequent ,.,As Leonardo Vinci said , ?€?. Big data workload design patterns help simplify the decomposition of the business use cases into workloads. The workloads can then be mapped methodically to various building blocks of Big data solution architecture. , 
Big Data, housed in new and disruptive technologies, is expected to account for more than 50 percent of the world?€?s data in the next five years, according to a a new study. While it offers huge and untapped value, the inevitable result is stress and strain on the world?€?s Interent infrastructure as companies seek to manage this explosion of information.,The new study, released jointly by Internet Research Group and Infineta Systems??a provider of WAN optimization systems, examines how big data is affecting enterprise WAN (Wide Area Network) throughout the country.,Big Data ?€? which is defined as datasets whose size is beyond the ability of typical database software tools to capture, store, manage and analyze ?€? ??is most often found in petabyte to exabyte size, and is unstructured, distributed and in flat schemas. ??As big data continues to grow, the industry anticipates both enormous change and untapped value for enterprises. According to Infineta?€?s report, most companies will adopt key Big Data technologies in the next year to 12-18 months.,All this data in need of capture, storage, processing and distribution has the potential to clog networks. About .5 Gbps of bandwidth is needed per petabye of Big Data under management by Hadoop, an open source platform for large-scale??computing.?? The bandwidth demand can result in compromises in the latency, speed and reliability of the enterprise WAN.,Infineta is interested in this topic, as the privately-held company based in San Jose, California supplies products that support critical machine-scale workflows across the data center interconnect. However, the study findings highlight developing trends that are impacting the entire data center industry.,Key trends identified by Infineta include:,The report finds that organizations are deploying Hadoop clusters as a centralized service offering so that individual divisions don?€?t have to ??build and run their own, and that ?€?bigger is better?€? when it comes to processing batch workloads.,This set up leads to Big Traffic ?€? data movement between clusters, within a data center and between data centers. Data movement includes but is not limited to replication and synchronization, which will become especially important as Hadoop becomes a significant factor in enterprise storage. Big Traffic data movement services support Big Data analytics, regulatory compliance requirements, high availability services and security services.
HOPKINTON, Mass., Dec. 5, 2011 -- EMC Corporation??today unveiled the findings of the largest-ever global survey of the data science community. Spanning the United States, the United Kingdom, France, Germany, India and China, the EMC Data Science Study reveals and quantifies a rampant scarcity across the globe for the prerequisite skills necessary for a company to capitalize on the opportunities found at the intersection of Big Data and data analytics. Only one-third of companies are able to effectively use new data to assist their business decision-making, gain competitive advantage, drive productivity growth, yield innovation and reveal customer insights.,The survey revealed that the explosion of digital data created by mobile sensors, social media, surveillance, medical imaging, smart grids and the like -- combined with new tools for analyzing it all - has created a corresponding explosion in the opportunity to generate value and insights from the data. As such, the business demand for data scientists has quickly outpaced the supply of talent.,The EMC Data Science Study respondents included nearly 500 members of the data science community globally including: data scientists and professionals from related disciplines such as data analysts, data specialists, business intelligence analysts, information analysts and data engineers globally, all of whom have IT decision-making authority.,Key Findings,Informed Decision-making - Only 1/3 of respondents are very confident in their company's ability to make business decisions based on new data.,Looming Talent Shortage - 65% of data science professionals believe demand for data science talent will outpace the supply over the next 5 years - with most feeling that this supply will be most effectively sourced from new college graduates.,Barriers to Data Science Adoption - Most commonly cited barriers to data science adoption include: Lack of skills or training (32%) budget/resources (32%), the wrong organizational structure (14%) and lack of tools/technology (10%).,Customer Insights - Only 38% of business intelligence analysts and data scientists strongly agree that their company uses data to learn more about customers.,New Technology Fueling Growth - 83% of respondents believe that new tools and emerging technology will increase the need for data scientists.,Lack of Data Accessibility - Only 12% of business intelligence professionals and 22% of data scientists strongly believe employees have the access to run experiments on data - undermining a company's ability to rapidly test and validate ideas and thus its approach to innovation.,Advanced Degrees - Data scientists are 3 times as likely as business intelligence professionals to have a Master's or Doctoral degree.,Augmenting Business Intelligence - Although respondents found an increasing need for data scientists in their firm, only 12% saw today's business intelligence professionals as the most likely source to meet that demand.,Higher-Level Skills - Data scientists require significantly greater business and technical skills than today's business intelligence professional. According to the Data Science Study, they are twice as likely to apply advanced algorithms to data, but also 37% more likely to make business decisions based on that data.,Love the Work - The study discovered highly favorable attitudes toward the companies where they work. In fact, data scientists believe their IT functions are better aligned and better able to attract talent, are ahead in key technology areas like cloud computing and not surprisingly rate their company's data analysis and visualization abilities very favorably compared to the views of business intelligence professionals.,Involved Across the Data Lifecycle - Data scientists are more likely than business intelligence professionals to be involved across the data lifecycle--from acquiring new data sets to making business decisions based on the data. This includes filtering and organizing data as well as representing data visually and telling a story with data.,Tools of the Trade - Data scientists are more likely than business intelligence professionals to use scripting languages, including Python, Perl, BASH and AWK. Yet, Excel remains the tool of choice for both data scientists and business intelligence executives, followed closely by SQL.,Data Scientists Quotes,Andreas Weigend, Ph.D Stanford, Head of the Social Data Lab at Stanford, former Chief Scientist Amazon.com,"We live in a data-driven world. Increasingly, the efficient operation of organizations across sectors relies on the effective use of vast amounts of data. Making sense of big data is a combination of organizations having the tools, skills and more importantly, the mindset to see data as the new "oil" fueling a company. Unfortunately, the technology has evolved faster than the workforce skills to make sense of it and organizations across sectors must adapt to this new reality or perish.",Michael Driscoll, Ph.D Boston University, Co-Founder and CTO at MetaMarkets,"Neither tools nor people alone can solve the challenges of Big Data. They must work together and that is the promise of data science. Despite advances in software tools, the number of people with experience using these tools, and with real-life exposure to large-scale data sets, is small. Data science is a young field, and its growth will be fueled as much by technology as through the mentorship of new acolytes by leading practitioners.",EMC Executive Quote,Jeremy Burton, EVP and Chief Marketing Officer, EMC Corporation,"The Big Data era has arrived in full force, bringing with it an unprecedented opportunity to transform business and the way we work and live. Through the convergence of massive scale-out storage, next-generation analytics and visualization capability, the technology is in place. What's needed to fully realize its value is a vibrant, interconnected, highly-skilled and empowered data science community to reveal relevant trend patterns and uncover new insights hidden within.",Additional Resources:,Read the EMC Data Science Study in full,Read Chuck Hollis' blog: "Understanding The New Rock Star: The EMC Data Science Survey",Read the EMC Education Services release announcing new training and certification program focused entirely on Data Science & Big Data Analytics,Connect with EMC via Twitter, Facebook, YouTube, LinkedIn and ECN,About the EMC Data Science Study,EMC conducted an independent research study to measure attitudinal and behavioral profiles of data scientists and business intelligence professionals globally and across industries. 462 IT decision makers who self-identified as either a data scientist or business intelligence professional participated in this market research study through participation with research partner Toluna. An additional 35 participated by invitation through their previous attendance at the EMC Data Scientist Summit and the online data scientist community Kaggle. The survey was designed to provide a first ever look into the behavioral patterns, thoughts, and needs of the data science community.,About EMC,EMC Corporation is a global leader in enabling businesses and service providers to transform their operations and deliver IT as a service. Fundamental to this transformation is cloud computing. Through innovative products and services, EMC accelerates the journey to cloud computing, helping IT departments to store, manage, protect and analyze their most valuable asset -- information -- in a more agile, trusted and cost-efficient way. Additional information about EMC can be found at??,.,EMC is a registered trademark of EMC Corporation in the United States and/or other countries. All other trademarks are the property of their owners.
The Washington Education Association (WEA, in Washington State) is partnering with Aon Hewitts (Illinois), a verification company, to eliminate a specific type of health insurance fraud: teachers reporting non-qualifying people as dependents, such as an unemployed friend with no health insurance. The fraud is used by "nice" people (teachers) to provide health insurance to people who would otherwise have none, by reporting them as spouse or kids.,Interestingly, I saw the letter sent to all WEA teachers. It requires you to fill lots of paperwork and provide multiple identity proofs (tax forms, birth certificates, marriage certificates etc.) similar to ID documents (I9 form) requested to be allowed to work for a company.,It is easy to cheat on the requested paper documentation that you have to mail to the verification company (e.g. by producing fake birth certificates or claiming you don't have one, etc). In addition, asking people to fill so much paperwork is a waste of time and natural resources (trees used to produce paper), and results in lots of errors, privacy issues and ID theft risk, and costs lots of money to WEA.,So why don't they use modern methods to detect fraud: data mining techniques to detect suspicious SSN's, identifying SSN's reported as dependent by multiple households based on IRS tax data, SSN's not showing up in any tax forms submitted to the IRS, address mismatch detection, etc. (note that a 5-day old baby probably has no record in the IRS database, yet he is eligible as a dependent for tax or health insurance purposes).,Why not use data mining technology, instead of paper - with all the advantages that data mining offers over paper? What advantages does paper offer? I don't see any.,:,This should eliminate most of the fraud, at a very low cost, and with very little burden on teachers.
What's your cloud integration strategy? If you're like most IBM i shops, much of your data interchange is handled via good old EDI or flat file transfers. But the rapid spread of cloud services is hastening the move to more sophisticated forms of data and application integration and interchange. According to EXTOL??which develops integration broker software for IBM i and other platforms, the day is fast approaching when companies will need new techniques for integrating cloud services into their business processes.,EXTOL vice president of product management Jim O'Leary attended the Gartner??conference in Las Vegas confessed to keeping his "tech filter" on all the time.,"I know how these things work, and what makes sense and what doesn't," he says. "I will predict the hype around cloud apps and cloud-based integration is going to get much greater next year. Unfortunately, I think that's going to lead some people to believe, it's inexpensive, it's 10 times easier and 10 times better, and it's only when they get into the implementation that they realize what they have to deal with.",Cloud app vendors sell us on a vision of business flexibility and adaptability and "always-on" readiness, and there certainly is some truth to those claims. The capability to rapidly prototype and start working with a new??application, for example, brings real business benefits. Targeted business applications with sophisticated iPad interfaces and built-in analytics are becoming the norm, and people will use them. Similarly, industry-specific data feeds from the cloud are becoming more prevalent, advanced, and necessary for day-to-day operations.,However, the twin bugaboos of integration and security are likely to dog the cloud in the near term. The reality is that achieving cloud nirvana--defined as a harmonious co-existence of on-premise and cloud-based apps, with all the boring IT, security, and compliance stuff that entails--will take time and money and sweat and maybe even a few tears before something repeatable and securable and fully integrate-able is worked out.,"The hype will certainly go up, and I think we'll see a lot more people at least dip their toe in the water," O'Leary continues. "I don't see a lot of people who have jumped in wholesale to cloud-based integration. I think that's a few years off. There are many cases where companies are doing very well with EDI and flat files and haven't encountered a big enough requirement to go behind that. But sooner or later, just about everyone is going to be faced with.,EXTOL is preparing for this future with its Java-based integration broker, called EXTOL Business Integrator (EBI). Introduced in the early 2000s, EBI offers all of the EDI transformation capabilities of its legacy EXTOL EDI Integrator for i product, and adds a number of more advanced features, including a process automation manager, transformation manager, application- and database-specific connectors, and a graphical business process flow modeler. The software packages all of these components into a single product, eliminating the need for mid-market customers to cobble together their own enterprise service bus (ESB) solution.,EBI can tackle a range of business transformation tasks that the EDI product can't, including business process integration using XML and SOAP- and REST-based Web services, and database and cloud integration. The software includes managed file transfer (MFT) product features, like support for FTPS and the capability to automatically kick off processes upon discovery of new files, such as transforming the data and reloading it in a different system.,O'Leary admits it can be tough to categorize EBI. "EBI is an extremely flexible product in that respect, and for that reason it's hard to pinpoint how customers might use it," he says. "We get reactions of shock when people see what they can do with a tool like ours. They say, 'Gee, I didn't know I could do that kind of stuff.'",The last major release of EBI??introduced a Smart Mapping feature that helps people connect source and target systems and choose the correct transformation to process. EXTOL estimates it can eliminate 20 to 90 percent of the time needed to create a new map.,The next release of EBI, due out in early 2012, will introduce a new sharable transformation library that EXTOL expects will take integration up another notch. Version 2.6 will feature a cloud-based mechanism for sharing mapping patterns and best practices between EXTOL customers. If a customer has defined and created a map that translates between, say, an EDIFACT file and a SAPiDOC file, that map can be shared with other EXTOL customers Subsequently, that can accelerate the map's roll-out to a greater degree than Smart Mapping could yield.,There will be a learning curve to the new world of cloud integrations, but with the work EXTOL is doing with simplifying transformations, the company is helping to flatten it a bit and masking some of the technological complexity for customers.,Any edge that EXTOL can provide will be welcome as they embark in the new cloud world, O'Leary says. "Customers are going to have to integrate with cloud-based applications and services," he says. "And when they do that, they're going to have to do data integration to back-end databases so they can keep synched up--they're going to have to connect internal apps with external apps. I think that transition is something that just about every company of a certain size is going to face at some point."
This year has seen consolidation and engineering around improving the basic storage and data processing engines of NoSQL and Hadoop. That will doubtless continue, as we see the unruly menagerie of the Hadoop universe increasingly packaged into distributions, appliances and on-demand cloud services. Hopefully it won?€?t be long before that?€?s dull, yet necessary, infrastructure.,Looking up the stack, there?€?s already a line up of cool tools for data scientists and not to be left out, the??Hadoop connectors for established analytical tools such as R and Tableau. Yet the idea is to go into next year making big data more powerful: frankly by to??decreasing?? the cost of generating experiments.,Here are two ways in which big data can be made more powerful.,Hadoop?€?s batch-oriented processing is sufficient for many use cases, especially where the frequency of data reporting doesn?€?t need to be up-to-the-minute. However, batch processing isn?€?t always adequate, particularly when serving online needs such as mobile and web clients, or markets with real-time changing conditions such as finance and advertising.,Over the next few years we?€?ll see the adoption of scalable frameworks and platforms for handling streaming, or near real-time, analysis and processing. In the same way that Hadoop has been borne out of large-scale web applications, these platforms will be driven by the needs of large-scale location-aware mobile, social and sensor use.,For some applications, there just isn?€?t enough storage in the world to store every piece of data your business might receive: at some point you need to make a decision to throw things away. Having streaming computation abilities enables you to analyze data or make decisions about discarding it without having to go through the store-compute loop of map/reduce.,Emerging contenders in the real-time framework category include Storm,??from Twitter, and S4,??from Yahoo.,Your own data can become that much more potent when mixed with other datasets. For instance, add in weather conditions to your customer data, and discover if there are weather related patterns to your customers?€? purchasing patterns. Acquiring these datasets can be a pain, especially if you want to do it outside of the IT department, and with some exactness. The value of data marketplaces??is in providing a directory to this data, as well as streamlined, standardized methods of delivering it. Microsoft?€?s direction of integrating its Azure marketplace??right into analytical tools foreshadows the coming convenience of access to data.,As Data??Scientist??and??their??teams??become a recognized part of companies, we?€?ll see a more regularized expectation of their roles and processes. One of the driving attributes of a successful data science team is its level of integration into a company?€?s business operations, as opposed to being a sidecar analysis team.,Software developers already have a wealth of infrastructure that is both logistical and social, including wikis and source control, along with tools that expose their process and requirements to business owners. Integrated data science teams will need their own versions of these tools to collaborate effectively. One example of this is EMC Greenplum?€?s??,, which provides a social software platform for data science. In turn, use of these tools will support the emergence of data science process within organizations.,Data science teams will start to evolve repeatable processes, hopefully agile ones. They could do worse than to look at the ground-breaking work newspaper data teams are doing at news organizations such as The Guardian and New York Times: given short timescales these teams take data from raw form to a finished product, working hand-in-hand with the journalist.,Visualization fulfills two purposes in a data workflow: explanation and exploration. While business people might think of a visualization as the end result, data scientists also use visualization as a way of looking for questions to ask and discovering new features of a dataset.,If becoming a data-driven organization is about fostering a better feel for data among all employees, visualization plays a vital role in delivering data manipulation abilities to those without direct programming or statistical skills.

Caringo announced a partnership to provide a solution for retaining and accessing structured data in Caringo object storage software powered by CAStor. The combination allows users to store increasing volumes of structured and semi-structured data from databases, logs and other machine-generated data sources along with unstructured content such as audio, documents, e-mails, images, and videos in CAStor.,The combined solution provides a horizontal platform that enterprises can use to increase efficiency and dramatically reduce the cost of their infrastructure required to retain and manage data. ISVs and service providers can use Caringo and RainStor to offer new data retention services at higher margins, including those delivered via the cloud. The solution is ideal for use cases where database data must be stored with file-based data for long periods of time such as in Financial Services, Communications, Healthcare and Life Science industries. For example a healthcare organization can now store their patient medical images as well as patient documents and structured database medical records for years or decades in CAStor.,RainStor's Big Data Management database contains patented compression and de-duplication techniques, reducing the data footprint size by 95% or more compared to traditional approaches. Unlike other forms of binary compression, data retained in RainStor can be stored and queried on-demand at a granular level via SQL or standard business intelligence tools without the need for re-hydration. Certified integration with CAStor makes RainStor the only database that can store and access structured data within an object storage platform for compliance and business analysis.,Caringo CAStor provides Big Data storage utilizing any combination of x86 hardware at up to 98% disk utilization. With integrated automated optimization, data protection and zero-provision expansion, one system administrator can easily manage over 10 PB of storage. The integration of RainStor technology means that IT Administrators can leverage these benefits for enterprises that want to manage both structured and unstructured Big Data from a single environment.,"RainStor enables enterprises to keep and query as much data as they want, for as long as they want," said RainStor CEO John Bantleman. "RainStor's certified integration with CAStor is a unique and powerful combination that can retain all types of data at scale. Our common philosophy of being storage hardware agnostic and seeking simplicity of administration, results in the lowest total cost of ownership of keeping Big Data.","In order to continue to extract value from Big Data it must be accessible and stored in a system that can scale massively and economically, as fast as data is generated," said Mark Goros, Caringo CEO. "Our partnership with RainStor provides these benefits taking accessibility one step further by offering the ability to query both structured and unstructured data in long-term storage. This means that data will remain actionable as long as organizations need it to be, which provides undeniable value.",About RainStor,RainStor provides Big Data management software. RainStor's database enables the world's largest companies to keep and access limitless amounts of data for as long as they want at the lowest cost. It features the highest level of compression on the market, together with high performance on-demand query and simplified management. RainStor runs natively on a variety of architectures and infrastructure including Hadoop. RainStor's partners include Amdocs, AdaptiveMobile, Dell, HP, Informatica, Qosmos and Teradata.,About Caringo,Caringo is the global leader in object storage and developer of the Caringo Object Storage Platform powered by CAStor -- software that enables massive scalability and future-proof accessibility of unstructured data. Caringo object storage enables cloud storage solutions, big data repositories and general storage infrastructure replacing NAS, tape and VTL. Our field-proven technology utilizes any combination of x86 server hardware to build boundless storage solutions accessible via HTTP. Caringo object storage is integrated into leading data management applications from CommVault and Symantec, among many others. Organizations of all sizes can now avoid vendor lock-in, reduce costs, and guarantee data accessibility for decades to come. Caringo has over 400 customers ranging from SMBs to Fortune 500 companies.


The smart grid is leading the power industry into a data and analytics boom as its implementation phase gives way to its value phase, according to Christine Richards, senior analyst with Energy Central's Utility Analytics Institute.,Richards defined the decade from 2000 to 2010 as the smart grid's development phase, the five-year period from 2007 to 2012 as the infrastructure, or implementation phase and the decade from 2010 to 2020 as the value phase.,In the value phase, grid optimization, intelligent asset management and the integration of renewable resources and electric vehicles will be achieved by transforming raw data into actionable intelligence for all business and operational units of a utility.,In a recent webcast on the topic,??participants??clearly connected the traditional and new sources of data, how that data is processed for integrity ("data you can trust" ), how it is mashed-up with other data to create myriad insights and how those insights can be presented to enable evidence-based decisions, often in real time.,The traditional basis for data creation and interpretation for energy companies??typically??begins with a geographic information system (GIS), an outage management system, SCADA (supervisory control and data acquisition) substations and field devices. With the advent of advanced metering infrastructure (AMI) and other grid modernization steps, data sources will multiply, become richer and the value of analytics increased accordingly.,Raw data generation is transformed by analytics into useful guidance for decision-making related to multiple areas "Smart Grid Deployment " which major energy companies??across??the country plan to use to provide: customer empowerment, outage/distribution management, condition-based asset maintenance and the integration of distributed renewable energy sources.,The proper analysis of data will transform the utility's role from reacting to historical events and outcomes to predictive, proactive stances that avoid outages or power quality issues.,One difficulty in conveying new, data-enabled capabilities, of course, is describing how such things get done in terms of systems.??Leveraging Data from the "Plant Floor" to decision maker via discrete data with limited value and culled to reached the status of actionable information with enterprise value in formats useful, visual representations (visualization) such that the data generation to outputs as integrated reports, dashboards,??analytics??and answers to self-service queries.
First, let me say how pleased I am to be a part of the Data Science Central community. While I was the VP of Marketing at EMC Greenplum, I had a front row seat to the dynamic and very exciting ?€?Big Data?€? and ?€?Data Science?€? shows. EMC Greenplum has played a pivotal role in helping organizations of all sizes start the process of orienting their businesses and decision-making around data. It has been said, and I?€?ll say it again ?€?the companies that embrace the power of data and live that life will be the winners. The companies that don?€?t, no doubt in my mind, will lose. So, it is with great pleasure that I share with you the identity of 5 startups that will join the likes of EMC Greenplum in providing meaningful opportunities for their employees and customers., , Ben Werther and Pete Schlampp are two guys that I?€?ve had the pleasure of working with before. Ben, at Greenplum and Pete, at IronPort. What they are building at Platfora embodies two of the tenets that I believe will drive the real future of Big Data: Speed and Ease. (the other is beauty). Yes, Gartner has their ?€?Velocity, Volume, Variety.?€? And I have mine: Speed, Ease, and Beauty! , Cassandra in the NoSQL movement has not enjoyed the hype of let?€?s say?€?Mongo, but I view that as a good thing. It?€?s a great entrepreneurs?€? story how co-founders Matt Pfeil and Jonathan Ellis, while at Rackspace, had a mind meld to create DataStax, and the vision to wrap support and services around Cassandra so mere mortals could reap the benefits. My favorite Cassandra users: Adobe, Netflix, Spotify, and Soundcloud! , Former Mint.com folks have formed a company that in my opinion can be the leading ?€?punch line?€? for big data. Truth be told, all kinds of smart people can do ridiculously amazing analytics on vast amounts of data, but if you can?€?t articulate a clear and compelling story about the work --then why bother? The spirit of visual.ly is clear: storytelling. And they take it seriously. Here?€?s a bit about their take on ethics:,We agree to the following principles to support data analysis and visualization:,Most succinctly stated, Visual.ly's policy is one that embodies accuracy, honesty, and transparency. , , The Josh James startup has assembled an impressive team. I love that communications great, Julie Kehoe, formerly of the OutCast Agency, made the trek from NYC to Domo HQ in Utah. I view that as a sign that something special is going on there. The company?€?s focus on the user experience aspect of BI is right on the money. It will be fun to watch the customer traction at Domo, beyond ASU and the Fort Worth Police department. , , Yes, saving the best for last. Metamarkets. I think that this is THE company to watch in 2012. Why? Because they have the opportunity to raise the Big Data bar so high that other companies become irrelevant. Stay tuned. 
According to the Gartner report, "Demand trends have given rise to new challenges in 2011. Fresh needs arising from contemporary challenges are presenting new opportunities in this market as buyers seek to address data integration as a critical aspect of a coherent information management capability, and to integrate disparate data sources (including emerging sources such as 'big data') and new data types into a cohesive and usable set of information."(1) SAP is helping its customers address these challenges with its support of "big data" through the SAP HANA??? appliance software. ??,SAP offers a comprehensive data integration portfolio, which inclueds SAP?? BusinessObjects??? Data Services software, SAP NetWeaver?? Process Integration offering, Sybase Replication Server and SybasePower Designer software.,The 4.0 release of SAP BusinessObjects Data Services highlights new capabilities for information governance with SAP?? BusinessObjects??? Information Steward software and tighter integrations with the SAP?? BusinessObjects??? Business Intelligence platform and SAP?? Business Suite software.,Experiencing strong adoption globally, the 4.0 release of enterprise information management EIM e??solutions from the????SAP BusinessObjects??portfolio. ??SAP helps organizations to better assess and act decisively on huge volumes of data both within and outside their businesses, enabling the delivery of integrated, accurate and timely data ?€? both structured and unstructured ?€? across the enterprise.
Companies are capturing and digitizing more information than ever before. According to IDC, the world produced one zettabyte (1,000,000,000,000 gigabytes) of data annually. Fueling this data explosion are over five billion mobile phones, 30 billion pieces of content shared on Facebook per month, 20 billion Internet searches per month, and millions of networked sensors connected to mobile phones, energy meters, automobiles, shipping containers, retail packaging and more. Big Data is a platform for transforming all of this data into actionable items for business decision making.,The barriers to entry for Big Data analytics are rapidly shrinking. Big Data cloud services like Amazon Elastic MapReduce and Microsoft?€?s Hadoop distribution for Windows Azure allow companies to spin up Big Data projects without upfront infrastructure costs and allow them to respond quickly to scale-out requirements. Commercial vendor support from companies like Cloudera can speed development and deliver more value from Big Data projects. Bundled server options such as Oracle?€?s Big Data Appliance offer fast setup and scale-out solutions. Finally, modular data center designs are emerging as a way to efficiently manage hardware and scale-out rapidly and cost-effectively.,Companies likely to get the most out of Big Data analytics include:
I am a big fan of Ooyala, even though I cannot pronounce the name of the company properly! Here is a link to a video that has helped me with my pronunciation ailment. ,Ooyala has a great report available called "Video Index Report" where "analytics meets big data."?? Executive Summary and key findings below:,Ooyala?€?s technology helps media companies and marketers around the globe manage, monetize, deliver and analyze their videos across a variety of platforms and devices. The information generated each day provides Big Data insights into global viewer engagement, multi-device viewing, content discovery, sharing via social media, and hundreds of other trends and variables., ,This new report marks the first time Ooyala has offered a quarterly overview of the state of online video. The data set is vast: Ooyala handles more than 1 billion analytics pings per day, which reflect the anonymized viewing behavior of over 100 million monthly global unique users. Ooyala distills this data to help publishers make better decisions on how to tap new markets, grow audiences and increase revenue.,The complete report can be downloaded here. 

With the abundance of ?€?big data?€? in the field of analytics, and all the challenges today?€?s immense data volume is causing, it may not be particularly fashionable or pressing to discuss missing values. After all, who cares about missing data points when there are petabytes of more observations out there???,As the objective of any data gathering process is to gain knowledge about a domain, missing values are obviously undesirable. A missing datum does without a doubt reduce our knowledge about any individual observation, but implications for our understanding of the whole domain may not be so obvious, especially when there seems to be an endless supply of data.??,Missing values are encountered in virtually all real-world data collection processes. Missing values could be the result of non-responses in surveys, poor record-keeping, server outages, attrition in longitudinal surveys or the faulty sensors of a measuring device, etc. What?€?s often overlooked is that not properly handling missing observations can lead to misleading interpretations or create a false sense of confidence in one?€?s findings, regardless of how many more complete observations might be available.,Despite the intuitive nature of this problem, and the fact that almost all quantitative studies are affected by it, applied researchers have given it remarkably little attention in practice. Burton and Altman (2004) state this predicament very forcefully in the context of cancer research: ?€?We are concerned that very few authors have considered the impact of missing covariate data; it seems that missing data is generally either not recognized as an issue or considered a nuisance that it is best hidden.?€???,As missing values processing (beyond the na??ve ad-hoc approaches) can be a demanding task, both methodologically and computationally, the principal objective of this paper is to propose a new and hopefully easier approach by employing Bayesian networks. It is not our intention to open the proverbial ?€?new can of worms?€?, and thus distract researchers from their principal study focus, but rather we want to demonstrate that Bayesian networks can reliably, efficiently and intuitively integrate missing values processing into the main research task.
When you do a book search on Amazon using keywords, in the search result page you see 16 results. If you've bought one (or more) of these books, it still shows up on the search result page. It would be easy to eliminate that book that you already purchased (e.g. by showing it at the bottom of the page), and replacing it by another book that you haven't purchased yet. Sounds to me like an obvious, simple optimization trick. Maybe Amazon's statisticians use very complex models that change parameters of their search relevancy algorithms??in real time based on thousands of variables, but fail to identify a simple change that should provide possibly a 1% lift to these particular users. At least they should A/B test my idea, if it is not too difficult to implement.,On a different note, they should refine their algorithms used to detect authors purchasing their own books, or authors paying friends to post good reviews for their books / bad reviews for competing books, or any type of fake reviews. Buying your own book is an easy way to get better ranking - just like clicking on your own ads on Google improves your position, as long as it is performed via 3rd parties to avoid detection. More on this later.,:

"Ben Rooney, of WSJ Europe, recently posted a quick interview with EMC's Pat Gelsinger discussing Big Data. In my opinion, Gelsinger is one of the most thoughtful (and successful) high tech execs in the entire industry. His perspective on Big Data is spot on.,Read the blog and watch the video??, And/or watch the video below!,
"
J.D. Opdyke and Alex Cavallo,In operational risk measurement, the estimation of severity distribution parameters is the main driver of capital estimates, yet this remains a non-trivial challenge for many reasons.????Maximum likelihood estimation (MLE) does not adequately meet this challenge because of its well-documented non-robustness to modest violations of idealized textbook model assumptions, specifically that the data are independent and identically distributed (i.i.d.), which is clearly violated by operational loss data.????Yet even under i.i.d. data, capital estimates based on MLE are, on average, biased upwards, sometimes dramatically, due to Jensen?€?s inequality.?? This overstatement of the true risk profile increases as the heaviness of the severity distribution tail increases, so dealing with data collection thresholds by using truncated distributions, which have thicker tails, increases MLE-related capital bias considerably.????In addition, truncation typically induces or augments correlation between a distribution?€?s parameters, and this exacerbates the non-robustness of MLE.????This paper derives influence functions for MLE for a number of severity distributions, both truncated and not, to analytically demonstrate its non-robustness and its sometimes counterintuitive behavior under truncation.?? Empirical influence functions are then used to compare MLE against robust alternatives such as the Optimally Bias-Robust Estimator (OBRE) and the Cram??r-von Mises (CvM) estimator.?? The ultimate focus, however, is on the economic and regulatory capital estimates generated by these three estimators.?? The mean adjusted single-loss approximation (SLA) is used to translate these parameter estimates into Value-at-Risk (VaR) based estimates of regulatory and economic capital.?? The results show that OBRE estimators are very promising alternatives to MLE for use with actual operational loss event data, whether truncated or not, when the ultimate goal is to obtain accurate (non-biased) and robust capital estimates.



Services like social networks, web analytics, and intelligent e-commerce often need to manage data at a scale too big for a traditional database. Complexity increases with scale and demand, and handling big data is not as simple as just doubling down on your RDBMS or rolling out some trendy new technology. Fortunately, scalability and simplicity are not mutually exclusive?€?you just need to take a different approach. Big data systems use many machines working in parallel to store and process data, which introduces fundamental challenges unfamiliar to most developers.,??teaches you to build these systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy to understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to implement them in practice, and how to deploy and operate them once they're built.,??shows you how to build the back-end for a real-time service called SuperWebAnalytics.com?€?our version of Google Analytics. As you read, you'll discover that many standard RDBMS practices become unwieldy with large-scale data. To handle the complexities of Big Data and distributed systems, you must drastically simplify your approach. This book introduces a general framework for thinking about big data, and then shows how to apply technologies like Hadoop, Thrift, and various NoSQL databases to build simple, robust, and efficient systems to handle it.,This book requires no previous exposure to large-scale data analysis or NoSQL tools. Familiarity with traditional databases is helpful.,??is an engineer at Twitter. He was previously Lead Engineer at BackType, a marketing intelligence company, that was acquired by Twitter in July of 2011. He is the author of two major open source projects: Storm, a distributed realtime computation system, and Cascalog, a tool for processing data on Hadoop. He is a frequent speaker and writes a blog at nathanmarz.com.,??is an engineer at Twitter who uses Cascalog and ElephantDB to process and analyze many terabytes of data in near real-time. He is also the lead developer on FORMA, an open-source deforestation monitoring system in use by a number of top research institutions. He is a committer on Cascalog, ElephantDB, Pallet and a number of other open source Clojure projects.,This Early Access version of??,??enables you to receive new chapters as they are being written. You can also interact with the authors to ask questions, provide feedback and errata, and help shape the final manuscript on the??,??to read more content when it is released and to receive news about this book.
Accretive Health,Chicago, IL.,??(NYSE: AH) is looking for PHDs and PHD students from top schools with a passion for machine learning, algorithms, AI, data mining and bioinformatics to solve a massive problem.,We want to talk with you about tackling one of the most complex and compelling issues in this country: the need to reduce healthcare costs while increasing the quality of patient care. I'm hoping you're intrigued by this challenge.,We have amassed close to 1B data sets of insurance claims over the last several years and intend to mine them. We think you could play a critical role in leveraging the data to solve this problem. You'll be working with other brilliant PHDs and PHD students from top schools.,Accretive Health is the leading provider of Revenue Cycle Management expertise in this country. Using our proprietary technology, we help hospital systems reduce their financial "leakage" and increase profitability. We are an 8 year old company that is the market leader in a high growth and dynamic industry. We went public in 2010, in a market that virtually was non-existent, and have secured close to 80% returns for our original investors. Our year over year growth was 35% from 2010 to 2011. Click on the following url to learn more about Accretive Health:??,.,Please contact me at the address below to discuss your passions and our project in great detail.,Lori,Lori Brown, Vice President,Accretive Health, Inc. | 401 N. Michigan Avenue, Suite 2700 | Chicago, IL 60611,M: 312/543/0567| lobrown@accretivehealth.com |??
I often get asked about the relevance of "Big Data" for not-so-big companies. A recent blog post at theinfobloom.com by Brandy Courtade talks about something that I believe in for all "Big Data" participants: mindset, and the importance of having a new one!,Read the entire post??
Industry thought leaders:??
The recent story in New York Times by Charles Duhigg: , - which detailed Target success in using analytics to identify pregnant women from changes in their buying habit - has spread like a wildfire across media.???? The reaction from non-technical people was mostly negative, although it was not clear what exactly Target did wrong, and why Target actions are different from what Facebook, Google, or any other large company - all of which analyze their customers and try to predict (and influence) their behavior.,A typical comment I saw was:,Are there some personal things we don't want to be predicted? If so, then people who want privacy should stay off computers and pay cash everywhere, since we now live in a very digital, connected world, and leave many digital "crumbs" from which behavior can be predicted.??,Some people can be identified even in anonymous data. The second Netflix prize was cancelled because researchers have found how to identify several participants (,) even though the data was completely anonymized.?? AOL release of its anonymized search logs led to a similar ,.,I am not sure where the line is in predicting personal behavior, but the trend in the US has been towards less privacy. The privacy laws are much stricter in EU than in the US. However the Facebook generation, even in EU, has become much more comfortable is publishing their private information online.,Was Target wrong??? Vote in the current KDnuggets Poll which is asking

The folks at Ooyala continue to impress with their quarterly Video Index Report. Checkout Data Scientist, Matt Pisienski's perspective. Oh, and stay tuned until the end and watch the outtakes --that's where the real story is told!,.


What is the best Excel to HTML converter? I've found Tableizer at??,, it's free and it's a web app. But I'm wondering if there are other convenient solutions.,More generally, what is the best tool for format conversion (that can do ALL conversions without destroying the original format, such as changing a date into a number or worse, text format)?
 , 
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??

As the amount of digital information generated by businesses and organizations continues to grow exponentially, a challenge ?€?or as some have put it, a crisis?€?has developed.,There just aren?€?t enough people with the required skills to analyze and interpret this information?€?transforming it from raw numerical (or other) data into actionable insights ?€? the ultimate aim of any Big Data-driven initiative.,??recently carried out by researchers at Gartner??found that more than half of the business leaders they queried felt their ability to carry out analytics was restricted by the difficulty in finding the right talent.,Overcoming this problem is a challenge that all companies will have to face, and market leaders?€?aware that they have more to lose than many by falling behind in the race to keep up with technology?€?have come up with some innovative solutions.,Walmart decided to apply one of the fundamental weapons in the Big Data arsenal?€?crowdsourcing?€?to the problem, with positive results.,Last year, they turned to crowdsourced analytics competition platform Kaggle. At Kaggle, an army of ?€?armchair data scientists?€? apply their skills to analytical problems submitted by companies, with the designer of the best solution being rewarded ?€? sometimes financially, in this case with a job.,Mandar Thakur, senior recruiter for Walmart?€?s Technology division, told me ?€?The Kaggle competition created a buzz about Walmart and our analytics organization. People always knew that Walmart generates and has a lot of data, but the best part was that this let people see how we are using it strategically.?€?,Candidates were provided with a set of historical sales data from a sample of stores, along with associated sales events, such as clearance sales and price rollbacks. They were asked to come up with models showing how these events would affect sales across a number of departments.,As a result, several people were hired into the analytics team, and the competition was held again this year. This time candidates were asked to predict how weather would impact sales of different products.,This crowdsourced approach led to some interesting appointments of people who, as Thakur says, wouldn?€?t have been considered for an interview based on their resumes alone. One for example had a very strong background in physics but no formal analytics background. ?€?He has a different skillset ?€? and if we hadn?€?t gone down the Kaggle route, we wouldn?€?t have acquired him.?€?,Kaggle?€?s competitions are open to anyone ?€? so what kind of person does an organization look for, when considering how to fill vacancies which could be vital to the future growth of the business?,?€?Fundamentally,?€? says Thakur, ?€?we need people who are absolute data geeks?€?people who love data, and can slice it, dice it and make it do what they want it to do.,?€?Having said that, there is one very important aspect we look for, which perhaps differentiates a data analyst from other technologists. It exponentially improves their career prospects if they can match this technical, data-geek knowledge with great communication and presentation skills.?€?,In other words, as well as being able to wring unexpected and game-changing insights from the unlikeliest of data, they must be able to explain them to a room full of (often non-technical) business executives and marketing suits.,?€?Someone who has this combination of skills can rise to the top very quickly?€?, explains Thakur.,And by ?€?the top?€?, he does mean the top ?€? career pathways are in place for a new hire beginning an entry-level role as a data science associate to rise to senior director level, becoming an Enterprise??Decision Scientist guiding the overall company direction through statistical analysis.,Physicists aside, most new recruits still come with a ?€?traditional?€? background in the academic disciplines required for data analysis?€?statistics, mathematics, computer science and business analytics.,A working knowledge of Python??or R???€?two of the programming languages most commonly used for analyzing large digital datasets, is also usually expected. The biggest challenge can be finding candidates with experience in the most cutting edge analytics applications, such as those involving machine learning. Many people will not have the opportunity to learn this at school, and experts are often self-taught.,Walmart also recently ran a recruitment campaign across social media using the Twitter??hashtag #lovedata, to raise its profile among the online data science community and avert their eyes away from Silicon Valley and towards Bentonville, Arkansas, when scoping for job opportunities.,Analytics and big data is now integrated into every vertical within Walmart, so once on the team, every new staff member with these responsibilities takes part in the Analytics Rotation Program?€?spending a period within every department to get an overview of how analytics is used across the company.,?€?This allows them to combine their analytical knowledge?€?whether they have gained it in education or in work experience?€?and helps to assimilate that knowledge with what Walmart is doing in different pockets of their business. Once they are on the ground running, they can run in the right direction,?€? says Thakur.,Another valuable source of job applicants is referrals. Data fans?€?academic, industrial or armchair?€?are as active in online communities and social networks as any other breed of techies. Provide one with a great job, and he or she is likely to spread the word to their associates.,The company?€?s ramping-up of recruitment into its analytical team coincides with the launch of Walmarts attempt to build the largest ever private cloud. This huge database will hold all of the data gathered from its online operations and more than 11,000 bricks and mortar stores. Once completed it is estimated that more than 40 petabytes of information will be collected, and available for analysis, every day.,This will undoubtedly lead to an unprecedented demand for people with the skills to turn this data into actionable insights, and to explain to others how to do so.,Thakur says ?€?The staffing supply and demand gap is always there, especially when it comes to emerging technology.,?€?We have found innovative and creative ways to go about finding talent to carry out our data science and analytics, however we are always, always, always on the lookout for people who can come in, contribute and catapult our organization even further.?€?
 
Our entire family loves going to Olive Garden and every time we go, we order the same items from the menu!,Having access to the Olive Garden nutrition data is like stumbling on a pot of data gold, you never know that sinfully tasty food might be the most nutritious dish, or is it?,Let us start with the appetizers,??,So what is the worst combination of drinks + appetizers and main course?,If you choose the above food combination ,,,A whopping 3k calories + 1k calories from Fat = 4k total calories and 180 grams of Carbs + 100 grams of Fat.,Know this fact that no matter what discount coupon you use to get the above combo deal, the final Calorie bill is not going to be any trimmer!,Check the complete analysis here??
In this post, I propose that IoT analytics should be a part of 'Smart objects' and discuss the implications of doing so,The term ?€?Smart objects?€? has been around from the times of??,.,However, as we have started building Smart objects, I believe that the meaning and definition has evolved.,Here is my view on how the definition of Smart Objects has changed in the world of Edge Computing and increasing processing capacity,At a minimum, a smart Object should have 3 things,a) An Identity ex ipv6,b) Sensors / actuators,c) A radio (Bluetooth / cellular etc),In addition, a smart object could incorporate,a) Physical context ex location,b) Social context ex proximity in social media,To extend even more, Smartness could incorporate analytics,Some of these analytics could be performed on the device itself ex computing at the edge concept from Intel, Cisco and others.,However, Edge Computing as discussed today, still has some??,For example:,a)???????? The need to incorporate multiple feeds from different sensors to reach a decision ?€?at the edge?€?,b) ?? ??The need for a workflow process i.e. actions based on readings ?€? again often at the edge with it?€?s accompanying security and safety measures,To manage multiple sensor feeds, we need to understand concepts like??,??(source freescale).,We already have some rudimentary workflow through mechanisms like??,In addition, the rise of CPU capacity leads to greater intelligence on the device ?€? for example??,So, in a nutshell, its a evolving concept especially if we include IoT analytics in the definition of Smart objects (and that some of these analytics could be performed at the Edge) ??..,We cover these ideas in the??,??and also at the courses I teach at Oxford University,Comments welcome
The idea of "big data" could conjure up Turing-like images of massive cloud storage centers humming away in a dessert. It's true that up to this point, computing power has been a barrier to analytics, research, and the exploitation of big data. It takes a lot of computing power to process increasingly complicated and elaborate data sets. Companies, therefore, must make choices about which data to analyze, and the extent to which they can dive in. An incomplete view of your analytics can lead directly to false conclusions.,Fortunately, there are options. Advancements in analytics and big data software now mean that people who are interested in using big data to their advantage can run interactive queries on commercially-available laptops.,The key to understanding any sort of analytics or data is not the quantity, ,. Very large data sets allow us to determine a correlation between two seemingly uncorrelated things. But big data sets also give us a whole lot of noise, and analysts can waste a lot of time trying to correlate two things that don't relate to each other.,If we define "big" data as all of the data you can possibly get your hands on -- integrating every data source you have -- then you will always require massive computing power, and you will spend a lot of time chasing dead ends. But it will certainly be big. The biggest companies in the world who have the time and manpower to chase all of these ideas can make some nice infographics. However, a lot of that information probably won't lead to actionable insights.,If we define "big" data as being able to use as much data as you need to derive actionable insights for your company to improve your product or bottom line, then we're actually talking about a much more manageable amount of information, one that can easily be processed on a laptop, thanks to advances in software like BigObject Analytics. Reducing the dependency on running complicated distribution systems for big data analysis means more flexibility for companies, and they can get a wider view of their overall analytics.,Data and analytics are about insights. Here at BigObject we believe that the ability to make cross-connections is the key to turning information into knowledge. In order to calculate connections and correlations, we developed something called "Cross-Link" that drives our BigObject Analytics platform. Data is captured and structured in BigObject in a hierarchical way that users can quickly have a "view" of what the data looks like in different dimensions. The language used in BigObject Shell is similar to SQL statements, which is the most commonly used query language. And it all happens on your laptop, making it possible to process billions of lines of data. Imagine that!,BigObject uses Docker to as the primary delivery method, meaning that it is kept up-to-date no matter which operating system you're running.??
A lot of interesting images can be found on Google. You can search for machine learning cartoons, fake data scientists, Excel maps or any keyword, and get a bunch of interesting images or charts, though the images barely change over time (Google algorithms are very conservative).,Anyway, here's some really interesting stuff. It definitely proves how popular infographics are, and the growth of big data. Many of these infographics are of high quality, well thought out and based on real research.,To see / access all of them at once, ,??then select "Image" rather than "Web" search. Or ,. More infographics ,.,??
"In fact men will fight for a superstition quite as quickly as for a living truth - often more so, since a superstition is so intangible you cannot get at it to refute it, but truth is a point of view, and so is changeable.",On the 1st of July, I decided to set up a professional group on LinkedIn in order to create a hype free Agora for Big Data dialogue. I called the group??,??and although it is a closed group, all those with interest in an open, informed and honest exchange of ideas on data, from whatever angle they are coming from, are very welcome to join in. (URL:?? ,),So, why is the group called??,??and not something more generic, such as The Data Contrarians?,It was a hard call, but in the end, I decided to go with the spirit of the times, for as overhyped and laden down with tripe as it might be, and the name stuck. This doesn't mean that the group is limited to airing views, opinion and speculation just about Big Data. There's a wealth of related data out there, and it's not all as big as Google's stash. Indeed, one of my pet projects for the world of Big Data and the Internet of Things is in discovering, designing and developing ways to reduce the Big Data footprint, early and often, without losing the essence of what the data might be able to tell us.,I believe that??,??group fulfils a number of useful functions:,Of course, these are simply suggestions, and what may not be false today, may be true sometime in the future, and vice versa. Nevertheless, that encapsulates the general spirit.,As with other groups, we can all contribute discussions, comments, links to other material advertise jobs and promote what our businesses, and what we ourselves are doing.,In particular, I would be interested in hearing the facts about real-life Big Data success stories. I want to see things that have some value, not just ambiguous references to what might work or allusions to success stories that are just too successful to talk about in anything but the vaguest or the most venal, pompous or preposterous of terms. I want simple, clear and coherent, and I'm sure I'm not alone in this desire. I want to hear:,I like these seven bullet-points, because they are essentially the types of things I can talk about when it comes to Data Warehousing and Risk Management and Reporting. I also think that the time has come for us to move on to less prosaic and woolly allusions to Big Data successes, and indeed share the skinny on both real Big Data successes and real Big Data failures.,Because that's how we all progress, right?
Quick chart comparing Bigfoot vs UFO news/search activity.,Bigfoot and UFO remain elusive but know their ways to make news from time to time.,Looking at the Google Trends data for both the entities we clearly see the spikes in their news activities.,??details to see what caused the spike in the year 2008 for Bigfoot
Infographic based on the article:
---
One question I get asked a lot by my clients recently is: Should we go for Hadoop or Spark as our big data framework? Spark has overtaken Hadoop as the most active open source Big Data project. While they are not directly comparable products, they both have many of the same uses.,In order to shed some light onto the issue of ?€?Spark versus Hadoop?€? I thought an article explaining the essential differences and similarities of each might be useful. As always, I have tried to keep it accessible to anyone, including those without a background in computer science.,Hadoop and Spark are both Big Data frameworks ?€? they provide some of the most popular tools used to carry out common Big Data-related tasks.,, for many years, was the leading open source Big Data framework but recently the newer and more advanced Spark has become the more popular of the two Apache Software Foundation tools.,However they do not perform exactly the same tasks, and they are not mutually exclusive, as they are able to work together. Although??,??is reported to work up to 100 times faster than Hadoop in certain circumstances, it does not provide its own distributed storage system.,Distributed storage is fundamental to many of today?€?s Big Data projects as it allows vast multi-petabyte datasets to be stored across an almost infinite number of everyday computer hard drives, rather than involving hugely costly custom machinery which would hold it all on one device. These systems are scalable, meaning that more drives can be added to the network as the dataset grows in size.,As I mentioned, Spark does not include its own system for organizing files in a distributed way (the file system) so it requires one provided by a third-party. For this reason many Big Data projects involve installing Spark on top of Hadoop, where Spark?€?s advanced analytics applications can make use of data stored using the Hadoop Distributed File System (HDFS).,What really gives Spark the edge over Hadoop is speed. Spark handles most of its operations ?€?in memory?€? ?€? copying them from the distributed physical storage into far faster logical RAM memory. This reduces the amount of time consuming writing and reading to and from slow, clunky mechanical hard drives that needs to be done under Hadoop?€?s MapReduce system.,MapReduce writes all of the data back to the physical storage medium after each operation. This was originally done to ensure a full recovery could be made in case something goes wrong ?€? as data held electronically in RAM is more volatile than that stored magnetically on disks. However Spark arranges data in what are known as Resilient Distributed Datasets, which can be recovered following failure.,Spark?€?s functionality for handling advanced data processing tasks such as real time stream processing and machine learning is way ahead of what is possible with Hadoop alone. This, along with the gain in speed provided by in-memory operations, is the real reason, in my opinion, for its growth in popularity. Real-time processing means that data can be fed into an analytical application the moment it is captured, and insights immediately fed back to the user through a dashboard, to allow action to be taken. This sort of processing is increasingly being used in all sorts of Big Data applications, for example recommendation engines used by retailers, or monitoring the performance of industrial machinery in the manufacturing industry.,Machine learning ?€? creating algorithms which can ?€?think?€? for themselves, allowing them to improve and ?€?learn?€? through a process of statistical modelling and simulation, until an ideal solution to a proposed problem is found, is an area of analytics which is well suited to the Spark platform, thanks to its speed and ability to handle streaming data. This sort of technology lies at the heart of the latest advanced manufacturing systems used in industry which can predict when parts will go wrong and when to order replacements, and will also lie at the heart of the driverless cars and ships of the near future. Spark includes its own machine learning libraries, called MLib, whereas Hadoop systems must be interfaced with a third-party machine learning library, for example Apache Mahout.,The reality is, although the existence of the two Big Data frameworks is often pitched as a battle for dominance, that isn?€?t really the case. There is some crossover of function, but both are non-commercial products so it isn?€?t really ?€?competition?€? as such, and the corporate entities which do make money from providing support and installation of these free-to-use systems will often offer both services, allowing the buyer to pick and choose which functionality they require from each framework.,Many of the big vendors (i.e Cloudera) now offer Spark as well as Hadoop, so will be in a good position to advise companies on which they will find most suitable, on a job-by-job basis. For example, if your Big Data simply consists of a huge amount of very structured data (i.e customer names and addresses) you may have no need for the advanced streaming analytics and machine learning functionality provided by Spark. This means you would be wasting time, and probably money, having it installed as a separate layer over your Hadoop storage. Spark, although developing very quickly, is still in its infancy, and the security and support infrastructure is not as advanced.,The increasing amount of Spark activity taking place (when compared to Hadoop activity) in the open source community is, in my opinion, a further sign that everyday business users are finding increasingly innovative uses for their stored data. The open source principle is a great thing, in many ways, and one of them is how it enables seemingly similar products to exist alongside each other ?€? vendors can sell both (or rather, provide installation and support services for both, based on what their customers actually need in order to extract maximum value from their data.
Information about provided services, customers and transactions can be stored in different database systems and data warehouses, depending on the way in which a company operates.,Due to such arrangements, even the simplest analyses or report may require??,For an analyst this situation is frequently the source of difficulties ?€???,In a situation like this it is convenient to build Data Marts.??,??This data is pulled from multiple sources, processed in a uniform manner, documented and optimized.,Data Marts frequently contain data aggregated on the customer level, such as the average number of transactions in last 6 months, number of cash loans drawn by the client during the last 12 months, etc. When the computed aggregate values are available, it is much easier to prepare reports.,Data Marts are built only once, at the start of the analytical process, but they are cyclically and automatically updated, so as to contain??,Fig. 1. Data Mart in an analytical project ?€? an example,Building a Data Mart can be especially useful in corporate projects, where there are numerous different distributed data sources and the amount of data is very large. This was the case in one of our projects. The company had??,The data structure??,??It was necessary to build an analytical Data Mart, so that all the information required to build the models would be available in one place. In effect:,Moreover, the ability to update the Data Mart on a daily basis enabled us to use it for marketing campaigns. For constructing the Data Mart in this project we used our own??,.,Depending on the business requirements,??,. Before deciding how many and what kind of Data Marts to build, it is necessary to formulate concrete business requirements. In other words, one must determine what information is needed and what kinds of analyses, models and reports will be made.,An analytical Data Mart is??,. It may be a??,, in order to facilitate advanced analyses for e.g. risk assessment, automation of data quality control, and verification of the effectiveness of deployed analytical models.,The original blog can be viewed 


For statistical process control, a number of single charts that jointly monitor both process mean and variability recently have been developed. For quality control-related hypothesis testing, however, there has been little analogous development of joint mean-variance tests: only one two-sample statistic that is not computationally intensive has been designed specifically for the one-sided test of Ho: Mean2<=Mean1 and StDev2<=StDev1 vs. Ha: Mean2>Mean1 OR StDev2>StDev1 (see Opdyke, 2006). Many have proposed other commonly used tests, such as tests of stochastic dominance, exceedance tests, or permutation tests for this joint hypothesis, but the first can exhibit prohibitively poor type I error control, and the latter two can have virtually no power under many conditions. This paper further develops and generalizes the maximum test proposed in Opdyke (2006) and demonstrates via extensive simulations that, for comparing two independent samples under typical quality control conditions, it a) always maintains good type I error control; b) has good power under symmetry and modest power under notable asymmetry; and c) often has dramatically more power and much better type I error control than the only other widely endorsed competitor. The statistic (OBMax2) is not computationally intensive, and although initially designed for quality control testing in regulatory telecommunications, its range of application is as broad as the number of quality control settings requiring a one-sided, joint test of both the mean and the variance.



From MIT's Technology Review -??
Spam is a kind of messaging where the cost of sending is usually negligible and the receiver and the ISP pays the cost in terms of bandwidth usage.??,An example of a manual approach to detecting spam is using knowledge engineering. When you are aware of what is spam and what is not, you can usually filter it by creating a set of rules like,, is a blog post about one such implementation. However, a manual rules based approach doesn't scale because of active human spammers circumventing any manual rules. Therefore a machine learning related approach is necessary.,Machine learning based approaches don?€?t need specifying rules explicitly instead you need a decent amount of data pre classified as spam and not spam. You can use specific algorithms to learn rules to classify the data.??,??,An important problem in using the machine learning algorithms is that most of the algorithms can only classify numerical objects like vectors to overcome this we usually convert text data into vectors of numbers expressing certain features in the message. It is to be noted that more than the algorithm being used the features you choose determine the success and failure rate of the filter to a large extent.??,Paul Graham articulated the need for a machine learning approach in his seminal essay called ",". The approach used Bayesian classifiers on a bag of words features to classify the text. Naive bayesian techniques correlate spam and ham mail to different tokens in the email and use bayesian formula to calculate the probability of an email received being ??spam. The tokens can be anything, like words in the mail or words in header and html of the mail or phrases in the text.??,??explains about his spam filtering technique. Gary ,.??,Another simple method is the??k Nearest Neighbors Classifier where??a text is classified as spam or not spam based on the majority vote of K nearest neighbours. The algorithm requires pre classified feature vectors. All the vectors are just stored. To classify a new text document, its feature vector is extracted and its distance from every vector in the training set is calculated and its assigned to the class of the majority members of the K nearest neighbours. If K=1 it is simply assigned to the class of the nearest neighbour and if K=3 it is assigned to a class of majority of 3 different neighbors. Here is an example of how this algorithm can be used to check for spam ,:,Given a message x, determine its k nearest neighbors among the??messages in the training set. If there are more spams among these??neighbors, classify given message as spam. Otherwise classify it as??legitimate mail.
I asked myself this question a??few months ago. Next I thought: What is the definition of Data Science? So the??,hands on and also lookup definitions of related topics such as Data Mining and??Machine Learning. Looking at the discussions and posts around Data Science it??, ,This does not really help to??
Big data is a new marketing term that highlights the everincreasing and exponential growth of data in every aspect of our lives. The term big data originated from within the open-source community, where there was an effort to develop analytics processes that were faster and more scalable than traditional data warehousing, and could extract value from the vast amounts of unstructured and semistructured data produced daily by web users. Consequently, big data origins are tied to web data, though today big data is used in a larger context. Read more??,??......,Infographic Source, ",";??
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,: ??,??,??|??,??|??,??|??,??|??,??|??,??|??
This is an interesting article recently published in ,. The author gathered data from Glassdoor.com, to rank companies. Glassdoor.com is a website where employees make comments about, and rate their company, and can even post their job title and salary range. Keep in mind that the author is not a statistician, and his analysis is based on user-generated content, which, like all reviews, can be fake. Yet the author removed companies with less than 10 reviews, to make this table more robust. So if your start-up is not listed, it's not because it's not in the top, but because there are too few ratings to make a sound conclusion about it.,The picture below show companies at the very top. ,??for full list and comments by the author, and to check out if some important companies are missing. Or you may create your own table, scraping Glassdoor's reviews (or hiring an intern) to produce your analysis.,Below is ,??(screenshot), showing summary stats for two companies.??Why not create your own API for company rankings, and even sell your collected data in bulk, to interested organizations? That would be a nice job for a data scientist, with potential revenue stream!
??,??,??,??,??,??,??,??,??
 , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 
??,C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier.,C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set S = {s_1, s_2, ...} of already classified samples. Each sample s_i consists of a p-dimensional vector (x_{1,i}, x_{2,i}, ...,x_{p,i}) , where the x_j represent attributes or features of the sample, as well as the class in which s_i falls.,At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists.,??, ,??, ,??,??,??,??,??,??,??,??, A continuously updated list of Python resources for these algorithms is available on??,.??
What is the semantic layer you ask?,The semantic layer is the underpinning of modern business intelligence platforms. Vendors with robust semantic capabilities command more than 50% of a $14 billion plus market:,This data actually understates the actual market share and wide spread usage of semantic layers within enterprise BI shops. Several BI tools in the "Others" category, like Birst also offer solid semantic capabilities. The real number might be closer to 60% or even higher.,So yeah, it's a big deal, worth at least $7 billion. It was pioneered by Business Objects in the early 90's well before the acquisition by SAP and they still remain the market leader in BI after all these years.,Back to the original question - what is the semantic layer exactly???,:,Close, but I think thats too high level and simplistic. Here is my definition:,Semantic layers map concepts like Customer, Product, Clicks, and Revenue to a set of data transformation rules, aggregation rules, datasources, tables, and columns, allowing users to ask novel questions without knowing anything about the underlying structure of data. There are three big reasons why enterprise IT loves the semantic layer:,Okay, if you are still reading this, you are either a really good friend, a data nerd, or a very curious individual. In any case, thanks and congratulations you're about to peek into the future.,Now for some bad news for the semantic layer fans (including myself in this group). After a decade and a half, we are seeing major cracks in this strategy and sometimes downright collapse. End users are running away en masse from centrally managed BI tools and the semantic layer is partly to blame. The proof is simply in the meteoric rise of Tableau who once called enterprise BI a??,. That brick for better or worse is the semantic layer. Here are three reasons end users are fleeing from enterprise BI and semantic layers:,It's time to unleash the semantic layer. The great unbundling of the BI stack is already underway with Visualizations out of the gate first. The next to go will be the Semantic Layer.,Startups like mine, Vero Analytics, are already working hard to solve the many problems I described above while preserving the benefits of a semantic layer. Over the next months and years you will see the emergence of a new category of tools under the banner of??,??Like Visualization Tools, this new category will target business units and end users with lower learning curves, intuitive user interfaces, and a collaboration model that makes sense for the 21st century. It will at once enable transparency of what and how Revenue should be calculated, while allowing those rules to seamlessly evolve over a collaborative framework.,These new tools will marry reporting, analytics, and data preparation into one workflow. This means users will be able to self service questions like "Who are my Top 10 Customers", then follow it up by enriching, correcting, and transforming that dataset. All the while adding and discovering new semantic concepts that becomes shareable and reusable across the whole organization.,The enterprise BI stack will start to change with a bring your own visualization tool strategy at the top level. Each business unit will and should be free to choose their preferred visualizations toolkit even if that means sticking with excel. This tier of visualizations will sit on top of the Semantic Data Prep tier with direct integration from visualization tools. Here users will define data sets, build pipelines, collaborate on semantic definitions, and possibly even cache datasets.,I am looking forward to this future where truth and creativity lives together.
This is a brief overview of my paper ?€?,,?€? which I?€?ll be presenting on June 8, at the , workshop at ,.?? The paper provides a novel method for extrapolating a precision-recall point to a different level of recall, and advocates making performance comparisons by extrapolating results for all systems to the same level of recall if the systems cannot be evaluated at exactly the same recall.,Recall, R, is the proportion of the relevant documents retrieved by the information retrieval (IR) system, and precision, P, is the proportion of retrieved documents that are relevant.?? It is sometimes desirable to have high recall while also having high precision in order to find most of the relevant documents without having a lot of non-relevant documents mixed in, but higher recall is usually accompanied by lower precision.,Some IR systems generate??a , for each document, allowing the documents to be sorted so that the ones that are deemed most likely to be relevant appear at the top of the list.?? A precision-recall curve can be generated for such systems by computing the precision and recall for each possible point in the sorted list, viewing the documents above that point as being the retrieved documents.?? Unfortunately, we sometimes need to compare performance of IR systems without knowing the entire precision-recall curve.?? For example, some systems only provide a binary yes/no relevance prediction instead of a relevance score, so we have only a single precision-recall point for such systems.,Making judgments about the relative performance of two IR systems knowing only a single precision-recall point for each system is problematic?€?if one system has higher recall but lower precision for a particular task, is it the better system for that task?,There are various performance measures like the F, score that combine precision and recall into a single number to allow performance comparisons:,F,??= 2*P*R/(P + R),The value of the F, score will be between R and P, and it tends to be closer to the smaller of the two, so it is impossibly to get a large F, if either the precision or the recall is small.,Unfortunately, such measures often assume a trade-off between precision and recall that is not appropriate for the actual information need (I?€?ve written about problems with the?? F, score ,).?? To understand the problem, it is useful to look at how??F, varies as a function of the recall where it is measured.?? Here are two precision-recall curves, with the one on the left being for an easy categorization task and the one on the right being for a hard task, with the??F, score corresponding to each point on the precision-recall curve superimposed:,If we pick a single point from the precision-recall curve and compute the value of F, for that point, the resulting??F, is very sensitive to the precision-recall point we choose.?? F, is maximized at 46% recall in the graph on the right, which means that the trade-off between precision and recall that??F, deems to be reasonable implies that it is not worthwhile to retrieve more than 46% of the relevant documents for that task because precision suffers too much when you push to higher recall (i.e., when you go farther down the list of documents sorted by relevance score to find additional relevant documents you encounter too many non-relevant ones).?? That is simply not compatible with the information need in some situations.,For example, a litigant can request that its opposition or a third party be compelled by the court to produce documents that satisfy some relevance criteria so that evidence that may resolve the dispute can be discovered. When electronic documents are involved this process is called e-discovery.?? High recall (e.g., 75% or more) is required to satisfy the court.?? High precision is desirable to reduce the number of non-relevant documents that will undergo expensive human review before the documents are turned over to the requesting party.?? When determining the recall that is necessary the court will take cost into account, but it does so in light of the value of the case and the likelihood that e-discovery will turn up useful evidence?€?this is called "proportionality."?? The F, score cannot possibly strike the right balance between recall and precision because it is completely oblivious to the value of the case.?? Other problems with the??F, score are detailed in the paper.,The strong dependence that??F, has on recall as we move along the precision-recall curve means that it is easy to draw wrong conclusions about which system is performing better when performance is measured at different levels of recall.?? This strong dependence on recall occurs because the contours of equal??F, are not shaped like precision-recall curves, so a precision-recall curve will cut across many contours.???? In order to have the freedom to measure performance at recall levels that are relevant for e-discovery without drawing wrong conclusions about which system is performing best, the paper proposes a performance measure that has constant-performance contours that are shaped like precision-recall curves, so the performance measure depends much less on the recall level where the measurement is made than??F, does. In other words, the proposed performance measure aims to be sensitive to how well the system is working while being insensitive to the specific point on the precision-recall curve where the measurement is made.?? This graph compares the constant-performance contours for??F, to the measure proposed in the paper:,Since the constant-performance contours are shaped like typical precision-recall curves, we can view this measure as being equivalent to extrapolating the precision-recall point to some other target recall level, like 75%, by simply finding an idealized precision-recall curve that passes through the point and moving along that curve to the target recall.?? This figure illustrates extrapolation of precision measurements for three different systems at different recall levels to 75% recall for comparison:,Finally, here is what the performance measure looks like if we evaluate it for each point in the two precision-recall curves from the first figure:,The blue performance curves are much flatter than the red??F, curves from the first figure, so the value is much less sensitive to the recall level where it is measured.?? As an added bonus, the measure is an extrapolated estimate of the precision that the system would achieve at 75% recall, so it is inversely proportional to the cost of the human document review needed (excluding training and testing) to reach 75% recall.,For more details, including the mathematical model used for the idealized precision-recall curves and some examples of how well the extrapolation works, read ,.
What skills are in demand in 'Analytics' domain ?,What are the most popular tools and products in 'Analytics' domain ?

Last year, there was much made of an unconfirmed report that a developer at Google was making $3 million a year. It made many people speculate that the high demand for data scientists coupled with the low supply was creating a salary bubble., couldn?€?t confirm the $3 million developer story, but it did some number crunching from publically available data and determined that the average developer at Google makes around $145,000 per year, including stock and bonuses, while the highest paid senior engineer might make as much as $1 million including stocks and bonuses.,But that?€?s Google. ??What can a data scientist expect to make elsewhere?,According to ,, the current U.S. average salary for a data scientist is $118,709, but it varies widely based on a number of factors.,One major factor that will determine salary levels is your actual title and relevant job skills and responsibilities.,Another factor is the type of company you work for. ??According to an , from 2013:, also has interesting data from anonymously reported salaries that shows averages at different companies. ??For example, data scientists at Groupon right now are reportedly earning the most at an average of $164k while Booz Allen Hamilton is paying closer to $94k for the same job title.,????,Likewise, the type and number of tools a person uses on the job can increase the salary range. Salaries show data scientists who use the Hadoop cluster got paid more; average salary for those who don?€?t use it was $85k compared to $125k for those who use at least six tools or more.,Salaries depend on so many factors that it?€?s impossible to point to a single number and say, ?€?This is what you should be earning.?€? But it can be helpful if you are a data scientist (or are considering the field) to understand where your own salary fits into national averages.,Are you a data scientist or do you employ one? How does this information tally with your experience in the market? I?€?d love to hear your real-world experiences in the comments below.,As always, I am always keen to hear your views on the topic and invite you to comment with any thoughts you might have., , , , , 
"Please watch my video on Aster's principal component analysis or PCA. I not only show how Aster performs this analytic but I attempt to explain how PCA works and explain eigenvectors and eigenvalues. Genre: Statistical Analysis (Unsupervised Learning) Background: A process used to emphasize variability and bring out strong patterns in a dataset. This variability is expressed by principal components; which are directions of highest degree of variance. The first several principal components represent 80-90% of the variance and hence most important.,- Dimensional Reduction / Compression / Image Recognition,- Medical Diagnosis / Medical Imaging / Sensor Data,- Outlier Detection,
"

??,An introductory statistics class will characterize data elements into two classes ?€? those that describe what is being measured, and the measurements themselves.?? Math addresses the measurements.?? The descriptive elements, though, are a muddle of terms and methods.???? Data developers and analysts call them (almost) interchangeably:, Reference data?????????????? , Lookup data?????????????????????????? ??,Foreign Key fields,?? Domain values???????????????? , ??Qualitative data?? ?? ?? ?????? ,Non Ordered Discrete,?? Nominal?????????????????????????????????????? ,?? Categorical?????????????????????????????? ,?? Flags or Sets,??This discussion will call it all reference data, and focus on the volume of this information that is public and commonly shared.?? The article concludes with a list of sources that should be valuable to anyone using public data sets.?? A following article will discuss the uses of these elements for design, analysis, quality and control.,The more companies and nations interchange data, the more they need standards.?? So, it?€?s not surprising that major reference data management gravitates to industry associations and public bodies.?? But, time is always a problem.?? The ?€?diffusion of innovation?€? with its innovators, early adopters and laggards equally applies to companies and countries.??,The authorities and standards of reference data reflect their historical roots as well as these economic tensions.?? Design and use of reference data deserves careful consideration of public standards, both their origins and future integration. Today, these organizations are excellent starting points for public reference data:,??,??,??,??,.
I'm a visual kind of guy. You've probably read various descriptions of the steps involved in building a statistical model. Here is a graphical version which I like. Hope you will enjoy it too. Courtesy of my friends at ,, a big data analytics platform and solution company for Banking and Insurance.
Imagine being inside a child?€?s brain as she takes her major life decisions. Imagine the feelings of joy, sadness, fear, disgust and anger jostling with each other as she goes through the one of the biggest changes in her life, leaving her home. Imagine taking the Train of Thought over the Imagination Island and creating long-term core memories that define her growing personality traits (I mean) islands.,Well, the imagination just came alive in Disney?€?s latest movie ?€?Inside Out?€?. But of course, this is not meant to be a movie review. Or a recommendation. Though, you must see the movie, if you haven?€?t already. (That?€?s a subtle hint),Now imagine, actually, why imagine, you must be doing it today itself!?? Just about to take major a decision for your company based on Data Science.,You believe in data with all your heart and in your dreams you quote Sherlock Holmes:,So what?€?s your character you ask in this imagination game? Of course, you are the celebrated hero of today?€?s world, not the Superhero clad in the dark cloak with superpowers even nursery kids are no longer in awe of but the new superhero with a growing armor of super-powers like Cloud, Big Data, Python, IOT, (the list continues); the much talked about and much in demand,??,Now just imagine, if you could go behind the mind of the data scientist just the same way as Disney went behind the mind of the child in the movie, Inside Out?,What levers would you find? Which emotions would jostle with each other as the data scientist would show her super-powers and take the decision that will spell impact and change for her company?,Is it all about data? Or are there other levers that you need to push in order to make sure you reach the right decision? Are anger, fear, sadness, disgust and joy jostling with each other inside her brain as she takes her data-driven decision?,It is here I would like to make my main point.,For data science driven decisions to succeed then, knowing the data is not enough. The missing "D" in Data Science is a word we do not often associate with data science armor or job descriptions.,Drew Conway spoke about it as substantive experience in his DataScience Venn Diagram??,. Dr. Vincent Granville spoke about in DataScience Central and so did other people such as Jeff Heaton and Nathan Brixius. But it still seems to be a seldom voiced and unresolved debate, at best.,My point in this debate is that the best data scientists who can really drive business decisions and navigate the organizations through a culture of change are the ones who are not just data but also domain experts. Let?€?s look at 2 such far-reaching data-driven change decisions from history:,Florence Nightingale?€?s famous Coxcomb Chart showing deaths from diseases started with her understanding of the prevailing healthcare domain.,Alan Turing?€?s war-time breaking of the Enigma code owed its success as much to his reasoning powers as it did to his programming expertise.,The people mentioned may not be the usual data scientists but more fit the pattern of what Gartner is terming as the increasing crowd of ?€?Citizen DataScientists?€?.,So what?€?s going behind the Data Scientists" mind now? Confusion? Joy? Fear? Just like each superhero, each Data Scientist could have her own route to discovering her superpowers whether or not she has the official label and designation. The key point is that Data Scientists who are at this point of time dreaming of the next big ?€?Beer and Diaper?€? Eureka insights moment, might do well to pause a while in the ?€?Train of Thought?€? and look for the station called ?€?Domain Central?€? before they continue their data journey.,Remember this dialogue??from the same Mr. Holmes?,Which brings home the point about domain in a very,??way. Sherlock Holmes knew his domain. Hence he knew what data mattered and what didn?€?t. Knowing the domain, helps us create the right hypotheses which data science can help us test and prove or disprove. Without??domain understanding, data science could become a long fishing expedition in the ever-increasing data-lake and the Iceberg of Business could start melting long before the changes needed are really implemented.,"The Game is On". Let?€?s play this data science game, "Inside Out". Would love to hear your thoughts on this debate.

It?€?s official. Public concerns over the privacy of data used in digital approaches have reached an apex. Worried about the safety of digital networks, , want to gain control over what they increasingly sense as a loss of power over how their data is used. It?€?s not hard to wonder why. Look at the extent of coverage on the U.S. Government , last month and the sheer growth in the , against government and others overall. Then there is the increasing coverage on the inherent , built into the internet, through which most of our data flows. The costs of data breaches to individuals, ,, and , are adding up. And users are taking note., , , , In response, policy makers are actively meeting with industry lobbyists and privacy advocates to develop data governance policies and regulations that align existing , , and security laws with the new reality of data driven technologies. As negotiations continue, we will undoubtedly hear more about points of contention and debate from differing sides. , will defend the rights of developers to innovate and the need for free, unregulated space to do so. And , will defend the rights of users to be legally protected from the unauthorized and unwanted incursions that unregulated approaches can make into a user?€?s personal, private space. , , At this juncture, whether one or the other of these sides is ?€?right?€? is irrelevant. What matters is that consumers are not happy and policy makers are listening. While the existing laws may not (yet) specifically address every technology based data driven approach currently in existence, the ?€?Wild West?€? atmosphere in which the majority of data approaches have been developed to date is ,. Data driven approaches will be regulated, likely sooner than later., , In this new reality, developers no longer have the luxury of treating data privacy or security as secondary issues to worry about tomorrow, after they?€?ve worked out their core technology and approach. Now, they must consider whether their plans include data that can trigger privacy or security concerns and modify their approach accordingly. And they must prioritize privacy and build in measures to secure data at every stage of their process. This is true whether they are building approaches for themselves, for research purposes, or in the hopes of becoming the next entrepreneurial unicorn. , , , , If you?€?re not sure whether the data fueling your approach will raise privacy and security flags, consider the following. When it comes to data privacy and security, not all data is going to be of equal concern. Much depends on the level of detail in data content, data type, data structure, volume, and velocity, and indeed how the data itself will be used and released. , , First there is the data where security and privacy has always mattered and for which there is already an existing and well galvanized body of law in place. Foremost among these is classified or , data where data usage is highly regulated and enforced. Other data for which there exists a considerable body of international and national law regulating usage includes:, It may be possible to work with publicly released annualized and cleansed data within these areas without a problem, but the majority of granular data from which significant insight can be gleaned is protected.?? In most instances, scientists, researchers, and other authorized developers take years to appropriately acquire the expertise, build the personal relationships, and construct the technical, procedural and legal infrastructure to work with the granular data before implementing any approach. Even using publicly released datasets within these areas can be restricted, requiring either registration, the recognition of or affiliation to an appropriate data governing body, background checks, or all three before authorization is granted., , The second group of data that raises privacy and security concerns is personal data. Commonly referred to as Personally Identifiable Information (PII), it is any data that distinguishes individuals from each other. It is also the data that an increasing number of digital approaches rely on, and the data whose use tends to raise the most public ire. Personal data could include but is not limited to an individual?€?s:, In industries where being responsible for handling highly detailed personal data is the established business norm ?€? such as in the education, medical and financial fields ?€? there are already government regulations, business practices and data privacy and security laws that protect data from unauthorized usage, including across new digital platforms. But in many other industries, particularly in data driven industries where personal data has been treated as , and become the foundation of business models, there is currently little to no regulation. , , Regulation protecting privacy was already an issue before the digitization of data, when the largely manual process of collecting data through industry databases, surveys, phone books, and public records resulted in junk mail, telemarketing, and annoying ,. But now that businesses can automatically collect passive and active personal data via consumer behaviors and multiple physical sensors, consumers have become significantly more alarmed at how?? customization in services and products can lead to the loss of personal privacy. Take for example the growth in voices raised over the use of data from , or from , related to the interconnected ?€?Internet of Things?€?. In the new normal, the more that a data approach depends on data actively or passively collected on individuals, the more likely that consumers will speak up and demand privacy protection, even if they previously gave some form of tacit approval to use their data. , , Despite this new landscape, there are lots of different ways to use personal data, some of which may not trigger significant privacy or security concerns. This is particularly true in cases where , or data cannot be attributed to an individual. Whether individuals remain neutral to data approaches tends to be related to the level of control they feel they have over how their personal data is used. Some organizations that collect personal data extensively, such as , and ,, work to increasingly provide their users with methods to control their own data. But for ,, the lack of due diligence on data privacy in their approaches has already had their effect. , , A third category of data needing privacy consideration is the data related to good people working in difficult or dangerous places. Activists, journalists, politicians, whistle-blowers, business owners, and others working in contentious areas and conflict zones need secure means to communicate and share data without fear of retribution and personal harm.?? That there are parts of the world where individuals can be in mortal danger for speaking out is one of the reason that , (The Onion Router) has received substantial funding from multiple government and philanthropic groups, even at the high risk of enabling anonymized criminal behavior. Indeed, in the absence of alternate secure networks on which to pass data, many would be in grave danger, including those such as the organizers of the Arab Spring in 2010 as well as dissidents in Syria and elsewhere., , Yet it is easy for well-meaning, idealistic developers to implement worldwide services that do not take data privacy and security considerations into account simply because there was no perceived need for them and no laws, regulations or procedures to guide the process. Take for instance the experiences of the teams of volunteers seeking to build on successful efforts to crowdsource and geo-locate humanitarian crises around the globe. Crowdmapping initiatives have been effective in humanitarian contexts, like relief efforts in ,, where everyone is willing to share data for the greater good. But when it comes to geo-locating data on ,, spikes of alarm over the veracity of data and the safety of those who would provide good data increase profoundly. , , , , Bottom Line: if you are at all uneasy about internet insecurity, attacks from criminals, customer security and privacy concerns, the steady rise of regulatory environments, making sure you?€?re not breaking any laws, or protecting the welfare of others, you need to be proactive about data privacy and security in your approach.?? The best plan is to address any issues from the beginning. But even if your approach is already in development, consider taking the following steps to minimize the risk to the data that fuels your approach:, Taking these steps will not guarantee that you are either safe from breaches or from criticism. However, being proactive in your approach to privacy will help to assure you are recognized for implementing responsible data management practices that are clear and transparent, and that you do the utmost to protect the data you govern.??
Reference to Hadoop implies huge amount of data. The intend of the data is of course to derive insights that will help businesses stay competitive. "Scoring" the data is a common exercise in determining e.g. customer churn, fraud detection, risk mitigation, etc... It is one of the slowest analytics activities and especially when very large data set is involved. There are various fast scoring products in the market but they are very specialized and/or are provided by one vendor, usually requiring the entire scoring process to be done using its tools set. This poses a problem for those who build their scoring model using tools other than that of the scoring engine vendor.??,There is a democratic way of doing scoring. It relies on the use of the industry standard called PMML (Predictive Model Markup Language). Any tool used in building model, including scoring, that is able to export its model in PMML, will be able to score data in Hadoop/Hive in a flash using a universal PMML plug-in (UPPI) resident inside of Hadoop/Hive.??
The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday. The??,??is from the contribution marked with a +, where you will find the details.,??,??|??,??|??,??|??,??|??,??|??,??|??
 , , , , , ,Interested in similar content???

Congratulations! You just got the call ?€? you?€?ve been asked to start a data team to extract valuable customer insights from your product usage, improve your company?€?s marketing effectiveness, or make your boss look all ?€?data-savvy?€? (hopefully not just the last one of these). And even better, you?€?ve been given carte blanche to go hire the best people! But now the panic sets in ?€? who do you hire? Here?€?s a handy guide to the seven people you absolutely have to have on your data team. Once you have these seven in place, you can decide whether to style yourself more on , or ,.,Before we start, what kind of data team are we talking about here? The one I have in mind is a team that takes raw data from various sources (product telemetry, website data, campaign data, external data) and turns it into valuable insights that can be shared broadly across the organization. This team needs to understand both the technologies used to manage data, and the meaning of the data ?€? a pretty challenging remit, and one that needs a pretty well-balanced team to execute., , ,The Handyman can take a couple of battered, three-year-old servers, a copy of MySQL, a bunch of Excel sheets and a roll of duct tape and whip up a basic BI system in a couple of weeks. His work isn?€?t always the prettiest, and you should expect to replace it as you build out more production-ready systems, but the Handyman is an invaluable help as you explore datasets and look to deliver value quickly (the key to successful data projects). Just make sure you don?€?t accidentally end up with a thousand people accessing the database he?€?s hosting under his desk every month for your month-end financial reporting (ahem).,Really good handymen are pretty hard to find, but you may find them lurking in the corporate IT department (look for the person everybody else mentions when you make random requests for stuff), or in unlikely-seeming places like Finance. He?€?ll be the person with the really messy cubicle with half a dozen servers stuffed under his desk.,The talents of the Handyman will only take you so far, however. If you want to run a quick and dirty analysis of the relationship between website usage, marketing campaign exposure, and product activations over the last couple of months, he?€?s your guy. But for the big stuff you?€?ll need the Open Source Guru., ,I was tempted to call this person ?€?The , Guru?€?. Or ?€?The , Guru?€?, or ?€?The , Guru?€?, or ?€?The , Guru?€?, or?€? well, you get the idea. As you build out infrastructure to manage the large-scale datasets you?€?re going to need to deliver your insights, you need someone to help you navigate the bewildering array of technologies that has sprung up in this space, and integrate them.,Open Source Gurus share many characteristics in common with that most beloved urban stereotype, the ,. They profess to be free of corrupting commercial influence and pride themselves on plowing their own furrow, but in fact they are subject to the whims of fashion just as much as anyone else. Exhibit A: The , over the , of Hadoop, followed by the enormous fuss over the , of Spark. Exhibit B: , (on the men, anyway).,So be wary of Gurus who ascribe magical properties to a particular technology one day (?€?Impala?€?s, like, , amazing?€?), only to drop it like , the next (?€?Impala? Don?€?t even talk to me about Impala. , embarrassing.?€?) Tell your Guru that she?€?ll need to live with her recommendations for at least two years. That?€?s the blink of an eye in traditional IT project timescales, but a lifetime in Internet/Open Source time, so it will focus her mind on whether she really thinks a technology has legs (vs. just wanting to play around with it to burnish her resum??)., ,While your Open Source Guru can identify the right technologies for you to use to manage your data, and hopefully manage a group of developers to build out the systems you need, deciding , to put in those shiny distributed databases is another matter. This is where the Data Modeler comes in.,The Data Modeler can take an understanding of the dynamics of a particular business, product, or process (such as marketing execution) and turn that into a set of data structures that can be used effectively to reflect and understand those dynamics.,Data modeling is one of the core skills of a Data Architect, which is a more identifiable job description (searching for ?€?Data Architect?€? on LinkedIn generates about 20,000 results; ?€?Data Modeler?€? only generates around 10,000). And indeed your Data Modeler may have other Data Architecture skills, such as database design or systems development (they may even be a bit of an Open Source Guru). But if you do hire a Data Architect, make sure you don?€?t get one with , those more technical skills, because you need datasets which are genuinely useful and descriptive more than you need datasets which are beautifully designed and have subsecond query response times (ideally, of course, you?€?d have both). And in my experience, the data modeling skills are the rarer skills; so when you?€?re interviewing candidates, be sure to give them a couple of real-world tests to see how they would actually structure the data that you?€?re working with., , ,Between the Handyman, the Open Source Guru, and the Data Modeler, you should have the skills on your team to build out some useful, scalable datasets and systems that you can start to interrogate for insights. But who to generate the insights? Enter the Deep Diver.,Deep Divers (often known as Data Scientists) love to spend time wallowing in data to uncover interesting patterns and relationships. A good one has the technical skills to be able to pull data from source systems, the analytical skills to use something like , to manipulate and transform the data, and the statistical skills to ensure that his conclusions are statistically valid (i.e. he doesn?€?t mix up correlation with causation, or make pronouncements on tiny sample sizes). As your team becomes more sophisticated, you may also look to your Deep Diver to provide Machine Learning (ML) capabilities, to help you build out predictive models and optimization algorithms.,If your Deep Diver is good at these aspects of his job, then he may not turn out to be terribly good at taking direction, or communicating his findings. For the first of these, you need to find someone that your Deep Diver respects (this could be you), and use them to nudge his work in the right direction without being overly directive (because one of the magical properties of a really good Deep Diver is that he may take his analysis in an unexpected but valuable direction that no one had thought of before).,For the second problem ?€? getting the Deep Diver?€?s insights out of his head ?€? pair him with a Storyteller (see below)., , ,The Storyteller?€?s yin is to the Deep Diver?€?s yang. Storytellers love explaining stuff to people. You could have built a great set of data systems, and be performing some really cutting-edge analysis, but without a Storyteller, you won?€?t be able to get these insights out to a broad audience.,Finding a good Storyteller is pretty challenging. You do want someone who understands data quite well, so that she can grasp the complexities and limitations of the material she?€?s working with; but it?€?s a rare person indeed who can be really deep in data skills and also have good instincts around communications.,The thing your Storyteller should prize above all else is , It takes significant effort and talent to take a complex set of statistical conclusions and distil them into a simple message that people can take action on. Your Storyteller will need to balance the inherent uncertainty of the data with the ability to make concrete recommendations.,Another good skill for a Storyteller to have is ,. Some of the most light bulb-lighting moments I have seen with data have been where just the right visualization has been employed to bring the data to life. If your Storyteller can balance this skill (possibly even with some light visualization development capability, like using ,; at the very least, being a dab hand with Excel and PowerPoint or equivalent tools) with her narrative capabilities, you?€?ll have a really valuable player.,There?€?s no one place you need to go to find Storytellers ?€? they can be lurking in all sorts of fields. You might find that one of your developers is actually really good at putting together presentations, or one of your marketing people is really into data. You may also find that there are people in places like Finance or Market Research who can spin a good yarn about a set of numbers ?€? poach them., ,These next two people ?€? The Snoop and The Privacy Wonk ?€? come as a pair. Let?€?s start with the Snoop. Many analysis projects are hampered by a lack of primary data ?€? the product, or website, or marketing campaign isn?€?t instrumented, or you aren?€?t capturing certain information about your customers (such as age, or gender), or you don?€?t know what other products your customers are using, or what they think about them.,The Snoop hates this. He cannot understand why every last piece of data about your customers, their interests, opinions and behaviors, is not available for analysis, and he will push relentlessly to get this data. He doesn?€?t care about the privacy implications of all this ?€? that?€?s the Privacy Wonk?€?s job.,If the Snoop sounds like an exhausting pain in the ass, then you?€?re right ?€? this person is the one who has the team rolling their eyes as he outlines his latest plan to remotely activate people?€?s webcams so you can perform facial recognition and get a better Unique User metric. But he performs an invaluable service by constantly challenging the rest of the team (and other parts of the company that might supply data, such as product engineering) to be thinking about instrumentation and data collection, and getting better data to work with.,The good news is that you may not have to hire a dedicated Snoop ?€? you may already have one hanging around. For example, your manager may be the perfect Snoop (though you should probably not tell him or her that this is how you refer to them). Or one of your major stakeholders can act in this capacity; or perhaps one of your Deep Divers. The important thing is not to shut the Snoop down out of hand, because it takes relentless determination to get better quality data, and the Snoop can quarterback that effort. And so long as you have a good Privacy Wonk for him to work with, things shouldn?€?t get too out of hand., ,The Privacy Wonk is unlikely to be the most popular member of your team, either. It?€?s her job to constantly get on everyone?€?s nerves by identifying privacy issues related to the work you?€?re doing.,You need the Privacy Wonk, of course, to keep you out of trouble ?€? with the authorities, but also with your customers. There?€?s a large gap between what is technically legal (which itself varies by jurisdiction) and what users will find acceptable, so it pays to have someone whose job it is to figure out what the right balance between these two is. But while you may dread the idea of having such a buzz-killing person around, I?€?ve actually found that people tend to make , conservative decisions around data use when they don?€?t have access to high-quality advice about what they can do, because they?€?re afraid of accidentally breaking some law or other. So the Wonk (much like ,) turns out to be a pretty essential member of the team, and even regarded with some affection.,Of course, if you do as I suggest, and make sure you have a Privacy Wonk and a Snoop on your team, then you are condemning both to an eternal feud in the style of the , (though hopefully without the actual bloodshed). But this is, as they euphemistically say, a ?€?healthy tension?€? ?€? with these two pulling against one another you will end up with the best compromise between maximizing your data-driven capabilities and respecting your users?€? privacy., The one person we haven?€?t really covered is the person who needs to keep all of the other seven working effectively together: To stop the Open Source Guru from sneering at the Handyman?€?s handiwork; to ensure the Data Modeler and Deep Diver work together so that the right measures and dimensionality are exposed in the datasets you publish; and to referee the debates between the Snoop and the Privacy Wonk. This is you, of course ?€? The Cat Herder. If you can assemble a team with at least one of the above people, plus probably a few developers for the Open Source Guru to boss about, you?€?ll be well on the way to unlocking a ton of value from the data in your organization.,Think I?€?ve missed an essential member of the perfect data team? Tell me in the comments.

 , , , , 
Big Data Junk Yard strategy lets you play around with big data in its natural form. The approach allows enough runway for technology to ramp up infrastructure and for business to find right use cases through data discovery. It?€?s easy, economical and quicker way to get on the big data band wagon.(,).,With that said, there is a need for some method to madness though. If you want your Big Data Junk Yard to be productive, some discipline is required to nurture and grow it so that you can start mining valuable gold soon enough. Here are the three recommended basic steps:,It is important to draw a line in the sand. In my last blog, I strongly advocated for not investing too much time thinking about the use cases. However, that does not imply not investing any time at all and start hoarding every data set that?€?s available.,Aligning strategic business goals with the futuristic data needs will help defining the boundaries. This is also a great opportunity to engage business in the big data initiative. Start with a laundry list of data sources with some perceived value to business in next 3-5 years. Keep the list alive and agile but also use it as a guideline for your hoarding strategy. This will help you manage the mix and keep your junk yard from getting out of control.,Keeping it organized and tagged will help save a tremendous amount of time and efforts for data scientists looking for those gold nuggets in the data junk yard. Keep all Chevys in one corner and Hondas in another. It will make that car mechanic scavenging for 1999 Chevy parts really happy. Start with a comprehensive tag list to mark your data and schemas. There are multiple technology options that you can find in the Hadoop eco-system that can help keeping the yard in order.,Do not forget to spring clean. In order to enjoy the freedom of bringing home all new toys, we need to let go some old stuff that we know we will never play with again for sure. Hadoop may be a cheaper alternative to our traditional darling databases but it is still a finite resource.,It is recommended to form a joint governance group of business and technology stakeholders. The group can review the data discovery findings on a regular basis and can help align the big data strategy on what to ingest, what to digest and what to divest. This will be the first step of start refining the junk to get some gold.
Buzz words are one of my least favorite things, but as buzz words go, I can appreciate the term ?€?Data Lake.?€? It is one of the few buzz words that communicates a meaning very close to its intended definition. As you might imagine, with the advent of large scale data processing, there would be a need to name the location where lots of data resides, ergo, data lake. I personally prefer to call it a series of redundant commodity servers with Direct-Attached Storage, or hyperscale computing with large storage capacity, but I see the need for a shorter name. Also, the person that first coined the phrase could just have easily called it Data Ocean, but they were modest and settled for Data Lake. I like that.,Not to be outdone, and technology vendors being who they are, came up with their own ?€?original?€? buzz word, ?€?Data Reservoir?€?, implying a place where clean, processed data resides. Whatever you want to call it is fine with me as long as you understand that unless you follow a solid data management process, even your data reservoir could end up being a data landfill (also a popular buzz word). The point of this post is to demonstrate what a real data management process might look like by providing real examples, with real code, and, most importantly, to emphasize the importance of having a good process in place. Once implemented, you will have good data, from which solid analysis can be performed, and from which good decisions can be made.,The reason data quality suffers is that everyone is in a rush, it is costly, and can actually be very difficult and complex as you will see when you follow through the whole exercise below. It is much easier to take the easy way out. After all, executives don?€?t realize how bad their data is until they?€?ve been bitten, and even then it is difficult to trace it back to the root cause.,Have you ever wondered how many events were caused by poor data management practices that were subsequently blamed on the executive?€?s decision making process? Do you think the executives at AIG, or Lehman Brothers knew the full scope of their exposure to the subprime mortgage loans? Sure they were aware of problems, and they may have been given reports that were ?€?directionally correct?€?, they were just ?€?wrong.?€? Maybe the slope of that line should have been much steeper. Of course, greed can easily change the slope of a line. No way of telling, but when critical decisions are being made, being ?€?directionally correct?€? is not good enough.,The flow of data into an organization employs many stages and processes along the way. Each phase is critical to achieving the ultimate goal, and this goal is to provide reliable information that enables accurate conclusions and decisions. This discussion will focus on knowing the data, and cleaning, or tidying of data. This does not mean that other phases are less critical. Even with tidy data that is well understood, improper data collection procedures, faulty analysis and misleading reports can be just as devestating as bad data. Every phase is important, and errors at any phase can cause a break in the information data flow process.,Whether it is a Big Data scenario, or not, data quality must be managed to the maximum extent possible. I am not stating that all Big Data, or any other data, must be managed as closely as what I demonstrate below. However, I am stating that any data that is to be used for analysis upon which decisions will be made could, and, in most cases, should be managed with rigorous data integrity rules enforced.,A quick side note here, these principles apply to more than just data flowing in and out of a database for analysis. The same rigor must be applied in managing data of all kinds. All successful Service Oriented Architecture (SOA) and Enterprise Service Bus implementations have a mature Governance process in place. Service contracts are managed rigorously just as you would a normalized database. Authoritative data sources are defined, security for sensitive data is managed closely, and well defined data structures and schemas (Canonical models and XML Schemas) are defined to enable seamless information exchange, and so on. Managing data, at all levels is a complicated matter and requires a great deal of hard work and diligence to ensure its security and trustworthiness.,With the terms Velocity, Volume, and Variety used to define Big Data, images are conjured up of data coming in fast and furious, with little, or no time to plan for this flood of streaming data. However, it does not work that way, or at least it shouldn?€?t. Yes, a company should have an infrastructure and processes in place that allow for rapid and accurate responses to a fast paced economy. However, this does not mean that data should be integrated without proper due diligence.,Being flexible, agile, and responsive should not translate to being reactionary, or careless. Before Big Data, or any other data, is incorporated into a company?€?s knowledge stores, planning must occur. This planning involves knowing the data, understanding its value, and ensuring its proper management, and security. This discussion will hopefully highlight the importance of managing data to ensure its integrity.,The use case for this exercise is that a company will routinely download data from a 3rd party vendor. For the purposes of this discussion, the 3rd party data will come from the U.S. National Oceanic and Atmospheric Administration?€?s (NOAA) storm database. This data was selected beacuse it is relatively simple, yet complicated enough to demonstrate more advanced topics, and relatively small in size. It consists of three tables: 1) Details, 2) Fatalities, and 3) Locations. These three tables are broken up into 65 files each (one for each year starting in 1950 and ending in 2014). Of the three table types, there are a total of 73 columns and approximately 2.3M records across all three tables. There is also a fourth table called UGC that provides State and County codes. However, there are numerous problems with how this reference data has been used. It will be loaded and discussed briefly.,Obviously, the scope of this effort would be quite different at the enterprise level. Multiply what is done below by an appropriate factor and you will get an idea of the scope of work involved when you are dealing with several hundred distinct tables (not three) with several thousand columns. Getting to know and understand data is not a trivial task.,For this exercise the NOAA storm database was selected. Currently, this data set covers storm activity across the United States starting in January 1950 through December 2014. According to NOAA, ?€?NCDC has performed data reformatting and standardization of event types but has not changed any data values for locations, fatalities, injuries, damage, narratives and any other event specific information. Please refer to the Database Details page for more information:??,?€?,Given this disclosure, the expectation was that there would be one or two types of errors discovered, bringing me to the rapid conclusion: "even with data that has undergone a thorough review, checks should always be performed." Unfortunately, in very little time significant problems were discovered, thus the length of this post. For the sake of brevity, some solutions are just mentioned, and the code does not cover a production level solution.,This is not intended to be critical of NOAA. It is simply pointing out that all data can be corrupt, and enforcement of data integrity is a constant battle if not managed properly. NOAA is now on version 3.0 as May 12, 2014.??,??and it appears there is still room for improvement.,Use the following link to download the data used in this post:,The data was downloaded, and loaded using the R Programming language. The data store for this data is an Oracle 12c XE RDBMS. However, this could just as easily be a Hadoop Data Lake, Teradata, DB2, MySQL, or PostgreSQL database (part of the reason the RJDBC package was chosen instead of ROracle was to accommodate various databases - hopefully, this will at least minimize the required modifications).,Note: With data, there are many terms used to define data elements. For instance, in logical modeling the terms ENTITY and ATTRIBUTE are used, whereas in physical modeling TABLE and COLUMNS are used. To avoid confusion, we will use the physical modeling terminology for the remainder of the discussion.,The NOAA data will serve as a good example of how detailed the data study must be. The data could just as easily be bank, lending, financial markets and investment, commodities, vendor inventory lists, marketing data, . . . When data is being incorporated into a company infrastructure, whether it is a data lake, or just a couple of millon records going into an RDBMS, the concepts proposed could, and should, be implemented in most scenarios.,The first task is to develop a data dictionary (or acquire one from the 3rd Party Source if it exists). NOAA provides a good start to the data dictionary, but it is not complete.,A data dictionary typically consists of a document in spreadsheet, or table format with the following minimum information:,Here is a sample of the dictionary created for this project. This particular format does not show the column size, but that information is displayed in the next figure, the Physical Data Model:,Depending on the project, it is usually very helpful to develop a data model that depicts the table relationships. I prefer to use Computer Associates?€? Erwin Data Modeler. It is an industry standard for enterprise modeling, although there are many tools that would be suitable for smaller projects like this one, and XMLSPY is an excellent tool for depicting XML schemas. The model depicted below reflects how the data is modeled after having run through the exercise below. Previously, all character columns were varchar2(255), and there were no primary, or foreign keys assigned.,To develop the data model and dictionary, I could have used the documents provided by NOAA exclusively, and entered each table and column name. However, I tend to be lazy and find it much easier to load the data, tidy it as much as possible, which includes assigning primary, foreign, and alternate keys as necessary, modifying the data types through the use of R (detailed in the code below), and then reverse engineering the database using Erwin. This provides most of what is needed for the data dictionary. If a logical model was required, the names would need to be changed to a more human readable format. Then all I have to do is copy over the definitions and I?€?m done. So far, so good, and this did not require a lot of work, and exposed a lot of problems with the data as we will discuss now. Also, please keep in mind, this is four tables total, not a hundred.,Note: The model depicted is not a fully normalized model. The lack of normalization causes a few issues that are discussed below, but this model will suffice for our purposes.,Before heading into any data project, it is best to have some idea of what you are trying to accomplish. What are the requirements? What questions are you trying to answer? Defining the use case is an important step and no data project should be undertaken without clear objectives in mind. Believe it or not, the lack of well defined requirements is a common problem in data management. Agile has come to mean no documentation, and I don?€?t believe that was the intent. So, for no other reason than to remind everyone of its importance (including myself), we will at least use the following as our high level requirements for this project.,The objective of this analysis is to:,Due to space limitations on the post, if you would also like to see the associated code for this exercise, ??please go to the following link:??,A design like this is just asking for trouble and is really unnecessary when you have a date data type that enforces all of those rules for you. Why did Hadley Wickham develop lubridate if we were going to keep our date fields so neatly parsed? :) For this reason, the attributes pertaining to Dates (YEARMONTH, YEAR, MONTH, HOUR, . . . ) were removed from the data. This type of data is readily available from the DATE and TIME fields provided.,Note: Two duplicates do not in themselves seem to be troubling, out of 1301016 rows. However, keep in mind that all duplicates essentially cause a Cartesian join. Without contraints and checks, errors are magnified. In the Fatalities table there are 2013 duplicates, and as we will see there are many more problems introduced by the lack of integrity constraints.,I am sure this list is not exhaustive, but you get the idea. The fatalities table, if you look at the data, only provides a few details about the fatality, like date, age, and sex. The rest of the information about the type of weather event is contained in the Details table. There are 28 records in the Fatalities table with no matching record in the Details table. In other words, a fatality without a storm event.,The following Venn diagram gives you a visual of the differences in the data before and after removing duplicates and orphans, and enforcing referential integrity. Notice that there is some intersection between Fatalities and Locations. However, this just means that the EVENT that occurred at a specific location, caused fatalities. The data does not provide the location of the fatality. Both direct and indirect deaths could have occurred miles away in a hospital a month later.,So, how many of these match? Is every fatality record with a "D" for FATALITY_TYPE represented in the details table's DEATHS_DIRECT column? You can do the checks, but hopefully you get the point. This is denormalized, and unenforceable, which is not good. The difference in counts can be explained by missing historical data. The sex, age and date of every storm victim is probably unknown. However, inconsistencies are not a result of missing data, just mismanaged data.??,There would still be some work to do on this data, such as property and crop damages are not in a good format for analysis, the latitude and longitude in the locations table is inconsistent and needs standardization, and one thing that NOAA did not do was to categorize the storms more narrowly. However, I will stop here. If you have made it this far, you are probably a data geek like me, and you get my point(s). Hopefully, you have also developed a better understanding of what?€?s involved in maintaining data integrity. If you have something to add where I have overlooked, or misstated a point, or made an error, I would love to hear from you. Data geeks love data, and for the rare "non-data geek" that made it this far, its probably a lot like watching paint dry. I appreciate you hanging in there. Next time!
??It is far more important than being an expert in 50 types of linear or logistic regressions. And it can be acquired after a few years of??,.
Here is a list of top Python Machine learning projects on GitHub. A continuously ,.,module provides standardized Python access to toy problems as well as popular computer vision and natural language processing data sets.
"This 30 minute video aims to demystify predictive analytics and present the IBM SPSS predictive analytics portfolio. The contents of the video are as follows:,
,The video is also ,."
 , , , , , , , , , 
 , , , , , , , , , , , , , 
In Java programming, there is the idea of a "virtual machine." A ,??is a computer system that doesn't exist in real life. Yet programs can be written for it. The code is interpreted by a runtime environment. Through this arrangement, Java programs can operate on different operating systems rather than one exclusively. Depending on one's background, the concept of a "," might be unfamiliar. The ,??can be invoked to deal with emergencies. I recently wrote about participating in a tabletop exercise simulating a train derailment. Our group took on the form of the organization. This means that certain roles and functions, internal processes, and logistics could take shape in a short period of time. There was little need to build up from scratch. Whether there is an earthquake, pandemic, or terrorist attack, the abstract organization can be invoked. People can work on processes that "run" on this virtual environment - even before play or any incident occurs. In this blog, I will be writing about the possibility of implementing a standardized data system for the environment. I will discuss how data can be extracted from behavioural tagging in a transactional environment.,I recently wrote about the hazards of the "institutional response." I said that there is a need to gather data in order ensure the effectiveness of the response. Although I was writing in generalities, I was actually setting the preamble for this blog dealing with a data system for the virtual organization. The standardized structure - whether in its purely abstract form or instantiated to deal with an incident - represents a type of institutional response. The organization has to adapt to the circumstances of its operating environment including the availability of resources, the interactive dynamics between people, and the nature of the challenges that must be confronted. This sort of adaptation requires an abundance of data. Consider for example a war game (simulation). Seriously give some thought on how the behaviours and interactions of people should be converted into data and made accessible. How precisely should details be symbolically converted and retained? Vagueness and generalities don't work when writing computer code. Up to a point, the challenge relates to programming. However, I suspect that even skilled programmers would wonder about how to configure the data and data system to make calculations even possible.,At some point near the end of my derailment simulation - as the number of key events started to build up - I began questioning how a person might go about preserving details of the complex interactions. I pondered on how the data can be made accessible for study later - particularly using a computer. For organizations, there is probably a need to "externalize" the insights gained from simulations: learning should not be a process limited only to those that participate. It should be possible even for those that don't participate to learn from experiences. An idea came to mind as I was trying to deploy and keep track of my resources during the simulation. I thought that it might be worthwhile to translate certain types of behaviours into transactional data. Keeping in mind my earlier comments about the virtual nature of the organization, the data system if functional might run on every instance of the organization regardless of the exact application. ??I notice that are already management products that take advantage of the virtual org (,).,I think that when people talk about "transactions," they usually mean financial transactions. I have found that almost anything can acquire the look and feel of a transaction. A transaction can exist for any transferable asset - giving rise to data such as time, source, destination, quantity, type, and duration. Specifically in an emergency simulation, items that can be transacted might include the following: police cruisers, ambulances, buses, trucks, firefighters, police officers, nurses, food, clothing, beds, phones, and computers. I would say that there are limited dimensions when dealing with financial transactions. In a non-financial context, on the other hand, it is possible to "sell suggestions" and for others to "buy suggestions." A person can ask for "all available suggestions" and then accept a handful of a certain type. Converting group dynamics into transactions is probably no simple feat. I recognize that the conversation will not end with me. I just want to offer up some ideas.,Since I had resources available for deployment during the simulation, I was surprised that nobody asked me what I had to offer. I decided that - rather than relying on the attitudes, impressions, and instincts of participants - there had to be a friendly and constructive way for a person to "bring products to market" so all available resources are out in the open. It is not necessarily a participant's place to impose resources on other people. I also understand how, given competing priorities, a participant cannot necessarily be given the time or opportunity to routinely update others on the current availability of assets. I though it would benefit everybody for me to be able to maintain an up-to-date inventory of products and services in real time; this inventory would be presented to market maybe using a type of dashboard. In retail, the transactions that people know best occur at the point of purchase; of course, there are all sorts of transactions throughout the process long before the purchase. I propose that transactions can be used as a type of universal communications language,The "market" would have also served to tell me what other people needed. I could attempt to obtain the items not on my inventory but within my scope of influence. The data system would therefore have the ability to influence workflows and priorities within the standard organization. It could help people exhibit the most enabling and constructive behaviours possible given the pressing needs of the organization.,I had a difficult time keeping track of where I was putting resources - remembering how many assets were placed in which locations to perform what specific functions and duties. I also had difficulty determining and therefore reporting existing balances. I started using something almost resembling a ledger, which seemed rather awkward. I reasoned that it would be better for a computer to manage the bookkeeping: it could generate data streams to reflect my current status and also support future strategic analysis. I developed a tagging system as a precursor to the processing environment. The tagging approach is described using the two tables below: the top table is for the participant to use in his or her role gathering resources; the bottom table is used for the distribution.,CA0-StrikePackageObject: the collector needs any strike package available,DE0-StrikePackageSupplyObject: the dealer can offer a limited strike package,CA1-StikePackageAcceptanceObject: the collector will accept anything sent,In the above examples, the first three characters determine which table is being used and the placement on the table. The tags provide information relating only to the movement of objects. It might already be apparent from these few examples that the transactional environment converts simulation exercises essentially into a type of chess game. I know that playing chess is probably not widely regarded as a transactional activity. Nonetheless, there is a movement of assets on the chessboard. The pieces have primitive functions. A knight can move in L-formations. One would not send food and water to a knight in chess. However, the basic idea of deploying assets strategically in a transactionally-supported simulation is comparable to playing chess.,In a normal transaction for example in a retail setting, there is an exchange of cash for goods and services. No effort is made to understand the reasoning behind the transactions - probably because the reasons can be so varied. But I believe this is a certain reoccurring rationale applicable to emergency scenarios: 1) the perceived abundance of inventory in relation to the dealer; 2) the perceived abundance in relation to the collector; 3) the perceived abundance of the broader market excluding the collector; and 4) the perceived abundance of the broader market excluding the dealer. I will give an example. Assume the collector would like to obtain a piece of merchandise from the dealer. The dealer would therefore need to have some inventory. The collector approaches the dealer due to perceived abundance. However, the collector can also pursue other sources in the market if there is a perception of relative abundance on the exclusion of the dealer. The asset object can retain details of these perceptions.,Details of physicality can also be embedded in the object: location of the merchandise; destination; package dimensions; weight; and the name of the private sector supplier. It might seem wasteful to record these details within data objects. However, we have vast computer processing capabilities these days. Consequently, the tagging system holds information about the movement of assets while the objects have details about the underlying assets and rationale behind the exchanges.,A rather specialized area of data science deals with the need to extract meaning from large amounts of text. Any kind of dependency on normal conversation during an emergency can be problematic given the lack of normalcy in the situation. I have raised the use of transactions to get around not just the linguistic barriers but also lack of interactive norms that become apparent in big emergencies. For example, it can be said that there are probably differences between police departments deployed during a multi-jurisdictional emergency. But the lack of a stable situational language becomes really apparent once different functions become involved in the operation: e.g. police, paramedics, hospitals, fire departments, city public works, and maybe the military. Given the absence of commonalities in communication, this setting is actually problematic for the extraction of data - assuming the absence of a logical intermediary. ??The logical intermediary - that is to say, the transactional environment - can on one hand be considered a unifying language and on the other a means to capture computer-friendly data from all the turbulence.,I'm unsure how others approach the extraction of data from a relational data stream (a stream that explains how things relate to each other). I generally "throw events." Given the framework I just described, it would be possible for events to be thrown for all of the following: when people request for services; when there is an agreement to send services; when there are inadequate resources to meet the demand; when there are excess resources; when the services are sent; when delivery is required for a specific time and location; when all available suppliers are canvassed; when there is network failure to provide the service. The possibilities can get quite lengthy. When a particular situation occurs, an event symbolically representing that situation can be thrown, causing a registration of value such as an increment. How exactly the resulting numbers should be handled is quite a big topic on its own. For example, I'm sure some would make use of a statistical approach.,For now, I just want to focus on the how the data system on one hand can record and maintain transactional data - from many different participants and the group as a single entity - while on the other hand generating streams of data during the execution of the simulation. I emphasize this latter point because I think there tends to be a jump in reasoning that precariously diminishes the challenge of the undertaking. It's been said that highly skilled individuals are needed to do all sorts of complex calculations. That's rubbish of course. Computers do complex calculations. The theorization to enable computer processing requires some effort. Suffice it to say, "Let's collect some data" might not be as simple as it seems once a person has to break down behaviours and real-life events into computer code to enable bulk processing.,When I originally started writing this blog, I thought about expressing the dynamics in relation to "buyers" and "sellers." (I use these terms conceptually rather than literally. There isn't necessarily any real buying or selling.) I consider some participants in a simulation both buyers and sellers. What is bought from an outside supplier can later be sold. Consequently, I avoid the term "buyer" and instead make use of the term "collector." A collector can become a dealer. On the image below, I try to highlight the direction of flow between dealers and collectors involved in an ICP-EOC relationship; these dynamics existed during my derailment simulation. The symbol D-C indicates a dealer-collector function. S is a private sector supplier or the main source of a particular product or service.,I don't want to confuse matters by elaborating greatly on what is an ICP or EOC for those unfamiliar with the acronyms. The entities tend to be invoked or activated during an emergency. However, I will point out that both these entities usually exhibit the organizational similarities I mentioned earlier. Such instantiations might therefore represent candidates for the data system for the transactional environment. I certainly believe that for instances of the organization that persist for some time, the data system might be useful even on the absence of an emergency. The virtual org simply provides some reference points to plug in the data system.,My objective in this blog is just to get some ideas out for others to consider. I hope readers find the general concept of making use of transactions worth considering for their own initiatives. I discussed the distinction between transactional code and events. I said that events can be used to generate quantities, which might then be accessed using statistical methods. However, events do not have to be interpreted in a traditional mathematical sense - focused on the amplitudes. For example, events can be portrayed fractally for the purpose of pattern recognition. When dealing with complex data objects, I consider the idea of reducing the complexity into neat and simple numeric streams of numbers for convenient statistical analysis borderline blasphemous. However, I hope most would agree, irrespective of the approach or methodology ultimately used on the data, it is first necessary to collect data. This seems like a simple observation on the surface, but actually the underlying mechanics of such an undertaking are probably quite elusive.,The sophistication and stability of the virtual machine has helped to preserve usefulness of the Java code. I still use code that I wrote in the 1990s on different platforms. Similarly, I believe that the virtual org will create many data-oriented opportunities. When organizations talk about data silos, this represents a nagging problem that can adversely impair the performance of assets. When these silos exist in emergency situations, the ability to overcome them might mean the difference between life and death. If there appear to be silos, one way to determine the nature of the impairment is through testing and data gathering during stress testing - for example, during a simulation or exercise. In this blog, I have suggested that, in support of data-gathering, it can be useful to implement a data system intended for the virtual org; because it becomes possible to gain insights in relation to different instances of the organization regardless of the exact application.
These companies gather and process gigantic amounts of data to serve their clients and/or users. They make money out of selling summarized, processed, real-time data. They are poised to succeed in the IoT (Internet of Things) revolution, leveraging all sort of devices and API's to gather data, and,It is worth spending some time figuring out what kind of big data platform (Hadoop? NoSQL?) and analytic tools they use, as well as the data science techniques that they leverage, and the types of data scientists working for them. Some of this information might be available on ,. I've applied for data scientist positions at each of these four companies, and was never offered a position - not even as a contractor - so it looks like they have no problems finding data scientists (these companies are looking for ,, not horizontal ones, which is a bit surprising for startups).,Type of alerts sent to users: optimum route to reach a destination at a given time (in real time), integrated into your GPS. Or alerts for driverless cars. May provide information to users to avoid police traps, assuming this type of data is collected.,Managed by former Microsoft people.,From their website: ,Check out their??, and ,.,Type of alerts sent to users: relevant job alerts, new salary surveys.,Managed by former Amazon people.,From their website:??,Check out their ,??and ,.,Type of alerts sent to users:text messages to notify a Walmart client about relevant products / discounts he might be interested in. Alerts are sent when the client is in a store.??,??for more big data startups in the Seattle area.??

We have witnessed the rise of ,, since the emergence of Big Data. We certainly can explore the relationship of such two variables in terms of X & Y, to be worked with in terms of using Data Science. The use of Regression also on basic terms gives an a depiction of two variables X & Y to work with. These variables are:, Variables & , Variables,Let us take behavior of users of a financial institution. We take a hypothetical data (random sample) of 6 users visiting one specific website of a Line of Business or LOB in specific one hour.,: 1, 2, 3, 4, 5, 6 ,: 5, 17, 11, 8, 14, 5,The user behavior of another user or we can say User # 7, we need to predict his/her behavior of visiting one specific website, we will be using the statistical technique, which is called "Mean", which is the adding all visits by first randomly selected users, which becomes to total visits to be divided by the total number of users, which we can also say as Mean (Visits) = 60/6 = 10. This is the prediction we can do in terms of best estimate for user # 7 to visit the same site. This can also be considered Internal LOB Forensics of User Behavior. This can also be called the Measure of Variability. Let us now find the distance between our data on the good fit that we got after calculating the mean, which is 10 for users usage deviations ( Mean - Visit):,: -5, 7, 1, -2, 4, -5 {Let us add all + & - = -5-2-5 = -12 & 7+1+4 = 12},This means -12+12 = 0, our value is most likely the value of the next user's visit to the website, we have chosen the sample for. Let us now do a Sum of Squared Residuals or Errors, which is 120. This entire example is based on one dependent variable only, which is the visits of one specific website by some users with in a Line of Business. This predictive analytic discussion has introduced the idea of usage of a website, by some users using ,. We certainly can explore more, if we know, the time users have spent on that specific website or the number of pages the visited, in this case, we now can have both Independent and Dependent Variables available for us to work with to have our prediction on a better note.,Linear Regression is a continuity of??,??and??,. While working with Correlation we work with two variables as we discussed in this article X & Y, and there are points plotted on these X & Y on a graph.There is a relationship that we have explored between these plotted points. We can also say that the value of one variable is the function of another variable. It can also be shown as:,the value of y is a function of x??,The value of dependent variable??,??is always dependent on the value of dependent variable??,.,It is hoped that this article sheds some light on the basic use investigative forensics within a department or a Line of Business within an organization, which may be looking at the internal users' behavior to serve some clients using one single resource.
Let's say a set of documents 'S' has a large set of 'pure' texts.,On all documents in S, I am spelling normalisation method, which yields a normalised set S'.,Then I use the chosen method M (which method? ) to make clusters in S, obtaining a clustering result C.,Then I use the same method M to make clusters in S', obtaining a clustering results C'.,Finally I need to compare if there are statistically significant differences between C and C'.,Any help in identifying what technique or method (M) I should use for clustering the text documents?
Alchemy is a fascinating pseudo-science, long discredited as a real science; its fundamental principles still form the basis of many contemporary scientific theories. Alchemy is based on the premise that nothing in the universe is devoid of existential elements, and that these elements can be manipulated and even transmuted into other forms. The fabled Philosopher?€?s Stone is said to fulfil one of the main objectives of Alchemy, a legendary Alchemical substance said to be capable of turning base metals such as lead, into gold.,As in Alchemy, the notion of ?€?elements?€? in law is ubiquitous in literally every area of the law, there are prescribed elements of a crime, a delict (or tort), contractual agreements and so forth. The elements of law however, do not lend themselves to conventional scientific enquiry; the legislative provisions, common law and Latin maxims simply do not have any quantifiable mathematical proclivities. In math, 1+1 is an elementary platitude, in law, 1+1 is simply a foreign antigen, an anomaly the law simply cannot absorb and apply.,The scientific and mathematical ineptitude of the law is concerning, especially in a world where society has such little faith in an over burdened judicial system. Where there is so much slowness and uncertainty in legal processes, you would think that as a necessity, the law ought to know the answer to 1+1. I kept thinking that maybe if the law knew the answer to 1+1, it would be more precise, efficient, and quicker in its application. Perhaps if lawyers could discover a Philosopher?€?s Stone, we could turn the elements of law, into gold.,A mixture of sheer coincidence and subconscious intent led me to stumble upon my own Philosopher?€?s Stone, something that would breathe life into the scientifically inept nature of law, Data Analytics. Analytics is the computational analysis of data or statistics, this analysis allows for insights into patterns, predictions and decision making. Some Analytics tools include Artificial Intelligence, Machine Learning and Mathematical Algorithms. I soon discovered that just like the Alchemists of old, Analytics would allow me to master the art of transmutation. Analytics would allow me formulate methods of taking the law and transforming it into unique forms of intrinsic value, not gold in this instance, but into calculable scales and metrics, into elements that are scientifically and statistically receptive.,The term ?€?Algorithmic Lawyer?€? is tragically oxymoronic, lawyers are known for being eloquent, master tacticians in all things governed by law, not practitioners of advanced algorithms. Math has been known to induce the odd nervous twitch in even the best lawyers, however the prospect of providing clients with the most efficient and expedient legal services ought to make the twitch dissipate. The client?€?s needs always come first, even if those needs require lawyers to make uncomfortable adjustments. The cost of legal services has come under immense scrutiny as of late, with legislators even proposing a cap on legal fees. The quality of legal services is also a contentious issue, especially in the public sector. The general sentiment is that legal services for the indigent provided by state institutions such as Legal Aid and other NGO?€?s like law clinics are terminally inferior. This is a rather dubious assertion given the insurmountable work ?€?load these institutions have to work through. The private sector is also not immune to criticisms of below par legal services, regular complaints by clients to the Law Society can attest to this.,As society continues to make technological advancements clients are beginning to compel their lawyers and legal institutions to make technological advancements along with it. The solution to the low quality and/or high cost of some legal services is purely an algorithmic one. The inherent efficiency of Analytics guarantees heightened legal precision and expedience that translates into cost efficiency for both lawyers and their clients. Famed legal technology pioneer and former CEO of SeyfarthLean Consulting Ken Grady puts it Aptly: ?€?Lawyers and Law organizations that want to survive the re-structuring of the industry must become adept at understanding and using technology, including algorithms, to deliver what clients want in legal services. Waiting for clients to request such changes will put the organization at an extreme disadvantage. By the time the client asks, the need is so well developed that an unresponsive department or firm will be well behind the curve to develop an adequate reply?€?,This then begs the question, who is this algorithmic lawyer and what does he/she do? His day is made up of using predictive models that transform legal prediction from a purely speculative exercise into one of scientific and algorithmically informed conjecture. She is immersed in the world of Bayesian Theory, Generalized Sequential Patterns and Decision Science. He puts more time into legal strategy and problem solving than into onerous iterative legal tasks he knows Machine Learning algorithms would thrive at. She does not spend hours researching at the expense of formulating creative solutions, her research tools don?€?t merely give her a list of a few hundred cases that she still has to read through dealing with an isolated issue. Her Analytics models know the law inside and out, they give her intuitive research results in minutes. He need not read through entire judgements because a series of algorithms can read and process hundreds of them in seconds, draft the necessary pleas and subsequently predict outcomes and make decisions based on what they have read. He is not fettered by onerous drafting and manual document review, his Analytics processes can review a one thousand page document in minutes and through the use of Decision Science, direct him to the steps and decisions the contents of the document necessitate. The use of Pattern and Trend Recognition in contracts, legal transactions, statements and pleas is flawless. Not only can he predict the future, he is the future., He is not a ?€?glorified scribe?€?, a highly paid ?€?paper-pusher?€?, he need not be. She knows the statistical odds of winning a case, she also knows how to mathematically increase those odds. He can predict after how many days a person is most likely to breach a contract and predict the type of breach and why. She can predict which legislative provisions are most likely to be contravened, by whom, why, when and what effect this contravention will have. The mathematical mitigation of legal risk is a seamless task both intuitive and simple; these are but some of the advantages of algorithmic legal processes.,Algorithms are all around us, they dictate social, economic and even political constructs; it is time they dictate legal ones as well. Legal organizations that have the ability to harness this power are vested with infinite sources of insights, strategy and efficiency. What does this all mean for lawyers and legal organizations that are not yet algorithmic? It means that the time has come to familiarize themselves with algorithms and Analytics as a whole because the private and indeed some parts of the public sector already have. They are also beginning to demand more technologically advanced legal services. Terminally disruptive over-hauls of legal processes will not be helpful; instead the systematic and gradual assimilation of the law and its application is what is required. The Analytics industry has to ease the legal field into the world of algorithms with easy-to- use intuitive Analytics solutions.,When we founded Gotham Analytics it was with the view to introduce a transcendent system of advanced law to the world. The law is an ancient art that is thousands of years old, that means thousands of years of data, of experience, of wins and losses that is lost forever if we cannot find ways to hone in on it. Analytics solutions are able to consolidate, rationalize and distil all of that data into simple algorithmic processes that enhance the potency of contemporary legal solutions. Data Analytics comprises of a great deal of things; a small aspect of those things is everything that is of Alchemy and Algorithmic Lawyers.
Humanity is going to be okay! ?? The big bad robots are not going to come and get you...,In a recent??,, Bill Gates commented, ?€?First the machines will do a lot of jobs for us and not be super intelligent?€? A few decades after that though the intelligence is strong enough to be a concern. I agree with Elon Musk and some others on this and don?€?t understand why some people are not concerned.?€?,??As someone immersed in data analytics who works every day toward making machines smarter and more capable, I find this fear of machine intelligence alarmist and worse, limiting.,Fear of the unknown is as old as mankind. The potential for what we can do with data and machine intelligence far outstrips our limited understanding of what we can do with it. That doesn?€?t mean you don?€?t pursue it. It?€?s okay to be cautious. We should never lose sight of the need to protect people and preserve our humanity, but attitudes need to change with technology if we are to make advances.,In my role as an analytics evangelist, I go to all types of companies, look at their data, and propose things of value. Every day I am challenged to ?€?prove it?€? and yet, even when I do, some won?€?t believe their eyes. There?€?s a kneejerk reaction that makes people skeptical and fearful of change because analytics ask people to let go of some of our decision-making powers as human beings. We should embrace the capabilities of analytics and not rush to fear of losing control.,If I can come up with a computer doctor better than your current doctor, would you as a patient consider it? Would you as a doctor use it? For example, if we do an analysis of common genes between diseases such as obesity and asthma, we can construct a virtual dictionary that defines those genes. We can then take the human genome and check it against that dictionary to see who?€?s got those genes and use a proven data source to see who?€?s afflicted with either of the diseases. With that information we can predict who?€?s obese and who?€?s asthmatic, and vice versa. If we can do that across a collection of diseases, we would have a tool for being proactive with healthcare and promoting wellness.
Your firm is awash in workforce data and your team of data scientists have been charged with making sense of it and extracting meaningful insights the business can use. How do you do this? There are many different approaches to analyzing workforce data to choose from but the amount of relevant insight yielded really depends on the goals of the analysis.,If the goal is to take more of a??traditional approach to workforce analytics, where the same datasets are massaged to try to squeeze ever more meaning from them then there?€?s no need to change the??,??approach.,However, if the goal is to introduce a new dataset that more directly measures a huge factor affecting business performance (people themselves),??,??for??,, then it?€?s worth considering adding Talent Analytics?€? dataset??,. Talent Analytics?€? dataset isn?€?t a replacement for what workforce analytics currently measures, but an enhancement.,Current ?€?workforce analytics?€? measures things like ?€? Attrition went down by 15% last year. Traditional workforce analytics can answer ?€?why?€? to some degree, but could use a lot of help in this area. If you add our talent analytics to this mix, you could find out if there are any patterns about??,, the people who are leaving.,What is interesting is that with the examples above, in addition to reporting on why history happened, the addition of our dataset moves the discussion towards,??instead of only reporting on what has happened.,Meaning, having talent analytics would allow leaders to predict the employee impact on performance before rolling out a new program. Having talent analytics available for inclusion in predictive models allows businesses to look for and often find??,.


The??,??is always published Monday.??Starred articles are new additions or updated content, posted between Thursday and Sunday.,??,??|??,??|??,??|??,??|??,??|??,??|??
Where , the happiest place in New York City? A mathematics and statistics team from the University of Vermont, with a little help from Twitter data, seems to know! (hint: UWS!), ,Read the entire post??
There?€?s a massive telescope on the drawing board that hasn?€?t even started construction yet, but when it?€?s finished in 2024,,it?€?ll generate more data in a single day than the entire Internet. For scientists to ensure they?€?ll be able to handle all that raw information, they need to start working on new computing technologies now. Fortunately, IBM is on it., The computing giant is collaborating with ASTRON (the Netherlands Institute of Radio Astronomy) to develop the next-generation computer tech needed to handle the colossal amount of data captured by the Square Kilometer Array (SKA), a new radio telescope that will spread sensing equipment over a span 3,000 kilometers wide, or about the width of the continental U.S.A. , Read the entire feature here: 

There is a great blog post at Sigmod by Joe Hellerstein about MADlib:,This effort is fundamentally in support of the quickly growing data science community:,Read the entire post??
IBM big data analytics software and powerful IBM systems to improve wind turbine placement for optimal energy output. Turbine placement is a major challenge for the renewable energy industry, and Vestas expects to accelerate the adoption of wind energy internationally and expand its business into new markets by overcoming this challenge. ??,Read more here:??

Recently, I?€?ve been feeling like I?€?ve stepped through a looking glass to another similar-but-very-different world. I?€?m steeped in 20+ years in corporate data warehousing and business intelligence practice. Throughout that time, there have been big and small technology improvements, but nothing truly disruptive (although??,??are coming).,Meanwhile, a completely different thread of data analysis has emerged, with roots in open-source software, notably Hadoop, primarily designed for processing massive amounts of semi-structured web data. As the technology has advanced, it?€?s making more and more??,.,The people using these new technologies have founded their own visions of what the role of an analyst looks like, or as they call it, a ?€?data scientist?€?.??,??and Jeff Hammerbacher coined the term a few years ago (there?€?s a??,??by??,), and DJ recently wrote??,.,Read the entire feature here:??

So the question is?€?when do you sample and when do you not??? And does it even matter anymore in the world of big data? ??,As I?€?ll lay out here, in most cases today there is no point in wasting energy worrying about it.?? As long as a few basic criteria are met, do whatever you prefer.,First, let?€?s take care of the cases where sampling just won?€?t work.?? If you need to find the top 100 spending customers, you can?€?t do that with a sample.?? You?€?ll have to look at every single customer to accurately identify the top 100.?? However, such scenarios, while common, aren?€?t the most prevalent type of analytic requirement.?? They do represent an easy victory for the ?€?no sampling?€? crowd, however.?? Similarly, even a model built on a sample will need to be applied to the universe to use it appropriately.?? So, when it comes time to deploy, sampling isn?€?t an option.,Read more at:??


